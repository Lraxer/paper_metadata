@article{DBLP:journals/tmc/LiZXLWL25,
	author = {Zhetao Li and
                  Xiyu Zeng and
                  Yong Xiao and
                  Chengxin Li and
                  Wentai Wu and
                  Haolin Liu},
	title = {Pattern-Sensitive Local Differential Privacy for Finite-Range Time-Series
                  Data in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {1--14},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3445973},
	doi = {10.1109/TMC.2024.3445973},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZXLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-series data is crucial for the development of mobile crowdsensing (MCS). Participant’s privacy is one of the major concerns because MCS data often contain sensitive individual information. Existing privacy-preserving mechanisms for time-series data do not preserve salient patterns of the time series and take into account that the perturbed data may fall outside the valid data interval, leading to data distortion. To overcome these deficiencies, we first perform dynamic feature extraction and incorporate an adaptive sampling scheme that is sensitive to the distinction of short-term patterns and stable patterns. Then a Bounded Laplace (BLP) mechanism is adopted with a theoretical guarantee on the data perturbation range so as to address the issue of data going beyond the valid range. We establish theoretically that the proposed Adaptive Sampling and Randomized perturbation mechanism based on dynamic Temporal patterns (ASRT) satisfies the metric-based  w w -event  ϵ \\epsilon -LDP for privacy protection. Empirical results of extensive experiments on realworld datasets demonstrate that our proposed method is superior to existing protection mechanisms and the efficacy of our ASRT in enhancing data utility without introducing outliers.}
}


@article{DBLP:journals/tmc/ZhangHTL25,
	author = {Junyang Zhang and
                  Jiahui Hou and
                  Ye Tian and
                  Xiang{-}Yang Li},
	title = {WordWhisper: Exploiting Real-Time, Hardware-Dependent IoT Communication
                  Against Eavesdropping},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {15--29},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3443333},
	doi = {10.1109/TMC.2024.3443333},
	timestamp = {Thu, 26 Jun 2025 15:39:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure protocol-independent communication is increasingly demanding to support information exchange among neighbor Internet of Things (IoT) devices. For example, recent works utilize ultrasound at the resonant frequency range of a gyroscope to build communication between a speaker and the gyroscope. However, they are vulnerable to eavesdropping attacks and may have limitations in communication delays. In this work, we present WordWhisper, an efficient, word-level, and speaker-to-gyroscope communication system, with which only the target device can receive the correct information. We theoretically analyze Micro-Electro-Mechanical System (MEMS) gyroscope resonance and propose a hardware-dependent mechanism to defend against eavesdropping, making non-target gyroscopes receive ineffective information. Note that WordWhisper is free of costly data collection from gyroscopes, we train and update our decoding model based on the synthesized data (generated from theoretical MEMS resonance analysis) rather than the costly collected data from gyroscopes. Meanwhile, we address the challenge of eavesdropping when it comes to multiple attackers. We evaluate WordWhisper over 50 MEMS gyroscopes and 100 words. Extensive evaluations demonstrate that WordWhisper can achieve word-level communication with 99.33% accuracy while the recognition accuracy drops to a random guess for the non-target. Our decoding delay is lower than 0.63 seconds.}
}


@article{DBLP:journals/tmc/ZhangZZYGLDC25,
	author = {Congwei Zhang and
                  Yifei Zou and
                  Zuyuan Zhang and
                  Dongxiao Yu and
                  Jorge Torres G{\'{o}}mez and
                  Tian Lan and
                  Falko Dressler and
                  Xiuzhen Cheng},
	title = {Distributed Age-of-Information Scheduling With {NOMA} via Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {30--44},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3459101},
	doi = {10.1109/TMC.2024.3459101},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZZYGLDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many emerging applications in edge computing require processing of huge volumes of data generated by end devices, using the freshest available information. In this paper, we address the distributed optimization of multi-user long-term average Age-of-Information (AoI) objectives in edge networks that use NOMA transmission. This poses a challenge of non-convex online optimization, which in existing work often requires either decision making in a combinatorial space or a global view of entire network states. To overcome this challenge, we propose a reinforcement learning-based framework that adopts a novel hierarchical decomposition of decision making. Specifically, we propose three different types of distributed agents to learn with respect to efficiency of AoI scheduling, fairness of AoI scheduling, as well as a high-level policy balancing these potentially conflicting design objectives. Not only does the proposed decomposition improve learning performance due to disentanglement of different design objectives/rewards, but it also enables the algorithm to learn the best policy while also learning the explanations – as actions can be directly compared in terms of the design objectives. Our evaluations show that the proposed algorithm improves the long-term average AoI by  200 % − 300 % 200\\%{-}300\\%  and 400% compared to prior works with NOMA and the optimal solution without NOMA, respectively.}
}


@article{DBLP:journals/tmc/HuangWWYFSWL25,
	author = {Ziyao Huang and
                  Weiwei Wu and
                  Kui Wu and
                  Hang Yuan and
                  Chenchen Fu and
                  Feng Shan and
                  Jianping Wang and
                  Junzhou Luo},
	title = {{LI2:} {A} New Learning-Based Approach to Timely Monitoring of Points-of-Interest
                  With {UAV}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {45--61},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461708},
	doi = {10.1109/TMC.2024.3461708},
	timestamp = {Tue, 20 May 2025 16:48:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWWYFSWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) play a critical role in disaster response, swiftly gathering information from various points-of-interest (PoIs) across extensive areas. The freshness of this information is measured by the age of information (AoI), representing the time since the latest information acquisition of a specific PoI. However, devising AoI-minimizing routes for UAVs in obstructed post-disaster environments poses unique challenges that have yet to be fully overcome. Obstacles, like post-disaster barriers, can impede direct flight paths between PoIs, and limited battery life requires energy-conscious route planning. Additionally, existing solutions fail to universally minimize varying data freshness requirements. This research addresses the AoI-driven UAV travel problem, seeking to establish periodic routes that optimize AoI metrics while considering energy and general graph constraints. We develop a learning-based algorithm to enhance the current route iteratively, utilizing guidance from a deep reinforcement learning (DRL) agent and executing a series of operations to potentially decrease AoI while adhering to topological and energy constraints. The algorithm is validated on real post-disaster datasets, demonstrating significant improvements in various AoI metrics compared to other learning-based approaches. Furthermore, our algorithm outperforms approximation algorithms and can approach the global optimum when tailored to existing AoI-minimizing problems.}
}


@article{DBLP:journals/tmc/LiTHLZ25,
	author = {Dongxu Li and
                  Yuanming Tian and
                  Chuan Huang and
                  Qingwen Liu and
                  Shengli Zhou},
	title = {Design and Performance of Resonant Beam Communications - Part {I:}
                  Quasi-Static Scenario},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {62--71},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3458415},
	doi = {10.1109/TMC.2024.3458415},
	timestamp = {Fri, 14 Feb 2025 15:01:46 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiTHLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios. Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed. Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam. With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel. Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds. Finally, numerical results validate our analysis. Part II of this paper discusses the performance of the RBCom system under the mobile scenario.}
}


@article{DBLP:journals/tmc/ZhangFXZHCY25,
	author = {Guoming Zhang and
                  Heqiang Fu and
                  Zhijie Xiang and
                  Xinyan Zhou and
                  Pengfei Hu and
                  Xiuzhen Cheng and
                  Yanni Yang},
	title = {Ambient Light Reflection-Based Eavesdropping Enhanced With cGAN},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {72--85},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3460392},
	doi = {10.1109/TMC.2024.3460392},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangFXZHCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sound eavesdropping using light has been an area of considerable interest and concern, as it can be achieved over long distances. However, previous work has often lacked stealth (e.g., active emission of laser beams) or been limited in the range of realistic applications (e.g., using direct light from a device’s indicator LED or a hanging light bulb). In this paper, we present EchoLight, a non-intrusive, passive and long-range sound eavesdropping method that utilizes the extensive reflection of ambient light from vibrating objects to reconstruct sound. We analyze the relationship between reflection light signals and sound signals, particularly in situations where the frequency response of reflective objects and the efficiency of diffuse reflection are suboptimal. Based on this analysis, we have introduced an algorithm based on cGAN to address the issues of nonlinear distortion and spectral absence in the frequency domain of sound. We extensively evaluate EchoLight’s performance in a variety of real-world scenarios. It demonstrates the ability to accurately reconstruct audio from a variety of source distances, attack distances, sound levels, light sources, and reflective materials. Our results reveal that the reconstructed audio exhibits a high degree of similarity to the original audio over 40 meters of attack distance.}
}


@article{DBLP:journals/tmc/WangJM25,
	author = {Zhihao Wang and
                  Dingde Jiang and
                  Shahid Mumtaz},
	title = {Network-Wide Data Collection Based on In-Band Network Telemetry for
                  Digital Twin Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {86--101},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3456584},
	doi = {10.1109/TMC.2024.3456584},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangJM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Digital Twin Network (DTN) establishes a real-time virtual mirror of physical networks. Data collection plays an essential role in DTN, which collects the status data of physical network for building highly consistent digital twins. In this paper, we present a network-wide data collection scheme based on In-band Network Telemetry (INT). To build a lifelike mirror of the physical network, the probing path set is required to cover all links so that network topology, traffic load, and port-level device information is captured. We present a Latency-aware High-degree Replicated First (LHRF) vertex-cut graph partitioning algorithm to partition the network into several balanced subgraphs while trying to replicate the high-degree vertexes among partitions first. LHRF aims to balance the length and accumulated latency of the probing paths. With shorter and stabler probing latencies, the information received by digital twin can reflect the latest and consistent network-wide status. To prevent the packets from being fragmented due to overlong paths, a deep limited search (DLS) based path planning algorithm is employed to generate non-overlapped probing paths covering all edges in the separated subgraphs. Simulation results demonstrate that the proposed scheme generates more balanced INT paths with constrained path length and shorter, stabler probing delay.}
}


@article{DBLP:journals/tmc/HeHWLHA25,
	author = {Hanxiang He and
                  Xintao Huan and
                  Jing Wang and
                  Yong Luo and
                  Han Hu and
                  Jianping An},
	title = {P\({}^{\mbox{3}}\)ID: {A} Privacy-Preserving Person Identification
                  Framework Towards Multi-Environments Based on Transfer Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {102--116},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3459944},
	doi = {10.1109/TMC.2024.3459944},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeHWLHA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concerns surrounding privacy leakages caused by prevalent vision-based person identifications are countless. A promising privacy-preserving solution is to identify the wireless signals reflecting persons, which, however, faces a major challenge of losing efficacy in multi-environments. In this paper, we work on person identification based on wireless signals using transfer learning, toward tackling the performance deterioration across environments. We investigate the feature variations induced by environmental shifts based on data measurements. Lay our foundation on the feature alignment concept, we propose a novel wireless-based person identification framework using transfer learning. In the framework, we integrate a series of signal processing methods including signal selection, pre-processing, and augmentation, where the first includes a reference environment to assist the feature extraction while the latter two respectively reduce the data noise and improve the data diversity. We also propose a model generalization method where a neural network is employed to align features from different environments, which facilitates the extraction of environment-independent features while incorporating both person and environment information. On a real wireless testbed consisting of an Impulse Radio Ultra-WideBand (IR-UWB) radar, we build and publicly release a dataset with 22,264 samples of ten individuals from three environments, varying in testing distance and obstruction condition. Extensive experimental evaluations demonstrate that the proposed framework can improve the identification accuracy across environments, and surpasses state-of-the-art methods by up to 18.06%.}
}


@article{DBLP:journals/tmc/YiAKM25,
	author = {Juheon Yi and
                  Utku G{\"{u}}nay Acer and
                  Fahim Kawsar and
                  Chulhong Min},
	title = {Argus: Enabling Cross-Camera Collaboration for Video Analytics on
                  Distributed Smart Cameras},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {117--134},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3459409},
	doi = {10.1109/TMC.2024.3459409},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YiAKM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing video analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13× and 2.19× (4.86× and 1.60× compared to the state-of-the-art), while achieving comparable tracking quality.}
}


@article{DBLP:journals/tmc/HuQZLCGCL25,
	author = {Pengfei Hu and
                  Yuhang Qian and
                  Tianyue Zheng and
                  Ang Li and
                  Zhe Chen and
                  Yue Gao and
                  Xiuzhen Cheng and
                  Jun Luo},
	title = {t-READi: Transformer-Powered Robust and Efficient Multimodal Inference
                  for Autonomous Driving},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {135--149},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462437},
	doi = {10.1109/TMC.2024.3462437},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuQZLCGCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust perception become imperative. However, existing fusion methods often make two assumptions rarely holding in practice: i) similar data distributions for all inputs and ii) constant availability for all sensors. Because, for example, lidars have various resolutions and failures of radars may occur, such variability often results in significant performance degradation in fusion. To this end, we present t-READi, an adaptive inference system that accommodates the variability of multimodal sensory data and thus enables robust and efficient perception. t-READi identifies variation-sensitive yet structure-specific model parameters; it then adapts only these parameters while keeping the rest intact. t-READi also leverages a cross-modality contrastive learning method to compensate for the loss from missing modalities. Both functions are implemented to maintain compatibility with existing multimodal deep fusion methods. The extensive experiments evidently demonstrate that compared with the status quo approaches, t-READi not only improves the average inference accuracy by more than 6% but also reduces the inference latency by almost 15× with the cost of only 5% extra memory overhead in the worst case under realistic data and modal variations.}
}


@article{DBLP:journals/tmc/Ganesan25,
	author = {Ashwin Ganesan},
	title = {The Structure of Hypergraphs Arising in Cellular Mobile Communication
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {150--164},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3460170},
	doi = {10.1109/TMC.2024.3460170},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/Ganesan25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An assumption that researchers have often used to model interference in a wireless network is the unit disk graph model. While many theoretical results and performance guarantees have been obtained under this model, an open research direction is to extend these results to hypergraph interference models. Motivated by recent results that the worst-case performance of the distributed maximal scheduling algorithm is characterized by the interference degree of the hypergraph, in the present work we investigate properties of the interference degree of the hypergraph and the structure of hypergraphs arising from physical constraints. We show that the problem of computing the interference degree of a hypergraph is NP-hard and we prove some properties and results concerning this hypergraph invariant. We investigate which hypergraphs are realizable, i.e. which hypergraphs arise in practice, based on physical constraints, as the interference model of a wireless network. In particular, a question that arises naturally is: what is the maximal value of  r r  such that the hypergraph  K 1 , r K_{1,r}  is realizable? We determine this quantity for various integral and nonintegral values of the path loss exponent of signal propagation. We also investigate hypergraphs generated by line networks.}
}


@article{DBLP:journals/tmc/LiLXXJ25,
	author = {Yuchen Li and
                  Weifa Liang and
                  Zichuan Xu and
                  Wenzheng Xu and
                  Xiaohua Jia},
	title = {Budget-Constrained Digital Twin Synchronization and Its Application
                  on Fidelity-Aware Queries in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {165--182},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3455357},
	doi = {10.1109/TMC.2024.3455357},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLXXJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advance of mobile edge computing (MEC) and the Internet of Things (IoT), digital twin (DT) has become an emerging technology for provisioning IoT services between the real world and the cyber world. In this paper, we consider the state updating of DTs in an MEC network through synchronizing DTs with their physical objects. We make use of an energy-constrained UAV for data collection in a sensor network, as an illustrative example for the DT state updating of each object (sensor), and then use the DT data of objects (sensors) later for fidelity-aware query services. To this end, we first formulate a novel DT state staleness minimization, under a given update budget per update round. We then propose an optimal algorithm for a special case of the problem where the budget per update round is exactly  K K  objects synchronizing with their DTs. We then devise an algorithm for the DT state staleness minimization problem by reducing to the award collection maximization problem, assuming that the volume of the update data generated by each object per update round is given. Otherwise, we adopt a deep learning method to predict the volume of the update data. To demonstrate the importance of the DT state staleness in practical applications, we consider fidelity-aware query services in the MEC network, and we develop a cost-effective evaluation plan for each query. We finally evaluate the performance of the proposed algorithms through simulations. Simulation results demonstrate that the proposed algorithms are promising.}
}


@article{DBLP:journals/tmc/GongYXCN25,
	author = {Yongkang Gong and
                  Haipeng Yao and
                  Zehui Xiong and
                  C. L. Philip Chen and
                  Dusit Niyato},
	title = {Blockchain-Aided Digital Twin Offloading Mechanism in Space-Air-Ground
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {183--197},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3455417},
	doi = {10.1109/TMC.2024.3455417},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GongYXCN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space-air-ground (SAG) integrated heterogenous networks can provide pervasive intelligence services for various ground users (GUs). The network can help cellular networks release network resources and alleviate congestion pressure. Moreover, one important application of the network is that digital twin (DT) can enable nearly-instant wireless connectivity and highly-reliable data mapping from physical systems to digital world in a real-time fashion. The integration of SAG and DT (SAG-DT) reduces the gap between data analysis and physical status, which can further realize robust edge intelligence services. However, the random computation task arrival, time-varying channel gains, and the lack of mutual trust among ground GUs hinder better quality of service in the promising SAG-DT network. In this paper, we envision a SAG-DT integrated blockchain model to transfer the task data to the aerial network, and then perform the computation offloading, energy harvesting and privacy protection. Moreover, we propose a Lyapunov-aided multi-agent deep federated reinforcement learning (MADFRL) algorithm framework to optimize the CPU cycle frequency, the size of block, the number of DTs, and harvested energy to minimize the execution costs and privacy overhead. Extensive performance analyses indicate that the MADFRL algorithm framework can strengthen the data privacy via blockchain verification mechanism and approaches the optimal performance on the basis of lower computation complexity. Finally, simulation results corroborate that the proposed Lyapunov-aided MADFRL algorithm is superior to advanced benchmarks in terms of execution costs, task processing quantities and privacy overhead.}
}


@article{DBLP:journals/tmc/LiuZFSHT25,
	author = {Hao Liu and
                  Yinghai Zhou and
                  Binxing Fang and
                  Yanbin Sun and
                  Ning Hu and
                  Zhihong Tian},
	title = {{PHCG:} {PLC} Honeypoint Communication Generator for Industrial IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {198--209},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3455564},
	doi = {10.1109/TMC.2024.3455564},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZFSHT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of mobile and wireless technologies, the industrial manufacturing sector has entered the era of automation. The proliferation of mobile devices, sensor networks, and remote monitoring systems enables factory equipment to be more flexibly connected and controlled. However, the trend towards industrial networks also brings new challenges. Industrial control systems (ICSs) and programmable logic controllers (PLCs) are more susceptible to hacker attacks and interference. Honeypoints have been developed to protect ICSs from addressing these threats, including potential internal attacks. Honeypoints are active deception systems that mitigate the limitations of conventional defense mechanisms, which successfully entice and neutralize internal enemies. This paper presents the PLC Honeypoint Communication Generator (PHCG), enhancing honeypoint protective capabilities in industrial IoT systems. Using an automated construction process, PHCG provides a convenient and efficient deployment method, ensuring quick and effective functioning. The functionality of a PLC relies on a data generation model trained on PLC response data. This model allows PHCG to imitate genuine PLC responses accurately when given authorized commands. The experimental results illustrate the adaptability of information produced by PHCG in different communication processes, with satisfactory timescales for both model training and data generation.}
}


@article{DBLP:journals/tmc/DaiHLXXL25,
	author = {Penglin Dai and
                  Biao Han and
                  Ke Li and
                  Xincao Xu and
                  Huanlai Xing and
                  Kai Liu},
	title = {Joint Optimization of Device Placement and Model Partitioning for
                  Cooperative {DNN} Inference in Heterogeneous Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {210--226},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3457793},
	doi = {10.1109/TMC.2024.3457793},
	timestamp = {Fri, 31 Oct 2025 14:06:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiHLXXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {EdgeAI represents a compelling approach for deploying DNN models at network edge through model partitioning. However, most existing partitioning strategies have primarily concentrated on homogeneous environments, neglecting the effect of device placement and their inapplicability to heterogeneous settings. Moreover, these strategies often rely on either data parallelism or model parallelism, each presenting its own limitations, including data synchronization and communication overhead. This paper aims at enhancing inference performance through a pipeline system of devices through leveraging both parallel and sequential relationships among them. Accordingly, the problem of Multi-Device Cooperative DNN Inference is formulated by optimizing both device placement and model partitioning, taking into account the unique characteristics of heterogeneous edge resources and DNN models, with the goal of maximizing throughput. To this end, we propose an evolutionary device placement technique to determine the pipeline stage of devices by enhancing a variant of particle swarm optimization. Subsequently, an adaptive model partitioning strategy is developed by combining intra-layer and inter-layer model partitioning based on dynamic programming and the input-output mapping of DNN layers, respectively, to accommodate edge resource limitations. Finally, we construct a simulation model and a prototype, and the extensive results demonstrate that our proposed algorithm outperforms current state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/YangHLACYC25,
	author = {Yanni Yang and
                  Pengfei Hu and
                  Jun Luo and
                  Zhenlin An and
                  Jiannong Cao and
                  Dongxiao Yu and
                  Xiuzhen Cheng},
	title = {Romeo: Fault Detection of Rotating Machinery via Fine-Grained mmWave
                  Velocity Signature},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {227--242},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3463955},
	doi = {10.1109/TMC.2024.3463955},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangHLACYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time velocity monitoring is pivotal for fault detection of rotating machinery. However, existing methods rely on either troublesome deployments of optical encoders and IMU sensors or various tachometers delivering coarse-grained velocity measurements insufficient for fault detection. To overcome these limitations, we propose Romeo as the first work to exploit the mmWave radar for rotating machinery fault detection by extracting a fine-grained velocity signature. Though mmWave radars should capture instant rotation information with their claimed high sensitivity and sampling rate, direct adoption entails significant efforts for high-precision velocity measurement per radar to handle; particularly, exhausted system calibration and noise interference. To this end, we first develop a phase-velocity model to characterize the relationship between the mmWave signal phase and the fine-grained angular velocity. We then explore the geometric properties of specific positions in the rotation trajectory to precisely calibrate the rotation sensing model, leading to an iterative algorithm for accurate angular velocity measurement. Finally, we propose a simple yet effective fault detection algorithm by extracting a unique velocity signature. Our extensive experiments show Romeo achieves a median error of 0.4 ∘ ^\\circ /s for fine-grained angular speed measurement, outperforming SOTA solutions with over ×16 angular speed granularity and ×7 measurement precision.}
}


@article{DBLP:journals/tmc/ZhangGLC25,
	author = {Tong Zhang and
                  Yu Gou and
                  Jun Liu and
                  Jun{-}Hong Cui},
	title = {Traffic Load-Aware Resource Management Strategy for Underwater Wireless
                  Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {243--260},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3459896},
	doi = {10.1109/TMC.2024.3459896},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources.}
}


@article{DBLP:journals/tmc/XueWPXL25,
	author = {Jing Xue and
                  Die Wu and
                  Jian Peng and
                  Wenzheng Xu and
                  Tang Liu},
	title = {Charger Placement With Wave Interference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {261--275},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3460403},
	doi = {10.1109/TMC.2024.3460403},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XueWPXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To guarantee the reliability for WRSNs, placing sufficient static chargers effectively ensures charging coverage for the entire network. However, this approach leads to a considerable number of sensors located within charging overlaps. The destructive wave interference caused by concurrent charging in these overlaps may weaken sensors received power, thereby negatively impacting charging performance. This work addresses a CHArging utIlity maximizatioN (CHAIN) problem, which aims to maximize the overall charging utility while considering wave interference among multiple chargers. Specifically, given a set of stationary sensors, we investigate how to determine optimal positions for a fixed number of chargers. To tackle this problem, we first develop a charging model with wave interference, then propose a two-step charger placement scheme to identify the optimal charger positions. In the first step, we maximize the overall additive power of the waves involved in interference by selecting an appropriate initial position for each charger. Then, in the second step, we maximize the overall charging utility by finding the optimal final position for each charger around its initial position. Finally, to evaluate the performance of our scheme, we conduct extensive simulations and field experiments and the results suggest that CHAIN performs better than the existing algorithms.}
}


@article{DBLP:journals/tmc/ChenCHTZ25,
	author = {Wenjie Chen and
                  Haoyu Chen and
                  Tingxuan Han and
                  Wei Tong and
                  Sheng Zhong},
	title = {Secure Two-Party Frequent Itemset Mining With Guaranteeing Differential
                  Privacy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {276--292},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3464744},
	doi = {10.1109/TMC.2024.3464744},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenCHTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent itemset mining is an essential task in data analysis. Therefore, it is crucial to design practical methods for privacy-preserving frequent itemset mining, enabling private data analysis. For two-party data analysis tasks, each party possesses its portion of the data and is reluctant to share the data with the other. While secure computation can enable two-party frequent itemset mining, the output of exact top- k k  itemsets may still leave the adversary a chance to infer the sensitive information. Differential privacy has been utilized in various data analysis tasks to safeguard participating individuals. However, addressing how to ensure differential privacy for two-party frequent itemset mining remains unexplored. To prevent each party’s data from being leaked to the other while achieving differential privacy for releasing the output, this paper investigates the problem of differentially private two-party frequent itemset mining. We first propose a practical method that can efficiently select the frequent items of the union of two confidential databases in a differentially private way without the need to combine all elements. Then we extend this technique for general frequent itemset mining. Extensive experiments were conducted on real-world datasets, and the results show that the proposed method can achieve satisfactory utility with affordable overheads.}
}


@article{DBLP:journals/tmc/LiangZW25,
	author = {Yu Liang and
                  Sheng Zhang and
                  Jie Wu},
	title = {Scrava: Super Resolution-Based Bandwidth-Efficient Cross-Camera Video
                  Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {293--305},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461879},
	doi = {10.1109/TMC.2024.3461879},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiangZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massively deployed cameras form a tightly connected network which generates video streams continuously. Benefiting from advances in computer vision, automated real-time analytics of video streams can be of practical value in various scenarios. As cameras become more dense, cross-camera video analytics has emerged. Combining video contents from multiple cameras for analytics is certainly more promising than single-camera analytics, which can realize cross-camera pedestrian tracking and cross-camera complex behavior recognition. Some works focused on optimization of cross-camera video analytic applications, but most of them ignore specific network situation between cameras and edge servers. Furthermore, most of them ignore the super resolution technique, which is proven to be a source of efficiency. In this paper, we first verify the potential gain of super resolution on cross-camera video analytic tasks. Then, we design and implement a cross-camera real-time video streaming analytic system,  S c r a v a {\\mathsf {Scrava}} , which leverages super resolution to augment low-resolution videos and simultaneously reduce bandwidth consumption.  S c r a v a {\\mathsf {Scrava}}  enables real-time cross-camera video analytics and enhances video segments with the SR module under poor network conditions. We take cross-camera pedestrian tracking as an example, and experimentally verifies the effectiveness of super resolution on real-time cross-camera video analytics. Compared with using low-resolution video segments,  S c r a v a {\\mathsf {Scrava}}  can improve the F1 score by 47.16%, verifying the feasibility of exploiting super resolution to improve the performance of real-time cross-camera video analytic systems.}
}


@article{DBLP:journals/tmc/YuHPKK25,
	author = {Seung Min Yu and
                  Kyuwon Han and
                  Jihong Park and
                  Seong{-}Lyun Kim and
                  Seung{-}Woo Ko},
	title = {Combinatorial Data Augmentation: {A} Key Enabler to Bridge Geometry-
                  and Data-Driven WiFi Positioning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {306--320},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3465510},
	doi = {10.1109/TMC.2024.3465510},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuHPKK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the emergence of various wireless sensing technologies, numerous positioning algorithms have been introduced in the literature, categorized into geometry-driven positioning (GP) and data-driven positioning (DP). These approaches have respective limitations, e.g., a non-line-of-sight issue for GP and the lack of a high-dimensional and labeled dataset for DP, which could be complemented by integrating both methods. To this end, this paper aims to introduce a novel principle called combinatorial data augmentation (CDA), a catalyst for the two approaches’ seamless integration. Specifically, GP-based data samples augmented from different positioning element combinations are called preliminary estimated locations (PELs), which can be used as high-dimensional inputs for DP. We confirm the CDA’s effectiveness from field experiments based on WiFi round-trip times (RTTs) and inertial measurement units (IMUs) by designing several CDA-based positioning algorithms. First, we show that CDA offers various metrics quantifying each PEL’s reliability, thereby extracting important PELs for WiFi RTT positioning. Second, CDA helps compute the observation error covariance matrix of a Kalman filter for fusing two position estimates derived by WiFi RTTs and IMUs. Third, we use the important PELs and the above position estimate as the corresponding input feature and the real-time label for fingerprint-based positioning as a representative DP algorithm. It provides accurate and reliable positioning results, with an average positioning error of 1.58 (m) and a standard deviation of 0.90 (m).}
}


@article{DBLP:journals/tmc/ZhiBCXWXH25,
	author = {Mingjian Zhi and
                  Yuanguo Bi and
                  Lin Cai and
                  Wenchao Xu and
                  Haozhao Wang and
                  Tianao Xiang and
                  Qiang He},
	title = {Knowledge-Aware Parameter Coaching for Communication-Efficient Personalized
                  Federated Learning in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {321--337},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3464512},
	doi = {10.1109/TMC.2024.3464512},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhiBCXWXH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized Federated Learning (pFL) can improve the accuracy of local models and provide enhanced edge intelligence without exposing the raw data in Mobile Edge Computing (MEC). However, in the MEC environment with constrained communication resources, transmitting the entire model between the server and the clients in traditional pFL methods imposes substantial communication overhead, which can lead to inaccurate personalization and degraded performance of mobile clients. In response, we propose a Communication-Efficient pFL architecture to enhance the performance of personalized models while minimizing communication overhead in MEC. First, a Knowledge-Aware Parameter Coaching method (KAPC) is presented to produce a more accurate personalized model by utilizing the layer-wise parameters of other clients with adaptive aggregation weights. Then, convergence analysis of the proposed KAPC is developed in both the convex and non-convex settings. Second, a Bidirectional Layer Selection algorithm (BLS) based on self-relationship and generalization error is proposed to select the most informative layers for transmission, which reduces communication costs. Extensive experiments are conducted, and the results demonstrate that the proposed KAPC achieves superior accuracy compared to the state-of-the-art baselines, while the proposed BLS substantially improves resource utilization without sacrificing performance.}
}


@article{DBLP:journals/tmc/GaoOCGLLH25,
	author = {Demin Gao and
                  Liyuan Ou and
                  Yongrui Chen and
                  Xiuzhen Guo and
                  Ruofeng Liu and
                  Yunhuai Liu and
                  Tian He},
	title = {Physical-Layer {CTC} From {BLE} to Wi-Fi With {IEEE} 802.11ax},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {338--351},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462941},
	doi = {10.1109/TMC.2024.3462941},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GaoOCGLLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi is the de facto standard for providing wireless access to the Internet in the 2.4 GHz ISM band. Tens of billions of Wi-Fi devices (e.g., smartphones) have been shipped worldwide with limited types of wireless radios operating only when Wi-Fi connectivity is available, making it challenging to access data in heterogeneous IoT devices. However, the direct connection between Wireless Personal Area Network (WPAN) technologies, such as Bluetooth, and Wi-Fi presents challenges due to the inherent distinct physical layer. In our work, a novel communication method called BlueWi has been introduced, which serves as a cross technology communication method that enables BLE devices to establish connections and engage in communication with Wi-Fi based WPAN networks. We let BLE signals hitchhike on ongoing Wi-Fi signals, enabling Wi-Fi to recognize specific BLE signal waveforms in the frequency domain. By analyzing the decoded Wi-Fi payload, BlueWi can retrieve the BLE data, ensuring this method remains fully compatible with existing commodity Wi-Fi hardware. The direct sequence spread spectrum scheme is appended to handle general BLE frames and can be considered as “COPY” operation, which allows for better correlation and detection of the signal at the receiver. Evaluations conducted using both USRP and commodity devices have demonstrated that BlueWi can achieve concurrent wireless communication from BLE commercial chips to Wi-Fi networks with a frame reception rate exceeding 96%.}
}


@article{DBLP:journals/tmc/NieCYYJ25,
	author = {Jiali Nie and
                  Yuanhao Cui and
                  Zhao{-}Hui Yang and
                  Weijie Yuan and
                  Xiaojun Jing},
	title = {Near-Field Beam Training for Extremely Large-Scale {MIMO} Based on
                  Deep Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {352--362},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462960},
	doi = {10.1109/TMC.2024.3462960},
	timestamp = {Thu, 27 Mar 2025 16:52:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NieCYYJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extremely Large-scale Array (ELAA) is considered a frontier technology for future communication systems, playing a crucial role in enhancing the rate and spectral efficiency of wireless networks. As ELAA employs a multitude of antennas operating at higher frequencies, users are typically situated in the near-field region where the spherical wavefront propagates. Near-field beam training requires information on both angle and distance, which inevitably leads to a significant increase in the beam training overhead. To address this challenge, we propose a near-field beam training method based on deep learning. Specifically, we employ a convolutional neural network (CNN) to efficiently extract channel characteristics from historical data by strategically selecting padding and kernel sizes. The negative value of the user average achievable rate is utilized as the loss function to optimize the beamformer, maximizing the achievable rate in multi-user networks without relying on predefined beam codebooks. Once deployed, the model requires only pre-estimated channel state information (CSI) to compute the optimal beamforming vector. Simulation results demonstrate that the proposed scheme achieves more stable beamforming gains and substantially outperforms traditional beam training approaches. Furthermore, owing to the inherent traits of deep learning methodologies, this approach substantially diminishes the near-field beam training overhead.}
}


@article{DBLP:journals/tmc/ChenZWHS25,
	author = {Ying Chen and
                  Jie Zhao and
                  Yuan Wu and
                  Jiwei Huang and
                  Xuemin Sherman Shen},
	title = {Multi-User Task Offloading in UAV-Assisted {LEO} Satellite Edge Computing:
                  {A} Game-Theoretic Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {363--378},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3465591},
	doi = {10.1109/TMC.2024.3465591},
	timestamp = {Tue, 11 Nov 2025 14:07:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZWHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicle (UAV)-assisted Low Earth Orbit (LEO) satellite edge computing (ULSE) networks can address the challenge communications issues in areas with harsh terrain and achieve global wireless coverage to provide services for mobile user devices (MUDs). This paper studies the LEO-UAV task offloading problem where MUDs compete for limited resources in the ULSE networks. We formulate the optimization problem with the goal of minimizing the cost of all MUDs while meeting resource constraint and satellite coverage time constraint. We first theoretically prove that this problem is NP-hard. We then reformulate the problem as a LEO-UAV task offloading game (LUTO-Game), and show that there is at least one Nash equilibrium solution for the LUTO-Game. We propose a joint UAV and LEO satellite task offloading (JULTO) algorithm to obtain the Nash equilibrium offloading strategy, and analyze the performance of the worst-case offloading strategy obtained by the JULTO algorithm. Finally, extensive experiments, including convergence analysis and comparison experiments, are carried out to validate the effectiveness of our JULTO algorithm.}
}


@article{DBLP:journals/tmc/ZhangGMXZGHS25,
	author = {Jie Zhang and
                  Song Guo and
                  Xiaosong Ma and
                  Wenchao Xu and
                  Qihua Zhou and
                  Jingcai Guo and
                  Zicong Hong and
                  Jun Shan},
	title = {Model Decomposition and Reassembly for Purified Knowledge Transfer
                  in Personalized Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {379--393},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3466227},
	doi = {10.1109/TMC.2024.3466227},
	timestamp = {Sun, 19 Jan 2025 14:43:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGMXZGHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized federated learning (pFL) is to collaboratively train non-identical machine learning models for different clients to adapt to their heterogeneously distributed datasets. State-of-the-art pFL approaches pay much attention on exploiting clients’ inter-similarities to facilitate the collaborative learning process, meanwhile, can barely escape from the irrelevant knowledge pooling that is inevitable during the aggregation phase, and thus hindering the optimization convergence and degrading the personalization performance. To tackle such conflicts between facilitating collaboration and promoting personalization, we propose a novel pFL framework, dubbed pFedC, to first decompose the global aggregated knowledge into several compositional branches, and then selectively reassemble the relevant branches for supporting conflicts-aware collaboration among contradictory clients. Specifically, by reconstructing each local model into a shared feature extractor and multiple decomposed task-specific classifiers, the training on each client transforms into a mutually reinforced and relatively independent multi-task learning process, which provides a new perspective for pFL. Besides, we conduct a purified knowledge aggregation mechanism via quantifying the combination weights for each client to capture clients’ common prior, as well as mitigate potential conflicts from the divergent knowledge caused by the heterogeneous data. Extensive experiments over various models and datasets demonstrate the effectiveness and superior performance of the proposed algorithm.}
}


@article{DBLP:journals/tmc/ChenLJMX25,
	author = {Xingcan Chen and
                  Chenglin Li and
                  Chengpeng Jiang and
                  Wei Meng and
                  Wendong Xiao},
	title = {WiPhase: {A} Human Activity Recognition Approach by Fusing of Reconstructed
                  WiFi {CSI} Phase Features},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {394--406},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461672},
	doi = {10.1109/TMC.2024.3461672},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLJMX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition (HAR) is an important task in the field of human-computer interaction. Given the penetration of WiFi devices in our daily lives, HAR using WiFi channel state information (CSI) is a more cost-efficient and comfortable approach. However, most existing approaches ignore the correlation between CSI sub-carriers, which makes their models inefficient and need to rely on deeper and more complex networks to further improve performance. To solve these problems, we propose a reconstructed WiFi CSI phase based HAR approach (WiPhase), which contains a two-stream model to fuse both temporal features and sub-carrier correlation features of reconstructed CSI phase. Specifically, a gated pseudo-Siamese network (GPSiam) is designed to capture the temporal features of the reconstructed sparse CSI phase integration representation (CSI-PIR), and a dynamic resolution based graph attention network (DRGAT) is designed to capture the nonlinear correlation between CSI sub-carriers by the reconstructed CSI phase graph. Furthermore, dendrite network (DD) makes the final decision by combining the features output from GPSiam and DRGAT. Experimental results show that WiPhase outperforms the existing state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/LuoZOZZWC25,
	author = {Ke Luo and
                  Kongyange Zhao and
                  Tao Ouyang and
                  Xiaoxi Zhang and
                  Zhi Zhou and
                  Hao Wang and
                  Xu Chen},
	title = {Efficient Coordination of Federated Learning and Inference Offloading
                  at the Edge: {A} Proactive Optimization Paradigm},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {407--421},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3466844},
	doi = {10.1109/TMC.2024.3466844},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoZOZZWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from hardware upgrades and deep learning techniques, more and more end devices can independently support a variety of intelligent applications. Further powered by edge computing technologies, the end-edge collaboration paradigm becomes one mainstream approach for achieving advanced edge intelligence (EI). To fully exploit the system resources, it is desirable to coordinate diverse EI services efficiently. Thus, we present a novel framework to jointly optimize the cost-performance trade-off for two distinct but typical EI services, where end devices simultaneously perform federated learning (FL) model training and conduct model inference with the assistance of edge offloading. However, balancing the long-term cost-performance trade-off is highly non-trivial, especially in the absence of knowledge of future system dynamics. Moreover, the capacity heterogeneity further increases the difficulty of service coordination among resource-limited end devices. To overcome these challenges, we first analyze the optimality of inference offloading decisions with and without FL model training and quantify their mutual effects due to local resource contention. By incorporating the loss estimation of FL training model, we then propose a novel proactive policy with theoretical guarantees, which proactively controls the stopping of FL training procedure to balance well the trade-offs between FL model performance and resource costs while fulfilling the inference performance requirements. Extensive results show the efficiency and robustness of our proposed algorithm for EI service coordination in dynamic end-edge collaboration scenarios.}
}


@article{DBLP:journals/tmc/SalehAI25,
	author = {Emad Saleh and
                  Malek M. Alsmadi and
                  Salama Ikki},
	title = {{FD} {MU-MIMO} Systems: Performance Analysis in the Presence of Imperfect
                  {CSI} and Non-Ideal Transceivers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {422--434},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462721},
	doi = {10.1109/TMC.2024.3462721},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SalehAI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work outlines a framework for full-duplex (FD) multiple-input multiple-output (MIMO) communication systems considering practical conditions, such as imperfect channel state information (CSI) and hardware impairments (HWIs). We analyze the performance of FD multi-user (MU) MIMO systems, specifically studying the effects of practical channel estimation errors and HWIs on the spectral efficiency (SE) performance of FD MU-MIMO systems. Maximum ratio combining/maximum ratio transmission (MRC/MRT) and zero-forcing reception/zero-forcing transmission (ZFR/ZFT) linear detectors/precoders are considered at the base station (BS). Moreover, linear minimum mean square error (LMMSE) and least square (LS) error estimation are used to estimate the channel at the BS. Mathematical derivations for the lower bounds of uplink (UL) and downlink (DL) SEs are presented in the context of imperfect CSI and HWIs. Computer simulations validate the analytical derivations. The results demonstrate the tightness of the obtained bounds.}
}


@article{DBLP:journals/tmc/WangLJZ25,
	author = {Xiaolu Wang and
                  Zijian Li and
                  Shi Jin and
                  Jun Zhang},
	title = {Achieving Linear Speedup in Asynchronous Federated Learning With Heterogeneous
                  Clients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {435--448},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461852},
	doi = {10.1109/TMC.2024.3461852},
	timestamp = {Mon, 11 Aug 2025 18:24:45 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework called Delayed Federated Averaging (DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with different stale global models at their own paces. Theoretical analyses demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par with the results of FedAvg for solving nonconvex problems. More importantly, DeFedAvg is the first AFL algorithm that provably achieves the desirable linear speedup property, which indicates its high scalability. Additionally, we carry out extensive numerical experiments using real datasets to validate the efficiency and scalability of our approach when training deep neural networks.}
}


@article{DBLP:journals/tmc/WuYGTWJ25,
	author = {Jianqiu Wu and
                  Zhongyi Yu and
                  Jianxiong Guo and
                  Zhiqing Tang and
                  Tian Wang and
                  Weijia Jia},
	title = {Two-Stage Deep Energy Optimization in IRS-Assisted UAV-Based Edge
                  Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {449--465},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461719},
	doi = {10.1109/TMC.2024.3461719},
	timestamp = {Mon, 03 Mar 2025 22:25:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuYGTWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating wireless-powered Mobile Edge Computing (MEC) with Unmanned Aerial Vehicles (UAVs) leverages computation offloading services for mobile devices, significantly enhancing the mobility and control of MEC networks. However, current research has not focused on customizing system designs for Terahertz (THz) communication networks. When dealing with THz communication, one must account for blockage vulnerability due to severe THz wave propagation attenuation and insufficient diffraction. The Intelligent Reflecting Surface (IRS) can effectively address these limitations in the model, enhancing spectrum efficiency and coverage capabilities while reducing blockage vulnerability in THz networks. In this paper, we introduce an upgraded MEC system that integrates IRS and UAVs into THz communication networks, focusing on a binary offloading policy for studying the computation offloading problem. Our primary objective is to optimize the energy consumption of both UAVs and User Electronic Devices, alongside refining the phase shift of the IRS reflector. The problem is a Mixed Integer Non-Linear Programming problem known as NP-hard. To tackle this challenge, we propose a two-stage deep learning-based optimization framework named Iterative Order-Preserving Policy Optimization (IOPO). Unlike exhaustive search methods, IOPO continually updates offloading decisions through an order-preserving quantization method, thereby accelerating convergence and reducing computational complexity, especially when handling complex problems with extensive solution spaces. The numerical results demonstrate that the proposed algorithm significantly improves energy efficiency and achieves near-optimal performance compared to benchmark methods.}
}


@article{DBLP:journals/tmc/LiLOZYZ25,
	author = {Ting Li and
                  Yinlong Liu and
                  Tao Ouyang and
                  Hangsheng Zhang and
                  Kai Yang and
                  Xu Zhang},
	title = {Multi-Hop Task Offloading and Relay Selection for IoT Devices in Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {466--481},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462731},
	doi = {10.1109/TMC.2024.3462731},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLOZYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To bridge the gap of conventional single-hop task offloading schemes in infrastructure-free scenarios, multi-hop task offloading schemes for IoT devices in Mobile Edge Computing (MEC) are desired to jointly optimize task offloading decisions and routing paths. In this paper, we investigate a hierarchical multi-hop edge computing framework and propose a joint Task Offloading and Relay Selection (TORS) scheme. It considers real-time computation at each relay node and employs directional searches to facilitate the task execution and results reporting at the fastest speed. However, finding the optimal TORS solution is a formidable challenge due to the time-varying network environments, the strong interdependence of decision sets across different time slots, and the high computational complexity. To address these challenges, we first leverage Lyapunov optimization to transform the stochastic TORS problem into a deterministic per-slot block problem, avoiding the need for extensive system prior knowledge. Subsequently, we propose a Soft Actor-Critic (SAC)-based algorithm, SAC-TORS, to find a satisfactory TORS solution with minimal computational complexity in a distributed manner. Accordingly, each IoT device can independently make self-determined and directional decisions with observable network information. Through extensive experiments, we demonstrate that the SAC-TORS outperforms state-of-the-art solutions, achieving performance improvements of up to 66%.}
}


@article{DBLP:journals/tmc/LiSLRJH25,
	author = {Xujing Li and
                  Sheng Sun and
                  Min Liu and
                  Ju Ren and
                  Xuefeng Jiang and
                  Tianliu He},
	title = {FedCRAC: Improving Federated Classification Performance on Long-Tailed
                  Data via Classifier Representation Adjustment and Calibration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {1},
	pages = {482--499},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3466208},
	doi = {10.1109/TMC.2024.3466208},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiSLRJH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has been a popular distributed training paradigm that enables to train a shared model with data privacy protection. However, non-Independent Identically Distribution and long-tailed data distribution characteristics across mobile devices results in evident performance degradation, especially for classification tasks. Although plenty of research studies devote to alleviating classification performance degradation caused by highly-skewed data distribution, they still cannot improve the distinguishability of model representation on hard-to-learn tail classes, and face obvious divergence of local classifiers in FL setting. To this end, we propose Federated Classifier Representation Adjustment and Calibration to improve the representation distinguishability of tail classes and achieve inter-client representation alignment with acceptable resource consumption on attaching operations. We first design a Class Similarity-Aware Margin matrix to enlarge class representation discrepancy and improve local classifier discriminability on tail classes during client-side local training process. To mitigate the divergence of local classifiers across clients, we further propose the Self Distillation Classifier Calibration to achieve the aggregated global classifier calibration with the assistance of generated pseudo representation samples via self-distillation manner. We conduct various experiments under wide-range long-tailed and heterogeneous data settings. Experimental results show that FedCRAC outperforms state-of-the-art methods in terms of accuracy and resource consumption.}
}


@article{DBLP:journals/tmc/HuangC25,
	author = {Miaona Huang and
                  Jun Chen},
	title = {Proactive Mobility Load Balancing Through Interior-Point Policy Optimization
                  for Open Radio Access Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {500--506},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3407979},
	doi = {10.1109/TMC.2024.3407979},
	timestamp = {Fri, 26 Sep 2025 08:27:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The future B5G/6G wireless networks are required to intelligently operate and optimize in all ranges of scenarios, however, most of the functions in self-organizing network (SON) architectures are rule-based and reactive, these rigid functions can not keep up with the dynamic nature of the future wireless networks. In this article, we propose an active load balancing framework considering mobility load balancing (MLB) and mobility robust optimization (MRO), which are two essential functions to ensure a seamless user experience for the future B5G/6G. The proposed method is based on random subspace Bayesian additive regression tree (RS-BART) and interior point policy optimization (IPO). Specifically, user trajectory is firstly forecasted by utilizing RS-BART, and then the joint problem of MLB and MRO is further solved by IPO. Simulations based on open radio access network (O-RAN) reveal that the proposed method reduces load deviation and radio link failure ratio by 40% and 50% compared to the previous ones.}
}


@article{DBLP:journals/tmc/Alorainy25,
	author = {Abdulaziz Alorainy},
	title = {Packet Scheduling in Multi-Flow Carrier Aggregation With QoS Provisioning:
                  Cross-Layer Design and Performance Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {507--524},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3425628},
	doi = {10.1109/TMC.2024.3425628},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/Alorainy25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-flow carrier aggregation (CA) is a key solution for improving data rates in heterogeneous networks (HetNets). Efficient data splitting is important to enhance the performance of multi-flow CA. In this paper, packet scheduling in multi-flow CA is investigated via a cross-layer design approach with comprehensive quality of service (QoS) provisioning. In particular, an approximation of the average queuing delay (AQD) in terms of the average buffers’ length and the average packet arrivals is presented. Then, using the presented AQD approximation, various packet scheduling schemes are developed in this paper. First, a low-complexity buffer and link aware heuristic packet scheduling scheme is proposed. Next, in order to reduce backhaul signalling overhead, a hybrid packet scheduling scheme that combines heuristic packet scheduling and random packet scheduling is considered. Then, an optimal packet scheduling scheme, albeit of high computational complexity, is proposed as a benchmark for other packet scheduling schemes. Also, the performances of the proposed packet scheduling schemes are analyzed and various data link layer performance parameters such as AQD, packet loss probability (PLP) and statistical queuing delay bound (SDB) are taken into consideration. Moreover, the complexity and the signalling overhead of the proposed packet scheduling schemes are investigated. Presented numerical results demonstrate that the heuristic packet scheduling scheme can achieve a performance that is similar to the performance of the optimal packet scheduling scheme while overcoming complexity and feasibility issues of the optimal packet scheduling scheme. Also, the hybrid packet scheduling scheme reduces signalling overhead while reaping some of the benefits of using buffer and link aware packet scheduling.}
}


@article{DBLP:journals/tmc/WuLCLJMW25,
	author = {Jinxiao Wu and
                  Xuanshu Luo and
                  Siwei Chen and
                  Yongqiang Lyu and
                  Xiangyang Ji and
                  Yan Meng and
                  Dongsheng Wang},
	title = {Continuous Authentication via Wrist Photoplethysmogram: An Extensive
                  Study},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {525--538},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3443208},
	doi = {10.1109/TMC.2024.3443208},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuLCLJMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous authentication (CA) based on wrist photoplethysmogram (PPG) has been increasingly studied, but still requires further extensive investigation on PPG reliability over time and heart rates for real-world deployments. In this paper, we first analyze the inadequacy of current research, i.e., limited generalization capability for new users and insufficient experiments due to the absence of across-session data under different heart rates (HR). To address these problems, we then propose a unified and scalable feature extraction framework for wrist PPG-based CA. Given a continuous PPG waveform, our framework first encodes the PPG of each period separately, then extracts variability features contained in consecutive multi-period PPG for user authentication. On two datasets with a total of 155 subjects, we evaluate the performances of our system using different across-session levels and HR intervals, respectively. Despite more stringent experimental settings, we achieve even better performances than in previous studies. Using the subject-exclusive cross-validation protocol, our system reaches an average accuracy of 92.1% under the constraint of equal error rates in across-session evaluation, and average accuracy ranges from 86.4% (high HR) to 91.4% (low HR) for different HR intervals.}
}


@article{DBLP:journals/tmc/YuanZLGCWY25,
	author = {Shijing Yuan and
                  Qingshi Zhou and
                  Jie Li and
                  Song Guo and
                  Hongyang Chen and
                  Chentao Wu and
                  Yang Yang},
	title = {Adaptive Incentive and Resource Allocation for Blockchain-Supported
                  Edge Video Streaming Systems: {A} Cooperative Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {539--556},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3437745},
	doi = {10.1109/TMC.2024.3437745},
	timestamp = {Mon, 06 Oct 2025 14:28:11 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuanZLGCWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing significantly enhanced the growth of edge-assistant video streaming applications. However, challenges such as unpredictable wireless conditions, resource constraints, and task redundancy have intertwined impacts on the overall performance of edge video streaming systems (EVS). Therefore, it is essential to have an integrated framework that addresses resource management, computational offloading, and video task preprocessing. Existing optimization strategies often neglect the simultaneous management of computational offloading, resource allocation, and video task preprocessing, leading to a suboptimal system utility. Moreover, they struggle to handle high-dimensional decision variables. On the other hand, learning-based adaptive schemes fall short in integrating distributed decisions and ensuring the scalability of wireless devices. Additionally, current approaches lack adaptive incentives. To bridge these gaps, we propose a novel framework called AIRA, which is based on improved multi-agent reinforcement learning (MARL) and smart contracts. AIRA manages resources, video compression, and adaptive incentives in a distributed manner. It consists of a MARL-driven cooperative learning algorithm (CLA) and a smart contract-guided adaptive incentive mechanism. Leveraging an actor-critic structure, the CLA enables wireless devices to master strategies for resource allocation, video task compression, and offloading, utilizing historical data. Notably, the CLA incorporates an attention mechanism to select pivotal tuples from the observation-action pairings among different agents, ensuring improved scalability and computational prowess. Evaluations based on real-world trajectories demonstrate that AIRA enables adaptive incentives. Compared to state-of-the-art approaches, CLA effectively enhances the long-term system utility and scalability of EVS.}
}


@article{DBLP:journals/tmc/LyuJWSWLHZ25,
	author = {Wenjun Lyu and
                  Xiaolong Jin and
                  Haotian Wang and
                  Yiwei Song and
                  Shuai Wang and
                  Yunhuai Liu and
                  Tian He and
                  Desheng Zhang},
	title = {Towards Workload-Constrained Efficient Order Assignment in Last-Mile
                  Delivery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {557--570},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3469236},
	doi = {10.1109/TMC.2024.3469236},
	timestamp = {Mon, 10 Nov 2025 08:08:13 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LyuJWSWLHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient order assignment in last-mile delivery benefits customers, couriers, and the platform. State-of-the-practice order assignment is based on the static delivery area partition, which cannot adapt well to the dynamic order quantity and destination distributions on different days. State-of-the-art methods focus on balancing order amounts or the payoff among couriers dynamically, neglecting the courier's workload in delivering orders. This paper explores the courier's heterogeneous behaviors for delivering orders to different destinations to measure the courier's workload and then achieve more efficient order assignments under the fair workload constraint. We design a workload-constrained order assignment system, called WORD, to reduce the cost of the last-mile delivery, i.e., the couriers’ total travel distance and overdue order rate. Specifically, the heterogeneous behaviors for delivering orders are first utilized for workload calculation. Then a two-stage order assignment framework is designed, including a sort-based initialization algorithm for initializing the assignment under the fair workload constraint and a coalition-game-based improvement algorithm for improving the assignment. Extensive evaluation results with real-world logistics data from one of the largest logistics companies in China show that WORD reduces the cost of the order assignment by up to 51.9% under the fair workload constraint compared to the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/ZhouZGLQ25,
	author = {Xiaobo Zhou and
                  Jiaxin Zeng and
                  Shuxin Ge and
                  Xilai Liu and
                  Tie Qiu},
	title = {Collaborative Video Streaming With Super-Resolution in Multi-User
                  {MEC} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {571--584},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461685},
	doi = {10.1109/TMC.2024.3461685},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZGLQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-increasing quality of experience (QoE) demand for video streaming has prompted the integration of video super-resolution and multi-access edge computing networks (MEC). With super-resolution, the low-resolution frames can be reconstructed into high-resolution ones by edge node and end device collaboratively, which is beneficial in improving QoE. However, the existing works focus on designing video streaming strategies in single-user scenarios, which cannot be applied to multi-user scenarios due to the resource contention among users, as well as the huge solution space of coupled bitrate selection and workload share between edge-end. To fill this gap, we propose a collaborative video streaming strategy with super-resolution in multi-user MEC networks, named Co-Video, to maximize the average QoE by making optimal bitrate selection and workload share. We first formulate the problem as an optimization problem towards maximum average QoE, where the QoE incorporates playback delay, video quality, and smoothness. Then, we transform the optimization problem into a partially observable Markov decision process (POMDP) and exploit the Co-Video strategy based on the multi-agent soft actor-critic (MASAC) algorithm. Specifically, Co-Video utilizes the branching actor network to converge to good policy stably. Finally, trace-driven simulations on real-world bandwidth traces demonstrate that Co-Video outperforms the state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/ZhangHXSZZL25,
	author = {Xiaotong Zhang and
                  Qingqiao Hu and
                  Zhen Xiao and
                  Tao Sun and
                  Jiaxi Zhang and
                  Jin Zhang and
                  Zhenjiang Li},
	title = {Few-Shot Adaptation to Unseen Conditions for Wireless-Based Human
                  Activity Recognition Without Fine-Tuning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {585--599},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3462466},
	doi = {10.1109/TMC.2024.3462466},
	timestamp = {Thu, 21 Aug 2025 14:48:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHXSZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless-based human activity recognition (WHAR) enables various promising applications. However, since WHAR is sensitive to changes in sensing conditions (e.g., different environments, users, and new activities), trained models often do not work well under new conditions. Recent research uses meta-learning to adapt models. However, they must fine-tune the model, which greatly hinders the widespread adoption of WHAR in practice because model fine-tuning is difficult to automate and requires deep-learning expertise. The fundamental reason for model fine-tuning in existing works is because their goal is to find the mapping relationship between data samples and corresponding activity labels. Since this mapping reflects the intrinsic properties of data in the perceptual scene, it is naturally related to the conditions under which the activity is sensed. To address this problem, we exploit the principle that under the same sensing condition, data of the same activity class are more similar (in a certain latent space) than data of other classes, and this property holds invariant across different conditions. Our main observation is that meta-learning can actually also transform WHAR design into a learning problem that is always under similar conditions, thus decoupling the dependence on sensing conditions. With this capability, general and accurate WHAR can be achieved, avoiding model fine-tuning. In this paper, we implement this idea through two innovative designs in a system called RoMF. Extensive experiments using FMCW, Wi-Fi and acoustic three sensing signals show that it can achieve up to 95.3% accuracy in unseen conditions, including new environments, users and activity classes.}
}


@article{DBLP:journals/tmc/XiaoXWHZ25,
	author = {Linchang Xiao and
                  Zili Xiao and
                  Di Wu and
                  Miao Hu and
                  Yipeng Zhou},
	title = {{CRS:} {A} Cost-Aware Resource Scheduling Framework for Deep Learning
                  Task Orchestration in Mobile Clouds},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {600--613},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3464491},
	doi = {10.1109/TMC.2024.3464491},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoXWHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) has found extensive application in supporting various mobile applications. The efficient execution of DL tasks is paramount for ensuring the effectiveness of AI-driven mobile applications. While previous research has predominantly focused on minimizing the completion time of DL tasks, the associated cost of execution has often been overlooked. Nonetheless, cost becomes a critical factor, particularly when utilizing DL infrastructure rented from third-party cloud service providers. In this paper, we propose a cost-aware resource scheduling framework named CRS for orchestrating DL task execution in mobile cloud systems. Our aim is to minimize server rental costs by strategically orchestrating DL jobs with diverse deadlines and workload scales across rented cloud servers. We formally define the problem and prove its NP-hardness by reducing it to a multiple knapsack problem (MKP). To solve this problem, we devise an approximation algorithm with a guaranteed upper bound performance ratio of  1 + 1 e − 1 1+\\frac{1}{e-1} . We evaluate CRS against state-of-the-art baselines through simulations of various job arrival scenarios in a real elastic mobile cloud system. The results demonstrate that CRS, on average, reduces rental costs by 45.1% compared to other baselines, while simultaneously achieving a shorter average job completion time (JCT) and maximum job completion time (i.e., makespan).}
}


@article{DBLP:journals/tmc/ZhaoWSHGC25,
	author = {Hang Zhao and
                  Shengling Wang and
                  Hongwei Shi and
                  Jian{-}Hui Huang and
                  Yu Guo and
                  Xiuzhen Cheng},
	title = {Exploitation and Confrontation: Sustainability Analysis of Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {614--626},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3463417},
	doi = {10.1109/TMC.2024.3463417},
	timestamp = {Mon, 13 Oct 2025 17:13:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoWSHGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Game theory is an effective analytical tool for crowdsourcing. Existing studies based on it share a commonality: the influence of players’ decisions is bilateral. However, the status is broken by the zero-determinant (ZD) strategy, where the ZD player can unilaterally control the opponent's expected payoff. Thereby, crowdsourcing games trigger conclusions that differ from traditional ones. By addressing three questions, this paper is the first work to analyze the turbulence in crowdsourcing caused by the inequality between the requestor and the worker in the ZD game. The first question reveals the potential for the requestor to exploit the worker; the second question quantifies the worker's tolerance towards exploitation, providing a basis for confrontation; the third question serves as the cornerstone for maintaining the crowdsourcing, regulating the requestor's exploitative behavior. To answer these questions, we extend ZD strategies from binary games to continuous ones, not only revealing the requestor's dominance but also enriching the theoretical system of ZD strategies and broadening their application. Furthermore, we introduce the worker's dissatisfaction degree, identifying the exponential trend and decay rate, revealing optimal timing and speed for the worker's effective confrontation and maximum exploitation for the requestor. Numerical simulations have validated the effectiveness of our analyses.}
}


@article{DBLP:journals/tmc/ZhangZKZM25,
	author = {Fenglin Zhang and
                  Zhebin Zhang and
                  Le Kang and
                  Anfu Zhou and
                  Huadong Ma},
	title = {mmTAA: {A} Contact-Less Thoracoabdominal Asynchrony Measurement System
                  Based on mmWave Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {627--641},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3461784},
	doi = {10.1109/TMC.2024.3461784},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZKZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Thoracoabdominal Asynchrony (TAA) is a key metric in respiration monitoring, which characterizes the non-parallel periodical motion of human's rib cage (RC) and abdomen (AB) during each breath. Long-term measurement of TAA plays a significant role in respiration health tracking. Existing TAA measurement methods including Respiratory Inductive Plethysmography (RIP) and Optoelectronic Plethysmography (OEP) all intrusive to subjects and have certain requirements on operation conditions, which limit their usage to hospital scenario. To address this gap, we propose mmTAA, the first mmWave-based, non-intrusive TAA measurement system ready for ubiquitous usage in daily-life. In mmTAA, we design a Two-stage RC-AB centroid finding module, aiming to identify the most probable location of RC-AB centroid, which can best represent RC and AB in mmWave sensing scenario. Subsequently, we design TAANet, a novel Convolutional Neural Network (CNN)-based architecture with residual modules, tailored for TAA measurement. Meanwhile, in order to address the imbalance of continuous data, we add imbalance information equalizer including feature and label equalizer during network training. We implement mmTAA on a commonly used multi-antenna mmWave radar. We prototype, deploy and evaluate mmTAA on 25 subjects and 25.7h data in total. mmTAA achieves 4.01 ∘ ^{\\circ }  MAE and 1.56 ∘ ^{\\circ }  average error, close to OEP method.}
}


@article{DBLP:journals/tmc/LaiLWLXWLL25,
	author = {Zeqi Lai and
                  Weisen Liu and
                  Qian Wu and
                  Hewu Li and
                  Jingxi Xu and
                  Yibo Wang and
                  Yuanjie Li and
                  Jun Liu},
	title = {SpaceRTC: Unleashing the Low-Latency Potential of Mega-Constellations
                  for Wide-Area Real-Time Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {642--661},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470330},
	doi = {10.1109/TMC.2024.3470330},
	timestamp = {Fri, 18 Jul 2025 10:39:51 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LaiLWLXWLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User-perceived latency is important for the quality of experience (QoE) of wide-area real-time communications (RTC). With the rapid development of low Earth orbit (LEO) mega-constellations, this paper explores a futuristic yet important problem facing the RTC community: can we exploit emerging mega-constellations to facilitate low-latency RTC globally? We carry out our quest in three steps. First, through a measurement study associated with a large number of geo-distributed RTC users, we quantitatively expose that the meandering routes in the client-to-cloud and inter-cloud-site segment of existing cloud-based RTC architecture are critical culprits for the high latency issue suffered by wide-area RTC sessions. Second, we propose SpaceRTC, a satellite-cloud cooperative framework that dynamically selects relay servers upon satellites and cloud sites to build an overlay network which enables diverse close-to-optimal paths. SpaceRTC judiciously allocates RTC flows of different sessions upon the network to facilitate low-latency interactions and adaptively selects bitrates to offer high user-perceived QoE in energy-limited space circumstance. Finally, we implement a testbed based on public constellation information and real-world RTC traces. Extensive experiments demonstrate that SpaceRTC can deliver near-optimal interactive latency, with up to 53.3% average latency reduction and 103.6% average bitrate improvement as compared to other state-of-the-art cloud-based solutions.}
}


@article{DBLP:journals/tmc/ChenJHZWXLL25,
	author = {Siyu Chen and
                  Hongbo Jiang and
                  Jingyang Hu and
                  Tianyue Zheng and
                  Mengyuan Wang and
                  Zhu Xiao and
                  Daibo Liu and
                  Jun Luo},
	title = {Echoes of Fingertip: Unveiling {POS} Terminal Passwords Through Wi-Fi
                  Beamforming Feedback},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {662--676},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3465564},
	doi = {10.1109/TMC.2024.3465564},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenJHZWXLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years, point-of-sale (POS) terminals are no longer limited to wired connections, with many relying on Wi-Fi for data transmission. Although Wi-Fi offers the convenience of wireless connectivity, it introduces significant security vulnerabilities. This work presents a non-intrusive method for eavesdropping POS passwords via Wi-Fi sensing, named  B e a m T h i e f {\\mathsf {BeamThief}} . Instead of conventional Wi-Fi Channel State Information (CSI) readings, our approach employs Wi-Fi Beamforming Feedback Information (BFI) for an eavesdropping attack. Compared to CSI, which can only be extracted through intruding into the Access Point (AP) or from a limited selection of commercial Wi-Fi cards (e.g., Intel-5300), BFI readings can be more readily obtained from a broad array of commercial Wi-Fi devices. A key technological contribution of  B e a m T h i e f {\\mathsf {BeamThief}}  is the development of an analysis model for predicting finger motion trajectories. This model is based on the physical relationship between BFI readings and finger motion, thus eliminating the need for extensive labeled training data. Furthermore, we employ Maximum Ratio Combining (MRC) to enhance the BFI series, ensuring performance across various scenarios. We implement  B e a m T h i e f {\\mathsf {BeamThief}}  using everyday commercial Wi-Fi devices and conduct a series of experiments to assess the impact of this attack. Experimental results demonstrate that  B e a m T h i e f {\\mathsf {BeamThief}}  achieves an accuracy rate 79 % \\%  in inferring 6-digit POS passwords within the top-100 attempts.}
}


@article{DBLP:journals/tmc/PengYWX25,
	author = {Cheng Peng and
                  Jun Yin and
                  Lei Wang and
                  Fu Xiao},
	title = {Bison: {A} Binary Sparse Network Coding Based Contents Sharing Scheme
                  for D2D-Enabled Mobile Edge Caching Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {677--695},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3463735},
	doi = {10.1109/TMC.2024.3463735},
	timestamp = {Tue, 19 Aug 2025 14:03:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengYWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge caching network (MEN), which enables popular or reusable content caching and sharing among adjacent mobile edge devices, has become a promising solution to reduce the traffic and burden over backhaul links. Network coding (NC), represented by classical random linear network coding (RLNC), is utilized to facilitate content delivery and increase throughput in MEN. However, as the harsh decoding condition results in unacceptable time and storage overhead, classical RLNC schemes struggle to be widely deployed in practice. In this work, we propose a cost-effective NC-based content-sharing scheme based on binary sparse network coding (BSNC), called Bison, for D2D-enabled MEN. Based on the shared relationship between the binary sparse coded block (BSCB), Bison first designs a caching maintenance module to characterize the sharing progress and maintain the caching state of each edge node. Then, Bison defines a matching metric named neighbor utility to evaluate neighbors’ matching values by considering nodes’ demand and content decodability. Guiding by the metric, Bison achieves the most beneficial matching relationship among edge nodes through a proposed online matching policy. Finally, Bison devises a coded block delivery strategy to enable the sharing of valuable content between two matched edge nodes. Extensive experiments in simulations and real-world Android testbeds demonstrate its effectiveness and efficiency, wherein Bison is at least 30% less than the RLNC-based scheme on time consumption and at least 10% less than the classical BSNC-based scheme on storage overhead. The results also show that our matching policy and coded block delivery strategy can perform with a low response latency on edge and mobile devices.}
}


@article{DBLP:journals/tmc/HuanLC25,
	author = {Ouwen Huan and
                  Tao Luo and
                  Mingzhe Chen},
	title = {Multi-Modal Image and Radio Frequency Fusion for Optimizing Vehicle
                  Positioning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {696--708},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3469252},
	doi = {10.1109/TMC.2024.3469252},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuanLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a multi-modal vehicle positioning framework that jointly localizes vehicles with channel state information (CSI) and images is designed. In particular, we consider an outdoor scenario where each vehicle can communicate with only one BS, and hence, it can upload its estimated CSI to only its associated BS. Each BS is equipped with a set of cameras, such that it can collect a small number of labeled CSI, a large number of unlabeled CSI, and the images taken by cameras. To exploit the unlabeled CSI data and position labels obtained from images, we design an meta-learning based hard expectation-maximization (EM) algorithm. Specifically, since we do not know the corresponding relationship between unlabeled CSI and the multiple vehicle locations in images, we formulate the calculation of the training objective as a minimum matching problem. To reduce the impact of label noises caused by incorrect matching between unlabeled CSI and vehicle locations obtained from images and achieve better convergence, we introduce a weighted loss function on the unlabeled datasets, and study the use of a meta-learning algorithm for computing the weighted loss. Subsequently, the model parameters are updated according to the weighted loss function of unlabeled CSI samples and their matched position labels obtained from images. Simulation results show that the proposed method can reduce the positioning error by up to 61% compared to a baseline that does not use images and uses only CSI fingerprint for vehicle positioning.}
}


@article{DBLP:journals/tmc/ChenLSL25,
	author = {Siguang Chen and
                  Qun Li and
                  Yanhang Shi and
                  Xue Li},
	title = {Debiased Device Sampling for Federated Edge Learning in Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {709--721},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3464740},
	doi = {10.1109/TMC.2024.3464740},
	timestamp = {Mon, 25 Aug 2025 12:15:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a privacy-preserved distributed machine learning paradigm, federated edge learning (FEL) was designed to absorb knowledge from user devices to construct intelligent services without transmitting raw data. However, this paradigm depends on the local training and model parameter transmission of user devices, therefore the computing power, storage capacity and network resources of the devices become the key factors to achieve energy well-budgeted and timely message transmission FEL. While in the wireless networks, those resources for devices are normally heterogeneous or limited. This paper aims to offer tangible solutions for optimal convergence and Quality of Service (QoS) assurance of FEL in wireless networks. First, we define a mathematical model for energy-efficient message transmission of FEL and formulate an optimization problem involving device sampling and resource allocation to attain optimal training convergence within energy and time constraints. Second, we theoretically analyze the impact of limited resources on sampling strategies and training convergence, thus simplifying the optimization problem for solvability. Third, we introduce an iterative heuristic algorithm that utilizes available resources to reduce client sampling bias. Extensive experiments show that our method can effectively obtain the debiased sampling strategy, and outperforms similar methods by minimizing device disconnection due to energy use and enhancing model convergence and performance.}
}


@article{DBLP:journals/tmc/QuWLHC25,
	author = {Xidi Qu and
                  Shengling Wang and
                  Kun Li and
                  Jian{-}Hui Huang and
                  Xiuzhen Cheng},
	title = {TidyBlock: {A} Novel Consensus Mechanism for DAG-based Blockchain
                  in IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {722--735},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3464494},
	doi = {10.1109/TMC.2024.3464494},
	timestamp = {Sun, 14 Sep 2025 12:31:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QuWLHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of directed acyclic graph (DAG)-based blockchain and Internet of Things (IoT) aims at improving the efficiency of data storage. However, if massive IoT data are not placed in an organized way, the search and usage of the data for upper-level applications can be burdensome, since they have to examine the data block by block, which also increases the difficulty of data verification, affecting consensus efficiency. To maintain the high throughput advantage of DAG-based blockchain applied in IoT and improve the data analysis efficiency, we propose a novel consensus mechanism named TidyBlock, including the transaction collation mechanism for block generation and the block selection mechanism for verification. The first mechanism can tidy up scattered transactions before they are packaged into blocks, while the second one can collate blocks to facilitate verification, realizing a two-layer collation of IoT data so as to increase analysis efficiency of upper-level IoT applications. Additionally, the second mechanism can provide a self-driven incentive for rational participants to follow the first one in case they are reluctant to do extra collation work. Theoretical analysis is provided to demonstrate the validity of our proposed algorithms by formal methods. Extensive simulations based on synthetic data verify the rationality and effectiveness of the proposed mechanisms.}
}


@article{DBLP:journals/tmc/LiuLLLGYWMDY25,
	author = {Sicong Liu and
                  Hao Luo and
                  Xiaochen Li and
                  Yao Li and
                  Bin Guo and
                  Zhiwen Yu and
                  Yuzhan Wang and
                  Ke Ma and
                  Yasan Ding and
                  Yuan Yao},
	title = {AdaKnife: Flexible {DNN} Offloading for Inference Acceleration on
                  Heterogeneous Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {736--748},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3466931},
	doi = {10.1109/TMC.2024.3466931},
	timestamp = {Sat, 23 Aug 2025 18:55:15 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLLLGYWMDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of deep neural network (DNN) intelligence into embedded mobile devices is expanding rapidly, supporting a wide range of applications. DNN compression techniques, which adapt models to resource-constrained mobile environments, often force a trade-off between efficiency and accuracy. Distributed DNN inference, leveraging multiple mobile devices, emerges as a promising alternative to enhance inference efficiency without compromising accuracy. However, effectively decoupling DNN models into fine-grained components for optimal parallel acceleration presents significant challenges. Current partitioning methods, including layer-level and operator or channel-level partitioning, provide only partial solutions and struggle with the heterogeneous nature of DNN compilation frameworks, complicating direct model offloading. In response, we introduce AdaKnife, an adaptive framework for accelerated inference across heterogeneous mobile devices. AdaKnife enables on-demand mixed-granularity DNN partitioning via computational graph analysis, facilitates efficient cross-framework model transitions with operator optimization for offloading, and improves the feasibility of parallel partitioning using a greedy operator parallelism algorithm. Our empirical studies show that AdaKnife achieves a 66.5% reduction in latency compared to baselines.}
}


@article{DBLP:journals/tmc/WuZWZNL25,
	author = {Yi Wu and
                  Xiande Zhang and
                  Tianhao Wu and
                  Bing Zhou and
                  Phuc Nguyen and
                  Jian Liu},
	title = {3D Facial Tracking and User Authentication Through Lightweight Single-Ear
                  Biosensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {749--762},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470339},
	doi = {10.1109/TMC.2024.3470339},
	timestamp = {Fri, 14 Feb 2025 20:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuZWZNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facial landmark tracking and 3D reconstruction have gained considerable attention due to their numerous applications such as human-computer interactions, facial expression analysis, and emotion recognition, etc. Traditional approaches require users to be confined to a particular location and face a camera under constrained recording conditions, which prevents them from being deployed in many application scenarios involving human motions. In this paper, we propose the first single-earpiece lightweight biosensing system, BioFace-3D, that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations. Our single-earpiece biosensing system takes advantage of the cross-modal transfer learning model to transfer the knowledge embodied in a high-grade visual facial landmark detection model to the low-grade biosignal domain. After training, our BioFace-3D can directly perform continuous 3D facial reconstruction from the biosignals, without any visual input. Additionally, by utilizing biosensors, we also showcase the potential for capturing both behavioral aspects, such as facial gestures, and distinctive individual physiological traits, establishing a comprehensive two-factor authentication/identification framework. Extensive experiments involving 16 participants demonstrate that BioFace-3D can accurately track 53 major facial landmarks with only 1.85 mm average error and 3.38% normalized mean error, which is comparable with most state-of-the-art camera-based solutions. Experiments also show that the system can authenticate users with high accuracy (e.g., over 99.8% within two trials for three gestures in series), low false positive rate (e.g., less 0.24%), and is robust to various types of attacks.}
}


@article{DBLP:journals/tmc/SoorkiAABCS25,
	author = {Mehdi Naderi Soorki and
                  Hossein Aghajari and
                  Sajad Ahmadinabi and
                  Hamed Bakhtiari Babadegani and
                  Christina Chaccour and
                  Walid Saad},
	title = {Catch Me If You Can: Deep Meta-RL for Search-and-Rescue Using LoRa
                  {UAV} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {763--778},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3468382},
	doi = {10.1109/TMC.2024.3468382},
	timestamp = {Wed, 08 Oct 2025 16:59:46 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SoorkiAABCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Long-range (LoRa) wireless networks have been widely proposed as efficient wireless access networks for battery-constrained Internet of Things (IoT) devices. However, applying the LoRa-based IoT network in search-and-rescue (SAR) operations will have limited coverage caused by high signal attenuation due to terrestrial blockages, especially in highly remote areas. To overcome this challenge, using unmanned aerial vehicles (UAVs) as a flying LoRa gateway to transfer messages from ground LoRa nodes to the ground rescue station can be a promising solution. In this paper, an artificial intelligence-empowered SAR operation framework using a UAV-assisted LoRa network in different unknown search environments is designed and implemented. The problem of the flying LoRa (FL) gateway control policy is modeled as a partially observable Markov decision process to move the UAV towards the LoRa transmitter carried by a lost person in the known remote search area. A deep reinforcement learning (RL)-based policy is designed to determine the adaptive FL gateway trajectory in a given search environment. Then, as a general solution, a deep meta-RL framework is used for SAR in any new and unknown environments. The proposed deep meta-RL framework integrates the information of the prior FL gateway experience in the previous SAR environments to the new environment and then rapidly adapts the UAV control policy model for SAR operation in a new and unknown environment. To analyze the performance of the proposed framework in real-world scenarios, the proposed SAR system is experimentally tested in three environments: a university campus, a wide plain, and a slotted canyon at Mongasht mountain ranges, Iran. Experimental results show that if the deep meta-RL-based control policy is applied instead of the deep RL-based one, the number of SAR time slots decreases from 141 to 50. Moreover, in the slotted canyon environment, the UAV energy consumption under the deep meta-RL policy is respectively 57% and 23% less than the deep RL and Actor-Critic RL policies.}
}


@article{DBLP:journals/tmc/DuDLLRZ25,
	author = {Yicong Du and
                  Huan Dai and
                  Hongbo Liu and
                  Guyue Li and
                  Yanzhi Ren and
                  Ke Zhang},
	title = {Efficient and Error-Free Secret Key Generation Leveraging Sorted Indices
                  Matching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {779--793},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3465042},
	doi = {10.1109/TMC.2024.3465042},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuDLLRZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secret key generation exploiting inherent channel randomness stands as an important paradigm for physical-layer security in wireless networks. However, existing work relying on quantization has some difficulties in eliminating inconsistent key bits due to the impact of ambient noise. Recent studies propose to match the segmented channel samples (i.e., channel episodes) of similar variation patterns between legitimate peers to achieve error-free key generation, but they also suffer from high computational overhead and reduced accuracy for large key lengths. This work proposes a secret key generation method based on sorted indices matching (SIM-SKG), aiming at efficient and error-free key generation. Specifically, we sort the channel samples to ensure each channel episode with a unique variation pattern for accurate matching. To avoid the impact of half-duplex communication mode and ambient noise, we propose to match the indices instead of the channel samples as in existing studies. We also develop a noise perturbation scheme that further mitigates the ambiguity during indices matching. Extensive experimental studies demonstrate the high efficiency and accuracy of SIM-SKG under various scenarios for both RSS and CSI channel measurements. Specifically, SIM-SKG achieves error-free key generation with a length of 2048 bits within as little as 1.7  m s e c msec . Moreover, theoretical analyses and experiments also confirm the security of the SIM-SKG method against various attacks.}
}


@article{DBLP:journals/tmc/WangLZLCJY25,
	author = {Hanling Wang and
                  Tianyu Li and
                  Mei Zhang and
                  Qing Li and
                  Huan Cui and
                  Yong Jiang and
                  Zhenhui Yuan},
	title = {Joint Configuration Optimization and {GPU} Allocation for Multi-Tenant
                  Real-Time Video Analytics on Resource-Constrained Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {794--811},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3465434},
	doi = {10.1109/TMC.2024.3465434},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLZLCJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying deep neural network (DNN) models on resource-constrained edge devices for real-time video analytics poses significant challenges due to the high resource demands of these models. Current edge-based video analytics approaches often overlook optimizing deep learning models and GPU resource allocations in multi-tenant scenarios. In this paper, we present JSAS-MTMGS, a collaborative video analytics system employing three innovative design strategies. First, we propose a novel video configuration optimization space based on a joint DNN model sharing and splitting scheme to balance computational loads for collaborative processing. This approach reduces network transmission data volume and alleviates resource contention. Second, we design a GPU resource allocation scheme that combines GPU batching with spatial sharing to optimize GPU utilization and increase system throughput, all without relying on costly offline latency collection. Finally, we define the configuration optimization problem alongside GPU allocation as a convex problem and apply convex optimization to make scheduling decisions dynamically. Our experiments demonstrate that JSAS-MTMGS has the best service quality among all compared algorithms.}
}


@article{DBLP:journals/tmc/FemeniasR25,
	author = {Guillem Femenias and
                  Felip Riera{-}Palou},
	title = {From Cells to Freedom: 6G's Evolutionary Shift With Cell-Free
                  Massive {MIMO}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {812--829},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3468003},
	doi = {10.1109/TMC.2024.3468003},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FemeniasR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell-free massive MIMO (CF-mMIMO) is emerging as a technological pillar for future sixth generation (6G) mobile networks, promising consistently high spectral and energy efficiencies across the coverage area. Despite the reported advantages of CF-mMIMO over traditional cellular-based massive MIMO (mMIMO), the extensive deployment of access points (APs) and the associated fronthaul links present significant economic and logistical challenges. This paper proposes a transitional framework to facilitate the gradual integration of CF-mMIMO into existing cellular infrastructures, allowing mobile network operators to progressively realize the benefits of a distributed network topology. A comprehensive analysis of the spectral, energy, and computational efficiencies in heterogeneous network scenarios, incorporating both macrocellular and cell-free components, is presented. Our contributions include a unified assessment framework encompassing spectral, energy and computational aspects, a novel channel virtualization mechanism for effective downlink precoding, and a realistic industry-backed power consumption model for joint network operation. The potential performance gains are demonstrated and guidelines for the incremental deployment of CF-mMIMO are provided through detailed numerical results, ensuring a balanced trade-off between integration costs and operational benefits. This approach aims to leverage the capabilities of emerging network architectures to achieve a seamless evolution towards fully distributed 6G networks.}
}


@article{DBLP:journals/tmc/ZhangGFWHG25,
	author = {Yu Zhang and
                  Yanmin Gong and
                  Lei Fan and
                  Yu Wang and
                  Zhu Han and
                  Yuanxiong Guo},
	title = {Quantum-Assisted Joint Virtual Network Function Deployment and Maximum
                  Flow Routing for Space Information Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {830--844},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3466857},
	doi = {10.1109/TMC.2024.3466857},
	timestamp = {Fri, 30 Jan 2026 16:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGFWHG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network function virtualization (NFV)-enabled space information network (SIN) has emerged as a promising method to facilitate global coverage and seamless service. This paper proposes a novel NFV-enabled SIN to provide end-to-end communication and computation services for ground users. Based on the multi-functional time expanded graph (MF-TEG), we jointly optimize the user association, virtual network function (VNF) deployment, and flow routing strategy (U-VNF-R) to maximize the total processed data received by users. The original problem is a mixed-integer linear program (MILP) that is intractable for classical computers. Inspired by quantum computing techniques, we propose a hybrid quantum-classical Benders’ decomposition (HQCBD) algorithm. Specifically, we convert the master problem of the Benders’ decomposition into the quadratic unconstrained binary optimization (QUBO) model and solve it with quantum computers. To further accelerate the optimization, we also design a multi-cut strategy based on the quantum advantages in parallel computing. Numerical results demonstrate the effectiveness and efficiency of the proposed algorithm and U-VNF-R scheme.}
}


@article{DBLP:journals/tmc/ZhengLJLYDCYYL25,
	author = {Naiyu Zheng and
                  Yuanchun Li and
                  Shiqi Jiang and
                  Yuanzhe Li and
                  Rongchun Yao and
                  Chuchu Dong and
                  Ting Chen and
                  Yubo Yang and
                  Zhimeng Yin and
                  Yunxin Liu},
	title = {AdaWiFi, Collaborative WiFi Sensing for Cross-Environment Adaptation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {845--858},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3474853},
	doi = {10.1109/TMC.2024.3474853},
	timestamp = {Sun, 07 Dec 2025 22:17:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengLJLYDCYYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) based Wi-Fi sensing has witnessed great development in recent years. Although decent results have been achieved in certain scenarios, Wi-Fi based activity recognition is still difficult to deploy in real smart homes due to the limited cross-environment adaptability, i.e. a well-trained Wi-Fi sensing neural network in one environment is hard to adapt to other environments. To address this challenge, we propose AdaWiFi, a DL-based Wi-Fi sensing framework that allows multiple Internet-of-Things (IoT) devices to collaborate and adapt to various environments effectively. The key innovation of AdaWiFi includes a collective sensing model architecture that utilizes complementary information between distinct devices and avoids the biased perception of individual sensors and an accompanying model adaptation technique that can transfer the sensing model to new environments with limited data. We evaluate our system on a public dataset and a custom dataset collected from three complex sensing environments. The results demonstrate that AdaWiFi is able to achieve significantly better sensing adaptation effectiveness (e.g. 30% higher accuracy with one-shot adaptation) as compared with state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/LiZLLCSYL25,
	author = {Zhao Li and
                  Lijuan Zhang and
                  Chengyu Liu and
                  Siwei Le and
                  Jie Chen and
                  Kang G. Shin and
                  Zheng Yan and
                  Jia Liu},
	title = {Interference Recycling: Effective Utilization of Interference for
                  Enhancing Data Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {859--874},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3467339},
	doi = {10.1109/TMC.2024.3467339},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLLCSYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless communication technologies, Internet of Things (IoT) has emerged as one of the most important application scenarios. Due to the high density of IoT devices and the limited spectrum resources, along with the miniaturization and sustainability requirements of these devices, the development of low-cost interference management (IM) methods has become crucial for widespread use of IoT. Interference has long been known to harm network performance. Since a desired signal can be distorted by interference, and thus be incorrectly decoded at the destination, we argue that interference can also be transformed intentionally to extract the desired data from interfering signal(s). Based on this observation, we propose Interference ReCycling (IRC) for the IoT. Under IRC, a recycling signal is generated using the interference a victim IoT device is subjected to, and then sent by the device’s associated gateway. Under the influence of the recycling signal, the desired data of the interfered/victim IoT transmission-pair can be recovered from the interference at the IoT device. We also show that the interfered user’s spectral efficiency (SE) with IRC can be optimized further by properly distributing the transmit power used for the desired signal’s transmission and the recycling signal. We validate the feasibility of IRC by implementing the method on the Universal Software Radio Peripheral (USRP) platform. Our theoretical analysis, experimental and numerical evaluation have shown that the proposed IRC can fully exploit interference, and hence can significantly improve the SE of the victim IoT device compared to other existing IM methods.}
}


@article{DBLP:journals/tmc/LiJSL25,
	author = {Zhen Li and
                  Chunxiao Jiang and
                  Jiachen Sun and
                  Jianhua Lu},
	title = {Resource Collaboration Between Satellite and Wide-Area Mobile Base
                  Stations in Integrated Satellite-Terrestrial Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {875--889},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3472081},
	doi = {10.1109/TMC.2024.3472081},
	timestamp = {Mon, 01 Dec 2025 14:11:25 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiJSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integrated satellite-terrestrial network with cascaded downlinks from satellites to wide-area mobile base stations and subsequently to terrestrial users enables global communication for terrestrial 4G/5G cellular users and is widely used in emergency rescue scenarios. However, in this network, satellites and wide-area mobile base stations are controlled by distinct resource scheduling systems with disparate packet queues, which means resources allocated by the satellite to the wide-area mobile base stations may not match the resources allocated by the wide-area mobile base stations to the terrestrial users, leading to coordination inefficiencies and resource wastage. To tackle this challenge, a resource collaborative scheduling mechanism based on cooperative game theory for cascaded downlinks is established, which effectively adapts to distinct resource scheduling systems with various QoS constraints. Then, the utility function of the Nash product is converted into a max-min problem, and a convex transformation method is proposed for the non-convex optimization problem. Simulation results demonstrate that the proposed collaborative scheduling mechanism effectively improves resource utilization and the transmission rate of cascaded downlinks.}
}


@article{DBLP:journals/tmc/DaiLSMCRWZL25,
	author = {Jiongyu Dai and
                  Lianjun Li and
                  Ramin Safavinejad and
                  Shadab Mahboob and
                  Hao Chen and
                  Vishnu V. Ratnam and
                  Haining Wang and
                  Jianzhong Zhang and
                  Lingjia Liu},
	title = {O-RAN-Enabled Intelligent Network Slicing to Meet Service-Level Agreement
                  {(SLA)}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {890--906},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3476338},
	doi = {10.1109/TMC.2024.3476338},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiLSMCRWZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing plays a critical role in enabling multiple virtualized and independent network services to be created on top of a common physical network infrastructure. In this paper, we introduce a deep reinforcement learning (DRL)-based radio resource management (RRM) solution for radio access network (RAN) slicing under service-level agreement (SLA) guarantees. The objective of this solution is to minimize the SLA violation. Our method is designed with a two-level scheduling structure that works seamlessly under Open Radio Access Network (O-RAN) architecture. Specifically, at an upper level, a DRL-based inter-slice scheduler is working on a coarse time granularity to allocate resources to network slices. And at a lower level, an existing intra-slice scheduler such as proportional fair (PF) is working on a fine time granularity to allocate slice dedicated resources to slice users. This setting makes our solution O-RAN compliant and ready to be deployed as an ‘xApp’ on the RAN Intelligent Controller (RIC). For performance evaluation and proof of concept purposes, we develop two platforms, one industry-level simulator and one O-RAN compliant testbed; evaluation on both platforms demonstrates our solution’s superior performance over conventional methods.}
}


@article{DBLP:journals/tmc/LaiWXLNS25,
	author = {Wenhai Lai and
                  Wenyu Wang and
                  Fan Xu and
                  Xin Li and
                  Shaobo Niu and
                  Kaiming Shen},
	title = {Adaptive Blind Beamforming for Intelligent Surface},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {907--923},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3468618},
	doi = {10.1109/TMC.2024.3468618},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LaiWXLNS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Configuring intelligent surface (IS) or passive antenna array without any channel knowledge, namely blind beamforming, is a frontier research topic in the wireless communication field. Existing methods in the previous literature for blind beamforming include the RFocus and the CSM, the effectiveness of which has been demonstrated on hardware prototypes. However, this paper points out a subtle issue with these blind beamforming algorithms: the RFocus and the CSM may fail to work in the non-line-of-sight (NLoS) channel case. To address this issue, we suggest a grouping strategy that enables adaptive blind beamforming. Specifically, the reflective elements (REs) of the IS are divided into three groups; each group is configured randomly to obtain a dataset of random samples. We then extract the statistical feature of the wireless environment from the random samples, thereby coordinating phase shifts of the IS without channel acquisition. The RE grouping plays a critical role in guaranteeing performance gain in the NLoS case. In particular, if we place all the REs in the same group, the proposed algorithm would reduce to the RFocus and the CSM. We validate the advantage of the proposed blind beamforming algorithm in the real-world networks at 3.5 GHz aside from simulations.}
}


@article{DBLP:journals/tmc/FanBPJSD25,
	author = {Rong Fan and
                  Azzedine Boukerche and
                  Pan Pan and
                  Zhigang Jin and
                  Yishan Su and
                  Fei Dou},
	title = {Secure Localization for Underwater Wireless Sensor Networks via {AUV}
                  Cooperative Beamforming With Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {924--938},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3472643},
	doi = {10.1109/TMC.2024.3472643},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanBPJSD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In harsh underwater environments, the localization of network nodes faces severe challenges due to open deployment environments. Most existing underwater localization methods suffer from privacy leaks. However, privacy protection schemes applied in terrestrial networks are not viable for underwater acoustic networks due to stratification effects and multipath complexities. In this paper, we introduce a secure localization scheme for underwater wireless sensor networks (UWSNs) utilizing cooperative beamforming among mobile underwater anchor nodes. With this scheme, the underwater sensor communicates and ranges with mobile anchor nodes to perform self-localization via time difference of arrival (TDOA) algorithm. However, the presence of eavesdroppers poses a threat by intercepting information emitted by the anchors. To avoid localization information leakage, then we model the secure localization requirement as a multi-anchors multi-objective dual joint optimization problem to enhance both security and energy performance. The deep reinforcement learning (DRL)-based multi-agent deep deterministic policy gradient (MADDPG) algorithm is applied to solve the optimization problem. Both simulation and field experimental results robustly validate the efficiency and accuracy of the proposed secure localization scheme.}
}


@article{DBLP:journals/tmc/HuangZNCW25,
	author = {Liang Huang and
                  Bincheng Zhu and
                  Runkai Nan and
                  Kaikai Chi and
                  Yuan Wu},
	title = {Attention-Based {SIC} Ordering and Power Allocation for Non-Orthogonal
                  Multiple Access Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {939--955},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470828},
	doi = {10.1109/TMC.2024.3470828},
	timestamp = {Tue, 25 Feb 2025 12:23:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZNCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-orthogonal multiple access (NOMA) emerges as a superior technology for enhancing spectral efficiency, reducing latency, and improving connectivity compared to orthogonal multiple access. In NOMA networks, successive interference cancellation (SIC) plays a crucial role in decoding user signals sequentially. The challenge lies in the joint optimization of SIC ordering and power allocation, a task complicated by the factorial nature of ordering combinations. This study introduces an innovative solution, the Attention-based SIC Ordering and Power Allocation (ASOPA) framework, targeting an uplink NOMA network with dynamic SIC ordering. ASOPA aims to maximize weighted proportional fairness by employing deep reinforcement learning, strategically decomposing the problem into two manageable subproblems: SIC ordering optimization and optimal power allocation. We use an attention-based neural network to process real-time channel gains and user weights, determining the SIC decoding order for each user. A baseline network, serving as a mimic model, aids in the reinforcement learning process. Once the SIC ordering is established, the power allocation subproblem transforms into a convex optimization problem, enabling efficient calculation of optimal transmit power for all users. Extensive simulations validate ASOPA’s efficacy, demonstrating a performance closely paralleling the exhaustive method, with over 97% confidence in normalized network utility. Compared to the current state-of-the-art implementation, i.e., Tabu search, ASOPA achieves over 97.5% network utility of Tabu search. Furthermore, ASOPA has two orders of magnitude less execution latency than Tabu search when $N=10$ and even three orders magnitude less execution latency less than Tabu search when $N=20$ . Notably, ASOPA maintains a low execution latency of approximately 50 milliseconds in a ten-user NOMA network, aligning with static SIC ordering algorithms. Furthermore, ASOPA demonstrates superior performance over baseline algorithms besides Tabu search in various NOMA network configurations, including scenarios with imperfect channel state information, multiple base stations, and multiple-antenna setups. These results underscore the robustness and effectiveness of ASOPA, demonstrating its ability to ability to achieve good performance across various NOMA network environments.}
}


@article{DBLP:journals/tmc/LiuP25,
	author = {Yanbing Liu and
                  Chunyi Peng},
	title = {Handling Failures in Secondary Radio Access Failure Handling in Operational
                  5G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {956--969},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3477462},
	doi = {10.1109/TMC.2024.3477462},
	timestamp = {Sat, 11 Oct 2025 18:21:43 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we conduct a measurement study with three US operators to reveal three types of problematic failure handling on secondary radio access which have not been reported before. Compared to primary radio access failures, secondary radio access failures do not hurt radio access availability but significantly impact data performance, particularly when 5G is used as secondary radio access to boost throughput. Improper failure handling results in significant throughput loss, which is unnecessary in most instances. We then pinpoint the root causes behind these three types of problematic failure handling. When 5G provides higher throughput, failures are more likely to be falsely triggered by a specific event, causing the User Equipment (UE) to unnecessarily lose well-performing 5G connections. Moreover, after failures, the recovery of secondary radio access may fail due to inconsistent parameter settings or be delayed due to missing specific signaling fields. To address these issues, we propose SCGFailure Manager (SFM), a solution to optimize the detection and recovery of secondary radio access failures. Our evaluation results demonstrate that SFM can effectively avoid 60%-80% of problematic failure handling and double throughput in more than half of failure instances.}
}


@article{DBLP:journals/tmc/ShahHVCJ25,
	author = {Devan Shah and
                  Ruoqi Huang and
                  Nisha Vinayaga{-}Sureshkanth and
                  Tingting Chen and
                  Murtuza Jadliwala},
	title = {ScooterID: Posture-Based Continuous User Identification From Mobility
                  Scooter Rides},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {970--984},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3473609},
	doi = {10.1109/TMC.2024.3473609},
	timestamp = {Sat, 06 Sep 2025 20:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShahHVCJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobility scooters serve as a powerful last-mile transportation tool for people with mobility challenges. Given the unique riding behavior and posture of mobility scooter riders, such user-specific mobility scooter ride data has tremendous potential towards the design of continuous user identification and authentication mechanisms. However, there have been no prior research efforts in the literature exploring this unique modality for the design of continuous user identification techniques. To address this gap, this paper proposes ScooterID, the first framework which employs rider posture data collected from cameras on mobility scooters to continuously identify (and authenticate) users/riders. As part of this framework, a machine learning based model comprising of a spatio-temporal Graph Convolutional Network and a body-part-informed encoder is designed to effectively capture a user’s subtle upper-body movements during mobility scooter rides into discriminating embedding vectors. These embeddings can then be used to reliably and continuously identify and authenticate users/riders. Experiments with real-world mobility scooter ride data show that ScooterID achieves high levels of authentication accuracy with few enrollment video samples. ScooterID also performs efficiently on resource-constrained devices (e.g., Raspberry Pis) and is robust against adversarial perturbations to authentication inputs.}
}


@article{DBLP:journals/tmc/WangWYLWFH25,
	author = {Xin Wang and
                  Yanhan Wang and
                  Ming Yang and
                  Feng Li and
                  Xiaoming Wu and
                  Lisheng Fan and
                  Shibo He},
	title = {FedSiam-DA: Dual-Aggregated Federated Learning via Siamese Network
                  for Non-IID Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {985--998},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3472898},
	doi = {10.1109/TMC.2024.3472898},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangWYLWFH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an effective mobile edge computing framework that enables multiple participants to collaboratively train intelligent models, without requiring large amounts of data transmission while protecting privacy. However, FL encounters challenges due to non-independent and identically distributed (non-IID) data from different participants. The existing methods, whether focusing on local training or global aggregation, often suffer from insufficient unilateral optimization. Achieving effective local-global collaborative optimization, particularly in the absence of additional reference models or datasets, is both crucial and challenging. To address this, we propose a novel approach: Dual-Aggregated Federated learning based on a triple Siamese network (FedSiam-DA). This method enhances the FL algorithm on both client and server sides. On the client side, we establish a triple Siamese network incorporating a stop-gradient scheme, which leverages a contrastive learning strategy to control the update directions of local models. On the server side, we introduce a dual aggregation mechanism with dynamic weights for local updates, improving the global model’s ability to assimilate personalized knowledge from local models. Extensive experiments on multiple benchmark datasets demonstrate that FedSiam-DA significantly improves model performance under non-IID data conditions compared to existing methods.}
}


@article{DBLP:journals/tmc/ZhouZY25,
	author = {Qihang Zhou and
                  Xinglin Zhang and
                  Zheng Yang},
	title = {Unknown Worker Recruitment With Long-Term Incentive in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {999--1015},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3471569},
	doi = {10.1109/TMC.2024.3471569},
	timestamp = {Tue, 20 Jan 2026 14:46:52 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many mobile crowdsensing applications require efficient recruitment of workers whose qualities are often unknown a priori. While prior research has explored multi-armed bandit-based mechanisms with short-term incentives to address this unknown worker recruitment challenge, these mechanisms mostly neglect the enduring participation issues stemming from privacy concern and selection starvation in the long-term task. Therefore, in this paper, we focus on incentivizing long-term participation of unknown workers, thereby providing crucial assurance for crowdsensing applications. We first establish an auction framework based on shuffle differential privacy (SDP), where we leverage SDP’s privacy amplification effect to mitigate privacy-related utility loss when dealing with the privacy-sensitive worker and the utility-sensitive platform. Following this, we model the selection requirements of workers as fairness constraints and propose two novel fairness-aware incentive mechanisms, GFA and IFA, to ensure group and individual fairness for unknown workers, respectively. Theoretical analyses highlight the desirable properties of GFA and IFA, complemented by an in-depth exploration of fairness violation and regret. Finally, numerical simulations are conducted on two real-world datasets, validating the superior performance of the proposed mechanisms.}
}


@article{DBLP:journals/tmc/NingJWNGL25,
	author = {Zhaolong Ning and
                  Hongjing Ji and
                  Xiaojie Wang and
                  Edith C. H. Ngai and
                  Lei Guo and
                  Jiangchuan Liu},
	title = {Joint Optimization of Data Acquisition and Trajectory Planning for
                  UAV-Assisted Wireless Powered Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1016--1030},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470831},
	doi = {10.1109/TMC.2024.3470831},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NingJWNGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of Internet of Things (IoT) technology has led to the emergence of a large number of Intelligent Sensing Devices (ISDs). Since their limited physical sizes constrain the battery capacity, wireless powered IoT networks assisted by Unmanned Aerial Vehicles (UAVs) for energy transfer and data acquisition have attracted great interest. In this paper, we formulate an optimization problem to maximize system energy efficiency while satisfying the constraints of UAV mobility and safety, ISD quality of service and task completion time. The formulated problem is constructed as a Constrained Markov Decision Process (CMDP) model, and a Multi-agent Constrained Deep Reinforcement Learning (MCDRL) algorithm is proposed to learn the optimal UAV movement policy. In addition, an ISD-UAV connection assignment algorithm is designed to manage the connection in the UAV sensing range. Finally, performance evaluations and analysis based on real-world data demonstrate the superiority of our solution.}
}


@article{DBLP:journals/tmc/RohBKK25,
	author = {Emily Jimin Roh and
                  Hankyul Baek and
                  Donghyeon Kim and
                  Joongheon Kim},
	title = {Fast Quantum Convolutional Neural Networks for Low-Complexity Object
                  Detection in Autonomous Driving Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1031--1042},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470328},
	doi = {10.1109/TMC.2024.3470328},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RohBKK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object detection applications, especially in autonomous driving, have drawn attention due to the advancements in deep learning. Additionally, with continuous improvements in classical convolutional neural networks (CNNs), there has been a notable enhancement in both the efficiency and speed of these applications, making autonomous driving more reliable and effective. However, due to the exponentially rapid growth in the complexity and scale of visual signals used in object detection, there are limitations regarding computation speeds while conducting object detection solely with classical computing. Motivated by this, this paper proposes the quantum object detection engine (QODE), which implements a quantum version of CNN, named QCNN, in object detection. Furthermore, this paper proposes a novel fast quantum convolution algorithm that processes the multi-channel of visual signals based on a small number of qubits and constructs the output channel data, thereby achieving relieved computational complexity. Our QODE, equipped with fast quantum convolution, demonstrates feasibility in object detection with multi-channel data, addressing a limitation of current QCNNs due to the scarcity of qubits in the current era of quantum computing. Moreover, this paper introduces a heterogeneous knowledge distillation training algorithm that enhances the performance of our QODE.}
}


@article{DBLP:journals/tmc/RamakanthTM25,
	author = {Rudrapatna Vallabh Ramakanth and
                  Vishrant Tripathi and
                  Eytan H. Modiano},
	title = {Monitoring Correlated Sources: AoI-Based Scheduling is Nearly Optimal},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1043--1054},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3471391},
	doi = {10.1109/TMC.2024.3471391},
	timestamp = {Tue, 25 Feb 2025 09:00:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RamakanthTM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the design of scheduling policies to minimize the monitoring error of a collection of correlated sources, where only one source can be observed at any given time. We model correlated sources as a discrete-time Wiener process, where the increments are multivariate normal random variables, with a general covariance matrix that captures the correlation structure between the sources. Under a Kalman filter-based optimal estimation framework, we show that the performance of all scheduling policies oblivious to instantaneous error can be lower and upper bounded by the weighted sum of Age of Information (AoI) across the sources for appropriately chosen weights. We use this insight to design scheduling policies that are only a constant factor away from optimality, and make the rather surprising observation that AoI-based scheduling that ignores correlation is sufficient to obtain performance guarantees. We also derive scaling results showing that the optimal error scales roughly as the square of the system's dimensionality, even with correlation. Finally, we provide simulation results to verify our claims.}
}


@article{DBLP:journals/tmc/LiwangGHCWJ25,
	author = {Minghui Liwang and
                  Zhibin Gao and
                  Seyyedali Hosseinalipour and
                  Zhipeng Cheng and
                  Xianbin Wang and
                  Zhenzhen Jiao},
	title = {Long-Term or Temporary? Hybrid Worker Recruitment for Mobile Crowd
                  Sensing and Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1055--1072},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3470993},
	doi = {10.1109/TMC.2024.3470993},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiwangGHCWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper explores an interesting worker recruitment challenge where the mobile crowd sensing and computing (MCSC) platform hires workers to complete tasks with varying quality requirements and budget limitations, amidst uncertainties in worker participation and local workloads. We propose an innovative hybrid worker recruitment framework that combines offline and online trading modes. The offline mode enables the platform to overbook long-term workers by pre-signing contracts, thereby managing dynamic service supply. This is modeled as a 0-1 integer linear programming (ILP) problem with probabilistic constraints on service quality and budget. To address the uncertainties that may prevent long-term workers from consistently meeting service quality standards, we also introduce an online temporary worker recruitment scheme as a contingency plan. This scheme ensures seamless service provisioning and is likewise formulated as a 0-1 ILP problem. To tackle these problems with NP-hardness, we develop three algorithms, namely, i) exhaustive searching, ii) unique index-based stochastic searching with risk-aware filter constraint, iii) geometric programming-based successive convex algorithm. These algorithms are implemented in a stagewise manner to achieve optimal or near-optimal solutions. Extensive experiments demonstrate our effectiveness in terms of service quality, time efficiency, etc.}
}


@article{DBLP:journals/tmc/GuoWQQ25,
	author = {Qi Guo and
                  Di Wu and
                  Yong Qi and
                  Saiyu Qi},
	title = {Dual Class-Aware Contrastive Federated Semi-Supervised Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1073--1089},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3474732},
	doi = {10.1109/TMC.2024.3474732},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoWQQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, the effectiveness of these methods is challenged by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method considers both the local class-aware distribution of each client's data and the global class-aware distribution of all clients’ data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objective for different clients to tackle large deviations and incorporates contrastive information in the feature space to mitigate confirmation bias. Additionally, DCCFSSL introduces an authentication-reweighted aggregation technique to improve the server's aggregation robustness. Our comprehensive experiments show that DCCFSSL outperforms current state-of-the-art methods on three benchmark datasets and surpasses the FedAvg with relabeled unlabeled clients on CIFAR-10, CIFAR-100, and STL-10 datasets.}
}


@article{DBLP:journals/tmc/TangMYL25,
	author = {Boyi Tang and
                  Yijun Mo and
                  Chen Yu and
                  Huiyu Liu},
	title = {End-to-End Steady-State Adaptive Slicing Method for Dynamic Network
                  State and Load},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1090--1104},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3473908},
	doi = {10.1109/TMC.2024.3473908},
	timestamp = {Tue, 25 Feb 2025 12:59:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangMYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing has become a primary function of 5G/6G network resource management. However, the existing slicing schemes have not sufficiently discussed the reconfiguration optimization schemes brought by user behavior changes and mobile network environment fluctuations, leading to excessive service interruption rates and slice reconfiguration costs in dynamic environments. To address this problem, this paper proposes an End-to-end Steady-state Adaptive slicing method for Dynamic network state and load (ESAD). To realize the steady-state slicing decisions, ESAD takes the steady-state degree of network slicing and reconfiguration cost as the objective and constructs the slicing reconfiguration probability evaluation function based on the service load dynamics function and the time-varying function of the network channel conditions. To improve the predictability and steady-state degree of the slicing decision, ESAD introduces an ensemble deep learning method to predict the load service fluctuation based on the user behavior model and employs reinforcement learning to compute the channel dynamics boundary, which guides the slicing decision to balance the network dynamics factors. Experiments on quality of service assurance for 5G cloud game rendering class prove that ESAD can reduce reconfiguration probability and long-term reconfiguration cost by 49.45%–58.50% while improving system QoS assurance and capacity.}
}


@article{DBLP:journals/tmc/MhapsekarADD25,
	author = {Rahul Umesh Mhapsekar and
                  Lizy Abraham and
                  Steven Davy and
                  Indrakshi Dey},
	title = {Application Adaptive Light-Weight Deep Learning (AppAdapt-LWDL) Framework
                  for Enabling Edge Intelligence in Dairy Processing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1105--1119},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3475634},
	doi = {10.1109/TMC.2024.3475634},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MhapsekarADD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dairy industry is experiencing a surge in data from Edge devices, using spectroscopic techniques for milk quality assessment. Milk spectral data can help understand the species of milk producer and detect inter-species adulteration. Transmitting raw milk spectral data to the cloud for processing faces challenges due to limited network resources such as bandwidth, computational memory, and energy availability. Edge processing offers a solution by training data closer to the source, enhancing efficiency and real-time analysis by providing reduced latency, improved accuracy, resource-aware computation, and real-time customization. However, traditional Deep Learning (DL) methods such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) struggle on resource-constrained Edge devices due to complexity. To address this, we propose an Edge-Centric Application-Adaptive Light-Weight DL approach (AppAdapt-LWDL) for milk species identification and adulteration detection. Our method optimizes DL models via double model optimization, involving low-magnitude pruning and post-training quantization. Our novel application-adaptive algorithm balances speed and accuracy by determining the pruning ratio automatically for the specific application. The chosen model is then quantized for smaller databases, ideal for embedded devices. The AppAdapt-LWDL framework significantly accelerates training, speeds up inferencing, enhances energy efficiency, and maintains accuracy based on application needs.}
}


@article{DBLP:journals/tmc/ZhuZXWHLWS25,
	author = {Jialiang Zhu and
                  Hao Zheng and
                  Wenchao Xu and
                  Haozhao Wang and
                  Zhiming He and
                  Yuxuan Liu and
                  Shuang Wang and
                  Qi Sun},
	title = {Harmonizing Global and Local Class Imbalance for Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1120--1131},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3476340},
	doi = {10.1109/TMC.2024.3476340},
	timestamp = {Fri, 12 Sep 2025 08:19:09 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuZXWHLWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is to collaboratively train a global model among distributed clients by iteratively aggregating their local updates without sharing their raw data, whereby the global modal can approximately converge to the centralized training way over a global dataset that composed of all local datasets (i.e., union of all users’ local data). However, in real-world scenarios, the distributions of the data classes are often imbalanced not only locally, but also in the global dataset, which severely deteriorate the FL performance due to the conflicting knowledge aggregation. Existing solutions for FL class imbalance either focus on the local data to regulate the training process or purely aim at the global datasets, which often fail to alleviate the class imbalance problem if there is mismatch between the local and global imbalance. Considering these limitations, this paper proposes a Global-Local Joint Learning method, namely GLJL, which simultaneously harmonizes the global and local class imbalance issue by jointly embedding the local and the global factors into each client’s loss function. Through extensive experiments over popular datasets with various class imbalance settings, we show that the proposed method can significantly improve the model accuracy over minority classes without sacrificing the accuracy of other classes.}
}


@article{DBLP:journals/tmc/XuZLLTS25,
	author = {Yang Xu and
                  Shanshan Zhang and
                  Chen Lyu and
                  Jia Liu and
                  Tarik Taleb and
                  Norio Shiratori},
	title = {{TRIMP:} Three-Sided Stable Matching for Distributed Vehicle Sharing
                  System Using Stackelberg Game},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1132--1148},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3475481},
	doi = {10.1109/TMC.2024.3475481},
	timestamp = {Thu, 24 Apr 2025 10:26:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuZLLTS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Vehicle Sharing System (DVSS) leverages emerging technologies such as blockchain to create a secure, transparent, and efficient platform for sharing vehicles. In such a system, both efficient matching of users with available vehicles and optimal pricing mechanisms play crucial roles in maximizing system revenue. However, most existing schemes utilize user-to-vehicle (two-sided) matching and pricing, which are unrealistic for DVSS due to the lack of participation of service providers. To address this issue, we propose in this paper a novel Three-sided stable Matching with an optimal Pricing (TRIMP) scheme. First, to achieve maximum utilities for all three parties simultaneously, we formulate the optimal policy and pricing problem as a three-stage Stackelberg game and derive its equilibrium points accordingly. Second, relying on these solutions from the Stackelberg game, we construct a three-sided cyclic matching for DVSS. Third, as the existence of such a matching is NP-complete, we design a specific vehicle sharing algorithm to realize stable matching. Extensive experiments demonstrate the effectiveness of our TRIMP scheme, which optimizes the matching process and ensures efficient resource allocation, leading to a more stable and well-functioning decentralized vehicle sharing ecosystem.}
}


@article{DBLP:journals/tmc/ZadnikKTMJ25,
	author = {Jakub Z{\'{a}}dn{\'{\i}}k and
                  Michel Kieffer and
                  Anthony Trioux and
                  Markku J. M{\"{a}}kitalo and
                  Pekka J{\"{a}}{\"{a}}skel{\"{a}}inen},
	title = {CV-Cast: Computer Vision-Oriented Linear Coding and Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1149--1162},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3478048},
	doi = {10.1109/TMC.2024.3478048},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZadnikKTMJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote inference allows lightweight edge devices, such as autonomous drones, to perform vision tasks exceeding their computational, energy, or processing delay budget. In such applications, reliable transmission of information is challenging due to high variations of channel quality. Traditional approaches involving spatio-temporal transforms, quantization, and entropy coding followed by digital transmission may be affected by a sudden decrease in quality (the digital cliff) when the channel quality is less than expected during design. This problem can be addressed by using Linear Coding and Transmission (LCT), a joint source and channel coding scheme relying on linear operators only, allowing to achieve reconstructed per-pixel error commensurate with the wireless channel quality. In this paper, we propose CV-Cast: the first LCT scheme optimized for computer vision task accuracy instead of per-pixel distortion. Using this approach, for instance at 10 dB channel signal-to-noise ratio, CV-Cast requires transmitting 28% less symbols than a baseline LCT scheme in semantic segmentation and 15% in object detection tasks. Simulations involving a realistic 5G channel model confirm the smooth decrease in accuracy achieved with CV-Cast, while images encoded by JPEG or learned image coding (LIC) and transmitted using classical schemes at low Eb/N0 are subject to digital cliff.}
}


@article{DBLP:journals/tmc/WangWQXGF25,
	author = {Jie Wang and
                  Jingmiao Wu and
                  Yingwei Qu and
                  Qi Xiao and
                  Qinghua Gao and
                  Yuguang Fang},
	title = {Multi-Target Device-Free Positioning Based on Spatial-Temporal mmWave
                  Point Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1163--1180},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3474671},
	doi = {10.1109/TMC.2024.3474671},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangWQXGF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-free positioning (DFP) using mmWave signals is an emerging technique that could track a target without attaching any devices. It conducts position estimation by analyzing the influence of targets on their surrounding mmWave signals. With the widespread utilization of mmWave signals, DFP will have many potential applications in tracking pedestrians and robots in intelligent monitoring systems. State-of-the-art DFP work has already achieved excellent positioning performance when there is one target only, but when there are multiple targets, the time-varying target state, such as entering or leaving of the wireless coverage area and close interactions, makes it challenging to track every target. To solve these problems, in this paper, we propose a spatial-temporal analysis method to robustly track multiple targets based on the high precision mmWave point cloud information. Specifically, we propose a high precision spatial imaging strategy to construct fine-grained mmWave point cloud of the targets, design a spatial-temporal point cloud clustering method to determine the target state, and then leverage a gait based identity and trajectory association scheme and a particle filter to achieve robust identity-aware tracking. Extensive evaluations on a 77 GHz mmWave testbed have been conducted to demonstrate the effectiveness and robustness of our proposed schemes.}
}


@article{DBLP:journals/tmc/CaoCZCHLLL25,
	author = {Zhiqiang Cao and
                  Yun Cheng and
                  Zimu Zhou and
                  Yongrui Chen and
                  Youbing Hu and
                  Anqi Lu and
                  Jie Liu and
                  Zhijun Li},
	title = {Edge-Cloud Collaborated Object Detection via Bandwidth Adaptive Difficult-Case
                  Discriminator},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1181--1196},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3474743},
	doi = {10.1109/TMC.2024.3474743},
	timestamp = {Thu, 13 Nov 2025 10:48:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoCZCHLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object detection, a fundamental task in computer vision, is crucial for various intelligent edge computing applications. However, object detection algorithms are usually heavy in computation, hindering their deployments on resource-constrained edge devices. Traditional edge-cloud collaboration schemes, like deep neural network (DNN) partitioning across edge and cloud, are unfit for object detection due to the significant communication costs incurred by the large size of intermediate results. To this end, we propose a Difficult-Case based Small-Big model (DCSB) framework. It employs a difficult-case discriminator on the edge device to control data transfer between the small model on the edge and the large model in the cloud. We also adopt regional sampling to further reduce the bandwidth consumption and create a discriminator zoo to accommodate the varying networking conditions. Additionally, we extend DCSB to video tasks by developing an adaptive sampling rate update algorithm, aiming to minimize computational demands without sacrificing detection accuracy. Extensive experiments show that DCSB can detect 97.26%-97.96% objects while saving 74.37%-82.23% network bandwidth, compared to cloud-only methods. Furthermore, DCSB significantly outperforms the latest DNN partitioning methods, reducing inference time by 92.60%-95.10% given an 8Mbps transmission bandwidth. In video tasks, DCSB matches the detection accuracy of leading video analysis methods while cutting the computational overhead by 40%.}
}


@article{DBLP:journals/tmc/GuoSLPCH25,
	author = {Jiani Guo and
                  Shanshan Song and
                  Jun Liu and
                  Miao Pan and
                  Jun{-}Hong Cui and
                  Guangjie Han},
	title = {{AS-MAC:} An Adaptive Scheduling {MAC} Protocol for Reducing the End-to-End
                  Delay in AUV-Assisted Underwater Acoustic Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1197--1211},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3475428},
	doi = {10.1109/TMC.2024.3475428},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoSLPCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous Underwater Vehicle (AUV)-assisted Underwater Acoustic Networks (UANs) are promising for complex ocean applications. In essence, an AUV-assisted UAN is still dominated by fixed nodes, and Time Division Multiple Access (TDMA)-based Medium Access Control (MAC) protocols have undisputed practicability in such fixed nodes-dominated UANs since they are simple and easy to deploy. However, AUV-assisted UANs may exist dynamic bidirectional data streams, while most existing protocols assume UANs have a unidirectional data stream, and their fixed scheduling sequence results in the long end-to-end delay in AUV-assisted UANs. In this paper, we first reveal a phenomenon between the data stream and the scheduling sequence, derived from real-world experiments: their consistent direction decreases the packet waiting delay but increases the slot length, and vice versa. To optimize the end-to-end delay, UANs with dynamic bidirectional data streams expect the MAC protocol to provide a flexible scheduling sequence. To this end, we propose a low-delay Adaptive Scheduling MAC protocol (AS-MAC) based on TDMA for AUV-assisted UANs. In AS-MAC, we analyze the relationship between scheduling sequence and data stream, extracting two significant factors: slot length and packet delay. Afterwards, we design Slot Length Model (SLM) and Packet Delay Model (PDM) to analyze the end-to-end delay of different data streams. Based on these two models, we present a Scheduling Sequence and Slot Length allocation Algorithm (SSSLA) to adaptively provide the minimum end-to-end delay for current bidirectional data streams. Extensive simulation results show that AS-MAC efficiently addresses severe queue congestion of the state-of-the-art protocols and reduces the end-to-end delay of different dynamic streams in various scenarios.}
}


@article{DBLP:journals/tmc/YingXWYXZJJLZT25,
	author = {Chenhao Ying and
                  Fuyuan Xia and
                  David S. L. Wei and
                  Xinchun Yu and
                  Yibin Xu and
                  Weiting Zhang and
                  Xikun Jiang and
                  Haiming Jin and
                  Yuan Luo and
                  Tao Zhang and
                  Dacheng Tao},
	title = {{BIT-FL:} Blockchain-Enabled Incentivized and Secure Federated Learning
                  Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1212--1229},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3477616},
	doi = {10.1109/TMC.2024.3477616},
	timestamp = {Mon, 03 Mar 2025 22:25:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YingXWYXZJJLZT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Harnessing the benefits of blockchain, such as decentralization, immutability, and transparency, to bolster the credibility and security attributes of federated learning (FL) has garnered increasing attention. However, blockchain-enabled FL (BFL) still faces several challenges. The primary and most significant issue arises from its essential but slow validation procedure, which selects high-quality local models by recruiting distributed validators. The second issue stems from its incentive mechanism under the transparent nature of blockchain, increasing the risk of privacy breaches regarding workers’ cost information. The final challenge involves data eavesdropping from shared local models. To address these significant obstacles, this paper proposes a Blockchain-enabled Incentivized and Secure Federated Learning (BIT-FL) framework. BIT-FL leverages a novel loop-based sharded consensus algorithm to accelerate the validation procedure, ensuring the same security as non-sharded consensus protocols. It consistently outputs the correct local model selection when the fraction of adversaries among validators is less than  1 / 2 1/2  with synchronous communication. Furthermore, BIT-FL integrates a randomized incentive procedure, attracting more participants while guaranteeing the privacy of their cost information through meticulous worker selection probability design. Finally, by adding artificial Gaussian noise to local models, it ensures the privacy of trainers’ local models. With the careful design of Gaussian noise, the excess empirical risk of BIT-FL is upper-bounded by  O ( ln n min n 3 / 2 min + ln n n ) \\mathcal {O}(\\frac{\\ln n_{\\min}}{ n_{\\min}^{3/2}}+\\frac{\\ln n}{n}) , where  n n  represents the size of the union dataset, and  n min n_{{\\min}}  represents the size of the smallest dataset. Our extensive experiments demonstrate that BIT-FL exhibits efficiency, robustness, and high accuracy for both classification and regression tasks.}
}


@article{DBLP:journals/tmc/HuangW25,
	author = {Long Huang and
                  Chen Wang},
	title = {Biometric Encoding for Replay-Resistant Smartphone User Authentication
                  Using Handgrips},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {2},
	pages = {1230--1248},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3474673},
	doi = {10.1109/TMC.2024.3474673},
	timestamp = {Fri, 14 Feb 2025 20:50:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Biometrics have been widely applied for user authentication. However, existing biometric authentications are vulnerable to biometric spoofing, because they can be observed and forged. In addition, they rely on verifying biometric features that rarely change. To address this issue, we propose to verify the handgrip biometric that can be unobtrusively extracted by acoustic signals when the user holds the phone. This biometric is uniquely associated with the user’s hand geometry, body-fat ratio, and gripping strength, which are hard to reproduce. Furthermore, we propose two biometric encoding techniques (i.e., temporal-frequential and spatial) to convert static biometrics into dynamic biometric features to prevent data reuse. In particular, we develop a biometric authentication system to work with the challenge-response protocol. We encode the ultrasonic signal according to a random challenge sequence and extract a distinct biometric code as the response. We further develop two decoding algorithms to decode the biometric code for user authentication. Additionally, we investigate multiple new attacks and explore using a latent diffusion model to solve the acoustic noise discrepancies between the training and testing data to improve system performance. Extensive experiments show our system achieves 97% accuracy in distinguishing users and rejects 100% replay attacks with  0.6 s  0.6 \\, s  challenge sequence.}
}


@article{DBLP:journals/tmc/ChengLCY25,
	author = {Yuxia Cheng and
                  Chengchao Liang and
                  Qianbin Chen and
                  F. Richard Yu},
	title = {An Efficient Resource Allocation Scheme With Uncertain Network Status
                  in Edge Computing-Enabled Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1249--1263},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3412810},
	doi = {10.1109/TMC.2024.3412810},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChengLCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative resource allocation is crucial for reducing overhead and enhancing resource utilization in edge computing-enabled networks. To ensure a satisfactory user experience, we recognize the importance of considering information uncertainty in resource allocation. Therefore, we explore information uncertainty in edge computing-enabled networks, especially within the complex environment of resource coupling. However, existing methods lack a comprehensive and robust solution for coordinating wireless, transport, and computing resource under this information uncertainty. This paper addresses this gap by proposing a joint optimization of access point (AP) selection, computing node association, and traffic engineering, aiming to maximize network utility under the uncertain conditions of wireless status and application QoS requirements. The constraints under these uncertainties are modeled as chance constraints, complicating the problem's solvability. We adopt the Bernstein approximation to establish convex conservative approximations of the chance constraints. Given the problem's substantial size and computational complexity, the alternating direction method of multipliers is employed to solve the approximated problem in a distributed manner. We further derive the closed solutions of the corresponding sub-problems. Extensive simulations validate the superiority of our proposed scheme, demonstrating its ability to achieve a good trade-off between meeting user requirements and optimizing resource utilization.}
}


@article{DBLP:journals/tmc/KimCW25,
	author = {Jeongyun Kim and
                  Andrea Conti and
                  Moe Z. Win},
	title = {Travel Demand Modeling and Estimation for High-Dimensional Mobility},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1264--1277},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3435436},
	doi = {10.1109/TMC.2024.3435436},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KimCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The massive amount of data related to spatiotemporal mobility offers new opportunities to understand human mobility with applications in various sectors, including transportation, logistics, and safety. However, the increase in the volume and in the dimension of mobility data makes it challenging to retrieve important information and critical features of spatiotemporal mobility. This paper develops a method to estimate probabilistic occurrences of travel demands considering interactions between origin, destination, and departure time. First, we reveal the important features in the complex structure of mobility data and identify mobility patterns. Then, we derive a data-driven model, accounting for mobility patterns, to estimate and predict travel demands. We quantify the accuracy of the proposed method for a case study using both New York city yellow taxi trip data and for-hire vehicles trip data over the entire city. Results show the accuracy of the proposed method compared to existing approaches.}
}


@article{DBLP:journals/tmc/ZhangV25,
	author = {Meng Zhang and
                  Deepanshu Vasal},
	title = {Large-Scale Mechanism Design for Networks: Superimposability and Dynamic
                  Implementation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1278--1292},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3499958},
	doi = {10.1109/TMC.2024.3499958},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network utility maximization (NUM) is a fundamental framework for optimizing next-generation networks. However, self-interested agents with private information pose challenges due to potential system manipulation. To address these challenges, the literature on economic mechanism design has emerged. Existing mechanisms are not suited for large-scale networks due to their complexity, high implementation costs, and difficulty to adapt to dynamic settings. This paper proposes a large-scale mechanism design framework that mitigates these limitations. As the number of agents  I I  approaches infinity, their incentive to misreport decreases rapidly at a rate of  O ( 1 / I 2 ) \\mathcal {O}(1/I^{2}) . We introduce a superimposable framework applicable to any NUM algorithm without modifications, reducing implementation costs. In the dynamic setting, the large-scale mechanism design framework introduces the decomposability of the problem, enabling agents to align their own interests with the objectives of the dynamic NUM problem. This alignment helps overcome the additional, more stringent incentive constraints encountered in dynamic settings. Extending our results to dynamic settings, we present the design of a Dynamic Large-Scale mechanism with desirable properties and the corresponding Dynamic Superimposable Large-Scale mechanism. Our numerical experiments validate the fact that our proposed schemes are approximately  I I  times faster than the seminal VCG mechanism.}
}


@article{DBLP:journals/tmc/LinCFCWG25,
	author = {Zheng Lin and
                  Zhe Chen and
                  Zihan Fang and
                  Xianhao Chen and
                  Xiong Wang and
                  Yue Gao},
	title = {FedSN: {A} Federated Learning Framework Over Heterogeneous {LEO} Satellite
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1293--1307},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3481275},
	doi = {10.1109/TMC.2024.3481275},
	timestamp = {Sat, 27 Sep 2025 08:12:42 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinCFCWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communications but also for various machine learning applications. However, a ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, enabling FL on LEO satellites still face three critical challenges: i) heterogeneous computing and memory capabilities, ii) limited downlink/uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges. Specifically, we first present a novel sub-structure scheme to enable heterogeneous local model training considering different computing, memory, and communication constraints on LEO satellites. Additionally, we propose a pseudo-synchronous model aggregation strategy to dynamically schedule model aggregation for compensating model staleness. Extensive experiments with real-world satellite data demonstrate that FedSN framework achieves higher accuracy, lower computing, and communication overhead than the state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/LiHZCC25,
	author = {Jingyi Li and
                  Guangjing Huang and
                  Liekang Zeng and
                  Lin Chen and
                  Xu Chen},
	title = {Sequential Privacy Budget Recycling for Federated Vector Mean Estimation:
                  {A} Game-Theoretic Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1308--1321},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3484010},
	doi = {10.1109/TMC.2024.3484010},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiHZCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving vector mean estimation is a crucial primitive in federated analytics. Existing practices usually resort to Local Differentiated Privacy (LDP) mechanisms that inject random noise into users’ vectors when communicating with users and the central server. Due to the privacy-utility trade-off, the privacy budget has been widely recognized as the bottleneck resource that requires well-provisioning. In this paper, we explore the possibility of privacy budget recycling and propose a novel ChainDP framework enabling users to carry out data aggregation sequentially to recycle the privacy budget. We establish a sequential game to model the user interactions in our framework. We theoretically show the mathematical nature of the sequential game, solve its Nash Equilibrium, and design an incentive mechanism with provable economic properties. To alleviate potential privacy collusion attacks, we further derive a differentially privacy-guaranteed protocol to avoid holistic exposure. Our numerical simulation validates the effectiveness of ChainDP, showing that it can significantly save privacy budget as well as lower estimation error compared to the traditional LDP mechanism.}
}


@article{DBLP:journals/tmc/YangGZYNZL25,
	author = {Meiyi Yang and
                  Deyun Gao and
                  Weiting Zhang and
                  Dong Yang and
                  Dusit Niyato and
                  Hongke Zhang and
                  Victor C. M. Leung},
	title = {Deep Reinforcement Learning-Based Joint Caching and Routing in AI-Driven
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1322--1337},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3481276},
	doi = {10.1109/TMC.2024.3481276},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangGZYNZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To reduce redundant traffic transmission in both wired and wireless networks, optimal content placement problem naturally occurring in many applications is studied. In this paper, considering the limited cache capacity, unknown popularity distribution and non-stationary user demands, we address this problem by jointly optimizing content caching and routing with the objective of minimizing transmission cost. By optimizing the routing with the route-to-least cost-cache policy, the content caching process is modeled as a Markov decision process (MDP), aiming to maximize caching reward. However, the optimization problem consists of multiple nodes selecting caching contents, which leads to the combinatorial increase of the number of action dimensions with the number of possible actions. To handle this curse of dimensionality, we propose an intelligent caching algorithm by embedding action branching architecture into a dueling double deep Q-network (D3QN) to optimize caching decisions, and thus the agent at the controller can adaptively learn and track the underlying dynamics. Considering the independence of each branch, a marginal gain-based replacement rule is proposed to satisfy cache capacity constraint. Our simulation results show that compared with the prior art, the caching reward and hit rate of the proposed algorithm are increased by 35.3% and 33.6% respectively on average.}
}


@article{DBLP:journals/tmc/HuiSZTWZ25,
	author = {Ning Hui and
                  Qian Sun and
                  Jie Zeng and
                  Lin Tian and
                  Yuanyuan Wang and
                  Yiqing Zhou},
	title = {Mixed Numerology-Based Intelligent Resource Management in a Sliced
                  6G Space-Terrestrial Integrated Radio Access Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1338--1356},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494842},
	doi = {10.1109/TMC.2024.3494842},
	timestamp = {Mon, 26 Jan 2026 12:05:08 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuiSZTWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although resource sharing and mixed numerology among slices are promising for improving wireless resource utilization, these techniques can compromise isolation performance and cause serious inter numerology interference (INI). Therefore, this paper studies wireless resource management in a mixed numerology-based sliced 6G space–terrestrial integrated radio access network (STI-RAN) with the aim of reducing INI and guaranteeing isolation performance while decreasing interference from Doppler frequency shifts caused by the high-speed movement of low-orbit satellites. First, an isolation performance indicator is defined to evaluate different isolation performances, and a universal spectral distance model is formulated to rewrite the INI power model. Next, the dynamic wireless resource management problem is formulated in a discrete form, yielding a scheme called Flex- μ \\mu , which is designed to reduce the INI and Doppler frequency shifts, guarantee isolation performance, and enhance the SINR. Finally, an intelligent multi-characteristic matrix coding-based social group optimization (MultiMatrix-SGO) algorithm is designed to solve the proposed NP-hard discrete optimization problem. Compared with existing schemes, the system utility is efficiently increased by up to 58.32%, the SINR can converge to 38.44\u2009dB, and the isolation performance is guaranteed while the INI and Doppler frequency shifts are reduced.}
}


@article{DBLP:journals/tmc/WangSWLYYW25,
	author = {En Wang and
                  Zixuan Song and
                  Mengni Wu and
                  Wenbin Liu and
                  Bo Yang and
                  Yongjian Yang and
                  Jie Wu},
	title = {A New Data Completion Perspective on Sparse CrowdSensing: Spatiotemporal
                  Evolutionary Inference Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1357--1371},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480983},
	doi = {10.1109/TMC.2024.3480983},
	timestamp = {Wed, 02 Apr 2025 17:03:42 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSWLYYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile CrowdSensing (MCS) has emerged as a popular paradigm to engage mobile users in collaborative sensing tasks. However, its performance is hindered by its limited spatiotemporal range and the cost of data collection. An effective strategy is to integrate Sparse MCS with data completion, allowing for unsensed data inference. However, when confronted with situations where sensed data is excessively sparse, data inference results may be unsatisfactory due to several challenges including: 1) uneven data distribution, 2) complex spatiotemporal correlation, and 3) the presence of inference noise. To address these challenges, we propose a model named Spatiotemporal Evolutionary Inference (STEI) that achieves accurate inference of unsensed data in Sparse MCS. Specifically, we complete the unsensed data by uncovering strong local correlations in the data and gradually evolving those correlations to the global situation. In each evolution step, we thoroughly consider the impact of spatiotemporal consistency and difference. To minimize the interference of noise during the evolution process, we design an adaptive coefficient to enhance the dependence on sensed data. Finally, to validate the effectiveness of STEI, we conduct extensive qualitative and quantitative experiments using three popular datasets. The experimental results demonstrate that our approach excels in accurately inferring data, particularly in situations where the distribution of data is notably uneven.}
}


@article{DBLP:journals/tmc/XieCHHG25,
	author = {Bo Xie and
                  Haixia Cui and
                  Ivan Wang{-}Hei Ho and
                  Yejun He and
                  Mohsen Guizani},
	title = {Computation Offloading and Resource Allocation in {LEO} Satellite-Terrestrial
                  Integrated Networks With System State Delay},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1372--1385},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3479243},
	doi = {10.1109/TMC.2024.3479243},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XieCHHG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computing offloading optimization for energy saving is becoming increasingly important in low-Earth orbit (LEO) satellite-terrestrial integrated networks (STINs) since battery techniques have not kept up with the demand of ground terminal devices. In this paper, we design a delay-based deep reinforcement learning (DRL) framework specifically for computation offloading decisions, which can effectively reduce the energy consumption. Additionally, we develop a multi-level feedback queue for computing allocation (RAMLFQ), which can effectively enhance the CPU’s efficiency in task scheduling. We initially formulate the computation offloading problem with the system delay as Delay Markov Decision Processes (DMDPs), and then transform them into the equivalent standard Markov Decision Processes (MDPs). To solve the optimization problem effectively, we employ a double deep Q-network (DDQN) method, enhancing it with an augmented state space to better handle the unique challenges posed by system delays. Simulation results demonstrate that the proposed learning-based computing offloading algorithm achieves high levels of performance efficiency and attains a lower total cost compared to other existing offloading methods.}
}


@article{DBLP:journals/tmc/ShenSPLE25,
	author = {Pengfei Shen and
                  Yulin Shao and
                  Haoyuan Pan and
                  Lu Lu and
                  Yonina C. Eldar},
	title = {Channel Cycle Time: {A} New Measure of Short-Term Fairness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1386--1401},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3484177},
	doi = {10.1109/TMC.2024.3484177},
	timestamp = {Mon, 03 Mar 2025 22:25:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShenSPLE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper puts forth a new metric, dubbed channel cycle time (CCT), to measure the short-term fairness of communication networks. CCT characterizes the average duration between two consecutive successful transmissions of a user, during which all other users successfully accessed the channel at least once. In contrast to existing short-term fairness measures, CCT provides more comprehensive insight into the transient dynamics of communication networks, with a particular focus on users’ delays and jitter. To validate the efficacy of our approach, we analytically characterize the CCTs for two classical communication protocols: slotted Aloha and CSMA/CA. The analysis demonstrates that CSMA/CA exhibits superior short-term fairness over slotted Aloha. Beyond its role as a measurement metric, CCT has broader implications as a guiding principle for the design of future communication networks by emphasizing factors like fairness, delay, and jitter in short-term behaviors.}
}


@article{DBLP:journals/tmc/CaoYYZXJN25,
	author = {Jiangling Cao and
                  Liang Yang and
                  Dingcheng Yang and
                  Tiankui Zhang and
                  Lin Xiao and
                  Hongbo Jiang and
                  Dusit Niyato},
	title = {Trajectory Optimization and Pick-Up and Delivery Sequence Design for
                  Cellular-Connected Cargo AAVs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1402--1416},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480910},
	doi = {10.1109/TMC.2024.3480910},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoYYZXJN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider a cargo autonomous aerial vehicle (AAV)-aided multi-parcel pick-up and delivery network, where the communication ability of the AAV is provided by the ground base stations (GBSs). For such a system setup, our goal is to optimize the trajectory of the cargo AAV while minimizing the combined impact of total energy consumption and total outage time. Simultaneously, we aim to maximize overall user satisfaction throughout the entire flight duration. More specifically, we propose a pick-up and delivery of AAV (PDU) framework to address this problem and this framework consists of two parts. First, a simulated annealing (SA) algorithm is used to obtain the pick-up and delivery (P&D) order of parcels. On the basis of obtaining the P&D order through SA, we further use deep reinforcement learning (DRL) to optimize the flight trajectory of the AAV to ensure the expected communication quality between the AAV and GBSs. To verify the effectiveness of our proposed algorithms, we design three baseline strategies for comparison, and also investigate the effect of using the PDU framework with different weights. Finally, numerical results show that the performance of PDU strategy is improved by about 5%-30% compared with other strategies in solving the performance tradeoff of AAV energy consumption, communication quality, and user satisfaction.}
}


@article{DBLP:journals/tmc/ChenWHWW25,
	author = {Kaixin Chen and
                  Lei Wang and
                  Yongzhi Huang and
                  Kaishun Wu and
                  Lu Wang},
	title = {Optical Sensing-Based Intelligent Toothbrushing Monitoring System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1417--1436},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3479455},
	doi = {10.1109/TMC.2024.3479455},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWHWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incorrect brushing methods normally lead to poor oral hygiene, and result in severe oral diseases and complications. While effective brushing can address this issue, individuals often struggle with incorrect brushing, like aggressive brushing, insufficient brushing, and missing brushing. To break this stalemate, in this paper, we proposed LiT, a toothbrushing monitoring system to assess the brushing status on 16 surfaces using the Bass technique. LiT utilizes commercial LED toothbrushes’ blue LEDs as transmitters, and incorporates only two low-cost photodetectors as receivers on the toothbrush head. It is challenging to determine optimal deployment positions and minimize photodetectors number to establish the light transmission channel in oral cavity. To address these challenges, we established mathematical models within the oral cavity based on the two photodetectors’ deployment to theoretically validate the feasibility and prove robustness. Furthermore, we designed a comprehensive framework to fight against the implementation challenges including brushing action separation, light interference on the outer surfaces of front teeth, toothpaste diversity, user variations, brushing hand variability, and incorrect brushings. Experimental results demonstrate that LiT achieves a highly accurate surface recognition rate of 95.3%, an estimated error for brushing duration of 6.1%, and incorrect brushing detection accuracy of 96.9%. Furthermore, LiT retains stable capability under a variety of circumstances, such as various lighting conditions, user movement, toothpaste diversity, and left and right-handed users.}
}


@article{DBLP:journals/tmc/LiuJZW25,
	author = {Yihao Liu and
                  Jinyan Jiang and
                  Jumin Zhao and
                  Jiliang Wang},
	title = {Enable Practical Long-Range Multi-Target Backscatter Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1437--1452},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480137},
	doi = {10.1109/TMC.2024.3480137},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter sensing has emerged as a significant technology within the Internet of Things (IoT), prompting extensive research interest. This paper presents LoMu, the first long-range multi-target backscatter sensing system designed for low-cost tags operating under ambient LoRa. LoMuintroduces an orthogonal sensing model that processes backscatter signals from multiple tags to extract motion information. The design addresses several practical challenges, including near-far interference among multiple tags, phase offsets from unsynchronized transceivers, and phase errors due to frequency drift in low-cost tags. To overcome these issues, we propose a conjugate-based energy concentration method to extract high-quality signals and a Hamming-window-based method to mitigate the near-far problem. Additionally, we exploit the relationship between excitation and backscatter signals to synchronize the transmitter (TX) and receiver (RX) and combine double sidebands of backscatter signals to eliminate tag frequency drift. Furthermore, a novel joint estimation algorithm is introduced to exploit both amplitude and phase information in target signals, enhancing frequency sensing results and robustness. Our implementation and extensive experiments demonstrate that LoMucan accurately sense up to 35 tags simultaneously and achieve an average frequency sensing error of 0.5% at a range of 400 meters, which is  4 × 4\\times  the range of the state-of-the-art.}
}


@article{DBLP:journals/tmc/YanYSFY25,
	author = {Dawei Yan and
                  Panlong Yang and
                  Fei Shang and
                  Nikolaos M. Freris and
                  Yubo Yan},
	title = {Non-Intrusive and Efficient Estimation of Antenna 3-D Orientation
                  for WiFi APs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1453--1468},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3485228},
	doi = {10.1109/TMC.2024.3485228},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YanYSFY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The effectiveness of WiFi-based localization systems heavily relies on the spatial accuracy of WiFi AP. In real-world scenarios, factors such as AP rotation and irregular antenna tilt contribute significantly to inaccuracies, surpassing the impact of imprecise AP location and antenna separation. In this paper, we propose Anteumbler, a non-invasive, accurate, and efficient system for measuring the orientation of each antenna in physical space. By leveraging the fact that maximum received power occurs when a Tx-Rx antenna pair is perfectly aligned, we build a spatial angle model capable of determining antennas’ orientations without prior knowledge. However, achieving comprehensive coverage across the spatial angle necessitates extensive sampling points. To enhance efficiency, we exploit the orthogonality of antenna directivity and polarization, and adopt an iterative algorithm, thereby reducing the number of sampling points by several orders of magnitude. Additionally, to attain the required antenna orientation accuracy, we mitigate the influence of propagation distance using a dual plane intersection model while filtering out ambient noise. Our real-world experiments, covering six antenna types, two antenna layouts, two antenna separations ( λ / 2 \\lambda /2  and  λ \\lambda  ), and three AP heights, demonstrate that Anteumbler achieves median errors below  6 ∘ \\text{6}^\\circ  for both elevation and azimuth angles, and exhibits robustness in NLoS and dynamic environments. Moreover, when integrated into the reverse localization system, Anteumbler deployed over LocAP reduces antenna separation error by  10 m m 10 \\,\\mathrm{mm} , while for user localization system, its integration over SpotFi reduces user localization error by more than  1 m 1 \\,\\mathrm{m} .}
}


@article{DBLP:journals/tmc/HuangWWMM25,
	author = {Haojun Huang and
                  Qifan Wang and
                  Weimin Wu and
                  Wang Miao and
                  Geyong Min},
	title = {Accurate Prediction of Multi-Dimensional Required Resources in 5G
                  via Federated Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1469--1481},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480136},
	doi = {10.1109/TMC.2024.3480136},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWWMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The accurate prediction of required resources in terms of storage, computing and bandwidth is essential for 5G to host diverse services. The existing efforts illustrate that it is more promising to efficiently predict the unknown required resources with a third-order tensor compared to the 2D-matrix-based solutions. However, most of them fail to leverage the inherent features hidden in network traffic like temporal stability and service correlation to build a third-order tensor for the multi-dimensional required resource prediction in an intelligent manner, incurring coarse-grained prediction accuracy. Furthermore, it is difficult to build a third-order tensor with rate-varied measurements in 5G due to different lengths of measurement time slots. To address these issues, we propose an Accurate Prediction of Multi-Dimensional Required Resources (APMR) approach in 5G via Federated Deep Reinforcement Learning (FDRL). We first confirm the resource requests originated from different Base Stations (BSs) at varied measurement rates have similar features in service and time domains, but cannot directly form a series of regular tensors. Built on these observations, we reshape these measurement data to form a series of standard third-order tensors with the same size, which include many elements obtained from measurements and some unknown elements needed to be inferred. In order to obtain accurately predicted results, the FDRL-based tensor factorization approach is introduced to intelligently utilize multiple specific iteration rules for local model learning, and the accuracy-aware and latency-based depreciation strategies are exploited to aggregate local models for resource prediction. Extensive simulation experiments demonstrate that APMR can accurately predict the multi-dimensional required resources compared to the state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/ZhaoSLLCW25,
	author = {Guangrong Zhao and
                  Yiran Shen and
                  Feng Li and
                  Lei Liu and
                  Lizhen Cui and
                  Hongkai Wen},
	title = {Ui-Ear: On-Face Gesture Recognition Through On-Ear Vibration Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1482--1495},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480216},
	doi = {10.1109/TMC.2024.3480216},
	timestamp = {Tue, 27 Jan 2026 19:58:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoSLLCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the convenient design and prolific functionalities, wireless earbuds are fast penetrating in our daily life and taking over the place of traditional wired earphones. The sensing capabilities of wireless earbuds have attracted great interests of researchers on exploring them as a new interface for human-computer interactions. However, due to its extremely compact size, the interaction on the body of the earbuds is limited and not convenient. In this paper, we propose Ui-Ear, a new on-face gesture recognition system to enrich interaction maneuvers for wireless earbuds. Ui-Ear exploits the sensing capability of Inertial Measurement Units (IMUs) to extend the interaction to the skin of the face near ears. The accelerometer and gyroscope in IMUs perceive dynamic vibration signals induced by on-face touching and moving, which brings rich maneuverability. Since IMUs are provided on most of the budget and high-end wireless earbuds, we believe that Ui-Ear has great potential to be adopted pervasively. To demonstrate the feasibility of the system, we define seven different on-face gestures and design an end-to-end learning approach based on Convolutional Neural Networks (CNNs) for classifying different gestures. To further improve the generalization capability of the system, adversarial learning mechanism is incorporated in the offline training process to suppress the user-specific features while enhancing gesture-related features. We recruit 20 participants and collect a realworld datasets in a common office environment to evaluate the recognition accuracy. The extensive evaluations show that the average recognition accuracy of Ui-Ear is over 95% and 82.3% in the user-dependent and user-independent tasks, respectively. Moreover, we also show that the pre-trained model (learned from user-independent task) can be fine-tuned with only few training samples of the target user to achieve relatively high recognition accuracy (up to 95%). At last, we implement the personalization and recognition components of Ui-Ear on an off-the-shelf Android smartphone to evaluate its system overhead. The results demonstrate Ui-Ear can achieve real-time response while only brings trivial energy consumption on smartphones.}
}


@article{DBLP:journals/tmc/ZouWKJLW25,
	author = {Yongpan Zou and
                  Jianhao Weng and
                  Wenting Kuang and
                  Yang Jiao and
                  Victor C. M. Leung and
                  Kaishun Wu},
	title = {{\textdollar}\{{\textbackslash}sf Img2Acoustic\}{\textdollar}Img2Acoustic:
                  {A} Cross-Modal Gesture Recognition Method Based on Few-Shot Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1496--1512},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3481443},
	doi = {10.1109/TMC.2024.3481443},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZouWKJLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic-based human gesture recognition (HGR) offers diverse applications due to the ubiquity of sensors and touch-free interaction. However, existing machine learning approaches require substantial training data, making the process time-consuming, costly, and labor-intensive. Recent studies have explored cross-modal methods to reduce the need for large training datasets in behavior recognition, but they typically rely on open-source datasets that closely align with the target domain, limiting flexibility and complicating data collection. In this paper, we propose  I m g 2 A c o u s t i c {\\sf Img2Acoustic} , a novel cross-modal acoustic-based HGR approach that leverages models trained on open-source image datasets (i.e., EMNIST, Omniglot) to effectively recognize custom gestures detected via acoustic signals. Our model incorporates a task-aware attention layer (TAAL) and a task-aware local matching layer (TALML), enabling seamless transfer of knowledge from image datasets to acoustic gesture recognition. We implement  I m g 2 A c o u s t i c {\\sf Img2Acoustic}  on commercial devices and conduct comprehensive evaluations, demonstrating that our method not only delivers superior accuracy and robustness compared to existing approaches but also eliminates the need for extensive training data collection.}
}


@article{DBLP:journals/tmc/ReddyV25,
	author = {Y. Arun Kumar Reddy and
                  T. G. Venkatesh},
	title = {Proactive Obsolete Packet Management Based Analysis of Age of Information
                  for {LCFS} Heterogeneous Queueing System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1513--1529},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3481062},
	doi = {10.1109/TMC.2024.3481062},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ReddyV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper analyzes the Age of Information (AoI), focusing on transmitting status updates from source to destination. We analyze the Age of Information in a system comprised of two heterogeneous servers with exponential distribution parameters  μ 1 \\mu _{1}  and  μ 2 \\mu _{2} , respectively. Our study adopts the stochastic hybrid systems (SHS) methodology to thoroughly assess the system’s performance. We explore various queueing disciplines, including Last-Come-First-Serve (LCFS) with work-conservative and LCFS with probabilistic routing, to accurately quantify AoI and Peak AoI (PAoI) metrics. We have used the Proactive Obsolete Packet Management (POPMAN) approach to identify and discard obsolete packets proactively, thus enhancing server processing efficiency and ensuring orderly packet reception. We also investigate the following parameters, such as the probability of preemption of packets, the probability of packets getting obsolete, the probability of informative packets, and optimal splitting probabilities. Results show an improvement in both AoI and PAoI within the LCFS with work-conservative queueing system with the integration of the POPMAN method. Furthermore, LCFS with probabilistic routing using the POPMAN approach performs similarly to conventional methods. In all the queueing systems studied, as the arrival rate  λ → ∞ \\lambda \\to \\infty , the average AoI and PAoI approach  1 / ( μ 1 + μ 2 ) 1/(\\mu _{1}+\\mu _{2}) . For c servers, they approach  1 / ( μ 1 + μ 2 + ⋯ + μ c ) 1/(\\mu _{1}+\\mu _{2}+\\cdots +\\mu _{c}) .}
}


@article{DBLP:journals/tmc/YuanLWXCCG25,
	author = {Xin Yuan and
                  Ning Li and
                  Kang Wei and
                  Wenchao Xu and
                  Quan Chen and
                  Hao Chen and
                  Song Guo},
	title = {Mobility and Cost Aware Inference Accelerating Algorithm for Edge
                  Intelligence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1530--1549},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3484158},
	doi = {10.1109/TMC.2024.3484158},
	timestamp = {Sat, 08 Nov 2025 18:29:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuanLWXCCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The edge intelligence (EI) has been widely applied recently. Splitting the model between device, edge server, and cloud can significantly improve the performance of EI. The model segmentation without user mobility has been investigated in detail in previous studies. However, in most EI use cases, the end devices are mobile. Few studies have been conducted on this topic. These works still have many issues, such as ignoring the energy consumption of mobile device, inappropriate network assumption, and low effectiveness on adapting user mobility, etc. Therefore, to address the disadvantages of model segmentation and resource allocation in previous studies, we propose mobility and cost aware model segmentation and resource allocation algorithm for accelerating the inference at edge (MCSA). Specifically, in the scenario without user mobility, the loop iteration gradient descent (Li-GD) algorithm is provided. When the mobile user has a large model inference task that needs to be calculated, it will take the energy consumption of mobile user, the communication and computing resource renting cost, and the inference delay into account to find the optimal model segmentation and resource allocation strategy. In the scenario with user mobility, the mobility aware Li-GD (MLi-GD) algorithm is proposed to calculate the optimal strategy. Then, the properties of the proposed algorithms are investigated, including convergence, complexity, and approximation ratio. The experimental results demonstrate the effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/tmc/WangWJWG25,
	author = {Haoyu Wang and
                  Jiazhao Wang and
                  Wenchao Jiang and
                  Shuai Wang and
                  Demin Gao},
	title = {Physical Layer Cross-Technology Communication via Explainable Neural
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1550--1566},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3480109},
	doi = {10.1109/TMC.2024.3480109},
	timestamp = {Thu, 22 May 2025 17:08:35 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWJWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-technology communication (CTC) facilitates seamless interaction between different wireless technologies. Most existing methods use reverse engineering to derive the required transmission payload, generating a waveform that the target device can successfully demodulate. However, traditional approaches have certain limitations, including reliance on specific reverse engineering algorithms or the need for manual parameter tuning to reduce emulation distortion. In this work, we present NNCTC, a framework for achieving physical layer cross-technology communication through explainable neural networks, incorporating relevant knowledge from the wireless communication physical layer into the neural network models. We first convert the various signal processing components within the CTC process into neural network models, then build a training framework for the CTC encoder-decoder structure to achieve CTC. NNCTC significantly reduces the complexity of CTC by automatically deriving CTC payloads through training. We demonstrate how NNCTC implements CTC in WiFi systems using OFDM and CCK modulation. On WiFi systems using OFDM modulation, NNCTC outperforms the WEBee and WIDE designs in terms of error performance, achieving an average packet reception ratio (PRR) of 92.3% and an average symbol error rate (SER) as low as 1.3%. In WiFi systems using OFDM modulation, the highest PRR can reach up to 99%.}
}


@article{DBLP:journals/tmc/HuNRLG25,
	author = {Zheyuan Hu and
                  Jianwei Niu and
                  Tao Ren and
                  Xuefeng Liu and
                  Mohsen Guizani},
	title = {SITOff: Enabling Size-Insensitive Task Offloading in D2D-Assisted
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1567--1584},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483951},
	doi = {10.1109/TMC.2024.3483951},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuNRLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC), along with device-to-device (D2D) assisted MEC (D-MEC), are promising technologies that could improve the quality-of-experience for mobile devices (MDs) by offloading their tasks to edge servers or nearby idle MDs. There is a popular trend to develop distributed task offloading algorithms using multi-agent reinforcement learning (MARL), whose adoption of central critics during training makes the offloading still size-sensitive. Therefore, this paper proposes a Size-Insensitive Task Offloading (SITOff) algorithm for D-MEC based on fully-distributed offloading without maintaining any central venue. Specifically, taking advantage of the inherent graph-like structure of D-MEC, SITOff adopts graphs to represent MDs’ states and relationships and form each MD's local knowledge about D-MEC through graph computation. Furthermore, considering the limitation of local knowledge in performing whole performance-oriented offloading, each MD utilizes D2D-transmitting to exchange knowledge with its neighbors and form a comprehensive knowledge about D-MEC to enhance the coordination of distributed offloading. Additionally, regarding the different impacts of neighbors’ knowledge, each MD leverages attention mechanisms to selectively learn its neighbors’ knowledge during knowledge-exchange. Extensive experimental results show the superiority of SITOff over state-of-the-art MARL-based offloading algorithms in D-MEC with various MDs, and the easy collaboration of SITOff with curriculum-learning for large-scale D-MEC offloading.}
}


@article{DBLP:journals/tmc/LiuFCYYH25,
	author = {Jianwei Liu and
                  Xinyue Fang and
                  Yike Chen and
                  Jiantao Yuan and
                  Guanding Yu and
                  Jinsong Han},
	title = {Real-Time Video Forgery Detection via Vision-WiFi Silhouette Correspondence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1585--1601},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483550},
	doi = {10.1109/TMC.2024.3483550},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuFCYYH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For safety guard and crime prevention, video surveillance systems have been pervasively deployed in many security-critical scenarios, such as the residence, retail stores, and banks. However, these systems could be infiltrated by the adversary and the video streams would be modified or replaced, i.e., under the video forgery attack. The prevalence of Internet of Things (IoT) devices and the emergence of Deepfake-like techniques severely emphasize the vulnerability of video surveillance systems under such attacks. To secure existing surveillance systems, in this paper we propose a vision-WiFi cross-modal video forgery detection system, namely WiSil. Leveraging a theoretical model based on the principle of signal propagation, WiSil constructs wave front information of the object in the monitoring area from WiFi signals. With a well-designed deep learning network, WiSil further recovers silhouettes from the wave front information. Based on a Siamese network-based semantic feature extractor, WiSil can eventually determine whether a frame is manipulated by comparing the semantic feature vectors extracted from the video’s silhouette with those extracted from the WiFi’s silhouette. We enhance the basic version of WiSil Fang et al. 2023 by developing a model compression method and a forgery trace localization method. Extensive experiments show that WiSil achieves 95% + +  accuracy in detecting tampered frames.}
}


@article{DBLP:journals/tmc/WangZXXWSYG25,
	author = {Chunjie Wang and
                  Xuhui Zhang and
                  Huijun Xing and
                  Liang Xue and
                  Shuqiang Wang and
                  Yanyan Shen and
                  Bo Yang and
                  Xinping Guan},
	title = {Joint Association, Beamforming, and Resource Allocation for Multi-IRS
                  Enabled {MU-MISO} Systems With {RSMA}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1602--1620},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483193},
	doi = {10.1109/TMC.2024.3483193},
	timestamp = {Wed, 18 Jun 2025 09:35:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZXXWSYG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems. This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes. The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively. Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems. The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms.}
}


@article{DBLP:journals/tmc/MiaoWLLCD25,
	author = {Yinbin Miao and
                  Guijuan Wang and
                  Xinghua Li and
                  Hongwei Li and
                  Kim{-}Kwang Raymond Choo and
                  Robert H. Deng},
	title = {Efficient and Secure Geometric Range Search Over Encrypted Spatial
                  Data in Mobile Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1621--1635},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3482321},
	doi = {10.1109/TMC.2024.3482321},
	timestamp = {Thu, 27 Mar 2025 16:44:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MiaoWLLCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of mobile computing and the popularity of mobile devices equipped with GPS technology, massive spatial data have become available. Enterprises upload encrypted spatial data to the mobile cloud to save local storage and computation costs. However, the existing secure Geometric Range Search (GRS) solutions are inefficient in terms of building, updating index structure and querying processes. Moreover, the index structures of existing GRS schemes based on Order Preserving Encryption (OPE) leak location order, which may lead to reconstruction attacks. To solve these issues, we first propose an efficient and secure GRS scheme using Radix-Tree, namely GRSRT-I. Specifically, we construct an index structure based on Radix-tree to achieve efficient search and update, then use homomorphic encryption NTRU to resist chosen-plaintext attack, finally design a dual-server architecture to alleviate the burdens on mobile users caused by multiple rounds of interactions. Furthermore, we propose an enhanced scheme, GRSRT-II, by combining Order-Revealing Encryption and OPE, which greatly improves the search efficiency while slightly reducing the security. We formally prove the security of our proposed schemes, and conduct extensive experiments to demonstrate that GRSRT-I can improve the query efficiency by up to at least 1.5 times when compared with previous solutions and GRSRT-II can achieve a higher level of search efficiency.}
}


@article{DBLP:journals/tmc/ZhuXLSK25,
	author = {Huixiang Zhu and
                  Yong Xiao and
                  Yingyu Li and
                  Guangming Shi and
                  Marwan Krunz},
	title = {SANSee: {A} Physical-Layer Semantic-Aware Networking Framework for
                  Distributed Wireless Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1636--1653},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483272},
	doi = {10.1109/TMC.2024.3483272},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuXLSK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contactless device-free wireless sensing has recently attracted significant interest due to its potential to support a wide range of immersive human-machine interactive applications using ubiquitously available radio frequency (RF) signals. Traditional approaches focus on developing a single global model based on a combined dataset collected from different locations. However, wireless signals are known to be location and environment specific. Thus, a global model results in inconsistent and unreliable sensing results. It is also unrealistic to construct individual models for all the possible locations and environmental scenarios. Motivated by the observation that signals recorded at different locations are closely related to a set of physical-layer semantic features, in this paper we propose SANSee, a semantic-aware networking-based framework for distributed wireless sensing. SANSee allows models constructed in one or a limited number of locations to be transferred to new locations without requiring any locally labeled data or model training. SANSee is built on the concept of physical-layer semantic-aware network (pSAN), which characterizes the semantic similarity and the correlations of sensed data across different locations. A pSAN-based zero-shot transfer learning solution is introduced to allow receivers in new locations to obtain location-specific models by directly aggregating the models trained by other receivers. We theoretically prove that models obtained by SANSee can approach the locally optimal models. Experimental results based on real-world datasets are used to verify that the accuracy of the transferred models obtained by SANSee matches that of the models trained by the locally labeled data based on supervised learning approaches.}
}


@article{DBLP:journals/tmc/WuWZW25,
	author = {Duo Wu and
                  Panlong Wu and
                  Miao Zhang and
                  Fangxin Wang},
	title = {{MANSY:} Generalizing Neural Adaptive Immersive Video Streaming With
                  Ensemble and Representation Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1654--1668},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487175},
	doi = {10.1109/TMC.2024.3487175},
	timestamp = {Mon, 14 Apr 2025 08:43:31 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of immersive videos has prompted extensive research into neural adaptive tile-based streaming to optimize video transmission over networks with limited bandwidth. However, the diversity of users’ viewing patterns and Quality of Experience (QoE) preferences has not been fully addressed yet by existing neural adaptive approaches for viewport prediction and bitrate selection. Their performance can significantly deteriorate when users’ actual viewing patterns and QoE preferences differ considerably from those observed during the training phase, resulting in poor generalization. In this paper, we propose MANSY, a novel streaming system that embraces user diversity to improve generalization. Specifically, to accommodate users’ diverse viewing patterns, we design a Transformer-based viewport prediction model with an efficient multi-viewport trajectory input output architecture based on implicit ensemble learning. Besides, we for the first time combine the advanced representation learning and deep reinforcement learning to train the bitrate selection model to maximize diverse QoE objectives, enabling the model to generalize across users with diverse preferences. Extensive experiments demonstrate that MANSY outperforms state-of-the-art approaches in viewport prediction accuracy and QoE improvement on both trained and unseen viewing patterns and QoE preferences, achieving better generalization.}
}


@article{DBLP:journals/tmc/NdikumanaNC25,
	author = {Anselme Ndikumana and
                  Kim Khoa Nguyen and
                  Mohamed Cheriet},
	title = {Digital Twin Backed Closed-Loops for Energy-Aware and Open RAN-Based
                  Fixed Wireless Access Serving Rural Areas},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1669--1683},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3482985},
	doi = {10.1109/TMC.2024.3482985},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NdikumanaNC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet access in rural areas should be improved to support digital inclusion and 5G services. Due to the high deployment costs of fiber optics in these areas, Fixed Wireless Access (FWA) has become a preferable alternative. Additionally, the Open Radio Access Network (O-RAN) can facilitate the interoperability of FWA elements, allowing some FWA functions to be deployed at the edge cloud. However, deploying edge clouds in rural areas can increase network and energy costs. To address these challenges, we propose a closed-loop system assisted by a Digital Twin (DT) to automate energy-aware O-RAN based FWA resource management in rural areas. We consider the FWA and edge cloud as the Physical Twin (PT) and design a closed-loop that distributes radio resources to edge cloud instances for scheduling. We develop another closed-loop for intra-slice resource allocation to houses. We design an energy model that integrates radio resource allocation and formulate ultra-small and small-timescale optimizations for the PT to maximize slice requirement satisfaction while minimizing energy costs. We then design a reinforcement learning approach and successive convex approximation to address the formulated problems. We present a DT that replicates the PT by incorporating solution experiences into future states. The results show that our approach efficiently uses radio and energy resources.}
}


@article{DBLP:journals/tmc/LiuZCWYNSH25,
	author = {Yunhao Liu and
                  Jia Zhang and
                  Yande Chen and
                  Weiguo Wang and
                  Songzhou Yang and
                  Xin Na and
                  Yimiao Sun and
                  Yuan He},
	title = {Real-Time Continuous Activity Recognition With a Commercial mmWave
                  Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1684--1698},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483813},
	doi = {10.1109/TMC.2024.3483813},
	timestamp = {Tue, 25 Nov 2025 12:56:57 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZCWYNSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {mmWave-based activity recognition technology has attracted widespread attention as it provides the ability of device-free, ubiquitous and accurate sensing. Recognition of human activities intrinsically demands to be real-time and continuous, but the state of the arts is still far limited with the capacity in this regard. The main obstacle lies in activity sequence segmentation, i.e., locating the boundaries between consecutive activities in an activity sequence. This is a daunting task, due to the unclear activity boundaries and the variable activity duration. In this paper, we propose ZuMa, the first mmWave-based approach to real-time continuous activity recognition. When resorting to a machine learning model for activity recognition, our insight is that the recognition confidence of the recognition model is highly correlated to the accuracy of activity sequence segmentation, so that the former can be utilized as a feedback metric to finely adjust the segmentation boundaries. Based on this insight, ZuMa is a coarse-to-fine grained approach, which includes the fast coarse-grained activity chunk extraction and the find-grained explicit segmentation adjustment and recognition. We have implemented ZuMa with the commercial mmWave radar and evaluated its performance under various settings. The results demonstrate that ZuMa achieves an average recognition error of 12.67%, which is 65.08% and 71.87% lower than that of the two baseline methods. The average recognition delay of ZuMa is only 1.86 s.}
}


@article{DBLP:journals/tmc/ZhengYLM25,
	author = {Xiaolong Zheng and
                  Fu Yu and
                  Liang Liu and
                  Huadong Ma},
	title = {LoRadar: An Efficient LoRa Channel Occupancy Acquirer Based on Cross-Channel
                  Scanning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1699--1714},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487835},
	doi = {10.1109/TMC.2024.3487835},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengYLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa is widely deployed for various applications. Though the knowledge of the channel occupancy is the prerequisite of many aspects of network management, acquiring the channel occupancy for LoRa is challenging due to the large number of possible channels. In this paper, we propose  L o R a d a r {\\sf LoRadar} , a novel LoRa channel occupancy acquirer based on cross-channel scanning. Our in-depth study finds that Channel Activity Detection (CAD) in a narrow band can indicate the channel activities of wide bands because they have the same slope in the time-frequency domain. Based on this finding, we design a cross-channel scanning mechanism that infers the channel occupancy states of all the overlapping channels by the distribution of CAD results. We elaborately select and adjust the CAD settings to enhance the distribution features and design a pattern correction method to cope with distribution distortions. We also design a CAD scheduler to deal with the low duty-cycle LoRa operations. We implement  L o R a d a r {\\sf LoRadar}  on commercial LoRa platforms and evaluate its performance in the indoor testbed and two outdoor deployed networks. The experimental results show that  L o R a d a r {\\sf LoRadar}  can achieve a detection accuracy of 0.99 and reduce the acquisition overhead by up to 90%, compared to the traversal-based methods.}
}


@article{DBLP:journals/tmc/ZhangLZZHFCH25,
	author = {Xianglong Zhang and
                  Feng Li and
                  Huanle Zhang and
                  Haoxin Zhang and
                  Zhijian Huang and
                  Lisheng Fan and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {Model Poisoning Attack Against Neural Network Interpreters in IoT
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1715--1730},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486218},
	doi = {10.1109/TMC.2024.3486218},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLZZHFCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neural network models have become integral to Internet of Things (IoT) systems, with applications spanning from industrial automation to critical infrastructure management. Despite their prevalence, the deployment of these models within IoT systems introduces distinctive security vulnerabilities. In particular, adversaries may execute model poisoning attacks, which aim to alter the decision-making processes of embedded models, leading to erroneous outcomes. Existing model poisoning attacks necessitate access to extensive auxiliary datasets, such as the training dataset itself or one with same distribution. These requirements often render such attacks impractical in IoT contexts, given the constrained storage and computational resources of IoT devices. This paper proposes the first model poisoning attack against interpreters without auxiliary datasets to manipulate the model’s behavior. We evaluate the attack on three real-world datasets, and results indicate that this attack can successfully coerce the targeted interpreters to produce outcomes aligned with an adversary’s intentions, while maintaining nearly indistinguishable performance from the original model, thereby ensuring its stealthiness. Furthermore, beyond directly affected interpreters, our experiments reveal that four additional interpreters coupled to the poisoned model are indirectly influenced, underscoring the attack’s transferability.}
}


@article{DBLP:journals/tmc/WuYHZW25,
	author = {Jiang Wu and
                  Yunchao Yang and
                  Miao Hu and
                  Yipeng Zhou and
                  Di Wu},
	title = {{FCER:} {A} Federated Cloud-Edge Recommendation Framework With Cluster-Based
                  Edge Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1731--1743},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3484493},
	doi = {10.1109/TMC.2024.3484493},
	timestamp = {Wed, 01 Oct 2025 13:59:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuYHZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The traditional recommendation system provides web services by modeling user behavior characteristics, which also faces the risk of leaking user privacy. To mitigate the rising concern on privacy leakage in recommender systems, federated learning (FL) based recommendation has received tremendous attention, which can preserve data privacy by conducting local model training on clients. However, devices (e.g., mobile phones) used by clients in a recommender system may have limited capacity for computation and communication, which can severely deteriorate FL training efficiency. Besides, offloading local training tasks to the cloud can lead to privacy leakage and excessive pressure to the cloud. To overcome this deficiency, we propose a novel federated cloud-edge recommendation framework, which is called FCER, by offloading local training tasks to powerful and trusted edge servers. The challenge of FCER lies in the heterogeneity of edge servers, which makes the parameter server (PS) deployed in the cloud face difficulty in judiciously selecting edge servers for model training. To address this challenge, we divide the FCER framework into two stages. In the first pre-training stage, edge servers expose their data statistical features protected by local differential privacy (LDP) to the PS so that edge servers can be grouped into clusters. In the second training stage, FCER activates a single cluster in each communication round, ensuring that edge servers with statistical homogenization are not repeatedly involved in FL. The PS only selects a certain number of edge servers with the highest data quality in each cluster for FL. Effective metrics are proposed to dynamically evaluate the data quality of each edge server. Convergence rate analysis is conducted to show the convergence of recommendation algorithms in FCER. We also perform extensive experiments to demonstrate that FCER remarkably outperforms competitive baselines by $3.85\\%-9.14\\%$ on HR@10 and $1.46\\%-11.77\\%$ on NDCG@10.}
}


@article{DBLP:journals/tmc/WangW25,
	author = {Tianxin Wang and
                  Xudong Wang},
	title = {DeepRP: Bottleneck Theory Guided Relay Placement for 6G Mesh Backhaul
                  Augmentation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1744--1758},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487020},
	doi = {10.1109/TMC.2024.3487020},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backhaul mesh networks are critical for ensuring coverage and connectivity of high-frequency 6G networks. To maintain high throughput, its architecture needs to be augmented by adding relays. However, how to place relays at appropriate sites poses two challenges: 1) there lacks a theory to capture the relationship between a certain change of network architecture and its throughput gain; 2) selecting the best sites for relays is a complicated combinatorial problem. To tackle the first challenge, this paper first establishes a clique-based bottleneck theory, through which a clique-based bottleneck structure of a given network architecture is constructed to determine the network throughput. Based on this bottleneck structure, clique gradients are then computed to quantify the impact of each clique on the overall network throughput. With the clique-based bottleneck theory, the second challenge is resolved by embedding clique gradients into a deep reinforcement learning (DRL) scheme. Specifically, the DRL actions are masked such that only the relay sites that match the highest clique gradients are selected. This DRL-based relay placement (DeepRP) scheme is evaluated via extensive simulations, and performance results show that it can boost network throughput by more than 50%, which is  10.4 − 32.1 % \\text{10.4} \\!-\\! \\text{32.1}\\%   higher than those of baseline schemes.}
}


@article{DBLP:journals/tmc/YueTLCXYZ25,
	author = {Yi Yue and
                  Xiongyan Tang and
                  Ying{-}Chang Liang and
                  Chang Cao and
                  Lexi Xu and
                  Wencong Yang and
                  Zhiyan Zhang},
	title = {DeepSelector: {A} Deep Learning-Based Virtual Network Function Placement
                  Approach in SDN/NFV-Enabled Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1759--1773},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3483779},
	doi = {10.1109/TMC.2024.3483779},
	timestamp = {Fri, 14 Nov 2025 10:16:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YueTLCXYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of Software-Defined Networks (SDN) and Network Function Virtualization (NFV) has popularized the adoption of the Service Function Chain (SFC) paradigm for efficient network service delivery. This paradigm leverages the flexibility and cost-effectiveness of deploying Virtual Network Functions (VNFs) as software entities or virtual machines on off-the-shelf servers. Chaining VNFs together allows traffic to be directed through the network as required. However, existing algorithms for traffic steering and routing path computation in SFC suffer from many challenges, including complexity, lack of scalability, and low time efficiency. This paper focuses on addressing the challenges associated with VNF placement and SFC chaining in SDN/NFV-enabled networks. Our objective is to identify an optimal solution for VNF placement that maximizes the utilization of network resources. We formulate the problem as a Binary Integer Programming (BIP) model to accomplish this. Additionally, we propose a novel algorithm called DeepSelector, which incorporates deep learning techniques and an intelligent node selection network to determine the optimal placement of VNFs for SFC requests. Through performance evaluation, we demonstrate that DeepSelector achieves high network resource utilization and offers efficient VNF placement computation, significantly improving overall network performance.}
}


@article{DBLP:journals/tmc/GaoHMWB25,
	author = {Yunqi Gao and
                  Bing Hu and
                  Mahdi Boloursaz Mashhadi and
                  Wei Wang and
                  Mehdi Bennis},
	title = {PipeSFL: {A} Fine-Grained Parallelization Framework for Split Federated
                  Learning on Heterogeneous Clients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1774--1791},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489642},
	doi = {10.1109/TMC.2024.3489642},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GaoHMWB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Split Federated Learning (SFL) improves scalability of Split Learning (SL) by enabling parallel computing of the learning tasks on multiple clients. However, state-of-the-art SFL schemes neglect the effects of heterogeneity in the clients’ computation and communication performance as well as the computation time for the tasks offloaded to the cloud server. In this paper, we propose a fine-grained parallelization framework, called PipeSFL, to accelerate SFL on heterogeneous clients. PipeSFL is based on two key novel ideas. First, we design a server-side priority scheduling mechanism to minimize per-iteration time. Second, we propose a hybrid training mode to reduce per-round time, which employs asynchronous training within rounds and synchronous training between rounds. We theoretically prove the optimality of the proposed priority scheduling mechanism within one round and analyze the total time per round for PipeSFL, SFL and SL. We implement PipeSFL on PyTorch. Extensive experiments on seven 64-client clusters with different heterogeneity demonstrate that at training speed, PipeSFL achieves up to 1.65x and 1.93x speedup compared to EPSL and SFL, respectively. At energy consumption, PipeSFL saves up to 30.8% and 43.4% of the energy consumed within each training round compared to EPSL and SFL, respectively.}
}


@article{DBLP:journals/tmc/ZhangGXYAC25,
	author = {Guanghui Zhang and
                  Jing Guo and
                  Mengbai Xiao and
                  Dongxiao Yu and
                  Vaneet Aggarwal and
                  Xiuzhen Cheng},
	title = {A Long-Term-Planning Learning Strategy to Coordinate Viewport Prediction
                  and Video Transmission in 360{\textdegree} Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1792--1804},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487998},
	doi = {10.1109/TMC.2024.3487998},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGXYAC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fueled by Metaverse, 360° video streaming has seen tremendous growth in the past years. However, our measurement reveals that current 360° streaming systems suffer from a dilemma that severely limits QoE. On the one hand, viewport prediction requires the shortest possible prediction distance for high predicting accuracy; On the other hand, video transmission requires more buffered data to compensate for bandwidth fluctuations otherwise substantial playback rebuffering would be incurred. There is so far no existing method that can break this dilemma so the QoE optimization for 360° video streaming was naturally bottlenecked. This work is the first attempt to tackle this challenge by developing QUTA – a novel learning-based streaming system. Specifically, according to our measurement, three kinds of internal streaming parameters have significant impacts on the prediction distance, namely, download pause, data rate threshold, and playback rate. On top of this, we design a new long-term-planning (LTP) continuous control deep reinforcement learning method that tunes the parameters dynamically based on the network condition and the streaming context. Extensive evaluations based on real system prototypes show that QUTA not only improves the prediction accuracy and QoE performance by up to 68.4% but also exhibits strong temporal and spatial robustness.}
}


@article{DBLP:journals/tmc/ZhangXAJ25,
	author = {Zhao Zhang and
                  Chunxiang Xu and
                  Man Ho Allen Au and
                  Changsong Jiang},
	title = {Privacy-Preserving Single-Sign-on With Fine-Grained Access Control
                  for IoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1805--1817},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486719},
	doi = {10.1109/TMC.2024.3486719},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangXAJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT-based sharing economy is a win-win business model, where a transferor owns idle IoT devices and transfers the right to use a device to a user for a fee. Considering usage of multiple devices and privacy preservation, anonymous single-sign-on (ASSO) is a feasible solution for authentication. ASSO allows a user to access multiple devices with one token issued by the transferor and prevents the transferor from identifying the user. We also observe that in the scenario of IoT-based sharing economy, the token should (i) support attributes since a device should be available only to users with specific attributes (e.g., age) and (ii) avoid incurring significant communication/computation overhead as IoT devices are resource-constrained. In this paper, we proposed PILOT, a privacy-preserving single-sign-on with fine-grained access control for IoT devices. When a user attempts to access a device, he/she requests a token from the transferor. The token is actually a blind signature that cannot be tracked, and contains the user’s attributes which facilitate fine-grained access control on the device. Besides, the token consists of only four group elements and verification of the token involves only several exponentiation operations. This renders PILOT superior in terms of communication/computation overhead and suitable for IoT devices.}
}


@article{DBLP:journals/tmc/LuDLWYXX25,
	author = {Qu Lu and
                  Hua Dai and
                  Pengyue Li and
                  Shuyan Wan and
                  Geng Yang and
                  Yang Xiang and
                  Fu Xiao},
	title = {Privacy-Preserving Contact Query Processing Over Trajectory Data in
                  Mobile Cloud Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1818--1832},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3488728},
	doi = {10.1109/TMC.2024.3488728},
	timestamp = {Wed, 24 Sep 2025 11:16:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuDLWYXX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the expansion of mobile devices and cloud computing, massive spatial trajectory data is generated and outsourced to the cloud for storage and analysis, enabling location-based mobile computing services. However, due to the sensitivity of the trajectory data, sharing it in plaintext could lead to privacy risks, especially in operations like contact queries. Thus, achieving secure and efficient contact queries based on the trajectory data in the cloud is a significant challenge. In this paper, we propose a privacy-preserving contact query processing over trajectory data in mobile cloud computing. The projection-based secure trajectory encoding is designed to convert trajectories into secure codes such that the comparison between the distance of two moving objects and the contact distance threshold is transformed into a problem of secure code matching. Adopting the secure code matching method, a baseline privacy-preserving contact query processing is proposed. To improve the query accuracy and efficiency, an amplification factor, an HTG-index and a filter table are designed for query processing optimization, based on which an enhanced privacy-preserving contact query processing is proposed. The game stimulation-based security analysis and experimental results show that the proposed query scheme is secure and performs well in query accuracy and efficiency.}
}


@article{DBLP:journals/tmc/GuoHSCGSJC25,
	author = {Xiuzhen Guo and
                  Yuan He and
                  Longfei Shangguan and
                  Yande Chen and
                  Chaojie Gu and
                  Yuanchao Shu and
                  Kyle Jamieson and
                  Jiming Chen},
	title = {Mighty: Towards Long-Range and High-Throughput Backscatter for Drones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1833--1845},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486993},
	doi = {10.1109/TMC.2024.3486993},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoHSCGSJC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While small drone video streaming systems create unprecedented video content, they also place a power burden exceeding 20% on the drone's battery, limiting flight endurance. We present  M i g h t y {\\sf Mighty} , a hardware-software solution to minimize the power consumption of a drone's video streaming system by offloading power overheads associated with both video compression and transmission to a ground controller.  M i g h t y {\\sf Mighty}  innovates a high performance co-design among: (1) a ring oscillator-based, ultra-low power backscatter radio; (2) a spectrally-efficient, non-linear, low-power physical layer modulation and multi-chain radio architecture; and (3) a lightweight video compression codec-bypassing software design. Our co-design exploits synergies among these components, resulting in joint throughput and range performance that pushes the known envelope. We prototype  M i g h t y {\\sf Mighty}  on PCB board and conduct extensive field studies both indoors and outdoors. The power efficiency of  M i g h t y {\\sf Mighty}  is about 16.6 nJ/bit. A head-to-head comparison with a DJI Mini2 drone's default video streaming system shows that  M i g h t y {\\sf Mighty}  achieves similar throughput at a drone-to-controller distance of up to 150 meters, with 34–55× improvement of power efficiency than WiFi-based video streaming solutions.}
}


@article{DBLP:journals/tmc/LiMFZXW25,
	author = {Yanan Li and
                  Xiao Ma and
                  Zhe Fu and
                  Ao Zhou and
                  Mengwei Xu and
                  Shangguang Wang},
	title = {Rethinking Cost-Efficient {VM} Scheduling on Public Edge Platforms:
                  {A} Service Provider's Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1846--1858},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3488082},
	doi = {10.1109/TMC.2024.3488082},
	timestamp = {Tue, 18 Nov 2025 13:49:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiMFZXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prior studies on traditional centralized clouds independently optimizing static resource utilization and dynamic bandwidth cost are not applicable to edge scenarios, where edge sites are interconnected by wide area networks (WAN) rather than local area networks (LAN) as within clouds. Due to the lack of knowledge about the actual status of public edge platforms and real-world edge datasets, existing influential literature on edge scenarios demonstrates significant disparities in optimization objectives and perspectives. To bridge this gap, we collaborate with a public edge platform and perform a comprehensive measurement, which reveals limitations of the status quo VM scheduling schemes and potential opportunities for improvement. However, resolving VM scheduling considering static resource utilization, dynamic bandwidth cost, and end users’ QoE in a cost-efficient manner faces several challenges, including coupled objectives, exponentially increased complexity, and spatiotemporal dynamics. To address the above challenges, in this work, we propose a holistic online framework that integrates combinatorial bandit-based VM migration and seasonality-aware VM request allocation at two distinct time granularities. Large-scale experiments based on a real-world dataset confirm that our online framework achieves near-offline bandwidth cost and resource utilization while significantly lowering time consumption.}
}


@article{DBLP:journals/tmc/ZhengLHLS25,
	author = {Zhirun Zheng and
                  Zhetao Li and
                  Cheng Huang and
                  Saiqin Long and
                  Xuemin Shen},
	title = {Defending Data Poisoning Attacks in DP-Based Crowdsensing: {A} Game-Theoretic
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1859--1876},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486689},
	doi = {10.1109/TMC.2024.3486689},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengLHLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Differential privacy (DP) is widely used for protecting privacy in crowdsensing by adding noises. However, malicious attackers can exploit noise to launch covert data poisoning attacks. In this paper, we propose a game-based defense approach to resist such data poisoning attacks in DP-based crowdsensing systems. In this approach, attackers are believed to be powerful as they can refine their attack strategy based on the observations of deployed defenders’ defense strategy. Specifically, the defenders formulate the defense as a functional minimization problem (which cannot be directly solved by numerical optimization algorithms because its decision variable is a set of functions), resisting data poisoning attacks by deleting data shared by identified malicious workers through the log-likelihood ratio test. To obtain a current defense strategy, the decision variable of the problem is relaxed into the coefficients of basis-based linear combinations through the variable-basis approximation, and then solved using the simulated annealing genetic algorithm. Correspondingly, the attackers formulate their attack strategy as a bi-level maximization problem (which is an NP-hard problem), biasing crowdsensing results as much as possible while remaining undetected. Since the attackers can know the defense strategy, they may bypass the defenders by constraining the expected log-likelihood ratio test. Additionally, the attackers can evade truth discovery methods deployed in crowdsensing using DP noise. To determine a current attack strategy, the bi-level problem is decomposed into upper-level and lower-level sub-problems, wherein the upper-level sub-problem is solved by the variational methods, and then these sub-problems are alternately optimized. Finally, we propose a local minimax points calculating algorithm to obtain an equilibrium point in the defenders-attackers game, thereby finding an optimal defense strategy to resist the powerful data poisoning attack. Extensive experiments on real-world and synthetic datasets show that the proposed game-based defense approach can effectively defend powerful and covert attackers.}
}


@article{DBLP:journals/tmc/FanWSHSYXMH25,
	author = {Linna Fan and
                  Bo Wu and
                  Xuan Shen and
                  Jun He and
                  Guanglei Song and
                  Gang Yang and
                  Chaocan Xiang and
                  Duohe Ma and
                  Yongfeng Huang},
	title = {HGExplainer: Heterogeneous Graph Explainer for IoT Device Identification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1877--1894},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486717},
	doi = {10.1109/TMC.2024.3486717},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanWSHSYXMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT device identification is vital for network asset and security management. However, existing methods use statistical features that can not identify IoT devices accurately in complex network environments. GraphIoT proposes using non-statistical features and building a heterogeneous graph neural network to identify IoT devices accurately. However, heterogeneous graph neural networks lack interpretability, which reduces trust in the model. Besides, it is difficult to deploy on resource-constrained devices, limiting the broad application of IoT device identification. To make IoT device identification interpretable, easy to deploy, and with high accuracy, we get the interpretation results of GraphIoT through interpretability and further build the rule set based on the interpretation results. Considering there is no suitable interpreter for GraphIoT with many nodes and edges, we propose HGExplainer, which reduces the time complexity by splitting the interpretation target into important relation solving and edge solving and uses a novel solution method, ExpandTree. Then, we also designed a rule extractor, which can build rule sets based on the interpretation results. Experimental results on Yourthings and UNSW datasets show that HGExplainer can build high fidelity, concise sample-level explanations in less than 3 seconds, and the established rule set can precisely identify IoT devices.}
}


@article{DBLP:journals/tmc/LiWCSWSLZ25,
	author = {Yang Li and
                  Dan Wu and
                  Jiahe Chen and
                  Weiyan Shi and
                  Leye Wang and
                  Lu Su and
                  Wenwei Li and
                  Daqing Zhang},
	title = {SigCan: Toward Reliable ToF Estimation Leveraging Multipath Signal
                  Cancellation on Commodity WiFi Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1895--1912},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3491337},
	doi = {10.1109/TMC.2024.3491337},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiWCSWSLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread deployment of WiFi infrastructure has facilitated the development of Time-of-Flight (ToF) based sensing applications. ToF estimation, however, is a challenging task due to the complexity of multipath effect. In this paper, we propose a phase difference based method for ToF estimation and uncover the potential of signal cancellation to mitigate the impact of multipath and noise on phase differences among subcarriers. To separate the moving target path from the complex multipath for ToF estimation, we suggest employing specific elimination methods tailored to the characteristics of different signal components. For dynamic multipath, we observe that when a given subcarrier propagates along two paths to the receiver, with path lengths differing by half a wavelength, the phase difference introduced by these two paths cancels each other out. Therefore, we propose two metrics to identify signals that satisfy this condition, utilizing both frequency diversity and spatial diversity. Additionally, we propose leveraging time diversity to eliminate the static multipath component and reduce the impact of noise. We implemented the methods with off-the-shelf WiFi devices and achieved mean errors of 15.36 cm and 21.05 cm for distance estimation in outdoor and indoor scenarios, outperforming state-of-the-art ToF estimation method by 50% error reduction.}
}


@article{DBLP:journals/tmc/LuoPL25,
	author = {Yu Luo and
                  Lina Pu and
                  Chun{-}Hung Liu},
	title = {Computing Power and Battery Charging Management for Solar Energy Powered
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1913--1927},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489028},
	doi = {10.1109/TMC.2024.3489028},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of energy harvesting capabilities into mobile edge computing (MEC) edge servers enables their deployment beyond the reach of electrical grids, expanding MEC services to isolated regions and geographically challenging terrains. However, the fluctuating nature of renewable energy sources, such as solar and wind, necessitates dynamic management of server computing power in response to variable energy harvesting rates. Unlike conventional models that assume predetermined amounts of harvested energy per time period, this study illustrates the complex interdependencies between server power consumption and variable energy harvesting rates due to battery charging characteristics. To address this, we introduce a novel energy harvesting model that comprehensively accounts for the interaction between computing power management and energy harvesting rates. We develop both offline and online offline optimal computing power management strategies aimed at maximizing the average computational capacity of edge servers. An analytical solution to the resulting nonlinear optimization problem is provided to determine the optimal computing power configurations. Simulation results indicate that the proposed strategy effectively balances energy harvesting rates and energy utilization, thereby enhancing computational performance in dynamic energy environments.}
}


@article{DBLP:journals/tmc/ChenZCJCN25,
	author = {Handi Chen and
                  Rui Zhou and
                  Yun{-}Hin Chan and
                  Zhihan Jiang and
                  Xianhao Chen and
                  Edith C. H. Ngai},
	title = {LiteChain: {A} Lightweight Blockchain for Verifiable and Scalable
                  Federated Learning in Massive Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1928--1944},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3488746},
	doi = {10.1109/TMC.2024.3488746},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZCJCN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Leveraging blockchain in Federated Learning (FL) emerges as a new paradigm for secure collaborative learning on Massive Edge Networks (MENs). As the scale of MENs increases, it becomes more difficult to implement and manage a blockchain among edge devices due to complex communication topologies, heterogeneous computation capabilities, and limited storage capacities. Moreover, the lack of a standard metric for blockchain security becomes a significant issue. To address these challenges, we propose a lightweight blockchain for verifiable and scalable FL, namely LiteChain, to provide efficient and secure services in MENs. Specifically, we develop a distributed clustering algorithm to reorganize MENs into a two-level structure to improve communication and computing efficiency under security requirements. Moreover, we introduce a Comprehensive Byzantine Fault Tolerance (CBFT) consensus mechanism and a secure update mechanism to ensure the security of model transactions through LiteChain. Our experiments based on Hyperledger Fabric demonstrate that LiteChain presents the lowest end-to-end latency and on-chain storage overheads across various network scales, outperforming the other two benchmarks. In addition, LiteChain exhibits a high level of robustness against replay and data poisoning attacks.}
}


@article{DBLP:journals/tmc/LiuDLGHHN25,
	author = {Zhang Liu and
                  Hongyang Du and
                  Junzhe Lin and
                  Zhibin Gao and
                  Lianfen Huang and
                  Seyyedali Hosseinalipour and
                  Dusit Niyato},
	title = {{DNN} Partitioning, Task Offloading, and Resource Allocation in Dynamic
                  Vehicular Networks: {A} Lyapunov-Guided Diffusion-Based Reinforcement
                  Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1945--1962},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486728},
	doi = {10.1109/TMC.2024.3486728},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuDLGHHN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle. To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization. Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time. To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem. Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions. Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency. Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions.}
}


@article{DBLP:journals/tmc/Lee25,
	author = {Byung Moo Lee},
	title = {Efficient Resource Management for Massive {MIMO} in High-Density Massive
                  IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1963--1980},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486712},
	doi = {10.1109/TMC.2024.3486712},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/Lee25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive MIMO technology offers a promising solution for supporting the simultaneous connectivity of a large number of ultra high-density massive IoT devices. However, due to limited resources, effective channel estimation becomes a challenge. One approach is to reuse the orthogonal reference signal (ORS) in a repetitive manner, while another option is to employ random non-orthogonal reference signals (NORS) for distributed massive IoT devices. In order to enhance performance in the face of high RS congestion from massive IoT devices, the strategic utilization of several critical resources becomes imperative. In this paper, we delve into the methodology of resource management to effectively address the severe high-density scenario of massive IoT devices. Specifically, we examine the system bandwidth, which proves particularly advantageous in bandwidth-limited environments. Exploiting the spatial domain, we leverage the distinctive features of massive MIMO to enable simultaneous parallel transmission by increasing the number of service antennas at the base station (BS). Furthermore, we explore the potential benefits of sectorization, a technique that involves dividing a circular cell into multiple sectors, thereby reducing RS congestion. Nevertheless, it is crucial to acknowledge that increasing these resources may entail certain trade-offs and could potentially have adverse effects on overall system performance. To gain comprehensive insights, we conduct a thorough performance analysis under various scenarios, aiming to identify key characteristics that can facilitate the optimal operation of massive MIMO in high-density IoT environments. Building upon our findings, we devise an algorithm that efficiently manages resources, ultimately leading to improved system performance.}
}


@article{DBLP:journals/tmc/ZhaoZHL25,
	author = {Mingxiong Zhao and
                  Rongqian Zhang and
                  Zhenli He and
                  Keqin Li},
	title = {Joint Optimization of Trajectory, Offloading, Caching, and Migration
                  for UAV-Assisted {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1981--1998},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486995},
	doi = {10.1109/TMC.2024.3486995},
	timestamp = {Tue, 11 Mar 2025 21:54:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {UAV-assisted MEC revolutionizes edge computing by deploying UAVs for real-time data processing in areas lacking infrastructure, supporting a wide range of applications from emergency responses to smart cities. Unlike edge servers, UAVs face substantial computational constraints, necessitating a comprehensive strategy that integrates UAV trajectory with task offloading, caching, and migration. Existing studies often overlook the synergy among these strategies, impacting their overall effectiveness. Furthermore, the focus on content pre-caching overlooks task caching’s critical role in addressing high computational demands with limited UAV resources. This research aims to jointly optimize UAV trajectories and task management strategies, including offloading, caching, and migration. Utilizing the Lyapunov optimization framework, we break down the complex optimization problem into manageable subproblems: UAV placement, user-UAV association, task offloading, scheduling, and bandwidth allocation, addressed iteratively using the Block Coordinate Descent method. Specifically, the scheduling subproblem is transformed into a non-convex quadratically constrained quadratic programming problem, managed effectively through semidefinite relaxation and a probabilistic mapping approach. Our simulations show that this integrated approach significantly boosts system throughput and reduces execution times compared to conventional methods. This study enhances the understanding of the interplay between UAV trajectory planning and task management, offering vital theoretical insights for advancing UAV-assisted MEC systems.}
}


@article{DBLP:journals/tmc/LuoPWH25,
	author = {Yu Luo and
                  Lina Pu and
                  Jun Wang and
                  Isaac Howard},
	title = {Enhancing In-Situ Structural Health Monitoring Through {RF} Energy-Powered
                  Sensor Nodes and Mobile Platform},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {1999--2013},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3491574},
	doi = {10.1109/TMC.2024.3491574},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoPWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This research contributes to long-term structural health monitoring (SHM) by exploring radio frequency energy-powered sensor nodes (RF-SNs) embedded in concrete. The RF-SN captures radio energy from a mobile radio transmitter for sensing and communication, which offers a cost-effective solution for consistent in-situ perception. To optimize the system performance across various situations, we’ve explored both active and passive communication methods. For the active RF-SN, we implement a specialized control circuit enabling the node to transmit data through ZigBee protocol at low incident power. For the passive RF-SN, radio energy is not only for power but also as a carrier signal, with data conveyed by modulating the amplitude of the backscattered radio wave. To address the challenge of significant attenuation of the backscattering signal in concrete, we utilize a square chirp-based modulation scheme for passive communication. This scheme allows the receiver to successfully decode the data even under a negative signal-to-noise ratio (SNR) condition. Performance modeling and optimization for both active and passive RF-SNs are provided in this study. The experimental results verify that an active RF-SN embedded in concrete at a depth of 13.5 cm can be effectively powered by a 915 MHz mobile radio transmitter with an effective isotropic radiated power (EIRP) of 32.5 dBm. This setup allows the RF-SN to send over 1 kB of data within 10 seconds, with an additional 1.7 kilobytes every 1.6 seconds of extra charging. For the passive RF-SN buried at the same depth, continuous data transmission at a rate of 224 bps with a 3% bit error rate (BER) is achieved when the EIRP of the transmitter is 23.6 dBm.}
}


@article{DBLP:journals/tmc/LiLZXGY25,
	author = {Xiaochen Li and
                  Sicong Liu and
                  Zimu Zhou and
                  Yuan Xu and
                  Bin Guo and
                  Zhiwen Yu},
	title = {ClassTer: Mobile Shift-Robust Personalized Federated Learning via
                  Class-Wise Clustering},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2014--2028},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487294},
	doi = {10.1109/TMC.2024.3487294},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLZXGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of mobile devices with abundant sensor data and computing power has driven the trend of federated learning (FL) on them. Personalized FL (PFL) aims to train tailored models for each device, addressing data heterogeneity from diverse user behaviors and preferences. However, due to dynamic mobile environments, PFL faces challenges in test-time data shifts, i.e., variations between training and testing. While this issue is well studied in generic deep learning through model generalization or adaptation, this issue remains less explored in PFL, where models often overfit local data. To address this, we introduce  C l a s s T e r {\\sf ClassTer} , a shift-robust PFL framework. We observe that class-wise clustering of clients in cluster-based PFL (CFL) can avoid class-specific biases by decoupling the training of classes. Thus, we propose a paradigm shift from traditional client-wise clustering to class-wise clustering, which allows effective aggregation of cluster models into a generalized one via knowledge distillation. Additionally, we extend ClassTer to asynchronous mobile clients to optimize wall clock time by leveraging critical learning periods and both intra- and inter-device scheduling. Experiments show that compared to status quo approaches,  C l a s s T e r {\\sf ClassTer}  achieves a reduction of up to 91% in convergence time, and an improvement of up to 50.45% in accuracy.}
}


@article{DBLP:journals/tmc/ZhaoZMW25,
	author = {Tianya Zhao and
                  Junqing Zhang and
                  Shiwen Mao and
                  Xuyu Wang},
	title = {Explanation-Guided Backdoor Attacks Against Model-Agnostic {RF} Fingerprinting
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2029--2042},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487967},
	doi = {10.1109/TMC.2024.3487967},
	timestamp = {Mon, 03 Mar 2025 22:25:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the proven capabilities of deep neural networks (DNNs) in identifying devices through radio frequency (RF) fingerprinting, the security vulnerabilities of these deep learning models have been largely overlooked. While the threat of backdoor attacks is well-studied in the image domain, few works have explored this threat in the context of RF signals. In this paper, we thoroughly analyze the susceptibility of DNN-based RF fingerprinting to backdoor attacks, focusing on a more practical scenario where attackers lack access to control model gradients and training processes. We propose leveraging explainable machine learning techniques and autoencoders to guide the selection of trigger positions and values, allowing for the creation of effective backdoor triggers in a model-agnostic manner. To comprehensively evaluate this backdoor attack, we employ four diverse datasets with two protocols (Wi-Fi and LoRa) across various DNN architectures. Given that RF signals are often transformed into the frequency or time-frequency domains, this study also assesses attack efficacy in the time-frequency domain. Furthermore, we experiment with potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack. Additionally, we consider the attack performance in the domain shift case.}
}


@article{DBLP:journals/tmc/ZhouWXQ25,
	author = {Xiaobo Zhou and
                  Chuanan Wang and
                  Qi Xie and
                  Tie Qiu},
	title = {V2I-Coop: Accurate Object Detection for Connected Automated Vehicles
                  at Accident Black Spots With {V2I} Cross-Modality Cooperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2043--2055},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3486758},
	doi = {10.1109/TMC.2024.3486758},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouWXQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate object detection with on-board LiDAR sensors is crucial for ensuring driving safety of Connected Automated Vehicles (CAVs), especially at accident black spots with more occlusions. Fortunately, road-side infrastructure equipped with traffic cameras is usually available at these places, offers an extensive field of view and encounters fewer occlusions, and thus can provide sustained assistance to CAVs to improve their object detection performance. However, vehicle-to-infrastructure (V2I) cooperative object detection is quite challenging due to modality heterogeneity, agent heterogeneity, and bandwidth limitations. To address these challenges, in this paper, we propose V2I-Coop, an accurate object detection approach with V2I cross-modality cooperation for CAVs to improve perception performance at accident black spots. In V2I-Coop, first, we extract bird-eye-view (BEV) features from both multi-view 2D images and 3D point clouds, which facilitates the feature fusion of different modalities. Next, the most valuable features from the images are adaptively selected according to available bandwidth and then transmitted to CAVs. Then, a cross-modality feature fusion algorithm is adopted at CAVs to mitigate the modality difference and improve the feature fusion efficiency. Finally, extensive experiments demonstrate that V2I-Coop significantly improves the 3D object detection performance of CAVs at accident black spots.}
}


@article{DBLP:journals/tmc/GeCJ25,
	author = {Chao Ge and
                  Ge Chen and
                  Zhipeng Jiang},
	title = {A "Breathing" Mobile Communication Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2056--2072},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3487213},
	doi = {10.1109/TMC.2024.3487213},
	timestamp = {Thu, 20 Mar 2025 14:45:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GeCJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The frequent migration of large-scale users leads to the load imbalance of mobile communication networks, which causes resource waste and decreases user experience. To address the load balancing problem, this paper proposes a dynamic optimization framework for mobile communication networks inspired by the average consensus in multi-agent systems. In this framework, all antennas cooperatively optimize their CPICH (Common Pilot Channel) transmit power in real-time to balance their busy-degrees. Then, the coverage area of each antenna would change accordingly, and we call this framework a “breathing” mobile communication network. To solve this optimization problem, two algorithms named BDBA (Busy-degree Dynamic Balancing Algorithm) and BFDBA (Busy-degree Fast Dynamic Balancing Algorithm) are proposed. Moreover, a fast network coverage calculation method is introduced, by which each antenna's minimum CPICH transmit power is determined under the premise of meeting the network coverage requirements. Besides, we present the theoretical analysis of the two proposed algorithms’ performance, which prove that all antennas’ busy-degrees will reach consensus under certain assumptions. Furthermore, simulations carried out on three large datasets demonstrate that our cooperative optimization can significantly reduce the unbalance among antennas as well as the proportion of over-busy antennas.}
}


@article{DBLP:journals/tmc/ZhangGGG25,
	author = {Zhenxiao Zhang and
                  Zhidong Gao and
                  Yuanxiong Guo and
                  Yanmin Gong},
	title = {Heterogeneity-Aware Cooperative Federated Edge Learning With Adaptive
                  Computation and Communication Compression},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2073--2084},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3492916},
	doi = {10.1109/TMC.2024.3492916},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGGG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the drawbacks of cloud-based federated learning (FL), cooperative federated edge learning (CFEL) has been proposed to improve efficiency for FL over mobile edge networks, where multiple edge servers collaboratively coordinate the distributed model training across a large number of edge devices. However, CFEL faces critical challenges arising from dynamic and heterogeneous device properties, which slow down the convergence and increase resource consumption. This paper proposes a heterogeneity-aware CFEL scheme called Heterogeneity-Aware Cooperative Edge-based Federated Averaging (HCEF) that aims to maximize the model accuracy while minimizing the training time and energy consumption via adaptive computation and communication compression in CFEL. By theoretically analyzing how local update frequency and gradient compression affect the convergence error bound in CFEL, we develop an efficient online control algorithm for HCEF to dynamically determine local update frequencies and compression ratios for heterogeneous devices. Experimental results show that compared with prior schemes, the proposed HCEF scheme can maintain higher model accuracy while reducing training latency and improving energy efficiency simultaneously.}
}


@article{DBLP:journals/tmc/ChaiWCYWH25,
	author = {Baili Chai and
                  Di Wu and
                  Jinyu Chen and
                  Mengyu Yang and
                  Zelong Wang and
                  Miao Hu},
	title = {{REM:} Enabling Real-Time Neural-Enhanced Video Streaming on Mobile
                  Devices Using Macroblock-Aware Lookup Table},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2085--2097},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3496443},
	doi = {10.1109/TMC.2024.3496443},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChaiWCYWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for mobile video streaming has seen a substantial surge in recent years. However, current platforms heavily depend on network capacity to ensure the delivery of high-quality video streams. The emergence of neural-enhanced video streaming presents a promising solution to address this challenge by leveraging client-side computation, thereby reducing bandwidth consumption. Nonetheless, deploying advanced super-resolution (SR) models on mobile devices is hindered by the computational demands of existing SR models. In this paper, we propose REM, a novel neural-enhanced mobile video streaming framework. REM utilizes a customized lookup table to facilitate real-time neural-enhanced video streaming on mobile devices. Initially, we conduct a series of measurements to identify abundant macroblock redundancies across frames in a video stream. Subsequently, we introduce a dynamic macroblock selection algorithm that prioritizes important macroblocks for neural enhancement. The SR-enhanced results are stored in the lookup table and efficiently reused to meet real-time requirements and minimize resource overhead. By considering macroblock-level characteristics of the video frames, the lookup table enables efficient and fast processing. Additionally, we design a lightweight macroblock-aware SR module to expedite inference. Finally, we perform extensive experiments on various mobile devices. The results demonstrate that REM enhances overall processing throughput by up to 10.2 times and reduces power consumption by up to 58.6% compared to state-of-the-art methods. Consequently, this leads to a 38.06% improvement in the quality of experience for mobile users.}
}


@article{DBLP:journals/tmc/LiuWLLZQCG25,
	author = {Wenhao Liu and
                  Jiazhi Wu and
                  Quanwei Lin and
                  Handong Luo and
                  Qi Zhang and
                  Kun Qiu and
                  Zhe Chen and
                  Yue Gao},
	title = {Efficient Satellite-Ground Interconnection Design for Low-Orbit Mega-Constellation
                  Topology},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2098--2109},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3490575},
	doi = {10.1109/TMC.2024.3490575},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWLLZQCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The low-orbit mega-constellation network (LMCN) is an important part of the space-air-ground integrated network system. An effective satellite-ground interconnection design can result in a stable constellation topology for LMCNs. A naïve solution is accessing the satellite with the longest remaining service time (LRST), which is widely used in previous designs. The Coordinated Satellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm, coordinates the establishment of ground-satellite links (GSLs). Compared with existing solutions, it reduces latency by 19% and jitter by 70% on average. However, CSGI only supports the scenario where terminals access only one satellite, and cannot fully utilize the multi-access capabilities of terminals. Additionally, CSGI's high computational complexity poses deployment challenges. To overcome these problems, we propose the Classification-based Longest Remaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario with multi-access capabilities. It adds optional paths during routing with low computational complexity, improving end-to-end communications quality. We conduct our 1000 s simulation from Brazil to Lithuania on the open-source platform Hypatia. Experiment results show that compared with CSGI, C-LRST reduces the latency and increases the throughput by approximately 60% and 40%, respectively. In addition, C-LRST's GSL switchings number is 14, whereas CSGI is 23. C-LRST has better link stability than CSGI.}
}


@article{DBLP:journals/tmc/GuQ25,
	author = {Yifan Gu and
                  Zhi Quan},
	title = {Adaptive Sampling for Age of Information in Non-Stationary Network
                  Traffic},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2110--2123},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493592},
	doi = {10.1109/TMC.2024.3493592},
	timestamp = {Mon, 03 Mar 2025 22:25:36 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time status updates play an important role in low-latency cyber-physical systems, in which the real network traffic statistics (i.e., transmission delay and/or error rate) are often unknown and non-stationary. In such cases, short-time age-of-information (ST-AoI) is more crucial than long-term average AoI, because instantaneous high ST-AoI could lead to system failures even if the long-term average AoI is low. In this paper, we propose an adaptive sampling control (ASC) scheme to ensure a low ST-AoI outage probability, defined as the probability of the average AoI in each control cycle, i.e., over a limited number of packets, exceeding a given threshold. This ASC scheme does not rely on an explicit statistical model for the non-stationary traffic behaviors. It establishes a dynamic linearization data model with a pseudo-partial derivative (PPD) parameter to capture the unknown and non-stationary traffic statistics. By estimating the PPD parameter in each control cycle, ASC can determine the sampling rates to ensure an extremely low ST-AoI outage probability. Both numerical simulation and real-world experiment show that the proposed ASC scheme significantly outperforms existing methods, reducing the ST-AoI outage probability almost by half.}
}


@article{DBLP:journals/tmc/ZhuHLZ25,
	author = {Shengchao Zhu and
                  Guangjie Han and
                  Chuan Lin and
                  Yu Zhang},
	title = {Underwater Target Tracking Based on Interrupted Software-Defined Multi-AUV
                  Reinforcement Learning: {A} Multi-AUV Time-Saving {MARL} Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2124--2136},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3490545},
	doi = {10.1109/TMC.2024.3490545},
	timestamp = {Fri, 30 Jan 2026 16:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of underwater materials technology and underwater robot technology, human exploitation of marine resources has been increasingly advanced, which has given rise to various application scenarios for Autonomous Underwater Vehicle (AUV) cluster networks, such as cooperative data collection and target tracking. In this paper, we aim to explore how to utilize networking and swarm intelligence to improve the AUV cluster network’s target tracking performance in a time-saving manner. Specifically, on account of our previous work, we introduce an underwater interrupted mechanism and propose an Interrupted Software-Defined Multi-AUV Reinforcement Learning (ISD-MARL) architecture. For MARL algorithm in ISD-MARL, we propose a time-saving MARL algorithm, S-MADDPG, integrating our proposed action optimization model and action network loss function, to accelerate the convergence of the MARL algorithm. Furthermore, to further improve the AUV cluster network’s path planning performance during the target tracking, we propose an Interrupted Tracking Path Planning Scheme (ITPPS) for the AUV cluster network based on the proposed ISD-MARL and S-MADDPG. The evaluation results showcase that our proposed scheme can effectively plan the underwater target tracking path for the AUV cluster network in a shorter time and outperform various mainstream strategies in terms of convergence speed and training time, etc.}
}


@article{DBLP:journals/tmc/ChengSNZZI25,
	author = {Runze Cheng and
                  Yao Sun and
                  Dusit Niyato and
                  Lan Zhang and
                  Lei Zhang and
                  Muhammad Ali Imran},
	title = {A Wireless AI-Generated Content {(AIGC)} Provisioning Framework Empowered
                  by Semantic Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2137--2150},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493375},
	doi = {10.1109/TMC.2024.3493375},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChengSNZZI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the significant advances in AI-generated content (AIGC) and the proliferation of mobile devices, providing high-quality AIGC services via wireless networks is becoming the future direction. However, the primary challenges of AIGC services provisioning in wireless networks lie in unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To this end, this paper proposes a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be generated and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion models within the semantic encoder and decoder to design a workload-adjustable transceiver thereby allowing adjustment of computational resource utilization in edge and local. In addition, a resource-aware workload trade-off (ROOT) scheme is devised to intelligently make workload adaptation decisions for the transceiver, thus efficiently generating, transmitting, and fine-tuning content as per dynamic wireless channel conditions and service requirements. Simulations verify the superiority of our proposed SemAIGC framework in terms of latency and content quality compared to conventional approaches.}
}


@article{DBLP:journals/tmc/MengZWJSL25,
	author = {Jin Meng and
                  Qinglin Zhao and
                  Weimin Wu and
                  Minghao Jin and
                  Penghui Song and
                  Yingzhuang Liu},
	title = {Enhancing {IEEE} 802.11ax Network Performance: An Investigation and
                  Modeling Into Multi-User Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2151--2165},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493032},
	doi = {10.1109/TMC.2024.3493032},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MengZWJSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study explores the performance optimization of uplink orthogonal frequency division multiple access (OFDMA)-based random access (UORA) in IEEE 802.11ax networks. UORA supports multi-user transmission via two methods, where users transmit either fixed-size or variable-size aggregated MAC protocol data units. However, three critical issues arise. 1 Existing studies only focus on the fixed-size method with low practicality, and overlook the impact of traffic load which leads to inaccurate evaluation of the network performance. 2 The variable-size method has never been studied due to a complex scenario, where user frames append padding bits to fulfill the transmission opportunity constraint. 3 In realistic networks, the variable-size method sacrifices throughput to achieve high practicality and low latency. To address the first two issues, we proposed two novel models based on queueing theory that accurately capture the impact of these transmission methods and various parameters (e.g., the traffic load and padding bits) on throughput, packet loss rate, and latency. To address Issue 3, we design a Dynamic Selection Algorithm of Transmission Methods (DSATM), which dynamically switches between the two transmission methods to enhance practicality, maximize throughput, and minimize latency. Finally, we conducted extensive simulations to verify the accuracy of our models and DSATM.}
}


@article{DBLP:journals/tmc/LiZGWHSC25,
	author = {Yadong Li and
                  Dongheng Zhang and
                  Ruixu Geng and
                  Jincheng Wu and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {IFNet: Deep Imaging and Focusing for Handheld {SAR} With Millimeter-Wave
                  Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2166--2180},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489641},
	doi = {10.1109/TMC.2024.3489641},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZGWHSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements have showcased the potential of handheld millimeter-wave (mmWave) imaging, which applies synthetic aperture radar (SAR) principles in portable settings. However, existing studies addressing handheld motion errors either rely on costly tracking devices or employ simplified imaging models, leading to impractical deployment or limited performance. In this paper, we present IFNet, a novel deep unfolding network that combines the strengths of signal processing models and deep neural networks to achieve robust imaging and focusing for handheld mmWave systems. We first formulate the handheld imaging model by integrating multiple priors about mmWave images and handheld phase errors. Furthermore, we transform the optimization processes into an iterative network structure for improved and efficient imaging performance. Extensive experiments demonstrate that IFNet effectively compensates for handheld phase errors and recovers high-fidelity images from severely distorted signals. In comparison with existing methods, IFNet can achieve at least 11.89 dB improvement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement in average structural similarity index measure (SSIM) on a real-world dataset.}
}


@article{DBLP:journals/tmc/Mei25,
	author = {Zhonghui Mei},
	title = {A Novel Method to Solve the Maximum Weight Clique Problem for Instantly
                  Decodable Network Coding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2181--2192},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489724},
	doi = {10.1109/TMC.2024.3489724},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/Mei25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Minimizing the decoding delay, the completion time, or the delivery time of instantly decodable network coding (IDNC) can all be approximated to a maximum weight clique (MWC) problem, which is well known to be NP hard. Due to its good tradeoff between performance and computational complexity, a heuristic approach named as maximum weight vertex (MWV) search is widely employed to select MWC for IDNC. However, in MWV, when there are few coding connection edges among the adjacent vertices of a vertex, its modified vertex weight cannot well reflect the weight of the MWC containing the vertex, which leads to incorrect selection of MWC. This paper proposes a new method to calculate the modified weight of a vertex by summing the weights of the vertices in the approximate maximum weight path (A-MWP) generated by this vertex. Since the vertices in an A-MWP can form a maximal clique, the proposed modified vertex weight may well indicate the weight of the MWC containing the vertex. The proposed algorithm has the same computational complexity as the MWV algorithm. Simulation results show that when employing any of the three performance metrics of IDNC, our proposed algorithm can achieve better system performance than the MWV algorithm.}
}


@article{DBLP:journals/tmc/WangZHT25,
	author = {Hao Wang and
                  Huijuan Zheng and
                  Guangjie Han and
                  Dong Tang},
	title = {An Adaptive Scheme for Protecting Source Location Privacy in Underwater
                  Acoustic Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2193--2202},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489722},
	doi = {10.1109/TMC.2024.3489722},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZHT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, the source location privacy (SLP) becomes a hot research interest in network security of Underwater Acoustic Sensor Networks (UASNs), and existing schemes are mostly proposed for a given scenario. Introducing source location privacy technologies inevitably increase the energy consumption of nodes, while they are widely deployed in available studies, resulting in massive energy wastage. Therefore, an adaptive scheme for protecting source location privacy (APSLP) in UASNs is proposed. The APSLP scheme first analyzes the possible locations of the adversary by trust method. Then, considering the lagging nature of the trust method, which means that the adversary may not stay in locations given by trust method, a hidden Markov-based backtracking method is proposed and location privacy methods are functioned according to the backtracking result. The simulation shows that even though the security level of the APSLP scheme is not the largest, the efficiency is the highest, approximately an increase of 69.1 % \\%  and 10.3 % \\%  compared with two comparison algorithms, respectively.}
}


@article{DBLP:journals/tmc/BaiGWSLX25,
	author = {Jing Bai and
                  Jinsong Gui and
                  Tian Wang and
                  Houbing Song and
                  Anfeng Liu and
                  Neal N. Xiong},
	title = {{ETBP-TD:} An Efficient and Trusted Bilateral Privacy-Preserving Truth
                  Discovery Scheme for Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2203--2219},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489717},
	doi = {10.1109/TMC.2024.3489717},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BaiGWSLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsensing (MCS) has emerged as a promising sensing paradigm for accomplishing large-scale tasks by leveraging ubiquitously distributed mobile workers. Due to the variability in sensory data provided by different workers, identifying truth values from them has garnered wide attention. However, existing truth discovery schemes either offer limited privacy protection or incur high participation costs and lower data aggregation quality due to malicious workers. In this paper, we propose an Efficient and Trusted Bilateral Privacy-preserving Truth Discovery scheme (ETBP-TD) to obtain high-quality truth values while preventing privacy leakage from both workers and the data requester. Specifically, a matrix encryption-based protocol is introduced to the whole truth discovery process, which keeps locations and data related to tasks and workers secret from other entries. Additionally, trust-based worker recruitment and trust update mechanisms are first integrated within a privacy-preserving truth discovery scheme to enhance truth value accuracy and reduce unnecessary participation costs. Our theoretical analyses on the security and regret of ETBP-TD, along with extensive simulations on real-world datasets, demonstrate that ETBP-TD effectively preserves workers’ and tasks’ privacy while reducing the estimated error by up to 84.40% and participation cost by 54.72%.}
}


@article{DBLP:journals/tmc/LanCCLLCS25,
	author = {Wenjun Lan and
                  Kongyang Chen and
                  Jiannong Cao and
                  Yikai Li and
                  Ning Li and
                  Qi Chen and
                  Yuvraj Sahni},
	title = {Security-Sensitive Task Offloading in Integrated Satellite-Terrestrial
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2220--2233},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3489619},
	doi = {10.1109/TMC.2024.3489619},
	timestamp = {Fri, 23 Jan 2026 07:32:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LanCCLLCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of sixth-generation (6G) communication technology, global communication networks are moving towards the goal of comprehensive and seamless coverage. In particular, low earth orbit (LEO) satellites have become a critical component of satellite communication networks. The emergence of LEO satellites has brought about new computational resources known as the LEO satellite edge, enabling ground users (GU) to offload computing tasks to the resource-rich LEO satellite edge. However, existing LEO satellite computational offloading solutions primarily focus on optimizing system performance, neglecting the potential issue of malicious satellite attacks during task offloading. In this paper, we propose the deployment of LEO satellite edge in an integrated satellite-terrestrial networks (ISTN) structure to support security-sensitive computing task offloading. We model the task allocation and offloading order problem as a joint optimization problem to minimize task offloading delay, energy consumption, and the number of attacks while satisfying reliability constraints. To achieve this objective, we model the task offloading process as a Markov decision process (MDP) and propose a security-sensitive task offloading strategy optimization algorithm based on proximal policy optimization (PPO). Experimental results demonstrate that our algorithm significantly outperforms other benchmark methods in terms of performance.}
}


@article{DBLP:journals/tmc/ZhangLGTZWL25,
	author = {Feifei Zhang and
                  Mao Li and
                  Jidong Ge and
                  Fenghui Tang and
                  Sheng Zhang and
                  Jie Wu and
                  Bin Luo},
	title = {Privacy-Preserving Federated Neural Architecture Search With Enhanced
                  Robustness for Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2234--2252},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3490835},
	doi = {10.1109/TMC.2024.3490835},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLGTZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of large-scale artificial intelligence services, edge devices are becoming essential providers of data and computing power. However, these edge devices are not immune to malicious attacks. Federated learning (FL), while protecting privacy of decentralized data through secure aggregation, struggles to trace adversaries and lacks optimization for heterogeneity. We discover that FL augmented with Differentiable Architecture Search (DARTS) can improve resilience against backdoor attacks while compatible with secure aggregation. Based on this, we propose a federated neural architecture search (NAS) framwork named SLNAS. The architecture of SLNAS is built on three pivotal components: a server-side search space generation method that employs an evolutionary algorithm with dual encodings, a federated NAS process based on DARTS, and client-side architecture tuning that utilizes Gumbel softmax combined with knowledge distillation. To validate robustness, we adapt a framework that includes backdoor attacks based on trigger optimization, data poisoning, and model poisoning, targeting both model weights and architecture parameters. Extensive experiments demonstrate that SLNAS not only effectively counters advanced backdoor attacks but also handles heterogeneity, outperforming defense baselines across a wide range of backdoor attack scenarios.}
}


@article{DBLP:journals/tmc/YinXMG25,
	author = {Jiangjin Yin and
                  Xin Xie and
                  Hangyu Mao and
                  Song Guo},
	title = {Efficient Missing Key Tag Identification in Large-Scale {RFID} Systems:
                  An Iterative Verification and Selection Method},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2253--2269},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493597},
	doi = {10.1109/TMC.2024.3493597},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YinXMG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency identification (RFID) system has been extensively employed to track missing items by affixing them with RFID tags. Many practical applications require to efficiently identify missing events for a specific subset of system tags (called key tags) due to their elevated importance. Existing methods primarily aim to identify all tags, which makes it challenging to specifically identify key tags because of interference from other non-key tags (called ordinary tags). In light of this, several key tag identification methods follow a two-step scheme that filters ordinary tags first and then identifies key tags. Nevertheless, this wastes too much time on tag filtering, resulting in low time efficiency. This paper presents a novel missing key tag identification protocol with two creative designs to gain high efficiency. First, we develop a novel verification technique that can rapidly determine the presence or absence of key tags amid the scenarios with both key tags and ordinary ones. By combining the ON-OFF Keying modulation, we could verify multiple key tags in a single slot, thereby reducing the total slots required. Second, we design a new selection technique that efficiently selects the unverified key tags for further verification, while filtering out the verified key tags and irrelevant ordinary tags to avoid redundant data transmission. Additionally, we present an enhancement protocol that leverages a preselection technique to avoid collecting useless tag responses, further boosting efficiency. We carry out rigorous theoretical analysis to optimize the performance of the proposed protocols. Both simulations and practical experiments demonstrate that our method is markedly superior to state-of-the-art solutions.}
}


@article{DBLP:journals/tmc/LiSPCYCLLX25,
	author = {Dongbo Li and
                  Yuchen Sun and
                  Jielun Peng and
                  Siyao Cheng and
                  Zhisheng Yin and
                  Nan Cheng and
                  Jie Liu and
                  Zhijun Li and
                  Chenren Xu},
	title = {Dual Network Computation Offloading Based on {DRL} for Satellite-Terrestrial
                  Integrated Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2270--2284},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493388},
	doi = {10.1109/TMC.2024.3493388},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiSPCYCLLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite-terrestrial integrated networks based on edge computing can provide computation offloading service to terminal devices in remote areas. However, it faces various limitations, including satellite energy consumption, computation delay, and environmental dynamics, etc. In this paper, we propose a satellite-terrestrial integrated cloud and edge computing network (STCECN) architecture, including satellite layer, terrestrial layer and cloud center, where computing resources exist in multi-layer heterogeneous edge computing clusters. Optimization of system delay and energy consumption is defined as a mixed-integer programming problem. Moreover, we present a deep reinforcement learning-based computation offloading decision algorithm that can adapt to the dynamics and variability of satellite networks. A dual network computation offloading decision method is proposed for delay and energy consumption based on deep reinforcement learning offloading (DRLO), including deep convolutional network update method, quantization strategy, and bandwidth resource allocation. Meanwhile, the proposed method is based on previous experience and integrates deviation adjustment strategies for decision making to solve the problem of pseudo-patch loss caused by satellite network switching. The simulation results indicate that the proposed method performs almost consistently with traditional heuristic algorithms, with only 20% of the time consumption of the latter, and the number of pseudo packet loss also decreases to the original 10–20%.}
}


@article{DBLP:journals/tmc/XuLPLL25,
	author = {Ziqi Xu and
                  Jingcheng Li and
                  Yanjun Pan and
                  Ming Li and
                  Loukas Lazos},
	title = {Harvesting Physical-Layer Randomness in Millimeter Wave Bands},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2285--2300},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3499876},
	doi = {10.1109/TMC.2024.3499876},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuLPLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The unpredictability of the wireless channel has been used as a natural source of randomness to build physical-layer security primitives for shared key generation, authentication, access control, proximity verification, and other security properties. Compared to pseudo-random generators, it has the potential to achieve information-theoretic security. In sub-6 GHz frequencies, the randomness is harvested from the small-scale fading effects of RF signal propagation in rich scattering environments. However, the RF propagation characteristics follow sparse models with clustered paths when devices operate in millimeter-wave (mmWave) bands (5G and Next-Generation networks, Wi-Fi in 60GHz). Millimeter-wave transmissions are typically directional to increase the gain and combat high signal attenuation, leading to stable and more predictable channels. In this paper, we first demonstrate that state-of-the-art methods relying on channel state information or received signal strength measurements fail to produce high randomness. Accounting for the unique features of mmWave propagation, we propose a novel randomness extraction mechanism that exploits the random timing of channel blockage to harvest random bits. Compared with the prior art in CSI-based and context-based randomness extraction, our protocol remains secure against passive and active Man-in-the-Middle adversaries co-located with the legitimate devices. We demonstrate the security properties of our method in a 28 GHz mmWave testbed in an indoor setting.}
}


@article{DBLP:journals/tmc/GengXLWWZ25,
	author = {Wei Geng and
                  Baidi Xiao and
                  Rongpeng Li and
                  Ning Wei and
                  Dong Wang and
                  Zhifeng Zhao},
	title = {Noise Distribution Decomposition Based Multi-Agent Distributional
                  Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2301--2314},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3492272},
	doi = {10.1109/TMC.2024.3492272},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GengXLWWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generally, Reinforcement Learning (RL) agent updates its policy by repetitively interacting with the environment, contingent on the received rewards to observed states and undertaken actions. However, the environmental disturbance, commonly leading to noisy observations (e.g., rewards and states), could significantly shape the performance of agent. Furthermore, the learning performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to noise due to the interference among intelligent agents. Therefore, it becomes imperative to revolutionize the design of MARL, so as to capably ameliorate the annoying impact of noisy rewards. In this paper, we propose a novel decomposition-based multi-agent distributional RL method by approximating the globally shared noisy reward by a Gaussian Mixture Model (GMM) and decomposing it into the combination of individual distributional local rewards, with which each agent can be updated locally through distributional RL. Moreover, a Diffusion Model (DM) is leveraged for reward generation in order to mitigate the issue of costly interaction expenditure for learning distributions. Furthermore, the monotonicity of the reward distribution decomposition is theoretically validated under nonnegative weights and increasing distortion risk function, while the design of the loss function is carefully calibrated to avoid decomposition ambiguity. We also verify the effectiveness of the proposed method through extensive simulation experiments with noisy rewards. Besides, different risk-sensitive policies are evaluated in order to demonstrate the superiority of distributional RL in different MARL tasks.}
}


@article{DBLP:journals/tmc/LiuLXLYCQ25,
	author = {Jianchun Liu and
                  Jun Liu and
                  Hongli Xu and
                  Yunming Liao and
                  Zhiwei Yao and
                  Min Chen and
                  Chen Qian},
	title = {Enhancing Semi-Supervised Federated Learning With Progressive Training
                  in Heterogeneous Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2315--2330},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3492140},
	doi = {10.1109/TMC.2024.3492140},
	timestamp = {Sun, 28 Sep 2025 09:10:49 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLXLYCQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an efficient distributed learning method that facilitates collaborative model training among multiple edge devices (or clients). However, current research always assumes that clients have access to ground-truth data for training, which is unrealistic in practice because of a lack of expertise. Semi-supervised federated learning (SSFL) has been proposed in many existing works to address this problem, which always adopts a fixed model architecture for training, bringing two main problems with varying amounts of pseudo-labeled data. First, the shallow model cannot have the capability to fit the increasing pseudo-labeled data, leading to poor training performance. Second, the large model suffers from an overfitting problem when exploiting a few labeled data samples in SSFL, and also requires tremendous resource (e.g., computation and communication) costs. To tackle these problems, we propose a novel framework, called star, which adopts progressive training to enhance model training in SSFL. Specifically, star gradually increases the model depth through adding the sub-module (e.g., one or several layers) from a shallow model, and performs pseudo-labeling for unlabeled data with a specialized confidence threshold simultaneously. Then, we propose an efficient algorithm to determine the appropriate model depth for each client with varied resource budgets and the proper confidence threshold for pseudo-labeling in SSFL. The experimental results demonstrate the high effectiveness of STAR. For instance, star can reduce the bandwidth consumption by about 40%, and achieve an average accuracy improvement of around 9.8% compared with the baselines, on CIFAR10.}
}


@article{DBLP:journals/tmc/ZhaoMWHAMZ25,
	author = {Liang Zhao and
                  Chaojin Mao and
                  Shaohua Wan and
                  Ammar Hawbani and
                  Ahmed Yassin Al{-}Dubai and
                  Geyong Min and
                  Albert Y. Zomaya},
	title = {{CAST:} Efficient Traffic Scenario Inpainting in Cellular Vehicle-to-Everything
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2331--2345},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3492148},
	doi = {10.1109/TMC.2024.3492148},
	timestamp = {Sun, 02 Nov 2025 21:28:49 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoMWHAMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising vehicular communication technology, Cellular Vehicle-to-Everything (C-V2X) is expected to ensure the safety and convenience of Intelligent Transportation Systems (ITS) by providing global road information. However, it is difficult to obtain global road information in practical scenarios since there will still be many vehicles on the road without onboard units (OBUs) in the near future. Specifically, although C-V2X vehicles have sensors that can perceive their surroundings and broadcast their perceived information to the C-V2X system, their line-of-sight (LoS) is limited and obscured by the environment, such as other vehicles and terrain. Besides, vehicles without OBUs cannot share their perceived information. These two problems cause extensive areas with unperceived information in the C-V2X system, and whether vehicles are in these areas is unknown. Thus, extending the perceivable range of the limited scenario for C-V2X applications that require global road information is necessary. To this end, this paper pioneers investigating the scenario inpainting task problem in C-V2X. To solve this challenging problem, we propose an effiCient trAffic Scenario inpainTing (CAST) solution consisting of a generative architecture and knowledge distillation, simultaneously considering the inpainting precision and computation efficiency. Extensive experiments have been conducted to demonstrate the effectiveness of CAST in terms of Precise Inpaint Rate (PIR), Rough Inpaint Rate (RIR), Lane-Level Inpaint Rate (LLIR), and Inpaint Confidence Error (ICE), paving the way for novel solutions for the inpainting problem in more complex road scenarios.}
}


@article{DBLP:journals/tmc/LiXLCFL25,
	author = {Han Li and
                  Ke Xiong and
                  Yuping Lu and
                  Wei Chen and
                  Pingyi Fan and
                  Khaled Ben Letaief},
	title = {Collaborative Task Offloading and Resource Allocation in Small-Cell
                  {MEC:} {A} Multi-Agent PPO-Based Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2346--2359},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3496536},
	doi = {10.1109/TMC.2024.3496536},
	timestamp = {Thu, 21 Aug 2025 14:21:18 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiXLCFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Small-cell mobile edge computing (SE-MEC) networks amalgamate the virtues of MEC and small-cell networks, enhancing data processing capabilities of user devices (UDs). Nevertheless, time-varying wireless channels, dynamic UD requirements, and severe interference among UDs make it difficult to fully exploit the limited network resources and stably provide computing services for UDs. Therefore, efficient task offloading and resource allocation (TORA) is essential. Moreover, since multiple small cells are deployed, decentralized TORA schemes are preferred in practice. Thus, this paper aims to design distributed adaptive TORA schemes for SE-MEC networks. In pursuit of an eco-friendly design, an optimization problem is formulated to minimize the total energy consumption (TEC) of UDs subject to delay constraints. To effectively deal with network's dynamic characteristics, the reinforce learning framework is applied, where the TEC minimization problem is first modeled as a partially observable Markov decision process (POMDP), and then an efficient multi-agent proximal policy optimization (MAPPO)-based scheme is presented to solve it. In the presented scheme, each small-cell base station (SBS) serves as an agent and is capable of making TORA decisions only with its own local information. To promote collaboration among multiple agents, a global reward function is designed. A state normalization mechanism is also introduced into the presented scheme for enhancing learning performance. Simulation results show that although the proposed MAPPO-based scheme works in a distributed manner, it achieves very similar performance to the centralized one. In addition, it is demonstrated that the state normalization mechanism has a significant effect on reducing TEC.}
}


@article{DBLP:journals/tmc/ChenQSW25,
	author = {Ning Chen and
                  Tie Qiu and
                  Weisheng Si and
                  Dapeng Oliver Wu},
	title = {DAiMo: Motif Density Enhances Topology Robustness for Highly Dynamic
                  Scale-Free IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2360--2375},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3492002},
	doi = {10.1109/TMC.2024.3492002},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenQSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Robust Topology is a key prerequisite to providing consistent connectivity for highly dynamic Internet-of-Things (IoT) applications that are suffering node failures. In this paper, we present a two-step approach to organizing the most robust IoT topology. First, we propose a novel robustness metric denoted as  I I , which is based on network motifs and is specifically designed to sensitively analyze the dynamic changes in topology resulting from node failures. Second, we introduce a Distributed duAl-layer collaborative competition optimization strategy based on Motif density (DAiMo). This strategy significantly expands the search space for optimal solutions and facilitates the identification of the optimal IoT topology. We utilize the motif density concept in the collaborative optimization process to efficiently search for the optimal topology. To support our approach, extensive mathematical proofs are provided to demonstrate the advantages of the metric  I I  in effectively perceiving changes in IoT topology and to establish the convergence of the DAiMo algorithm. Finally, we conduct comprehensive performance evaluations of DAiMo and investigate the influence of network motifs on the resilience and reliability of IoT topologies. Experimental results clearly indicate that the proposed method outperforms existing state-of-the-art topology optimization methods in terms of enhancing network robustness.}
}


@article{DBLP:journals/tmc/ChenLCC25,
	author = {Yuzhe Chen and
                  Yanjun Li and
                  Chung Shue Chen and
                  Kaikai Chi},
	title = {Exploring Long-Term Commensalism: Throughput Maximization for Symbiotic
                  Radio Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2376--2393},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3495015},
	doi = {10.1109/TMC.2024.3495015},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Symbiotic radio (SR), combining the advantages of cognitive radio and ambient backscatter communication (AmBC), stands as a promising solution for spectrum-and-energy-efficient wireless communications. In an SR network, backscatter devices (BDs) share the spectrum resources with the primary transmitter (PT) by utilizing the incident radio frequency (RF) signal from PT for uplink non-orthogonal multiple access (NOMA) transmission. The primary receiver (PR) decodes the signals of PT and BDs via the successive interference cancellation (SIC) technique. Our goal is to establish a long-term commensalistic relationship between PT and BDs. We address the problem of maximizing the long-term average sum rate of BDs while ensuring a minimum average rate for the PT by optimizing the power reflection coefficients of the BDs. We explicitly consider practical constraints such as the required power difference among signals for SIC decoding and the unknown future channel state information (CSI). We prove the NP-hardness of the offline version of the problem and subsequently employ the Lyapunov optimization technique to convert the original problem into a series of sub-problems in each individual time slot that can be solved in an online manner without relying on future CSI. We then utilize the successive convex optimization (SCO) technique to solve the non-convex sub-problems. Extensive simulations validate that our proposed Lyapunov-SCO algorithm achieves superior performance in terms of the average sum rate of BDs while ensuring PT’s required average rate. In addition, we provide discussions on extending the proposed solution to SR networks with multiple PT-PR pairs, high-mobility BDs, and enhancing fairness among BDs.}
}


@article{DBLP:journals/tmc/WangHYWC25,
	author = {Yuhang Wang and
                  Ying He and
                  F. Richard Yu and
                  Kaishun Wu and
                  Shanzhi Chen},
	title = {Intelligence-Based Reinforcement Learning for Dynamic Resource Optimization
                  in Edge Computing-Enabled Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2394--2406},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3506161},
	doi = {10.1109/TMC.2024.3506161},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangHYWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent transportation systems demand efficient resource allocation and task offloading to ensure low-latency, high-bandwidth vehicular services. The dynamic nature of vehicular environments, characterized by high mobility and extensive interactions among vehicles, necessitates considering time-varying statistical regularities, especially in scenarios with sharp variations. Despite the widespread use of traditional reinforcement learning for resource allocation, its limitations in generalization and interpretability are evident. To overcome these challenges, we propose an Intelligence-based Reinforcement Learning (IRL) algorithm. This algorithm utilizes active inference to infer the real world and maintain an internal model by minimizing free energy. Enhancing the efficiency of active inference, we incorporate prior knowledge as macro guidance, ensuring more accurate and efficient training. By constructing an intelligence-based model, we eliminate the need for designing reward functions, aligning better with human thinking, and providing a method to reflect the learning, information transmission and intelligence accumulation processes. This approach also allows for quantifying intelligence to a certain extent. Considering the dynamic and uncertain nature of vehicular scenarios, we apply the IRL algorithm to environments with constantly changing parameters. Extensive simulations confirm the effectiveness of IRL, significantly improving the generalization and interpretability of intelligent models in vehicular networks.}
}


@article{DBLP:journals/tmc/LiZNHLDA25,
	author = {Kai Li and
                  Jingjing Zheng and
                  Wei Ni and
                  Hailong Huang and
                  Pietro Li{\`{o}} and
                  Falko Dressler and
                  {\"{O}}zg{\"{u}}r B. Akan},
	title = {Biasing Federated Learning With a New Adversarial Graph Attention
                  Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2407--2421},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3499371},
	doi = {10.1109/TMC.2024.3499371},
	timestamp = {Tue, 19 Aug 2025 21:34:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZNHLDA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fairness in Federated Learning (FL) is imperative not only for the ethical utilization of technology but also for ensuring that models provide accurate, equitable, and beneficial outcomes across varied user demographics and equipment. This paper proposes a new adversarial architecture, referred to as Adversarial Graph Attention Network (AGAT), which deliberately instigates fairness attacks with an aim to bias the learning process across the FL. The proposed AGAT is developed to synthesize malicious, biasing model updates, where the minimum of Kullback-Leibler (KL) divergence between the user's model update and the global model is maximized. Due to a limited set of labeled input-output biasing data samples, a surrogate model is created, which presents the behavior of a complex malicious model update. Moreover, a graph autoencoder (GAE) is designed within the AGAT architecture, which is trained together with sub-gradient descent to reconstruct manipulatively the correlations of the model updates, and maximize the reconstruction loss while keeping the malicious, biasing model updates undetectable. The proposed AGAT attack is implemented in PyTorch, showing experimentally that AGAT successfully increases the minimum value of KL divergence of benign model updates by 60.9% and bypasses the detection of existing defense models. The source code of the AGAT attack is released on GitHub.}
}


@article{DBLP:journals/tmc/NajafiKMBC25,
	author = {Fatemeh Najafi and
                  Masoud Kaveh and
                  Mohammad Reza Mosavi and
                  Alessandro Brighente and
                  Mauro Conti},
	title = {{EPUF:} An Entropy-Derived Latency-Based {DRAM} Physical Unclonable
                  Function for Lightweight Authentication in Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2422--2436},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494612},
	doi = {10.1109/TMC.2024.3494612},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NajafiKMBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Physical Unclonable Functions (PUFs) are hardware-based mechanisms that exploit inherent manufacturing variations to generate unique identifiers for devices. Dynamic Random Access Memory (DRAM) has emerged as a promising medium for implementing PUFs, providing a cost-effective solution without the need for additional circuitry. This makes DRAM PUFs ideal for use in resource-constrained environments such as Internet of Things (IoT) networks. However, current DRAM PUF implementations often either disrupt host system functions or produce unreliable responses due to environmental sensitivity. In this paper, we present EPUF, a novel approach to extracting random and unique features from DRAM cells to generate reliable PUF responses. We leverage bitmap images of binary DRAM values and their entropy features to enhance the robustness of our PUF. Through extensive real-world experiments, we demonstrate that EPUF is approximately 1.7 times faster than existing solutions, achieves 100% reliability, produces features with 47.79% uniqueness, and supports a substantial set of Challenge-Response Pairs (CRPs). These capabilities make EPUF a powerful tool for DRAM PUF-based authentication. Based on EPUF, we then propose a lightweight authentication protocol that not only offers superior security features but also surpasses state-of-the-art authentication schemes in terms of communication overhead and computational efficiency.}
}


@article{DBLP:journals/tmc/LiuCLZ25,
	author = {Jingbo Liu and
                  Jiacheng Chen and
                  Zongxi Liu and
                  Haibo Zhou},
	title = {Enabling Feedback-Free {MIMO} Transmission for {FD-RAN:} {A} Data-Driven
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2437--2454},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3495719},
	doi = {10.1109/TMC.2024.3495719},
	timestamp = {Fri, 07 Mar 2025 18:30:59 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enhance flexibility and facilitate resource cooperation, a novel fully-decoupled radio access network (FD-RAN) architecture is proposed for 6G. However, the decoupling of uplink (UL) and downlink (DL) in FD-RAN makes the existing feedback mechanism ineffective. To this end, we propose an end-to-end data-driven MIMO solution without the conventional channel feedback procedure. Data-driven MIMO can alleviate the drawbacks of feedback including overheads and delay, and can provide customized precoding design for different BSs based on their historical channel data. It essentially learns a mapping from geolocation to MIMO transmission parameters. We first present a codebook-based approach, which selects transmission parameters from the statistics of discrete channel state information (CSI) values and utilizes nearest neighbor interpolation for spatial inference. We further present a non-codebook-based approach, which 1) derives the optimal precoder from the singular value decomposition (SVD) of the channel; 2) utilizes variational autoencoder (VAE) to select the representative precoder from the latent Gaussian representations; and 3) exploits Gaussian process regression (GPR) to predict unknown precoders in the space domain. Extensive simulations are performed on a link-level 5G simulator using realistic ray-tracing channel data. The results demonstrate the effectiveness of data-driven MIMO, showcasing its potential for application in FD-RAN and 6G.}
}


@article{DBLP:journals/tmc/FuYPCXR25,
	author = {Yongjian Fu and
                  Lanqing Yang and
                  Hao Pan and
                  Yi{-}Chao Chen and
                  Guangtao Xue and
                  Ju Ren},
	title = {MagSpy: Revealing User Privacy Leakage via Magnetometer on Mobile
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {3},
	pages = {2455--2469},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3495506},
	doi = {10.1109/TMC.2024.3495506},
	timestamp = {Fri, 21 Nov 2025 10:51:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FuYPCXR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various characteristics of mobile applications (apps) and associated in-app services can reveal potentially-sensitive user information; however, privacy concerns have prompted third-party apps to restrict access to data related to mobile app usage. This paper outlines a novel approach to extracting detailed app usage information by analyzing electromagnetic (EM) signals emitted from mobile devices during app-related tasks. The proposed system, MagSpy, recovers user privacy information from magnetometer readings that do not require access permissions. This EM leakage becomes complex when multiple apps are used simultaneously and is subject to interference from geomagnetic signals generated by device movement. To address these challenges, MagSpy employs multiple techniques to extract and identify signals related to app usage. Specifically, the geomagnetic offset signal is canceled using accelerometer and gyroscope sensor data, and a Cascade-LSTM algorithm is used to classify apps and in-app services. MagSpy also uses CWT-based peak detection and a Random Forest classifier to detect PIN inputs. A prototype system was evaluated on over 50 popular mobile apps with 30 devices. Extensive evaluation results demonstrate the efficacy of MagSpy in identifying in-app services (96% accuracy), apps (93.5% accuracy), and extracting PIN input information (96% top-3 accuracy).}
}


@article{DBLP:journals/tmc/WangYYLXGL25,
	author = {Lehao Wang and
                  Zhiwen Yu and
                  Haoyi Yu and
                  Sicong Liu and
                  Yaxiong Xie and
                  Bin Guo and
                  Yunxin Liu},
	title = {AdaEvo: Edge-Assisted Continuous and Timely {DNN} Model Evolution
                  for Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2485--2503},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2023.3316388},
	doi = {10.1109/TMC.2023.3316388},
	timestamp = {Fri, 16 May 2025 11:38:07 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYYLXGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile video applications today have attracted significant attention. Deep learning model (e.g., deep neural network, DNN) compression is widely used to enable on-device inference for facilitating robust and private mobile video applications. The compressed DNN, however, is vulnerable to the agnostic data drift of the live video captured from the dynamically changing mobile scenarios. To combat the data drift, mobile ends rely on edge servers to continuously evolve and re-compress the DNN with freshly collected data. We design a framework,  A d a E v o {\\sf AdaEvo} , that efficiently supports the resource-limited edge server handling mobile DNN evolution tasks from multiple mobile ends. The key goal of  A d a E v o {\\sf AdaEvo}  is to maximize the average quality of experience (QoE), i.e., the proportion of high-quality DNN service time to the entire life cycle, for all mobile ends. Specifically, it estimates the DNN accuracy drops at the mobile end without labels and performs a dedicated video frame sampling strategy to control the size of retraining data. In addition, it balances the limited computing and memory resources on the edge server and the competition between asynchronous tasks initiated by different mobile users. With an extensive evaluation of real-world videos from mobile scenarios and across four diverse mobile tasks, experimental results show that  A d a E v o {\\sf AdaEvo}  enables up to 34% accuracy improvement and 32% average QoE improvement.}
}


@article{DBLP:journals/tmc/ChengZYHZS25,
	author = {Zongrong Cheng and
                  Weiting Zhang and
                  Dong Yang and
                  Chuan Huang and
                  Hongke Zhang and
                  Xuemin Sherman Shen},
	title = {Intelligent End-to-End Deterministic Scheduling Across Converged Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2504--2518},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530486},
	doi = {10.1109/TMC.2025.3530486},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChengZYHZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deterministic network services play a vital role for supporting emerging real-time applications with bounded low latency, jitter, and high reliability. The deterministic guarantee is penetrated into various types of networks, such as 5G, WiFi, satellite, and edge computing networks. From the user’s perspective, the real-time applications require end-to-end deterministic guarantee across the converged network. In this paper, we investigate the end-to-end deterministic guarantee problem across the whole converged network, aiming to provide a scalable method for different kinds of converged networks to meet the bounded end-to-end latency, jitter, and high reliability demands of each flow, while improving the network scheduling QoS. Particularly, we set up the global end-to-end control plane to abstract the deterministic-related resources from converged network, and model the deterministic flow transmission by using the abstracted resources. With the resource abstraction, our model can work well for different underlying technologies. Given large amounts of abstracted resources in our model, it is difficult for traditional algorithms to fully utilize the resources. Thus, we propose a deep reinforcement learning based end-to-end deterministic-related resource scheduling (E2eDRS) algorithm to schedule the network resources from end to end. By setting the action groups, the E2eDRS can support varying network dimensions both in horizontal and vertical end-to-end deterministic-related network architectures. Experimental results show that E2eDRS can averagely increase 1.33x and 6.01x schedulable flow number for horizontal scheduling compared with MultiDRS and MultiNaive algorithms, respectively. The E2eDRS can also optimize 2.65x and 3.87x server load balance than MultiDRS and MultiNaive algorithms, respectively. For vertical scheduling, the E2eDRS can still perform better on schedulable flow number and server load balance.}
}


@article{DBLP:journals/tmc/LeeKOK25,
	author = {Junhyub Lee and
                  Insu Kim and
                  Sangeun Oh and
                  Hyosu Kim},
	title = {TaPIN: Reinforcing {PIN} Authentication on Smartphones With Tap Biometrics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2519--2533},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502902},
	doi = {10.1109/TMC.2024.3502902},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LeeKOK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {PIN authentication is the first line of defense for protecting private data on many smartphone applications, such as lock screens, messengers, and banking apps. However, existing PIN authentication systems have several constraints regarding security, usability, and robustness. To go beyond their limitations, this paper presents TaPIN, a reliable system that authenticates smartphone users with the collaborative use of PINs and tap biometrics. A user is first instructed to enter her PIN by tapping a smartphone screen for authentication. During the PIN entry, the user's fingertip collides with the screen, producing user-specific vibration and sound signals. TaPIN then senses the tap-induced signals and the collision properties, e.g., pressures and sizes, using the smartphone's built-in sensors and leverages them as biometric features. That is, it authenticates the user by verifying not only the entered PIN but also the collected features. Our experiments with 20 real-world users demonstrate that this two-factor authentication system is easy to use, more secure than existing methods, and deployable without dedicated hardware. For example, it accurately authenticates users with an average EER of 1.9% in stationary environments and maintains a reasonable level of security regardless of devices, tap styles, and noise.}
}


@article{DBLP:journals/tmc/XuYXFL25,
	author = {Jiahui Xu and
                  Yingbiao Yao and
                  Xin Xu and
                  Wei Feng and
                  Pei Li},
	title = {Joint Optimization of Task Offloading and Resource Allocation of Fog
                  Network by Considering Matching Externalities and Dynamics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2534--2550},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494793},
	doi = {10.1109/TMC.2024.3494793},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuYXFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to jointly optimize task offloading and resource allocation to minimize the task failure rate and task payments remains an unresolved challenge in fog networks. Focusing on this problem, this research formulates a novel task offloading and resource allocation model with two offloading modes and on-demand virtual resource units (VRUs). This model is decomposed into two sub-problems to solve: a joint task offloading and resource allocation optimization problem and a matching problem with externalities and dynamics. First, for a given terminal node (TN) and fog node (FN), this research theoretically derives the optimal offloading ratio and resource allocation strategy to minimize the payment of TNs for two offloading modes, i.e., immediate and queued offloading. Second, in the multi-TNs and multi-FNs scenario, the problem of making the task offloading decision is transformed into a many-to-one matching game by considering externalities and dynamics. Finally, a Deferred acceptance-based Loss ratio and Payment Minimized task Offloading and resource Allocation optimization (DLPMOA) algorithm is proposed to derive a stable and Pareto-optimal match. The simulation results show that the proposed DLPMOA has better performance in terms of task failure rate, task average payment, fog computing resource utilization, and fairness than the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/XieZHHQQ25,
	author = {Qi Xie and
                  Xiaobo Zhou and
                  Tianyu Hong and
                  Wenkai Hu and
                  Wenyu Qu and
                  Tie Qiu},
	title = {Towards Communication-Efficient Cooperative Perception via Planning-Oriented
                  Feature Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2551--2563},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3496856},
	doi = {10.1109/TMC.2024.3496856},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieZHHQQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous driving systems are fundamentally composed of sequential modular tasks, i.e., perception, prediction, and planning. For connected autonomous vehicles (CAVs), cooperative perception offers a promising solution to surpass their perception limitations, such as occlusion, by sharing sensing data with each other through wireless communication. Existing works typically prioritize sharing data from potential object-containing areas to maximize object detection accuracy under constrained communication resources. However, such detection-oriented approaches ignore a crucial fact that more accurate detection does not equal safer planning. Sharing large amounts of sensing data for detection accuracy can lead to communication resource wastage and performance degradation of subsequent driving tasks. To address this, we introduce Plan2comm, a communication-efficient cooperative perception framework via planning-oriented feature sharing, which shares only sensing data around planned trajectories to enable safer planning rather than mere detection accuracy. Specifically, a planning-oriented communication mechanism is designed to select and transmit the most valuable features from the perspective of the planning task. Moreover, an uncertainty-aware spatial-temporal feature fusion strategy is proposed to enhance high-quality information aggregation. Comprehensive experiments demonstrate that Plan2comm outperforms all other cooperative perception methods on motion prediction performance, and is more communication-efficient.}
}


@article{DBLP:journals/tmc/WangMXLZW25,
	author = {Chao Wang and
                  Xiao Ma and
                  Ruolin Xing and
                  Sisi Li and
                  Ao Zhou and
                  Shangguang Wang},
	title = {Delay- and Resource-Aware Satellite {UPF} Service Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2564--2579},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494043},
	doi = {10.1109/TMC.2024.3494043},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangMXLZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Executing 5G core network functions on satellites has become crucial to enhance satellite network management and service capabilities. The User Plane Function (UPF) is responsible for efficient data traffic forwarding and is envisioned as a key and pioneering core network function that will be deployed on satellites. However, managing and providing services with satellite UPFs face dual challenges. Limited satellite resources constrain the user scale that a satellite UPF can service, resulting in an unguaranteed service delay. Moreover, the extremely rapid mobility of satellites renders it difficult for satellite UPFs to provide seamless services. To address the above challenges, this paper presents the first-of-its-kind service optimization scheme for satellite UPFs in terms of switch control, state migration, and traffic routing. To provide guaranteed service delay, we provide a theoretical analysis based on the M/G/1 queue model, demonstrating the service delay-resource consumption trade-off. A satellite UPF switch control scheme is integrated into the service optimization process, which can decrease satellite UPF service delay while saving satellite resources by adjusting the switch control parameters. To provide seamless services, we propose a satellite UPF-oriented state-aware service migration and traffic routing (UPF service optimization) algorithm. A policy network-based reinforcement learning approach is employed to dynamically perceive the satellite network’s state as well as the satellite UPF switch state. Building upon the optimization of service delay through satellite UPF switch control, the processes of state-aware state migration and traffic routing are further employed to reduce delay, ensuring seamless service effectively. Experiments reveal that the proposed algorithm outperforms other benchmark algorithms under different metrics. The service delay is reduced by an average of 23.2% compared with other algorithms.}
}


@article{DBLP:journals/tmc/WeiGYXW25,
	author = {Xinliang Wei and
                  Xitong Gao and
                  Kejiang Ye and
                  Cheng{-}Zhong Xu and
                  Yu Wang},
	title = {A Quantum Reinforcement Learning Approach for Joint Resource Allocation
                  and Task Offloading in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2580--2593},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3496918},
	doi = {10.1109/TMC.2024.3496918},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiGYXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has revolutionized the way computational tasks are offloaded and latency is reduced by leveraging edge servers close to end devices. Efficient resource allocation and task offloading are crucial for enhancing system performance in MEC environments. Traditional reinforcement learning (RL) approaches have shown promise in optimizing resource allocation and task offloading problems. However, they often face challenges such as high computational complexity and the need for extensive training data. Quantum reinforcement learning (QRL) emerges as a promising solution to overcome these limitations by leveraging quantum computing principles to enhance efficiency and scalability. In this paper, we propose a hybrid quantum-classical non-sequential model for joint resource allocation and task offloading in MEC systems. Our model combines the advantages of RL in handling environmental dynamics and quantum computing in reducing adjustable parameters and accelerating the training process. Extensive experiments demonstrate that our proposed algorithm can achieve higher training and inference performance under various parameter settings compared to traditional RL models and previous QRL models.}
}


@article{DBLP:journals/tmc/QianCHBLZ25,
	author = {Hui Qian and
                  Hongmei Chai and
                  Ammar Hawbani and
                  Yuanguo Bi and
                  Na Lin and
                  Liang Zhao},
	title = {A Collaborative Error Detection and Correction Scheme for Safety Message
                  in {V2X}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2594--2611},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494713},
	doi = {10.1109/TMC.2024.3494713},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QianCHBLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle-to-Everything (V2X) technology plays a pivotal role in enabling real-time traffic coordination and safety, warning, and decision support. Within V2X, the Basic Safety Message (BSM) serves as the core to transmit critical vehicle status, location, and intention information to provide a foundation for ensuring reliable traffic safety and coordination mechanisms. Data accuracy stands as a key to the effectiveness and reliability of the V2X system, in which the transmission of error data can potentially result in severe traffic accidents. During vehicular operation, sensors may generate error data owing to looseness or external conditions. However, immediate sensor replacement is often impractical or infeasible. Therefore, this paper introduces a collaborative scheme involving vehicles, Road Side Units (RSUs), and Data Center (DC) to jointly enhance the accuracy of vehicle-transmitted BSMs. Our scheme involves analyzing statistical features of vehicle driving information to detect error BSMs. Subsequently, these detected errors are corrected by leveraging historical data from the vehicle and its relative relationship with surrounding vehicles. In addition, we propose a time optimization method to reduce the average processing time of each data by RSUs. The extensive experimental results demonstrate that the proposed scheme can accurately detect error BSMs and effectively correct error BSMs. The entire scheme also meets the requisite computational latency requirements.}
}


@article{DBLP:journals/tmc/SongLZFWHSC25,
	author = {Ruiyuan Song and
                  Zhi Lu and
                  Dongheng Zhang and
                  Liang Fang and
                  Zhi Wu and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {Unleashing the Potential of Self-Supervised {RF} Learning With Group
                  Shuffle},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2612--2627},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3497972},
	doi = {10.1109/TMC.2024.3497972},
	timestamp = {Thu, 20 Mar 2025 14:37:47 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SongLZFWHSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-supervised learning (SSL) is a powerful approach that learns general semantic representations from large-scale unlabeled data to make downstream tasks solve easier, offering significant potential in enhancing downstream performance and alleviating the appetite for large-scale annotated data. However, existing SSL techniques, predominantly designed for natural images, may be prone to shortcuts when applied to RF signals. This study presents surprising empirical findings showing that SSL can indeed learn meaningful RF representations by employing simple group shuffle (GS) and asymmetry augmentation techniques. The GS augmentation is inspired by blind calibration tasks in Time-Interleaved Analog-to-Digital Converters (TIADC). By treating the original RF signal as a composite output from sub-ADCs, GS augmentation enriches RF signals while preserving their global semantics. We also provide a theoretical validation of the GS augmentation’s singular value consistency. Notably, we observe that the shortcut is essentially a domain gap between the pre-trained and the downstream task models. This issue can be mitigated by an asymmetry augmentation technique, which maximizes the similarity between an original RF signal and its augmented version, rather than between two augmentations of the same RF signal. By integrating group shuffle and asymmetry augmentation (GSAA) into an existing contrastive learning framework, we develop an effective contrastive learning approach for RF signals. Our evaluations, spanning seven downstream RF sensing tasks across two general RF devices (WiFi and radar), strongly demonstrate that GSAA plays a significant role in advancing SSL-based solutions in RF sensing.}
}


@article{DBLP:journals/tmc/LiuOZDCW25,
	author = {Zhidan Liu and
                  Guofeng Ouyang and
                  Bolin Zhang and
                  Bo Du and
                  Chao Chen and
                  Kaishun Wu},
	title = {Joint Order Dispatching and Vehicle Repositioning for Dynamic Ridesharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2628--2643},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3493974},
	doi = {10.1109/TMC.2024.3493974},
	timestamp = {Tue, 01 Apr 2025 19:05:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuOZDCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic ridesharing has gained significant attention in recent years. However, existing ridesharing studies often focus on optimizing order dispatching and vehicle repositioning separately, leading to short-sighted decisions and underutilization of the ridesharing potential. In this paper, we propose a novel joint optimization framework called  J O D R \\mathtt {JODR} . By coordinating order dispatching and vehicle repositioning,  J O D R \\mathtt {JODR}  enhances ridesharing efficiency while ensuring high-quality service. The core idea of  J O D R \\mathtt {JODR}  is to dispatch ride orders with high demand in specific mobility directions to vehicles with sufficient available capacity, effectively balancing future supply and demand in those directions. To achieve this, we introduce a novel mobility value function that can predict the long-term mobility value of matching an order with its travel direction. By considering orders’ directional mobility values, service quality assessments, and available vehicle capacities,  J O D R \\mathtt {JODR}  formulates the order dispatching as a minimum-cost maximum-flow problem to derive the optimal order-vehicle assignments. Furthermore, the value function helps the intelligent repositioning of idle vehicles. Extensive experiments conducted on a large real-world dataset demonstrate the superiority of  J O D R \\mathtt {JODR}  over state-of-the-art methods across various performance metrics. These experimental results validate the effectiveness of  J O D R \\mathtt {JODR}  in improving the ridesharing efficiency and experience.}
}


@article{DBLP:journals/tmc/ZhouLZWCX25,
	author = {Jian Zhou and
                  Juewen Liang and
                  Lu Zhao and
                  Shaohua Wan and
                  Hui Cai and
                  Fu Xiao},
	title = {Latency-Energy Efficient Task Offloading in the Satellite Network-Assisted
                  Edge Computing via Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2644--2659},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502643},
	doi = {10.1109/TMC.2024.3502643},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLZWCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the demand for global computing coverage continues to surge, satellite edge computing emerges as a pivotal technology for the next generation of networks. Unlike ground-based edge computing, Low Earth Orbit (LEO) satellites face distinctive challenges, including high-speed mobility and resource limitations, etc. Therefore, effectively utilizing LEO satellites for global coverage services is crucial but challenging due to their dynamic coverage areas and diverse task requirements. To address these challenges, we introduce a novel dual-cloud edge collaborative task offloading architecture in the satellite network-assisted edge computing environment, namely, Satellite-Ground Task Offloading (SGTO). The architecture employs a Geostationary Earth Orbit (GEO) satellite and a ground cloud computing center as satellite cloud and ground cloud, respectively, and LEO satellites as edge nodes. We formally define the task offloading problem in the SGTO with the aim of minimizing the average latency and average energy consumption. We then propose an adaptive approach named SGTO-A from the perspective of satellites to adaptively solve the problem leveraging deep reinforcement learning. Specifically, we transform the task offloading problem into a Markov decision process and adopt the generalized proximal policy optimization (GePPO) algorithm to solve the problem. Finally, experimental results demonstrate that SGTO architecture and SGTO-A outperform the representative approaches in terms of average latency, average energy consumption and running time.}
}


@article{DBLP:journals/tmc/KimKHLJ25,
	author = {Juyeop Kim and
                  Soomin Kwon and
                  Ji Yoon Han and
                  Taegyeom Lee and
                  Ohyun Jo},
	title = {Design and Implementation of a Light-Weight Channel Vector Classifier
                  Based on Support Vector Machine for Real-Time 5G Beam Index Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2660--2672},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3494757},
	doi = {10.1109/TMC.2024.3494757},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KimKHLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning (ML) is recently considered a key technology for bringing outstanding performance to wireless communications. Conventional research has highlighted the potential of Support Vector Machines (SVMs), which train their model based on optimization theory, to enhance the performance of wireless communications. However, there are practical issues that makes SVM difficult to apply to a wireless communication system. SVM generally entails a heavy training process with high computational complexity, and the model requires a significant amount of time for training. Also, the entire dataset needs to be trained at once, requiring a substantial amount of memory for data storage. To enable SVM in wireless communications, we propose Real-Time Channel Vector Classifier (RTCVC), which employs a light-weight SVM model capable of training and processing incoming data in real-time. A novel input data pre-processing technique is implemented to reduce the computational overhead associated with calculating non-linear functions. The rearranged formulation of the original problem also allows each SVM sub-model to be trained distributively over time based on incremental parameters. For performance evaluation, we implement the RTCVC inter-operating with 5G beam index detection, whose detection probability has been theoretically proven to be significantly enhanced by SVM. The software modules of the RTCVC are based on LibSVM, a well-known open-source library for implementing SVM sub-models. The experimental results confirm that RTCVC significantly reduces training time while maintaining suitable performance for 5G beam index detection.}
}


@article{DBLP:journals/tmc/LiuCZCYT25,
	author = {Xiaoying Liu and
                  Anping Chen and
                  Kechen Zheng and
                  Kaikai Chi and
                  Bin Yang and
                  Tarik Taleb},
	title = {Distributed Computation Offloading for Energy Provision Minimization
                  in {WP-MEC} Networks With Multiple HAPs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2673--2689},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502004},
	doi = {10.1109/TMC.2024.3502004},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCZCYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a wireless powered mobile edge computing (WP-MEC) network with multiple hybrid access points (HAPs) in a dynamic environment, where wireless devices (WDs) harvest energy from radio frequency (RF) signals of HAPs, and then compute their computation data locally (i.e., local computing mode) or offload it to the chosen HAPs (i.e., edge computing mode). In order to pursue a green computing design, we formulate an optimization problem that minimizes the long-term energy provision of the WP-MEC network subject to the energy, computing delay and computation data demand constraints. The transmit power of HAPs, the duration of the wireless power transfer (WPT) phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing are jointly optimized adapting to the time-varying generated computation data and wireless channels of WDs. To efficiently address the formulated non-convex mixed integer programming (MIP) problem in a distributed manner, we propose a Two-stage Multi-Agent deep reinforcement learning-based Distributed computation Offloading (TMADO) framework, which consists of a high-level agent and multiple low-level agents. The high-level agent residing in all HAPs optimizes the transmit power of HAPs and the duration of the WPT phase, while each low-level agent residing in each WD optimizes its offloading decision, time allocation for offloading and CPU frequency for local computing. Simulation results show the superiority of the proposed TMADO framework in terms of the energy provision minimization.}
}


@article{DBLP:journals/tmc/ZhangHLWGC25,
	author = {Qinglong Zhang and
                  Rui Han and
                  Chi Harold Liu and
                  Guoren Wang and
                  Song Guo and
                  Lydia Y. Chen},
	title = {EdgeTA: Neuron-Grained Scaling of Foundation Models in Edge-Side Retraining},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2690--2707},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3504859},
	doi = {10.1109/TMC.2024.3504859},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHLWGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Foundation models (FMs) such as large language models are becoming the backbone technology for artificial intelligence systems. It is particularly challenging to deploy multiple FMs on edge devices, which not only have limited computational resources, but also encounter unseen input data from evolving domains or learning tasks. When new data arrives, existing prior art of FM mainly focuses on retraining compressed models of predetermined network architectures, limiting the feasibility of edge devices to efficiently achieve high accuracy for FMs. In this paper, we propose EdgeTA, a neuron-grained FM scaling system to maximize the overall accuracy of FMs promptly in response to their data dynamics. EdgeTA's key design features in scaling are (i) proxy mechanism, which adaptively transforms a FM into a compact architecture retaining the most important neurons to the input data, and (ii) neuron-grained scheduler, which jointly optimizes model sizes and resource allocation for all FMs on edge devices. Under tight retraining window and limited device resources, the design of EdgeTA can achieve most of the original FM's accuracy with much smaller retraining costs. We implement EdgeTA on FMs of natural language processing, computer vision and multimodal applications. Comparison results against state-of-the-art techniques show that our approach improves accuracy by 21.88% and reduces memory footprint and energy consumptions by 27.14% and 65.65%, while further achieving 15.96% overall accuracy improvement via neuron-grained scheduling.}
}


@article{DBLP:journals/tmc/ZhangZLGLL25,
	author = {Yuming Zhang and
                  Shengtong Zhu and
                  Yan Liu and
                  Lingfeng Guo and
                  Ji Li and
                  Jack Y. B. Lee},
	title = {Inter-Stream Adaptive Bitrate Streaming for Short-Video Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2708--2725},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3497954},
	doi = {10.1109/TMC.2024.3497954},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZLGLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Short-video services have seen explosive growth in recent years. Streaming over mobile networks is inherently challenging due to the latter's bandwidth fluctuations, motivating researchers to develop many sophisticated adaptive bitrate (ABR) algorithms to compensate. While ABR, together with prefetching, has been proposed for playlist streaming, its application to non-playlist streaming has received little attention. This work fills this gap by first exploring the efficacy of directly applying ABR to non-playlist streaming. Observing their limitations motivates the development of a new class of inter-stream bitrate adaptation (ISA) algorithms. Unlike ABR, ISA adapts bitrate on a per-video basis, which is not only simpler to implement and deploy but can even outperform ABR algorithms by up to 66.71% across a wide range of networks. Moreover, ISA and ABR are complementary such that they can be combined into Integrated Bitrate Adaptation (IBA) algorithms to raise performance gains further by up to 77.03%. In addition, this work develops a novel adaptive rebuffering duration (ARD) algorithm specifically designed for frame-based playback common in short-video services to further improve their performance under challenging network conditions. Together, ISA and ARD offer a new set of tools with progressive complexity-performance tradeoffs for enhancing the performance of short-video services.}
}


@article{DBLP:journals/tmc/WuLYZXX25,
	author = {Qingshun Wu and
                  Yafei Li and
                  Jinxing Yan and
                  Mei Zhang and
                  Jianliang Xu and
                  Mingliang Xu},
	title = {Adaptive Task Assignment in Spatial Crowdsourcing: {A} Human-in-The-Loop
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2726--2739},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3501734},
	doi = {10.1109/TMC.2024.3501734},
	timestamp = {Fri, 04 Jul 2025 22:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuLYZXX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, adaptive task assignment has been explored in spatial crowdsourcing. The challenge lies in how to adaptively partition the task stream to achieve the best utility for task assignment. A number of existing works have attempted to solve this challenge and achieve better performance by utilizing learning-based methods. Specifically, they mainly employ reinforcement learning to divide the task stream into a series of suitable batches and then perform task assignment in a batch fashion. Drawing inspiration from the effectiveness of human-machine collaborative decision-making, we aim to investigate human-in-the-loop methods to further enhance the performance of adaptive task assignment. In this paper, we propose a novel framework called Human-in-the-Loop Adaptive Partition (HLAP), which consists of two primary modules: Reinforcement Learning Partition Decision (RL-PD) and Human Supervision and Guidance (HSG). In the RL-PD module, we develop an RL agent, referred to as the decision-maker, by integrating the dual attention network into the Deep Q-Network (DQN) algorithm to capture cross-dimensional contextual information and long-range dependencies for a better understanding of the environment. In the HSG module, we design a human-in-the-loop mechanism to optimize the performance of the decision-maker, focusing on addressing two key issues: when and how humans interact with the decision-maker. Furthermore, to alleviate the heavy workload on humans, we construct a supervisor based on RL to oversee the decision-maker's partition process and adaptively determine when human intervention is necessary. We conduct extensive experiments on two real-world datasets, and the results demonstrate the efficiency and effectiveness of the HLAP framework.}
}


@article{DBLP:journals/tmc/HuYTW25,
	author = {Ruihan Hu and
                  Haochen Yuan and
                  Daimin Tan and
                  Zhongjie Wang},
	title = {Device Selection and Resource Allocation With Semi-Supervised Method
                  for Federated Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2740--2754},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3504271},
	doi = {10.1109/TMC.2024.3504271},
	timestamp = {Wed, 29 Oct 2025 17:44:09 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuYTW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of distributed learning and workflow orchestration, Federated Edge Learning has emerged as a solution, enabling multiple edge devices to collaboratively train a large model without the need for sharing raw data. Beyond considering bandwidth and computational resource limitations in the Internet of Things (IoT) environment, it is crucial to address the issue of IoT devices often collecting data that lacks timely annotations, which can lead to latency and label deficiency issues. In most Federated Edge Learning mechanisms, clients’ weights are selected for offloading to the server. In this paper, we propose a solution for dynamic edge selection and wireless network allocation under semi-supervised and privacy protection settings, termed Semi-supervised Scheduling and Allocation Optimization for Federated Edge Learning (SSAFL). SSAFL is designed to adapt to various scenarios, including channel state variations, device heterogeneity, resource incentives, deadline control, label deficiencies, and Non-IID data distributions. This adaptability is achieved through the utilization of an Incentive Optimization framework that encompasses bandwidth allocation and device scheduling policies. Within SSAFL, we introduce the concept of a weighted bipartite graph network to tackle the Incentive Optimization problem and achieve a balance in large-scale optimization of device selection. Additionally, to address the label deficiency issue, we devise a Dynamic Timer for deadline control for each client. Comprehensive and confidential results demonstrate that our proposed approach significantly outperforms other Federated Edge Learning baselines.}
}


@article{DBLP:journals/tmc/LuJMTCCW25,
	author = {Rongwei Lu and
                  Yutong Jiang and
                  Yinan Mao and
                  Chen Tang and
                  Bin Chen and
                  Laizhong Cui and
                  Zhi Wang},
	title = {Data-Aware Gradient Compression for {FL} in Communication-Constrained
                  Mobile Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2755--2768},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3504284},
	doi = {10.1109/TMC.2024.3504284},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuJMTCCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) in mobile environments faces significant communication bottlenecks. Gradient compression has proven as an effective solution to this issue, offering substantial benefits in environments with limited bandwidth and metered data. Yet, it encounters severe performance drops in non-IID environments due to a one-size-fits-all compression approach, which does not account for the varying data volumes across workers. Assigning varying compression ratios to workers with distinct data distributions and volumes is therefore a promising solution. This work derives the convergence rate of distributed SGD with non-uniform compression, which reveals the intricate relationship between model convergence and the compression ratios applied to individual workers. Accordingly, we frame the relative compression ratio assignment as an  n n -variable chi-squared nonlinear optimization problem, constrained by a limited communication budget. We propose DAGC-R, which assigns conservative compression to workers handling larger data volumes. Recognizing the computational limitations of mobile devices, we propose the DAGC-A, which is computationally less demanding and enhances the robustness of compression in non-IID scenarios. Our experiments confirm that the DAGC-R and DAGC-A can speed up the training speed by up to 25.43% and 16.65% compared to the uniform compression respectively, when dealing with highly imbalanced data volume distribution and restricted communication.}
}


@article{DBLP:journals/tmc/WangWYXMZG25,
	author = {Liang Wang and
                  Yaru Wang and
                  Zhiwen Yu and
                  Fei Xiong and
                  Lianbo Ma and
                  Huan Zhou and
                  Bin Guo},
	title = {Similarity Caching in Dynamic Cooperative Edge Networks: An Adversarial
                  Bandit Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2769--2782},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3500132},
	doi = {10.1109/TMC.2024.3500132},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWYXMZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unlike traditional edge caching paradigms, similarity edge caching enables the retrieval of similar content from local caches to fulfill user requests, reducing reliance on remote data centers and improving system performance. Although several pioneering works have contributed to similarity edge caching, most focus on single-edge nodes and/or static environment settings, which are impractical for real-world applications. To address this gap, we investigate the similarity caching problem in dynamic cooperative edge networks, where a set of edge nodes cooperatively serve requests generated from arbitrary distributions with similar content over fluctuating transmission links. This presents a significant challenge, as it requires balancing content similarity with delivery latency over the transmission network and learning the environment in real-time to optimize caching policies. We frame this problem within an adversarial Multi-Armed Bandit framework to accommodate the continuously changing operational environment. To solve this, we propose an online learning-based approach named MABSCP, which dynamically updates caching policies based on real-time feedback to minimize the service cost of edge caching networks. To enhance implementation efficiency, we devise both an offline compact strategy construction method and an online Gibbs sampling method. Finally, trace-driven simulation results demonstrate that our proposed approach outperforms several existing methods in terms of system performance.}
}


@article{DBLP:journals/tmc/ChenZZWSC25,
	author = {Jinbo Chen and
                  Dongheng Zhang and
                  Ganlin Zhang and
                  Haoyu Wang and
                  Qibin Sun and
                  Yan Chen},
	title = {Co-Sense: Exploiting Cooperative Dark Pixels in Radio Sensing for
                  Non-Stationary Target},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2783--2799},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3498048},
	doi = {10.1109/TMC.2024.3498048},
	timestamp = {Mon, 02 Feb 2026 13:11:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZZWSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio sensing has emerged as a promising solution for monitoring vital signs in a contactless manner. However, most of the existing designs focus on stationary target and struggle with body motion interference. While some efforts have been made to address this issue, the lack of a physical explanation for the motion elimination principle makes them work as a blind signal separation way and thus leaves the body motion elimination problem still as an open challenge. In this paper, we reveal for the first time the existence of “dark pixels”–specific points on the same rigid body parts that share the same body movement but exhibit varying physiological motions, with these variations still preserving the physiological rhythm. By exploiting the inherent relationship between the dark pixels, we propose a cooperative sensing framework, Co-Sense, that can achieve robust radio sensing for non-stationary targets in an explainable way. Through extensive experiments, Co-Sense demonstrates its superiority over existing methods, achieving effective motion cancellation and breath sensing with a median absolute respiratory rate (RR) error of 0.36 respiration per minute (RPM) and breath wave correlation of 0.61 under non-stationary scenarios. The results indicate the great potential of Co-Sense in enhancing the accuracy of vital sign sensing with radio signals, especially in real-world environments where targets are rarely stationary.}
}


@article{DBLP:journals/tmc/ChenYXXW25,
	author = {Weilin Chen and
                  Wei Yang and
                  Mingjun Xiao and
                  Lide Xue and
                  Shaowei Wang},
	title = {{LBDT:} {A} Lightweight Blockchain-Based Data Trading Scheme in Internet
                  of Vehicles Using Proof-of-Reputation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2800--2816},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3497934},
	doi = {10.1109/TMC.2024.3497934},
	timestamp = {Sun, 15 Jun 2025 20:53:25 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYXXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The exponential growth of data in the Internet of Vehicles (IoV) has created opportunities to improve traffic safety and efficiency through data trading. However, establishing trust among highly mobile and resource-constrained vehicles poses significant challenges for effective data trading in IoV. To address this issue, we propose a lightweight blockchain-based data trading scheme (LBDT), which ensures secure and efficient data trading in IoV. We introduce a proof-of-reputation (PoR) consensus mechanism to establish trustworthiness for data trading. Specifically, we use a progressive reputation mechainism to support the PoR consensus. LBDT utilizes a parallel-chain structure for the PoR consensus to minimize communication and storage costs while reducing transaction confirmation latency. Additionally, we adopt a double auction mechanism as an incentivizing strategy to encourage vehicle participation in data trading. We evaluate the performance of LBDT through extensive experiments. The experimental results demonstrate that LBDT is highly effective and secure, achieving a transaction latency of approximately 4 seconds. Moreover, LBDT successfully mitigates communication and storage overheads by over 90%, thus establishing its superiority over state-of-the-art solutions under comparable conditions.}
}


@article{DBLP:journals/tmc/GuWZ25,
	author = {Yixun Gu and
                  Jie Wang and
                  Shengjie Zhao},
	title = {{HT-FL:} Hybrid Training Federated Learning for Heterogeneous Edge-Based
                  IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2817--2831},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502686},
	doi = {10.1109/TMC.2024.3502686},
	timestamp = {Tue, 19 Aug 2025 21:34:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous rolling-out of edge computing, Federated Learning (FL) has become a promising solution for intelligent Internet-of-things (IoT). In addition to resource constraints, deploying FL schemes in IoT networks is greatly challenged by heterogeneity in multiple dimensions. While heterogeneity in data distribution and computation capability has been extensively studied, the impact of distinct, even hybrid training paradigms on FL performances remains largely unknown. To answer this open question in the IoT context, we propose a Hybrid-Training Federated Learning (HT-FL) algorithm for the power-constrained IoT networks, incorporating both sequential and parallel training that naturally adapts to various sub-network topologies, while greatly reducing the energy consumption during the training stage. We demonstrate through analysis that the convergence of HT-FL is theoretically guaranteed, achieving  O ( 1 K √ ) O (\\frac{1}{\\sqrt{K}})  for carefully chosen learning rates. Experiments on multiple datasets show that, the proposed HT-FL outperforms existing FL schemes on multiple training tasks under various data distribution settings, while reducing an average of 20% energy consumption. In a more practical sense, a self-adaptive parameter-tuning strategy is also designed for HT-FL deployment, which can be easily extended to other multi-layer FL schemes in complex application scenarios.}
}


@article{DBLP:journals/tmc/WangZLFZX25,
	author = {Chenxing Wang and
                  Fang Zhao and
                  Haiyong Luo and
                  Yuchen Fang and
                  Haichao Zhang and
                  Haoyu Xiong},
	title = {Towards Effective Transportation Mode-Aware Trajectory Recovery: Heterogeneity,
                  Personalization and Efficiency},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2832--2846},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3501280},
	doi = {10.1109/TMC.2024.3501280},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZLFZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We focus on the transportation-aware trajectory recovery problem, which is distinct from the conventional vehicle-based trajectory recovery, facing three major challenges: heterogeneity, personalization and efficiency. For the heterogeneity, the velocity of the mobile object is intrinsically correlated with the specific transportation mode, containing inherent heterogeneity. For the personalization, the trajectory data is complicated by substantial variations in users, which are different in personalized behaviors. For the efficiency, previous works mostly employ sequence-to-sequence framework which limits their efficiency due to the auto-regressive inference pattern. To address these challenges, we design a novel efficient and effective multi-modal deep model, coined as PTrajRec, for transportation-aware trajectory recovery. Specifically, we initially embed location, behavior, and transportation mode modalities in distinct channels, which not only reflect spatio-temporal information encapsulated in location sequences but also introduce the heterogeneity and personalization characteristics associated with mode and behavior sequences. For further modeling these modalities, we employ the auto-correlation mechanism to learn periodic dependencies on the temporal dimension and the graph attention mechanism to learn road network dependencies on the spatial dimension. At last, we propose a dual-view constraint mechanism to assist the efficient trajectory recovery framework and design three auxiliary tasks to address the inherent heterogeneity and efficiency design. Extensive experimental results on two real-world datasets demonstrate the superiority of our proposed method compared to state-of-the-art baselines with reduced computation cost and excellent performance.}
}


@article{DBLP:journals/tmc/ZhouGQW25,
	author = {Xiaobo Zhou and
                  Shuxin Ge and
                  Tie Qiu and
                  Xingwei Wang},
	title = {Preference-Aware Vehicle Repositioning Recommendation for MoD Systems:
                  {A} Coulomb Force Directed Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2847--2860},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502235},
	doi = {10.1109/TMC.2024.3502235},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouGQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle repositioning is widely used in Mobility on-Demand (MoD) systems to address supply-demand imbalances and improve order completion rates. Existing methods typically offer repositioning recommendations focused on enhancing vehicle coordination toward supply-demand re-balance. However, these methods often overlook the possibility that drivers may not follow these recommendations due to their personal preferences, leading to recommendation-decision inconsistency and further disrupting the supply-demand balance. To address this issue, we propose a preference-aware vehicle repositioning recommendation strategy for MoD systems, named FREE, which is based on a Coulomb Force directed approach. The core idea is to strike a balance between vehicle coordination and consistency between recommendations and driver decisions. First, we introduce a Coulomb force-based representation (CFR) to model coordination among vehicles. In this model, the interactions between vehicles and orders are represented as forces that drive the repositioning of vehicles. Next, we develop a driver preference learning model that accurately captures drivers’ preferences using triplet and consistency loss. We then integrate these preferences with the CFR into a multi-agent deep reinforcement learning (MADRL) based repositioning algorithm to generate optimal recommendations. Finally, we validate the effectiveness of FREE through simulations using real-world data, demonstrating its superiority over existing benchmarks.}
}


@article{DBLP:journals/tmc/LiZMYFXW25,
	author = {Yanan Li and
                  Penghong Zhao and
                  Xiao Ma and
                  Haitao Yuan and
                  Zhe Fu and
                  Mengwei Xu and
                  Shangguang Wang},
	title = {A Collaborative Cloud-Edge Approach for Robust Edge Workload Forecasting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2861--2875},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502683},
	doi = {10.1109/TMC.2024.3502683},
	timestamp = {Tue, 18 Nov 2025 13:49:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZMYFXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of edge computing in the post-COVID19 pandemic period, precise workload forecasting is considered the basis for making full use of the edge-limited resources, and both edge service providers (ESPs) and edge service consumers (ESCs) can benefit significantly from it. Existing paradigms of workload forecasting (i.e., edge-only or cloud-only) are improper, due to failing to consider the inter-site correlations and might suffer from significant data transmission delays. With the increasing adoption of edge platforms by web services, it is critical to balance both accuracy and efficiency in workload forecasting. In this paper, we propose XELASTIC, which offers three key improvements over the conference version. First, we redesigned the aggregation and disaggregation layers using GCNs to capture more complex relationships among workload series. Second, we introduced a supervised contrastive loss to enhance robustness against outliers, particularly for handling missing or abnormal data in real-world scenarios. Finally, we expanded the evaluation with additional baselines and larger datasets. Extensive experiments on realistic edge workload datasets collected from China’s largest edge service provider (Alibaba ENS) show that XELASTIC outperforms state-of-the-art methods, decreases time consumption, and reduces communication costs.}
}


@article{DBLP:journals/tmc/JiaoZDM25,
	author = {Wanguo Jiao and
                  Chang Sheng Zhang and
                  Wei Du and
                  Shuai Ma},
	title = {WiSDA: Subdomain Adaptation Human Activity Recognition Method Using
                  Wi-Fi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2876--2888},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3501299},
	doi = {10.1109/TMC.2024.3501299},
	timestamp = {Mon, 12 Jan 2026 19:08:43 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiaoZDM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition based on Wi-Fi signals has become one part of integrated sensing and communications, which has promising application prospects. Detecting activities across different domains is an important and challenging problem. To reduce model complexity and improve recognition accuracy, we propose a novel approach to realize activity recognition across domains, named WiSDA. The proposed WiSDA contains two parts: data augmentation and a deep learning model. The recursive plots method is employed as the data augmentation to transform Wi-Fi channel state information into images, which can take advantage of the image recognition ability of the latter deep learning model. The proposed learning model utilizes weighted cosine similarity to align feature distributions among sub-domains activated by a deep network layer across different domains, thereby a domain-independent feature representation is generated. Based on this representation, WiSDA can make the recognition decision independent of domains, then the cross-domain recognition accuracy is increased. The numerical results illustrate that WiSDA achieves higher recognition accuracy and has lower complexity. The cross-domain recognition accuracy ranges from 89% to 93% with offline pre-training. Enhancing the pre-trained WiSDA with limited samples boosts cross-domain recognition accuracy to 97%.}
}


@article{DBLP:journals/tmc/LiD25,
	author = {Hongbo Li and
                  Lingjie Duan},
	title = {To Optimize Human-in-the-Loop Learning in Repeated Routing Games},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2889--2899},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502076},
	doi = {10.1109/TMC.2024.3502076},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today navigation applications (e.g., Waze and Google Maps) enable human users to learn and share the latest traffic observations, yet such information sharing simply aids selfish users to predict and choose the shortest paths to jam each other. Prior routing game studies focus on myopic users in oversimplified one-shot scenarios to regulate selfish routing via information hiding or pricing mechanisms. For practical human-in-the-loop learning (HILL) in repeated routing games, we face non-myopic users of differential past observations and need new mechanisms (preferably non-monetary) to persuade users to adhere to the optimal path recommendations. We model the repeated routing game in a typical parallel transportation network, which generally contains one deterministic path and  N N  stochastic paths. We first prove that no matter under the information sharing mechanism in use or the latest routing literature’s hiding mechanism, the resultant price of anarchy (PoA) for measuring the efficiency loss from social optimum can approach infinity, telling arbitrarily poor exploration-exploitation tradeoff over time. Then we propose a novel user-differential probabilistic recommendation (UPR) mechanism to differentiate and randomize path recommendations for users with differential learning histories. We prove that our UPR mechanism ensures interim individual rationality for all users and significantly reduces  PoA = ∞ \\text{PoA}=\\infty  to close-to-optimal  PoA = 1 + 1 4 N + 3 \\text{PoA}=1+\\frac{1}{4N+3} , which cannot be further reduced by any other non-monetary mechanism. In addition to theoretical analysis, we conduct extensive experiments using real-world datasets to generalize our routing graphs and validate the close-to-optimal performance of UPR mechanism.}
}


@article{DBLP:journals/tmc/XiaHPWWC25,
	author = {Ming Xia and
                  Min Huang and
                  Qiuqi Pan and
                  Yunhan Wang and
                  Xiaoyan Wang and
                  Kaikai Chi},
	title = {Visualizing the Smart Environment in {AR:} An Approach Based on Visual
                  Geometry Matching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2900--2916},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3504960},
	doi = {10.1109/TMC.2024.3504960},
	timestamp = {Sun, 01 Feb 2026 13:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaHPWWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article presents Insight, an AR system for visualizing the IoT-enabled smart environment without relying on the unique appearances, barcodes, world coordinates, or wireless signals of IoT infrastructures. The system analyzes the camera video and motion data taken by mobile AR equipment to extract the self and cross visual geometries describing the poses and geographic distribution of nearby IoT devices. To recognize IoT devices using the extracted geometries, Insight operates in two phases. At deployment time, it learns pairwise mappings from the visual geometries to the corresponding device identities. After that, it leverages the geometries scanned at run time to look for a partial assignment to the recorded geometries, allowing it to automatically recognize the IoT devices in AR view. As such, our system turns the IoT device recognition task into a geometry matching problem, which is further formalized as to perform Subset, Incomplete, and Duplicated Point Cloud Registration (SID-PCR) in this work. We design a deep neural network paying specific edge- and spectral-wise graph attention to solve SID-PCR, and implement a prototype that adaptively requests visual geometry scan and registration operations for accurate recognition. The performance of Insight is validated using both synthetic data and a real-world testbed.}
}


@article{DBLP:journals/tmc/DengZZLYM25,
	author = {Kaikai Deng and
                  Dong Zhao and
                  Wenxin Zheng and
                  Yue Ling and
                  Kangwen Yin and
                  Huadong Ma},
	title = {G\({}^{\mbox{3}}\)R: Generating Rich and Fine-Grained mmWave Radar
                  Data From 2D Videos for Generalized Gesture Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2917--2934},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502668},
	doi = {10.1109/TMC.2024.3502668},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengZZLYM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter wave radar is gaining traction recently as a promising modality for enabling pervasive and privacy-preserving gesture recognition. However, the lack of rich and fine-grained radar datasets hinders progress in developing generalized deep learning models for gesture recognition across various user postures (e.g., standing, sitting), positions, and scenes. To remedy this, we resort to designing a software pipeline that exploits wealthy 2D videos to generate realistic radar data, but it needs to address the challenge of simulating diversified and fine-grained reflection properties of user gestures. To this end, we design G3R with three key components: i) a gesture reflection point generator expands the arm's skeleton points to form human reflection points; ii) a signal simulation model simulates the multipath reflection and attenuation of radar signals to output the human intensity map; iii) an encoder-decoder model combines a sampling module and a fitting module to address the differences in number and distribution of points between generated and real-world radar data for generating realistic radar data. We implement and evaluate G3R using 2D videos from public data sources and self-collected real-world radar data, demonstrating its superiority over other state-of-the-art approaches for gesture recognition.}
}


@article{DBLP:journals/tmc/WangQCGHS25,
	author = {Suhong Wang and
                  Tuanfa Qin and
                  Tingting Chen and
                  Wenhao Guo and
                  Yongle Hu and
                  Hongmin Sun},
	title = {A High-Reliability Small-Area Task Offloading Mechanism With Trust
                  Evaluation and Fuzzy Logic in Power IoTs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2935--2948},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502167},
	doi = {10.1109/TMC.2024.3502167},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangQCGHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to solve the problem that high-priority tasks can not be processed timely and reliably due to the disorder of multi-task and dynamicity in Power Internet of Things(PIoTs), a high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic(HRSATF) is proposed. First, considering task priority, preemptive priority queue is introduced to ensure high-priority tasks processed preferentially, and minimum resource allocation coefficients(MRACs) of tasks are solved to ensure the effectiveness of offloading. Second, the trust model between smart device(SD) and edge server(ES) is established, and ESs are divided into three priorities based on trust value and computing power by fast non-dominated sorting. Thirdly, fuzzy logic is applied to select target ES when the priorities of task and ES do not match or the ES is offline, and MRAC is used to schedule tasks between SD and ES. Finally, NSGA2 is modified (MNSGA2) to verify the effectiveness of HRSATF in terms of success rate, time, power consumption and load balancing, where success rate is increased by  102.3 % 102.3\\% , and time and power consumption are decreased by  90.7 % 90.7\\% ,  89.3 % 89.3\\%  at most, respectively.}
}


@article{DBLP:journals/tmc/YanCZL25,
	author = {Jialiang Yan and
                  Siyao Cheng and
                  Yang Zhao and
                  Jie Liu},
	title = {TapWristband: {A} Wearable Keypad System Based on Wrist Vibration
                  Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2949--2966},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3503417},
	doi = {10.1109/TMC.2024.3503417},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanCZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fine-grained human motion detection has become increasingly important with the growing popularity of human computer interaction (HCI). However, traditional gesture-based HCI systems often require the design of new operation modes rather than conforming to user habits, thus increasing system learning costs. In this paper, we present TapWristband, a novel wearable sensor-based vibration sensing system that detects finger tapping by measuring wrist vibrations. We first perform real-world experiments to collect measurements for modeling the effects of the tapping motion on wearable wristband sensors including piezoelectric transducer (PZT) and inertial measurement unit (IMU). We find that a damped vibration model can be used to represent the relaxing phase of a vibration response due to tapping motion. Thus, we propose a mutual cross-correlation-based event segmentation algorithm to extract the vibration signal during the relaxing phase. After that, we develop feature extraction and classification algorithms to recognize the tapping patterns of five fingers across twelve key locations of a keypad system. Finally, we performed extensive experiments with thirteen participants to evaluate our system. Experimental results show that our low-cost vibration sensing system can achieve an average accuracy of over 93% with a tapping speed of over 100 taps per minute in real-world tapping scenarios.}
}


@article{DBLP:journals/tmc/GuoGZW25,
	author = {Siyang Guo and
                  Yaming Guo and
                  Hui Zhang and
                  Junbo Wang},
	title = {Mitigating Update Conflict in Non-IID Federated Learning via Orthogonal
                  Class Gradients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2967--2978},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3503682},
	doi = {10.1109/TMC.2024.3503682},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoGZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasingly popular federated learning still faces the practical challenge of non-independent and identically distributed data. Most efforts to address this issue focus on limiting local updates or enhancing model aggregation. However, these methods either restrict the learning capacity of local models or overlook the negative knowledge transfer caused by local objective divergences. In contrast, we observe that the global update can be re-expressed as a weighted sum of the gradients of samples from different classes. Therefore, we hypothesize that the competition among local updates may arise from the conflict between the gradients of samples belonging to different classes. Inspired by this insight, we introduce the novel perspective of orthogonal class gradients, aimed at eliminating interference between updates from different classes without the aforementioned drawbacks. To this end, this paper presents FedOCF, which implements orthogonal class gradient constraints by encouraging orthogonality among features of different classes. Specifically, FedOCF maintains a generator to learn features that are orthogonal for different classes and utilizes it to regularize features learned in local learning. Theoretically, we also demonstrate that FedOCF can improve generalization performance through feature conditional distribution alignment during local learning. Extensive experiments validate the excellent performance of FedOCF in various heterogeneous scenarios.}
}


@article{DBLP:journals/tmc/ChenCSZLY25,
	author = {Xiangchun Chen and
                  Jiannong Cao and
                  Yuvraj Sahni and
                  Mingjin Zhang and
                  Zhixuan Liang and
                  Lei Yang},
	title = {Mobility-Aware Dependent Task Offloading in Edge Computing: {A} Digital
                  Twin-Assisted Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2979--2994},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3506221},
	doi = {10.1109/TMC.2024.3506221},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenCSZLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute tasks from end devices. Task offloading is a fundamental problem in CEC that decides when and where tasks are executed upon the arrival of tasks. However, the mobility of users often results in unstable connections, leading to network failures and resource underutilization. Existing works have not adequately addressed joint mobility-aware dependent task offloading and network flow scheduling, resulting in network congestion and suboptimal performance. To address this, we formulate an online joint mobility-aware dependent task offloading and bandwidth allocation problem, to improve the quality of service by reducing task completion time and energy consumption. We introduce a Mobility-aware Digital Twin-assisted Deep Reinforcement Learning (MDT-DRL) algorithm. Our digital twin model equips the reinforcement learning process by providing future states of mobile users, enabling efficient offloading plans for adapting to the mobile CEC system. Experimental results on real-world and synthetic datasets show that MDT-DRL surpasses state-of-the-art baselines on average task completion time and energy consumption.}
}


@article{DBLP:journals/tmc/WangTSZSDG25,
	author = {Qubeijian Wang and
                  Shiyue Tang and
                  Wen Sun and
                  Yin Zhang and
                  Geng Sun and
                  Hong{-}Ning Dai and
                  Mohsen Guizani},
	title = {Smart Shield: Prevent Aerial Eavesdropping via Cooperative Intelligent
                  Jamming Based on Multi-Agent Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {2995--3011},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3505206},
	doi = {10.1109/TMC.2024.3505206},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangTSZSDG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The spotlight on autonomous aerial vehicles (AAVs) is to enhance wireless communications while ignoring the potential risk of AAVs acting as adversaries. Due to their mobility and flexibility, AAV eavesdroppers pose an immeasurable threat to legitimate wireless transmissions. However, the existing fixed jamming scheme without cooperation cannot counter the flexible and dynamic AAV eavesdropping. In this article, a cooperative intelligent jamming scheme is proposed, authorizing ground jammers (GJs) to interfere with AAV eavesdroppers, generating specific jamming shields between AAV eavesdroppers and legitimate users. Toward this end, we formulate a secrecy capacity maximization problem and model the problem as a decentralized partially observable Markov decision process (Dec-POMDP). To address the challenge of the huge state space and action space with network dynamics, we leverage a deep reinforcement learning (DRL) algorithm with a dueling network and double-Q learning (i.e., dueling double deep Q-network) to train policy networks. Then, we propose a multi-agent mixing network framework (QMIX)-based collaborative jamming algorithm to enable GJs to independently make decisions without sharing local information. Additionally, we perform extensive simulations to validate the superiority of our proposed scheme and present useful insights into practical implementation by elucidating the relationship between the deployment settings of GJs and the instantaneous secrecy capacity.}
}


@article{DBLP:journals/tmc/NoahS25,
	author = {Yoav Noah and
                  Nir Shlezinger},
	title = {Distributed Learn-to-Optimize: Limited Communications Optimization
                  Over Networks via Deep Unfolded Distributed {ADMM}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3012--3024},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502574},
	doi = {10.1109/TMC.2024.3502574},
	timestamp = {Thu, 25 Dec 2025 12:46:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NoahS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed optimization is a fundamental framework for collaborative inference over networks. The operation is modeled as the joint minimization of a shared objective which typically depends on local observations. Distributed optimization algorithms, such as the distributed alternating direction method of multipliers (D-ADMM), iteratively combine local computations and message exchanges. A main challenge associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications to reach consensus. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging data to tune the hyperparameters of each iteration. These hyperparameters can either be agent-specific, aiming at achieving the best performance within a fixed number of iterations over a given network, or shared among the agents, allowing to learn to distributedly optimize over different networks. We specialize unfolded D-ADMM for two representative settings: a distributed sparse recovery setup, and a distributed machine learning learning scenario. Our numerical results demonstrate that the proposed approach dramatically reduces the number of communications utilized by D-ADMM, without compromising on its performance.}
}


@article{DBLP:journals/tmc/WangGNNYZW25,
	author = {Ye Wang and
                  Hui Gao and
                  Edith C. H. Ngai and
                  Kun Niu and
                  Tan Yang and
                  Bo Zhang and
                  Wendong Wang},
	title = {A Coverage-Aware High-Quality Sensing Data Collection Method in Mobile
                  Crowd Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3025--3040},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502158},
	doi = {10.1109/TMC.2024.3502158},
	timestamp = {Tue, 12 Aug 2025 12:01:03 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangGNNYZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we leverage unmanned aerial vehicles (UAVs) to enhance mobile crowd sensing (MCS) by addressing two critical challenges: uncontrollable data quality and inevitable unsensed points of interest (PoIs). We introduce a UAV-assisted method to deal with these challenges. To ensure the accuracy of sensing data contributed by human participants, the proposed truth discovery method utilizes UAV-collected sensing data as few-shot samples to train the truth discovery model, which is then employed to calibrate sensing data solely collected by human participants. Additionally, to meet the sensing coverage requirement, we present a method that predicts data values for unsensed PoIs by utilizing their historical sensing data and the sensed neighboring PoIs information. The method employs a graph neural network to capture spatio-temporal relationships of the sensing data, facilitating accurate estimation of unsensed PoIs. Through extensive simulations, our approaches demonstrate superior performance compared to existing methods, showcasing the potential of UAV-assisted MCS for overcoming challenges and enhancing data collection efficiency in various domains.}
}


@article{DBLP:journals/tmc/ZhangSLWWNL25,
	author = {Chuang Zhang and
                  Geng Sun and
                  Jiahui Li and
                  Qingqing Wu and
                  Jiacheng Wang and
                  Dusit Niyato and
                  Yuanwei Liu},
	title = {Multi-Objective Aerial Collaborative Secure Communication Optimization
                  via Generative Diffusion Model-Enabled Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3041--3058},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3502685},
	doi = {10.1109/TMC.2024.3502685},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangSLWWNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to flexibility and low-cost, unmanned aerial vehicles (UAVs) are increasingly crucial for enhancing coverage and functionality of wireless networks. However, incorporating UAVs into next-generation wireless communication systems poses significant challenges, particularly in sustaining high-rate and long-range secure communications against eavesdropping attacks. In this work, we consider a UAV swarm-enabled secure surveillance network system, where a UAV swarm forms a virtual antenna array to transmit sensitive surveillance data to a remote base station (RBS) via collaborative beamforming (CB) so as to resist mobile eavesdroppers. Specifically, we formulate an aerial secure communication and energy efficiency multi-objective optimization problem (ASCEE-MOP) to maximize the secrecy rate of the system and to minimize the flight energy consumption of the UAV swarm. To address the non-convex, NP-hard and dynamic ASCEE-MOP, we propose a generative diffusion model-enabled twin delayed deep deterministic policy gradient (GDMTD3) method. Specifically, GDMTD3 leverages an innovative application of diffusion models to determine optimal excitation current weights and position decisions of UAVs. The diffusion models can better capture the complex dynamics and the trade-off of the ASCEE-MOP, thereby yielding promising solutions. Simulation results highlight the superior performance of the proposed approach compared with traditional deployment strategies and some other deep reinforcement learning (DRL) benchmarks. Moreover, performance analysis under various parameter settings of GDMTD3 and different numbers of UAVs verifies the robustness of the proposed approach.}
}


@article{DBLP:journals/tmc/LiCLC25,
	author = {Gang Li and
                  Jun Cai and
                  Jianfeng Lu and
                  Hongming Chen},
	title = {Incentive Mechanism Design for Cross-Device Federated Learning: {A}
                  Reinforcement Auction Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3059--3075},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3508260},
	doi = {10.1109/TMC.2024.3508260},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiCLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the operational context of a cross-device federated learning (FL), the efficient allocation of resources, such as transmission powers, channels, and computation resources, significantly impacts overall performance. Existing research in cross-device FL has predominantly concentrated on either resource allocation to enhance training accuracy or incentivizing participation, while ignoring their integrated designs for further improving the performance in cross-device FL. Different from existing work, in this paper, we jointly integrate the power allocation, channel assignment, user selection, and allocation of computation frequency into the design of incentive mechanism, where each mobile user plays a dual role as both a buyer and a seller. Because of complex resource allocation, truthfulness guarantee in a dual role scenario, and unavailable prior information, the considered mechanism design problem is challenging. To tackle such combinatorial problem, we propose a Reinforcement Auction Mechanism (RAM), comprising two layers. The upper layer features a Hybrid Action Reinforcement Learning scheme to learn the outcomes of user selection and payments. In the lower layer, each selected mobile user optimizes its resources to maximize its utility. Theoretical analyses affirm that our proposed RAM ensures individual rationality and truthfulness. Extensive simulations have been conducted to validate the effectiveness of the proposed RAM.}
}


@article{DBLP:journals/tmc/HuCFMQY25,
	author = {Xinyi Hu and
                  Zihan Chen and
                  Chenyuan Feng and
                  Geyong Min and
                  Tony Q. S. Quek and
                  Howard H. Yang},
	title = {Sparsified Random Partial Model Update for Personalized Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3076--3091},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3507286},
	doi = {10.1109/TMC.2024.3507286},
	timestamp = {Sun, 07 Dec 2025 22:17:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuCFMQY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) stands as a privacy-preserving machine learning paradigm that enables collaborative training of a global model across multiple clients. However, the practical implementation of FL models often confronts challenges arising from data heterogeneity and limited communication resources. To address the aforementioned issues simultaneously, we develop a Sparsified Random Partial Update framework for personalized Federated Learning (SRP-pFed), which builds upon the foundation of dynamic partial model updates. Specifically, we decouple the local model into personal and shared parts to achieve personalization. For each client, the ratio of its personal part associated with the local model, referred to as the update rate, is regularly renewed over the training procedure via a random walk process endowed with reinforced memory. In each global iteration, clients are clustered into different groups where the ones in the same group share a common update rate. Benefiting from such design, SRP-pFed realizes model personalization while substantially reducing communication costs in the uplink transmissions. We conduct extensive experiments on various training tasks with diverse heterogeneous data settings. The results demonstrate that the SRP-pFed consistently outperforms the state-of-the-art methods in test accuracy and communication efficiency.}
}


@article{DBLP:journals/tmc/DaiSYWN25,
	author = {Miao Dai and
                  Gang Sun and
                  Hongfang Yu and
                  Sheng Wang and
                  Dusit Niyato},
	title = {User Association and Channel Allocation in 5G Mobile Asymmetric Multi-Band
                  Heterogeneous Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3092--3109},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3503632},
	doi = {10.1109/TMC.2024.3503632},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiSYWN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of mobile terminals, the continuous upgrading of services, 4G LTE networks are showing signs of weakness. To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks. The distinct propagation characteristics of mmWaves, microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective. Therefore, to narrowing the gap between theory, practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink, downlink of pico base stations, asymmetric service requirements, user communication continuity. We formulate the problem as integer nonlinear programming, prove its intractability. Thereby, we decouple it into three subproblems: user association, switch point selection, subchannel allocation, design an algorithm based on optimal matching, spectral clustering to solve it efficiently. The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, number of satisfied users.}
}


@article{DBLP:journals/tmc/XuMSB25,
	author = {Zimu Xu and
                  Antonio Di Maio and
                  Eric Samikwa and
                  Torsten Braun},
	title = {{CSTAR-FL:} Stochastic Client Selection for Tree All-Reduce Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3110--3129},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3507381},
	doi = {10.1109/TMC.2024.3507381},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuMSB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is widely applied in privacy-sensitive domains, such as healthcare, finance, and education, due to its privacy-preserving properties. However, implementing FL in dynamic wireless networks poses substantial communication challenges. Central to these challenges is the need for efficient communication strategies that can adapt to fluctuating network conditions and the growing number of participating devices, which can lead to unacceptable communication delays. In this article, we propose Stochastic Client Selection for Tree All-Reduce Federated Learning (CSTAR-FL), a novel approach that combines a probabilistic User Device (UD) selection strategy with a tree-based communication architecture to enhance communication efficiency in FL within densely populated wireless networks. By optimizing UD selection for effective model aggregation and employing an efficient data transmission structure, CSTAR-FL significantly reduces communication time and improves FL efficiency. Additionally, our approach ensures high global model accuracy under scenarios where data distribution is heterogeneous from User Device (UD)s. Extensive simulations in dynamic wireless network scenarios demonstrate that CSTAR-FL outperforms existing state-of-the-art methods, reducing model convergence time by up to 40% without losing the global model accuracy. This makes CSTAR-FL a robust solution for efficient and scalable FL deployments in high-density environments.}
}


@article{DBLP:journals/tmc/SunSWHLPNYL25,
	author = {Zemin Sun and
                  Geng Sun and
                  Qingqing Wu and
                  Long He and
                  Shuang Liang and
                  Hongyang Pan and
                  Dusit Niyato and
                  Chau Yuen and
                  Victor C. M. Leung},
	title = {{TJCCT:} {A} Two-Timescale Approach for UAV-Assisted Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3130--3147},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3505155},
	doi = {10.1109/TMC.2024.3505155},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunSWHLPNYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply discrepancy between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem. In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long timescale, we propose a convex optimization-based method for UAV trajectory control. Besides, we theoretically prove the stability and polynomial complexity of TJCCT. Extensive simulation results demonstrate that the proposed TJCCT is able to achieve superior performances in terms of the system utility, average processing rate, average completion delay, average completion ratio, and average cost, while meeting the energy constraints despite the trade-off of the increased energy consumption.}
}


@article{DBLP:journals/tmc/LiWLJY25,
	author = {Tianyu Li and
                  Hanling Wang and
                  Qing Li and
                  Yong Jiang and
                  Zhenhui Yuan},
	title = {CL-Shield: {A} Continuous Learning System for Protecting User Privacy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3148--3162},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3504721},
	doi = {10.1109/TMC.2024.3504721},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWLJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The video analytics system utilizes deep learning models (DNN) to perform inference on the videos captured by cameras. Continuous learning algorithms are used to address the data drift problem in video analytics systems. However, uploading images from deployment environments and processing on the cloud carry the risk of privacy leakage. In this paper, we have designed a system called CL-Shield to protect user’s privacy. First, we review the causes of privacy leakage in a continuous learning system and propose the objective of full privacy protection. Second, we design an online training mechanism based on a scene library to avoid direct uploading of user’s frames to the cloud server. Lastly, we design a fast training set search algorithm based on a novel Ebv-List, which effectively improves the speed of model updates. We collect various real-world scenario data to build our scene library and validate our system on a dataset of over 10 hours. The experiments demonstrate that our privacy-aware continuous learning system achieves an F1-score of over 92% compared to the conventional systems without protecting privacy and has long-term stability in analytic F1-score.}
}


@article{DBLP:journals/tmc/LiZCZ25,
	author = {Zhaohui Li and
                  Yongmin Zhang and
                  Lin Cai and
                  Yaoxue Zhang},
	title = {HearLoc: Locating Unknown Sound Sources in 3D With a Small-Sized Microphone
                  Array},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3163--3177},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3507035},
	doi = {10.1109/TMC.2024.3507035},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor Sound Source Localization (ISSL) is under growing focus with the rapid development of smart IOT intelligence. The predominant approaches typically involve constructing large microphone (Mic) array systems or extracting multiple angles of arrival (AOAs). However, the performance of these solutions is often constrained by the physical size of the array. Besides, there has been limited focus on 3D localization with a single small-sized Mic array. In this paper, we propose HearLoc, an ISSL system that can directly locate 3D sources with a ten- c m cm  Mic array. We demonstrate that the localization ability and dimensional capability can be significantly enhanced by incorporating the time differences of arrival (TDOAs) between the line-of-sight (LOS) and ECHO signals from nearby reflective surfaces. Our approach involves a localization method that selectively sums the correlation powers at useful TDOAs induced by each location. We also design a data processing pipeline with interpolation, normalization and pruning techniques to improve system accuracy and efficiency. To further enhance scalability, we design an iterative algorithm for the ISSL problem with multiple sources and an array location calibration scheme. Experiments demonstrate that the HearLoc can effectively locate sound sources, exhibiting  2 × 2\\times / 3.7 × 3.7\\times  improvements in accuracy for 2D and 3D localization, respectively, and a  4 × 4\\times  increase in efficiency compared to the existing AOA-based ISSL solutions.}
}


@article{DBLP:journals/tmc/WangSYAZHCC25,
	author = {Genglin Wang and
                  Zheng Shi and
                  Yanni Yang and
                  Zhenlin An and
                  Guoming Zhang and
                  Pengfei Hu and
                  Xiuzhen Cheng and
                  Jiannong Cao},
	title = {Wireless Eavesdropping on Wired Audio With Radio-Frequency Retroreflector
                  Attack},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3178--3195},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3505268},
	doi = {10.1109/TMC.2024.3505268},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSYAZHCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies have demonstrated the feasibility of eavesdropping on audio via radio frequency signals or videos, which capture physical surface vibrations from surrounding objects. However, these methods are inadequate for intercepting internally transmitted audio through wired media. In this work, we introduce radio-frequency retroreflector attack (RFRA) and bridge this gap by proposing an RFRA-based eavesdropping system, RF-Parrot 2 {}^{\\mathbf {2}} , capable of wirelessly capturing audio signals transmitted through earphone wires. Our system entails embedding a tiny field-effect transistor within the wire to establish a battery-free retroreflector, whose reflective efficiency is correlated with the amplitude of the audio signal. To preserve the details of audio signals, we designed a unique retroreflector using a depletion-mode MOSFET (D-MOSFET). This MOSFET can be triggered by any voltage level present in the audio signals, thus guaranteeing no information loss during activation. However, the D-MOSFET introduces a nonlinear convolution operation on the original audio, resulting in distorted audio eavesdropping. Thus, we devised an engineering solution which utilized a novel convolutional neural network in conjunction with an efficient Parallel WaveGAN vocoder to reconstruct the original audio. Our comprehensive experiments demonstrate a strong similarity between the reconstructed audio and the original, achieving an impressive 95% accuracy in speech command recognition.}
}


@article{DBLP:journals/tmc/DongZLDZS25,
	author = {Xuewen Dong and
                  Shuangrui Zhao and
                  Ximeng Liu and
                  Zijie Di and
                  Yuzhen Zhang and
                  Yulong Shen},
	title = {Joint Trajectory Planning and Task Offloading for {MIMO} AAV-Aided
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3196--3210},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3510272},
	doi = {10.1109/TMC.2024.3510272},
	timestamp = {Sun, 17 Aug 2025 16:45:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DongZLDZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing is conducive to reducing service response time and improving service quality by pushing cloud functions to a network's edges. Most existing works in edge computing focus on utility maximization of task offloading on static edges with a single antenna. Besides, trajectory planning of mobile edges, e.g., autonomous aerial vehicles (AAVs) is also rarely discussed. In this paper, we are the first to jointly discuss the deadline-ware task offloading and AAV trajectory planning problem in a multi-input multi-output (MIMO) AAV-aided mobile edge computing system. Due to discrete variables and highly coupling nonconvex constraints, we equivalently convert the original problem into a more solvable form by introducing auxiliary variables. Next, a penalty dual decomposition-based algorithm is developed to achieve a global optimal solution to the problem. Besides, we proposed a profit-based fireworks algorithm in a relatively lower time to reduce the execution time for large-scale networks. Extensive evaluation results reveal that our proposed optimal algorithms could significantly outperform static offloading algorithms and other algorithms by 25% on average.}
}


@article{DBLP:journals/tmc/SunKZ25,
	author = {Yuchang Sun and
                  Marios Kountouris and
                  Jun Zhang},
	title = {How to Collaborate: Towards Maximizing the Generalization Performance
                  in Cross-Silo Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3211--3222},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509852},
	doi = {10.1109/TMC.2024.3509852},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model's generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved by collaborating with other clients that have more training data and similar data distributions. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, which does not need to fix in advance the number of groups. We further analyze the convergence of HCCT for general non-convex loss functions which unveils the effect of data similarity among clients. Extensive simulations show that HCCT achieves better generalization performance than baseline schemes, whereas it degenerates to independent training and conventional FL in specific scenarios.}
}


@article{DBLP:journals/tmc/TsampaziDPBPHAM25,
	author = {Maria Tsampazi and
                  Salvatore D'Oro and
                  Michele Polese and
                  Leonardo Bonati and
                  Gwenael Poitau and
                  Michael Healy and
                  Mohammad Alavirad and
                  Tommaso Melodia},
	title = {PandORA: Automated Design and Comprehensive Evaluation of Deep Reinforcement
                  Learning Agents for Open {RAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3223--3240},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3505781},
	doi = {10.1109/TMC.2024.3505781},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TsampaziDPBPHAM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The highly heterogeneous ecosystem of Next Generation (NextG) wireless communication systems calls for novel networking paradigms where functionalities and operations can be dynamically and optimally reconfigured in real time to adapt to changing traffic conditions and satisfy stringent and diverse Quality of Service (QoS) demands. Open Radio Access Network (RAN) technologies, and specifically those being standardized by the O-RAN Alliance, make it possible to integrate network intelligence into the once monolithic RAN via intelligent applications, namely, xApps and rApps. These applications enable flexible control of the network resources and functionalities, network management, and orchestration through data-driven intelligent control loops. Recent work has showed how Deep Reinforcement Learning (DRL) is effective in dynamically controlling O-RAN systems. However, how to design these solutions in a way that manages heterogeneous optimization goals and prevents unfair resource allocation is still an open challenge, with the logic within DRL agents often considered as a opaque system. In this paper, we introduce PandORA, a framework to automatically design and train DRL agents for Open RAN applications, package them as xApps and evaluate them in the Colosseum wireless network emulator. We benchmark 23 xApps that embed DRL agents trained using different architectures, reward design, action spaces, and decision-making timescales, and with the ability to hierarchically control different network parameters. We test these agents on the Colosseum testbed under diverse traffic and channel conditions, in static and mobile setups. Our experimental results indicate how suitable fine-tuning of the RAN control timers, as well as proper selection of reward designs and DRL architectures can boost network performance according to the network conditions and demand. Notably, finer decision-making granularities can improve Massive Machine-Type Communications (mMTC)’s performance by $\\sim\\! 56\\%$ and even increase Enhanced Mobile Broadband (eMBB) Throughput by $\\sim\\! 99\\%$.}
}


@article{DBLP:journals/tmc/XuLZTZRJZ25,
	author = {Yang Xu and
                  Hangfan Li and
                  Cheng Zhang and
                  Zhiqing Tang and
                  Xiaoxiong Zhong and
                  Ju Ren and
                  Hongbo Jiang and
                  Yaoxue Zhang},
	title = {Blockchain-Enabled Multiple Sensitive Task-Offloading Mechanism for
                  {MEC} Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3241--3255},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3507153},
	doi = {10.1109/TMC.2024.3507153},
	timestamp = {Sun, 14 Sep 2025 11:27:36 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLZTZRJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As mobile devices proliferate and mobile applications diversify, Mobile Edge Computing (MEC) has become widely adopted to efficiently allocate computing resources at the network edge and alleviate network congestion. In the MEC initial phase, the absence of vital information presents challenges in devising task-offloading policies, and identifying malicious devices responsible for providing inaccurate feedback is complex. To fill in such gaps, we introduce a consortium blockchain-enabled Committee Voting based Task Offloading Model (CVTOM) to collaboratively formulate resource allocation policies and establish deterrence against malicious servers producing erroneous results intentionally. Different voting principle mechanisms of each committee member are first designed in a Blockchain-enabled system which helps to represent the system's resource status. Additionally, we propose a Multi-armed Bandits related Thompson Sampling based Adaptive Preference Optimization (TSAPO) algorithm for task-offloading policy, enhancing the timely identification of potent edge servers to improve computing resource utilization which first considers dynamic edge server space and parallel computing scenarios. The solid proof process greatly contributes to the theoretical analysis of the TSAPO. The simulation experiments demonstrate the delay and budget can be reduced by around 25% and 10% respectively, showcasing the superior performance of our approach.}
}


@article{DBLP:journals/tmc/XuYZJZWXL25,
	author = {Daliang Xu and
                  Wangsong Yin and
                  Hao Zhang and
                  Xin Jin and
                  Ying Zhang and
                  Shiyun Wei and
                  Mengwei Xu and
                  Xuanzhe Liu},
	title = {EdgeLLM: Fast On-Device {LLM} Inference With Speculative Decoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3256--3273},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3513457},
	doi = {10.1109/TMC.2024.3513457},
	timestamp = {Tue, 18 Nov 2025 13:49:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuYZJZWXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device's memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3× faster than existing engines.}
}


@article{DBLP:journals/tmc/YangCSSHP25,
	author = {Wenjun Yang and
                  Lin Cai and
                  Shengjie Shu and
                  Amir Sepahi and
                  Zhiming Huang and
                  Jianping Pan},
	title = {QoS-Driven Contextual {MAB} for {MPQUIC} Supporting Video Streaming
                  in Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3274--3287},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3507051},
	doi = {10.1109/TMC.2024.3507051},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangCSSHP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video streaming performance may degrade substantially in a mobile environment due to fast-changing wireless links. On the other hand, to provide ubiquitous services, heterogeneous static and mobile access and backbone networks will be integrated in the sixth-generation (6G) systems, so mobile users can take advantage of multiple access options for better services. Multi-path transport-layer protocols like Multi-Path QUIC (MPQUIC) show promise in utilizing multiple access links to address the impact of mobility. However, the optimal link selection that aims to provide statistical QoS guarantee for video streaming in a mobile environment with both user mobility and network mobility remains an open issue. In this paper, based on a lightweight Multi-Armed Bandit (MAB) technique, we develop a QoS-driven Contextual MAB (QC-MAB) framework for MPQUIC, which makes an intelligent access network selection and adaptively enables FEC coding to trade off delay, reliability and goodput. Extensive simulation results with ns-3 show that the proposed QC-MAB framework can outperform the state-of-the-art solutions. It achieves up to ten times lower video interruption ratio and three times higher goodput in highly dynamic mobile environments.}
}


@article{DBLP:journals/tmc/PaiMS25,
	author = {Vinay U. Pai and
                  Neelesh B. Mehta and
                  Chandramani Singh},
	title = {Novel Insights From a Cross-Layer Analysis of {TCP} and {UDP} Traffic
                  Over Full-Duplex WLANs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3288--3301},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3510099},
	doi = {10.1109/TMC.2024.3510099},
	timestamp = {Tue, 01 Apr 2025 19:05:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PaiMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Full-duplex (FD) communication is a promising new technology that enables simultaneous transmission and reception in wireless local area networks (WLANs). The benefits of FD on the medium access control (MAC) layer throughput in IEEE 802.11 WLANs are well-documented. However, cross-layer interactions between the FD MAC protocol and transport layer protocols such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) are less explored. We consider a WLAN with uplink and downlink TCP flows as well as UDP flows between stations (STAs) and a server via an FD access point (AP). We study an STA-initiated FD MAC protocol in which the AP can transmit on the downlink while receiving on the uplink. Using a novel FD-specific STA saturation approximation, Markov renewal theory, and fixed-point analysis, we derive novel expressions for the uplink and downlink TCP and UDP saturation throughputs. Our analysis shows that the AP is no longer a bottleneck and may be unsaturated unlike in conventional half-duplex (HD) WLANs. Despite greater contention and cross-link interference between STAs, FD achieves a higher TCP throughput than HD. FD causes a significant degradation in the UDP throughput. In the unsaturated regime, FD achieves a lower average downlink TCP packet delay than HD.}
}


@article{DBLP:journals/tmc/GongWZZCB25,
	author = {Houxin Gong and
                  Haishuai Wang and
                  Peng Zhang and
                  Sheng Zhou and
                  Hongyang Chen and
                  Jiajun Bu},
	title = {FedMTPP: Federated Multivariate Temporal Point Processes for Distributed
                  Event Sequence Forecasting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3302--3315},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509915},
	doi = {10.1109/TMC.2024.3509915},
	timestamp = {Mon, 06 Oct 2025 14:28:11 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongWZZCB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of mobile network technology and wearable mobile devices, user-scenario interactions generate a large amount of user behavioral data in the form of multivariate event sequences. Due to data isolation, these multi-scenario events need to be jointly trained to achieve better prediction results. However, traditional federated learning methods face significant challenges when handling distributed event sequences. And the effectiveness of existing modeling approaches for event sequences in federated contexts has not been thoroughly explored. To this end, we propose Federated Multivariate Temporal Point Processes (FedMTPP), which enables learning from distributed event sequences within a novel federated learning framework and leverages efficient event modeling technology, MTPP, to forecast future events. Specifically, FedMTPP restores the temporal structure of the original event sequence by rearranging event embeddings and redesigns the autoregressive-based hidden representation computation in traditional MTPP, making it more suitable for federated prediction tasks. Additionally, FedMTPP incorporates advanced encryption techniques to effectively safeguard user privacy and security. Experimental results on both synthetic and real datasets demonstrate that FedMTPP substantially improves the performance of local models and achieves results comparable to state-of-the-art centralized MTPP methods.}
}


@article{DBLP:journals/tmc/HanYZWBCZ25,
	author = {Rui Han and
                  Yue Yu and
                  Qingzhe Zeng and
                  Jiaxing Wang and
                  Lin Bai and
                  Jinho Choi and
                  Wei Zhang},
	title = {Offloading Game for Mobile Edge Computing With Random Access in IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3316--3329},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514204},
	doi = {10.1109/TMC.2024.3514204},
	timestamp = {Mon, 11 Aug 2025 14:01:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanYZWBCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Things (IoT), numerous devices and sensors are deployed to collect data sets. Although some IoT devices can process data locally, most devices may have limited power and computational capability. Since mobile edge computing (MEC) is a new paradigm to provide strong computing capability at the edge of networks close to users, these devices can offload their tasks to MEC servers. Therefore, designing an efficient computation offloading strategy to decide whether the tasks to be offloaded to MEC servers becomes crucial. In this paper, we study the computation offloading for IoT devices based on a non-cooperative game with one-shot random access, where users’ offloading decisions can be made independently to realize distributed offloading. In particular, we discuss the offloading game with and without sharing information among devices and find the Nash equilibrium (NE). Besides, we analyze the effective bandwidth as a performance metric from a device perspective, which considering the Quality of Service (QoS) of network layer while analyzing users’ offloading strategies. Simulation results show the effectiveness of proposed strategies and the impact of offloading tasks to users’ strategies in time-varying channel based on effective bandwidth.}
}


@article{DBLP:journals/tmc/HeYQHZ25,
	author = {Yuanyi He and
                  Peng Yang and
                  Tian Qin and
                  Jiawei Hou and
                  Ning Zhang},
	title = {Joint Encoding and Enhancement for Low-Light Video Analytics in Mobile
                  Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3330--3345},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514214},
	doi = {10.1109/TMC.2024.3514214},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeYQHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present our design and analysis of a Joint Encoding and Enhancement (JEE) system for low-light video analytics in mobile edge networks. First, it is observed that, relying solely on a single pipeline for encoding and enhancement of mobile videos proves insufficient, because of the fluctuations in end-edge bandwidth and computing resources. Therefore, two distinct pipelines are introduced in the JEE system, namely, the encode-decode-enhance pipeline and the enhance-encode-decode pipeline. We then characterize the relationship of accuracy, transmission overhead, and computing overhead of these two pipelines through extensive experiments. Considering the significant demands of transmission and computing for low-light videos, we formulate an optimization problem to strike a balance between accuracy and delay, where the available end-edge bandwidth and computing resources are unknown in advance. To solve this mixed-integer nonlinear programming problem, we propose an algorithm based on online gradient descent, enabling adaptive pipeline selection and joint encoding and enhancement configuration. Theoretical analysis indicates that the proposed algorithm achieves sub-linear dynamic regret, highlighting its capability to the accuracy improvement and delay reduction in online environments. Experimental comparison against baselines demonstrates that, JEE can achieve up to a 27.32% increase in accuracy and a 26.18% reduction in delay.}
}


@article{DBLP:journals/tmc/LinXYT25,
	author = {Yuhan Lin and
                  Haoran Xu and
                  Zhimeng Yin and
                  Guang Tan},
	title = {Edge Assisted Low-Latency Cooperative {BEV} Perception With Progressive
                  State Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3346--3358},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509716},
	doi = {10.1109/TMC.2024.3509716},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinXYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern intelligent vehicles (IVs) are equipped with a variety of sensors and communication modules, empowering Advanced Driver Assistance Systems (ADAS) and enabling inter-vehicle connectivity. This paper focuses on multi-vehicle cooperative perception, with a primary objective of achieving low latency. The task involves nearby cooperative vehicles sending their camera data to an edge server, which then merges the local views to create a global traffic view. While multi-camera perception has been actively researched, existing solutions often rely on deep learning models, resulting in excessive processing latency. In contrast, we propose leveraging the state estimation technique from the robotics field for this task. We explicitly model and solve for the system state, addressing additional challenges brought by object mobility and vision obstruction. Furthermore, we introduce a progressive state estimation pipeline to further accelerate system state notifications, supported by a motion prediction method that optimizes position accuracy and perception smoothness. Experimental results demonstrate the superiority of our approach over the deep learning method, with 12.0 × to 27.4 × reductions in server processing delay, while maintaining mean absolute errors below 1 m.}
}


@article{DBLP:journals/tmc/BaiXWHC25,
	author = {Lin Bai and
                  Jinpeng Xu and
                  Jiaxing Wang and
                  Rui Han and
                  Jinho Choi},
	title = {Efficient Hybrid Transmission for Cell-Free Systems via {NOMA} and
                  Multiuser Diversity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3359--3371},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514165},
	doi = {10.1109/TMC.2024.3514165},
	timestamp = {Mon, 11 Aug 2025 14:01:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaiXWHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell-free technology is considered a pivotal advancement for next-generation mobile communications, which can effectively enhance the quality of service for user equipments (UEs) located at the cell edge. For cell-free systems, in this paper, we propose a hybrid downlink transmission method that combines non-orthogonal multiple access (NOMA) and multiuser diversity (MUD). To evaluate the communication performance of the system, we derive closed-form expressions for both instantaneous and average sum rates of UEs using the NOMA and MUD transmission methods. Furthermore, we comprehensively investigate the spectrum efficiency of the NOMA and MUD transmission methods to provide a basis for selecting the hybrid transmission strategy. On the basis of the proposed hybrid transmission strategy, we can derive an optimal hybrid transmission strategy for the scenarios with two access points (APs) and two UEs. Particularly, we extend the aforementioned strategy to the scenarios with multiple UEs, and formulate an optimization problem to maximize the system spectrum efficiency subject to the transmission strategy and power allocation. Furthermore, we propose a low-complexity user selection strategy and power allocation algorithm to solve the problem. Numerical results demonstrate that the hybrid transmission method and power allocation strategy can achieve higher system spectrum efficiency. Our results reveal the influence of key parameters on the downlink spectrum efficiency, analytically and numerically.}
}


@article{DBLP:journals/tmc/GaoXFLYCY25,
	author = {Qinghang Gao and
                  Jianmao Xiao and
                  Zhiyong Feng and
                  Jingyu Li and
                  Yang Yu and
                  Hongqi Chen and
                  Qiaoyun Yin},
	title = {Optimization of Models and Strategies for Computation Offloading in
                  the Internet of Vehicles: Efficiency and Trust},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3372--3389},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509542},
	doi = {10.1109/TMC.2024.3509542},
	timestamp = {Thu, 01 Jan 2026 19:11:53 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GaoXFLYCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the Internet of Vehicles (IoV), vehicles will generate massive data and computation demands, necessitating computation offloading at the edge. However, existing research faces challenges in efficiency and trust. In this paper, we explore the IoV computation offloading from both user and edge facility provider perspectives, working to optimize the quality of experience (QoE), load balancing, and success rate based on challenges to efficiency and trust. First, two vehicle interconnection models are constructed to extend the linkable range of intra-road and inter-road vehicles while considering the maximum link time constraint. Then, a dynamic planning method is proposed, combining the reputation and feedback mechanisms, which can schedule edge resources online based on the cumulative computation latency of each service side, reliability value, and historical behavior. These two phases further improve the efficiency of edge services. Subsequently, blockchain is combined to optimize the trust problem of edge collaboration, and an edge-limited Byzantine fault tolerance local consensus mechanism is proposed to optimize consensus efficiency and ensure the reliability of edge services. Finally, this paper conducts dynamic experiments on real-world datasets, verifying the effectiveness of the proposed algorithm and models in multiple vehicle density datasets and experimental scenarios.}
}


@article{DBLP:journals/tmc/QiaoGZLZLC25,
	author = {Dewen Qiao and
                  Songtao Guo and
                  Jun Zhao and
                  Junqing Le and
                  Pengzhan Zhou and
                  Mingyan Li and
                  Xuetao Chen},
	title = {{ASMAFL:} Adaptive Staleness-Aware Momentum Asynchronous Federated
                  Learning in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3390--3406},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3510135},
	doi = {10.1109/TMC.2024.3510135},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiaoGZLZLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared with synchronous federated learning (FL), asynchronous FL (AFL) has attracted more and more attention in edge computing (EC) fields because of its strong adaptability to heterogeneous application scenarios. However, the non-independent and identically distributed (Non-IID) data across devices and the staleness-aware estimation of unreliable wireless connections and limited edge resources make it much more difficult to achieve better AFL-related applications. To handle this problem, we propose an Adaptive Staleness-aware Momentum Accelerated AFL (ASMAFL) algorithm to reduce the resources consumption of heterogeneous wireless communication EC (WCEC) scenarios, as well as decrease the negative impact of Non-IID data for model training. Specifically, we first introduce the staleness-aware parameter and a unified momentum gradient descent (GD) framework to reformulate AFL. Then, we establish global convergence properties of AFL, derive an upper bound on AFL convergence rate, and find that the bound is related to the staleness-aware parameter and Non-IIDness. Next, we formulate the bound into a minimization problem of resource consumption under given model accuracy, and the corresponding staleness-aware parameter of devices will be recomputed after each asynchronous aggregation to eliminate the differences of local models’ contribution to global model aggregation. Finally, extensive experiments are carried out to validate the superiority of ASMAFL in model accuracy, convergence rate, resources consumption, Non-IID issue, etc.}
}


@article{DBLP:journals/tmc/LiLLLWZY25,
	author = {Xiaoyu Li and
                  Jia Liu and
                  Zihao Lin and
                  Xuan Liu and
                  Yanyan Wang and
                  Shigeng Zhang and
                  Baoliu Ye},
	title = {Advancing {RFID} Technology for Virtual Boundary Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3407--3422},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514895},
	doi = {10.1109/TMC.2024.3514895},
	timestamp = {Wed, 24 Sep 2025 09:55:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLLLWZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A boundary is a physical or virtual line that marks the edge or limit of a specific region, which has been widely used in many applications, such as autonomous driving, virtual wall, and robotic lawn mowers. However, none of existing work can well balance the deployability and the scalability of a boundary. In this paper, we propose a brand new RFID-based virtual boundary scheme together with its detection algorithm called RF-Boundary, which has the competitive advantages of being battery-free and easy-to-maintain. We develop two technologies of phase gradient and dual-antenna AoA to address the key challenges posed by RF-boundary, in terms of lack of calibration information and multi-edge interference. Besides, we consider the presence of multipath in the real world applications, model the effect on signals in the dynamic scenarios, and demonstrate the robustness of our phase gradient-based scheme under multipath. We implement a prototype of RF-Boundary with commercial RFID systems and a mobile robot. Extensive experiments verify the feasibility as well as the good performance of RF-Boundary, with a mean detection error of only 8.6 cm.}
}


@article{DBLP:journals/tmc/LinYL25,
	author = {Chiao{-}Wen Lin and
                  De{-}Nian Yang and
                  Wanjiun Liao},
	title = {Mobile Tile-Based 360{\textdollar}{\^{}}{\textbackslash}circ{\textdollar}{\(\circ\)}
                  Video Multicast With Cybersickness Alleviation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3423--3440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514852},
	doi = {10.1109/TMC.2024.3514852},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual reality (VR) imaging is 360°, which requires a large bandwidth for video transmission. To address this challenge, tile-based streaming has been proposed to deliver only the focused part of the video instead of the entire one. However, the impact of cybersickness, akin to motion sickness, on tile selection in VR has not been explored. In this paper, we investigate Multi-user Tile Streaming with Cybersickness Control (MTSCC) in an adaptive 360 ∘ ^\\circ  video streaming system with multicast and cybersickness alleviation. We propose a novel  m 2 m^{2} -competitive online algorithm that utilizes Individual Sickness Indicator (ISI) and Bitrate Restriction Indicator (BRI) to evaluate user cybersickness tendency and network bandwidth efficiency. Moreover, we introduce the Video Loss Indicator (VLI) and Quality Variance Indicator (QVI) to assess video quality loss and quality difference between tiles. We also propose a multi-armed bandit (MAB) algorithm with confidence bound-based reward (video quality) and cost (cybersickness) estimation. The algorithm learns the weighting factor of each user's cost to slow down cybersickness accumulation for users with high cybersickness tendencies. We prove that the algorithm converges to an optimal solution over time. According to simulation with real network settings, our proposed algorithms outperform baselines in terms of video quality and cybersickness accumulation.}
}


@article{DBLP:journals/tmc/LiWHHCX25,
	author = {Guizhen Li and
                  Shuo Wang and
                  Yudong Huang and
                  Tao Huang and
                  Yuanhao Cui and
                  Zehui Xiong},
	title = {Optimizing Fault-Tolerant Time-Aware Flow Scheduling in {TSN-5G} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3441--3455},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3510604},
	doi = {10.1109/TMC.2024.3510604},
	timestamp = {Tue, 08 Apr 2025 20:23:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWHHCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of time-sensitive networking (TSN) and fifth-generation (5G) offers a promising solution for real-time and reliable data transmission in the Industrial Internet of Things (IIoT). However, current research focuses on traffic scheduling in TSN-5G networks to support low latency. New challenges arise when TSN-5G networks leverage time-aware shaper (TAS) and frame replication and elimination for reliability (FRER) to achieve low latency and high reliability. Simply combining TAS and FRER (SCTF) requires scheduling all time-triggered (TT) flows and their replica flows, which substantially increases the computational complexity of gate control lists (GCLs) and severely weakens scheduling capabilities. Moreover, the packet elimination function (PEF) in FRER may induce packet misordering. In this paper, we propose an efficient and fault-tolerant time-aware shaper (EF-TAS) mechanism for TSN-5G networks. EF-TAS only allocates timeslots for TT flows, while replica TT (RT) flows are delivered using a best-effort strategy. Due to the potential violation of deadlines in RT flows, we design an adaptive cyclic GCL window (ACGW)-based hybrid scheduling (AHS) algorithm to schedule TT and RT flows differentially. The AHS algorithm utilizes network calculus to ensure the timely arrival of RT flows without affecting the deterministic transmission of TT flows. In particular, we provide upper bounds on the amount of reordering to quantify the disorder caused by PEF and analyze the impact of introducing the packet ordering function (POF) on EF-TAS performance. The evaluation results show that EF-TAS not only meets the reliability and deadline requirements but also significantly reduces the total number of GCL entries and the computation time of GCLs compared to state-of-the-art methods.}
}


@article{DBLP:journals/tmc/ZhuZZCX25,
	author = {Jiahao Zhu and
                  Lu Zhao and
                  Jian Zhou and
                  Hui Cai and
                  Fu Xiao},
	title = {Do as the Romans Do: Location Imitation-Based Edge Task Offloading
                  for Privacy Protection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3456--3472},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509418},
	doi = {10.1109/TMC.2024.3509418},
	timestamp = {Tue, 07 Oct 2025 11:24:58 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuZZCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge computing, a user prefers offloading his/her task to nearby edge servers to maximize the offloading utility. However, this inevitably exposes the user's location privacy information when suffering from the side-channel attacks based on offloading decision behaviors and Received Signal Strength Indicators (RSSI). Existing works only consider the scenario with one untrusted edge server or defend only against one of the attacks. In this paper, we first study the edge task offloading problem with comprehensive privacy protection against these side-channel attacks from multiple edge servers. To address this problem while ensuring satisfactory offloading utility, we develop a Location Imitation-based Edge Task Offloading approach LITO. Specifically, we first determine a suitable perturbation region centered at the user's real location for a balance between offloading utility and privacy protection, and then propose a modified Laplace mechanism to generate a fake location meeting geo-indistinguishability within the region. Subsequently, to mislead the side-channel attacks to the fake location, we design an approximate algorithm and a transmit power control strategy to imitate the offloading decisions and RSSIs at the fake location, respectively. Theoretical analysis and experimental evaluations demonstrate the performance of LITO in improving privacy protection and guaranteeing offloading utility.}
}


@article{DBLP:journals/tmc/FengJSNZGYYZ25,
	author = {Lei Feng and
                  Xiaoyi Jiang and
                  Yao Sun and
                  Dusit Niyato and
                  Yu Zhou and
                  Shiyi Gu and
                  Zhixiang Yang and
                  Yang Yang and
                  Fanqin Zhou},
	title = {Resource Allocation for Metaverse Experience Optimization: {A} Multi-Objective
                  Multi-Agent Evolutionary Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {4},
	pages = {3473--3488},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509680},
	doi = {10.1109/TMC.2024.3509680},
	timestamp = {Wed, 24 Sep 2025 10:10:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengJSNZGYYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Metaverse, real-time, concurrent services such as virtual classrooms and immersive gaming require local graphic rendering to maintain low latency. However, the limited processing power and battery capacity of user devices make it challenging to balance Quality of Experience (QoE) and terminal energy consumption. In this paper, we investigate a multi-objective optimization problem (MOP) regarding power control and rendering capacity allocation by formulating it as a multi-objective optimization problem. This problem aims to minimize energy consumption while maximizing Meta-Immersion (MI), a metric that integrates objective network performance with subjective user perception. To solve this problem, we propose a Multi-Objective Multi-Agent Evolutionary Reinforcement Learning with User-Object-Attention (M2ERL-UOA) algorithm. The algorithm employs a prediction-driven evolutionary learning mechanism for multi-agents, coupled with optimized rendering capacity decisions for virtual objects. The algorithm can yield a superior Pareto front that attains the Nash equilibrium. Simulation results demonstrate that the proposed algorithm can generate Pareto fronts, effectively adapts to dynamic user preferences, and significantly reduces decision-making time compared to several benchmarks.}
}


@article{DBLP:journals/tmc/NguyenVYPH25,
	author = {Minh{-}Duong Nguyen and
                  Quang Do Vinh and
                  Zhaohui Yang and
                  Quoc{-}Viet Pham and
                  Won{-}Joo Hwang},
	title = {Distortion Resilience for Goal-Oriented Semantic Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3489--3501},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3456856},
	doi = {10.1109/TMC.2024.3456856},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NguyenVYPH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research efforts on Semantic Communication (SemCom) have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of Artificial Intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate distortion theory to analyze distortions induced by communication and compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented SemCom problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and experiments that demonstrate its effectiveness. The experimental results indicate that our proposed method enables accurate AI task performance while adhering to network constraints, establishing it as a valuable contribution to the field of signal processing. Furthermore, this work advances research in goal-oriented SemCom and highlights the significance of data-driven approaches in optimizing the performance of intelligent systems.}
}


@article{DBLP:journals/tmc/LiHQYN25,
	author = {Ran Li and
                  Chuan Huang and
                  Xiaoqi Qin and
                  Dong Yang and
                  Xinyao Nie},
	title = {Multicast Scheduling Over Multiple Channels: {A} Distribution-Embedding
                  Deep Reinforcement Learning Method},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3502--3519},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514169},
	doi = {10.1109/TMC.2024.3514169},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiHQYN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multicasting is an efficient technique for simultaneously transmitting common messages from the base station (BS) to multiple mobile users (MUs). Multicast scheduling over multiple channels, which aims to jointly minimize the energy consumption of the BS and the latency of serving asynchronized requests from the MUs, is formulated as an infinite-horizon Markov decision process (MDP) problem with a large discrete action space, multiple time-varying constraints, and multiple time-invariant constraints. To address these challenges, this paper proposes a novel distribution-embedding multi-agent proximal policy optimization (DE-MAPPO) algorithm, which consists of one modified MAPPO and one distribution-embedding module. The former one handles the large discrete action space and time-varying constraints by modifying the structure of the actor networks and the training kernel of the conventional MAPPO; and the latter one iteratively adjusts the action distribution to satisfy the time-invariant constraints. Moreover, a performance upper bound of the considered MDP is derived by solving a two-step optimization problem. Finally, numerical results demonstrate that our proposed algorithm outperforms the existing ones in terms of applicability, effectiveness, and robustness, and achieves comparable performance to the derived upper bound.}
}


@article{DBLP:journals/tmc/ChenXWHWLH25,
	author = {Weiwei Chen and
                  Xianjin Xia and
                  Shuai Wang and
                  Tian He and
                  Shuai Wang and
                  Gang Liu and
                  Caishi Huang},
	title = {Enabling Large Scale LoRa Parallel Decoding With High-Dimensional
                  and High-Accuracy Features},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3520--3536},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3517343},
	doi = {10.1109/TMC.2024.3517343},
	timestamp = {Sat, 19 Apr 2025 10:31:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXWHWLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRaWAN is a prominent technology for Low Power Wide Area Networks (LPWAN). However, the increasing network size has introduced a significant challenge: packet collisions resulting from concurrent transmissions in LoRaWAN. Previous studies either overlooked the issue by examining limited features or tackled it with intricate receivers employing up to eight antennas. To achieve a more favorable balance between implementation cost and system performance, we introduce  Hi 2 LoRa \\text{Hi}^{2}\\text{LoRa} —a solution utilizing highly dimensional and accurate features for LoRa concurrent decoding, implemented with only two receiving antennas. The feature dimensions are expanded through an exploration of various hardware imperfections and inherent channel state information specific to each transceiver pair. To enhance feature accuracy, low pass filters and BiLSTM networks are applied to capture and learn their temporal patterns. Additionally, an efficient collision suppression strategy is introduced to mitigate feature corruption from concurrently transmitted packets. Extensive real-world testbed evaluations demonstrate that the achievable concurrency in  Hi 2 LoRa \\text{Hi}^{2}\\text{LoRa}  approaches that of state-of-the-art approaches with significantly higher complexity (e.g., utilizing eight antennas) or exceeds prior work by a factor of 2.7 with comparable complexity (e.g., using two antennas).}
}


@article{DBLP:journals/tmc/WangZFFC25,
	author = {Xuan Wang and
                  Xuerong Zhao and
                  Chao Feng and
                  Dingyi Fang and
                  Xiaojiang Chen},
	title = {mmFinger: Talk to Smart Devices With Finger Tapping Gesture},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3537--3551},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3515044},
	doi = {10.1109/TMC.2024.3515044},
	timestamp = {Fri, 15 Aug 2025 09:23:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZFFC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contact-free finger gesture recognition unlocks plenty of applications in smart Human-Computer Interaction (HCI). However, existing solutions either require users to wear sensors on their fingers or use continuously monitored cameras, raising concerns regarding user comfort and privacy. In this paper, we propose mmFinger, an accurate and robust mmWave-based finger gesture recognition system that can extend the range of available custom commands. The core idea is that mmFinger leverages the finger tapping pattern as a basic gesture and encodes different number combinations of the basic gesture like Morse code. To enable reliable recognition across different locations and for various users, we carefully design a robust feature Dop-profile to effectively characterize finger movements. Furthermore, by leveraging the multi-views provided by multiple antennas of radar, we develop an adaptive weighted feature fusion network to enhance the system's robustness. Finally, we devise a novel sequence prediction network to enable the system to recognize new gestures without retraining. Comprehensive experiments demonstrate that mmFinger can achieve an average recognition accuracy of 92% for 36 predefined gestures and 88% for 5 new user-defined commands, and is robust against finger location and user diversity.}
}


@article{DBLP:journals/tmc/GeZQ25,
	author = {Shuxin Ge and
                  Xiaobo Zhou and
                  Tie Qiu},
	title = {R2Pricing: {A} MARL-Based Pricing Strategy to Maximize Revenue in
                  MoD Systems With Ridesharing and Repositioning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3552--3566},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514124},
	doi = {10.1109/TMC.2024.3514124},
	timestamp = {Thu, 01 May 2025 20:36:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GeZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pricing strategy is crucial for improving the revenue of mobility on-demand (MoD) systems by achieving supply-demand equilibrium across different city zones. Modern MoD systems commonly utilize order ridesharing and vehicle repositioning to improve the order completion rate while supporting this equilibrium, thereby improving the revenue. However, most existing pricing strategies overlook the effects of ridesharing and repositioning, resulting in supply-demand mismatch and revenue decline. To fill this gap, we propose a multi-agent reinforcement learning (MARL) based pricing strategy via a mutual attention mechanism, named R2Pricing, where the impact of ridesharing and repositioning is considered. First, we formulate the pricing with ridesharing and repositioning as an optimization problem toward maximum overall revenue. Then, we transform it into a MARL model, where the agent makes coupled decisions about order fare with ridesharing and vehicle income with repositioning for each zone. Next, the agents are clustered based on supply-demand observation and reward to train more efficiently. The pricing messages between agents are generated based on mutual information theory, which is then aggregated with an attention mechanism to estimate the impact of price differences among zones. Finally, simulations based on real-world data are conducted to demonstrate the superiority of R2Pricing over the benchmarks.}
}


@article{DBLP:journals/tmc/MeesenaNTS25,
	author = {Wasin Meesena and
                  Chanikarn Nikunram and
                  Stephen John Turner and
                  Sucha Supittayapornpong},
	title = {Minimizing Age of Processed Information Over Unreliable Wireless Network
                  Channels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3567--3578},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3520913},
	doi = {10.1109/TMC.2024.3520913},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MeesenaNTS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The freshness of real-time status processing of time-sensitive information is crucial for many applications, including flight control, image processing, and autonomous vehicles. In this paper, unprocessed information is sent from sensors to a base station over a shared, unreliable wireless network. The base station has a set of dedicated non-preemptive processors with constant processing times to process information from each sensor. The age of processed information is the time elapsed since the generation of the packet that the processor most recently processed. Our objective is to minimize the expected weighted sum of this age over an infinite time horizon. Here, the challenge is the coupling between a scheduling problem under unreliable communications and the processing times. We first break the coupling by tracking the age of information during processing and derive a lower performance bound of the objective. We then design a stationary randomized policy and a Max-Weight policy for two queueing disciplines: no queues and single-packet queues to achieve our objective. We prove that these policies achieve performance within a factor of two from the optimal. In addition, we prove queues are useful to the stationary randomized policies in highly unreliable or large network settings. Our analytical results are further validated by numerical experiments.}
}


@article{DBLP:journals/tmc/YuLLCPS25,
	author = {Ercong Yu and
                  Shanyun Liu and
                  Qiang Li and
                  Hongyang Chen and
                  H. Vincent Poor and
                  Shlomo Shamai},
	title = {Graph-Based Joint Client Clustering and Resource Allocation for Wireless
                  Distributed Learning: {A} New Hierarchical Federated Learning Framework
                  With Non-IID Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3579--3596},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3515037},
	doi = {10.1109/TMC.2024.3515037},
	timestamp = {Mon, 06 Oct 2025 14:28:11 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuLLCPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical federated learning (HFL) is a key technology enabling distributed learning with reduced communication overhead. However, practical HFL systems encounter two major challenges: limited resources and data heterogeneity. In particular, limited resources can result in intolerable system latency, while heterogeneous data across clients can significantly degrade model accuracy and convergence rates. To address these issues and fully leverage the potential of HFL, we propose a novel framework called graph-based joint client and resource orchestration. This framework addresses the challenges of practical networks through joint client clustering and resource allocation. First, we propose a learning process where edge servers employ hypernetworks to achieve edge aggregation. This method can generate personalized client models and extract data distributions without directly exposing data distributions. Then, to characterize the joint effects of limited resources and data heterogeneity, we propose a graph-based modeling method and formulate a joint optimization problem that aims to balance data distributions and minimize latency. Subsequently, we propose a graph neural network-based algorithm to tackle the formulated problem with low-complexity optimization. Numerical results demonstrate significant benefits over existing algorithms in terms of convergence latency, model accuracy, scalability, and adaptability to new distributions.}
}


@article{DBLP:journals/tmc/ChenWW25,
	author = {Liyue Chen and
                  Xiaoxiang Wang and
                  Leye Wang},
	title = {Exploring Context Generalizability in Citywide Crowd Mobility Prediction:
                  An Analytic Framework and Benchmark},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3597--3614},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3517332},
	doi = {10.1109/TMC.2024.3517332},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contextual features are important data sources for building citywide crowd mobility prediction models. However, the difficulty of applying context lies in the unknown generalizability of contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we present a unified analytic framework and a large-scale benchmark for evaluating context generalizability. The benchmark includes crowd mobility data, contextual data, and advanced prediction models. We conduct comprehensive experiments in several crowd mobility prediction tasks such as bike flow, metro passenger flow, and electric vehicle charging demand. Our results reveal several important observations: (1) Using more contextual features may not always result in better prediction with existing context modeling techniques; in particular, the combination of holiday and temporal position can provide more generalizable beneficial information than other contextual feature combinations. (2) In context modeling techniques, using a gated unit to incorporate raw contextual features into the deep prediction model has good generalizability. Besides, we offer several suggestions about incorporating contextual factors for building crowd mobility prediction applications. From our findings, we call for future research efforts devoted to developing new context modeling solutions.}
}


@article{DBLP:journals/tmc/ShiZLMWGLCSTZCQ25,
	author = {Xiaohang Shi and
                  Sheng Zhang and
                  Meizhao Liu and
                  Lingkun Meng and
                  Liu Wei and
                  Yingcheng Gu and
                  Kai Liu and
                  Huanyu Cheng and
                  Yu Song and
                  Lei Tang and
                  Andong Zhu and
                  Ning Chen and
                  Zhuzhong Qian},
	title = {Mystique: User-Level Adaptation for Real-Time Video Analytics in Edge
                  Networks via Meta-RL},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3615--3632},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514088},
	doi = {10.1109/TMC.2024.3514088},
	timestamp = {Sun, 14 Sep 2025 12:30:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShiZLMWGLCSTZCQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural network (DNN)-based real-time video analytics service, as a core module for numerous crucial applications such as augmented reality (AR), has garnered increasing research attention, where mobile edge computing (MEC) is often leveraged to mitigate its real-time processing burden on resource-constrained user devices. For Quality of Experience (QoE) optimization, latest works employ reinforcement learning (RL)-based methods to adaptively adjust configurations (e.g., resolution and frame rate), yet still presenting significant challenges. Firstly, we observe a substantial diversity in QoE patterns among users. Given that existing methods integrate a fixed QoE pattern in parameter training, it is intuitive to customize a policy network for each user. However, this necessitates significant training investment, failing to support on-the-fly deployment for new users. Secondly, given the dual dynamics from both the network and video content in edge video analytics system, existing methods often fall into the dilemma of fitting newly emerged and diverse system states with offline-trained fixed parameters. While it is promising to employ online learning algorithms, most of them struggle to catch up with the high dynamics. We hence propose Mystique. In real-time edge video analytics domain, it is the first meta-RL-based user-level configuration adaptation framework. Mystique establishes an initial model in offline meta training with model-agnostic meta-learning (MAML), enabling swift online adaptation to new users and system states through limited gradient updates from initial parameters. Comprehensive experiments illustrate that Mystique can improve QoE by 42% on average compared to prior works.}
}


@article{DBLP:journals/tmc/XuZDLM25,
	author = {Leiyang Xu and
                  Xiaolong Zheng and
                  Xinrun Du and
                  Liang Liu and
                  Huadong Ma},
	title = {WiCamera: Vortex Electromagnetic Wave-Based WiFi Imaging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3633--3649},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3519623},
	doi = {10.1109/TMC.2024.3519623},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuZDLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current WiFi imaging approaches focus on monitoring dynamic targets to facilitate easy object distinction and capture rich signal reflections for image construction. In static object imaging, massive antenna array or emulated antenna array is often necessary. We propose WiCamera, a novel WiFi imaging prototype that utilizes vortex electromagnetic waves (VEMWs) to monitor stationary human postures using commodity WiFi, by generating human silhouettes with only  3 × 3 3 \\times 3  MIMO. VEMWs possess a helical wavefront with different phase variations, enabling the imaging of stationary objects through different OAM (Orbital Angular Momentum) modes with time-division multiplexing. WiCamera emits three OAM modes waves from WiFi devices and utilizes their phase variations for imaging. By ray tracing the received signals to a target image plane, WiCamera generates a wavefront image. A generative adversarial network (GAN)-based model is further utilized to refine the wavefront image and create a high-resolution human silhouette. The system's output images are evaluated using metrics such as structural similarity index measure (SSIM) and Szymkiewicz-Simpson coefficient (SSC), comparing them to ground truth images captured by cameras. The evaluation shows that WiCamera performs consistently well in various environments and with different users, with an SSIM reaching up to 0.89 and an SSC reaching up to 0.93.}
}


@article{DBLP:journals/tmc/GuoDSHZ25,
	author = {Xiaolin Guo and
                  Fang Dong and
                  Dian Shen and
                  Zhaowu Huang and
                  Jinghui Zhang},
	title = {Resource-Efficient {DNN} Inference With Early Exiting in Serverless
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3650--3666},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514993},
	doi = {10.1109/TMC.2024.3514993},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoDSHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless Edge Computing (SEC) has gained widespread adoption in improving resource utilization due to its triggered event-driven model. However, deploying deep neural network (DNN) inference services directly in SEC leads to resource inefficiencies, which stem from two key factors. First, existing methods adopt model-wise function encapsulation, which requires the entire DNN model to occupy memory throughout its execution lifecycle. This increases both memory footprint and occupancy time. Second, uniform DNN inference for diversity input leads to redundant computations and additional inference time. To this end, we propose REDI, a novel framework that leverages fine-grained block-wise function encapsulation and progressive inference to provide resource-efficient DNN inference while ensuring latency requirements. REDI enables the release of memory from already inferred shallow networks and allows each request to exit early based on input data complexity, eliminating redundant computations. To fully unleash the potential, REDI jointly considers resource heterogeneity, data diversity, and environment dynamics to investigate the block-wise function placement problem. We introduce an uncertainty-aware online learning-driven algorithm with bounded regret. Finally, we conduct extensive trace-driven experiments to evaluate our methods, demonstrating that REDI achieves a significant speedup of up to  6.52 × 6.52\\times  in terms of resource usage cost compared to state-of-the-art methods.}
}


@article{DBLP:journals/tmc/DengCZZYGPL25,
	author = {Mingyu Deng and
                  Chao Chen and
                  Wanyi Zhang and
                  Jie Zhao and
                  Wei Yang and
                  Suiming Guo and
                  Huayan Pu and
                  Jun Luo},
	title = {HyperRegion: Integrating Graph and Hypergraph Contrastive Learning
                  for Region Embeddings},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3667--3684},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3515154},
	doi = {10.1109/TMC.2024.3515154},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengCZZYGPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Region representations (also called embeddings) are useful for various urban computing tasks. While graph-based region representation learning methods have shown outstanding performance, they encounter two major challenges: 1) the pervasive data noise and missing data can affect the quality of the constructed region graphs; and 2) high-order relationships (i.e., group-wise relationships) among regions are often insufficiently modeled and sometimes entirely overlooked. To this end, we propose HyperRegion, an unsupervised region representation learning framework that integrates graph and hypergraph contrastive learning to learn comprehensive region embeddings from multi-modal data. Built upon a region hybrid graph network, this framework models both pair-wise and group-wise dependencies involving POI semantics, mobility patterns, geographic neighbors, and visual semantics. To mitigate the impact of data noise and missing data, graph and hypergraph contrastive learning are performed in parallel, and a cross-module contrast is further introduced to facilitate information exchange and collaboration. Extensive experiments on real-world datasets across three downstream tasks demonstrate that HyperRegion outperforms all baselines, particularly improving check-in prediction by reducing MAE and RMSE by approximately 8.5% and 8.2%, respectively, and increasing  R 2 R^{2}  by about 7%.}
}


@article{DBLP:journals/tmc/ZhangWHWTWN25,
	author = {Yuhan Zhang and
                  Ran Wang and
                  Jie Hao and
                  Qiang Wu and
                  Yidan Teng and
                  Ping Wang and
                  Dusit Niyato},
	title = {Service Function Chain Deployment With VNF-Dependent Software Migration
                  in Multi-Domain Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3685--3702},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514173},
	doi = {10.1109/TMC.2024.3514173},
	timestamp = {Tue, 13 Jan 2026 20:25:57 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWHWTWN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the 6G era, user demand for low-latency, cost-effective extreme services such as extended reality (XR) and holographic communications has significantly increased. Multi-domain networks, known for their vast capacity and coverage, are essential in fulfilling the growing demand for high-performance services. Despite their potential, these networks face challenges with domain isolation, requiring a software defined network (SDN) controller for inter-domain communication. Network function virtualization (NFV) enhances flexibility of service delivery with customizable service function chain (SFC), yet prior research falls short in delivering low-latency, cost-efficient services in multi-domain NFV networks alongside an unreasonable assumption that software on physical nodes can support the execution of all virtualization network functions (VNFs). In this paper, we study the problem of SFC deployment with VNF-dependent software migration (SD-VDSM) in multi-domain networks. Particularly, we first formulate the problem by setting an objective to minimize the end-to-end communication delay and the associated costs of service provisioning, while simultaneously ensuring load balancing across multi-domain networks. However, complexity of the issue escalates to an intractable level due to the intertwined nature of SFC deployment strategies and VNF-dependent software migration tactics, which mutually influence each other intricately. To tackle this issue, we propose an innovative heuristic algorithm, designated as the Joint SFC Deployment with VNF-Dependent Software Migration Algorithm (JSD-VDSMA). Comprising three fundamental steps, this algorithm is crafted to adeptly resolve the complexities of service provisioning across multi-domain networks. A suite of rigorous experimental assessments is detailed, demonstrating the capability of our proposed JSD-VDSMA. Through these comparative analyses, we demonstrate its effectiveness not only to increase the service acceptance rate but also to diminish both the end-to-end communication delay and resource utilization costs in comparison to its counterparts.}
}


@article{DBLP:journals/tmc/SongWGSRCGYZZ25,
	author = {Wenchao Song and
                  Zhu Wang and
                  Yifan Guo and
                  Zhuo Sun and
                  Zhihui Ren and
                  Chao Chen and
                  Bin Guo and
                  Zhiwen Yu and
                  Xingshe Zhou and
                  Daqing Zhang},
	title = {FinerSense: {A} Fine-Grained Respiration Sensing System Based on Precise
                  Separation of Wi-Fi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3703--3718},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514311},
	doi = {10.1109/TMC.2024.3514311},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongWGSRCGYZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study introduces a novel approach for preventing overexertion in home fitness through fine-grained detection of respiratory parameters. To overcome the robustness limitation associated with using a composite signal for wireless sensing, we introduce an optimization-based signal separation model. This model effectively disentangles composite signals into static and dynamic components, while preserving the intricate details of target movements or activities. Specifically, by constructing a reference signal derived from the dominant static component, we eliminate time-varying phase shifts and leverage the invariant property of the dynamic component’s amplitude for precise separation. A system called FinerSense is developed, which is able to accurately and robustly detect fine-grained respiratory parameters such as respiration rate, depth, and inhalation-to-exhalation ratio with accuracy rates exceeding 97%, 95%, and 91%, respectively. Extensive experiments show that the developed system outperforms state-of-the-art baselines significantly, empowering users to optimize exercise intensity and duration while mitigating the risk of overexertion. We believe that this work is able to facilitate the seamless transition of wireless sensing systems from laboratory prototypes to practical and user-friendly applications.}
}


@article{DBLP:journals/tmc/LiWLZLD25,
	author = {Zhetao Li and
                  Junru Wu and
                  Saiqin Long and
                  Zhirun Zheng and
                  Chengxin Li and
                  Mianxiong Dong},
	title = {User-Driven Privacy-Preserving Data Streams Release for Multi-Task
                  Assignment in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3719--3734},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3516885},
	doi = {10.1109/TMC.2024.3516885},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWLZLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task assignment is widely used in mobile crowdsensing (MCS) to efficiently utilize limited resources such as shared user pool, user capability constraints and so on. In MCS, users need to submit data streams to perform sensing tasks, which involve a large amount of private information. However, the privacy leakage when users perform tasks across different types and submit multimodal data streams in multi-task assignment has not been fully addressed in current works. Privacy requirements vary for users with different activity levels in multi-task assignment. Specifically, users with higher activity levels tend to handle more task types and submit more data types, which poses more serious consequences of privacy leakage. Meanwhile, the privacy requirements of users are dynamic due to the user’s changing activity. In this work, we propose a user-driven local differential privacy framework for multi-task assignment called UD-LDP. First, we design a flexible privacy model called  w w -adjacent-event privacy to provide accurate privacy protection for users with different activity levels. Then, we introduce information entropy to quantify privacy requirements of user’s activity in real-time. After that, we propose a privacy-aware budget allocation method to dynamically allocate personalized privacy budgets for each user. At last, we design a variance-optimized selection method that chooses rational privacy budgets and users for release to improve data utility. The effectiveness of our framework is supported by experiments conducted on both real-world and synthetic datasets.}
}


@article{DBLP:journals/tmc/XuLLGLL25,
	author = {Wen Xu and
                  Zhetao Li and
                  Haolin Liu and
                  Yunjun Gao and
                  Xiaofei Liao and
                  Kenli Li},
	title = {Differentially Private Weighted Graphs Publication Under Continuous
                  Monitoring},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3735--3749},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514153},
	doi = {10.1109/TMC.2024.3514153},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuLLGLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph data analysis has been used in various real-world applications to improve services or scientific research, which, however, may expose sensitive personal information. Differential privacy (DP) has become the gold standard for publishing graph data while still protecting personal privacy. However, most existing studies over differentially private graph data publication mainly focus on static unweighted graphs. As interactions between entities in real systems are often dynamically changing and associated with weights, it is desirable to consider the more general scenario of continuous weighted graph publication under DP in the temporal dimension. Therefore, we investigate the problem of publishing weighted graphs satisfying DP under continuous monitoring. Specifically, we consider a server that continuously monitors user data and publishes a sequence of weighted graph snapshots. We propose SwgDP, a novel framework that leverages historical graph data to guide current snapshot generation. SwgDP consists of four key components: node adaptive sampling, dynamic weight optimization, prediction-based community detection and weighted graph generation. We demonstrate that SwgDP satisfies DP, and comprehensive experiments on four real-world datasets and four commonly used graph metrics show that SwgDP can effectively synthesize weighted graph at any time step.}
}


@article{DBLP:journals/tmc/WuHR25,
	author = {Zongshen Wu and
                  Chin{-}Ya Huang and
                  Parameswaran Ramanathan},
	title = {{MIQA:} An Application Agent for Immersive Content Delivery Over Millimeter
                  Waves},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3750--3763},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514973},
	doi = {10.1109/TMC.2024.3514973},
	timestamp = {Thu, 01 May 2025 20:36:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuHR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The highly directional nature of the millimeter wave (mmWave) beams causes several challenges in using that spectrum to meet the communication demands of immersive applications. The mmWave beams are especially susceptible to misalignments and blockages caused by user movements. As a result, mmWave channels are vulnerable to large quality fluctuations, which in turn, degrades the end-to-end performance of immersive applications. In this paper, we propose a reinforcement learning (RL) based application-layer plugin that works in conjunction with the QUIC protocol to combat the challenges of mmWave networks. The plug-in called Millimeter wave based Immersive QUIC Agent (MIQA) uses the RL model to help modulate the sending rate along with the congestion control scheme of QUIC. To evaluate the effectiveness of MIQA, we conduct experiments on a mmWave augmented immersive testbed. The evaluation results show that MIQA significantly improves the immersive experience by increasing the end-to-end throughput and by decreasing the end-to-end latency.}
}


@article{DBLP:journals/tmc/SoodLNKFY25,
	author = {Keshav Sood and
                  Shigang Liu and
                  Dinh Duc Nha Nguyen and
                  Neeraj Kumar and
                  Bohao Feng and
                  Shui Yu},
	title = {Alleviating Data Sparsity to Enhance {AI} Models Robustness in IoT
                  Network Security Context},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3764--3778},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3525463},
	doi = {10.1109/TMC.2025.3525463},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SoodLNKFY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Internet of Things (IoT) networks, the IoT sensors collect valuable raw data required to sustain Artificial Intelligence (AI) based networks operation. AI models are data-driven as they use the data to make accurate network security, management, and operational decisions. Unfortunately, the sensors are deployed in harsh environments which affects the sensor behaviour and eventually the networks’ operations. Further, IoT devices are typically vulnerable to a range of malicious events. Therefore, IoT sensor's correct operation including resilience to failure is essential for sustained operations. Naturally, the state variables of time-series data can be changed, i.e., the data streams generated in these situations can be incorrect, incomplete or missing, and sparse presenting a significant challenge for real-time decision-making ability of AI models to make explainable and intelligent management and control decisions. In this paper, we aim to alleviate this fundamental problem to predict the missing and faulty reading correctly so that the decision-making ability of the AI models should not deteriorate in the presence of incorrect, missing, and highly imbalanced data sets. We use a novel approach using fuzzy-based information decomposition to recover the missed data values. We use three data sets, and our preliminary results show that our approach effectively recovers the missed or compromised data samples and help AI models in making accurate decision. Finally, the limitations and future work of this research have been discussed.}
}


@article{DBLP:journals/tmc/XiaMHHWOFNM25,
	author = {Qingxin Xia and
                  Jaime Morales and
                  Yongzhi Huang and
                  Takahiro Hara and
                  Kaishun Wu and
                  Hirotomo Oshima and
                  Masamitsu Fukuda and
                  Yasuo Namioka and
                  Takuya Maekawa},
	title = {Self-Supervised Learning for Complex Activity Recognition Through
                  Motif Identification Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3779--3793},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514736},
	doi = {10.1109/TMC.2024.3514736},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaMHHWOFNM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to the cost of collecting labeled sensor data, self-supervised learning (SSL) methods for human activity recognition (HAR) that effectively use unlabeled data for pretraining have attracted attention. However, applying prior SSL to COMPLEX activities in real industrial settings poses challenges. Despite the consistency of work procedures, varying circumstances, such as different sizes of packages and contents in a packing process, introduce significant variability within the same activity class. In this study, we focus on sensor data corresponding to characteristic and necessary actions (sensor data motifs) in a specific activity such as a stretching packing tape action in an assembling a box activity, and propose to train a neural network in self-supervised learning so that it identifies occurrences of the characteristic actions, i.e., Motif Identification Learning (MoIL). The feature extractor in the network is subsequently employed in the downstream activity recognition task, enabling accurate recognition of activities containing these characteristic actions, even with limited labeled training data. The MoIL approach was evaluated on real-world industrial activity data, encompassing the state-of-the-art SSL tasks with an improvement of up to 23.85% under limited training labels.}
}


@article{DBLP:journals/tmc/TunDPH25,
	author = {Yan Kyaw Tun and
                  Gy{\"{o}}rgy D{\'{a}}n and
                  Yu Min Park and
                  Choong Seon Hong},
	title = {Joint {UAV} Deployment and Resource Allocation in THz-Assisted MEC-Enabled
                  Integrated Space-Air-Ground Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3794--3808},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3516655},
	doi = {10.1109/TMC.2024.3516655},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TunDPH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC)-enabled integrated space-air-ground (SAG) networks have drawn much attention recently, as they can provide communication and computing services to wireless devices in areas that lack terrestrial base stations (TBSs). Leveraging the ample bandwidth in the terahertz (THz) spectrum, in this paper, we propose MEC-enabled integrated SAG networks with collaboration among unmanned aerial vehicles (UAVs). We then formulate the problem of minimizing the energy consumption of devices and UAVs in the proposed MEC-enabled integrated SAG networks by optimizing tasks offloading decisions, THz sub-bands assignment, transmit power control, and UAVs deployment. The formulated problem is a mixed-integer nonlinear programming (MILP) problem with a non-convex structure, which is challenging to solve. We thus propose a block coordinate descent (BCD) approach to decompose the problem into four sub-problems: 1) device task offloading decision problem, 2) THz sub-band assignment and power control problem, 3) UAV deployment problem, and 4) UAV task offloading decision problem. We then propose to use a matching game, concave-convex procedure (CCP) method, successive convex approximation (SCA), and block successive upper-bound minimization (BSUM) approaches for solving the individual subproblems. Finally, extensive simulations are performed to demonstrate the effectiveness of our proposed algorithm.}
}


@article{DBLP:journals/tmc/LakhlefDSBC25,
	author = {Issam Eddine Lakhlef and
                  Badis Djamaa and
                  Mustapha R{\'{e}}da Senouci and
                  Abbas Bradai and
                  Yahia Mohamed Cherif},
	title = {{PIM-LLN:} Protocol Independent Multicast for Low-Power and Lossy
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3809--3825},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514942},
	doi = {10.1109/TMC.2024.3514942},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LakhlefDSBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In resource-constrained Internet of Things (IoT) environments like Low-power and Lossy Networks (LLNs), efficient communication protocols are essential. In this context, IP multicast protocols play a crucial role, facilitating the transmission of data packets from a single source to multiple recipients, thereby conserving bandwidth, power, and time for numerous LLN applications, such as over-the-air programming, information dissemination, and device configuration. Despite their usefulness, existing multicast solutions face several challenges, including scalability, energy efficiency, and reliability. To tackle such issues, this paper introduces Protocol Independent Multicast for LLNs (PIM-LLN). PIM-LLN employs a multicast distribution tree anchored at the border router, a multi-path data dissemination mechanism, and an efficient retransmission technique to route streams exclusively to regions with group members reducing energy consumption and bandwidth usage while improving response times and reliability. Through comprehensive simulations and public testbed experiments, we meticulously assess PIM-LLN’s performance, benchmarking it against state-of-the-art solutions under different scenarios. Our findings underscore the scalability, reliability, reduced latency, and efficient resource utilization of PIM-LLN in terms of memory, bandwidth, and energy. Notably, PIM-LLN, as compared to state-of-the-art solutions, achieves a similar level of reliability while reducing overhead by up to 50%.}
}


@article{DBLP:journals/tmc/SongHDCCG25,
	author = {Yuxiao Song and
                  Daojing He and
                  Minghui Dai and
                  Sammy Chan and
                  Kim{-}Kwang Raymond Choo and
                  Mohsen Guizani},
	title = {Blockchain Assisted Trust Management for Data-Parallel Distributed
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3826--3843},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521443},
	doi = {10.1109/TMC.2024.3521443},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongHDCCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning models can support decision-making in mobile terminals (MTs) deployments, but their training generally requires massive datasets and abundant computation resources. This is challenging in practice due to the resource constraints of many MTs. To address this issue, data-parallel distributed learning can be conducted by offloading computation tasks from MTs to the edge-layer nodes. To facilitate the establishment of trust, one can leverage trust management, say to use trust values derived from local model quality and evaluations by other nodes as access criteria. Nonetheless, security and performance considerations remain unsolved. In this paper, we propose a blockchain-assisted dynamic trust management scheme for distributed learning, which comprises nodes attributes registration, trust calculation, information saving, and block writing. The proof of stake (PoS) consensus mechanism is leveraged to enable efficient consensus among the nodes using trust values as stakes. The incentive mechanism and corresponding dynamic optimization are then proposed to further improve system performance and security. The reinforcement-learning approach is leveraged to provide the optimal strategy for nodes’ local iterations and selection. Simulations and security analysis demonstrate that our proposed scheme can achieve an optimal trade-off between efficiency and quality of distributed learning while maintaining system security.}
}


@article{DBLP:journals/tmc/NaharBDD25,
	author = {Ankur Nahar and
                  Nishit Bhardwaj and
                  Debasis Das and
                  Sajal K. Das},
	title = {A Hypergraph Approach to Deep Learning Based Routing in Software-Defined
                  Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3844--3859},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3520657},
	doi = {10.1109/TMC.2024.3520657},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NaharBDD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Vehicular Networks (SDVNs) revolutionize modern transportation by enabling dynamic and adaptable communication infrastructures. However, accurately capturing the dynamic communication patterns in vehicular networks, characterized by intricate spatio-temporal dynamics, remains a challenge with traditional graph-based models. Hypergraphs, due to their ability to represent multi-way relationships, provide a more nuanced representation of these dynamics. Building on this hypergraph foundation, we introduce a novel hypergraph-based routing algorithm. We jointly train a model that incorporates Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU) using a Deep Deterministic Policy Gradient (DDPG) approach. This model carefully extracts spatial and temporal traffic matrices, capturing elements such as location, time, velocity, inter-dependencies, and distance. An integrated attention mechanism refines these matrices, ensuring precision in capturing vehicular dynamics. The culmination of these components results in routing decisions that are both responsive and anticipatory. Through detailed empirical experiments using a testbed, simulations with OMNeT++, and theoretical assessments grounded in real-world datasets, we demonstrate the distinct advantages of our methodology. Furthermore, when benchmarked against existing solutions, our technique performs better in model interpretability, delay minimization, rapid convergence, reducing complexity, and minimizing memory footprint.}
}


@article{DBLP:journals/tmc/WangCWL25,
	author = {Qian Wang and
                  Siguang Chen and
                  Meng Wu and
                  Xue Li},
	title = {Digital Twin-Empowered Federated Incremental Learning for Non-IID
                  Privacy Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3860--3877},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3517592},
	doi = {10.1109/TMC.2024.3517592},
	timestamp = {Mon, 25 Aug 2025 12:15:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a compelling distributed learning paradigm without sharing local original data. However, with ubiquitous non-independent and identically distributed (non-IID) privacy data, the FL suffers from severe performance loss and the privacy leakage by inference attacks. Existing solutions lack a cohesive framework with theoretical support, and their performance optimization and privacy protection are inter-inhibitive or high-cost. In this paper, we propose a digital twin (DT)-empowered federated incremental learning method to tackle the above challenges. First, we construct a DT-empowered federated incremental learning model to achieve cooperative awareness of performance and privacy-preservation. Second, a diffusion model-based selective data synthesis method is designed to provide auxiliary data for FL, it can avoid unnecessary overhead while ensuring the quality of synthetic samples under non-IID. Besides, it alleviates the negative impact of non-IID by allocating a class-balanced sub-dataset to each DT with IID setting. Third, we develop a DT-empowered alternating incremental learning method initiatively, under the premise of ensuring the confidentiality of original dataset, it can achieve efficient FL performance under non-IID with a small amount of synthetic samples. Furthermore, in order to estimate the contribution of each local model accurately, we investigate a comentropy-based federated aggregation strategy, which can obtain a superior global model. By sufficient theoretical analysis, we prove that the proposed methodology can achieve consistent enhancement of performance and privacy-preservation. Simultaneously, the experiments demonstrate that our methodology has efficient privacy-preserving property, it also outperforms other benchmarks on the accuracy and stability of the global model, especially in highly heterogeneous scenarios.}
}


@article{DBLP:journals/tmc/ZhangGFWHG25a,
	author = {Yu Zhang and
                  Yanmin Gong and
                  Lei Fan and
                  Yu Wang and
                  Zhu Han and
                  Yuanxiong Guo},
	title = {Quantum-Assisted Online Task Offloading and Resource Allocation in
                  MEC-Enabled Satellite-Aerial-Terrestrial Integrated Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3878--3889},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3519060},
	doi = {10.1109/TMC.2024.3519060},
	timestamp = {Fri, 30 Jan 2026 16:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGFWHG25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of Internet of Things (IoT), multi-access edge computing (MEC)-enabled satellite-aerial-terrestrial integrated network (SATIN) has emerged as a promising technology to provide massive IoT devices with seamless and reliable communication and computation services. This paper investigates the cooperation of low Earth orbit (LEO) satellites, high altitude platforms (HAPs), and terrestrial base stations (BSs) to provide relaying and computation services for vastly distributed IoT devices. Considering the uncertainty in dynamic SATIN systems, we formulate a stochastic optimization problem to minimize the time-average expected service delay by jointly optimizing resource allocation and task offloading while satisfying the energy constraints. To solve the formulated problem, we first develop a Lyapunov-based online control algorithm to decompose it into multiple one-slot problems. Since each one-slot problem is a large-scale mixed-integer nonlinear program (MINLP) that is intractable for classical computers, we further propose novel hybrid quantum-classical generalized Benders’ decomposition (HQCGBD) algorithms to solve the problem efficiently by leveraging quantum advantages in parallel computing. Numerical results validate the effectiveness of the proposed MEC-enabled SATIN schemes.}
}


@article{DBLP:journals/tmc/LiWLTC25,
	author = {Yahong Li and
                  Yingjie Wang and
                  Gang Li and
                  Xiangrong Tong and
                  Zhipeng Cai},
	title = {Determining Task Assignments for Candidate Workers Based on Trajectory
                  Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3890--3902},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3518534},
	doi = {10.1109/TMC.2024.3518534},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWLTC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of sensor-equipped mobile devices, Mobile Crowd Sensing (MCS) has emerged as an efficient method for information gathering. In smart city environmental sensing, workers can acquire data by merely being within the sensing area. Currently, most studies select opportunistic workers based on the workers’ prior preferences and ignore the effect of movement trajectories on potential opportunistic workers. This may result in the selected opportunistic workers being less-than-ideal, or even ignoring the failure of some tasks to be accomplished, thus resulting in a waste of resources. Therefore, this paper proposes a Recruitment Framework for judging Opportunistic Workers based on Movement Trajectories (RFOW-MT), a two-phase framework for worker recruitment. In the offline phase, combining the neural network model Long Short-Term Memory (LSTM) and Geohash algorithm, an algorithm to detect the set of candidate opportunistic workers is proposed, solving the problems of location privacy and search efficiency. In the online phase, in order to maximize the task spatial coverage under the task budget constraint, a task allocation algorithm based on geographic location packed grouping is proposed. Finally, RFOW-MT outperforms other methods in terms of task spatial coverage and runtime as verified by experiments on real datasets.}
}


@article{DBLP:journals/tmc/YuDCWWCDZ25,
	author = {Nan Yu and
                  Haipeng Dai and
                  Lin Chen and
                  Xiaoyu Wang and
                  Shuai Wang and
                  Guihai Chen and
                  Bin Dong and
                  Mingwei Zhang},
	title = {Fault-Tolerant Wireless Charger Placement},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3903--3917},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3517544},
	doi = {10.1109/TMC.2024.3517544},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuDCWWCDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many real-life applications, wireless chargers are deployed outdoor or in public area or even unattended environment such as hotels, restaurants, retail stores. They are exposed to various risks and malicious attacks that may break them down and further incur significant cost (e.g., battery replacement and maintenance) or performance degradation. Hence, we consider the problem of Fault-tolerant wIreless chaRger placeMent (FIRM): given a set of wireless chargers and a set of tasks to be collaboratively conducted by a set of rechargeable devices, determining where to deploy the chargers to maximize the worst-cast overall task charging utility subject to the constraint that up to  τ \\tau  chargers may break down. FIRM is a non-linear combinatorial two-level optimization problem. We first consider a relaxed version of FIRM (FIRM-R for short) corresponding to the inner optimization problem in FIRM. To address FIRM-R, we first propose an area discretization scheme to convert the infinite solution space into finite candidate positions. We then devise a power allocation method, based on which we prove that FIRM-R falls into the realm of maximizing a monotone submodular function under a uniform constraint. We then propose a constant-factor approximation algorithm to solve FIRM-R. Taking the above approximation algorithm as a subroutine, we further develop an approximation algorithm that solves FIRM with a constant-factor approximation ratio. Our extensive simulations and field experiments demonstrate that the overall charging utility of our proposed algorithm FIRM considering fault tolerance by greedy removal of  τ \\tau  chargers outperforms the that of FIRM-R without considering fault tolerance by greedy removal of  τ \\tau  chargers by at least 119.89%.}
}


@article{DBLP:journals/tmc/ChenHWFZT25,
	author = {Yange Chen and
                  SuYu He and
                  Baocang Wang and
                  Zhanshen Feng and
                  Guanghui Zhu and
                  Zhihong Tian},
	title = {A Verifiable Privacy-Preserving Federated Learning Framework Against
                  Collusion Attacks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3918--3934},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3516119},
	doi = {10.1109/TMC.2024.3516119},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenHWFZT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most of the current federated learning schemes aimed at safeguarding privacy exhibit vulnerability to collusion attacks and lack a verification mechanism for participants to consolidate the aggregation results of the parameter server, leading to privacy breaches for users and inaccurate model training outcomes. In order to address these issues, we propose a verifiable privacy-preserving federated learning framework against collusion attacks. Primarily, the federated learning scheme is reconfigured utilizing the ElGamal encryption algorithm, which effectively safeguards the data privacy of participants in scenarios involving collusion between certain participants and servers. Additionally, the introduction of the assistant server can realize the joint decryption of the gradient ciphertext by the non-collusive parameter server and assistant server, which can effectively resist the internal attack of a single parameter server model in the process of data upload. Third, this scheme designs a verification mechanism that enables participants to effectively verify the accuracy and integrality of the parameter server's aggregated results, preventing the parameter server from returning incorrect aggregation results to participants. Experimental results and performance analysis demonstrate that our proposed scheme not only fortifies security measures but also upholds the precision of model training, surpassing the security and correctness of many existing methodologies.}
}


@article{DBLP:journals/tmc/FallahMRL25,
	author = {Mohammad Amir Fallah and
                  Mehdi Monemi and
                  Mehdi Rasti and
                  Matti Latva{-}aho},
	title = {Near-Field Spot Beamfocusing: {A} Correlation-Aware Transfer Learning
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3935--3949},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3519382},
	doi = {10.1109/TMC.2024.3519382},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FallahMRL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Three-dimensional (3D) spot beamfocusing (SBF), in contrast to conventional angular-domain beamforming, concentrates radiating power within a very small volume in both radial and angular domains in the near-field zone. Recently the implementation of channel-state-information (CSI)-independent machine learning (ML)-based approaches have been developed for effective SBF using extremely large-scale programmable metasurface (ELPMs). These methods involve dividing the ELPMs into subarrays and independently training them with Deep Reinforcement Learning to jointly focus the beam at the desired focal point (DFP). This paper explores near-field SBF using ELPMs, addressing challenges associated with lengthy training times resulting from independent training of subarrays. To achieve a faster CSI-independent solution, inspired by the correlation between the beamfocusing matrices of the subarrays, we leverage transfer learning techniques. First, we introduce a novel similarity criterion based on the phase distribution image (PDI) of subarray apertures. Then we devise a subarray policy propagation scheme that transfers the knowledge from trained to untrained subarrays. We further enhance learning by introducing quasi-liquid layers as a revised version of the adaptive policy reuse technique. We show through simulations that the proposed scheme improves the training speed about 5 times. Furthermore, for dynamic DFP management, we devised a DFP policy blending process, which augments the convergence rate up to 8-fold.}
}


@article{DBLP:journals/tmc/SeyediRAL25,
	author = {Zahra Seyedi and
                  Farhad Rahmati and
                  Mohammad Ali and
                  Ximeng Liu},
	title = {Decentralized Data Integrity Inspection Offloading in Edge Computing
                  Systems Using Potential Games},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3950--3961},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3516583},
	doi = {10.1109/TMC.2024.3516583},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SeyediRAL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge storage is becoming an increasingly appealing alternative for data owners (DOs), offering benefits like decreased latency and minimized bandwidth usage compared to traditional cloud storage solutions. Nonetheless, stored data within edge servers (ESs) remains vulnerable to disruptions. Existing data integrity auditing schemes face challenges such as the costs of third-party auditors (TPA), unreliable and delayed audit results, and effective management of data inspection concerning time and energy consumption. To tackle these challenges, we introduce DIVO, a decentralized data inspection approach. DIVO leverages ESs as each others’ auditors, removing the necessity for a centralized party, thereby mitigating collision risks and potential biases in audit results. We propose a game-theoretic technique to efficiently manage data inspection and verification offloading to ESs. By formulating the decision-making issue as a strategic game for optimally allocating verification tasks among multiple ESs, we establish the presence of Nash equilibrium and design a strategy to attain it. Through comprehensive security and performance evaluations, DIVO has been shown to operate securely within the random oracle model while delivering notable efficiency improvements over recent methods. Our analysis highlights that DIVO surpasses a wide range of recent approaches in both communication and computation efficiency.}
}


@article{DBLP:journals/tmc/WangYMLC25,
	author = {Zunliang Wang and
                  Haipeng Yao and
                  Tianle Mai and
                  Zhipei Li and
                  C. L. Philip Chen},
	title = {Learning-Driven Swarm Intelligence: Enabling Deterministic Flows Scheduling
                  in {LEO} Satellite Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3962--3978},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3517618},
	doi = {10.1109/TMC.2024.3517618},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYMLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past decade, low-Earth-orbit (LEO) satellite networks have emerged as a critical infrastructure in communication systems, providing wide coverage, high reliability, and global connectivity. Recently, the development of 6G technologies has challenged the LEO satellite networks to guarantee deterministic scheduling for time-sensitive services. However, traditional deterministic networking techniques fall short for LEO satellite networks. First, these techniques impose strict time constraints, but in LEO satellite networks, delay and jitter typically range in the tens of milliseconds, which exceed these limits and render them infeasible. Second, the dynamic topologies of LEO satellite networks challenge the inflexible scheduling strategies generated by these techniques, leading to sub-optimal performance and potential strategy failures. To tackle the first problem, we propose a Cycle Specified Queuing and Forwarding (CSQF) based deterministic flows scheduling mechanism. It relaxes strict time constraints by employing cyclic multi-queue scheduling, enabling more flexible and reliable long-distance transmission. For the second problem, we propose a learning-based swarm intelligence method for deterministic flows scheduling in dynamic LEO satellite networks. It includes an algorithm that combines a Dynamic Graph Convolutional Network (DGCN) with an Adaptive Ant Colony Optimization (ACO) algorithm, referred to as the DGCN-ACO algorithm. The DGCN captures the dynamic feature of the network and generates the heuristic information. The Adaptive ACO utilizes the heuristic information and considers each flow's attribute to generate multi-path scheduling strategies for each deterministic flow, as well as updates the DGCN. The experiment results demonstrate the effectiveness of our proposed algorithm.}
}


@article{DBLP:journals/tmc/TangLGLZK25,
	author = {Fengxiao Tang and
                  Linfeng Luo and
                  Zhiqi Guo and
                  Yangfan Li and
                  Ming Zhao and
                  Nei Kato},
	title = {Semi-Distributed Network Fault Diagnosis Based on Digital Twin Network
                  in Highly Dynamic Heterogeneous Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3979--3992},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3519576},
	doi = {10.1109/TMC.2024.3519576},
	timestamp = {Mon, 26 Jan 2026 20:24:52 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangLGLZK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Highly dynamic heterogeneous networks (HDHNs), characterized by high node mobility and heterogeneity, frequently experience complex and recurrent network faults. Conventional centralized fault diagnosis methods demand real-time collection of extensive network-wide data, while distributed approaches often exhibit limited fault detection capabilities. Additionally, machine learning-based fault diagnosis methods are challenged by the scarcity of labeled fault samples required for training. To address these limitations, this study proposes a semi-distributed network fault diagnosis architecture based on a digital twin network (DTN). The proposed architecture facilitates the extraction of a comprehensive labeled fault dataset that closely replicates real-world network conditions. Using this dataset, we perform centralized training of an enhanced anomaly detection model, FTS-LSTM, to infer fault types at the node level. To overcome the drawbacks of both centralized and distributed approaches, we further introduce a semi-distributed fault diagnosis algorithm (SDFD) that integrates fault types and severity levels identified by nodes to infer overall network faults. The proposed fault diagnosis scheme is validated on a semi-physical DTN simulation platform, demonstrating its effectiveness in realistic scenarios.}
}


@article{DBLP:journals/tmc/HuWSLLWJC25,
	author = {Chuang Hu and
                  Nanxi Wu and
                  Siping Shi and
                  Xuan Liu and
                  Bing Luo and
                  Kanye Ye Wang and
                  Jiawei Jiang and
                  Dazhao Cheng},
	title = {PriFairFed: {A} Local Differentially Private Federated Learning Algorithm
                  for Client-Level Fairness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {3993--4005},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3516813},
	doi = {10.1109/TMC.2024.3516813},
	timestamp = {Tue, 11 Nov 2025 11:35:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuWSLLWJC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local Differential Privacy (LDP) is a mechanism used to protect training privacy in Federated Learning (FL) systems, typically by introducing noise to data and local models. However, in real-world distributed edge systems, the non-independent and identically distributed nature of data means that clients in FL systems experience varying sensitivities to LDP-introduced noise. This disparity leads to fairness issues, potentially discouraging marginal clients from contributing further. In this paper, we explore how to enhance client-level performance fairness under LDP conditions. We model an FL system with LDP and formulate the problem PriFair using regularization, which assigns varied noise amplitudes to clients based on federated analytics. Additionally, we develop PriFairFed, a Tikhonov regularization-based algorithm that eliminates variable dependencies and optimizes variables alternately, while also offering a theoretical privacy guarantee. We further experimented with the algorithm on a real-world system with 20 Raspberry Pi clients, showing up to a 73.2% improvement in client-level fairness compared to existing state-of-the-art approaches, while maintaining a comparable level of privacy.}
}


@article{DBLP:journals/tmc/ZhangZZYLFHCH25,
	author = {Xianglong Zhang and
                  Huanle Zhang and
                  Guoming Zhang and
                  Yanni Yang and
                  Feng Li and
                  Lisheng Fan and
                  Zhijian Huang and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {Membership Inference Attacks Against Incremental Learning in IoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4006--4021},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521216},
	doi = {10.1109/TMC.2024.3521216},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZZYLFHCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) devices are frequently deployed in highly dynamic environments and need to continuously learn new classes from data streams. Incremental Learning (IL) has gained popularity in IoT as it enables devices to learn new classes efficiently without retraining model entirely. IL involves fine-tuning the model using two sources of data: a small amount of representative samples from the original training dataset and samples from the new classes. However, both data sources are vulnerable to Membership Inference Attack (MIA). Fortunately, the existing MIAs result in poor performance against IL, because they ignore features such as the similarity between old and new models at the old classification layer. This paper presents the first MIA against IL, capable of determining not only whether a sample was used for training/fine-tuning but also distinguishing whether it belongs to the representative dataset or the new classes (unique in IL). Extensive experiments validate the effectiveness of our attack across four real-world datasets. Our attack achieves an average attack success rate of 74.03% in the white-box setting (model structure and parameters are known) and 70.08% in the black-box setting. Importantly, our attack is not sensitive to the IL hyper-parameters (e.g., distillation temperature), confirming its accurate, robust, and practical.}
}


@article{DBLP:journals/tmc/NiazZKYN25,
	author = {Fahim Niaz and
                  Jian Zhang and
                  Muhammad Khalid and
                  Muhammad Younas and
                  Ashfaq Niaz},
	title = {mmFruit: {A} Contactless and Non-Destructive Approach for Fine-Grained
                  Fruit Moisture Sensing Using Millimeter-Wave Technology},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4022--4039},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3520914},
	doi = {10.1109/TMC.2024.3520914},
	timestamp = {Thu, 01 May 2025 20:36:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NiazZKYN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensing offers a promising approach for non-destructive and contactless identification of the moisture content in fruits. Traditional methods assess fruit quality based on external features, such as color, shape, size, and texture. However, fruits often appear perfect externally while being rotten inside. Thus, accurately measuring internal conditions is crucial. This paper introduces mmFruit, a non-destructive and ubiquitous system that employs mmWave signals for precise and robust moisture level sensing in thin and thick pericarp fruits. We propose a novel dual incidence moisture estimation model for regular moisture monitoring to achieve high granularity and eliminate fruit type and size dependency. Additionally, we leverage unique reflection responses across different mmWave frequencies to provide discriminative information about fruit moisture levels. Our comprehensive theoretical model demonstrates how fruits’ refractive index, attenuation factor, and elasticity can be estimated by eliminating fruit type dependency. We developed an electric field distribution model utilizing two receiving antennas to address the challenge of varying fruit sizes through a differential approach, aiming to improve overall robustness. mmFruit integrates a customized Spatial-invariant network (SpI-Net) to eliminate interference from different frequencies and locations, ensuring stable moisture monitoring regardless of target displacement. Extensive experiments were conducted over a month in varied environments on seven types of fruits with thin and thick pericarps (apple, pear, peach, mango, orange, dragon fruit, and watermelon). The results demonstrate that mmFruit achieves a commendable RMSE of 0.276 in moisture estimation. It accurately distinguishes fruits with minor moisture level differences (0% to 7%) with 93.6% accuracy and higher moisture differences (45% to 65%) with over 95.1% accuracy, even in scenarios involving diverse displacements and rotations.}
}


@article{DBLP:journals/tmc/GuanCKYS25,
	author = {Xin Guan and
                  Zhixing Chen and
                  Yibin Kang and
                  Qi Yan and
                  Qingjiang Shi},
	title = {On User Scheduling for Fixed Wireless Access via Channel Statistics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4040--4052},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3524565},
	doi = {10.1109/TMC.2024.3524565},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuanCKYS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional multi-user scheduling in cellular networks are required to make a decision every transmission time interval (TTI) of at most several milliseconds. Only quite simple schemes can be implemented under the stringent time constraint, resulting in far-from-optimum performance. In this paper, we focus on the case of scheduling multiple users in a fixed wireless access (FWA) network with stable channel characteristics. We propose a scheduling approach by which a high-quality scheduling decision based on statistical channel state information (CSI) is made across all TTIs instead of making simple TTI-level decisions. The proposed design is essentially a mixed-integer non-smooth non-convex stochastic problem. We first replace the indicator functions in the formulation by smooth sigmoid functions to tackle nonsmoothness. By leveraging deterministic equivalents (D.E.), we then convert the original stochastic problem into an approximated deterministic one, followed by linear relaxation of the integer constraints. However, the converted problem is still nonconvex due to implicit equation constraints formerly introduced by D.E. Therefore, we employ implicit optimization technique to compute the gradient explicitly, with which we further propose an algorithm design based on a modified version of Frank–Wolfe method. Numerical results verify the effectiveness of our proposed scheme.}
}


@article{DBLP:journals/tmc/FangY25,
	author = {Xiuwen Fang and
                  Mang Ye},
	title = {Noise-Robust Federated Learning With Model Heterogeneous Clients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4053--4071},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522573},
	doi = {10.1109/TMC.2024.3522573},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FangY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables multiple devices to collaboratively train models without sharing their raw data. Considering that clients may prefer to design their own models independently, model heterogeneous FL has emerged. Additionally, due to the annotation uncertainty, the collected data usually contain unavoidable and varying noise, which cannot be effectively addressed by existing FL algorithms. This paper presents a novel solution that simultaneously handles model heterogeneity and label noise in a single framework. It is featured in three aspects: (1) For the communication between heterogeneous models, we directly align the model feedback by utilizing the easily-accessible public data, which does not require additional global models or relevant data for collaboration. (2) For internal label noise in each client, we design a dynamic label refinement strategy to mitigate the negative effects. (3) For challenging noisy feedback from other participants, we design an enhanced client confidence re-weighting scheme, which adaptively assigns corresponding weights to each client in the collaborative learning stage. Extensive experiments validate the effectiveness of our approach in mitigating the negative effects of various noise rates and types under both model homogeneous and heterogeneous FL settings.}
}


@article{DBLP:journals/tmc/GongLLDWJ25,
	author = {Jiahui Gong and
                  Yu Liu and
                  Tong Li and
                  Jingtao Ding and
                  Zhaocheng Wang and
                  Depeng Jin},
	title = {{STTF:} {A} Spatiotemporal Transformer Framework for Multi-task Mobile
                  Network Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4072--4085},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521245},
	doi = {10.1109/TMC.2024.3521245},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongLLDWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately predicting mobile traffic and accessed user amount is of great importance to network resource allocation, energy saving, etc. However, due to the complicated environmental contexts and complex interaction between mobile traffic and connected users, mobile network prediction is still challenging. Besides, the existing works could not be applied to large-scale networks because of the limited hardware resources and unacceptable time cost. In this work, we propose the spatiotemporal transformer framework for the multi-task mobile network prediction. Our proposed model contains three key parts. First, to capture the complex interaction between mobile traffic and connected users, we propose the temporal cross-attention encoder. Then, to identify and extract the most relevant information from various semantic relationships, we propose the hierarchical spatial encoder. This information is then used to create a more comprehensive representation of the network. Finally, the subgraph sampling method could significantly reduce the amount of computing power required and have comparable performance to the methods that input the whole network, enabling the model for real-world applications. Extensive experiments demonstrate that our proposed model significantly outperforms the state-of-the-art models by over 17% in both mobile traffic prediction and connected user prediction.}
}


@article{DBLP:journals/tmc/LiCZLWZ25,
	author = {Juan Li and
                  Zishang Chen and
                  Tianzi Zang and
                  Tong Liu and
                  Jie Wu and
                  Yanmin Zhu},
	title = {Reinforcement Learning-Based Dual-Identity Double Auction in Personalized
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4086--4103},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521304},
	doi = {10.1109/TMC.2024.3521304},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiCZLWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning participants have two identities: model trainers and model users. As model users, participants care most about the performance of the final model on their own distributions, which is called Personal Model Performance (PMP). This makes training a single global model to accommodate all participants impractical because the data distributions of participants are heterogeneous. As model trainers, due to high training costs, participants are reluctant to contribute models if incentives are not enough. With the combination of the above two reasons, we propose a dual-identity double auction as an incentive mechanism in personalized federated learning, allowing directional selection between model users and model trainers, both of which are served by FL participants. Within the double auction framework, we devise a reinforcement learning-based model selection method. This method selects a set of models for each buyer to bid on. The bought models are aggregated to be a personalized model to achieve higher PMP. Additionally, we implement a transaction partition-based approach for determining clearing prices and winning pairs. We address the challenge of the unavailability of private yet essential data distribution information, the coupled influence of model selection and auction results on PMP, and more utility improvement ways of multi-demand dual-identity participants. Finally, our double auction optimizes the PMP of all participants and ensures the truthfulness of multi-demand dual-identity participants, which is harder compared with single-demand single-identity participants.}
}


@article{DBLP:journals/tmc/GongYXYCYBD25,
	author = {Yongkang Gong and
                  Haipeng Yao and
                  Zehui Xiong and
                  Dongxiao Yu and
                  Xiuzhen Cheng and
                  Chau Yuen and
                  Mehdi Bennis and
                  M{\'{e}}rouane Debbah},
	title = {Multi-Modal Federated Learning Based Resources Convergence for Satellite-Ground
                  Twin Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4104--4117},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521399},
	doi = {10.1109/TMC.2024.3521399},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongYXYCYBD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite-ground twin networks (SGTNs) are regarded as a promising service paradigm, which can provide mega access services and powerful computation offloading capabilities via cloud-fog automation functions. Specifically, cloud-fog automation technologies are collaboratively leveraged to enable dense connectivity, pervasive computing, and intelligent control in terrestrial industrial cyber-physical systems, whose system-level privacy security can be strengthened via blockchain based consensus protocol. Moreover, digital twin (DT) can shorten the gap between physical unities and digital space to enable instant data mapping in SGTNs environments. However, complex multi-modal network environments, such as stochastic task size, dynamic low earth orbit location, and time-varying channel gains, hinder better performance metrics in terms of energy consumption, throughput and privacy overhead. Hence, we establish a SGTN integrated cloud-fog automation model to transfer task data to low earth orbit satellites, and then execute broad communication access, powerful computation offloading, and efficient twin control. Next, we propose a Lyapunov stability theory based multi-modal federated learning (LST-MMFL) method to optimize the battery energy, the size of block, computation frequency, and the number of twin control for minimizing the total energy consumption and privacy overhead. Furthermore, we design a novel blockchain based transaction verification protocol to strengthen privacy security, derive performance upper bounds of SGTN model, and fulfill the long-term average task as well as energy queue constraints. Finally, massive simulation results show that the proposed LST-MMFL algorithm outperforms existing state-of-the-art benchmarks in line with energy consumption, available battery level, networked control and privacy protection overhead.}
}


@article{DBLP:journals/tmc/XiaoYYWLA25,
	author = {Yang Xiao and
                  Huihan Yu and
                  Ying Yang and
                  Yixing Wang and
                  Jun Liu and
                  Nirwan Ansari},
	title = {Adaptive Joint Routing and Caching in Knowledge-Defined Networking:
                  An Actor-Critic Deep Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4118--4135},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521247},
	doi = {10.1109/TMC.2024.3521247},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoYYWLA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By integrating the software-defined networking (SDN) architecture with the machine learning-based knowledge plane, knowledge-defined networking (KDN) is revolutionizing established traffic engineering (TE) methodologies. This paper investigates the challenging joint routing and caching problem in KDN-based networks, managing multiple traffic flows to improve long-term quality-of-service (QoS) performance. This challenge is formulated as a computationally expensive non-convex mixed-integer non-linear programming (MINLP) problem, which exceeds the capacity of heuristic methods to achieve near-optimal solutions. To address this issue, we present DRL-JRC, an actor-critic deep reinforcement learning (DRL) algorithm for adaptive joint routing and caching in KDN-based networks. DRL-JRC orchestrates the optimization of multiple QoS metrics, including end-to-end delay, packet loss rate, load balancing index, and hop count. During offline training, DRL-JRC employs proximal policy optimization (PPO) to smooth the policy optimization process. In addition, the learned policy can be seamlessly integrated with conventional caching solutions during online execution. Extensive experiments demonstrate the comprehensive superiority of DRL-JRC over baseline methods in various scenarios. Meanwhile, DRL-JRC consistently outperforms the heuristic baseline under partial policy deployment during execution. Compared to the average performance of the baseline methods, DRL-JRC reduces the end-to-end delay by 51.14% and the packet loss rate by 40.78%.}
}


@article{DBLP:journals/tmc/RenZYLZH25,
	author = {Yuzheng Ren and
                  Haijun Zhang and
                  Fei Richard Yu and
                  Wei Li and
                  Pincan Zhao and
                  Ying He},
	title = {Industrial Internet of Things With Large Language Models (LLMs): An
                  Intelligence-Based Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4136--4152},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522130},
	doi = {10.1109/TMC.2024.3522130},
	timestamp = {Sun, 08 Jun 2025 12:35:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenZYLZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs), as advanced AI technologies for processing and generating natural language text, bring substantial benefits to the Industrial Internet of Things (IIoT) by enhancing efficiency, decision-making, and automation. Nevertheless, their deployment faces significant obstacles due to high computational and energy demands, which often exceed the capabilities of many industrial devices. To overcome these challenges, edge-cloud collaboration has become increasingly essential, assisting in offloading LLMs tasks to reduce the computational load. However, traditional reinforcement learning (RL)-based strategies for LLMs task offloading encounter difficulties with generalization ability and defining explicit, appropriate reward functions. Therefore, in this paper, we propose a novel framework for offloading LLMs inference tasks in IIoT, utilizing a Decentralized Identifier (DID)-based identity management system for trusted task offloading. Furthermore, we introduce an intelligence-based RL (IRL) approach, which sidesteps the need for defining specific reward functions. Instead, it uses “intelligence” as a metric to evaluate cognitive improvements and adapt to varying environmental preferences, significantly improving generalizability. In our experiments, we employ the GPT-J-6B model and utilize the Human Eval dataset to assess its ability to tackle programming challenges, demonstrating the superior performance of our proposed solution compared to existing methods.}
}


@article{DBLP:journals/tmc/LinZHYCW25,
	author = {Gengyu Lin and
                  Zhengyang Zhou and
                  Qihe Huang and
                  Kuo Yang and
                  Shifen Cheng and
                  Yang Wang},
	title = {FairSTG: Countering Performance Heterogeneity via Collaborative Sample-Level
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4153--4168},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522476},
	doi = {10.1109/TMC.2024.3522476},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinZHYCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples and challenging ones, and fairness objectives for immediately suppressing sample-level performance heterogeneity. Experiments on four spatiotemporal datasets demonstrate that our FairSTG significantly improves the fairness quality while maintaining comparable forecasting accuracy. Case studies show FairSTG can counter both spatial and temporal performance heterogeneity by our sample-level retrieval and compensation, and our work can potentially alleviate the risks on spatiotemporal resource allocation for underrepresented urban regions.}
}


@article{DBLP:journals/tmc/ZhuHLZ25a,
	author = {Shengchao Zhu and
                  Guangjie Han and
                  Chuan Lin and
                  Fan Zhang},
	title = {Underwater Multiple {AUV} Cooperative Target Tracking Based on Minimal
                  Reward Participation-Embedded {MARL}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4169--4182},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521028},
	doi = {10.1109/TMC.2024.3521028},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHLZ25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the rapid advancement of Multi-Agent Reinforcement Learning (MARL) has introduced a new paradigm for intelligent underwater target tracking within Autonomous Underwater Vehicle (AUV) cluster networks, enabling these networks to intelligently collaborate in target tracking. However, the limited scalability of MARL poses significant challenges to the performance of AUV cluster networks in tracking tasks. Specifically, MARL models trained on a fixed agents lose their effectiveness when the agent count changes, underscoring the critical need to enhance MARL’s scalability to accommodate an arbitrary number of agents. This paper addresses the pressing issue of MARL’s scalability in the context of AUV cluster network-based target tracking. Specifically, we propose an Elastic Software-Defined Multi-Agent Reinforcement Learning (ESD-MARL) architecture to enhance the scalability of AUV cluster networks. Moreover, we propose an Incremental Multi-Agent Reinforcement Learning algorithm based on Minimal Reward Participation (IMARL-MRP) that allows for the expansion of the agents without retraining. By integrating the ESD-MARL with the IMARL-MRP, we propose an elastic underwater target tracking scheme, achieving high-performance target tracking with enhanced scalability. Evaluation results demonstrate that the proposed approach effectively enhances the scalability of MARL, enabling the arbitrary expansion of the AUV cluster network, thus supporting scalable and efficient underwater target tracking.}
}


@article{DBLP:journals/tmc/WangLMTLD25,
	author = {Yunwei Wang and
                  Xinghua Li and
                  Yinbin Miao and
                  Qiuyun Tong and
                  Ximeng Liu and
                  Robert H. Deng},
	title = {{PLRQ:} Practical and Less Leakage Range Query Over Encrypted Mobile
                  Cloud Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4183--4201},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521366},
	doi = {10.1109/TMC.2024.3521366},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLMTLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a fundamental service in mobile cloud computing, range query has attracted extensive attention. But the existing secure range query schemes not only leak data privacy but also have low query efficiency. To address those issues, we first design a novel range-matched code to convert the range query into code set matching, which aims to hide the order relationship of outsourced data as well as the index of most significant different bit. Based on the designed range-matched code, we propose a Practical and Less Leakage Range Query scheme over encrypted mobile cloud data (PLRQ) by integrating XOR filter and multiset hash function. Security analysis shows that PLRQ achieves semantic security and avoids data privacy leakage. Extensive experiments using real datasets demonstrate that, compared with two state-of-the-art solutions-RngMatch and LSRQ, our proposed PLRQ improves the query efficiency both by 2 orders of magnitude, and reduces the storage cost on Cloud Service Provider by about 79.5% and 73.6% respectively.}
}


@article{DBLP:journals/tmc/LiXYZCLS25,
	author = {Danyang Li and
                  Jingao Xu and
                  Zheng Yang and
                  Yishujie Zhao and
                  Hao Cao and
                  Yunhao Liu and
                  Longfei Shangguan},
	title = {Taming Event Cameras With Bio-Inspired Architecture and Algorithm:
                  {A} Case for Drone Obstacle Avoidance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4202--4216},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521044},
	doi = {10.1109/TMC.2024.3521044},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiXYZCLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast and accurate obstacle avoidance is crucial to drone safety. Yet existing on-board sensor modules such as frame cameras and radars are ill-suited for doing so due to their low temporal resolution or limited field of view. This paper presents BioDrone, a new design paradigm for drone obstacle avoidance using stereo event cameras. At the heart of BioDrone are three simple yet effective system designs inspired by the mammalian visual system, namely, a chiasm-inspired event filtering, a lateral geniculate nucleus (LGN)-inspired event matching, and a dorsal stream-inspired obstacle tracking. We implement BioDrone on FPGA through software-hardware co-design and deploy it on an industrial drone. In comparative experiments against two state-of-the-art event-based systems, BioDrone consistently achieves an obstacle detection rate of  > >  90%, and an obstacle tracking error of  < < 5.8 cm across all flight modes with an end-to-end latency of  < < 6.4 ms, outperforming both baselines by over 44%.}
}


@article{DBLP:journals/tmc/ZhouWTQQDS25,
	author = {Jianshan Zhou and
                  Mingqian Wang and
                  Daxin Tian and
                  Kaige Qu and
                  Guixian Qu and
                  Xuting Duan and
                  Xuemin Shen},
	title = {Reliability-Optimal UAV-Assisted Mobile Edge Computing: Joint Resource
                  Allocation, Data Transmission Scheduling and Motion Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4217--4234},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521934},
	doi = {10.1109/TMC.2024.3521934},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouWTQQDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed aerial vehicles (UAVs) play a crucial role in mobile edge computing (MEC) within space-air-ground integrated networks. They serve as aerial cloudlets, enabling task processing in close proximity to ground users. While numerous joint trajectory design and resource allocation schemes aim to enhance energy efficiency or computation rate, few focus on improving system reliability, which is often challenged by stochastic channels and node mobility. This paper presents a stochastic modeling perspective to derive a system reliability expression. Our reliability formulation incorporates the impacts of stochastic Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) air-to-ground communication channels, application data load, available bandwidth, offloading time, and transmission power. This comprehensive approach leads to a reliability-oriented joint optimization model that considers not only resource allocation and user data transmission scheduling but also the motion of UAVs. To solve this problem, we propose a low-complexity algorithm. By utilizing augmented Lagrangian multipliers, the algorithm transforms nonlinear constraints into a tractable formulation, enabling the utilization of legacy unconstrained optimization techniques. We provide a proof of convergence for this algorithm. Through simulations, we demonstrate that our proposed method guarantees convergence within finite iterations and improves the average communication reliability in comparison with several other joint optimization schemes.}
}


@article{DBLP:journals/tmc/RenYZLYYL25,
	author = {Yanzhi Ren and
                  Tingyuan Yang and
                  Yufei Zhou and
                  Hongbo Liu and
                  Jiadi Yu and
                  Haomiao Yang and
                  Hongwei Li},
	title = {Two-Factor Authentication Based on Acoustic Fingerprinting in Modulation
                  Domain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4235--4247},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522077},
	doi = {10.1109/TMC.2024.3522077},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenYZLYYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The two-factor authentication (2FA) has been increasingly used with the popularity of mobile devices. Currently, many existing 2FA schemes extract the devices’ acoustic fingerprints as the second factor. Nevertheless, they mainly consider deriving fingerprints from the raw acoustic waveforms for authentication, which are susceptible to the fingerprint variations caused by the environmental noise or the varying distance between devices. To address these vulnerabilities, we propose a robust system utilizing the distortions of modulated signals, which are incurred by the acoustic elements of mobile devices, as the proof for 2FA. Specifically, our system first designs a channel delay estimation scheme to accurately estimate the propagation delay from the speaker to the microphone by deriving the phase change of the received sinusoidal signal. To perform a robust authentication, we design a new acoustic fingerprinting scheme to remove the impacts of the varying distance and environmental noise from the demodulated PSK signals for fingerprint extraction. Moreover, our device authentication component designs a transfer learning-based scheme to capture the subtle differences in devices’ fingerprints for accurate device authentication. To the best of our knowledge, this is the first 2FA system that could extract acoustic fingerprints in modulation domain and can effectively withstand the impacts of channel distortions. We also confirm the accuracy and security of our system through extensive user experiments.}
}


@article{DBLP:journals/tmc/IchkovWPS25,
	author = {Aleksandar Ichkov and
                  Alexander Wietfeld and
                  Marina Petrova and
                  Ljiljana Simic},
	title = {{HBF} {MU-MIMO} With Interference-Aware Beam Pair Link Allocation
                  for Beyond-5G mm-Wave Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4248--4262},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526547},
	doi = {10.1109/TMC.2025.3526547},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/IchkovWPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks. In order to suppress co-scheduled users’ interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation. In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks. IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring. We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance. Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access. We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks.}
}


@article{DBLP:journals/tmc/JiangWLHQH25,
	author = {Shanshan Jiang and
                  Xue Wang and
                  Junhao Lin and
                  Chongwen Huang and
                  Zhihong Qian and
                  Zhu Han},
	title = {A Delay-Oriented Joint Optimization Approach for RIS-Assisted {MEC-MIMO}
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4263--4277},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521012},
	doi = {10.1109/TMC.2024.3521012},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangWLHQH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the paper, we propose a joint optimization algorithm based on the block coordinate descent (JOABCD) algorithm for reflective intelligent surface (RIS) assisted MEC-MIMO systems. First, we define the delay minimization function for both single user with multi-antenna and multiple users with single-antenna scenarios. Since the optimization function is an NP-hard problem, we decompose it into two subproblems: computing setting and communication setting using the block coordinate descent (BCD) iterative algorithm. The subproblem of resource allocation is solved using a bisection method, while the subproblem of transmit power and phase shift matrix is solved alternately. The optimal simulation results show that the JOABCD algorithm can realize a lower time latency and a higher sum achievable rate compared with the existing methods.}
}


@article{DBLP:journals/tmc/LiauHS25,
	author = {Yue{-}Shiuan Liau and
                  Y.{-}W. Peter Hong and
                  Jang{-}Ping Sheu},
	title = {Laser-Powered {UAV} Trajectory and Charging Optimization for Sustainable
                  Data-Gathering in the Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4278--4295},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3523281},
	doi = {10.1109/TMC.2024.3523281},
	timestamp = {Thu, 01 May 2025 20:36:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiauHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work examines the trajectory design and energy charging strategy of a data-gathering unmanned aerial vehicle (UAV). The UAV utilizes laser charging from high-altitude platforms (HAPs) to replenish its battery, enabling sustained travel across multiple data-gathering points. The trajectory is determined by a sequence of hovering positions at which the UAV stays to perform both data collection and energy charging. The UAV's hovering positions affect both the sensors’ transmission rates and the laser-charging efficiency. To minimize the total task completion time, it is necessary to choose hovering positions that consider both data upload and energy charging times. In this work, we first propose the Minimum Completion Time Trajectory and Charging Optimization (MinTime-TCO) algorithm, where the hovering positions and charging energies are optimized in turn using a block coordinate descent approach. Given the UAV's hovering positions, we propose the Minimum Charge Rate Search (MCRS) algorithm to optimize the charging energies at these positions. We show that MCRS is optimal in terms of minimizing the total task completion time. Then, given the charging energies, we propose the Hovering Position Optimization (HPO) algorithm, employing successive convex approximation to address the non-convexity of the optimization problem. We also propose a low-complexity alternative based on dynamic programming to further reduce computational complexity. Simulation results demonstrate the effectiveness of the proposed algorithms against several baseline strategies.}
}


@article{DBLP:journals/tmc/WangLHZLWM25,
	author = {Shengbo Wang and
                  Chuan Lin and
                  Guangjie Han and
                  Shengchao Zhu and
                  Zhixian Li and
                  Zhenyu Wang and
                  Yunpeng Ma},
	title = {Multi-AUV Cooperative Underwater Multi-Target Tracking Based on Dynamic-Switching-Enabled
                  Multi-Agent Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4296--4311},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3521889},
	doi = {10.1109/TMC.2024.3521889},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLHZLWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, autonomous underwater vehicle (AUV) swarms are gradually becoming popular and have been widely promoted in ocean exploration or underwater tracking, etc. In this paper, we propose a multi-AUV cooperative underwater multi-target tracking algorithm especially when the real underwater factors are taken into account. We first give normally modelling approach for the underwater sonar-based detection and the ocean current interference on the target tracking process. Then, based on software-defined networking (SDN), we regard the AUV swarm as a underwater ad-hoc network and propose a hierarchical software-defined multi-AUV reinforcement learning (HSARL) architecture. Based on the proposed HSARL architecture, we propose the “Dynamic-Switching” mechanism, it includes “Dynamic-Switching Attention” and “Dynamic-Switching Resampling” mechanisms which accelerate the HSARL algorithm's convergence speed and effectively prevents it from getting stuck in a local optimum state. Additionally, we introduce the reward reshaping mechanism for further accelerating the convergence speed of the proposed HSARL algorithm in early phase. Finally, based on a proposed AUV classification method, we propose a cooperative tracking algorithm called Dynamic-Switching-Based MARL (DSBM)-driven tracking algorithm. Evaluation results demonstrate that our proposed DSBM tracking algorithm can perform precise underwater multi-target tracking, comparing with many of recent research products in terms of various important metrics.}
}


@article{DBLP:journals/tmc/ZhangHLY25,
	author = {Chuangchuang Zhang and
                  Qiang He and
                  Fuliang Li and
                  Keping Yu},
	title = {Intelligent Task Offloading and Resource Allocation in Knowledge Defined
                  Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4312--4325},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522253},
	doi = {10.1109/TMC.2024.3522253},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging architecture, edge computing enables resource limited terminal devices to offload their computation tasks to edge servers in the vicinity, to efficiently reduce delay and energy consumption. However, the continuous expansion of network scale and rapid growth of network traffic in recent years have brought huge challenges to task offloading and resource allocation. To tackle the challenges, by integrating Knowledge Defined Networking (KDN) and edge computing technologies, we design a novel Knowledge defined Edge Computing (KEC) architecture, to achieve intelligent resource allocation and task offloading in dynamic large-scale edge computing networks. We formulate the task offloading and resource allocation optimization problem, to minimize delay and energy consumption, by considering resource requirements and controller deployment. To solve it, we present an intelligent Resource Allocation based Task Offloading (TORA) mechanism, where a Multi-Agent SD3 based resource allocation (MASD3) algorithm is devised to perform efficient resource allocation. To adapt to the rapid expansion of network scale, we design a resource Allocation based Controller Deployment and task offloading Decision (DACD) algorithm, to perform the optimal controller deployment and task offloading. Extensive simulation experiments demonstrate the effectiveness and efficiency of our proposed solution, and TORA mechanism outperforms comparison mechanisms on delay and energy consumption.}
}


@article{DBLP:journals/tmc/RahmanR25,
	author = {Haseen Rahman and
                  Catherine Rosenberg},
	title = {Resource Allocation for the Uplink of a Multi-User Massive {MIMO}
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4326--4338},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3522207},
	doi = {10.1109/TMC.2024.3522207},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RahmanR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the uplink resource management of a multi-user multiple-input-multiple-output single cell for Zero-Forcing receive combining transmission. We consider jointly power allocation, user selection and modulation and coding scheme selection over multiple subchannels. Our contributions are twofold: we first propose a quasi-optimal offline algorithm that provides a target performance and then design and validate an efficient online proportional fair algorithm that performs the above steps. Due to user power constraints, the offline optimization is conducted jointly for all subchannels within a time slot, a computationally intensive task, prompting the proposal of a greedy offline algorithm that we validate in two ways: 1) for a small number of users, by solving the general problem to quasi-optimality and 2) for a larger number of users, by solving again to quasi-optimality a transformed version of the general problem when the channels are assumed flat. From the offline study, we find that, given the right user selection, equal power allocation can be employed without much degradation in performance. We also see that the number of channels allocated to users varies widely depending upon their channel gains. Using these insights, we propose our efficient real-time online algorithm that has runtime competitiveness with a state-of-the-art benchmark.}
}


@article{DBLP:journals/tmc/BinKHCL25,
	author = {Kyungmin Bin and
                  Seyeon Kim and
                  Sangtae Ha and
                  Song Chong and
                  Kyunghan Lee},
	title = {NeuroBalancer: Balancing System Frequencies With Punctual Laziness
                  for Timely and Energy-Efficient {DNN} Inferences},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4339--4354},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3524628},
	doi = {10.1109/TMC.2024.3524628},
	timestamp = {Thu, 30 Oct 2025 17:42:41 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BinKHCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On-device deep neural network (DNN) inference is often desirable for user experience and privacy. Existing solutions have fully utilized resources to minimize inference latency. However, they result in severe energy inefficiency by completing DNN inference much earlier than the required service interval. It poses a new challenge of how to make DNN inferences in a punctual and energy-efficient manner. To tackle this challenge, we propose a new resource allocation strategy for DNN processing, namely punctual laziness that disperses its workload as efficiently as possible over time within its strict delay constraint. This strategy is particularly beneficial for neural workloads since a DNN comprises a set of popular operators whose latency and energy consumption are predictable. Through this understanding, we propose NeuroBalancer, an operator-aware core and memory frequency scaling framework that balances those frequencies as efficiently as possible while making timely inferences. We implement and evaluate NeuroBalancer on off-the-shelf Android devices with various state-of-the-art DNN models. Our results show that NeuroBalancer successfully meets a given inference latency requirements while saving energy consumption up to 43.9% and 21.1% compared to the Android’s default governor and up to 42.1% and 18.6% compared to SysScale, the state-of-the-art mobile governor on CPU and GPU, respectively.}
}


@article{DBLP:journals/tmc/JiangXY25,
	author = {Changsong Jiang and
                  Chunxiang Xu and
                  Guomin Yang},
	title = {AugSSO: Secure Threshold Single-Sign-On Authentication With Popular
                  Password Collection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4355--4370},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3525453},
	doi = {10.1109/TMC.2024.3525453},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Single-sign-on authentication is widely deployed in mobile systems, which allows an identity server to authenticate a mobile user and issue her/him with a token, such that the user can access diverse mobile services. To address the single-point-of-failure problem, threshold single-sign-on authentication (PbTA) is a feasible solution, where multiple identity servers perform user authentication and token issuance in a threshold way. However, existing PbTA schemes confront critical drawbacks. Specifically, these schemes are vulnerable to perpetual secret leakage attacks (PSLA): an adversary perpetually compromises secrets of identity servers (e.g., secret key shares or credentials) to break security. Besides, they fail to achieve popular password collection, which is an effective means of enhancing system security. In this paper, we propose a secure PbTA scheme with popular password collection, dubbed AugSSO. In AugSSO, we conceive an efficient key renewal mechanism that allows identity servers to periodically update secret key shares in batches, and require storage of hardened password-derived public keys in credentials for user authentication, thereby resisting PSLA. We also present a popular password collection mechanism, where an aggregation server is introduced to identify popular passwords without disclosing unpopular ones. We provide security analysis and performance evaluation to demonstrate security and efficiency of AugSSO.}
}


@article{DBLP:journals/tmc/YuLLNNH25,
	author = {Xi Yu and
                  Tiejun Lv and
                  Weicai Li and
                  Wei Ni and
                  Dusit Niyato and
                  Ekram Hossain},
	title = {Multi-Task Semantic Communication With Graph Attention-Based Feature
                  Correlation Extraction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4371--4388},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3525477},
	doi = {10.1109/TMC.2025.3525477},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuLLNNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task semantic communication can serve multiple learning tasks using a shared encoder model. Existing models have overlooked the intricate relationships between features extracted during an encoding process of tasks. This paper presents a new graph attention inter-block (GAI) module to the encoder/ transmitter of a multi-task semantic communication system, which enriches the features for multiple tasks by embedding the intermediate outputs of encoding in the features, compared to the existing techniques. The key idea is that we interpret the outputs of the intermediate feature extraction blocks of the encoder as the nodes of a graph to capture the correlations of the intermediate features. Another important aspect is that we refine the node representation using a graph attention mechanism to extract the correlations and a multi-layer perceptron network to associate the node representations with different tasks. Consequently, the intermediate features are weighted and embedded into the features transmitted for executing multiple tasks at the receiver. Experiments demonstrate that the proposed model surpasses the most competitive and publicly available models by 11.4% on the CityScapes 2Task dataset and outperforms the established state-of-the-art by 3.97% on the NYU V2 3Task dataset, respectively, when the bandwidth ratio of the communication channel (i.e., compression level for transmission over the channel) is as constrained as  1 12 \\frac{1}{12} .}
}


@article{DBLP:journals/tmc/ChenYHWH25,
	author = {Ying Chen and
                  Yaozong Yang and
                  Jintao Hu and
                  Yuan Wu and
                  Jiwei Huang},
	title = {A Game-Theoretical Approach for Distributed Computation Offloading
                  in {LEO} Satellite-Terrestrial Edge Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4389--4402},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526200},
	doi = {10.1109/TMC.2025.3526200},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYHWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the limitations of computing resources and battery capacity, the computation tasks of ground devices can be offloaded to edge servers for processing. Moreover, with the development of the low earth orbit (LEO) satellite technology, LEO satellite-terrestrial edge computing can realize a global coverage network to provide seamless computing services beyond the regional restrictions compared to the conventional terrestrial edge computing networks. In this paper, we study the computation offloading problem in the LEO satellite-terrestrial edge computing systems. Ground devices can offload their computation tasks to terrestrial base stations (BSs) or LEO satellites deployed on edge servers for remote processing. We formulate the computation offloading problem to minimize the cost of devices while satisfying resource and LEO satellite communication time constraints. Since each ground device competes for transmission and computing resources to reduce its own offloading cost, we reformulate this problem as the LEO satellite-terrestrial computation offloading game (LSTCO-Game). It is derived that there is an upper bound on transmission interference and computing resource competition among devices. Then, we theoretically prove that at least one Nash equilibrium (NE) offloading strategy exists in the LSTCO-Game. We propose the game-theoretical distributed computation offloading (GDCO) algorithm to find the NE offloading strategy. Next, we analyze the cost obtained by GDCO's NE offloading strategy in the worst case. Experiments are conducted by comparing the proposed GDCO algorithm with other computation offloading methods. The results show that the GDCO algorithm can effectively reduce the offloading cost.}
}


@article{DBLP:journals/tmc/PanFQCR25,
	author = {Hao Pan and
                  Yongjian Fu and
                  Ye Qi and
                  Yi{-}Chao Chen and
                  Ju Ren},
	title = {MagicWrite: One-Dimensional Acoustic Tracking-Based Air Writing System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4403--4418},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526185},
	doi = {10.1109/TMC.2025.3526185},
	timestamp = {Fri, 21 Nov 2025 10:51:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PanFQCR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Air writing technology enhances text input for IoT, VR, and AR devices, offering a spatially flexible alternative to physical keyboards. Addressing the demand for such innovation, this paper presents MagicWrite, a novel system utilizing acoustic-based 1D tracking, which is suitable for mobile devices with existing speaker and microphone infrastructure. Compared to 2D or 3D tracking of the finger, 1D tracking eliminates the need for multiple microphones and/or speakers and is more universally applicable. However, challenges emerge when using 1D tracking for recognizing handwritten letters due to trajectory loss and inter-user writing variability. To address this, we develop a general conversion technique that transforms image-based text datasets (e.g., MNIST) into 1D tracking trajectory data, generating artificial datasets of tracking traces (referred to as TrackMNISTs) to bolster system robustness and scalability. These tracking datasets facilitate the creation of personalized user databases that align with individual writing habits. Combined with a kNN classifier, our proposed MagicWrite ensures high accuracy and robustness in text input recognition while simultaneously reducing computational load and energy consumption. Extensive experiments validate that our proposed MagicWrite achieves exceptional classification accuracy for unseen users and inputs in five languages, marking it as a robust solution for air writing.}
}


@article{DBLP:journals/tmc/LuoLKWW25,
	author = {Fei Luo and
                  Anna Li and
                  Salabat Khan and
                  Kaishun Wu and
                  Lu Wang},
	title = {Bi-DeepViT: Binarized Transformer for Efficient Sensor-Based Human
                  Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4419--4433},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526166},
	doi = {10.1109/TMC.2025.3526166},
	timestamp = {Fri, 09 May 2025 20:26:56 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoLKWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer architectures are popularized in both vision and natural language processing tasks, and they have achieved new performance benchmarks because of their long-term dependencies modeling, efficient parallel processing, and increased model capacity. While transformers offer powerful capabilities, their demanding computational requirements clash with the real-time and energy-efficient needs of edge-oriented human activity recognition. It is necessary to compress the transformer to reduce its memory consumption and accelerate the inference. In this paper, we investigated the binarization of a transformer-DeepViT for efficient human activity recognition. For feeding sensor signals into DeepViT, we first processed sensor signals to spectrograms by using wavelet transform. Then we applied three methods to binarize DeepViT and evaluated it on three public benchmark datasets for sensor-based human activity recognition. Compared to the full-precision DeepViT, the fully binarized one (Bi-DeepViT) reduced about 96.7% model size and 99% BOPs (Bit Operations) with only a little accuracy compromised. Furthermore, we explored the effects of binarizing various components and latent binarization of DeepViT to understand their impact on the model. We also validated the performance of Bi-DeepViTs on two wireless sensing datasets. The result shows that a certain partial binarization can improve the performance of DeepViT. Our work is the first to apply a binarized transformer in HAR.}
}


@article{DBLP:journals/tmc/XuSWHPXC25,
	author = {Cangzhu Xu and
                  Shanshan Song and
                  Xiujuan Wu and
                  Guangjie Han and
                  Miao Pan and
                  Gaochao Xu and
                  Jun{-}Hong Cui},
	title = {A High Reliable Routing Protocol Based on Spatial-Temporal Graph Model
                  for Multiple Unmanned Underwater Vehicles Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4434--4450},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526158},
	doi = {10.1109/TMC.2025.3526158},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuSWHPXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increasing demands for versatile applications have spurred the rapid development of Unmanned Underwater Vehicle (UUV) networks. Nevertheless, multi-UUV movements exacerbates the spatial-temporal variability, leading to serious intermittent connectivity of underwater acoustic channel. Such phenomena challenge the identification of reliable paths for high-dynamic network routing. Existing routing protocols overlook the effects of UUV movements on forwarding path, typically selecting forwarders based solely on the current network state, which lead to instability in packet transmission. To address these challenges, we propose a Routing protocol based on Spatial-Temporal Graph model with Q-learning for multi-UUV networks (STGR), achieving high reliable and energy effective transmission. Specifically, a distributed Spatial-Temporal Graph model (STG) is proposed to depict the evolving variation characteristics (neighbor relationships, link quality, and connectivity duration) among underwater nodes over periodic intervals. Then we design a Q-learning-based forwarder selection algorithm integrated with STG to calculate reward function, ensuring adaptability to the ever-changing conditions. We have performed extensive simulations of STGR on the Aqua-Sim-tg platform and compared with the state-of-the-art routing protocols in terms of Packet Delivery Rate (PDR), latency, energy consumption and energy balance with different network settings. The results show that STGR yields 24.32 percent higher PDR on average than them in multi-UUV networks.}
}


@article{DBLP:journals/tmc/LiGGSG25,
	author = {Jiajie Li and
                  Bo Gu and
                  Shimin Gong and
                  Zhou Su and
                  Mohsen Guizani},
	title = {Can We Enhance the Quality of Mobile Crowdsensing Data Without Ground
                  Truth?},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4451--4465},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526277},
	doi = {10.1109/TMC.2025.3526277},
	timestamp = {Sun, 26 Oct 2025 20:08:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiGGSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) has emerged as a prominent trend across various domains. However, ensuring the quality of the sensing data submitted by mobile users (MUs) remains a complex and challenging problem. To address this challenge, an advanced method is needed to detect low-quality sensing data and identify malicious MUs that may disrupt the normal operations of an MCS system. Therefore, this article proposes a prediction- and reputation-based truth discovery (PRBTD) framework, which can separate low-quality data from high-quality data in sensing tasks. First, we apply a correlation-focused spatio-temporal Transformer network that learns from the historical sensing data and predicts the ground truth of the data submitted by MUs. However, due to the noise in historical data for training and the bursty values within sensing data, the prediction results can be inaccurate. To address this issue, we use the implications among the sensing data, which are learned from the prediction results but are stable and less affected by inaccurate predictions, to evaluate the quality of the data. Finally, we design a reputation-based truth discovery (TD) module for identifying low-quality data with their implications. Given the sensing data submitted by MUs, PRBTD can eliminate the data with heavy noise and identify malicious MUs with high accuracy. Extensive experimental results demonstrate that the PRBTD method outperforms existing methods in terms of identification accuracy and data quality enhancement.}
}


@article{DBLP:journals/tmc/ZhangZYXYGD25,
	author = {Yuanhong Zhang and
                  Weizhan Zhang and
                  Muyao Yuan and
                  Liang Xu and
                  Caixia Yan and
                  Tieliang Gong and
                  Haipeng Du},
	title = {Lightweight Configuration Adaptation With Multi-Teacher Reinforcement
                  Learning for Live Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4466--4480},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526359},
	doi = {10.1109/TMC.2025.3526359},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZYXYGD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of video data and advancements in Deep Neural Networks (DNNs) have greatly boosted live video analytics, driven by the growing video capture capabilities of mobile devices. However, resource limitations necessitate the transmission of endpoint-collected videos to servers for inference. To meet real-time requirements and ensure accurate inference, it is essential to adjust video configurations at the endpoint. Traditional methods rely on deterministic strategies, posing difficulties in adapting to dynamic networks and video content. Meanwhile, emerging learning-based schemes suffer from trial-and-error exploration mechanisms, resulting in a concerning long-tail effect on upload latency. In this paper, we propose a novel lightweight and robust configuration adaptation policy (LCA), which fuses heuristic and RL-based agents using multi-teacher knowledge distillation (MKD) theory. First, we propose a content-sensitive and bandwidth-adaptive RL agent and introduce a Lyapunov-based optimization agent for ensuring latency robustness. To leverage both agents’ strengths, we design a feature-guided multi-teacher distillation network to transfer their advantages to the student. The experimental results across two vision tasks (pose estimation and semantic segmentation) demonstrate that LCA significantly reduces transmission latency compared to prior work (average reduction of 47.11%-89.55%, 95-percentile reduction of 27.63%-88.78%) and computational overhead while maintaining comparable inference accuracy.}
}


@article{DBLP:journals/tmc/BaiWSH25,
	author = {Shiyu Bai and
                  Weisong Wen and
                  Dongzhe Su and
                  Li{-}Ta Hsu},
	title = {Graph-Based Indoor 3D Pedestrian Location Tracking With Inertial-Only
                  Perception},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4481--4495},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526196},
	doi = {10.1109/TMC.2025.3526196},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaiWSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pedestrian location tracking in emergency responses and environmental surveys of indoor scenarios tend to rely only on their own mobile devices, reducing the usage of external services. Low-cost and small-sized inertial measurement units (IMU) have been widely distributed in mobile devices. However, they suffer from high-level noises, leading to drift in position estimation over time. In this work, we present a graph-based indoor 3D pedestrian location tracking with inertial-only perception. The proposed method uses onboard inertial sensors in mobile devices alone for pedestrian state estimation in a simultaneous localization and mapping (SLAM) mode. It starts with a deep vertical odometry-aided 3D pedestrian dead reckoning (PDR) to predict the position in 3D space. Environment-induced behaviors, such as corner-turning and stair-taking, are regarded as landmarks. Multi-hypothesis loop closures are formed using statistical methods to handle ambiguous data association. A factor graph optimization fuses 3D PDR and behavior loop closures for state estimation. Experiments in different scenarios are performed using a smartphone to evaluate the performance of the proposed method, which can achieve better location tracking than current learning-based and filtering-based methods. Moreover, the proposed method is also discussed in different aspects, including the accuracy of offline optimization and proposed height regression, and the reliability of the multi-hypothesis behavior loop closures. The video (YouTube) or (BiliBili) is also shared to display our research.}
}


@article{DBLP:journals/tmc/LuHCLLL25,
	author = {Anqi Lu and
                  Youbing Hu and
                  Zhiqiang Cao and
                  Jie Liu and
                  Lingzhi Li and
                  Zhijun Li},
	title = {Enhancing Remote Sensing Image Scene Classification With Satellite-Terrestrial
                  Collaboration and Attention-Aware Transmission Policy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4496--4509},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526142},
	doi = {10.1109/TMC.2025.3526142},
	timestamp = {Thu, 13 Nov 2025 10:48:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuHCLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advancements in Earth observation sensors on low Earth orbit (LEO) satellites have significantly increased the volume of remote sensing images. This growth has led to challenges such as higher storage demands, downlink bandwidth stress, and transmission delays, particularly for real-time remote sensing image scene classification (RSISC). To address this, we propose a novel Satellite-Terrestrial Collaborative Scene Classification (STCSC) framework that integrates transmission and computation. The framework employs an attention-aware policy on the satellite, which adaptively determines the sequence of images and selection of image blocks for transmission, as well as these blocks’ sampling rates. This policy is based on image complexity and the real-time data transmission rate, prioritizing blocks crucial for downstream tasks. On the ground, a classification model processes the received image blocks, balancing classification accuracy and transmission delay. Moreover, we have developed a comprehensive simulation system to validate the performance of our framework, including simulations of the satellite, transmission, and ground modules. Simulation results demonstrate that our STCSC framework can reduce transmission delay by 76.6% while enhancing classification accuracy on the ground by 0.6%. Additionally, our attention-aware policy is compatible with any ground classification model.}
}


@article{DBLP:journals/tmc/ChingCKWCSH25,
	author = {Cheng{-}Wei Ching and
                  Xin Chen and
                  Chaeeun Kim and
                  Tongze Wang and
                  Dong Chen and
                  Dilma Da Silva and
                  Liting Hu},
	title = {AgileDART: An Agile and Scalable Edge Stream Processing Engine},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4510--4528},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526143},
	doi = {10.1109/TMC.2025.3526143},
	timestamp = {Thu, 22 Jan 2026 09:01:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChingCKWCSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge applications generate a large influx of sensor data on massive scales, and these massive data streams must be processed shortly to derive actionable intelligence. However, traditional data processing systems are not well-suited for these edge applications as they often do not scale well with a large number of concurrent stream queries, do not support low-latency processing under limited edge computing resources, and do not adapt to the level of heterogeneity and dynamicity commonly present in edge computing environments. As such, we present AgileDart, an agile and scalable edge stream processing engine that enables fast stream processing of many concurrently running low-latency edge applications’ queries at scale in dynamic, heterogeneous edge environments. The novelty of our work lies in a dynamic dataflow abstraction that leverages distributed hash table-based peer-to-peer overlay networks to autonomously place, chain, and scale stream operators to reduce query latencies, adapt to workload variations, and recover from failures and a bandit-based path planning model that re-plans the data shuffling paths to adapt to unreliable and heterogeneous edge networks. We show that AgileDart outperforms Storm and EdgeWise on query latency and significantly improves scalability and adaptability when processing many real-world edge stream applications’ queries.}
}


@article{DBLP:journals/tmc/ZengBTWH25,
	author = {Dake Zeng and
                  Akhtar Badshah and
                  Shanshan Tu and
                  Muhammad Waqas and
                  Zhu Han},
	title = {A Security-Enhanced Ultra-Lightweight and Anonymous User Authentication
                  Protocol for Telehealthcare Information Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4529--4542},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526519},
	doi = {10.1109/TMC.2025.3526519},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZengBTWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in smartphone and wearable device usage has propelled the advancement of the Internet of Things (IoT) applications. Among these, e-healthcare stands out as a fundamental service, enabling the remote access and storage of patient-related data on a centralized medical server (MS), and facilitating connections between authorized individuals such as doctors, patients, and nurses over the public Internet. However, the inherent vulnerability of the public Internet to diverse security threats underscores the critical need for a robust and secure user authentication protocol to safeguard these essential services. This research presents a novel, resource-efficient user authentication protocol specifically designed for healthcare systems. Our proposed protocol leverages the lightweight authenticated encryption with associated data (AEAD) primitive Ascon combined with hash functions and XoR, specifically tailored for encrypted communication in resource-constrained IoT devices, emphasizing resource efficiency. Additionally, the proposed protocol establishes secure session keys between users and MS, facilitating future encrypted communications and preventing unauthorized attackers from illegally obtaining users’ private data. Furthermore, comprehensive security validation, including informal security analyses, demonstrates the protocol's resilience against a spectrum of security threats. Extensive analysis reveals that our proposed protocol significantly reduces computational and communication resource requirements during the authentication phase in comparison to similar authentication protocols, underscoring its efficiency and suitability for deployment in healthcare systems.}
}


@article{DBLP:journals/tmc/AiBTWA25,
	author = {Xin Ai and
                  Akhtar Badshah and
                  Shanshan Tu and
                  Muhammad Waqas and
                  Iftekhar Ahmad},
	title = {An Improved Ultra-Lightweight Anonymous Authenticated Key Agreement
                  Protocol for Wearable Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4543--4557},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526076},
	doi = {10.1109/TMC.2025.3526076},
	timestamp = {Thu, 01 May 2025 20:36:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AiBTWA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For wearable devices with constrained computational resources, it is typically required to offload processing tasks to more capable servers. However, this practice introduces vulnerabilities to data confidentiality and integrity due to potential malicious network attacks, unreliable servers, and insecure communication channels. A robust mechanism that ensures anonymous authentication and key agreement is therefore imperative for safeguarding the authenticity of computing entities and securing data during transmission. Recently, Guo et al. proposed an anonymous authentication key agreement and group proof protocol specifically designed for wearable devices. This protocol, benefiting from the strengths of previous research, is designed to thwart a variety of cyber threats. However, inaccuracies in their protocol lead to issues with authenticity verification, ultimately preventing the establishment of secure session keys between communication entities. To address these design flaws, an improved ultra-lightweight protocol was proposed, employing cryptographic hash functions to ensure authentication and privacy during data transmission in wearable devices. Supported by rigorous security validations and analyses, the proposed protocol significantly boosts both security and efficiency, marking a substantial advancement over prior methodologies.}
}


@article{DBLP:journals/tmc/XuCLJZW25,
	author = {Yongliang Xu and
                  Hang Cheng and
                  Ximeng Liu and
                  Changsong Jiang and
                  Xinpeng Zhang and
                  Meiqing Wang},
	title = {{PCSE:} Privacy-Preserving Collaborative Searchable Encryption for
                  Group Data Sharing in Cloud Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4558--4572},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526232},
	doi = {10.1109/TMC.2025.3526232},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuCLJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative searchable encryption for group data sharing enables a consortium of authorized users to collectively generate trapdoors and decrypt search results. However, existing countermeasures may be vulnerable to a keyword guessing attack (KGA) initiated by malicious insiders, compromising the confidentiality of keywords. Simultaneously, these solutions often fail to guard against hostile manufacturers embedding backdoors, leading to potential information leakage. To address these challenges, we propose a novel privacy-preserving collaborative searchable encryption (PCSE) scheme tailored for group data sharing. This scheme introduces a dedicated keyword server to export server-derived keywords, thereby withstanding KGA attempts. Based on this, PCSE deploys cryptographic reverse firewalls to thwart subversion attacks. To overcome the single point of failure inherent in a single keyword server, the export of server-derived keywords is collaboratively performed by multiple keyword servers. Furthermore, PCSE extends its capabilities to support efficient multi-keyword searches and result verification and incorporates a rate-limiting mechanism to effectively slow down adversaries’ online KGA attempts. Security analysis demonstrates that our scheme can resist KGA and subversion attack. Theoretical analyses and experimental results show that PCSE is significantly more practical for group data sharing systems compared with state-of-the-art works.}
}


@article{DBLP:journals/tmc/ZhongKWYNNGX25,
	author = {Yue Zhong and
                  Jiawen Kang and
                  Jinbo Wen and
                  Dongdong Ye and
                  Jiangtian Nie and
                  Dusit Niyato and
                  Xiaozheng Gao and
                  Shengli Xie},
	title = {Generative Diffusion-Based Contract Design for Efficient {AI} Twin
                  Migration in Vehicular Embodied {AI} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {5},
	pages = {4573--4588},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526230},
	doi = {10.1109/TMC.2025.3526230},
	timestamp = {Fri, 09 May 2025 20:26:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhongKWYNNGX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embodied Artificial Intelligence (AI) bridges the cyberspace and the physical space, driving advancements in autonomous systems like the Vehicular Embodied AI NETwork (VEANET). VEANET integrates advanced AI capabilities into vehicular systems to enhance autonomous operations and decision-making. Embodied agents, such as Autonomous Vehicles (AVs), are autonomous entities that can perceive their environment and take actions to achieve specific goals, actively interacting with the physical world. Embodied Agent Twins (EATs) are digital models of these embodied agents, with various Embodied Agent AI Twins (EAATs) for intelligent applications in cyberspace. In VEANETs, EAATs act as in-vehicle AI assistants to perform diverse tasks supporting autonomous driving using generative AI models. Due to limited onboard computational resources, AVs offload EAATs to nearby RoadSide Units (RSUs). However, the mobility of AVs and limited RSU coverage necessitates dynamic migrations of EAATs, posing challenges in selecting suitable RSUs under information asymmetry. To address this, we construct a multi-dimensional contract theoretical model between AVs and alternative RSUs. Considering that AVs may exhibit irrational behavior, we utilize prospect theory instead of expected utility theory to model the actual utilities of AVs. Finally, we employ a Generative Diffusion Model (GDM)-based algorithm to identify the optimal contract designs, thus enhancing the efficiency of EAAT migrations. Numerical results demonstrate the superior efficiency of the proposed GDM-based scheme in facilitating EAAT migrations compared with traditional deep reinforcement learning methods.}
}


@article{DBLP:journals/tmc/ZhangZWLSC25,
	author = {Songli Zhang and
                  Zhenzhe Zheng and
                  Fan Wu and
                  Bingshuai Li and
                  Yunfeng Shao and
                  Guihai Chen},
	title = {{MIDDLE:} {A} Mobility-Driven Device-Edge-Cloud Federated Learning
                  Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4589--4606},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543723},
	doi = {10.1109/TMC.2025.3543723},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZWLSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) can be implemented in large-scale wireless networks in a hierarchical way, introducing edge servers as relays between the cloud server and devices. These devices are dispersed within multiple clusters coordinated by edges. However, the devices are typically mobile users with unpredictable trajectories, and the impact of their mobility on the model training process is not well-studied. In this work, we propose a new MobIlity-Driven feDerated LEarning framework, namely MIDDLE. MIDDLE addresses unbalanced model updates by capitalizing on model aggregation opportunities on mobile devices due to their mobility across edges. It consists of two components: on-device model aggregation, which aggregates models from different edges carried by mobile devices as they move across edges, and in-edge device selection, adjusting the current edge optimization direction through careful device selection. Theoretical analysis emphasizes that on-device model aggregation can reduce bias in model updating on edges and the cloud, thereby accelerating the FL model convergence. Building on this analysis, we introduce on-device global control averaging, modifying the training process on mobile devices and extending MIDDLE into  MIDDLE + \\text{MIDDLE}^{+} . Extensive experimental results validate that MIDDLE and  MIDDLE + \\text{MIDDLE}^{+}  can reduce the time steps to reach the target accuracy by 19.44% and 20.37% at least, respectively.}
}


@article{DBLP:journals/tmc/OthmanZMXHAZL25,
	author = {Wajdy Othman and
                  Hong Zhong and
                  Fuyou Miao and
                  Kaiping Xue and
                  Ammar Hawbani and
                  Ruhul Amin and
                  Liang Zhao and
                  Tao Li},
	title = {{CRT} and PUF-Based Self/Mutual-Healing Key Distribution Protocol
                  With Collusion Resistance and Revocation Capability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4607--4622},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528491},
	doi = {10.1109/TMC.2025.3528491},
	timestamp = {Tue, 22 Jul 2025 18:42:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OthmanZMXHAZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-healing group key distribution (SGKD) protocols guarantee the security of group communications by allowing authorized users to independently recover missed previous session keys from the current broadcast without retransmission. However, existing SGKD protocols have flaws: (1) collusion resistance and revocable nodes are both upper-bounded by the degree of polynomials used, (2) the disclosure of personal secrets enables the recovery of group key, (3) temporary revocation of a group member is not possible, and (4) a revoked node may obtain the session key when initiating mutual healing, moreover, a malicious node may cause the recovery of false group keys. To address these limitations, we propose an SGKD protocol using the Chinese remainder theorem (CRT) and Physical Unclonable Function (PUF). Our proposed SGKD protocol generates a PUF-based dynamic secret by stimulating nodes’ PUF using a polynomial-based encrypted challenge. This secret is then employed to retrieve a CRT-based encrypted group key. By combining PUF and CRT, we can generate dynamic secrets on the fly and reduce computation time significantly. Utilizing such a technique, our protocol achieves superior security goals, including resistance to any coalition of group nodes even if nodes’ personal secrets were disclosed. Furthermore, the proposed protocol provides an unlimited number of revocable nodes. Additionally, a revoked node can rejoin its group in later sessions without affecting backward secrecy. Moreover, the protocol provides a backward secrecy guaranteed mutual-healing feature free from desynchronization. Our performance and security analyses (i.e., theorem-based formal analysis, NS3-based experiment, and formal verification using the AVIPSA tool) show that our proposed protocol achieves stronger security goals and better efficiency in terms of computation, communication, and storage costs compared to existing SGKD schemes.}
}


@article{DBLP:journals/tmc/JuburSR25,
	author = {Mohammed Jubur and
                  Nitesh Saxena and
                  Faheem Ahmad Reegu},
	title = {Usability and Security Analysis of the Compare-and-Confirm Method
                  in Mobile Push-Based Two-Factor Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4623--4638},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3524093},
	doi = {10.1109/TMC.2024.3524093},
	timestamp = {Sat, 31 May 2025 23:18:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JuburSR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Push-based two-factor authentication (2FA) methods, such as the ”Just-Confirm” approach, are popular due to their user-friendly design, requiring users to simply approve or deny a push notification on their mobile device. However, these methods are vulnerable to ”concurrency attacks,” where an attacker attempts to log in immediately after the legitimate user, causing multiple push notifications that may lead to users inadvertently approving fraudulent access. This vulnerability arises because the login notifications are not uniquely bound to individual login attempts. To address this issue, Push-Compare-and-Confirm 2FA method enhances security by associating each login notification with a unique code displayed on both the authentication terminal and the push notification. Users are required to match these codes before confirming access, thereby binding the notification to a specific login attempt. Recognizing the ubiquity of mobile devices in daily life, we conducted a comprehensive user study with 65 participants to evaluate the usability and security of Push-Compare-and-Confirm. The study considered two scenarios: one where the user’s second-factor device (phone) is physically separate from the authentication terminal (e.g., logging in on a PC and confirming on the phone), and another where the phone serves as both the authentication terminal and the second-factor device. Participants completed 24 login trials, including both benign and attack scenarios, with varying code lengths (four characters and six characters). Our results indicate that while Push-Compare-and-Confirm maintains high usability in benign scenarios, with True Positive Rates (TPR) exceeding 95%, it presents significant challenges in attack detection. Participants correctly identified only about 50% of fraudulent login attempts, indicating a substantial vulnerability remains. These findings suggest that although Push-Compare-and-Confirm enhances security over standard push-based 2FA methods, additional measures—such as more intuitive interface designs, clearer visual cues, and user education on the importance of code verification—are necessary to improve attack detection rates without compromising usability.}
}


@article{DBLP:journals/tmc/ZhuangZYZCSCMH25,
	author = {Hui Zhuang and
                  Yihang Zhang and
                  Yanni Yang and
                  Guoming Zhang and
                  Zhe Chen and
                  Riccardo Spolaor and
                  Xiuzhen Cheng and
                  Prasant Mohapatra and
                  Pengfei Hu},
	title = {AccEmo: Accelerometer Based Human Emotion Recognition for Eyewear
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4639--4650},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529662},
	doi = {10.1109/TMC.2025.3529662},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuangZYZCSCMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing popularity of virtual reality applications, there is an increasing demand for more interactive entertainment, learning, social interactions, and other activities on eyewear devices. Recognizing users’ emotion and providing reliable feedback can significantly improve the immersive experience for users. However, previous works in emotion recognition required modifications to existing eyewear devices and the integration of additional sensors, or relied on specialized sensors in expensive commercial-grade eyewear devices, making direct deployment on existing consumer-grade eyewear devices challenging. In this paper, we propose AccEmo, the first system that analyzes the data from the built-in accelerometer sensor on eyewear devices to accurately recognize human emotion. AccEmo first employs signal processing technologies to process raw accelerometer data, and then uses a binary classification network to determine whether the accelerometer data is influenced by emotional changes. Subsequently, AccEmo proposes a network architecture based on residual neural network and channel-wise attention mechanism as a universal feature extractor to extract complex features related to human emotions from the accelerometer data. Finally, AccEmo uses personalized classifiers to achieve emotion recognition for different users. Extensive performance evaluation of AccEmo across diverse users demonstrates an exceptional average accuracy of 94.3%. Additionally, the robustness of AccEmo is validated through evaluations in various scenarios, yielding promising results.}
}


@article{DBLP:journals/tmc/ZhangCLYGQ25,
	author = {Yajing Zhang and
                  Cailian Chen and
                  Mingyan Li and
                  Chaoqun You and
                  Xinping Guan and
                  Tony Q. S. Quek},
	title = {Online Flow Scheduling in Virtualized Time- Sensitive Networks: {A}
                  Joint Admission Control and {VNF} Embedding Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4651--4667},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527749},
	doi = {10.1109/TMC.2025.3527749},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCLYGQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-sensitive networking (TSN) is proposed to satisfy the increasingly stringent demands of Industrial 4.0 for deterministic transmission. This is achieved by generating a series of centrally configured gate control lists to strictly restrict the forwarding time of arriving flows. However, such a centralized scheme requires prior information of all flows, severely impeding TSN from providing an online response to dynamic industrial applications. To solve this problem, we innovatively propose to use admission control (AC) to realize deterministic transmission in the virtualized TSN network. In this approach, AC is distributively executed on each node and link, whereby flows of applications are served by passing through a series of virtual network functions (VNFs). This distributed AC execution is regarded as a VNF embedding (VNE) process. Specifically, we propose a two-stage online framework, Smart Admission Control (SmartAC), to cater to dynamic applications. The first stage, referred to as the static stage, obtains a deterministic VNE solution by synthesizing AC decisions of individual TSN nodes and links. The second stage, referred to as the dynamic stage, fine-tunes VNE solutions obtained from the static stage to adapt to the harsh environment with insufficient resources or limited VNF migration budgets. Simulation results demonstrate the effectiveness of SmartAC in improving response rate and resource utilization ratio. Notably, SmartAC reduces runtime by 90% compared to existing algorithms and exhibits robustness across different network topologies.}
}


@article{DBLP:journals/tmc/IslamXUZKG25,
	author = {Md. Rashedul Islam and
                  Yong Xiang and
                  Md Palash Uddin and
                  Yao Zhao and
                  Jonathan Kua and
                  Longxiang Gao},
	title = {Multiple Edge Data Integrity Verification With Multi-Vendors and Multi-Servers
                  in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4668--4683},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528016},
	doi = {10.1109/TMC.2025.3528016},
	timestamp = {Fri, 30 Jan 2026 19:34:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/IslamXUZKG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensuring Edge Data Integrity (EDI) is imperative in providing reliable and low-latency services in mobile edge computing. Existing EDI schemes typically address single-vendor (App Vendor, AV) single-server (Edge Server, ES), single-vendor multi-server, and multi-vendor multi-server scenarios, which consider a single data replica cached by an ES from the AVs. However, the most practical scenario of Multi-Vendors and Multi-Servers with Multiple Data (MVMS-MD) cached by an ES from different AVs remains unexplored. Current solutions struggle when applied to this scenario due to increased computation and communication costs in the verification process across all ESs using the classical challenge-response per-data multi-round strategy. To tackle this issue, we propose a Multiple EDI-Verification (MEDI-V) approach in this paper. In particular, our MEDI-V utilizes an adaptive Merkle Hash Tree (ad-MHT) to efficiently generate a tree of multiple data replicas within each AV. Next, the dynamic mechanism computes minimal verification information using ad-MHT to create a challenge for individual ESs to produce EDI proofs. The ES then leverages its ad-MHT and the ES’s proof to send the reconstructed ad-MHT root to the AV for verification. Theoretical insights into MEDI-V’s correctness, efficiency, security, and comprehensive evaluations demonstrate its superiority in addressing MEDI issues in the MVMS-MD scenario.}
}


@article{DBLP:journals/tmc/PanLCLTL25,
	author = {Bangqi Pan and
                  Jianfeng Lu and
                  Shuqin Cao and
                  Jing Liu and
                  Wenlong Tian and
                  Minglu Li},
	title = {Bilateral Pricing for Dynamic Association in Federated Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4684--4697},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527048},
	doi = {10.1109/TMC.2025.3527048},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PanLCLTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Devices and servers in Federated Edge Learning (FEL) are self-interested and resource-constrained, making it critical to design incentives to improve model performance. However, dynamic network conditions raise energy consumption, while data heterogeneity undermines device cooperation. Current research overlooks the interplay between system efficiency and device clustering, resulting in suboptimal updates. To address these challenges, we develop BENCH, a bilateral pricing mechanism consisting of three core rules aimed at incentivizing participation from both devices and servers. Specifically, we first design a reward allocation rule, based on the Rubinstein bargaining model, which dynamically allocates rewards. Theoretically, we derive a closed-form solution for this rule, demonstrating BENCH achieves Nash equilibrium. Secondly, we design a device partitioning rule that leverages modularity to group similar devices, facilitating personalized edge aggregation to accelerate local data adaptation. Thirdly, we design an edge matching rule that employs the Kuhn-Munkres algorithm to balance the load at edge servers, thus minimizing the congestion. Together, these three rules enable hierarchical optimization of pricing and associations, effectively mitigating the impact of dynamic costs and device heterogeneity. Extensive experiments demonstrate BENCH’s effectiveness in increasing device participation by 28.81% and improving model performance by 2.66% compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/ZhangAWR25,
	author = {Zhou Zhang and
                  Saman Atapattu and
                  Yizhu Wang and
                  Marco Di Renzo},
	title = {Distributed {MAC} for RIS-Assisted Multiuser Networks: {CSMA/CA} Protocol
                  Design and Statistical Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4698--4715},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530851},
	doi = {10.1109/TMC.2025.3530851},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangAWR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This research focuses on the challenges of distributed Medium Access Control (MAC) protocols involving Reconfigurable Intelligent Surfaces (RISs), which are still in early development. The study explores optimal channel access for multiple source-destination pairs in distributed networks with the assistance of multiple RISs. Three key issues are addressed: joint scheme for channel contention, RISs’ channel state information (CSI) acquisition, and RIS-assisted channel access; tradeoff between overhead and effective data transmission; and low-complexity distributed network operation. To achieve maximum network throughput, the paper proposes an optimal distributed Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) strategy with opportunistic RIS assistance based on statistical optimization. The proposed MAC strategy's optimality in terms of average network throughput is rigorously derived. Closed-form expressions for threshold functions of rewards for making decisions are derived, and an easy-to-implement distributed channel access algorithm is provided with online complexity  O ( 2 L ) \\mathcal {O}(2^{L}) , where  L L  denotes the number of distributed RISs. The proposed MAC strategy is then refined, and a low-complexity distributed algorithm is developed with complexity  O ( L ) \\mathcal {O}(L) . Numerical simulations verify the theoretical results, demonstrating the efficiency of the proposed strategy. This work introduces innovative solutions and analytical frameworks for the distributed MAC problem with RIS assistance, significantly advancing existing research.}
}


@article{DBLP:journals/tmc/ChangXXN25,
	author = {Zikai Chang and
                  Wei Xiong and
                  Ning Xie and
                  Dusit Niyato},
	title = {Improving Wireless Security With Phase-Tag Physical-Layer Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4716--4732},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527220},
	doi = {10.1109/TMC.2025.3527220},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChangXXN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authentication is a fundamental requirement and a crucial topic in wireless communications. This paper focuses on improving the security of the prior tag-based Physical-Layer Authentication (PLA) schemes. We propose two phase-tag-based PLA schemes to overcome the limitations of prior tag-based PLA schemes. First, we propose the Phase-Tag-based PLA (PT-PLA) scheme, which superimposes a tag to the phase of the transmitted signal rather than the amplitude. Second, we propose the Adaptive Phase-Tag-based PLA (APT-PLA) scheme, where an appropriate parameter of the PT-PLA scheme is adaptively set for achieving a better trade-off among robustness, security, and compatibility. Rigorous theoretical analyses of the proposed schemes are conducted in high-order modulation systems, such as  M M -PSK and  M {M} -QAM systems, with a focus on evaluating their robustness, security, and compatibility. We also discuss advantages and disadvantages of these schemes and offer helpful recommendations according to different scenarios. Furthermore, we implement the proposed schemes and perform extensive simulations to compare their performance comprehensively. Simulation results demonstrate a perfect match with the theoretical results, verifying the superiority of the proposed schemes over the prior schemes.}
}


@article{DBLP:journals/tmc/YangLSDRWW25,
	author = {Wei Yang and
                  Chi Lin and
                  Yu Sun and
                  Haipeng Dai and
                  Jiankang Ren and
                  Lei Wang and
                  Guowei Wu},
	title = {Accurate 3D Wireless Charging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4733--4746},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526945},
	doi = {10.1109/TMC.2025.3526945},
	timestamp = {Thu, 06 Nov 2025 14:15:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangLSDRWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Rechargeable Sensor Networks (WRSNs) have become an important research issue as they can overcome the energy bottleneck problem of wireless sensor networks. However, existing 2D charging methods suffer from significant errors in 3D scenarios, which leads to a huge gap between theoretical results and practical applications, hindering the widespread adoption of WRSNs. In this paper, we address the chargIng utility maximizatioN problem In 3D environmenT (INIT) and provide a general solution suitable for any type of transceiver antenna. Specifically, we first establish an accurate 3D charging model to quantify the received power of sensors in 3D environments. Second, we design an angle-distance discretization scheme to determine appropriate charging spots for the Mobile Charger (MC). Then, we transform the mobile charging problem into a submodular function maximization problem and propose an approximation algorithm with guaranteed performance to solve it. Finally, our method has been extensively evaluated through experiments and simulations and has demonstrated considerable advantages over other comparison algorithms in real-world 3D environments. On average, it has achieved an impressive 34.8% improvement in charging utility and a remarkable 56.1% reduction in the number of dead sensors.}
}


@article{DBLP:journals/tmc/XuNZ25,
	author = {Renjie Xu and
                  Keivan Nalaie and
                  Rong Zheng},
	title = {FastTuner: Fast Resolution and Model Tuning for Multi-Object Tracking
                  in Edge Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4747--4761},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526573},
	doi = {10.1109/TMC.2025.3526573},
	timestamp = {Sat, 31 May 2025 23:18:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuNZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-object tracking (MOT) is the “killer app” of edge video analytics. Deploying MOT pipelines for live video analytics poses a significant system challenge due to their computation-intensive nature. In this paper, we propose FastTuner, a model-agnostic framework that aims to accelerate MOT pipelines by adapting frame resolutions and backbone models. Unlike prior works that utilize a separate and time-consuming online profiling procedure to identify the optimal configuration, FastTuner incorporates multi-task learning to perform configuration selection and object tracking through a shared model. Multi-resolution training is employed to further improve the tracking accuracy across different resolutions. Furthermore, two workload placement schemes are designed for the practical deployment of FastTuner in edge video analytics systems. Extensive experiments demonstrate that FastTuner can achieve 1.1%–9.2% higher tracking accuracy and 2.5%–25.5% higher speed compared to the state-of-the-art methods, and accelerate end-to-end processing by 1.7%–22.5% in a real-world testbed consisting of an embedded device and an edge server.}
}


@article{DBLP:journals/tmc/WuXCZQDC25,
	author = {Jintao Wu and
                  Xiaolong Xu and
                  Guangming Cui and
                  Yiwen Zhang and
                  Lianyong Qi and
                  Wanchun Dou and
                  Zhipeng Cai},
	title = {Fairness-Aware Budgeted Edge Server Placement for Connected Autonomous
                  Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4762--4776},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526873},
	doi = {10.1109/TMC.2025.3526873},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuXCZQDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) considerably enhances the capabilities and performance of connected autonomous vehicles (CAVs) by deploying edge servers (ESs) on roadside units (RSUs) near CAVs, thereby ensuring low-latency services. Given the constrained and costly nature of ES resources (computing, storage, and bandwidth), equitable ES utilization is critical for CAV operations. However, fairness considerations are often overlooked in current budgeted edge server placement (ESP) strategies, potentially worsening resource imbalances and compromising user experience. This paper investigates the fairness-aware budgeted edge server placement (FESP) problem within RSUs, proving its NP-hardness. To address FESP, we first propose FESP-O, an integer programming-based optimal approach for small-scale problems, followed by FESP-APX, an approximation approach for large-scale scenarios that provides near-optimal solutions. We analyze the time complexity and approximation ratio of our proposed algorithms and validate their efficacy through experiments on real-world datasets. Extensive experimental results demonstrate significant performance improvements over baseline and state-of-the-art methods, indicating practical suitability and efficiency.}
}


@article{DBLP:journals/tmc/WuZSYGWLLHSC25,
	author = {Zhi Wu and
                  Dongheng Zhang and
                  Zixin Shang and
                  Yuqin Yuan and
                  Hanqin Gong and
                  Binquan Wang and
                  Zhi Lu and
                  Yadong Li and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {Learning-Based Tracking-Before-Detect for Unconstrained Indoor Human
                  Tracking Using {RF} Signal},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4777--4793},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529501},
	doi = {10.1109/TMC.2025.3529501},
	timestamp = {Mon, 19 May 2025 14:32:20 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZSYGWLLHSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human tracking plays a crucial role in various wireless sensing applications. However, recent advancements have primarily focused on constrained experimental scenarios with less interference, often involving a few individuals performing actions in an empty space without obstacles. In empirical unconstrained scenarios, such as daily office scenes, severe interference and attenuation caused by chaotic environments is inevitable which results in dramatic performance degradation. In this paper, we introduce TBDNet, which incorporates tracking-before-detect (TBD) from conventional signal processing into learning-based models, achieving impressive tracking performance in unconstrained scenarios. TBDNet follows first-track-then-detect pipeline. It maps input heatmap sequence into high-level frame-wise features to adapt the time-varying intensity distribution and motion pattern of targets. After that, the temporal information is accumulated in feature space to obtain trace proposals. We then predict the accurate positions and probability of traces at each timestamp. To assess the efficiency of TBDNet, we collect and release the first RF-UNIT (RF-based Unconstrained Indoor Tracking) dataset, which comprises 4,030,880 radar heatmaps and the corresponding tracking annotations under 6 different scenarios. To our knowledge, RF-UNIT is the first dataset for RF-based human tracking in unconstrained scenes. We anticipate that TBDNet and the RF-UNIT dataset will significantly contribute to the advancement of RF-based sensing technologies.}
}


@article{DBLP:journals/tmc/LuoWLSHWLWWC25,
	author = {Guofeng Luo and
                  Lijuan Weng and
                  Yunqian Li and
                  Yilu Sun and
                  Yayao Hong and
                  Yongyi Wu and
                  Ruixiang Luo and
                  Leye Wang and
                  Cheng Wang and
                  Longbiao Chen},
	title = {FireExpert: Fire Event Identification and Assessment Leveraging Cross-Domain
                  Knowledge and Large Language Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4794--4810},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528413},
	doi = {10.1109/TMC.2025.3528413},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoWLSHWLWWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fire events threaten the safety of residents and the health of ecosystems in affected areas, and post-disaster recovery efforts also require a large investment of resources and time. In recent years, the rising frequency of fire events has motivated local governments to strengthen their monitoring and emergency response efforts. However, current fire event identification methods can only identify the presence of a fire, without the ability to distinguish its specific category. In addition, when a fire occurs, the lack of information about the affected areas makes it challenging for emergency management authorities to take timely and effective rescue measures. To address these issues, we propose a two-stage framework for fire event identification and assessment. Specifically, in the first stage, based on multi-band fused remote sensing images and heterogeneous environmental images, the proposed framework not only identifies various fire events but also accurately identifies the boundaries of the fire events. In the second stage, integrating the results of fire event identification with social media data and domain knowledge, we present a real-time assessment agent for fire events based on the large language model. This agent enables timely and accurate analysis of the impact of fires on the affected areas. We evaluate our method on a real-world authority dataset, and results show that our framework identifies fire events with an F1-score of 61.0 % \\%  and a mAP of 57.7 % \\% , which outperforms state-of-the-art baseline methods. In addition, the assessment results of fire events in real cases indicate that the proposed fire event assessment agent can assist emergency responders in obtaining timely and accurate information.}
}


@article{DBLP:journals/tmc/MungariPGC25,
	author = {Federico Mungari and
                  Corrado Puligheddu and
                  Andres Garcia{-}Saavedra and
                  Carla{-}Fabiana Chiasserini},
	title = {{O-RAN} Intelligence Orchestration Framework for Quality-Driven xApp
                  Deployment and Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4811--4828},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527707},
	doi = {10.1109/TMC.2025.3527707},
	timestamp = {Sat, 31 May 2025 23:18:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MungariPGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid evolution of 5G networks, with diverse traffic classes and demanding services, highlights the importance of Open Radio Access Networks (O-RAN) for enabling RAN intelligence and performance optimization. Machine Learning-powered xApps offer novel network control opportunities, but their resource demands necessitate efficient orchestration. To address these issues, we present OREO, an O-RAN xApp orchestrator that, using a multi-layer graph model, aims to maximize the number of RAN services concurrently deployed while minimizing their overall energy consumption. OREO's key innovation lies in the concept of sharing xApps across RAN services when they include semantically equivalent functions and meet quality requirements. Despite the NP-hard nature of the problem, numerical results show that OREO offers a lightweight and scalable solution that closely and swiftly approximates the optimum in several different scenarios. Also, OREO outperforms state-of-the-art benchmarks by enabling the co-existence of more RAN services (14.3% more on average and up to 22%), while reducing resource expenditure (by 48.7% less on average and up to 123% for computing resources). Moreover, using an experimental prototype deployed on the Colosseum network emulator and using real-world RAN services, we show that OREO leads to substantial resource savings (up to 66.7% of computing resources) while its xApp sharing policy can significantly enhance quality of service.}
}


@article{DBLP:journals/tmc/LuoXHCYZCXL25,
	author = {Chengwen Luo and
                  Zhuoqing Xie and
                  Yuhan Huang and
                  Gecheng Chen and
                  Haiyi Yao and
                  Jin Zhang and
                  Long Cheng and
                  Weitao Xu and
                  Jianqiang Li},
	title = {LaserKey: Eavesdropping Keyboard Typing Leveraging Vibrational Emanations
                  via Laser Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4829--4844},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529919},
	doi = {10.1109/TMC.2025.3529919},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoXHCYZCXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconstructing keyboard input through side-channel attacks has posed significant threats to user security. While conventional keystroke eavesdropping attacks have demonstrated effectiveness using side channels such as acoustic signals, they are usually shorter in range and can be significantly affected by environmental noises. In this paper, we propose LaserKey, a novel keystroke eavesdropping technique that leverages the long-range and noise-resistant nature of lasers to achieve a more stealthy side-channel attack. We utilize laser sensors to accurately capture the subtle vibrations induced on laptop screens by keystrokes, and innovatively design a laser-driven deep learning-based keystroke recognition model with the inputs being the Mel-frequency Cepstral Coefficien (MFCC), Time Difference of Arrival (TDoA), and amplitude features extracted from such vibration signals. Through systematic experiments, we demonstrate that LaserKey achieves a 92.2% single-key recognition accuracy. By combining multiple single-key recognition capabilities based on this, we then realize the end-to-end word-level recognition. Moreover, to mitigate the recognition errors caused by the changes in keystroke positions, we introduce a meta-learning based domain generalization approach for achieving robust laser position calibration. Results show that LaserKey achieves as low as 3% character error rate (CER) for word-level recognition, proving its effectiveness for long-range and high-accuracy keystroke eavesdropping, and highlighting the necessity for countermeasures in the future.}
}


@article{DBLP:journals/tmc/BianSCX25,
	author = {Jieming Bian and
                  Cong Shen and
                  Mingzhe Chen and
                  Jie Xu},
	title = {Indirect-Communication Federated Learning via Mobile Transporters},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4845--4857},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527405},
	doi = {10.1109/TMC.2025.3527405},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BianSCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a distributed machine learning framework that efficiently reduces communication and preserves privacy. Existing FL algorithms typically rely on the assumption of direct communication between the server and clients for model data exchange. However, this assumption does not apply in many real-world scenarios where appropriate communication infrastructure is lacking, such as in remote smart sensing. To overcome this challenge, we propose a new framework, FedEx (Federated Learning via Model Express Delivery). FedEx employs mobile transporters, such as Unmanned Aerial Vehicles (UAVs), to establish indirect communication channels between the server and clients. We have developed two algorithms under this framework: FedEx-Sync and FedEx-Async, which differ based on whether the transporters operate on a synchronized or asynchronized schedule. Although indirect communication introduces variable delays in global model dissemination and local model collection, we demonstrate the convergence of both FedEx versions. Additionally, we explore the energy consumption of transporters, integrating it with the convergence bounds and proposing a bi-level optimization algorithm for efficient client assignment and route planning. Our experiments, conducted on two public datasets in a simulated environment, further demonstrate the efficacy of FedEx.}
}


@article{DBLP:journals/tmc/CaiWWZFZ25,
	author = {Guiliang Cai and
                  Qiang Wu and
                  Ran Wang and
                  Lianyi Zhi and
                  Xiaoming Fu and
                  Hongke Zhang},
	title = {Enabling Ultralow-Latency Services With Ubiquitous Mobility by Means
                  of a Compact Network Architecture},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4858--4873},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526971},
	doi = {10.1109/TMC.2025.3526971},
	timestamp = {Sun, 14 Sep 2025 12:42:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiWWZFZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of emerging services such as cellular vehicle-to-everything and immersive video service, network connections have further evolved from tangible physical connections to intangible virtual connections such as content, services, and computing resources, and the application scenarios have become more abundant. The mobile ultra-service, which is characterized by ultra-low latency, ultra-high reliability, and ubiquitous mobility, is becoming one of the most representative traffic types. However, the existing mobile network architecture has not evolved sufficiently to meet the specific requirements of these mobile ultra-services, the mobility anchors introduce unnecessary node and link latency, leaving space for further optimization. A compact network architecture (ComArch) is proposed in this paper for ultralow-latency services with ubiquitous mobility. ComArch is designed with a mapping control plane and a generalized forwarding plane to collaboratively implement packet forwarding in mobile scenarios. The generalized forwarding plane handles packet forwarding, while the mapping control plane manages terminals’ identifier and locator mapping entries. The node latency introduced by mobility anchors is eliminated, and an efficient routing scheme is proposed to find the optimal mandatory nodes in the forwarding path, thereby reducing unnecessary link latency. Experimental results show that ComArch can effectively reduce end-to-end delay while saving resources.}
}


@article{DBLP:journals/tmc/WangCGBJ25,
	author = {Yiming Wang and
                  Shuang Cheng and
                  Nianzhen Gao and
                  Ting Bi and
                  Tao Jiang},
	title = {{JSFBA:} Joint Segment and Frame Bitrate Adaptation for Real-Time
                  Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4874--4888},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526867},
	doi = {10.1109/TMC.2025.3526867},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCGBJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the intensive computing resource requirements, real-time video analytics applications typically need to transmit video to a server. However, the transmission inevitably suffers from network bandwidth limitations and fluctuations, making it challenging to guarantee video analytics performance. In this paper, we aim to maximize video analytics accuracy while maintaining low latency and frame loss rate, and propose a joint segment and frame bitrate adaptation (JSFBA) framework for real-time video analytics, which incorporates two reinforcement learning-based algorithms to adapt to bandwidth at both the segment and frame levels. Initially, considering the effect of video encoding on video analytics, we employ a bitrate control method to design a segment-level bitrate adaptation (SLBA) algorithm with a unique reward function. Based on the historical information of the video segments, SLBA selects the appropriate bitrate for each segment. Subsequently, by leveraging the ability to generate multiple bitrates in scalable video coding (SVC), we design a frame-level bitrate adaptation (FLBA) algorithm, which adapts to bandwidth in a more fine-grained manner by determining the number of layers sent for each frame. Extensive experiments on large-scale network traces reveal that JSFBA effectively balances various video analytics performance metrics and achieves maximum utility compared to state-of-the-art solutions.}
}


@article{DBLP:journals/tmc/ChenGXLSGC25,
	author = {Quan Chen and
                  Song Guo and
                  Wenchao Xu and
                  Jing Li and
                  Tuo Shi and
                  Hong Gao and
                  Zhipeng Cai},
	title = {Average AoI Minimization With Directional Charging for Wireless-Powered
                  Network Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4889--4906},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527559},
	doi = {10.1109/TMC.2025.3527559},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenGXLSGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information (AoI) has been proposed as a new performance metric to capture the freshness of data. At wireless-powered network edge, the source nodes first need to be charged ready for update transmissions, which means the system AoI is not only decided by the scheduling of update transmissions but also by the designing of charging plan. However, the existing works either only focused on the point of scheduling update transmissions or have a rigid assumption that only one source node can be charged per time. Aiming at making the work more practical and general, we investigate the average AoI optimization problem at wireless-powered network edge with directional charging. First, the theoretical bound of the weighted sum of average AoI of the entire network with a directional charger is analyzed, which is proved to be related to nodes’ maximum transmitting interval and the charging strategy. An optimal charging time computation algorithm is proposed to obtain the maximum transmitting interval of each source node by considering the overlapped areas of different charging orientations. After then, an AoI-aware periodical charging scheduling algorithm is proposed to compute a periodical charging schedule while the average AoI is bounded, including a charging period  T T  and the charging orientations assigned to each time slot within  T T . The proposed algorithm is proved to have an approximation ratio of up to 1.5625. Furthermore, several approximate algorithms are also proposed for average AoI optimization with multiple chargers and network bandwidth constraint. Finally, the extensive simulations demonstrate the high performance of the proposed algorithm in terms of AoI.}
}


@article{DBLP:journals/tmc/YangDXXNH25,
	author = {Yaoqi Yang and
                  Hongyang Du and
                  Zehui Xiong and
                  Renhui Xu and
                  Dusit Niyato and
                  Zhu Han},
	title = {Exploring Impacts of Age of Information on Data Accuracy for Wireless
                  Sensing Systems: An Information Entropy Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4907--4924},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527587},
	doi = {10.1109/TMC.2025.3527587},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangDXXNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensing systems have been employed in the field of healthcare, environment monitoring, and smart agriculture, etc. Since the freshness and accuracy indicators of the sensing data are critical to wireless sensing systems, it is of great significance to ensure their performances simultaneously, i.e., the Age of Information (AoI) and information entropy of the sensing data should be jointly optimized. In this regard, we first establish the wireless sensing system models, including AoI and information entropy expressions. Next, from the information entropy viewpoint, we theoretically analyze an impact of the AoI on data accuracy. Then, we formulate the joint optimization problem of AoI, information entropy, and sensing energy consumption. Furthermore, we propose two numerical algorithms to solve the formulated problem in the known or unknown transmission environment, respectively. Finally, we evaluate the correctness and effectiveness of our proposals under various parameter settings, where the proposed scheme can obtain a better sum-weighted performance on AoI, information entropy, and sensing energy consumption than baselines in the literature.}
}


@article{DBLP:journals/tmc/ZhongZCWZBH25,
	author = {Hong Zhong and
                  Dan Zhou and
                  Jie Cui and
                  Li Wang and
                  Jing Zhang and
                  Irina Bolodurina and
                  Debiao He},
	title = {Security-Enhanced Data Sharing via Efficient Sanitization for VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4925--4938},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527935},
	doi = {10.1109/TMC.2025.3527935},
	timestamp = {Tue, 05 Aug 2025 13:38:50 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhongZCWZBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread deployment of vehicular ad-hoc networks (VANETs), data sharing has garnered considerable attention as a core feature of VANETs. Attribute-based proxy re-encryption (ABPRE) enables fine-grained access control and provides flexible ciphertext updates. The initially authorized vehicle generates the re-encryption key to enable ciphertext-to-ciphertext conversion in the cloud, allowing ciphertext to be shared with new recipients. However, initially authorized vehicles may not always be trustworthy and could share data with malicious receivers. In addition, the computation and communication overhead of ABPRE hinders its widespread application in VANETs. To address these issues, we propose a lightweight sanitizable scheme for edge-assisted VANETs based on ABPRE. In this scheme, the re-encryption key is verified by a sanitizer, to prevent the data from being shared with malicious data receivers. In addition, key-splitting techniques and edge computing are employed to reduce the communication and computation overhead of re-encryption. A comprehensive security analysis and performance evaluation demonstrate that the proposed scheme is efficient and practical.}
}


@article{DBLP:journals/tmc/ChaiQL25,
	author = {Haoye Chai and
                  Xiaoqian Qi and
                  Yong Li},
	title = {Spatio-Temporal Knowledge Driven Diffusion Model for Mobile Traffic
                  Generation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4939--4956},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527966},
	doi = {10.1109/TMC.2025.3527966},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChaiQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generating mobile traffic in urban environments is important for network planning and optimization. However, existing models show weakness in capturing spatial-temporal dynamics between mobile traffic and urban environments. This makes it difficult for the models to generate high-fidelity traffic data and control the generation process across different regions in large-scale urban environments, ultimately affecting the effectiveness of optimization strategies. In this paper, we propose a Spatio-Temporal Knowledge-driven Diffusion model (STK-Diff) for controllable mobile traffic generation. We construct an Urban Knowledge Graph (UKG) to fully characterize the urban features, which incorporates both the spatial and semantic relations of different entities, such as base stations, business areas, and functional regions. Based on the constructed UKG, we design the denoising network of diffusion model with a temporal extraction module and a spatial connection module. These two modules capture the correlations of mobile traffic and environment features via a frequency attention mechanism and spatial graph learning scheme, so as to make a strong controllability on the generated mobile traffic. Extensive experiments on three real-world datasets show that the proposed framework not only improves generation fidelity by up to 19%, but also enhances the controllability to generate specific patterns, with a gain of surpassing 15%.}
}


@article{DBLP:journals/tmc/XiaoYCGMH25,
	author = {Liyang Xiao and
                  Yanni Yang and
                  Zhe Chen and
                  Yue Gao and
                  Prasant Mohapatra and
                  Pengfei Hu},
	title = {CRFusion: Fine-Grained Object Identification Using RF-Image Modality
                  Fusion},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4957--4970},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527872},
	doi = {10.1109/TMC.2025.3527872},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoYCGMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object identification is a pivotal enabling technique for smart home and manufacturing applications. Traditional methodologies for object identification predominantly rely on a singular sensor modality, which inherently limits their ability to furnish a detailed characterization of the target object. Addressing this deficiency, in this paper, we fill this gap by introducing CRFusion, the first-of-its-kind system that integrates the object RGB image and the radio frequency (RF) signal reflected by the object for fine-grained object identification. CRFusion leverages the complementary characteristics between visible light and radio frequency modalities to simultaneously determine the category and material of target objects. We design a multifaceted object feature from the RF signal, called the Energy Reflection Factor (ERF), which not only reveals the object texture but complements the image modality for identifying the object category. By integrating the characteristics of radar, we obtain radar feature maps based on the ERF of target objects. Additionally, we have developed a modality fusion network to comprehensively integrate the image and ERF features. We conducted a comprehensive evaluation of CRFusion using a commercial mmWave radar development board and camera. The results show that CRFusion achieves a classification accuracy of over 96%, demonstrating its robustness, and potential for application.}
}


@article{DBLP:journals/tmc/SunLYDRWW25,
	author = {Yu Sun and
                  Chi Lin and
                  Wei Yang and
                  Haipeng Dai and
                  Jiankang Ren and
                  Lei Wang and
                  Guowei Wu},
	title = {Through-Wall Mobile Charging: Theory, Methodology, and Implementation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4971--4986},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3527440},
	doi = {10.1109/TMC.2025.3527440},
	timestamp = {Thu, 06 Nov 2025 14:15:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunLYDRWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Power Transfer (WPT) has revolutionized the field of Wireless Rechargeable Sensor Networks (WRSNs), enabling sustainable operation of sensor nodes. Traditional mobile charging methods often require sensors to be within line-of-sight or physically accessed by the mobile charger, which may potentially lead to user safety or privacy concerns. Addressing this concern, this work is the first to introduce and validate the feasibility of Through-Wall charging. We formulate the Wireless charging thrOugh Walls (WOW) problem to simultaneously enhance user safety and maximize charging utility. Our approach leverages fundamental principles of electromagnetics to construct an accurate charging model for Magnetic Resonance Coupling-based WPT systems. Additionally, we thoroughly analyze the impact of wall obstruction and provide a generalized framework for through-wall charging. By employing discretization techniques and approximation algorithms, we derive a near-optimal solution to the WOW problem. Extensive simulations and test-bed experiments demonstrate that our proposed approach reduces the reliance on physical access to devices, simplifies deployment in complex environments, and thereby optimizes the travel paths of mobile chargers and enhances the overall performance and lifetime of WRSNs. Compared to conventional methods, our method benefits from more reasonable scheduling order and path construction, achieving an average energy efficiency improvement of 27.8%.}
}


@article{DBLP:journals/tmc/LimJKLSK25,
	author = {Dawoon Lim and
                  Subin Jeong and
                  Bitna Kim and
                  Yelan Lee and
                  Hyejin Shin and
                  Juyeop Kim},
	title = {Field Evaluation of a Softwarized Modem From the Perspective of 5G
                  Cell Search},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {4987--5002},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526753},
	doi = {10.1109/TMC.2025.3526753},
	timestamp = {Tue, 18 Nov 2025 13:19:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LimJKLSK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modem softwarization, where baseband signals are fully processed using software on a general-purpose CPU, is a promising technology in mobile communications due to its simplicity and flexibility in realizing various features. On the other hand, many still question the effectiveness of a softwarized modem in commercial environments concerning performance and complexity. Motivated by this perspective, this paper presents the design and implementation of a softwarized modem with the specific feature of 5G cell search for field evaluation. Based on the baseline algorithms of 5G cell search in Open Air Interface (OAI), we propose a new software architecture which can efficiently manage a 5G cell search procedure and decompose the overall 5G cell search into sub-algorithms. We also design and implement novel sub-algorithms that enhance the detection of Synchronization Signal Blocks (SSBs). Our softwarized modem utilizes dual-rate sampling to significantly reduce computation complexity during timing offset estimation. It also adaptively detects synchronization signals or cell identities based on the presence of inter-cell interference or multi-path fading. The performance evaluation through field experiments concludes that our softwarized modem outperforms the baseline, and the proposed sub-algorithms are effective in enhancing cell search performance. The detection probability and time consumption results for our softwarized modem confirm that it is feasible for commercial uses.}
}


@article{DBLP:journals/tmc/MishraG25,
	author = {Rahul Mishra and
                  Hari Prabhat Gupta},
	title = {Towards Understanding the Impact of Participant and its Wearable Devices
                  in Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5003--5015},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530818},
	doi = {10.1109/TMC.2025.3530818},
	timestamp = {Sat, 31 May 2025 23:18:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MishraG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of wearable smart devices has increased due to their seamless monitoring of vital signs during daily activities. Federated learning leverages these devices along with participants’ smartphones to fine-tune pre-trained models. Moreover, calibrating the differences between wearables and smartphones in terms of sampling rates, orientations, activity correlation, battery power, and other factors is challenging. Thus, the paper introduces a participant and wearable selection cross-device federated learning approach. It leverages criteria such as the activity wearable(s) relationship, data quality, battery life, sampling rate, and so on to perform the wearable selection. The server evaluates and estimates the utility of each participant and selects those with higher utility in each communication round. We then figure out the optimal weighted contribution of each participant to perform robust aggregation. We also use knowledge distillation techniques to develop a high-performing and lightweight wearable model. Finally, we conduct simulation and real-world experiments on existing datasets and compare our approach with state-of-the-art. The result shows an improvement of  3 − 4 % 3\\!\\!-\\!\\!4\\%  in accuracy via fine-tuning from selected wearable data.}
}


@article{DBLP:journals/tmc/YinRXT25,
	author = {Jiaming Yin and
                  Weixiong Rao and
                  Yu Xiao and
                  Keshuang Tang},
	title = {Cooperative Path Planning With Asynchronous Multiagent Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5016--5030},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526979},
	doi = {10.1109/TMC.2025.3526979},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YinRXT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the number of vehicles grows in urban cities, planning vehicle routes to avoid congestion and decrease commuting time is important. In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs, namely MSD-SPP, to minimize the average travel time of all routing paths. The asynchronous setting in MSD-SPP, i.e., vehicles may not simultaneously complete routing actions, makes it challenging for cooperative route planning among multiple agents and leads to ineffective route planning. To tackle this issue, in this paper, we propose a two-stage framework of inter-region and intra-region route planning by dividing an entire road network into multiple sub-graph regions. Next, the proposed asyn-MARL model allows efficient asynchronous multi-agent learning by three key techniques. First, the model adopts a low-dimensional global state to implicitly represent the high-dimensional joint observations and actions of multi-agents. Second, by a novel trajectory collection mechanism, the model can decrease the redundancy in training trajectories. Additionally, with a novel actor network, the model facilitates the cooperation among vehicles towards the same or close destinations, and a reachability graph can prevent infinite loops in routing paths. On both synthetic and real road networks, the evaluation result demonstrates that asyn-MARL outperforms state-of-the-art planning approaches.}
}


@article{DBLP:journals/tmc/PanYWZZZZ25,
	author = {Zijie Pan and
                  Zuobin Ying and
                  Yajie Wang and
                  Chuan Zhang and
                  Weiting Zhang and
                  Wanlei Zhou and
                  Liehuang Zhu},
	title = {Feature-Based Machine Unlearning for Vertical Federated Learning in
                  IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5031--5044},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530529},
	doi = {10.1109/TMC.2025.3530529},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PanYWZZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of the Internet of Things (IoT), managing the deluge of data generated by distributed devices presents unique challenges, particularly concerning privacy and the efficient use of computational resources. Vertical Federated Learning (VFL) offers a promising avenue for collaborative machine learning without centralizing data, thereby addressing privacy concerns inherent in traditional approaches. However, as data privacy laws and personal data deletion requests become more prevalent, the necessity for effective machine unlearning strategies within VFL frameworks grows increasingly important. To this end, this paper introduces a novel approach to feature-based machine unlearning tailored specifically for VFL systems in IoT networks. Our methodology enables the selective removal of data influence from trained models without the need for full retraining, thus preserving model utility while ensuring compliance with privacy requirements. By integrating a combination of feature relevance measuring techniques and efficient communication protocols, our solution minimizes the data footprint on network nodes, reduces bandwidth consumption, and maintains the integrity and performance of the learning models. To the best of our knowledge, our proposed framework represents the first practical approach to enable machine unlearning within vertical federated learning environments. We demonstrate the effectiveness of our approach through rigorous evaluation using several IoT datasets, highlighting significant improvements in unlearning efficiency and model robustness compared to existing techniques. Our work not only furthers the development of sustainable and compliant machine learning models in IoT but also sets a foundational framework for future research in secure and efficient data management within federated environments.}
}


@article{DBLP:journals/tmc/DengYZGLWM25,
	author = {Xiaoheng Deng and
                  Haoyu Yang and
                  Jingjing Zhang and
                  Jinsong Gui and
                  Siyu Lin and
                  Xin Wang and
                  Geyong Min},
	title = {Task Offloading in Internet of Vehicles: {A} DRL-Based Approach With
                  Representation Learning for {DAG} Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5045--5060},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531887},
	doi = {10.1109/TMC.2025.3531887},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengYZGLWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid evolution of the Internet-of-Vehicles (IoV) has amplified the need for mobile computing resources, driving the shift toward offloading tasks to edge servers or vehicles with idle resources to optimize computational efficiency. To this end, an approach based on Deep Reinforcement Learning (DRL) is presented in this paper, termed DVTP, which integrates Variational Graph Attention Networks (VGAT) and Transformer models to optimize Directed Acyclic Graph (DAG) task scheduling in vehicular networks. DVTP effectively captures both the spatiotemporal information and task dependencies, enabling more accurate and efficient task offloading decisions. Extensive simulation experiments demonstrate that DVTP outperforms traditional methods in reducing task completion times across various multi-vehicle and multi-edge server scenarios, showcasing its potential for real-world IoV applications.}
}


@article{DBLP:journals/tmc/LinPHYFZ25,
	author = {Na Lin and
                  Cong Peng and
                  Ammar Hawbani and
                  Cunqian Yu and
                  Yanbo Fan and
                  Liang Zhao},
	title = {Energy Efficient AAV-Assisted Bidirectional Relaying System for Multi-Pair
                  User Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5061--5077},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3526981},
	doi = {10.1109/TMC.2025.3526981},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinPHYFZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomousaerial vehicles (AAVs), or drones, are garnering considerable focus in the realm of wireless communications research because to their notable characteristics, including exceptional mobility, versatile deployment capabilities, and robustness in maintaining line-of-sight (LoS) links. This paper studies a AAV-assisted bidirectional relaying system for multi-pair user devices (UDs), where a rotary-wing AAV is used to serve as a mobile relay for providing information transmission between UDs belonging to a pair on the ground. The UDs in a pair communicate with each other via the AAV relay employing the physical-layer network coding (PNC) technique. To trade off fair communications with system energy consumption, we jointly optimize transmission scheduling and association, AAV relay and UD transmission power, and AAV trajectory to maximize system energy efficiency during AAV relay communications. Due to the formulated problem being mixed-integer and nonconvex programming, it proves to be excessively complex to solve. For ease of solution, this problem is initially decomposed into three sub-problems. Next, by adopting the block coordinate descent (BCD) method, the successive convex approximation (SCA) method, and the Dinkelbach method, an efficient iterative algorithm is proposed that alternately solves variables of each sub-problem while fixing others. The numerical results demonstrate that our designed scheme is capable of substantially improving the system energy efficiency in comparison with other baseline schemes and benchmark schemes.}
}


@article{DBLP:journals/tmc/DuYYRZL25,
	author = {Caihui Du and
                  Jihong Yu and
                  Zhenyu Yan and
                  Ju Ren and
                  Rongrong Zhang and
                  Yun Li},
	title = {Efficient Subcarrier-Level {OFDM} Backscatter Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5078--5093},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529265},
	doi = {10.1109/TMC.2025.3529265},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuYYRZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most of the existing OFDM backscatter systems adopt phase-modulated schemes to embed tag data, suffering from symbol-level modulation limitation, heavy synchronization accuracy reliance, and small tolerability to symbol time offset (STO) / carrier frequency (CFO) offset. We introduce SubScatter, the first subcarrier-level frequency-modulated OFDM backscatter which is able to tolerate bigger synchronization errors, STO, and CFO. The unique feature of SubScatter is our subcarrier shift keying (SSK) modulation. This method pushes the modulation granularity to the subcarrier by encoding and mapping tag data into different subcarrier patterns. We also design a tandem frequency shift (TFS) scheme that enables SSK with low cost and low power. Furthermore, we design SubScatter+ that shows these advantages while providing an even higher throughput without requiring more subcarrier patterns. We prototype and test SubScatter and SubScatter+, and the results show that our systems outperforms prior works in terms of effectiveness and robustness. Specifically, SubScatter has 743 kbps throughput that is 3.1 times and 14.9 times higher than RapidRider and MOXcatter, respectively. It also has a lower BER under noise and interferences which is over 6 times better than RapidRider or MOXcatter. Moreover, our proposed SubScatter+ could increase the throughput of SubScatter by 30%.}
}


@article{DBLP:journals/tmc/XuLWYLYD25,
	author = {Yuanbo Xu and
                  Jiawei Liu and
                  En Wang and
                  Bo Yang and
                  Dongming Luan and
                  Yongjian Yang and
                  Jing Deng},
	title = {Rethinking the Effect of Sparse Data Completion on Sparse Mobile Crowdsensing
                  Tasks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5094--5105},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531362},
	doi = {10.1109/TMC.2025.3531362},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLWYLYD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) is a powerful technique that enables a variety of urban tasks, including temperature monitoring, location-based services, and urban path recommendations. However, these tasks often face the challenge of sparse and incomplete sensing data, undermining their effectiveness and reliability. Sparse data completion (SDC) methods have been developed to infer missing or unobserved data by leveraging spatio-temporal correlations to tackle this issue. This forms the core concept of the sparse mobile crowdsensing problem (SMCS), which aims to improve the performance of downstream tasks through inferred data. Despite the potential benefits, most existing SMCS methods fail to consider the trade-off between the cost of SDC and the benefits for downstream tasks. These methods often treat SDC and downstream tasks as independent modules, resulting in suboptimal outcomes. In this paper, we investigate the impact of SDC on the SMCS paradigm, both qualitatively and quantitatively. We establish the upper bound of performance achievable when applying SDC in SMCS under different levels of sensing data sparsity. Based on these studies and findings, we propose a practical and flexible framework called SDC-EVA, Sensing Data Completion EVAluation framework. This framework allows for applying different SDC methods in SMCS, considering factors such as computing complexity, storage space, and associated costs. Our proposed framework allows researchers to assess the necessity and feasibility of integrating SDC into SMCS systems before designing and deploying them in real-world scenarios. This assessment can be tailored to specific data sparsity and contextual information. To validate the effectiveness of our proposed evaluation framework, we conduct experiments in various real-world scenarios involving different combinations of SDC and downstream tasks. The results demonstrate the superiority of our framework in improving the performance of SMCS. By presenting these findings, we aim to contribute to developing SMCS techniques and provide valuable insights for researchers and practitioners.}
}


@article{DBLP:journals/tmc/YangZSMXC25,
	author = {Xiansheng Yang and
                  Yuan Zhuang and
                  Min Shi and
                  Qian Meng and
                  Jun Xiong and
                  Yue Cao},
	title = {DeepVLP: {A} Graph Neural Network-Based Denoising and Signals Optimization
                  Framework for Visible Light Positioning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5106--5123},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528440},
	doi = {10.1109/TMC.2025.3528440},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangZSMXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visible Light Positioning (VLP) has emerged as a promising technique in the Internet of Things landscape and gained increasing attention worldwide due to its widely existing infrastructure, high precision, and cost-effectiveness. Recently, ratio and difference-based VLP systems have been used to reduce errors from environmental noise, ambient light, and device differences. However, there may be intricate interference patterns that simple ratios and differences struggle to address. Moreover, a single LED often has limited capability to achieve self-diagnosis and self-correction. In fact, the information from other LEDs can be used to refine the signal and suppress interference. Thus, we propose to organize the VLP system in a graph and use the Graph Neural Network to model the interrelationships among LED lamps. This allows us to optimize the signals and further efficiently suppress interferences by simultaneously considering multiple LED lamps. In addition, the precisions of LEDs’ measurements is different due to various factors (e.g., distances and powers), and low-precision measurements may reduce the performance of the VLP system. To address this issue, we incorporate an attention layer to allow our model to give higher weights to high-precision measurements. Finally, the long short-term memory network is used to model the temporal dependencies between adjacent positions in a trajectory. Taking these modules together, we develop a robust VLP system called DeepVLP. The comprehensive experiments demonstrate that DeepVLP achieves better performance than state-of-the-art methods.}
}


@article{DBLP:journals/tmc/CaoDDGZ25,
	author = {Chenhong Cao and
                  Yue Ding and
                  Miaoling Dai and
                  Wei Gong and
                  Xibin Zhao},
	title = {Real-Time Cross-Domain Gesture and User Identification via {COTS}
                  WiFi},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5124--5137},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532295},
	doi = {10.1109/TMC.2025.3532295},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoDDGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based gesture recognition has emerged as a promising alternative to computer vision, enabling seamless integration and enhanced interaction in human-computer interaction systems. Simultaneously identifying users during gesture recognition is vital for improving security and personalization. However, existing WiFi-based dual-task recognition approaches often rely on handcrafted features, which hinder precision and introduce delays in cross-domain scenarios. To address these challenges, we propose WiDual, a real-time system for cross-domain gesture recognition and user identification using WiFi signals. By integrating spatial and channel attention mechanisms, WiDual adaptively extracts crucial features for dual-task recognition. The system employs Channel State Information (CSI) visualization to convert WiFi signals into images, facilitating efficient feature extraction and minimizing information loss and latency. Furthermore, a collaborative module fuses gesture and user identity features, enhancing recognition performance. Experimental evaluations on a public dataset with six gestures and six users across diverse environments demonstrate WiDual's effectiveness. It achieves 96% accuracy in cross-domain gesture recognition and 91.27% in user identification. Compared to state-of-the-art methods, WiDual improves user identification accuracy by 26%, gesture recognition by 8%, and reduces processing time sixfold, showcasing its potential for real-time applications.}
}


@article{DBLP:journals/tmc/ChenJCCLM25,
	author = {Zheyi Chen and
                  Qingnan Jiang and
                  Lixian Chen and
                  Xing Chen and
                  Jie Li and
                  Geyong Min},
	title = {{MC-2PF:} {A} Multi-Edge Cooperative Universal Framework for Load
                  Prediction With Personalized Federated Deep Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5138--5154},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528404},
	doi = {10.1109/TMC.2025.3528404},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenJCCLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging load prediction techniques support up-front and rational resource provisioning in edge systems to enhance system efficiency and Quality-of-Service (QoS). Classic prediction methods may handle loads with apparent trends, but they cannot achieve accurate prediction for highly-variable edge loads. With the advantage of sequential data analysis, recurrent neural networks (RNNs) are often used for load prediction but reveal limited generalization ability and low training efficiency. Moreover, it is hard to obtain a well-performed prediction model by discrete single-edge training with insufficient historical data. To address these important challenges, we propose a novel Multi-edge Cooperative universal framework for load Prediction with Personalized Federated deep learning (MC-2PF), enabling multi-edge cooperative training of load prediction models. Specifically, to solve the client-drift issue in federated learning (FL) caused by distinct data distribution, we customize personalized models for each edge by independent control parameters and theoretically analyze the model convergence improvement. Meanwhile, we prove the generalization bound of the MC-2PF and its universality to RNN-based prediction models through a practical example. Using the real-world testbed and load datasets, extensive experiments verify the effectiveness and practicality of the MC-2PF for different RNN-based prediction models. Compared to state-of-the-art frameworks, the MC-2PF achieves higher prediction accuracy, faster convergence, and stronger adaptiveness.}
}


@article{DBLP:journals/tmc/ShiLTFBC25,
	author = {Fangzhan Shi and
                  Wenda Li and
                  Chong Tang and
                  Yuan Fang and
                  Paul V. Brennan and
                  Kevin Chetty},
	title = {ML-Track: Passive Human Tracking Using WiFi Multi-Link Round-Trip
                  {CSI} and Particle Filter},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5155--5172},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529897},
	doi = {10.1109/TMC.2025.3529897},
	timestamp = {Sat, 31 May 2025 23:18:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShiLTFBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we present ML-Track, an innovative uncooperative passive tracking system leveraging WiFi communication signals between multiple devices. Our approach is realized with three pivotal techniques. First, we introduce a novel protocol termed multi-link round-trip CSI, which enables multi-link bistatic Doppler detection within a WiFi network. Second, a phase error cancellation method is developed, and we demonstrate a 0.92 rad reduction in error (0.96 to 0.04 rad) experimentally. Lastly, we propose a particle-filter-based back-end to track a moving human in the room passively without the need for the participant to carry any type of cooperative or active device. A prototype system is constructed using four Raspberry Pi CM4 units and subjected to real-world evaluations. Experimental results indicate a median error of approximately 0.23 m for tracking, which corresponds to a relative error of 5.8% based on the 4 m side length of the experimental field. Compared to existing studies, a distinct advantage of our system is it can run with non-MIMO (single-antenna) WiFi devices, making it particularly suitable for budget or low-profile WiFi hardware. This compatibility makes it an ideal fit for real-world Internet-of-Things (IoT) devices. Moreover, in terms of computational demands, our solution excels, delivering real-time performance on the Raspberry Pi CM4 while utilizing just 20% of its CPU capability and drawing a modest 2.5 watts of power.}
}


@article{DBLP:journals/tmc/CaoLSCLL25,
	author = {Pan Cao and
                  Lei Lei and
                  Gaoqing Shen and
                  Shengsuo Cai and
                  Xiaojiao Liu and
                  Xiaochang Liu},
	title = {{AAV} Swarm Cooperative Search Based on Scalable Multiagent Deep Reinforcement
                  Learning With Digital Twin-Enabled Sim-to-Real Transfer},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5173--5188},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530438},
	doi = {10.1109/TMC.2025.3530438},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLSCLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative target search (CTS) technology is highly desirable in various multi-autonomous aerial vehicle (AAV) applications. However, searching for unknown targets in a dynamic threatening environment is a challenging problem, especially for AAVs with limited sensing range and communication capabilities. Besides, traditional searching methods lack scalability and efficient collaboration among the AAV swarm in dynamic environments. In this work, a digital twin (DT)-enabled distributed CTS approach was presented for AAV swarms and achieving sim-to-real transfer. Specifically, a new scalable multi-agent reinforcement learning (MARL) based algorithm called SAMARL is adopted to improve effectiveness and adaptability, combining a multi-head attention mechanism. In SAMARL, a scalable observation space with graph representation and an environmental cognition map is designed to thoroughly consider the target search rate, area coverage, and safety assurance. Then, a DT-driven training framework is proposed to facilitate the continuous evolution of MARL models and address the tradeoff between training speed and environment fidelity. Furthermore, we innovatively develop a distributed AAV swarm digital twin cooperative target search validation system, including real flight control, communication simulation tools, and a 3D physics engine. Extensive simulations validate its superiority compared to state-of-the-art strategies. More importantly, we also conduct real-world flight experiments on different scale mission areas and AAV swarms, further demonstrating the generalization and scalability of trained models.}
}


@article{DBLP:journals/tmc/ZhangLXWYC25,
	author = {Guanghui Zhang and
                  Ke Liu and
                  Mengbai Xiao and
                  Bingshu Wang and
                  Dongxiao Yu and
                  Xiuzhen Cheng},
	title = {{SLVS:} {A} Self-Learning Approach to Achieve Near-Second Low-Latency
                  Video Streaming Under Highly Variable Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5189--5201},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528635},
	doi = {10.1109/TMC.2025.3528635},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLXWYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fueled by the rapid advances in high-speed mobile networks, live video streaming has seen explosive growth in recent years and some DASH-based algorithms were specifically proposed for low-latency video delivery. We conducted a measurement study for the state-of-the-art algorithms with large-scale network traces. It reveals that these algorithms are susceptible to network condition changes due to the use of solo universal adaptation logics, resulting in the playback latency that has substantial variations across highly fluctuating networks. To tackle this challenge, this paper proposes Stateful Live Video Streaming (SLVS), which is a novel self-learning approach that learns the various network features and optimizes the adaptation logic separately for different network conditions, then dynamically tunes the logic at runtime, so that bitrate decision can better match the changing networks. Moreover, we further generalize SLVS to complement the streaming platform already in service to make it compatible with any live streaming services. Extensive evaluations based on real system prototypes show that SLVS can control playback latency down to 1 s while improving Quality-of-Experience (QoE) by 17.7% to 31.8%. Moreover, it has strong robustness to maintain near-second latency over highly fluctuating networks as well as long periods of video viewing.}
}


@article{DBLP:journals/tmc/ZhangDYLR25,
	author = {Ye Zhang and
                  Yongheng Deng and
                  Sheng Yue and
                  Qiushi Li and
                  Ju Ren},
	title = {DualRec: {A} Collaborative Training Framework for Device and Cloud
                  Recommendation Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5202--5213},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3528967},
	doi = {10.1109/TMC.2025.3528967},
	timestamp = {Mon, 26 Jan 2026 12:34:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangDYLR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems (RS) play a vital role in various domains. However, under recent data regulations like General Data Protection Regulation (GDPR), traditional RS that rely on collecting user's interaction data centrally face significant challenges. Federated learning (FL) enables collaborative model training among users while keeping their private data locally. Yet, the constrained resources of devices often limit the size of the learned model, resulting in suboptimal recommendation performance. To overcome the dilemma of data accessibility and model size, we propose DualRec, a novel collaborative training framework for device and cloud recommendation models. In DualRec, users train lightweight models on devices to harness their local private data, while a larger model is simultaneously trained on the cloud server to exploit its substantial resources. Devices and the cloud server collaboratively train their models, compensating for individual limitations of model size and data availability, enabling mutual empowerment and benefits. Specifically, we introduce an efficient aggregation mechanism for recommendation models to boost the collaborative training performance of device models. With the learned device models, we propose to generate pseudo user interaction data to train the server model. To enhance the training performance of the server model, we design an automated denoising mechanism to mitigate the negative impact of noisy samples in the generated pseudo dataset. Finally, the learned knowledge of the server model is distilled to device models for enhanced on-device recommendation performance. Extensive experiments demonstrate the superior performance of DualRec compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/ChengZZKXWN25,
	author = {Yujun Cheng and
                  Weiting Zhang and
                  Zhewei Zhang and
                  Jiawen Kang and
                  Qi Xu and
                  Shengjin Wang and
                  Dusit Niyato},
	title = {SnapCFL: {A} Pre-Clustering-Based Clustered Federated Learning Framework
                  for Data and System Heterogeneities},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5214--5228},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529487},
	doi = {10.1109/TMC.2025.3529487},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChengZZKXWN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has emerged as a promising framework to address data privacy concerns associated with mobile devices, in contrast to conventional Machine Learning (ML). However, traditional FL encounters significant challenges due to the heterogeneities among different clients. Clustered Federated Learning (CFL) has demonstrated effectiveness in mitigating the data heterogeneity challenge, which significantly limits a broader application of FL. Nevertheless, existing CFL approaches often tightly couple the clustering process with the main FL process, affecting the flexibility and performance of CFL. In this paper, we propose a pre-clustering-based CFL approach, named SnapCFL, which decouples the CFL process into pre-clustering and main FL stages, considering both the impact of heterogeneity on CFL accuracy and the framework's flexibility. The pre-clustering stage models the measurement of data similarity as a two-sample hypothesis testing problem to more accurately group clients and alleviate data heterogeneity. In the main FL stage, a constraint-based client selection method is employed to address the system heterogeneity problem. We conduct extensive experiments using popular datasets with various heterogeneity settings. The results demonstrate that SnapCFL achieves excellent performance in terms of accuracy and efficiency. Compared to five other state-of-the-art approaches, SnapCFL can improve model accuracy by 0.7% ∼ \\sim 36.4%, and achieve the same level of accuracy with at least 0.08× the convergence time.}
}


@article{DBLP:journals/tmc/WangZCL25,
	author = {Tao Wang and
                  Yang Zhao and
                  Ming{-}Ching Chang and
                  Jie Liu},
	title = {Open-Set Occluded Person Identification With mmWave Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5229--5244},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529735},
	doi = {10.1109/TMC.2025.3529735},
	timestamp = {Tue, 05 Aug 2025 14:08:43 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency sensors can penetrate non-metal objects and provide complementary information to vision sensors for person identification (PID) purposes. However, there is a lack of research on millimeter wave (mmWave) radar for PID under occlusions, particularly in addressing the open-set recognition problem. Thus, we propose an open-set occluded PID (OSO-PID) framework that can deal with various obstacle and occlusion scenarios with open-set recognition capability. We first introduce a new dataset, mmWave-ocPID, comprising mmWave radar measurements and RGB-depth images, collected from 23 human subjects. We next design a novel neural network, mm-PIDNet, for occluded person identification using mmWave radar measurements. mm-PIDNet incorporates a transformer encoder, a bidirectional long short-term memory module, and a novel supervised contrastive learning module to improve PID performance. For open-set recognition, we enhance the mmWave radar-based PID method by integrating supervised contrastive learning with the Weibull models, which can identify out-of-distribution samples. We perform extensive indoor experiments with a variety of obstacles and occlusion scenarios. Our experimental results show that mm-PIDNet achieves an F1-score of 0.93 on average, outperforming state-of-the-art methods by up to 13.41% for occluded cases. For open-set PID, the OSO-PID framework achieves an F1-score above 0.8 when the openness is less than 14.36%.}
}


@article{DBLP:journals/tmc/MoghadasFVFW25,
	author = {Serly Moghadas and
                  Claudio Fiandrino and
                  Narseo Vallina{-}Rodriguez and
                  Marco Fiore and
                  Joerg Widmer},
	title = {DeExp: Revealing Model Vulnerabilities for Spatio-Temporal Mobile
                  Traffic Forecasting With Explainable {AI}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5245--5263},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531544},
	doi = {10.1109/TMC.2025.3531544},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MoghadasFVFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ability to perform mobile traffic forecasting effectively with Deep Neural Networks (DNN) is instrumental to optimize resource management in 5G and beyond generation mobile networks. However, despite their capabilities, these Deep Neural Networks (DNN)s often act as complex opaque-boxes with decisions that are difficult to interpret. Even worse, they have proven vulnerable to adversarial attacks which undermine their applicability in production networks. Unfortunately, although existing state-of-the-art EXplainable Artificial Intelligence (XAI) techniques are often demonstrated in computer vision and Natural Language Processing (NLP), they may not fully address the unique challenges posed by spatio-temporal time-series forecasting models. To address these challenges, we introduce DeExp in this paper, a tool that flexibly builds upon legacy EXplainable Artificial Intelligence (XAI) techniques to synthesize compact explanations by making it possible to understand which Base Stations (BSs) are more influential for forecasting from a spatio-temporal perspective. Armed with such knowledge, we run state-of-the-art Adversarial Machine Learning (AML) techniques on those BSs to measure the accuracy degradation of the predictors under adversarial attacks. Our comprehensive evaluation uses real-world mobile traffic datasets and demonstrates that legacy XAI techniques spot different types of vulnerabilities. While Gradient-weighted Class Activation Mapping (GC) is suitable to spot BSs sensitive to moderate/low traffic injection, LayeR-wise backPropagation (LRP) is suitable to identify BSs sensitive to high traffic injection. Under moderate adversarial attacks, the prediction error of the BSs identified as vulnerable can increase by more than 250%.}
}


@article{DBLP:journals/tmc/ChenJH25,
	author = {Jianan Chen and
                  Honglu Jiang and
                  Qin Hu},
	title = {Utility-Enhanced Personalized Privacy Preservation in Hierarchical
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5264--5279},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531919},
	doi = {10.1109/TMC.2025.3531919},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenJH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed learning framework that allows clients to jointly train a model by uploading parameter updates rather than sharing local data. FL deployed on a client-edge-cloud hierarchical architecture, named Hierarchical Federated Learning (HFL), can accelerate model training and accommodate more clients with reduced communication cost via edge aggregation. Unfortunately, HFL suffers from privacy risks since the submitted parameters from clients are vulnerable to privacy attacks. To address this issue, we propose a novel Differential Privacy (DP) definition tailored for HFL, i.e., Group Local Differential Privacy (GLDP). We design the Sampling-Randomizing-Shuffling (SRS) mechanism to implement GLDP in HFL, where the sampling process is employed to achieve a stronger level of privacy protection with less noise added. By combining the randomized response and the shuffling mechanism, our proposed SRS mechanism can achieve client-level personalization within  ρ k \\rho _{k} -GLDP for privacy preservation while balancing model performance and privacy protection in HFL. Privacy analysis and convergence analysis are conducted to provide theoretical performance guarantees. Experimental results based on real-world datasets verify the effectiveness of SRS.}
}


@article{DBLP:journals/tmc/ZhaoWGZQ25,
	author = {Mingxiong Zhao and
                  Zirui Wang and
                  Kun Guo and
                  Rongqian Zhang and
                  Tony Q. S. Quek},
	title = {Against Mobile Collusive Eavesdroppers: Cooperative Secure Transmission
                  and Computation in UAV-Assisted {MEC} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5280--5297},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529929},
	doi = {10.1109/TMC.2025.3529929},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoWGZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Uncrewed Aerial Vehicle (UAV)-assisted Mobile Edge Computing (MEC) networks, the security of transmission faces significant challenges due to the vulnerabilities of line-of-sight links and potential eavesdropping on two-hop links. This paper addresses these challenges with an innovative Cooperative Secure Transmission and Computation strategy (CSTC), specifically engineered for time-slotted UAV-assisted MEC networks plagued by mobile collusive eavesdroppers. These eavesdroppers significantly bolster their interception capabilities through coordinated and optimized movements, escalating the security threats. To neutralize these risks, the proposed CSTC employs the UAV and remote devices as helper nodes to emit jamming signals, thereby thwarting eavesdropping activities, while simultaneously facilitating the efficient relay of users’ tasks to the base station for advanced processing. The CSTC aims to maximize the sum Secrecy Transmission Rate (STR) satisfying task latency constraints. It involves a joint optimization of UAV trajectory, jamming beamformers, transmit power, and data offloading strategy to expedite task transmission. Additionally, a real-time computation scheduling approach is developed based on a newly defined metric, the Urgency Degree of Users (UDoU), to enhance task processing efficiency. Our extensive simulations validate that the CSTC not only elevates the sum STR but also consistently meets latency constraints, demonstrating its robustness against advanced mobile eavesdropping techniques.}
}


@article{DBLP:journals/tmc/HanPWL25,
	author = {Xinhui Han and
                  Haoyuan Pan and
                  Zhaorui Wang and
                  Jianqiang Li},
	title = {Successive Interference Cancellation-Enabled Timely Status Update
                  in Linear Multi-Hop Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5298--5311},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3529462},
	doi = {10.1109/TMC.2025.3529462},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanPWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the timely status update in linear multi-hop wireless networks, where a source tries to deliver status update packets to a destination through a sequence of half-duplex relays. Timeliness is measured by the age of information (AoI) metric. Maintaining a low AoI at the destination typically necessitates frequent transmission of update packets from the source. However, high packet transmission frequency in multi-hop scenarios can result in mutual wireless interference at intermediate relays. Specifically, when an intermediate relay receives wireless signals of a new packet from its previous node, simultaneous transmission of an old packet by its subsequent node to the next hop may cause wireless signals to interfere at the intermediate relay, conventionally leading to packet collision. A key motivation to solve this issue is that the intermediate relay has previously received the old packet (which can thus be forwarded to the subsequent node for further relaying). Hence, successive interference cancellation (SIC) can be employed to mitigate interference of the old packet and recover the new packet. This paper designs an SIC-enabled packet relaying scheme tailored to low AoI. Initially focusing on a three-hop network, we subsequently extend our approach to general multi-hop networks. We model the multi-hop relaying scheme using a Markov chain to derive the theoretical average AoI. Theoretical and simulation results indicate that the SIC-enabled packet relaying scheme significantly reduces the average AoI compared to the non-SIC approaches, owing to an increased packet transmission frequency at the source and the effectiveness of SIC techniques at the relays.}
}


@article{DBLP:journals/tmc/ChaoWSHC25,
	author = {Ke Chao and
                  Shengling Wang and
                  Hongwei Shi and
                  Jian{-}Hui Huang and
                  Xiuzhen Cheng},
	title = {Causal Analysis and Risk Assessment for Batch Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5312--5323},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532285},
	doi = {10.1109/TMC.2025.3532285},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChaoWSHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The way of task posting serves as the main pillar in achieving an efficient crowdsourcing market. Pioneer solutions on task posting can be categorized as retail task posting and batch task posting. Unlike retail task posting, which simply matches the most suitable worker to tasks, batch task posting considers the collaborations not only between workers and tasks but also among tasks, which brings high efficiency, low costs, and satisfactory task completion rates. However, the state of the arts on batch task posting leverage specific attributes to combine tasks as bundles for posting, leading to limited scalability. Hence, we propose a causal analysis framework for batch crowdsourcing to achieve an attribute-independent batch crowdsourcing solution that disentangles multi-factors to uncover the posting merits of tasks bundled at optimal prices, based on which an approximately optimal algorithm is further introduced to form reasonable bundles for posting. Since batch crowdsourcing may incur losses due to short-term profit fluctuation, a risk assessment method is proposed to encourage the requestor to act properly for loss mitigations. Our work explores the causal analysis and risk assessment in batch crowdsourcing for the first time, with the following highlights: 1) generality. It proposes a composite metric for gauging task bundles which avoids the issue of attribute dependence in the state of the arts, resulting in better universality; 2) synergy. By collaboratively considering the “value” and “relative position” of variables, our work derives results reflecting causal relationships rather than naive correlations; and 3) precision. We not only elucidate the probability of risk in batch crowdsourcing but also delineate the rate function governing its probability decay. This allows a requestor to know when and how fast to halt batch task posting.}
}


@article{DBLP:journals/tmc/FanCCL25,
	author = {Wenhao Fan and
                  Penghui Chen and
                  Xiongfei Chun and
                  Yuan'an Liu},
	title = {MADRL-Based Model Partitioning, Aggregation Control, and Resource
                  Allocation for Cloud-Edge-Device Collaborative Split Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5324--5341},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530482},
	doi = {10.1109/TMC.2025.3530482},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanCCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Split Federated Learning (SFL) has emerged as a promising paradigm to enhance FL by partitioning the Machine Learning (ML) model into parts and deploying them across clients and servers, effectively mitigating the workload on resource-constrained devices and preserving privacy. Compared to cloud-device-based and edge-device-based SFL, cloud-edge-device collaborative SFL offers both lower communication latency and wider network coverage. However, existing works adopt a uniform model partitioning strategy for different devices, ignoring the heterogeneous nature of device resources. This oversight leads to severe straggler problems, making the training process inefficient. Moreover, they do not consider joint optimization of model aggregation control and computing and communication resource allocation, and lack distributed algorithm design. To address these issues, we propose a joint resource management scheme for cloud-edge-device collaborative SFL to optimize the training latency and energy consumption of all devices. In our scheme, the partitioning strategy is optimized for each device based on resource heterogeneity. Meanwhile, we jointly optimize the aggregation frequency of ML models, computing resource allocation for all devices and edge servers, and transmit power allocation for all devices. We formulate a coordination game among all edge servers and then design a distributed optimization algorithm employing partially observable Multi-Agent Deep Reinforcement Learning (MADRL) with integrated numerical methods. Extensive experiments are conducted to validate the convergence of our algorithm and demonstrate the superiority of our scheme via evaluations under multiple scenarios and in comparison with four reference schemes.}
}


@article{DBLP:journals/tmc/HuTWH25,
	author = {Gang Hu and
                  Yinglei Teng and
                  Nan Wang and
                  Zhu Han},
	title = {Faster Convergence on Heterogeneous Federated Edge Learning: An Adaptive
                  Clustered Data Sharing Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5342--5356},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533566},
	doi = {10.1109/TMC.2025.3533566},
	timestamp = {Mon, 15 Dec 2025 14:11:41 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuTWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Edge Learning (FEL) emerges as a pioneering distributed machine learning paradigm for the 6\xa0G Hyper-Connectivity, harnessing data from the IoT devices while upholding data privacy. However, current FEL algorithms struggle with non-independent and non-identically distributed (non-IID) data, leading to elevated communication costs and compromised model accuracy. To address these statistical imbalances, we introduce a clustered data sharing framework, mitigating data heterogeneity by selectively sharing partial data from cluster heads to trusted associates through sidelink-aided multicasting. The collective communication pattern is integral to FEL training, where both cluster formation and the efficiency of communication and computation impact training latency and accuracy simultaneously. To tackle the strictly coupled data sharing and resource optimization, we decompose the optimization problem into the clients clustering and effective data sharing subproblems. Specifically, a distribution-based adaptive clustering algorithm (DACA) is devised basing on three deductive cluster forming conditions, which ensures the maximum sharing yield. Meanwhile, we design a stochastic optimization based joint computed frequency and shared data volume optimization (JFVO) algorithm, determining the optimal resource allocation with an uncertain objective function. The experiments show that the proposed framework facilitates FEL on non-IID datasets with faster convergence rate and higher model accuracy in a resource-limited environment.}
}


@article{DBLP:journals/tmc/LuoZHXCDWJ25,
	author = {Ruikun Luo and
                  Zujia Zhang and
                  Qiang He and
                  Mengxi Xu and
                  Feifei Chen and
                  Xiaohai Dai and
                  Song Wu and
                  Hai Jin},
	title = {Cost-Effective Edge Data Caching With Failure Tolerance and Popularity
                  Awareness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5357--5369},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531967},
	doi = {10.1109/TMC.2025.3531967},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoZHXCDWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the mobile edge computing environment, caching data in edge storage systems can significantly reduce data retrieval latency for users while saving the costs incurred by cloud-edge data transmissions for app vendors. Existing edge data caching (EDC) methods prioritize popular data and aim to minimize users’ data retrieval latency and system storage costs jointly. However, these EDC methods often rely on the assumption that data popularity always follows certain distributions. As a result, they cannot properly adapt to the fluctuations in data popularity due to user mobility or unexpected increases in user demands. Meanwhile, unlike cloud data centers, complex and fragile edge servers are more likely to experience physical failures or network outages, presenting new challenges for EDC strategies. Specifically, when an edge server fails or experiences an outage, cached data may become temporarily unavailable, leading to increased latency as requests are redirected to alternative servers or the cloud. In this paper, to enable uncertainty-aware edge data caching (uEDC), we first model the problem as a robust optimization problem and propose an optimal algorithm named uEDC-B to find the optimal uEDC solution. To address the high computational complexity of uEDC-B, we introduce an approximate algorithm named uEDC-L based on linear decision rules. Theoretical analysis and extensive experiments on a real-world dataset demonstrate that the proposed methods outperform two state-of-the-art approaches in handling the uncertainties in data popularity and edge server failure with a significant performance improvement of 59.27% in data retrieval latency and 55.07% in data caching cost.}
}


@article{DBLP:journals/tmc/WangLLQL25,
	author = {Jianhui Wang and
                  Zhetao Li and
                  Haolin Liu and
                  Tie Qiu and
                  Hongbin Luo},
	title = {A Trust-Based Computation Offloading Framework in Mobile Cloud-Edge
                  Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5370--5385},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530480},
	doi = {10.1109/TMC.2025.3530480},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLLQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud service centers (CSCs) can purchase edge computation resources to improve service quality in mobile cloud-edge computing networks. However, edge servers (ESs) are owned by different entities, and dishonest entities may launch computational forgery attacks, i.e., the ES falsely reports its idle computation resources to win more tasks for increased revenue. Most existing approaches ignore the threat of dishonest ESs. To address the challenges, we design a Trust-based Computation Offloading (TCO) framework. First, we construct the problem for minimizing the difference between the CSC's cost and the expected revenue (DCER), which is a mixed-integer nonlinear programming problem. Second, we develop a trust-based computation offloading method that quickly finds a good solution by decomposing the problem. Finally, a two-tier trust evaluation method was proposed to obtain accurate trust values. Experimental results indicate that TCO's comprehensive performance surpasses the benchmarks and significantly enhances computation offloading reliability with a lower performance loss. Notably, tasks are preferentially offloaded to honest ESs to ensure their revenue and promote ESs’ honesty under the TCO framework. Additionally, compared with no trust mechanisms, TCO reduces the service timeout count in an interval by 34.37% - 73.80% with a performance loss of only 1.42% - 4.10%.}
}


@article{DBLP:journals/tmc/NamLLL25,
	author = {Wooseung Nam and
                  Sungyong Lee and
                  Joohyun Lee and
                  Kyunghan Lee},
	title = {nCTX: {A} Neural Network-Powered Lossless Compressive Transmission
                  Using Shared Information},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5386--5399},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530950},
	doi = {10.1109/TMC.2025.3530950},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NamLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we explore the possibility of a new delivery method for lossless data, namely compressive transmission. It aims at minimizing the transmission data volume at runtime by exploiting the tailored information shared between the sender and the receiver. There are two approaches to leverage shared information for compression: 1) using a DNN-based codec as a proxy for shared information and 2) applying redundancy elimination using deduplication. However, these approaches have not been studied in depth to utilize the trade-off between the compression rate and the amount of shared information. Compared to these approaches, compressive transmission is unique as it fully leverages the abundance of information available on both sides, which is chosen and placed purposely. To bring the concept to reality, we propose nCTX, a neural network-powered Compressive Transmission System that adaptively exploits a generative model and matching blocks. nCTX extracts the optimal semantic data from the input data, exploiting shared information to closely imitate the original and compensate it with the offset (i.e., difference). Extensive evaluations in mobile platforms confirm that nCTX reduces the transmission volume significantly by 25.8% and 23.3% compared to FLIF and RC, the state-of-the-art image codecs, respectively, in comparable or shorter computation times.}
}


@article{DBLP:journals/tmc/TiranaTIC25,
	author = {Joana Tirana and
                  Dimitra Tsigkari and
                  George Iosifidis and
                  Dimitris Chatzopoulos},
	title = {Minimization of the Training Makespan in Hybrid Federated Split Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5400--5417},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533033},
	doi = {10.1109/TMC.2025.3533033},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TiranaTIC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parallel Split Learning (SL) allows resource-constrained devices that cannot participate in Federated Learning (FL) to train deep neural networks (NNs) by splitting the NN model into parts. In particular, such devices (clients) may offload the processing task of the largest model part to a computationally powerful helper, and multiple helpers may be employed and work in parallel. In hybrid federated and split learning (HFSL), on the other hand, devices can participate in the training process through any of the two protocols (SL and FL), depending on the system's characteristics. This could considerably reduce the maximum training time over all clients (makespan), especially in highly heterogeneous scenarios. In this paper, we study the joint problem of the training protocol selection, client-helper assignments, and scheduling decisions, to minimize the training makespan. We prove this problem is NP-hard and propose two solution methods: one based on the decomposition of the problem by leveraging its inherent symmetry, and a second fully scalable one. Through numerical evaluations using our testbed's measurements, we build a solution strategy comprising these methods. Moreover, this strategy finds a near-optimal solution and achieves a shorter makespan than the baseline schemes by up to 71%.}
}


@article{DBLP:journals/tmc/WangCWN25,
	author = {Ran Wang and
                  Lundan Cai and
                  Qiang Wu and
                  Dusit Niyato},
	title = {Service Function Chain Deployment With Intrinsic Dynamic Defense Capability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5418--5432},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532210},
	doi = {10.1109/TMC.2025.3532210},
	timestamp = {Sun, 14 Sep 2025 12:42:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCWN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Service Function Chain (SFC) leverages Network Function Virtualization (NFV) and Software-Defined Networking (SDN) for flexible deployment, creating customized service chains tailored to specific applications. As NFV and SDN technologies play crucial roles in the SFC implementation, any security risk that arises in an NFV/SDN network can potentially pose a threat to SFC. Thus, SFC becomes vulnerable to network security attacks. To address this, intrinsic security technologies, including moving target defense and mimic defense, offer proactive protection against both known and unknown threats. It is expected to break through traditional security protection mechanisms such as “enhanced”, “plug-in” and “passive” defense. This paper proposes an intrinsic dynamic defense architecture to equip SFC with active defense capabilities, shifting from passive reactive mechanism based on prior knowledge to an active defense against various attacks. The architecture comprises two models and five modules, including a sub-pool partitioning algorithm that enhances heterogeneity across sub-pools by splitting the heterogeneous replica pool into several sub-pools among replica VNFs. To meet Quality of Service (QoS) requirements like latency, cost, and security, we formulate a multi-objective optimization problem with three objectives: latency, cost, and defense success rate. Following that, we propose a dynamic Deep Reinforcement Learning (DRL)-based deployment algorithm. This algorithm selects appropriate VNFs based on heterogeneity and historical information, improving SFC and VNF security against external attacks. Extensive experiments validate that our architecture significantly enhances network security, provided that this improvement comes at the expense of limited cost and latency.}
}


@article{DBLP:journals/tmc/JayabalWGZPS25,
	author = {Raymond J. Jayabal and
                  David Tung Chong Wong and
                  Lee Kee Goh and
                  Xiaojuan Zhang and
                  Chin Ming Pang and
                  Sumei Sun},
	title = {Survey, Design and Evaluation of {TGT-HC:} {A} Time-Aware Shaper {MAC}
                  for Wireless {TSN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5433--5445},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3535413},
	doi = {10.1109/TMC.2025.3535413},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JayabalWGZPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-Reliable Low-Latency Communication (URLLC) and Time-Sensitive Networking (TSN) are essential for enhancing 5G and Wi-Fi 6/7 to support real-time industrial automation. However, our survey shows that existing Medium Access Control (MAC) schemes still face unresolved latency issues. This paper introduces the Transmission Gating Time Hyperchannel (TGT-HC), a novel contention-free Carrier-Sense Multiple Access (CSMA) scheme driven by a per-flow Time-Aware Shaper (TAS) scheduler. Our analytical results, simulations, and prototyping with the Universal Software Radio Peripheral (USRP) demonstrate that TGT-HC achieves latency performance comparable to a First-Come-First-Served (FCFS) single server for real-time cyclic traffic, even under high frame error rates (FERs). Given its promising performance, we advocate for reconsidering contention-free CSMA as a viable MAC scheme in next-generation URLLC/TSN.}
}


@article{DBLP:journals/tmc/BiJ25,
	author = {Ye Bi and
                  Chunfu Jia},
	title = {Towards Resilience 5G-V2N: Efficient and Privacy-Preserving Authentication
                  Protocol for Multi-Service Access and Handover},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5446--5463},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532120},
	doi = {10.1109/TMC.2025.3532120},
	timestamp = {Sat, 31 May 2025 23:18:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BiJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The booming 5G cellular networks sparked tremendous interest in supporting more sophisticated critical use cases through vehicle-to-network (V2N) communications. However, the inherent technical vulnerabilities and densification of 5G raise new security and efficiency challenges. The existing secondary authentication fails to support multi-service access. The random access process lacks authentication of the gNB, possibly leading to fake base station attacks (FBS). Moreover, related research extends key forward/backward secrecy (KF/BS) to require that it also applies to gNBs, thus invalidating most existing schemes. This paper introduces a comprehensive security framework for 5G-V2N that seamlessly integrates with existing standardized architecture to provide privacy-preserving mutual authentication and key agreement for the full service cycle. Specifically, we propose new secondary authentication involving gNBs and support single request access to multi-services. Second, incorporating the service migration idea, we design the g2g (gNB-to-gNB) channel establishment phase to promote secure context share. Finally, the proposed efficient handover phase achieves the security properties of enhanced KF/BS, known randomness secrecy and privacy-preserving, and avoids FBS. We verify the proposed protocol using three different formal techniques: provably secure, BAN-logic, and AVISPA tool. Extensive experimental results and comparison show that our scheme excels in computational and communication efficiencies, and detecting malicious events.}
}


@article{DBLP:journals/tmc/SimonAWFHK25,
	author = {Bernd Simon and
                  Paul Adrian and
                  Patrick Weber and
                  Patrick Felka and
                  Oliver Hinz and
                  Anja Klein},
	title = {A Bargaining Approach for Service Placement in Multi-Access Edge Computing
                  With Information Asymmetries},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5464--5481},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533045},
	doi = {10.1109/TMC.2025.3533045},
	timestamp = {Sat, 31 May 2025 23:18:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SimonAWFHK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC) refers to deploying computation resources, known as cloudlets or edge servers, near the edge of the mobile network. Services like augmented reality (AR) benefit from MEC by service placement, which refers to installing service-specific software and allocating resources on cloudlets. Service placement in MEC improves service quality and potentially reduces costs compared to centralized cloud computing approaches. The main stakeholders in MEC are infrastructure providers (IPs), who manage the MEC infrastructure, and service providers (SPs), who offer services to users. Both have unique technical and economic perspectives, such as resource demands, resource availability, and costs. Information asymmetries exist as only IPs have access to information about their resources, and only SPs have information about service usage and resource demands. This work addresses challenges of service placement in MEC from a multi-stakeholder, techno-economic perspective. We introduce a model including the stakeholders’ technical and economic goals and information asymmetries. To solve this problem efficiently, we propose a multi-stakeholder bargaining mechanism, termed Nash Backward Induction with Linear Equilibrium Strategies (NBI-LES). In a case study with 544 users and 16 SPs, we achieve  79 % \\text{79}{\\%}  of the optimal reduction in traffic given by a centralized optimal service placement strategy.}
}


@article{DBLP:journals/tmc/YuLAS25,
	author = {Liangkun Yu and
                  Zhirun Li and
                  Nirwan Ansari and
                  Xiang Sun},
	title = {Hybrid Transformer Based Multi-Agent Reinforcement Learning for Multiple
                  Unpiloted Aerial Vehicle Coordination in Air Corridors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5482--5495},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532204},
	doi = {10.1109/TMC.2025.3532204},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuLAS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advanced Air Mobility (AAM) seeks to establish a next-generation air transportation system by leveraging autonomous unpiloted aerial vehicles (UAVs) to transport passengers and cargo between locations previously underserved or unserved by traditional aviation. Achieving AAM at scale requires overcoming significant challenges in airspace management, classification, and traffic control to safely accommodate the increasing volume of UAV operations. This paper presents a comprehensive design for air corridors to facilitate efficient aerial transport and formulates a multi-UAV coordination problem within these corridors. The objective is to enable each UAV to autonomously make control decisions based on local observations gathered from onboard sensors. This decentralized control approach is modeled as a multi-agent partially observable Markov decision process (POMDP), aiming at minimizing UAV travel time while ensuring adherence to corridor boundaries and collision avoidance. To address the complexities posed by varying state dimensions and types, we propose a novel Hybrid Transformer-based Multi-agent Reinforcement Learning (HTransRL) architecture. HTransRL integrates a customized transformer model into an actor-critic network, effectively processing both sequential and non-sequential observed states of varying sizes while capturing their correlations. This enables safe and efficient UAV navigation. Simulation results show that in test environments similar to or simpler than training scenarios, HTransRL achieves a successful arrival rate exceeding 90% in worst-case test scenarios. In test environments more complex than training scenarios, HTransRL demonstrates superior scalability compared to two baseline methods, achieving higher arrival rates and comparable travel times.}
}


@article{DBLP:journals/tmc/ZhanYFHXY25,
	author = {Cheng Zhan and
                  Huan Yan and
                  Rongfei Fan and
                  Han Hu and
                  Shubin Xu and
                  Jian Yang},
	title = {Online Energy and Interference Management for Dynamic Target Tracking
                  With Cellular-Connected {UAV}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5496--5510},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532276},
	doi = {10.1109/TMC.2025.3532276},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanYFHXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular-connected Unmanned Aerial Vehicles (UAVs) have significant potential for target tracking in future cellular networks due to their broad coverage and operational flexibility. In this paper, we consider a multi-cell cellular network with a cellular-connected UAV for target tracking, which encounters challenges such as unpredictable flight energy consumption from the stochastic movements of the tracking target and severe uplink interference from ground devices (GDs). To tackle these challenges, we propose a multi-stage stochastic optimization framework focused on energy-efficient target tracking with interference coordination. Our objective is to optimize the long-term average uplink throughput of both aerial users and GDs by jointly optimizing the UAV's trajectory, power allocation, and cell association across multiple orthogonal communication resource blocks (RBs). The formulated stochastic non-convex problem is first transformed into a deterministic problem for each time slot by using the Lyapunov optimization framework. An online optimization strategy is proposed, utilizing the optimal structure, alternative optimization, and successive convex approximation (SCA) techniques. Simulation results show that the proposed approach significantly enhances network throughput and UAV energy queue stability compared to existing baseline schemes.}
}


@article{DBLP:journals/tmc/HuangLWYDFL25,
	author = {Zihan Huang and
                  Tong Li and
                  Xing Wang and
                  Kexin Yang and
                  Chao Deng and
                  Junlan Feng and
                  Yong Li},
	title = {Predicting Mobile App Usage With Context-Aware Dynamic Hypergraphs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5511--5524},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3532992},
	doi = {10.1109/TMC.2025.3532992},
	timestamp = {Thu, 21 Aug 2025 07:47:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLWYDFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {App usage prediction aims to predict the next app most likely to be used based on historical behaviors, which is beneficial for smartphone system optimization, such as system resource management, battery energy optimization, and user experience enhancement. Existing studies have treated it as a simple time series prediction problem and overlooked the sessionization characteristic of mobile app usage, i.e., neglecting the intent context in which the user interacts with apps. In this paper, we explore the context of user intents and incorporate app sessionization features into prediction models to improve prediction accuracy. Specifically, we first extract the semantic meaning of spatio-temporal contextual information of app usage by constructing an urban knowledge graph. Second, we devise a hypergraph-based embedding model to extract the hyper-relations of intra-session apps. Third, we utilize a self-attention mechanism to fuse intra-session apps’ representations and combine spatio-temporal contextual embedding to form the session representation. We further leverage a transformer for inter-session intent transition modeling to extract users’ dynamic intent (i.e., the semantic meaning of sessions) for app usage. Finally, we jointly fuse dynamic intent and recently used app features using the MLP model for the prediction. The novelty of our method is that we are the first to leverage dynamic hypergraphs for modeling sessionization features, and we model both inter-session and intra-session relations. We evaluate our model based on two real-world datasets collected in Shanghai and Nanchang. In terms of prediction accuracy, mean reciprocal rank, and normalized discounted cumulative gain, our proposed framework outperforms state-of-the-art baselines by more than 30% in the Shanghai dataset and 20% in the Nanchang dataset, respectively.}
}


@article{DBLP:journals/tmc/MartinBMORB25,
	author = {Adrian Martin and
                  Isabel de la Bandera and
                  Adriano Mendo and
                  Jos{\'{e}} Outes and
                  Juan Ramiro{-}Moreno and
                  Raquel Barco},
	title = {Federated Deep Reinforcement Learning for {ENDC} Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5525--5535},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3534661},
	doi = {10.1109/TMC.2025.3534661},
	timestamp = {Sun, 15 Jun 2025 21:07:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MartinBMORB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G New Radio (NR) network deployment in Non-Stand Alone (NSA) mode means that 5G networks rely on the control plane of existing Long Term Evolution (LTE) modules for control functions, while 5G modules are only dedicated to the user plane tasks, which could also be carried out by LTE modules simultaneously. The first deployments of 5G networks are essentially using this technology. These deployments enable what is known as E-UTRAN NR Dual Connectivity (ENDC), where a user establish a 5G connection simultaneously with a pre-existing LTE connection to boost their data rate. In this paper, a single Federated Deep Reinforcement Learning (FDRL) agent for the optimization of the event that triggers the dual connectivity between LTE and 5G is proposed. First, single Deep Reinforcement Learning (DRL) agents are trained in isolated cells. Later, these agents are merged into a unique global agent capable of optimizing the whole network with Federated Learning (FL). This scheme of training single agents and merging them also makes feasible the use of dynamic simulators for this type of learning algorithm and parameters related to mobility, by drastically reducing the number of possible combinations resulting in fewer simulations. The simulation results show that the final agent is capable of achieving a tradeoff between dropped calls and the user throughput to achieve global optimum without the need for interacting with all the cells for training.}
}


@article{DBLP:journals/tmc/BaiHWLN25,
	author = {Tong Bai and
                  Bo Hou and
                  Zhipeng Wang and
                  Dong Liu and
                  Arumugam Nallanathan},
	title = {Content-Aware Joint Knob Configuration and Resource Allocation for
                  Edge Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5536--5550},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533596},
	doi = {10.1109/TMC.2025.3533596},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaiHWLN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Characterized by its ease of low-latency response, edge computing is capable of supporting real-time video analytics applications, constituting an edge video analytics paradigm, where the joint knob configuration and network scheduling design has drawn ever-escalating research attention. However, the potential of edge video analytics has not been fully exploited, owing to the limitations of the state-of-the-art as follows. i) The eminent impact of video content on accuracy performance has been ignored. ii) The variables that can be tuned are not fully considered in scheduling. iii) The heuristic algorithm-based solutions are far from the optimal. To fill in this gap, in this paper, we conceive a content-aware joint knob configuration and resource allocation scheme for edge video analytics. Concretely, fed with the features extracted from the video content, a deep neural network (DNN)-based predictor is proposed to predict the configuration-accuracy performance in a real-time manner. With an aid of the predictive results, we formulate an accuracy-maximization problem as an integer programming problem, by optimizing the variables, including resolution, frame rate, video analytic model, network bandwidth, and computational resource subject to the latency constraints. To solve this problem in an efficient manner, we devise a novel low-complexity dynamic programming method. Simulation results verify the efficiency of our content-aware joint knob configuration and resource allocation scheme. Quantitatively, a 3.3% gap is attained towards the upper bound in terms of the accuracy in an object detection scenario, relying on the scheme proposed.}
}


@article{DBLP:journals/tmc/SongSYN25,
	author = {Liangjun Song and
                  Gang Sun and
                  Hongfang Yu and
                  Dusit Niyato},
	title = {{ESPD-LP:} Edge Service Pre-Deployment Based on Location Prediction
                  in {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5551--5568},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533005},
	doi = {10.1109/TMC.2025.3533005},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongSYN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of real-time applications, services has made Multi-access Edge Computing (MEC) essential for delivering low-latency, high-performance computing. The effectiveness of MEC, however, is largely contingent on the efficient pre-deployment of services. Despite its importance, efficient service pre-deployment is challenged by the inherent unpredictability of user mobility, the fluctuating conditions of network environments. Accurately predicting user locations, dynamically optimizing resource allocation across geographically distributed MEC servers are complex tasks that are essential to minimizing latency, maximizing data transmission efficiency. The variability in user movement patterns, network bandwidth further exacerbates these challenges, often leading to increased latency, diminished performance, which can negate the advantages offered by MEC. To address these challenges, this paper introduces a novel edge service pre-deployment scheme based on location prediction (ESPD-LP). The ESPD-LP scheme leverages historical user trajectory data to predict future locations, facilitating proactive, strategic resource allocation via a user-centric bidirectional matching algorithm across multiple MEC servers. By pre-deploying services in anticipation of user needs, this approach optimizes data transmission rates, reduces pre-deployment latency, significantly enhancing the overall performance of MEC systems. A comprehensive analysis reveals that the ESPD-LP scheme consistently outperforms similar approaches, with a 41% increase in data transmission rates, a 31% reduction in pre-deployment latency compared to the JO-CDSD, MEC-RDESN schemes, demonstrating consistently superior performance.}
}


@article{DBLP:journals/tmc/YueYKS25,
	author = {Gaofeng Yue and
                  Li Yan and
                  Liuwang Kang and
                  Chao Shen},
	title = {AdapLDP-FL: An Adaptive Local Differential Privacy for Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5569--5583},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533090},
	doi = {10.1109/TMC.2025.3533090},
	timestamp = {Sun, 15 Jun 2025 20:52:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YueYKS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a technique that allows multiple participants to co-train machine learning models, while also enhancing privacy by avoiding the exposure of local data. However, it is important to note that despite its effectiveness, there is still a potential risk of leaking users’ private information through weight analysis during FL updates. Local Differential Privacy (LDP) is a technique used to prevent individual information leakage by adding noise to the user's model parameters. However, FL based on LDP lacks dynamic optimization and adaptation considering privacy and data utility, especially regarding noise constraints. This paper investigates FL under the scenario of noise optimization with LDP. Specifically, given a certain privacy budget, we design the adaptive LDP method via a noise scaler, which adaptively optimizes the noise size of every client. Second, we dynamically tailor the model direction after adding noise by the designed a direction matrix, to overcome the model drift problem caused by adding noises to the client model. Finally, our method achieves higher accuracy than some existing works with the same privacy level and the convergence speed is significantly improved.}
}


@article{DBLP:journals/tmc/ZengZZZIZ25,
	author = {Zhihao Zeng and
                  Xiaoning Zhang and
                  Yangming Zhao and
                  Ahmed Zoha and
                  Muhammad Ali Imran and
                  Yan Zhang},
	title = {Accelerating Federated Codistillation via Adaptive Computation Amount
                  at Network Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {6},
	pages = {5584--5597},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3533591},
	doi = {10.1109/TMC.2025.3533591},
	timestamp = {Wed, 11 Jun 2025 21:01:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZengZZZIZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of Federated Learning (FL) empowers IoT devices to collectively train a shared model without local data exposure. In order to address the issue of Non-IID that causes model performance degradation, the recently proposed federated codistillation framework has shown great potential. However, due to the system heterogeneity of devices, the federated codistillation framework still faces a synchronization barrier issue, resulting in a non-negligible waiting time with a fixed computation amount (epoch or batch size) assigned. In this paper, we propose Adaptive Computation Amount Allocation (ACAA) to accelerate federated codistillation. Specifically, we leverage a criterion, solution inexactness, to quantify the computation amount. We dynamically adjust the solution inexactness of devices based on their computing power and bandwidth to enable them nearly simultaneous completion of training, reducing synchronization waiting time without sacrificing the training performance. The minimum required computation amount is determined by the coefficient of the distillation term and the gradient dissimilarity bound of Non-IID. We theoretically analyze the convergence of ACAA. Extensive experiments show that, compared to benchmark algorithms, ACAA can accelerate training by up to 5×.}
}


@article{DBLP:journals/tmc/XueSLWMWZX25,
	author = {Jingjing Xue and
                  Sheng Sun and
                  Min Liu and
                  Yuwei Wang and
                  Xuying Meng and
                  Jingyuan Wang and
                  Junbo Zhang and
                  Ke Xu},
	title = {Burst-Sensitive Traffic Forecast via Multi-Property Personalized Fusion
                  in Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5598--5614},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538871},
	doi = {10.1109/TMC.2025.3538871},
	timestamp = {Wed, 01 Oct 2025 11:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XueSLWMWZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For distributed network traffic prediction with data localization and privacy protection, Federated Learning (FL) enables collaborative training without raw data exchange across Base Stations (BSs). Nevertheless, traffic across BSs exhibit inherently heterogeneous trend burst and smooth fluctuation properties, but existing FL methods model single-scale series from only one view, which cannot simultaneously capture diverse trend and fluctuation properties, especially distinct burst distributions. In this paper, we propose Personalized Federated Forecasting with Multi-property Self-fusion (P2FMS), which can represent multi-scale traffic properties from different views. With precise multi-property representations, a fusion-level prediction decision is learned for each client in a personalized manner to promptly sense traffic bursts and improve forecasting performance in non-IID settings. Specifically, P2FMS decomposes the traffic series into distinct time scales, based on which, we effectively extract closeness, period, and trend properties from different views. The closeness and period are embedded through global-view representations with spatial correlations, while non-stationary trends are individually fitted from the client-side view. Furthermore, a personalized combiner is designed to accurately quantify the proportion of general fluctuation raws (i.e., closeness and period) and specific trend property in predictions, which enables multi-property self-fusion for each client to accommodate heterogeneous traffic patterns and enhance prediction accuracy. Besides, an alternant training mechanism is introduced to optimize property representation and fusion modules with the convergence guarantee. Extensive experiments on real-world datasets show that P2FMS outperforms status quo methods in both prediction performance and convergence time.}
}


@article{DBLP:journals/tmc/DengZLLX25,
	author = {Qingyong Deng and
                  Qinghua Zuo and
                  Zhetao Li and
                  Haolin Liu and
                  Yong Xie},
	title = {Privacy-Preserving Stable Data Trading for Unknown Market Based on
                  Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5615--5631},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3534201},
	doi = {10.1109/TMC.2025.3534201},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DengZLLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsensing Data Trading (CDT) has emerged as a novel data trading paradigm, where market stability is crucial during the transaction matching process. However, most existing CDT systems usually assume that the preferences of both parties are known and the third-party trading platform is trustworthy, which is impractical in real-world scenarios and leads to significant challenges in reliability and privacy preservation. To address these challenges, we propose a Privacy-Preserving and Stable Data Trading for Unknown Market based on Blockchain and Bilateral Reputation (PPSDT-UMBBR) scheme in the decentralized CDT system. First, a privacy-preserving bilateral preference initialization method is designed to achieve the initial matching of buyers and sellers without exposing their location and attribute privacy. Then, a stable matching method based on dynamic bilateral preference updating is proposed, integrating Differential Privacy, Stable matching theory, and a strategy based on Asymmetric Bilateral Preferences with Multi-Armed Bandits (DPS-ABPMAB). Finally, we theoretically analyze the security and prove that the market outcome is  δ \\delta -stable. Furthermore, compared to other benchmark methods based on real datasets, our proposed DPS-ABPMAB algorithm improves the average accumulative reward by at least 4.22%, and reduces the average accumulative regret and the mean evaluation error rate by at least 66.86% and 7.35%, respectively.}
}


@article{DBLP:journals/tmc/WangLZLXY25,
	author = {Haiyan Wang and
                  Penghui Liu and
                  Xiaoxiong Zhong and
                  Fucai Luo and
                  Bin Xiao and
                  Yuanyuan Yang},
	title = {{PAC-MC:} An Efficient Password-Based Access Control Framework for
                  Time Sequence Aware Media Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5632--5648},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3534861},
	doi = {10.1109/TMC.2025.3534861},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLZLXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud storage makes it easier for users to access and share data remotely, but it often requires integration with cryptographic technologies to address consumer-oriented applications, such as fine-grained data access, secure data sharing and retrieval. This paper focuses on the fine-grained access problem of media applications based on time sequence, that is, certain critical media applications based on time sequences should ideally be accessible only to authorized clients. The traditional keyword-based searchable encryption (SE) allows effective search and access over encrypted data while preserving data privacy, but most existing solutions do not support temporal access control (i.e., a mechanism that grants access permissions to users within a specified time range). In this paper, we propose PAC-MC, an efficient password-based access control framework for media cloud relying on content control with the time sequence attribute. PAC-MC not only supports multi-keyword search using any monotonic boolean formulas but also allows media owners to control content-encryption keys for different time periods with an updatable password. Furthermore, it supports the self-retrieval of content-encryption keys. In addition, PAC-MC is provably secure under the standard model. Finally, the detailed performance evaluation results and experimental comparisons indicate that PAC-MC is very efficient and outperforms the previous solutions in terms of computation, communication, and storage costs.}
}


@article{DBLP:journals/tmc/HuSZFY25,
	author = {Han Hu and
                  Kaifeng Song and
                  Cheng Zhan and
                  Rongfei Fan and
                  Jian Yang},
	title = {Joint Service Caching and Resource Allocation Over Different Timescales
                  in Satellite Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5649--5664},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3534779},
	doi = {10.1109/TMC.2025.3534779},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuSZFY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of edge computing into satellite networks offers a promising solution for extending computational services to remote and underserved areas. To effectively provide a variety of computing services, it is essential to cache the corresponding services on satellites. However, challenges exist such as dynamic computing requests that vary over time and space, energy constraints due to restricted power supply, as well as limited storage capacity on satellites and the impracticality of frequently adjusting service deployments. To tackle such challenges, this paper proposes a two-timescale joint optimization framework to minimize energy consumption in satellite edge computing networks while ensuring the delay requirements, by jointly optimizing service placement and task offloading, as well as computation resource and power allocation. On a larger timescale, we optimize service caching placement by strategically deploying services on satellites and ground devices (GDs) based on long-term service request statistics, aiming to minimize the total average delay over each time frame. We develop an efficient iterative algorithm by employing penalty-based methods and Lagrange duality techniques to achieve suboptimal service deployment. On a smaller timescale, we optimize task offloading and resource allocation in shorter time slots, adapting to dynamic traffic fluctuations to minimize energy consumption while meeting delay constraints. We utilize alternating optimization and quadratic transform methods to efficiently allocate resources and schedule tasks. Extensive simulations demonstrate the effectiveness and superiority of our framework over benchmark schemes, revealing significant reductions in delay and energy consumption. The results also highlight the trade-offs between task delay and energy consumption, as well as between transmit power and energy consumption.}
}


@article{DBLP:journals/tmc/OuyangZXLP25,
	author = {Yan Ouyang and
                  Feng Zeng and
                  Neal N. Xiong and
                  Anfeng Liu and
                  Witold Pedrycz},
	title = {{MWRS:} {A} MAB-Based Worker Recruitment Scheme With Tripartite Stackelberg
                  Game for Reliable Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5665--5680},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3535567},
	doi = {10.1109/TMC.2025.3535567},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OuyangZXLP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsensing (MCS) has emerged as a compelling paradigm for data sensing and collection, leveraging the widespread adoption of mobile devices and the active participation of numerous users. Despite its potential, MCS faces critical challenges, particularly in recruiting reliable workers and acquiring high-quality sensing data. Most existing approaches assume prior information on worker quality and are vulnerable to collusion attacks, especially having not comprehensively considered workers’ reliability and stability. To address these problems, we propose a Multi-Armed Bandit (MAB) based Worker Recruitment Scheme (MWRS) integrated with the Tripartite Stackelberg Game (TSG) for MCS. Specifically, a trust evaluation and truth inference mechanism is introduced to assess the trustworthiness of workers through active truth detection. To enhance recruitment quality, we employ a trust-aware worker selection mechanism that utilizes a modified Upper Confidence Bound (UCB) algorithm, achieving an optimal balance between exploration and exploitation. Furthermore, the interactions among participants are modeled using a TSG framework, which formulates their respective payoffs to determine optimal decision-making strategies, thus achieving mutually beneficial outcomes. Extensive evaluations on real-world datasets demonstrate that our proposed scheme improves total quality by up to 30.8% and reduces regret by up to 80.3% compared to existing methods.}
}


@article{DBLP:journals/tmc/TangLLLWL25,
	author = {Jian Tang and
                  Xiuhua Li and
                  Hui Li and
                  Penghua Li and
                  Xiaofei Wang and
                  Victor C. M. Leung},
	title = {Joint Class-Balanced Client Selection and Bandwidth Allocation for
                  Cost-Efficient Federated Learning in Mobile Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5681--5698},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539284},
	doi = {10.1109/TMC.2025.3539284},
	timestamp = {Sun, 01 Feb 2026 13:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangLLLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has significant potential to protect data privacy and mitigate network burden in mobile edge computing (MEC) networks. However, due to the system and data heterogeneity of mobile clients (MCs), client selection and bandwidth allocation is key for achieving cost-efficient FL in MEC networks with limited bandwidth. To address these challenges, we investigate the issue of joint client selection and bandwidth allocation for reducing the cost (i.e., latency and energy consumption) of FL training. We formulate the problem and decompose it into a holistic subproblem to reduce the number of rounds and a partial subproblem to reduce the costs of FL each round. We propose a joint class-balanced client selection and bandwidth allocation (CBCSBA) framework to address the whole problem. Specifically, for the holistic subproblem, CBCSBA combines MCs into groups, each having data distribution as close as possible to class-balanced distribution; For the partial subproblem, CBCSBA reduces costs by exploratively selecting a group and sequentially optimizing the latency and energy consumption of MCs within the group. Experimental results show that CBCSBA outperforms the baseline frameworks in reducing latency by 28.2% and energy consumption by 25.3% on average in the considered four datasets.}
}


@article{DBLP:journals/tmc/ShanYCCY25,
	author = {Xiaohu Shan and
                  Haiyang Yu and
                  Yurun Chen and
                  Yuwen Chen and
                  Zhen Yang},
	title = {{S2A-P2FS:} Secure Storage Auditing With Privacy-Preserving Flexible
                  Data Sharing in Cloud-Assisted Industrial IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5699--5715},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538057},
	doi = {10.1109/TMC.2025.3538057},
	timestamp = {Wed, 06 Aug 2025 18:51:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShanYCCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of the Industrial Internet of Things (IIoT) has led to an explosion of industrial data. Due to computing and storage capacity limitations, IIoT devices often outsource the collected data to remote cloud servers. Unfortunately, cloud storage and cloud sharing services are not as reliable as they claim to be. Existing schemes aim to check data integrity in the cloud through cloud auditing. However, they suffer from a number of security and privacy vulnerabilities. The challenge of designing a secure storage auditing framework for industrial IoT comes from two aspects: 1) lack of physical protection of data owner IIoT devices; 2) privacy issues due to auditing of sensitive shared data. Inspired by the aforementioned challenges, we design the secure storage audit framework to support flexible cloud data sharing in IIoT: S2A-P2FS. The first contribution in our work is the Polynomial Prefix Message Authentication Code(P2MAC) design. We design an innovative P2MAC data structure as a label, which can simultaneously achieve efficient data verification in cloud data storage and privacy protection in flexible cloud data sharing for cloud auditing. The second contribution is the design of a unique Physical Unclonable Function(PUF) for IIoT. Harsh industrial conditions hinder the stable operation of PUFs. To protect the trustness of IIoT data owners, we propose a robust PUF-based physical protection mechanism for IIoT devices. The key point is that the required key is not stored in the memory of IIoT but hidden within its physical structure. A security analysis was conducted to demonstrate the robustness of S2A-P2FS against known vulnerabilities. A prototype was implemented in a real-world IIoT scenario. Experimental results indicate that, compared to state-of-the-art schemes, S2A-P2FS achieves over a 3x speedup in computational time and requires only 67.5% of the communication cost.}
}


@article{DBLP:journals/tmc/ZhaoCWLHL25,
	author = {Yong Zhao and
                  Jiannong Cao and
                  Xingwei Wang and
                  Fuliang Li and
                  Qiang He and
                  Xiaojie Liu},
	title = {Power Optimization for Low Transmission Delay in Software Defined
                  Data Center Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5716--5730},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538791},
	doi = {10.1109/TMC.2025.3538791},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoCWLHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Data Center Networks (SDDCNs) utilizes Software Defined Networking (SDN) as a network architecture to achieve highly flexible, programmable, and automated management of Data Center Networks (DCNs). The high energy consumption of DCNs remains a persistent and significant challenge. Thus, the energy saving is crucial and imperative for DCNs. Current energy-efficient solutions primarily rely on flow consolidation and dynamic device sleeping techniques to reduce energy consumption. However, these approaches often yield long Flow Completion Time (FCT), potentially resulting in violations of service-level agreements, particularly for delay-sensitive applications. In this paper, we formulate the problem of minimizing the power consumption in SDDCNs as key objective while ensuring timely FCT for delay-sensitive applications. To solve this problem, we first introduce the Active Network Generation (ANG) approach, which generates a minimal active subnet with the least number of active devices while meeting the current traffic demand. Subsequently, we propose two algorithms based on the type of applications: the Delay-Tolerant Flow Route (DTFR) algorithm for delay-tolerant applications and the Delay-Sensitive Flow Route (DSFR) algorithm for delay-sensitive applications. Simulation results demonstrate that our propose solution achieves an energy-saving rate of up to 67.77% and significantly reduces FCT compared to benchmark solutions.}
}


@article{DBLP:journals/tmc/SunXLWKNM25,
	author = {Geng Sun and
                  Jian Xiao and
                  Jiahui Li and
                  Jiacheng Wang and
                  Jiawen Kang and
                  Dusit Niyato and
                  Shiwen Mao},
	title = {Aerial Reliable Collaborative Communications for Terrestrial Mobile
                  Users via Evolutionary Multi-Objective Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5731--5748},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3536093},
	doi = {10.1109/TMC.2025.3536093},
	timestamp = {Fri, 12 Sep 2025 07:38:44 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunXLWKNM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous aerial vehicles (AAVs) have emerged as the potential aerial base stations (BSs) to improve terrestrial communications. However, the limited onboard energy and antenna power of a AAV restrict its communication range and transmission capability. To address these limitations, this work employs collaborative beamforming through a AAV-enabled virtual antenna array to improve transmission performance from the AAV to terrestrial mobile users, under interference from non-associated BSs and dynamic channel conditions. Specifically, we introduce a memory-based random walk model to more accurately depict the mobility patterns of terrestrial mobile users. Following this, we formulate a multi-objective optimization problem (MOP) focused on maximizing the transmission rate while minimizing the flight energy consumption of the AAV swarm. Given the NP-hard nature of the formulated MOP and the highly dynamic environment, we transform this problem into a multi-objective Markov decision process and propose an improved evolutionary multi-objective reinforcement learning algorithm. Specifically, this algorithm introduces an evolutionary learning approach to obtain the approximate Pareto set for the formulated MOP. Moreover, the algorithm incorporates a long short-term memory network and hyper-sphere-based task selection method to discern the movement patterns of terrestrial mobile users and improve the diversity of the obtained Pareto set. Simulation results demonstrate that the proposed method effectively generates a diverse range of non-dominated policies and outperforms existing methods. Additional simulations demonstrate the scalability and robustness of the proposed CB-based method under different system parameters and various unexpected circumstances.}
}


@article{DBLP:journals/tmc/LiuWGPF25,
	author = {Yulin Liu and
                  Jie Wang and
                  Qinghua Gao and
                  Miao Pan and
                  Yuguang Fang},
	title = {Robust Device-Free mmWave Sensing With Specular Reflection Interference
                  Mitigation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5749--5764},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538112},
	doi = {10.1109/TMC.2025.3538112},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWGPF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-Free mmWave Sensing (DFWS) could sense target state by analyzing how target activities influence the surrounding mmWave signals. It has emerged as a promising sensing technology. However, when employing DFWS indoors, specular reflection interference arises due to the specular reflectors. This interference often induces ghost targets, impacting the accurate estimation of the number and position of targets, resulting in degradation in sensing performance. To tackle this issue, we delve into the generation mechanism of specular reflection interference and analyze its multi-domain characteristics. Through exploration, we discern its temporal sparsity, spatial symmetry or collinearity, and frequency correlation characteristics, and propose four metrics to measure them, accordingly. Specifically, we propose a temporal characteristic quantitative evaluation metric based on identity matching, spatial symmetry and collinearity quantitative evaluation metrics based on geometric analysis, and a frequency correlation quantitative evaluation metric based on Doppler velocity correction, respectively. Based on these metrics, we design a novel Specular Reflection Interference Mitigation (SRIM) method and develop a robust SRIM-DFWS prototype system based on a 60 GHz mmWave radar to validate our proposed method. Experimental results demonstrate that our proposed method could achieve accurate and effective mitigation of specular reflection interference in device-free target tracking.}
}


@article{DBLP:journals/tmc/KharjanaSS25,
	author = {Mebanjop Kharjana and
                  Subhas Chandra Sahana and
                  Goutam Saha},
	title = {Securing Autonomous {UAV} Cluster With Blockchain-Based Threshold
                  Key Management System Utilizing Crypto-Asset and Multisignature},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5765--5778},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538462},
	doi = {10.1109/TMC.2025.3538462},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KharjanaSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles deployed in remote locations rely on self-governed key management for their protection. However, conventional key management depends on a centralized ground-based station or single vehicle. Such a system is vulnerable to compromised certificate authority problems and single-points-of-failure. This paper proposed to resolve these vulnerabilities using a blockchain-based threshold key management system. The proposed system utilized blockchain’s concepts of crypto-asset and multisignature. Keys are defined as crypto-assets to improve their management in the blockchain network. Multisignature facilitates collaboration during key management based on a threshold value. The threshold value is also configurable to meet systems’ security and performance requirements. The proposed system secured the process of re-enforcement, sub-clustering, re-merging, and inter-cluster migration. Security analysis revealed that the proposed system complied with most key management security guidelines. The custom signature module used to authenticate intra-cluster communication was also verified as safe. Threats to the cluster were identified, assessed for risk, and mitigated accordingly. Performance analysis found that both AODV and DSDV routing protocols offer consistent performance but DSDV prevailed during the worst-case network scenario. The paper finally identified research gaps, including the requirement for an optimized mechanism for collecting consent signatures.}
}


@article{DBLP:journals/tmc/LiJMGT25,
	author = {Yue Li and
                  Xiangdong Jia and
                  Xiaoping Ma and
                  Yuxin Guo and
                  Hailong Tian},
	title = {A Tractable Analysis Model of Information Freshness for Mobile Edge
                  Computing Assisted IoT System With Layer-Coded {HARQ}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5779--5792},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539662},
	doi = {10.1109/TMC.2025.3539662},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiJMGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work focuses on an Internet of Things (IoT) status update system with the assistance of mobile edge computing (MEC) under short packet communications. The edge server includes a successive interference canceller (SIC) and a buffer pool. To simultaneously improve both information timeliness and throughput, a layer-coded hybrid automatic repeat request (L-HARQ) protocol is proposed especially considering backtrack decoding (BD) delay. In the MEC-assisted L-HARQ IoT status update system, the source successively transmits mixing updates to an access point (AP). The mixing update consists of the new generation and the part of the previously failed one. All the undecoded mixing packets are delivered to the edge server. Once a successful feedforward decoding (FD) occurs, the edge server attempts to recover undecoded mixing packets until a BD failure occurs or all buffers empty. The successful BD results are also delivered to the IR and the obtained prior information is fed back to the edge server for the next BD. By employing the stochastic hybrid system (SHS) model, the average age of informations (AoIs) are derived under circle-shift preemption (CS-P) and fixed preemption (F-P) policies. Considering the fact that the number of buffers is limited and the successful FD and BD do not always cause the reduction of system AoI, this work proposes a non-binary age evolution model as well as the state simplification mothed in SHS. The presented numerical results show that the CS-P and F-P policies obtain the enhanced information freshness compared with the non-preemptive blocking policy, and the CS-P policy outperforms the F-P one in terms of information freshness. The average AoI greatly depends on BD depth. When it is small, the average AoI is small. However, the two policies have the same average throughput that mainly depends on the size of buffer pool and the success probabilities of both FD and BD.}
}


@article{DBLP:journals/tmc/YuanWFS25,
	author = {Honggang Yuan and
                  Ting Wang and
                  Min Fu and
                  Yuanming Shi},
	title = {{GIRP:} Energy-Efficient QoS-Oriented Microservice Resource Provisioning
                  via Multi-Objective Multi-Task Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5793--5807},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547339},
	doi = {10.1109/TMC.2025.3547339},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuanWFS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microservice architecture has revolutionized web service development by facilitating loosely coupled and independently developable components distributed as containers or virtual machines. While existing studies emphasize end-to-end latency, this paper investigates energy-efficient quality-of-service (QoS)-oriented microservice provisioning, focusing on both QoS satisfaction and power consumption (PC) conservation. We propose the Green and Intelligent Resource Provision (GIRP) architecture, integrating a data-driven energy-latency-aware resource allocation and scheduling manager to balance latency and PC. To reconcile the trade-offs involved, a dual-objective optimization problem is formulated to minimize latency and energy use by selecting proper servers, allocating CPU cores, and determining service replicas. To address challenges with discrete variables, dual objectives, and implicit mappings, we leverage a model-free deep deterministic policy gradient-based reinforcement learning algorithm. Specifically, we develop a multi-task agent via the Multi-gate Mixture-of-Experts model to simultaneously make two separate actions regarding CPU core numbers and service replica numbers, followed by a single-task agent to determine service scheduling. Extensive experiments on the DeathStarBenchmark testbed validate GIRP’s effectiveness, demonstrating approximately 52% resource savings and a 43% reduction in PC compared to leading methods like Sinan, Firm, and heuristic-based algorithms. These results highlight GIRP’s capability to optimize microservice orchestration by balancing end-to-end latency and power efficiency.}
}


@article{DBLP:journals/tmc/LiGLLBG25,
	author = {Dawei Li and
                  Yuxiao Guo and
                  Di Liu and
                  Qifan Liu and
                  Song Bian and
                  Zhenyu Guan},
	title = {How to Prevent Social Media Platforms From Knowing the Images You
                  Share With Friends},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5808--5823},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3538885},
	doi = {10.1109/TMC.2025.3538885},
	timestamp = {Wed, 05 Nov 2025 10:44:46 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiGLLBG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in image sharing on social media platforms escalates private information extraction for commercial use, increasing user demand for privacy protection. However, the dynamics of group communication within online social networks and the image compression imposed by platforms present significant challenges to secure key exchange and reliable image sharing in existing solutions. In this paper, we propose PrivSocial to prevent social media platforms from extracting private information in images shared within group communications. Specifically, we propose two frameworks, a server-based framework and a subscription-based framework, making PrivSocial applicable to different social media platforms and providing users with optional security levels, enhancing the flexibility and efficiency. To achieve intra-group key agreement and ensure image privacy protection, both frameworks integrate optimized continuous group key agreement and a novel image encryption scheme resisting compression. We implement an Android-based Priv-raster application and deploy a prototype on Twitter. Furthermore, we evaluate the proposed encryption scheme, and experimental results show that it has efficient encryption and decryption performance while being resistant to jigsaw puzzle solver attacks. The multi-user simulation experiments also demonstrate that the processing time of a single user is mere milliseconds, and the scheme can efficiently support tens of thousands of groups.}
}


@article{DBLP:journals/tmc/LiuLCZSYW25,
	author = {Xiaochen Liu and
                  Fan Li and
                  Yetong Cao and
                  Shengchun Zhai and
                  Binghui Shi and
                  Song Yang and
                  Yu Wang},
	title = {A Wearable PPG-Based Monitoring System for Personalized Free Weight
                  Training},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5824--5838},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540165},
	doi = {10.1109/TMC.2025.3540165},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLCZSYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Free weight training (FWT) is of utmost importance for physical well-being. The success of FWT depends largely on choosing the suitable workload, as improper selections can lead to suboptimal outcomes or injury. Current workload estimation approaches rely on manual recording and specialized equipment with limited feedback. Therefore, we introduce PPGSpotter, a wearable PPG-based FWT monitoring system in a convenient, low-cost, and fine-grained manner. By characterizing the arterial geometry compressions caused by the deformation of distinct muscle groups, PPGSpotter can infer essential FWT factors such as current workload, repetitions, and exercise type and provide recommendations for workload adjustment. To remove pulse-related interference, we develop an arterial interference elimination approach based on adaptive filtering, effectively extracting the pure motion-derived signal (MDS). Furthermore, we explore 2D representations of MDS within the phase space to extract spatiotemporal information, enabling PPGSpotter to address the challenge of resisting sensor shifts. Finally, we leverage a multi-task CNN-based network and workload adjustment guidance to achieve personalized FWT monitoring. Extensive experiments with 15 participants confirm that PPGSpotter can achieve promising workload estimation (0.59 kg RMSE), repetitions estimation (0.96 reps RMSE), and exercise type recognition (91.57% F1-score) while providing valid workload adjustment recommendations (0.22 kg RMSE).}
}


@article{DBLP:journals/tmc/ZhangWZCL25,
	author = {Xinyi Zhang and
                  Chunyang Wang and
                  Yanmin Zhu and
                  Jian Cao and
                  Tong Liu},
	title = {Multi-Agent Deep Reinforcement Learning With Trajectory Prediction
                  for Task Migration-Assisted Computation Offloading},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5839--5856},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539945},
	doi = {10.1109/TMC.2025.3539945},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWZCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing has become an effective paradigm to provide offloading services for computation-intensive and delay-sensitive tasks on vehicles. However, high mobility of vehicles usually incurs spatio-temporal load-imbalances among edge servers. Therefore, task migration is employed to maintain dynamic workload balancing by transmitting excessive tasks from overloaded to underloaded servers. Recent studies adopt deep reinforcement learning approaches to generate offloading and migration decisions based on current observations of systems. However, we argue that the migration direction is highly dependent on vehicular movements, and task migration towards the wrong direction could lead to additional delays. Therefore, we emphasize the importance of guiding task migration via exploring prospective trajectories of vehicles. We propose a Mobility-Aware Cooperative Multi-Agent (MCMA) deep reinforcement learning approach to make vehicle-by-vehicle decisions in multi-edge computation offloading scenarios. A two-stage decision framework is designed to solve the joint optimization problem of computation offloading and resource allocation. Additionally, an Informer-based multi-step vehicular trajectory prediction module is incorporated to enhance the capability of forecasting vehicular movements. Extensive experiments and analysis are conducted on synthetic and realistic scenarios, showing that our approach consistently outperforms both heuristic and DRL-based methods. The simulation scenarios and source codes are publicly available here.}
}


@article{DBLP:journals/tmc/ZhangHMGG25,
	author = {Yongchao Zhang and
                  Jia Hu and
                  Geyong Min and
                  Jie Gao and
                  Nektarios Georgalas},
	title = {Real-Time Distributed Charging Station Recommendation for Electric
                  Vehicles: {A} Federated Meta-RL Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5857--5870},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539496},
	doi = {10.1109/TMC.2025.3539496},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHMGG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growth of Electric Vehicles (EVs) places an increasingly heavy burden on the limited charging infrastructure, necessitating an effective charging station recommendation strategy that assists EVs in finding the most suitable charging stations. Deep reinforcement learning is a promising technology that has been applied to optimize EVs’ charging recommendations. However, existing schemes have low scalability and high communication costs as they usually require collecting real-time information on both charging requests and charger availability at various stations during policy training or execution. To address this challenge, we develop a real-time distributed charging station recommendation approach, named ReDirect, to minimize the charging duration experienced by EVs, considering dynamic charging requests of EVs and fluctuating availability at charging stations. ReDirect employs federated meta-reinforcement learning (RL) to empower distributed stations to collaboratively learn effective recommendation strategies and make decisions without sharing their local information, yielding improved scalability, reduced communication overhead, and enhanced data privacy. Furthermore, we conduct a rigorous theoretical analysis of the convergence performance of ReDirect. Extensive experimental results on real-world datasets demonstrate that ReDirect performs closely to the centralized recommendation algorithm and outperforms several state-of-the-art distributed algorithms in EV charging duration while realizing a balanced distribution of charging requests across multiple stations.}
}


@article{DBLP:journals/tmc/RuanZYSCC25,
	author = {Yucheng Ruan and
                  Chengcheng Zhao and
                  Zeyu Yang and
                  Yuanchao Shu and
                  Peng Cheng and
                  Jiming Chen},
	title = {PicaCAN: Reverse Engineering Physical Semantics of Signals in {CAN}
                  Messages Using Physically-Induced Causalities},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5871--5887},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541102},
	doi = {10.1109/TMC.2025.3541102},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RuanZYSCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of Connected and Autonomous Vehicles, In-Vehicle Network attacks have garnered heightened research scrutiny due to vehicles’ increasing connectivities to the external environment. The common characteristic among these attacks is to tamper with targeted powertrain-related signals in the Powertrain Controller Area Network (PT-CAN) and further physically threaten vehicles’ safety. These powertrain-related signals are encoded within CAN messages grounded by the syntax specification, which is proprietary to Original Equipment Manufacturers and publicly unavailable. Thus, to undertake comprehensive security analysis and strategies, reverse engineering PT-CAN to the semantic level is urgently needed. However, the existing methods rely on interactions (injecting challenge signals/actions) with the targeted vehicle, and certain manual efforts are required. To fill this gap, we propose PicaCAN, a novel framework to extract signals from CAN messages and reverse engineer their physical semantics based on physically induced causality. Once access to the CAN traffic, PicaCAN offers the researcher an eye on the vehicle’s powertrain system, decoding binaries flows into powertrain-related signals automatically. We experimentally evaluate PicaCAN on PT-CAN of three automobiles containing two power types. The experimental results show that PicaCAN could successfully extract physical signals representing all targeted semantics (pedals, engine speed, etc.) from two Internal Combustion Engine Vehicles and one Hybrid Electric Vehicle under EV mode.}
}


@article{DBLP:journals/tmc/WangZHGS25,
	author = {Jie Wang and
                  Yao Zhu and
                  Yulin Hu and
                  Mustafa Cenk Gursoy and
                  Anke Schmeink},
	title = {Average Reliability-Optimal Offloading for Mobile Edge Computing in
                  Low-Latency Industrial IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5888--5902},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541661},
	doi = {10.1109/TMC.2025.3541661},
	timestamp = {Tue, 05 Aug 2025 13:26:42 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZHGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider a multi-access mobile edge computing (MEC) network with multiple sensors and one MEC server in industrial Internet of Things networks, where the MEC server provides a joint computation service (in the computation phase) for a set of sub-tasks offloaded by different sensors (in the communication phase). Due to the requirements of low latency and ultra reliability, we utilize finite blocklength information theory to characterize the reliability of the communication phase and exploit extreme value theory to investigate the delay violation probability in the computation phase. Following these characterizations, we derive the average end-to-end error probability of the entire service and provide two average end-to-end reliability-optimal design frameworks considering fixed frames structure and dynamic frames structure, in both of which the goal is to minimize the average end-to-end error probability by optimally allocating the total time length to each frame, as well as allocating each frame length to the communication phase and the computation phase. For the fixed frames structure, the original problem is decomposed, and the joint convexity of the decomposed sub-problems is rigorously proved, and the optimal solutions are obtained by the proposed optimal time allocation algorithm. Moreover, for the dynamic frames structure, we reformulate the optimization problem by introducing an average time constraint. By exploiting Lagrange multipliers, we transform the reformulated optimization problem into a dual problem with strong duality, the solutions of which can be obtained by the proposed time allocation algorithm. Via simulations, we validate the proven convexity and the approximation in our analytical model and evaluate the performance for both fixed frames length structure and dynamic frames length structure.}
}


@article{DBLP:journals/tmc/FengWD25,
	author = {Weichao Feng and
                  Shuoyao Wang and
                  Yu Dai},
	title = {Adaptive 360-Degree Streaming: Optimizing With Multi-Window and Stochastic
                  Viewport Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5903--5915},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541748},
	doi = {10.1109/TMC.2025.3541748},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengWD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tile-based approach is widely adopted in adaptive 360-degree video streaming systems, due to its efficiency in managing limited bandwidth resources. Recently, significant research efforts have been devoted to viewport-prediction-enabled bitrate adaptation for tile-based 360-degree Adaptive Bit-Rate (ABR) streaming, towards improving the average video quality while reducing rebuffering. However, the inherent uncertainty of users’ viewports has posed limitations on users’ Quality of Experience (QoE) for tile-based 360-degree ABR streaming. In this paper, we introduce a multi-window and stochastic viewport prediction approach to address the viewport uncertainty. In particular, considering our goal of maximizing the expectation of future QoE, we investigate a viewport distribution prediction model, to cope with the inherent randomness. Additionally, to accommodate the varying gap between the playback and the download process, we explore the multiple-window viewport prediction models to capture different prediction gaps. Even with the utilization of distributional prediction and multi-window models, predicting viewports far into the future is still inherently challenging. Accordingly, we propose a patience pattern temporarily suspending the download process, allowing for the accumulation of additional head movement trajectory data. Finally, we employ a model predictive control (MPC) approach for sequential decision-making, formulating the MPC problem as a mixed-integer non-linear programming (MINLP) task. To mitigate the computational burden associated with solving MINLP, we introduce a mixed-integer linear programming transformation to achieve efficient decision-making. Extensive experiments, utilizing real-world traces and user head movement trajectories, demonstrate that the proposed method outperforms state-of-the-art methods, improving overall QoE performance by 16.75% –18.91% .}
}


@article{DBLP:journals/tmc/OkegbileGTCNS25,
	author = {Samuel Dayo Okegbile and
                  Haoran Gao and
                  Oluwasegun Talabi and
                  Jun Cai and
                  Dusit Niyato and
                  Xuemin Shen},
	title = {Optimizing Federated Semantic Learning in Distributed AIGC-Enabled
                  Human Digital Twins: {A} Multi-Criteria and Multi-Shard User Selection
                  Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5916--5933},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541191},
	doi = {10.1109/TMC.2025.3541191},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OkegbileGTCNS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Artificial intelligence-generated content (AIGC) has been proposed as a solution to meet the requirements of ultra-reliable, secure, and privacy-preserving connectivity in human digital twin (HDT) networks. In such an AIGC-enhanced HDT, contents representing the true statuses of physical twins are generated in the virtual environment for the immediate update and evolution of the corresponding virtual twins (VTs). However, adopting a distributed AIGC in HDT presents several challenges, including the need for personalized VTs, data privacy concerns, and insufficient contextual understanding. This paper introduces a multi-layer federated semantic learning framework to address these challenges, incorporating batch learning to meet the training requirements for semantic-channel encoders and decoders. Furthermore, we introduce a novel user association framework to maximize the overall system performance under shard formation constraints. We then formulate a long-term joint optimization problem for user selection over finite learning periods. A novel Lyapunov-based online optimization strategy was proposed to mitigate the impact of time-varying and unpredictable training conditions. Additionally, we introduce a multi-arm bandit-based method and a context-centric user selection approach to solve the optimization problem. The results demonstrate that the proposed user association framework addresses the limitations of existing approaches, thereby improving the overall performance of the multi-shard AIGC-enhanced HDT.}
}


@article{DBLP:journals/tmc/YangLC25,
	author = {Wei Yang and
                  Zhixiang Li and
                  Sheng Chen},
	title = {Environment Independent Gait Recognition Based on Wi-Fi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5934--5950},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540011},
	doi = {10.1109/TMC.2025.3540011},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gait recognition plays a pivotal role in the area of mobile computing. While various research approaches leverage images, radar, RF signals, pressure sensors, wearables, and other methods, utilizing Wi-Fi signals for gait recognition offers distinct advantages such as a wide sensing range, simple deployment, and passive sensing capabilities. However, traditional gait recognition systems relying on Wi-Fi signals often suffer from performance degradation due to variations in walking directions and environmental conditions. To address this issue, in this paper we propose EIGait, a gait recognition system based on Wi-Fi signal time-frequency spectrograms. EIGait enhances the robustness and generalizability of extracted features through spectrogram augmentation, self-contrastive learning, and domain-adversarial training. Particularly, improvements to ResNet in EIGait yield a Spectrogram ResNet, which is better suited for time-frequency spectrograms. In addition, using merely a single pair of Wi-Fi transmitter and receiver, and by minimal signal denoising, we achieve the state-of-the-art performance. To evaluate the performance of EIGait, we conduct extensive experiments. In a typical indoor environment, EIGait achieves F1 scores ranging from 98.11% to 98.31% for four to eight individuals. In cross-direction gait recognition, we obtain F1 scores of 96.64% to 94.45% for four to eight individuals. Moreover, under the more challenging conditions of cross-room gait recognition, EIGait attains F1 scores of 92.09% to 89.61% for four to eight individuals. Additionally, we conduct experiments on the public dataset 3.0, and the results also demonstrate significant superiority.}
}


@article{DBLP:journals/tmc/WangMLD25,
	author = {Chenhao Wang and
                  Yang Ming and
                  Hang Liu and
                  Yutong Deng},
	title = {Dual Fine-Grained Authentication Without Trusted Authority for Data
                  Collection in {TDT} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5951--5963},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539281},
	doi = {10.1109/TMC.2025.3539281},
	timestamp = {Thu, 14 Aug 2025 10:48:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangMLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In transportation 5.0, digital twin (DT) is considered a promising paradigm to integrate physical entities into cyber physical systems by collecting massive data. However, the open collection process and key exposure issues bring critical security challenges. Furthermore, applying the existing authentication schemes to data collection in transportation DT (TDT) systems encounters three deficiencies: 1) forward security for collected data can only be achieved at a coarse-grained level; 2) one or more additional trusted authorities are introduced, causing the robustness of TDT systems to be downgraded; 3) dynamic attribute updating and revocability of physical entities are rarely considered. Therefore, we propose a dual fine-grained authentication scheme (DFAS) in this paper. Our DFAS can not only ensure data integrity and authenticity but also enable fine-grained access control, namely, only registered physical entities with authorized attributes can generate valid signatures. Meanwhile, DFAS provides the key puncturing for physical entities to guarantee fine-grained forward security without relying on any trusted authority. In addition, a non-interactive attribute updating and revocation of malicious entities are realized in DFAS. Finally, the security analysis indicates that DFAS can deal with various security challenges for data collection in TDT systems. The performance evaluation demonstrates that DFAS is efficient and practical.}
}


@article{DBLP:journals/tmc/KongXYCKZT25,
	author = {Hao Kong and
                  Jiahong Xie and
                  Jiadi Yu and
                  Yingying Chen and
                  Linghe Kong and
                  Yanmin Zhu and
                  Feilong Tang},
	title = {Realistic Facial Expression Reconstruction Using Millimeter Wave},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5964--5980},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540877},
	doi = {10.1109/TMC.2025.3540877},
	timestamp = {Mon, 22 Sep 2025 20:27:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KongXYCKZT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The technology of facial expression reconstruction has paved the way for various face-centric applications such as virtual reality (VR) modeling, human-computer interaction, and affective computing. Existing vision-based solutions present challenges in privacy leakage and poor lighting conditions. In this paper, we introduce a nonintrusive facial expression reconstruction system, mm3DFace, which uses a millimeter wave (mmWave) radar to reconstruct facial expressions in a privacy-preserving and passive manner. mm3DFace first captures and pre-processes mmWave signals reflected by a human face, and extracts intricate facial geometric features using a ConvNeXt model integrated with triple loss embedding. Subsequently, mm3DFace derives pose-invariant facial representations utilizing region-divided affine transformation, and further generates individual facial shapes with 68 facial landmarks. Then, dynamic facial expressions with 3D facial avatars are reconstructed to exhibit realistic facial expressions. Finally, mm3DFace enables micro-expression recognition with mmWave signals, which ensures the capability of describing tiny facial changes. Through extensive real-world experiments involving 15 participants, mm3DFace achieves a normalized mean error of 3.94%, a mean absolute error of 2.30 mm, and a 3D-mean absolute error of 4.10 mm in tracking 68 facial landmarks, which demonstrates the efficacy and practicality of mm3DFace in real-world 3D facial reconstruction scenarios.}
}


@article{DBLP:journals/tmc/LiCKCX25,
	author = {Xuebing Li and
                  Byungjin Cho and
                  Saimanoj Katta and
                  Jos{\'{e}} Costa{-}Requena and
                  Yu Xiao},
	title = {Aeacus: QUIC-Powered Low-Latency and Strong-Consistency Name Resolution
                  in 5G},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5981--5995},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539590},
	doi = {10.1109/TMC.2025.3539590},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCKCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Domain Name System (DNS) serves as a foundational networking service, yet its inherent time-to-live (TTL)-based cache mechanism presents a conundrum—striving for both low query latency and robust cache consistency proves challenging. To address this, we introduce Aeacus: a middleware seamlessly integrated into the 5G core, engineered to optimize name resolution for QUIC. Aeacus adeptly fortifies DNS with substantial cache consistency by capitalizing on QUIC handshake states to detect cache inconsistency, without compromising query delay. Furthermore, Aeacus orchestrates the amalgamation of DNS queries and QUIC handshake messages, effectively truncating one round-trip of message exchange and reviving expired DNS cache to bolster cache hit rates. Our dual-pronged deployment, encompassing both commercial and test 5G networks, demonstrates Aeacus’ prowess. In direct comparison with DNS, Aeacus successfully truncates connection setup delays by a remarkable 8.9% to 71.8%, all while introducing a mere 5.9% overhead attributed to supplementary packet processing and forwarding expenses. Importantly, existing DNS-based systems reap the benefits of Aeacus without necessitating modifications. We demonstrate Aeacus’ seamless enhancement of DNS-based load balancers, extending QUIC's 0-RTT handshake to include 0-RTT connection setup and service migration.}
}


@article{DBLP:journals/tmc/ZhangDQYZWWC25,
	author = {Haihan Zhang and
                  Haipeng Dai and
                  Yu Qiu and
                  Enze Yu and
                  Ruiben Zhou and
                  Weijun Wang and
                  Jingwu Wang and
                  Guihai Chen},
	title = {Optimizing Monitoring Utility of Uncrewed Aerial Vehicles Considering
                  Adverse Effects},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {5996--6013},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543399},
	doi = {10.1109/TMC.2025.3543399},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangDQYZWWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For Unmanned Aerial Vehicles (UAVs) monitoring tasks, capturing high quality images of target objects is important for subsequent recognition. Concerning the problem, many prior works study placement/trajectory planning for UAVs to maximize the quality of captured images. However, all of them overlook a fact that UAV monitoring may cause a huge risk/annoyance on living objects. In this paper, we investigate the novel problem of oPtimizing uncrewed aErial vehicles plAcement by Considering both monitoring utility and adverse Effects (PEACE). We propose an approach to solve PEACE, which is proved to be NP-hard. Overall, our approach achieves a  1 − 1 e − ε 1- \\frac{1}{e}-\\varepsilon  approximation ratio. First, we approximate the original problem of PEACE as a classical problem of Monotone Submodular function Maximization under a Uniform Matroid constraint (MSMUM) with a controlled gap. Then, for MSMUM, we propose a combination of algorithms achieving a  1 − 1 e 1-\\frac{1}{e}  approximation and  O ( n log n ) O(n\\log n)  time complexity considering the correlation among the UAV monitoring strategies. The proposed algorithms outperform existing algorithms for MSMUM through theoretical analysis and experimental results. Extensive simulations and field experiments demonstrate the effectiveness of our approach, achieving performance gains of 9.0% to 1434.5% compared to existing methods.}
}


@article{DBLP:journals/tmc/KeNZMLWPA25,
	author = {Ying Ke and
                  Zihan Ni and
                  Di Zhang and
                  Xiaqing Miao and
                  Chee Yen Leow and
                  Shuai Wang and
                  Gaofeng Pan and
                  Jianping An},
	title = {Information Freshness in Multi-Hop Satellite IoT Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6014--6029},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540259},
	doi = {10.1109/TMC.2025.3540259},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KeNZMLWPA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space-air-ground integration has become paramount in the next generation of wireless communication systems in the era marked by the seamless integration of terrestrial and celestial domains. On the other hand, the age of information (AoI) has recently emerged as a vital metric for evaluating the timeliness and freshness of data in these multi-hop communication systems. This paper focuses on investigating the information freshness of multi-hop satellite IoT systems while considering several automatic repeat request (ARQ) and hybrid ARQ (HARQ) schemes over different hops to promise the reliability of data transmissions. Specifically, a group of remote sensors transmits their data to a terrestrial base station (B) via the code-division multiple access (CDMA) strategy to exploit CDMA’s natural merits, e.g., anti-jamming and simultaneous transmissions. Then, B sends these received data to a data destination (D) via a satellite (R) under the transparent forwarding strategy. We derive the closed form of the outage probability for the CDMA protocol considering multi-user interference (MUI) and the closed form of the end-to-end outage probability for the three kinds of ARQ and HARQ schemes on the dual-hop B-R-D transmission, and then finally derive the expression of the corresponding AoI. Finally, numerical results show that simulations match well with the theoretical results, proving the analysis’s correctness and the pros and cons of the different ARQ/HARQ schemes.}
}


@article{DBLP:journals/tmc/LingJCCWDG25,
	author = {Xintong Ling and
                  Rui Jiang and
                  Weihang Cao and
                  Mingkai Chen and
                  Jiaheng Wang and
                  Zhi Ding and
                  Xiqi Gao},
	title = {Exploring {MEC} Server Strategy in Blockchain Networks: Mining for
                  Mobile Users or for Self},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6030--6044},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544311},
	doi = {10.1109/TMC.2025.3544311},
	timestamp = {Thu, 29 Jan 2026 17:25:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LingJCCWDG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain-based decentralized applications (DApps) offer enhanced security and decentralization features; nevertheless, their maintenance demands substantial computational resources and poses challenges for deployment in mobile networks. To address this, a number of studies have explored offloading blockchain mining tasks from mobile users to mobile edge computing (MEC) servers. However, the existing literature overlooks the fact that MEC servers can not only mine for mobile users but also mine for themselves, potentially explaining why MEC mining offloading has not gained broad acceptance within the industry. In this work, we exploit a more practical case and rethink the question of whether MEC servers lease computing power to mobile users by taking into account that MEC servers can mine for themselves. We establish a game model and apply backward induction to analytically characterize Nash equilibria for mining strategies adopted by MEC and mobile users. Our findings suggest that, if MEC can mine for self, MEC would mine for mobile users only under specific conditions where mobile users possess superior information gathering capability (at least better than the MEC server) or the whole blockchain system exhibits significant network value. We further provide a series of simulations to verify our conclusion and illustrate the impact of network parameters on the strategies of both sides.}
}


@article{DBLP:journals/tmc/ZhangZYXWCLZ25,
	author = {Duo Zhang and
                  Xusheng Zhang and
                  Zhehui Yin and
                  Yaxiong Xie and
                  Hewen Wei and
                  Zhaoxin Chang and
                  Wenwei Li and
                  Daqing Zhang},
	title = {mmRotation: Unlocking Versatility of a Single mmWave Radar via Azimuth
                  Panning and Elevation Tilting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6045--6061},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539985},
	doi = {10.1109/TMC.2025.3539985},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZYXWCLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor mmWave-based sensing technologies have garnered substantial interest from both the industrial and academic. Yet, the intrinsic challenge posed by the limited Field-of-View (FOV) of mmWave radars significantly restricts their coverage. This limitation necessitates careful selection of installation positions and orientations to optimize performance, thereby severely curtailing the versatility and widespread adoption of these systems. Traditionally, expanding coverage involved increasing the number of radar units. This paper introduces a novel approach to enhance the FOV by incorporating mobility, achieved by affixing the radar onto a pan-tilt unit capable of rotating along both the horizontal and azimuthal. Nevertheless, the disparity between the pan-tilt and the radar presents significant challenges for accurately rotating the radar's orientation. To mitigate this, we propose an automated calibration algorithm for radar and pan-tilt, ensuring precise calibration. Additionally, we have devised a radar orientation adjustment algorithm intended to automatically align the radar's FOV with the positions of detected objects to facilitate various applications. Through three case studies, we have demonstrated that mmRotation can greatly expand the sensing range, enabling support for multiple applications on a single radar, such as vital signs monitoring and fall detection. Comprehensive experimental results underscore that our system surpasses the current state-of-the-art (SOTA).}
}


@article{DBLP:journals/tmc/CaiMLLLKC25,
	author = {Mingxin Cai and
                  Chen Ma and
                  Yuchen Li and
                  Zhonghao Lyu and
                  Yutong Liu and
                  Linghe Kong and
                  Guihai Chen},
	title = {{UCMM:} Unsupervised Convolutional Networks for Accurate and Efficient
                  Map Matching With Mobile Cellular Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6062--6074},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540300},
	doi = {10.1109/TMC.2025.3540300},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiMLLLKC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The map matching of cellular data reconstructs real trajectories of users by exploiting the sequential connections between mobile devices and cell towers. The difficulty in obtaining paired cellular-GPS data and the cellular variation compromise the accuracy and reliability of existing map matching approaches. In this paper, we propose a novel unsupervised convolutional network for cellular map matching (UCMM) to address these challenges. UCMM employs a dual encoder-decoder network to capture a shared representation from both the cellular and GPS domains in an unsupervised manner. It leverages a dedicated convolutional architecture to tackle the varying lengths of output sequential data. An attention mechanism is specially introduced to deal with the cellular variation. The effectiveness of UCMM is demonstrated through comprehensive evaluations, which show that UCMM achieves a substantial improvement in matching accuracy and deduction of training time compared with the best-known prior works. These improvements make UCMM a significant advancement in the field of map matching.}
}


@article{DBLP:journals/tmc/LiCGLSPL25,
	author = {Hongtao Li and
                  Ziqi Chen and
                  Fengxian Guo and
                  Nan Li and
                  Yaohua Sun and
                  Mugen Peng and
                  Yuanwei Liu},
	title = {Coordinating Communication and Computing for Wireless {VR} in Open
                  Radio Access Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6075--6089},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540111},
	doi = {10.1109/TMC.2025.3540111},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiCGLSPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by diverse applications, radio access networks (RAN) are expected to embrace built-in computing and intelligence, forming a versatile wireless computing platform that closely integrates communication and computing. To fully unleash the potential of such a synergistic system, it is essential to coordinate communication and computing with intelligence unlocked by the radio intelligent controllers (RICs) in O-RAN. Building on the groundwork established by existing theoretical studies and simulations, we develop a platform that can emulate the events in the real-world system in more detail, bringing theoretical works closer to practical implementation. In this paper, we first introduce ns-GP-O-RAN, a software simulation platform developed over ns-3, enabling communication, computation task processing, large-scale data collection, and testing of system-level orchestration policies through user-level control. Taking virtual reality (VR) as an example, we formulate the computation offloading problem and develop a prediction-based computation offloading xAPP, which contains a prediction phase to predict users’ end-to-end (E2E) performance with the deep neural network and a system-level decision-making phase for global orchestration with the differential evolution algorithm. We evaluate the system capacity and E2E latency over the developed ns-GP-O-RAN, which is more effective than existing approaches.}
}


@article{DBLP:journals/tmc/ChenWZLHL25,
	author = {Yuang Chen and
                  Chang Wu and
                  Fangyu Zhang and
                  Chengdi Lu and
                  Yongsheng Huang and
                  Hancheng Lu},
	title = {Topology-Aware Microservice Architecture in Edge Networks: Deployment
                  Optimization and Implementation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6090--6105},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539312},
	doi = {10.1109/TMC.2025.3539312},
	timestamp = {Sat, 01 Nov 2025 12:00:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWZLHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a ubiquitous deployment paradigm, integrating microservice architecture (MSA) into edge networks promises to enhance the flexibility and scalability of services. However, it also presents significant challenges stemming from dispersed node locations and intricate network topologies. In this paper, we have proposed a topology-aware MSA characterized by a three-tier network traffic model encompassing the service, microservices, and edge node layers. This model meticulously characterizes the complex dependencies between edge network topologies and microservices, mapping microservice deployment onto link traffic to accurately estimate communication delay. Building upon this model, we have formulated a weighted sum communication delay optimization problem considering different types of services. Then, a novel topology-aware and individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to solve the problem efficiently, which accurately senses the network topology and incorporates an individual-adaptive mechanism in a genetic algorithm to accelerate the convergence and avoid local optima. Extensive simulations show that, compared to the existing deployment schemes, TAIA-MD improves the communication delay performance by approximately 30% to 60% and effectively enhances the overall network performance. Furthermore, we implement the TAIA-MD scheme on a practical microservice physical platform. The experimental results demonstrate that TAIA-MD achieves superior robustness in withstanding link failures and network fluctuations.}
}


@article{DBLP:journals/tmc/ZhaoEH25,
	author = {Haotian Zhao and
                  Kamran Entesari and
                  Sebastian Hoyos},
	title = {Multi-Channel Analog Beamforming Transceiver for mmWave Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6106--6118},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539169},
	doi = {10.1109/TMC.2025.3539169},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoEH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces an analog multi-channel millimeter-wave transceiver architecture that offers advantages in terms of low hardware complexity and computational efficiency compared to digital beamforming and hybrid beamforming techniques. Also, it is known that analog beamforming with a single phase-shifter network faces limitations in maintaining consistent accuracy across a wideband spectrum. To this end, the proposed architecture leverages the inherent bandwidth-splitting property of the multi-channel transceiver. Thus, each sub-band signal is processed by its corresponding channel in the transceiver with an independent analog beamformer per channel. This approach can significantly improve the beamforming accuracy in a wideband communication system such as 5G and future 6G cellular networks. The simulation results demonstrate that increasing the channels in the multi-channel transceiver enables multi-channel analog beamforming to achieve a comparable bit-error-rate (BER) performance to digital beamforming when interference is not considered. Moreover, when interference is present, the proposed multi-channel analog beamforming exhibits enhanced resilience to high power interference compared with digital beamforming with limited analog-to-digital conversion resolution.}
}


@article{DBLP:journals/tmc/AmmousCWV25,
	author = {Mustafa Ammous and
                  Hui Chen and
                  Henk Wymeersch and
                  Shahrokh Valaee},
	title = {3D Cooperative Positioning via {RIS} and Sidelink Communications With
                  Zero Access Points},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6119--6136},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541575},
	doi = {10.1109/TMC.2025.3541575},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AmmousCWV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surfaces (RISs) are expected to be a main component of future 6G networks due to their capability to create a controllable wireless environment, achieve extended coverage, and improve localization accuracy. In this paper, we present a novel cooperative positioning use case of the RIS in mmWave frequencies and show that in the presence of RIS, together with sidelink communications, localization with zero access points (APs) is possible. We show that multiple (at least three) half-duplex single-antenna user equipments (UEs) can cooperatively estimate their positions through device-to-device communications with a single RIS as an anchor without the need for any APs. We start by formulating a three-dimensional positioning problem with Cramér-Rao lower bound (CRLB) derived for performance analysis. After that, we discuss the RIS profile design and the power allocation strategy between the UEs. Then, we propose low-complexity estimators for estimating the channel parameters and UEs’ positions. Finally, we evaluate the performance of the proposed estimators and RIS profiles in the considered scenario via extensive simulations and show that sub-meter level positioning accuracy can be achieved under multi-path propagation.}
}


@article{DBLP:journals/tmc/YanYSHYL25,
	author = {Dawei Yan and
                  Panlong Yang and
                  Fei Shang and
                  Feiyu Han and
                  Yubo Yan and
                  Xiang{-}Yang Li},
	title = {Pushing the Limits of WiFi-Based Gait Recognition Towards Non-Gait
                  Human Behaviors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6137--6153},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540863},
	doi = {10.1109/TMC.2025.3540863},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanYSHYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based gait recognition technologies have seen significant advancements in recent years. However, most existing approaches rely on a critical assumption: users must walk continuously and maintain a consistent body posture. This poses a substantial challenge when users engage in non-periodic or discontinuous behaviors (e.g., stopping, starting, or turning mid-walk), which can disrupt the extraction of gait-related features and degrade recognition performance. To address this issue, we propose freeGait, a novel approach designed to mitigate the impact of non-gait behaviors in WiFi-based gait recognition systems. Our solution models this problem as domain adaptation, where we learn domain-independent representations to isolate gait features from behavior-dependent noise. We treat human behaviors with labeled user data as source domains and behaviors without user labels as target domains. However, applying domain adaptation directly is challenging due to the ambiguous classification boundaries in the target domains for WiFi signals. To overcome this, we align the posterior distributions between the source and target domains and constrain the conditional distribution within the target domains to enhance gait classification accuracy. Additionally, we implement a data augmentation module to generate data resembling the labeled data, while supervised learning ensures distinctiveness between users. Our experiments, conducted with 20 participants across 3 different scenarios, demonstrate that freeGait can accurately predict data across 15 domains by labeling only a small subset from 6 source domains, achieving up to a 45% improvement in user classification accuracy compared to existing methods.}
}


@article{DBLP:journals/tmc/NanHYZN25,
	author = {Zhaojun Nan and
                  Yunchu Han and
                  Jintao Yan and
                  Sheng Zhou and
                  Zhisheng Niu},
	title = {Robust Task Offloading and Resource Allocation Under Imperfect Computing
                  Capacity Information in Edge Intelligence Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6154--6167},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539296},
	doi = {10.1109/TMC.2025.3539296},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NanHYZN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge intelligence systems, task offloading and resource allocation policies critically depend on the required computing capacity of the task, which can only be accurately measured after execution, presenting significant design challenges. In this paper, we address the problem of robust task offloading and resource allocation under imperfect computing capacity information, where the exact value as well as distribution knowledge of the required computing capacity cannot be obtained in advance. Specifically, we formulate the energy-time cost (ETC) minimization problem using min-max robust optimization. To tackle this challenging issue, we propose a decoupling method. This method first assumes the offloading policy is predetermined and derives two independent subproblems: local ETC and edge ETC. Then, we provide a closed-form optimal solution for the local ETC problem. The edge ETC problem is equivalently transformed into a geometric programming (GP) problem, and we introduce an effective iterative algorithm to obtain a stationary point, utilizing successive convex approximation (SCA). Finally, we design a coordinate descent (CD)-based algorithm to optimize the offloading policy effectively. Extensive simulations demonstrate that the proposed policy significantly outperforms other benchmark methods, achieving near-optimal performance even in the presence of high estimation errors in computing capacity.}
}


@article{DBLP:journals/tmc/SohnKL25,
	author = {Vaughn Sohn and
                  Suhwan Kim and
                  Hyang{-}Won Lee},
	title = {Joint Frame Drop and Object Detection Task Offloading for Mobile Devices
                  via {RL} With Lyapunov Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6168--6182},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539356},
	doi = {10.1109/TMC.2025.3539356},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SohnKL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object detection has become an increasingly important application for mobile devices. However, state-of-the-art object detection relies heavily on deep neural network, which is often burdensome to compute on mobile devices. To this end, we develop a layering framework for joint video frame drop and object detection task offloading. In the lower layer, by invoking Lyapunov optimization, we devise an algorithm for partitioning and offloading the computation tasks of deep neural networks. This algorithm also specifies the flow control for admitting the application traffic into the network. In the upper layer, we use the flow control as a form of guidance in the action space in order to develop a reinforcement learning (RL) algorithm that selectively drops video frames with object detection performance in consideration. By the nature of design, this Lyapunov-guided RL guarantees the network stability. We show through simulations that our Lyapunov-guided RL drops video frames with reasonable object detection performance and reduced latency while keeping the network stable. We also implemented our algorithm on the remote-controlled (RC) car equipped with microprocessor and GPU, and demonstrate the applicability of our algorithm to real-time object detection tasks from the video stream generated as the RC car moves.}
}


@article{DBLP:journals/tmc/AmesakaWSSS25,
	author = {Takashi Amesaka and
                  Hiroki Watanabe and
                  Yuta Sugiura and
                  Masanori Sugimoto and
                  Buntarou Shizuki},
	title = {EarLock: Personal Authentication System for Hearables Using Sound
                  Leakage Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6183--6196},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540584},
	doi = {10.1109/TMC.2025.3540584},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AmesakaWSSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Earphone-type wearable devices, also known as “hearables,” will have many functions in the future. Some of those functions will require authentication of the wearer for access to the user's privacy information or settlement of payments. In this study, we propose a new personal authentication system for hearables called EarLock. EarLock authenticates the wearer by acquiring and analyzing ear canal and auricle shape information using sound leakage from the device. The system can be implemented using a speaker and external microphone that are highly compatible with hearables. We implemented three prototype devices and investigated EarLock's authentication performance under various practical scenarios, including walking conditions, noisy environments, and situations with object interference. Experimental results showed that the in-ear, open-ear, and bone-conduction devices achieved balanced accuracy (BAC) scores of 87.2–93.7%, 83.4–94.7%, and 85.9–90.0%.}
}


@article{DBLP:journals/tmc/BaiZWXXJ25,
	author = {Jing Bai and
                  Yuxiang Zhang and
                  Yiran Wang and
                  Zhu Xiao and
                  Yong Xiong and
                  Licheng Jiao},
	title = {Robust Motion-Guided Frame Sampler With Interpretive Evaluation for
                  Video Action Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6197--6208},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541580},
	doi = {10.1109/TMC.2025.3541580},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaiZWXXJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the presence of redundancy and interference, frame sampling is a promising but challenging solution to mitigate the expensive computation of video action recognition. Although the motion prior has shown great potential for frame selection, existing motion-based strategies suffer from limitations in terms of robustness and interpretive evaluation. In this paper, we devise a robust frame sampling strategy called positive motion guided sampler (PMGSampler). It consists of two procedures, local motion capture and global motion statistics. At the local level, we propose two concepts about inter-frame motion amplitude and motion continuity, which helps to perceive the movement of subjects and identify abnormal events that may generate negative pseudo-motion information. Then, through a global analysis of the obtained local motions, the sampler becomes more sensitive to informative frames and robust to outliers. The proposed sampler can be applied to most existing models for improving recognition accuracy. We conduct extensive experiments on four widely-used benchmarks to demonstrate the superiority of our PMGSampler over other methods of the same type. In addition, to analyse how sampled frames influence action recognition, we present a visual interpretation method for video models, termed as spatio-temporal class activation map (STCAM). By introducing spatial and temporal branches, our STCAM is able to visualise the salience of spatio-temporal features. With the help of STCAM, we can further intuitively evaluate the performance of different sampling strategies.}
}


@article{DBLP:journals/tmc/ZhangLXJY25,
	author = {Yuncan Zhang and
                  Weifa Liang and
                  Zichuan Xu and
                  Xiaohua Jia and
                  Yuanyuan Yang},
	title = {Profit Maximization of Delay-Sensitive, Differential Accuracy Inference
                  Services in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6209--6224},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540017},
	doi = {10.1109/TMC.2025.3540017},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLXJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of Artificial Itelligence (AI) and edge computing has sparked significant interest in edge inference services. In this paper, we consider delay-sensitive, differential accuracy inference services in a Mobile Edge Computing (MEC) network while meeting user stringent delay and accuracy requirements. We formulate two novel profit maximization problems under static and dynamic settings of service request arrivals, with the aim of maximizing the accumulative profit of admitted requests. We assign differential accuracy service requests to the corresponding resolution instances of their requested service models, assuming that each resolution instance can serve up to  L ≥ 1 L\\geq 1  the same type of service requests. Since the profit maximization problem is NP-hard, we first formulate an Integer Linear Program (ILP) solution if the problem size is small or medium; otherwise, we devise a constant randomized algorithm with high probability. Then, we consider dynamic service request admissions without the knowledge of future request arrivals for a given finite time horizon, for which we develop a simple yet effective prediction mechanism to accurately predict the number of different resolution instances of each model needed, and pre-deploy the predicted number of resolution instances into cloudlets to reduce instantiating delays. We then devise an online algorithm with a provable competitive ratio for the dynamic profit maximization problem by leveraging the primal-dual dynamic updating technique. Finally, we evaluate the performance of the proposed algorithms by simulations. The simulation results demonstrate that the proposed algorithms are promising.}
}


@article{DBLP:journals/tmc/DongLZCTW25,
	author = {Chongwu Dong and
                  Weidong Li and
                  Zhi Zhou and
                  Xu Chen and
                  Zhihong Tian and
                  Wushao Wen},
	title = {Delay-Sensitive Task Offloading With Edge Caching Through Martingale-Based
                  Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6225--6242},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540413},
	doi = {10.1109/TMC.2025.3540413},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DongLZCTW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the forthcoming era of 6G networks, delay-sensitive applications for Internet of Things (IoT) are poised to become the prevailing services with ultra-reliable and low-latency (URLLC) requirements. Unlike traditional video caching, IoT-based edge caching faces unique challenges due to diverse data types, update frequencies, and computational needs, requiring integrated storage and computational resource management. To support the more stringent requirements for these innovative applications, mobile edge computing (MEC) is introduced to enhance the service reliability of delay-sensitive applications in the 6G era. However, task offloading, as an indispensable procedure in MEC, would encounter many challenges, such as network jitter and resource insufficiency, possibly leading to unpredictable queuing delays and other negative issues. To ensure reliable services in a dynamical MEC environment, the caching-enabled MEC network has emerged as a novel architecture, placing computing and storage resources in the edge network. In this paper, we investigate the caching-enabled MEC to support reliable task offloading for delay-sensitive applications, with a focus on IoT scenarios. In our system model, we formulate the task process as a two-hop tandem queuing system with limited capacity, including task transmission and computation queues. The Martingale theory is leveraged to analyze the delay violation probability in this system, demonstrating how the offloading and caching decisions affect the end-to-end (E2E) delay. Besides, task offloading and resource allocation policies are integrated to reduce high system costs, including energy consumption and cache resource rental costs. Based on the delay analysis of martingale theory, we propose an advanced deep reinforcement learning (DRL) algorithm called Dynamic Request Aware Soft Actor-Critic (DRA-SAC) algorithm to achieve minimal system costs by obtaining the optimal task offloading and resource allocation policies, including caching and computation resources. We conduct some illustrative studies to evaluate the proposed scheme. The algorithm we have put forward outperforms benchmark algorithms regarding both cache hit ratio and system cost.}
}


@article{DBLP:journals/tmc/FuDCSZH25,
	author = {Shucun Fu and
                  Fang Dong and
                  Runze Chen and
                  Dian Shen and
                  Jinghui Zhang and
                  Qiang He},
	title = {Multi-Dimensional Training Optimization for Efficient Federated Synergy
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6243--6258},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540566},
	doi = {10.1109/TMC.2025.3540566},
	timestamp = {Mon, 12 Jan 2026 20:31:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FuDCSZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge learning (EL) is an end-to-edge collaborative learning paradigm enabling devices to participate in model training and data analysis, opening countless opportunities for edge intelligence. As a promising EL framework, federated synergy learning (FSyL) mitigates the computation and communication overhead on resource-constrained devices by offloading partial model layers to the edge server for synergistic training. Nevertheless, due to the system and statistical heterogeneity, naively using existing FSyL methods is significantly time-consuming and causes accuracy degradation. Motivated by this issue, this paper introduces a novel FSyL framework that integrates multi-dimensional training optimization and formulates the edge learning cost minimization (ELCM) problem. To tackle the ELCM efficiently, we design OL-MG, an OnLine Model Splitting and Resource Provisioning Game. Specifically, we first reformulate and decompose the original ELCM based on data quality evaluation. Then, given a model splitting decision, we determine the optimal resource provisioning in Sub-problem1, based on which optimal model splitting in Sub-problem2 is modeled as a potential game. Subsequently, we introduce a decentralized algorithm to find a Nash equilibrium (NE) solution. Furthermore, we further extend OL-MG to support a budget-aware multi-edge scenario. Extensive experiments demonstrate that the proposed mechanism significantly outperforms state-of-the-art methods in cost-saving and accuracy improvement.}
}


@article{DBLP:journals/tmc/PaulRCSHC25,
	author = {Sibendu Paul and
                  Kunal Rao and
                  Giuseppe Coviello and
                  Murugan Sankaradas and
                  Y. Charlie Hu and
                  Srimat T. Chakradhar},
	title = {CamTuner: Adaptive Video Analytics Pipelines via Real-Time Automated
                  Camera Parameter Tuning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6259--6274},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540667},
	doi = {10.1109/TMC.2025.3540667},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PaulRCSHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Video Analytics Pipelines (VAP), Analytics Units (AUs) such as object detection and face recognition operating on remote servers rely heavily on surveillance cameras to capture high-quality video streams to achieve high accuracy. Modern network cameras offer an array of parameters that directly influence video quality. While a few of such parameters, e.g., exposure, focus and white balance, are automatically adjusted by the camera internally, the others are not. We denote such camera parameters as non-automated (NAUTO) parameters. In this work, we first show that in a typical surveillance camera deployment, environmental condition changes can have significant adverse effect on the accuracy of insights from the AUs, but such adverse impact can potentially be mitigated by dynamically adjusting NAUTO camera parameters in response to changes in environmental conditions. Second, since most end-users lack the skill or understanding to appropriately configure these parameters and typically use a fixed parameter setting, we present CamTuner, to our knowledge, the first framework that dynamically adapts NAUTO camera parameters to optimize the accuracy of AUs in a VAP in response to adverse changes in environmental conditions. CamTuner is based on SARSA reinforcement learning and it incorporates two novel components: a light-weight analytics quality estimator and a virtual camera that drastically speed up offline RL training. Our controlled experiments and real-world VAP deployment show that compared to a VAP using the default camera setting, CamTuner enhances VAP accuracy by detecting 15.9% additional persons and 2.6% –4.2% additional cars (without any false positives) in a large enterprise parking lot. CamTuner opens up new avenues for elevating video analytics accuracy, transcending mere incremental enhancements achieved through refining deep-learning models.}
}


@article{DBLP:journals/tmc/HuangLLCW25,
	author = {Liang Huang and
                  Yuqi Li and
                  Hongyuan Liang and
                  Kaikai Chi and
                  Yuan Wu},
	title = {Enhanced {VR} Experience With Edge Computing: The Impact of Decoding
                  Latency},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6275--6292},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541741},
	doi = {10.1109/TMC.2025.3541741},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLLCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual reality (VR) applications have revolutionized digital interaction by providing immersive experiences. 360$^{\\circ }$ VR video streaming has experienced significant growth and popularity as a pivotal VR application. However, the combination of limited network bandwidth and the demand for high-quality videos frequently hinders the achievement of a satisfactory quality of experience (QoE). Although prior methods have enhanced QoE, the effects of decoding latency have been poorly studied. It is technically challenging to design a quality adaptation algorithm that can balance the pursuit of high-quality videos and the limitation of limited bandwidth resources. To address this challenge, we propose an edge-end architecture for 360$^{\\circ }$ VR video streaming and aim to enhance overall QoE by solving a performance optimization problem. Specifically, our experiments on commercial mobile devices in real-world situations reveal that decoding latency significantly influences QoE. First, decoding latency plays a major role in contributing to end-to-end latency, which exceeds the transmission latency. Second, decoding latency can differ considerably between devices with varying computational capabilities. Building on this insight, we propose a novel  l atency- a ware  q uality  a daptation (LAQA) algorithm. LAQA lies in developing a solution that can allocate video quality in real-time and enhance overall QoE. LAQA involves not only the quality of the received content, the transmission latency and the quality variance, but also the decoding latency and the fairness of the user quality. Subsequently, we formulate a combinatorial optimization problem to maximize overall QoE. Through extensive validation with experimental data from real-world situations, LAQA offers a promising approach to enhance QoE and ensure fairness performance in different devices. In particular, LAQA achieves 16.77% and 10.66% enhancement over the state-of-the-art combinatorial optimization and reinforcement learning algorithm, respectively, in terms of QoE at 4K resolution. Furthermore, LAQA ensures excellent scalability by simulating the number of users ranging from 15 to 60, making it a robust solution for diverse and growing user scales.}
}


@article{DBLP:journals/tmc/AsheralievaNW25,
	author = {Alia Asheralieva and
                  Dusit Niyato and
                  Xuetao Wei},
	title = {Dynamic Distributed Model Compression for Efficient Decentralized
                  Federated Learning and Incentive Provisioning in Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6293--6314},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543295},
	doi = {10.1109/TMC.2025.3543295},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AsheralievaNW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study decentralized federated learning (DFL) in edge computing networks where edge nodes (ENs) collaboratively train their artificial intelligence (AI) models in a serverless manner without sharing local data. We consider the following critical DFL challenges: i) scarce bandwidth resources of ENs; ii) dynamic, heterogeneous edge environment; iii) incentive provisioning and complex tradeoffs between the DFL performance and training costs. To resolve these challenges, we develop a new model compression method where ENs utilize dynamic, non-identical compression rates to improve the communication efficiency of DFL under time-varying, heterogeneous resource constraints. We show that our method can be formulated as a graphical Markov potential game where ENs act as players deciding on their compression factors and the number of data samples used for model updates. Each EN is incentivized to participate in DFL through rewards based on the EN's contribution to training. We prove that our game has a dominant pure-strategy Nash equilibrium (NE) maximizing its potential function and propose a dynamic distributed compression algorithm in which each EN can find its dominant strategy independently. We show that this algorithm converges to the Pareto-optimal NE, representing the most efficient solution of our game enhancing the DFL performance with minimal costs.}
}


@article{DBLP:journals/tmc/ChenHMNLZ25,
	author = {Zheyi Chen and
                  Sijin Huang and
                  Geyong Min and
                  Zhaolong Ning and
                  Jie Li and
                  Yan Zhang},
	title = {Mobility-Aware Seamless Service Migration and Resource Allocation
                  in Multi-Edge IoV Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6315--6332},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540407},
	doi = {10.1109/TMC.2025.3540407},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenHMNLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for Internet-of-Vehicles (IoV) applications. However, due to high vehicle mobility and finite communication coverage of base stations, it is hard to maintain uninterrupted and high-quality services without proper service migration among MEC servers. Existing solutions commonly rely on prior knowledge and rarely consider efficient resource allocation during the service migration process, making it hard to reach optimal performance in dynamic IoV environments. To address these important challenges, we propose SR-CL, a novel mobility-aware seamless Service migration and Resource allocation framework via Convex-optimization-enabled deep reinforcement Learning in multi-edge IoV systems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP) problem of service migration and resource allocation into two sub-problems. Next, we design a new actor-critic-based asynchronous-update deep reinforcement learning method to handle service migration, where the delayed-update actor makes migration decisions and the one-step-update critic evaluates the decisions to guide the policy update. Notably, we theoretically derive the optimal resource allocation with convex optimization for each MEC server, thereby further improving system performance. Using the real-world datasets of vehicle trajectories and testbed, extensive experiments are conducted to verify the effectiveness of the proposed SR-CL. Compared to benchmark methods, the SR-CL achieves superior convergence and delay performance under various scenarios.}
}


@article{DBLP:journals/tmc/HuLSFZY25,
	author = {Han Hu and
                  Yifeng Lyu and
                  Kaifeng Song and
                  Rongfei Fan and
                  Cheng Zhan and
                  Jian Yang},
	title = {An Efficient Two-Stage Networking Topology Design for Mega-Constellation
                  of Low Earth Orbit Satellites},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6333--6347},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540671},
	doi = {10.1109/TMC.2025.3540671},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuLSFZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellites play a crucial role in providing high-speed internet to remote areas and ensuring network resilience during outages. The design of efficient satellite constellations requires optimizing network topology, which is a complex task due to the large solution space and the need for fault tolerance. This paper presents the AlphaSat algorithm, a two-phase approach to improve latency and network robustness in LEO constellations. In the initialization phase, Monte Carlo Tree Search (MCTS) is used to generate an initial topology by selecting links from a vast search space. In the refinement phase, an edge-switching method is applied to enhance network resilience and performance. AlphaSat is evaluated on OneWeb, Starlink, and Telesat mega-constellations, demonstrating superior performance over existing algorithms. The results show significant reductions in latency ranging from 4.7% to 44.5% and improvements in network robustness, increasing by 3.3% to 28.3%. Furthermore, AlphaSat effectively balances network load and optimizes power consumption, offering a promising solution for efficient and resilient LEO satellite network design.}
}


@article{DBLP:journals/tmc/HeHHL25,
	author = {Yu He and
                  Guangjie Han and
                  Yun Hou and
                  Chuan Lin},
	title = {Environment-Tolerant Trust Opportunity Routing Based on Reinforcement
                  Learning for Internet of Underwater Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6348--6360},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3540774},
	doi = {10.1109/TMC.2025.3540774},
	timestamp = {Mon, 22 Sep 2025 16:06:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeHHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Underwater Things (IoUT) has garnered significant interest due to its potential applications in monitoring underwater environments. However, the unique characteristics of acoustic communication, such as long propagation delays and high attenuation, present considerable obstacles for achieving efficient and dependable data transmission. Opportunistic routing is a crucial technique for enhancing packet delivery ratios by selecting a set of forwarding nodes and utilizing their cooperative forwarding to boost network throughput. Nevertheless, choosing an excessive number of forwarding nodes can lead to wasteful energy usage and extended communication delays. Moreover, the overlooked trustworthiness of forwarded nodes in most research works can undermine the effectiveness of opportunistic routing. Therefore, this study presents a novel trust opportunistic routing scheme that employs reinforcement learning to achieve resilience in constantly changing underwater settings. The combination of reinforcement learning and trust management enables the proposed opportunistic routing scheme to adapt to the unstable underwater environment and unknown malicious attacks. Initially, a method is introduced for measuring environmental fitness by considering multiple trust factors, including communication success rate, data reliability, and location dynamics. The proposed scheme then uses reinforcement learning to develop a reliable opportunistic routing method based on quantified state information. This component employs the obtained state to formulate action strategies and obtains reward values from environmental inputs. The reward update equation integrates these qualities to optimize the deployment of superior action strategies, finally achieving trust opportunistic routing for underwater data collection. Fundamental experimental results demonstrate that the proposed protocol performs exceptionally well in demanding underwater conditions, outperforming existing methods in packet transmission rate, energy efficiency, and end-to-end delay.}
}


@article{DBLP:journals/tmc/WangHXYLSL25,
	author = {Haodong Wang and
                  Jiangping Han and
                  Kaiping Xue and
                  Jiayu Yang and
                  Jian Li and
                  Qibin Sun and
                  Jun Lu},
	title = {{CAAF:} An NDN-Based Cache-Aware Adaptive Forwarding Strategy for
                  Reliable Content Delivery in VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6361--6375},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543458},
	doi = {10.1109/TMC.2025.3543458},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangHXYLSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high mobility in Vehicular Ad-hoc Networks (VANETs) significantly affects the reliability of data transmission. To solve this problem, Named Data Networking (NDN)-based VANETs are proposed, utilizing in-network caching and named-based forwarding to overcome the dual challenges of mobility and connectivity. Although in-network caching enhances content availability, a strategy that accurately locates and efficiently utilizes the cached content in VANETs with highly dynamic environments is still lacking. In this paper, we propose a novel NDN-based cache-aware adaptive forwarding (CAAF) strategy for VANETs. CAAF proactively predicts content locations and ensures reliable content retrieval by adaptively selecting forwarding nodes that prioritize fast delivery and stable transmission. Specifically, we design a content information table for each vehicle to record information about the Interest packets it receives. Furthermore, these tables are updated periodically across all vehicles and a prediction model is used to predict real-time in-network caching during the update interval. Subsequently, we execute a filter mechanism to sieve candidate forwarding vehicles that satisfy both the accessibility and stability requirements. These candidates are then evaluated using a multi-attribute decision-making method across diverse parameters to determine the optimal forwarding node. Our extensive simulation results demonstrate that the proposed CAAF outperforms the state-of-the-art forwarding strategy regarding content retrieval delay and Interest satisfaction ratio across diverse scenarios.}
}


@article{DBLP:journals/tmc/LeeYLWL25,
	author = {Chi{-}Han Lee and
                  De{-}Nian Yang and
                  Guang{-}Siang Lee and
                  Chih{-}Hang Wang and
                  Wanjiun Liao},
	title = {Joint View Selection, Multigroup Multicast Beamforming, and {DIBR}
                  for RIS-Aided Multi-View Videos},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6376--6393},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543297},
	doi = {10.1109/TMC.2025.3543297},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LeeYLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of multi-view videos (MVV) transmission is an irresistible trend. Concurrently, reconfigurable intelligent surface (RIS)-assisted wireless communication has drawn significant attention. We observe that the view selection based on the base station and the view synthesis based on depth-image-based rendering (DIBR) can effectively reduce power consumption. Therefore, this paper studies the view selection and synthesis for RIS-aided MVV in multigroup multicast beamforming. To deal with this complicated scenario, we investigate a problem, named the joint View selection, Multicast beamforming, and DIBR (JVMD), to minimize the total multicast beamforming power, the view transmission operation power, and view synthesis, subject to quality-of-service (QoS), RIS phase shifts, view selection, and DIBR constraints. Unfortunately, the mathematical model is a complicated mixed discrete-continuous optimization problem. To tackle this challenging problem, we designed an algorithm, named View selection, Beamforming, RIS phase, and DIBR (VBRD) algorithm. First, we deal with the discrete optimization problem of selecting the view. VBRD uses the dual-based approximation methodology to round back a primal's integer solution. Then, in the continuous optimization problem, we apply the alternating optimization (AO) method to determine beamforming, RIS phase, and DIBR. Finally, simulation results show the performance of exploiting view synthesis for RIS-assisted wireless communication.}
}


@article{DBLP:journals/tmc/ZhuZTBH25,
	author = {Zhenhan Zhu and
                  Yanchao Zhao and
                  Maoxing Tang and
                  Yanling Bu and
                  Hao Han},
	title = {Robust and Effort-Efficient Image-Based Indoor Localization With Generative
                  Features},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6394--6412},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541045},
	doi = {10.1109/TMC.2025.3541045},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuZTBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Image-based indoor localization using smartphones has become popular, leveraging visual landmarks and fingerprint extraction for localization. Fingerprint density significantly affects accuracy, but collecting dense, high-resolution fingerprints during on-site surveys is labor-intensive and incurs high computation/storage costs during matching. Additionally, efficient fingerprint extraction often constrains users to specific shooting poses, with deviations markedly reducing localization accuracy. To address these challenges, we introduce ARGILS, an Automated Real-time Generative Image Localization System. The key idea is to use cross sparse sampling instead of dense sampling, generate fingerprint features for missing locations, and quickly match locations through feature orthogonal decomposition. Cross sparse sampling ensures full coverage of scene features and helps to generate missing fingerprints. To maintain high localization resolution with sparse sampling, we designed a distance-constrained generative adversarial network to generate fingerprints for unsampled locations. Additionally, we developed an orthogonal fingerprint extraction method to decompose image features into horizontal and vertical directions in 2D space. To improve robustness against obstacles, we implemented a scanning localization scheme using key frame filtering and clustering. We have implemented ARGILS and performed extensive real-world evaluations. Experiment results show that when reducing 75% site survey effort, the average location error of ARGILS is around 2.5m in a shopping mall, 48% higher than state-of-the-art methods. ARGILS can also efficiently speed up localization process, with the time consumption ranging from 0.1 to 0.3 seconds on smartphones of various configurations.}
}


@article{DBLP:journals/tmc/CaiZLYWSS25,
	author = {Qing Cai and
                  Yiqing Zhou and
                  Ling Liu and
                  Hanxiao Yu and
                  Yihao Wu and
                  Ningzhe Shi and
                  Jinglin Shi},
	title = {Query-Aware Semantic Encoder-Based Resource Allocation in Task-Oriented
                  Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6413--6429},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541636},
	doi = {10.1109/TMC.2025.3541636},
	timestamp = {Thu, 14 Aug 2025 10:50:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiZLYWSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task-oriented communications with semantic encoders are promising to enhance the communication efficiency, by selecting and transmitting valuable data according to task requirements/queries. However, existing semantic encoders lack the capability to track the changing in queries, leading to biased data selection. This paper proposes a query-aware semantic encoder, i.e., Query-Data Cross (QDC) encoder for task-oriented communications. By consistently focusing on data features that are most relevant to the current query at the transmitter, QDC can adapt to changing queries. Based on the dynamic semantic relevance obtained by QDC, a relevance-based data selection and bandwidth allocation optimization (RDSBA) problem is formulated, considering a multi-device task-oriented communication system, where devices should transmit valuable data with high relevance to the queries broadcasted by the base station (BS). RDSBA aims to maximize the data profit of all devices, which is defined as the difference between the relevance of data selected for the BS and the cost of obtaining the data. Then, a DRL-based data selection and bandwidth allocation (DRL-DB) algorithm is proposed to solve the NP-hard optimization problem. Simulation results demonstrate that QDC can smartly track the changing in queries and achieve an accuracy of at least 85% in relevance evaluation, more than 8% higher than existing schemes. Based on the relevance provided by QDC, the proposed RDSBA scheme with DRL-DB can increase the data profit by at least 18%, comparing to existing schemes.}
}


@article{DBLP:journals/tmc/ZhangHZSXJ25,
	author = {Pinchang Zhang and
                  Keshuang Han and
                  Yuanyu Zhang and
                  Yulong Shen and
                  Fu Xiao and
                  Xiaohong Jiang},
	title = {Distributed Physical Layer Authentication Framework Exploiting Array
                  Pattern Feature for mmWave {MIMO} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6430--6445},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3541725},
	doi = {10.1109/TMC.2025.3541725},
	timestamp = {Sun, 17 Aug 2025 16:45:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHZSXJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authentication in millimeter-Wave (mmWave) Multiple-Input Multiple-Output (MIMO) systems is a critical issue due to the unique characteristics of mmWave communication, such as highly directional beamforming and the ability to support massive device connectivity. To address this challenge, this paper proposes a novel low-complexity decision-level-based Distributed Physical Layer Authentication (DPLA) framework to combat identity-based impersonation attacks in mmWave MIMO systems. The DPLA framework leverages Beam Pattern (BP) deviation, which arises from hardware-specific gain errors, as a key authentication feature. A fusion center is introduced to make the final authentication decision by aggregating local decisions from multiple collaborative nodes, enabling multi-directional perception. Specifically, a low-complexity hybrid combining fusion rule is carefully designed to accommodate the fully connected structure of mmWave MIMO systems, balancing computational efficiency and authentication performance. A rigorous performance analysis is conducted by deriving closed-form analytical expressions for the probabilities of correct detection and false alarm. Furthermore, the asymptotic detection and discrimination performance are systematically analyzed in the large-scale antenna regime. To further enhance authentication accuracy, digital signaling matrices are designed using the deflection coefficient maximization principle. The feasibility of the proposed framework is validated through a comprehensive evaluation, demonstrating its superior robustness and efficiency compared to benchmark methods.}
}


@article{DBLP:journals/tmc/HeTXS25,
	author = {Pengcheng He and
                  Yijia Tang and
                  Fan Xu and
                  Qingjiang Shi},
	title = {Optimization-Inspired Graph Neural Network for Cellular Network Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6446--6459},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3542434},
	doi = {10.1109/TMC.2025.3542434},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeTXS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of wireless communications has driven the need for careful optimization of network parameters to improve network performance and reduce operational cost. Traditional methods, however, struggle with the vast number of tunable parameters and lack scalability in diverse network scenarios. To address these challenges, this paper introduces an optimization-inspired bipartite graph neural network (Bi-GNN) approach for scalable network optimization. Our approach leverages the bipartite structure of network topologies, and incorporates a message-passing mechanism by unfolding the Zeroth-Order Block Coordinate Projected Gradient Descent (ZO-BCPGD) algorithm, which ensures not only high-performance optimization but also manageable computational demand. We demonstrate the permutation and dimensionality equivariance property of the Bi-GNN, which significantly enhances the model’s generalizability across various network structures and sizes. Furthermore, we theoretically analyze the expressive power and generalization ability of the Bi-GNN, demonstrating its adeptness at complex network optimization tasks. The training process, parallel execution, and practical implementation techniques are also discussed to ensure the model’s applicability in real-world scenarios. Numerical results verify that the Bi-GNN outperforms existing methods in both coverage ratios and computational cost. Furthermore, our approach exhibits robust scalability across various network scenarios, making it a versatile tool for optimizing a wide range of wireless networks.}
}


@article{DBLP:journals/tmc/HuangZWMM25,
	author = {Haojun Huang and
                  Junhao Zhang and
                  Bang Wang and
                  Wang Miao and
                  Geyong Min},
	title = {Joint Mobile Energy Replenishment and Data Gathering in Wireless Sensor
                  Networks via Federated Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6460--6473},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543009},
	doi = {10.1109/TMC.2025.3543009},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZWMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the proliferation of wireless energy transfer for Wireless Sensor Networks (WSNs), which are mainly used for data gathering in real-world applications. A number of studies have investigated mobile vehicle scheduling to charge sensor nodes via wireless Mobile Chargers (MCs). Unfortunately, most of them cannot parallelly charge all nodes in an intelligent manner with the global network attributes. Furthermore, the time-variable charging ignores the optimal data gathering, resulting in poor Joint Energy Replenishment and Data Gathering (JERDG). To fill this gap, this paper proposes a Federated Deep Reinforcement Learning (FDRL)-based JERDG (FERG) solution for WSNs. To this end, FERG first partitions the networks into a set of clusters to distribute the workload evenly among multiple MCs, and then designs an FDRL-based framework that incorporates various time-variant network attributes to determine the optimal schedule for charging and data gathering via multiple MCs and a base station (BS). The BS as the cloud server is responsible for global training of JERDG models, while multiple MCs will parallelly train local models to jointly charge energy-exhausted nodes and gather the data from all nodes in clusters. To reserve more personalized characteristics of each cluster, a density-based partial aggregation strategy is designed to train the global model. Furthermore, a reward-weighted update and selection solution is proposed to generate and exploit reference samples with high rewards. Simulation results obtained from various scenarios demonstrate that FERG significantly outperforms the state-of-the-art approaches in terms of network lifetime, energy efficiency and data collection latency.}
}


@article{DBLP:journals/tmc/RahmanCTP25,
	author = {Aisha B. Rahman and
                  Panagiotis Charatsaris and
                  Eirini Eleni Tsiropoulou and
                  Symeon Papavassiliou},
	title = {Symbiotic Resource Pricing in the Computing Continuum Era},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6474--6487},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3542017},
	doi = {10.1109/TMC.2025.3542017},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RahmanCTP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Though extensive research efforts have been devoted to the problem of computing resource pricing, they mainly focus on single computing paradigms. In this paper, we provide a holistic approach to this problem, by treating the whole computing continuum, consisting of cloud, edge, and fog computing providers, simultaneously offering their resources to the users. Within such a complex setting, we establish the concept of symbiotic computing resource pricing and sharing, where the computing providers and the users coexist within a mutually beneficial ecosystem, sharing services and resources as a means of ensuring their business survival and service satisfaction. Under this prism, we introduce two key pricing families, namely the non-cooperative one which involves competition and is treated through game theoretic approaches, and the cooperative resource pricing (full or partial), which addresses complex scenarios through optimization and coalition. A thorough performance assessment is provided, through modeling and simulation, in order to highlight and quantify the key characteristics and tradeoffs of the various resource pricing approaches introduced.}
}


@article{DBLP:journals/tmc/WangWWZMHH25,
	author = {Xueyi Wang and
                  Xingwei Wang and
                  Chen Wang and
                  Rongfei Zeng and
                  Lianbo Ma and
                  Qiang He and
                  Min Huang},
	title = {Truthful Online Combinatorial Auction-Based Mechanisms for Task Offloading
                  in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6488--6502},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3542135},
	doi = {10.1109/TMC.2025.3542135},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWWZMHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computation (MEC) is envisioned as a prospective approach for processing the computation-intensive and delay-sensitive tasks of smart mobile devices (SMDs) through offloading them to base stations (BSs) nearby. In fact, efficient task offloading mechanisms are crucial to accomplish an MEC system. The key challenge is to make on-spot decisions upon the arrival of each task and at the same time achieve truthfulness of each SMD. The challenge further escalates, when the unique characteristics of an MEC system, such as locality constraint, delay constraint, etc., are explicitly considered. To solve the challenge, we present a truthful online combinatorial auction-based mechanism (TOCA) for task offloading in an MEC system. Specifically, we first devise the candidate offloading scheme determination algorithm, aiming to determine the candidate offloading schemes of an SMD upon the arrival of its task. Next, we devise the winning offloading scheme selection and pricing algorithm based on the online primal-dual optimization framework, to decide the winning scheme among the SMD's candidate offloading schemes and calculate its payment. By solid theoretical analysis, we verify that TOCA achieves truthfulness, individual rationality and computational efficiency and a smaller competitive ratio. Trace-driven simulation studies validate the effectiveness and efficacy of TOCA.}
}


@article{DBLP:journals/tmc/TianZFZLL25,
	author = {Shujuan Tian and
                  Xinjie Zhu and
                  Bochao Feng and
                  Zhirun Zheng and
                  Haolin Liu and
                  Zhetao Li},
	title = {Partial Offloading Strategy Based on Deep Reinforcement Learning in
                  the Internet of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6517--6531},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543976},
	doi = {10.1109/TMC.2025.3543976},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TianZFZLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by the increasing demands of vehicular tasks, edge offloading has emerged as a promising paradigm to enhance quality of experience (QoE) in Internet of Vehicles (IoV) networks. This approach enables vehicles to offload computation-intensive tasks to edge servers, resulting in reduced computation delays and lower energy consumption. However, traditional binary offloading limits the efficiency of edge offloading. To address this gap, we propose a partial offloading strategy that jointly optimizes the offloading ratio, computation, and communication resources in IoV. Recognizing the varying priorities of vehicular tasks regarding task delay and energy consumption, we formulate two distinct scenarios: one focused on minimizing delay and the other on minimizing energy consumption. Furthermore, we employ a reinforcement learning approach to establish a multi-dimensional joint optimization function by setting different objectives for each scenario. Based on this framework, we introduce a multi-state iteration deep deterministic policy gradient algorithm (SIDDPG), which effectively determines task partitioning and resource allocation. Simulation results demonstrate that the proposed algorithm outperforms benchmark schemes in terms of task delay and energy consumption.}
}


@article{DBLP:journals/tmc/ZhuZLHZWL25,
	author = {Peizhao Zhu and
                  Yuzheng Zhu and
                  Wenyuan Li and
                  Yanbo He and
                  Yongpan Zou and
                  Kaishun Wu and
                  Victor C. M. Leung},
	title = {{CHAR:} Composite Head-Body Activities Recognition With a Single Earable
                  Device},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6532--6549},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548647},
	doi = {10.1109/TMC.2025.3548647},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuZLHZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 89.7% and 85.1% in sufficient data and insufficient data cases, respectively.}
}


@article{DBLP:journals/tmc/LiDLZZPWX25,
	author = {Tong Li and
                  Yukuan Ding and
                  Jiaxin Liang and
                  Kai Zheng and
                  Xu Zhang and
                  Tian Pan and
                  Dan Wang and
                  Ke Xu},
	title = {Toward Optimal Broadcast Mode in Offline Finding Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6550--6565},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545561},
	doi = {10.1109/TMC.2025.3545561},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiDLZZPWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes ElastiCast, a novel Bluetooth Low Energy (BLE) broadcast mode that reduces the neighbor discovery latency in offline finding networks (OFNs). ElastiCast adapts the broadcast mode of the lost devices to the scan modes of the finder devices, considering their diversity. We start with an overview of OFNs, followed by a detailed analysis of the issues and challenges of existing solutions, which motivates the design of ElastiCast. Then we provide Blender, a simulator that models the neighbor discovery behavior of different broadcasters and scanners. By adopting Blender, ElastiCast can be implemented with three components: Local Optima Estimation, Common Interest Extraction, and Interval Multiplexing, in which we capture the key features of BLE neighbor discovery and globally optimize the broadcast mode interacting with diverse scan modes. Experimental evaluation results and commercial product deployment experience demonstrate that ElastiCast is effective in achieving stable and bounded neighbor discovery latency within the power budget.}
}


@article{DBLP:journals/tmc/YuYBWRD25,
	author = {Xiangbin Yu and
                  Chenghong Yang and
                  Jiawei Bai and
                  Kezhi Wang and
                  Yun Rui and
                  Xiaoyu Dang},
	title = {Joint Design of Power Allocation and Beamforming for IRS-Assisted
                  Millimeter-Wave Communication System With Imperfect {CSI}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6566--6582},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545413},
	doi = {10.1109/TMC.2025.3545413},
	timestamp = {Sun, 01 Feb 2026 13:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuYBWRD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the joint power allocation (PA), passive beamforming (BF) and hybrid BF (HBF) including digital and analogue BFs are designed for an intelligent reflecting surface (IRS)-assisted millimeter-Wave (mmWave) communication system with imperfect channel state information (CSI) and multiple mobile users to optimize the weighted sum rate (WSR) and energy efficiency (EE). The achievable WSR and EE of the IRS-mmWave system are first derived based on imperfect cascaded CSI for performance optimization. Then, the non-convex constrained problem is formulated to maximize the WSR, where the PA, HBF, phase and amplitude of IRS elements are jointly optimized. Given PA and passive BF (PBF), closed-form suboptimal HBF is obtained for each iteration. Also, given HBF and PBF, using the block coordinate descent (BCD) methods, closed-form PA is derived. Moreover, the phase and amplitude of IRS elements are derived for PBF design during each iteration. With the obtained HBF, the digital and analogue BFs are also derived. Based on this, joint schemes of PA, HBF and PBF are developed. Besides, an efficient iterative algorithm based upon the alternating optimization (AO), weighted minimum mean-square error (WMMSE) and Dinkelbach methods are presented for EE maximization and the suboptimal solution is obtained. Correspondingly, the energy-efficient design for joint PA, HBF and PBF is provided. Simulation results verify the proposed solutions.}
}


@article{DBLP:journals/tmc/LiuZYSYL25,
	author = {Ziwei Liu and
                  Jifei Zhu and
                  Jiaqi Yang and
                  Yimao Sun and
                  Yanbing Yang and
                  Jun Luo},
	title = {ReflexGest: Recognizing Hand Gestures Under VLC-Capable Lamps},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6583--6594},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545340},
	doi = {10.1109/TMC.2025.3545340},
	timestamp = {Mon, 17 Nov 2025 12:09:02 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZYSYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a main approach towards touch-free human-computer interaction, hand gesture recognition (HGR) has long been a research focus for both academia and industry. Meanwhile, visible light communication (VLC) has become increasingly popular with VLC-ready commercial products (e.g., Philips lamps) available on the market. These facts provoke us to ask: can we leverage a VLC-ready lamp to realize integrated sensing and communication (ISAC) by conducting both HGR and VLC simultaneously? To this end, we propose ReflexGest as our answer to this question. ReflexGest is implemented upon a table lamp for the sake of practicality; this VLC-ready lamp is equipped with a ring-shaped light-emitting diode (LED) array and a photodiode (PD, for light intensity sensing) originally aiming for up/down-link VLCs. Demanding hand gestures to be performed between the lamp and a table surface, ReflexGest exploits the variation of the reflection and their unique correlation with the corresponding hand gestures to achieve HGR. In particular, ReflexGest first handles the limited sensing ability of the PD by enhancing the LED lamp and thus diversifying the light emission patterns. Moreover, ReflexGest combats the reflection interference from varying table surfaces via an adversarial learning technique to distill only the features relevant to hand gestures. Our extensive evaluations demonstrate that ReflexGest is able to deliver accurate HGR under realistic VLC traffic.}
}


@article{DBLP:journals/tmc/ZhuLHWHW25,
	author = {Xiuzhen Zhu and
                  Limei Lin and
                  Yanze Huang and
                  Xiaoding Wang and
                  Sun{-}Yuan Hsieh and
                  Jie Wu},
	title = {Forward Legal Anonymous Group Pairing-Onion Routing for Mobile Opportunistic
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6595--6612},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544674},
	doi = {10.1109/TMC.2025.3544674},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLHWHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Opportunistic Networks (MONs) often experience frequent interruptions in end-to-end connections, which increases the likelihood of message loss during delivery and makes users more susceptible to various cyber attacks. However, most currently proposed anonymous routing protocols are primarily designed for networks with stable connections, making it challenging to protect user identities in MONs. To address these challenges, we propose FLAG-POR (Forward Legal Anonymous Group Pairing-Onion Routing), a novel anonymous routing protocol specifically tailored to enhance message delivery anonymity and security in MONs. Specifically, we abstract the mobile opportunistic network as a contact graph. By introducing the concept of “groups” into the pairing-onion routing protocol, which encrypts messages and relay nodes layer by layer, we develop a novel group-based pairing-onion routing protocol. This protocol ensures message confidentiality and relay node anonymity, while also improving message forwarding rates, as any node within a group can potentially act as a relay. To ensure message authenticity, we employ the efficient SM2 signing algorithm to generate signatures for the message source. Furthermore, by incorporating parameters such as the public key validity period and master key validity period into the group pairing-onion routing protocol, we achieve forward security in message delivery. We conduct a thorough theoretical analysis of the protocol’s security and performance. The experimental results demonstrate that our FLAG-POR protocol outperforms baseline anonymous protocols in terms of delivery success rate, traceability rate, path anonymity, and node anonymity. Additionally, the FLAG-POR scheme effectively resists three potential threats to the routing system: collusion attack threat, node identification threat, and path identification threat, in any situation.}
}


@article{DBLP:journals/tmc/TsaiLW25,
	author = {Cheng{-}Wei Tsai and
                  Kuang{-}Hsun Lin and
                  Hung{-}Yu Wei},
	title = {Device Power Saving With Time-Frequency Adaptation: Joint {BWP-DRX}
                  Design With {BWP} Switching Delay Considered},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6613--6627},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547978},
	doi = {10.1109/TMC.2025.3547978},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TsaiLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today's ever-growing data traffic landscape, optimizing network power efficiency and performance has become crucial. Discontinuous Reception (DRX) and Bandwidth Parts (BWP) are two key technologies that fulfill this pursuit. DRX is a time-domain power-saving technology that allows user equipment (UE) to switch off their radio frequency module. BWP switching is a frequency domain operation that allows UE to operate on only partial bandwidth for power saving. Investigating the interaction and trade-off between DRX and BWP is a must to optimize network efficiency and enhance network performance. This work proposed a novel BWP-DRX joint mechanism and its analytical model that leverages the concept of “Detect time” with the consideration of BWP switching delay. The model reduces packet loss rate by 50%, packet delay by 36% and increases the energy efficiency rate by 50% when arrival rate is high with the trade-off of 12% power efficiency reduction when arrival rate is low compared to the model without Detect time. The influence of each parameter is further analyzed to reach the best network efficiency under different traffic conditions.}
}


@article{DBLP:journals/tmc/CaiXSSWGZ25,
	author = {Yunyun Cai and
                  Wei Xi and
                  Yuhao Shen and
                  Cerui Sun and
                  Shuai Wang and
                  Wei Gong and
                  Jizhong Zhao},
	title = {Individualized Data Generation in Personalized Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6628--6642},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545244},
	doi = {10.1109/TMC.2025.3545244},
	timestamp = {Thu, 06 Nov 2025 14:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaiXSSWGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most Personalized Federated Learning (PFL) algorithms merge the model parameters of each client with other (similar or generic) model parameters to optimize the personalized model (PM). However, the merged model parameters in these algorithms may fit low relevance data, thereby limiting the performance of PM. In this paper, we generate similar data for each client through the collaboration of a generic model (GM) on the server, rather than merging model parameters. To train a generator capable of generating data for all classes on the server without real data, we employ the GM as the discriminator in adversarial training with the generator. Additionally, we introduce a similarity assessment metric, which allows for the assessment of the similarity between local data and data from other classes. Nevertheless, the presence of non-IID data among clients can weaken the performance of the GM, consequently impacting the training of the generator and similarity assessment. To address this issue, we design a directive mechanism so that GM can be optimized during adversarial training without the need for additional training. The experimental results validate the superiority of our algorithm over state-of-the-art algorithms in terms of accuracy, loss, and convergence speed.}
}


@article{DBLP:journals/tmc/GuoLLXMVLM25,
	author = {Jingjing Guo and
                  Jiaxing Li and
                  Zhiquan Liu and
                  Yupeng Xiong and
                  Yong Ma and
                  Athanasios V. Vasilakos and
                  Xinghua Li and
                  Jianfeng Ma},
	title = {{LCEFL:} {A} Lightweight Contribution Evaluation Approach for Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6643--6657},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545140},
	doi = {10.1109/TMC.2025.3545140},
	timestamp = {Wed, 08 Oct 2025 13:29:49 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoLLXMVLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prerequisite for implementing incentive mechanisms and reliable participant selection schemes in federated learning is to obtain the contribution of each participant. Available evaluation methods for participant contributions require the server to possess a test dataset, often impractical. Additionally, the excessively high complexity of these works is unacceptable when training complex models in large-scale federated learning system. To address these issues, we propose a lightweight contribution evaluation method for federated learning participants, named LCEFL, based on model projection theory, which does not require the server to provide a test dataset. In addition, a model compression method is designed to be used in LCEFL to reduce the computational complexity. Furthermore, a trusted aggregation method based on LCEFL is proposed, where the weight of each participant's local model is determined by its trust level, which can be calculated using its contribution evaluation result. Experimental results show that LCEFL can achieve nearly the same accuracy as schemes based on Shapley Value, while significantly reducing computational overhead by more than 50%. Compared to available aggregation methods, the proposed trusted aggregation scheme is able to accelerate the convergence speed of the global model and improve its accuracy by 2% to 45%.}
}


@article{DBLP:journals/tmc/ZhangCCL25,
	author = {Jinbei Zhang and
                  Chunpeng Chen and
                  Kechao Cai and
                  John C. S. Lui},
	title = {Incremental Least-Recently-Used Algorithm: Good, Robust, and Predictable
                  Performance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6658--6672},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547066},
	doi = {10.1109/TMC.2025.3547066},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a replacement algorithm for file caching in mobile edge computing (MEC) networks. While there are numerous schemes for file replacement, it remains a challenge to achieve good, robust, and predictable performance simultaneously. To address this challenge, we introduce a general scheme called Incremental Least-Recently-Used (iLRU), which builds on the classic Least-Recently-Used (LRU) algorithm. iLRU initially caches only a “portion” of the file upon the first request and incrementally caches more when there are more requests for the file. In this regard, the request frequency can be inferred from the cached size without incurring additional overhead, where a larger cached size represents a higher request frequency. We derive the theoretical hit ratio of iLRU based on the Time-to-Live (TTL) analysis. With the Time-to-Live (TTL) analysis, we can theoretically derive the hit ratio and properties of iLRU and notably show that iLRU allocates more cache space to popular files, resulting in a higher hit ratio than LRU. Simulation results demonstrate the superior performance of iLRU and validate the accuracy of the theoretical hit ratio. Furthermore, we conduct simulations over various real-world traces to show that iLRU outperforms existing schemes across various real-world traces, defenestrating the robustness of iLRU.}
}


@article{DBLP:journals/tmc/GaoZZY25,
	author = {Ronghao Gao and
                  Bo Zhang and
                  Qin{-}Yu Zhang and
                  Zhihua Yang},
	title = {Topology-Compressed Data Delivery in Large-Scale Heterogeneous Satellite
                  Networks: An Age-Driven Spatial-Temporal Graph Neural Network Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6673--6687},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544574},
	doi = {10.1109/TMC.2025.3544574},
	timestamp = {Wed, 27 Aug 2025 11:29:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoZZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Large-Scale Heterogeneous Satellite Networks (LSHSNs) integrating Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites, high-timeliness data delivery confronts dynamical connectivity and obvious latency, which heavily challenges existing graph-dependable transmission strategies requiring to obtain global topological information with huge computational cost and signaling overhead. To address this issue, in this paper, we propose an Age-predicting Local Information Dependable Transmission (ALIDT) mechanism for the LSHSN by considering the impact of time-varying topology on the timeliness of data, in which a novel metric of data freshness called Forwarding-aware Age of Information (FAoI) is well-designed to evaluate the timeliness in data forwarding at node. In particular, we develop a satellite Coverage-based Local Information Sharing (CLIS)-assisted Spatial-Temporal Graph Neural Network (STGNN) to extract the topological features in both temporal and spatial dimensions and a Graph Matching Network (GMN)-based topology compression algorithm to improve computation efficiency. The simulation results indicate that the proposed mechanism performs better in improving the storage overhead, throughput and average FAoI compared with the conventional Open Shortest Path First (OSPF) routing algorithm with Time-Varying Graph (TVG) model, GNN-based Multipath Routing (GMR) algorithm, and Gated Recurrent Units (GRU) based metric prediction algorithm in hybrid satellite networks, respectively.}
}


@article{DBLP:journals/tmc/JiangCXYCL25,
	author = {Hongbo Jiang and
                  Jiang{-}hao Cai and
                  Zhu Xiao and
                  Kehua Yang and
                  Hongyang Chen and
                  Jiangchuan Liu},
	title = {Vehicle-Assisted Service Caching for Task Offloading in Vehicular
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6688--6700},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545444},
	doi = {10.1109/TMC.2025.3545444},
	timestamp = {Mon, 06 Oct 2025 14:28:11 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangCXYCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of artificial intelligence (AI) enables vehicular edge computing (VEC) servers to be able to provide more intelligent services. However, the limited storage resources of VEC servers constrain the deployment of intelligent service contents, which greatly restricts the intelligence level of the VEC network. To resolve this problem, we first design a novel vehicle-assisted VEC network architecture and further propose VaCo, a Vehicle-assisted Collaborative caching system. VaCo allows VEC servers to download the cached service content from any vehicle in the VEC network to support task offloading. VaCo mainly considers the real-time scheduling problem of vehicle storage resources under the dynamic VEC network and the benefit problem caused by invoking vehicle resources under the highly dynamic load environment. VaCo models the vehicle storage resources as an independent resource pool and deploys a cross-VEC server content retrieval mechanism to achieve unified and efficient management of the storage resources of the vehicle cluster and the VEC server cluster. Then, we propose a multi-swarm collaborative optimization scheme to jointly optimize the service failure rate and cost, and further propose a Pareto-based optimization scheme to ensuring that VaCo can correctly evaluate the benefits of invoking vehicle resources in a dynamic VEC network. Finally, we implement VaCo and conduct extensive evaluations on real-world dataset. The experimental results on the real trajectory dataset show that VaCo can effectively utilize vehicle resources and ensure the benefits of both vehicles and VEC servers simultaneously.}
}


@article{DBLP:journals/tmc/WangLPCYJL25,
	author = {Shaowei Wang and
                  Jin Li and
                  Yun Peng and
                  Kongyang Chen and
                  Wei Yang and
                  Hui Jiang and
                  Jin Li},
	title = {Differential Private Data Stream Analytics in the Local and Shuffle
                  Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6701--6717},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559621},
	doi = {10.1109/TMC.2025.3559621},
	timestamp = {Sun, 24 Aug 2025 12:16:58 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLPCYJL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study online data analytics with differential privacy (DP) in decentralized settings. Specifically, online data analytics with local DP protection is widely adopted in real-world applications. Despite numerous endeavors in this field, significant gaps in utility and functionality remain when compared to its offline counterpart. We present an optimal, streamable mechanism: ExSub, for local DP sparse vector estimation. The mechanism enables a range of online analytics on streaming binary vectors, including multi-dimensional binary, categorical, or set-valued data. By leveraging the negative correlation of occurrence events in the sparse vector, we attain an optimal error rate under local privacy constraints, only requiring streamable computations. To surpass the error barrier of local privacy, we also study ExSub randomizer in the newly emerging (single-message) shuffle model of DP, and provide nearly-tight privacy amplification bounds therein. Additionally, we leverage the online shuffle model that independently permutes users’ messages at each timestamp, to design a simplified randomization strategy that can approximately reach Gaussian accuracy in central DP. Through experiments with both synthetic and real-world datasets, ExSub mechanism in the local model have been shown to reduce error by 40%–60% compared to SOTA approaches. The ExSub in the shuffle model can further reduce over 85% error, and the online shuffle protocol reduces over 99.7% error.}
}


@article{DBLP:journals/tmc/WangZZYSC25,
	author = {Guanzhong Wang and
                  Dongheng Zhang and
                  Tianyu Zhang and
                  Shuai Yang and
                  Qibin Sun and
                  Yan Chen},
	title = {Corrections to "Learning Domain-Invariant Model for WiFi-Based
                  Indoor Localization"},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6718},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3539443},
	doi = {10.1109/TMC.2025.3539443},
	timestamp = {Sun, 06 Jul 2025 13:22:35 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZYSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the above article [1], on page 13900, right column, there is an empty reference citation “[?]” in the sentence “By applying Model-Agnostic Meta-Learning (MAML) to fingerprint localization, MetaLoc [?] enables the model to quickly adapt to new environments based on the obtained meta-parameters, thus reducing human labor costs.” The missing reference is listed below as [2].}
}


@article{DBLP:journals/tmc/ZadnikKTMJ25a,
	author = {Jakub Z{\'{a}}dn{\'{\i}}k and
                  Michel Kieffer and
                  Anthony Trioux and
                  Markku J. M{\"{a}}kitalo and
                  Pekka J{\"{a}}{\"{a}}skel{\"{a}}inen},
	title = {Correction to "CV-Cast: Computer Vision-Oriented Linear Coding
                  and Transmission"},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6719},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3565860},
	doi = {10.1109/TMC.2025.3565860},
	timestamp = {Sun, 06 Jul 2025 13:22:35 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZadnikKTMJ25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the above article [1], on page 1151, eq. (6), there is an error in the equation. The correct equation is:  min . D , s.t. ∑ k = 1 K λ k β 2 k ⩽ P . (6) \\begin{equation*} \\min.\\,\\,D,\\,\\,\\text{s.t.} \\sum\\limits_{k = 1}^K {{{\\lambda }_k}\\beta _k^2 \\leqslant P.} \\tag{6} \\end{equation*}  min.D,s.t.∑k=1Kλkβk2⩽P.(6)}
}


@article{DBLP:journals/tmc/ShenYYY25,
	author = {Wei Shen and
                  Mang Ye and
                  Wei Yu and
                  Pong C. Yuen},
	title = {Build Yourself Before Collaboration: Vertical Federated Learning With
                  Limited Aligned Samples},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {7},
	pages = {6503--6516},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3543923},
	doi = {10.1109/TMC.2025.3543923},
	timestamp = {Sun, 06 Jul 2025 13:22:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenYYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical Federated Learning (VFL) has emerged as a crucial privacy-preserving learning paradigm that involves training models using distributed features from shared samples. However, the performance of VFL can be hindered when the number of shared or aligned samples is limited, a common issue in mobile environments where user data are diverse and unaligned across multiple devices. Existing approaches use feature generation and pseudo-label estimation for unaligned samples to address this issue, unavoidably introducing noise during the generation process. In this work, we propose Local Enhanced Effective Vertical Federated Learning (LEEF-VFL), which fully utilizes unaligned samples in the local learning before collaboration. Unlike previous methods that overlook private labels owned by each client, we leverage these private labels to learn from all local samples, constructing robust local models to serve as solid foundations for collaborative learning. Additionally, we reveal that the limited number of aligned samples introduces distribution bias from global data distribution. In this case, we propose to minimize the distribution discrepancies between the aligned samples and the global data distribution to enhance collaboration. Extensive experiments demonstrate the effectiveness of LEEF-VFL in addressing the challenges of limited aligned samples, making it suitable for VFL in mobile computing environments.}
}


@article{DBLP:journals/tmc/LeeTW25,
	author = {Chao{-}Yang Lee and
                  Ang{-}Hsun Tsai and
                  Li{-}Chun Wang},
	title = {Adaptive Stabilization Control by Deep Reinforcement Learning for
                  Hovering Drone Surveillance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6720--6733},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548421},
	doi = {10.1109/TMC.2025.3548421},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LeeTW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an adaptive stabilization control mechanism by using deep reinforcement learning (DRL) for hovering drones that have to execute a surveillance task for a long time. For long-endurance flights, we design and implement a buoyancy-aided autonomous aerial vehicle (AAV) that can use buoyancy lift to decrease the weight and increase the battery capacity so that the flight time can be significantly extended. However, the balloons of the buoyancy-aided AAV can cause “an inverted pendulum effect” and an instability issue on the drone attitude because the increased surface is easily affected by the gusty wind. We propose a buoyancy-aided adaptive stabilization control (BAASC) method with the DRL to stabilize the attitude and extend the flight time of the quadrotor-based buoyancy-aided AAV. This proposed model can immediately control the speeds of all rotors to balance the attitude based on the current state of the drone. Therefore, the degree of swing can be stabilized, and the inverted pendulum effect can be eliminated. The experimental results reveal that the designed buoyancy-aided AAV with the proposed BAASC scheme can effectively stabilize the attitude to extend the flight time by 112.8% compared with a nonbuoyancy-aided AAV under a gusty wind disturbance.}
}


@article{DBLP:journals/tmc/KangCSLK25,
	author = {Honggu Kang and
                  Seohyeon Cha and
                  Jinwoo Shin and
                  Jongmyeong Lee and
                  Joonhyuk Kang},
	title = {NeFL: Nested Model Scaling for Federated Learning With System Heterogeneous
                  Clients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6734--6746},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549600},
	doi = {10.1109/TMC.2025.3549600},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KangCSLK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables distributed training while preserving data privacy, but stragglers—slow or incapable clients can significantly slow down the total training time and degrade performance. To mitigate the impact of stragglers, system heterogeneity, including heterogeneous computing and network bandwidth, has been addressed. While previous studies have addressed system heterogeneity by splitting models into submodels, they offer limited flexibility in model architecture design, without considering potential inconsistencies arising from training multiple submodel architectures. We propose nested federated learning (NeFL), a generalized framework that efficiently divides deep neural networks into submodels using both depthwise and widthwise scaling. To address the inconsistency arising from training multiple submodel architectures, NeFL decouples a subset of parameters from those being trained for each submodel. An averaging method is proposed to handle these decoupled parameters during aggregation. NeFL enables resource-constrained devices to effectively participate in the FL pipeline, facilitating larger datasets for model training. Experiments demonstrate that NeFL achieves performance gain, especially for the worst-case submodel compared to baseline approaches (7.63% improvement on CIFAR-100). Furthermore, NeFL aligns with recent advances in FL, such as leveraging pre-trained models and accounting for statistical heterogeneity.}
}


@article{DBLP:journals/tmc/FurutanpeyZRPWD25,
	author = {Alireza Furutanpey and
                  Qiyang Zhang and
                  Philipp Raith and
                  Tobias Pfandzelter and
                  Shangguang Wang and
                  Schahram Dustdar},
	title = {{FOOL:} Addressing the Downlink Bottleneck in Satellite Computing
                  With Neural Feature Compression},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6747--6764},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544516},
	doi = {10.1109/TMC.2025.3544516},
	timestamp = {Fri, 16 Jan 2026 20:27:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FurutanpeyZRPWD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents an OEC-native and task-agnostic feature compression method that preserves prediction performance and partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While the encoding prioritizes features for downstream tasks, we can reliably recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Finally, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that the proposed approach permits downlinking over 100× the data volume without relying on prior information on the downstream tasks.}
}


@article{DBLP:journals/tmc/WangNXY25,
	author = {Weizheng Wang and
                  Dusit Niyato and
                  Zehui Xiong and
                  Zhimeng Yin},
	title = {AUTHFi: Cross-Technology Device Authentication via Commodity WiFi},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6765--6779},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547010},
	doi = {10.1109/TMC.2025.3547010},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangNXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosive growth of the Internet of Things (IoT) has dramatically increased the demand for secure mechanisms to protect against unauthorized access and attacks. Traditionally, expensive Software-Defined Radios (SDRs) have been utilized to gather IoT physical features, which are critical for reliable authentication. However, the high cost of SDRs makes them impractical for widespread deployment across the vast and diverse IoT ecosystem. In contrast, this paper presents AUTHFi, a novel cross-technology device authentication framework that transforms the SDR approach for collecting and authenticating IoT device signals (e.g., ZigBee and Bluetooth) by utilizing commercial WiFi devices. Specifically, AUTHFi leverages the recent advances in Cross-Technology Communication (CTC) to reconstruct the partial waveform of IoT transmission, thus eliminating the requirement for expensive SDRs. AUTHFi requires us to address several unique challenges. First, AUTHFi compensates for signal losses of the partial waveform to get more signal information. Then, it introduces an enhanced Carrier Frequency Offset (CFO) estimation and a fusion neural network that combines CFO and the reconstructed waveform for accurate device authentication. We implement AUTHFi based on RTL8812au (commodity WiFi) and CC2652P (commodity ZigBee/Bluetooth). Our thorough evaluation confirms that AUTHFi offers reliable authentication under various settings, achieving a maximum accuracy of 94.2%.}
}


@article{DBLP:journals/tmc/DuanLZLYWZS25,
	author = {Sijing Duan and
                  Feng Lyu and
                  Jing Zhang and
                  Huali Lu and
                  Peng Yang and
                  Huaqing Wu and
                  Yaoxue Zhang and
                  Xuemin Shen},
	title = {MoCo: Urban User Mobile Contact Detection Based on Cellular Signaling
                  Trace},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6780--6796},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545437},
	doi = {10.1109/TMC.2025.3545437},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuanLZLYWZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile contact exhibits user co-traveling events within the same transportation tool, which is crucial for resident profiling, face-to-face interaction detection, etc. In this paper, we investigate urban user mobile contact detection with cellular signaling traces, which is cost-efficient to enable large-scale detection. Specifically, we develop a data collection platform to collect substantial user signaling traces, covering different types of road scenarios within a city. With the collected traces, we perform systematic data analysis to reveal several technical challenges, which are sparsity of signaling trajectory, remote base station noise, and fuzzy matching difficulties. To address challenges, we propose a mobile contact detection method named MoCo. In MoCo framework, we first conduct data denoising to remove the noise from remote base stations. Then, we devise a spatio-temporal filter to eliminate unlikely mobile contact traces in both spatial and temporal domains, reducing the computational overhead. Finally, we design a detection network that integrates the submodules of data alignment, feature encoder, spatio-temporal representation learner, and user mobile contact detector. Extensive evaluation results demonstrate the superiority of MoCo in comparison with state-of-the-art baselines. Robust experiments show that MoCo can work efficiently in different transportation modes and urban densities.}
}


@article{DBLP:journals/tmc/LuoLJKWW25,
	author = {Fei Luo and
                  Anna Li and
                  Bin Jiang and
                  Salabat Khan and
                  Kaishun Wu and
                  Lu Wang},
	title = {ActivityMamba: {A} CNN-Mamba Hybrid Neural Network for Efficient Human
                  Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6797--6811},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544573},
	doi = {10.1109/TMC.2025.3544573},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoLJKWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current research in human activity recognition primarily emphasizes enhancing accuracy, with limited exploration into computational efficiency and hardware compatibility. Recently, Mamba has sparked substantial interest within the realm of deep learning. Mamba is a hardware-aware algorithm enabling very efficient training and inference. Researchers are applying Mamba to various tasks, demonstrating significant promise in both language and vision tasks. It is worthwhile to investigate the use of Mamba for efficient human activity recognition. In this paper, we proposed a hybrid neural network that integrates CNN and visual Mamba, called ActivityMamba. The SE-Mamba block in ActivityMamba utilizes both CNN’s local and Mamba’s global context modeling while keeping computation and memory efficiency. We evaluated the ActivityMamba on five public benchmark datasets collected by using three different sensing techniques. ActivityMamba achieved higher performance than vision transformers, vision Mamba, and CNNs with fewer FLOPs and parameters. It sets a new SOTA on all five datasets, which are 91.78% OA and 89.13% F1 on the USC-HAD dataset, 99.19% OA and 98.64% F1 on the UT-HAR dataset, 99.82% OA and F1 on the DIAT dataset, 98.59% OA and 98.65% F1 on the UCI-HAR dataset, and 95.41% OA and 93.14% F1 on the UniMib dataset. Our work is the first to investigate the CNN-Mamba hybrid network for efficient human activity recognition.}
}


@article{DBLP:journals/tmc/EncinasLagoDRSRC25,
	author = {Guillermo Encinas{-}Lago and
                  Francesco Devoti and
                  Marco Rossanese and
                  Vincenzo Sciancalepore and
                  Marco Di Renzo and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {COLoRIS: Localization-Agnostic Smart Surfaces Enabling Opportunistic
                  {ISAC} in 6G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6812--6826},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556326},
	doi = {10.1109/TMC.2025.3556326},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/EncinasLagoDRSRC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of Smart Surfaces in 6G communication networks, also dubbed as Reconfigurable Intelligent Surfaces (RISs), is a promising paradigm change gaining significant attention given its disruptive features. RISs are a key enabler in the realm of 6G Integrated Sensing and Communication (ISAC) systems where novel services can be offered together with the future mobile networks communication capabilities. This paper addresses the critical challenge of precisely localizing users within a communication network by leveraging the controlled-reflective properties of RIS elements without relying on more power-hungry traditional methods, e.g., GPS, adverting the need of deploying additional infrastructure and even avoiding interfering with communication efforts. Moreover, we go one step beyond: we build COLoRIS, an Opportunistic ISAC approach that leverages localization-agnostic RIS configurations to accurately position mobile users via trained learning models. Extensive experimental validation and simulations in large-scale synthetic scenarios show  5 % \\mathbf{5\\%}  positioning errors (with respect to field size) under different conditions. Further, we show that a low-complexity version running in a limited off-the-shelf (embedded, low-power) system achieves positioning errors in the  11 % \\mathbf{11\\%}  range at a negligible  + 2.7 % \\mathbf{+2.7\\%}  energy expense with respect to the classical RIS.}
}


@article{DBLP:journals/tmc/ZhouLWTCL25,
	author = {Zhiyi Zhou and
                  Xinxin Lu and
                  Lei Wang and
                  Yu Tian and
                  Yunbo Chen and
                  Bingxian Lu},
	title = {Incremental Wavelet-Capsules: {A} Cross-Environment Solution for WiFi
                  Identification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6827--6842},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544836},
	doi = {10.1109/TMC.2025.3544836},
	timestamp = {Thu, 01 Jan 2026 19:11:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLWTCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based identity recognition differs from traditional identification technologies as it is not limited by lighting conditions and does not require dense, specialized sensors or wearable devices. This makes it valuable in modern human–machine interactions. However, the diversity of real-world environmental conditions substantially limits the application of existing WiFi-based identity recognition algorithms, particularly when applied across different environments. As a solution, we introduce the incremental wavelet capsule (IWC) model, which combines a newly designed wavelet convolution layer with a capsule network to accelerate precise feature extraction. We adopt a hybrid incremental learning strategy, solving the catastrophic forgetting1 problem in cross-environment tasks and enabling the model to adapt to new environments in the data stream without forgetting the original environment. Furthermore, we developed a customized data augmentation method for WiFi signals, enhancing the model’s adaptability and stability across various environments. Experimental results show that the IWC model achieves an average recognition accuracy of 97.36% across five different environments and maintains an accuracy of 91.5% even when only 5% of the training data from a new environment is used. These findings demonstrate the model’s robust performance and practicality in cross-environment scenarios.}
}


@article{DBLP:journals/tmc/TongCZHYLY25,
	author = {Haonan Tong and
                  Mingzhe Chen and
                  Jun Zhao and
                  Ye Hu and
                  Zhaohui Yang and
                  Yuchen Liu and
                  Changchuan Yin},
	title = {Continual Reinforcement Learning for Digital Twin Synchronization
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6843--6857},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546507},
	doi = {10.1109/TMC.2025.3546507},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TongCZHYLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article investigates the adaptive resource allocation scheme for digital twin (DT) synchronization optimization over dynamic wireless networks. In our considered model, a base station (BS) continuously collects factory physical object state data from wireless devices to build a real-time virtual DT system for factory event analysis. Due to continuous data transmission, maintaining DT synchronization must use extensive wireless resources. To address this issue, a subset of devices is selected to transmit their sensing data, and resource block (RB) allocation is optimized. This problem is formulated as a constrained Markov process (CMDP) problem that minimizes the long-term mismatch between the physical and virtual systems. To solve this CMDP, we first transform the problem into a dual problem that refines RB constraint impacts on device scheduling strategies. We then propose a continual reinforcement learning (CRL) algorithm to solve the dual problem. The CRL algorithm learns a stable policy across historical experiences for quick adaptation to dynamics in physical states and network capacity. Simulation results show that the CRL can adapt quickly to network capacity changes and reduce normalized root mean square error (NRMSE) between physical and virtual states by up to 55.2%, using the same RB number as traditional methods.}
}


@article{DBLP:journals/tmc/JamshidihaPM25,
	author = {Saeed Jamshidiha and
                  Vahid Pourahmadi and
                  Abbas Mohammadi},
	title = {A Traffic-Aware Graph Neural Network for User Association in Cellular
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6858--6869},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545464},
	doi = {10.1109/TMC.2025.3545464},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JamshidihaPM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we utilize a graph neural network to perform joint user association and access point activation/deactivation to optimize network blockage. Instead of using theoretical models to characterize the distribution of network traffic, we use a real-world network traffic dataset. In order to train the graph neural network, our method leverages reinforcement learning, specifically employing the deep deterministic policy gradient (DDPG) algorithm. This approach allows us to benefit from the advantages of both value-based and policy-based reinforcement learning methods. A simple but flexible reward function is defined to capture the trade-off between the fraction of active access points and network blockage. The graph neural network's awareness of the network topology gives the proposed method a clear performance advantage over the commonly used greedy heuristic optimization method, reducing blockage by up to more than 50% on real-world network traffic data while also reducing computational complexity from  O ( N 3 ) \\mathcal {O}(N^{3})  to  O ( N ) \\mathcal {O}(N) . The proposed user association and access point activation method is not limited by the network architecture or technology, and can be applied to different generations of cellular networks.}
}


@article{DBLP:journals/tmc/HuangWDDY25,
	author = {Jun Huang and
                  Beining Wu and
                  Qiang Duan and
                  Liang Dong and
                  Shui Yu},
	title = {A Fast {UAV} Trajectory Planning Framework in RIS-Assisted Communication
                  Systems With Accelerated Learning via Multithreading and Federating},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6870--6885},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544903},
	doi = {10.1109/TMC.2025.3544903},
	timestamp = {Tue, 05 Aug 2025 22:51:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWDDY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable Intelligent Surface (RIS)-assisted uncrewed Aerial Vehicle (UAV) communications have been realized as essential to space-air-group system integration in the 6 G technology landscape. Trajectory planning plays a crucial role in RIS-assisted UAV communications to face the challenges of UAV’s limited power capacities and dynamic wireless channels. Existing solutions assume complete channel state information, focus on single-rotor UAVs, and rely heavily on time-consuming training processes for machine learning; thus, they lack applicability to deal with highly dynamic real-world scenarios. To fill these research gaps, we aim to characterize RIS-assisted UAV communications and design responsive and accurate UAV trajectory planning algorithms in this paper. We first develop a communication model with incomplete information and an energy consumption model for quadrotor UAVs. We then formulate UAV trajectory planning as an optimization problem to minimize UAV’s energy consumption while maintaining communication throughput. To solve this problem, we design an acceleration framework, FedX, for reinforcement learning (RL) solvers and present two fast trajectory planning algorithms, FedSAC and FedPPO, as instantiations of the FedX framework. Our evaluation results indicate that the proposed framework is effective and efficient–more than 3 times faster with 5 agents and 7 times faster with 10 agents than standard RL algorithms, making it suitable for using RL solvers within wireless networks and mobile computing environments. We also discuss and identify the pros and cons of our proposed framework.}
}


@article{DBLP:journals/tmc/LiuHWQJ25,
	author = {Yu Liu and
                  Chenyu Huang and
                  Kaiyi Wang and
                  Zheng Qin and
                  Wenqiang Jin},
	title = {MIPair: Exploiting Magnetic Induction for Laptop-Phone Pairing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6886--6900},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546758},
	doi = {10.1109/TMC.2025.3546758},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHWQJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transferring important files, photos, and other sensitive data between laptops and smartphones has become a routine necessity in daily life. Device pairing acts as the most fundamental need to secure the communication channel between two unconnected devices. Traditional pairing methods leveraging PINs or QR codes require tedious human efforts in the pairing procedures to establish a shared communication key. Nevertheless, these designs are vulnerable to shoulder-surfing attacks in which attackers might record and replay the pairing credentials. It is preferred to have more intuitive pairing designs that minimize users’ overhead in the pairing process while providing secure keys for communication purposes. In this paper, we propose MIPair for laptop-phone pairing by leveraging magnetic induction (MI) signals. MIPair is based on a key observation that changes in the CPU workload of a device cause variations in internal current, thereby inducing changes in surrounding magnetic fields. Moreover, the trends of MI signal variations are highly correlated with CPU workload trends. Thus, users simply need to place a smartphone on the keyboard of a laptop. By randomly altering the workload of the laptop through a stimulation program, the smartphone can capture MI signals with similar changing trends, thereby converting them into similar bit sequences that form the basis of a symmetric key. We propose essential techniques to overcome challenges such as time asynchronization between two devices, unfixed state transition time in MI signal, and shared key distribution from the two similar bit sequences. Our real-world experiments demonstrate reliable pairing as well as robustness against common attacks and high randomness of the generated keys. When generating a 128-bit key, MIPair achieves a successful pairing rate as high as 98% and a usable pairing time of 8.35 seconds.}
}


@article{DBLP:journals/tmc/ZhangMWYGZWG25,
	author = {Yao Zhang and
                  Yuyi Mao and
                  Hui Wang and
                  Zhiwen Yu and
                  Song Guo and
                  Jun Zhang and
                  Liang Wang and
                  Bin Guo},
	title = {Orchestrating Joint Offloading and Scheduling for Low-Latency Edge
                  {SLAM}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6901--6917},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547256},
	doi = {10.1109/TMC.2025.3547256},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangMWYGZWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visual Simultaneous Localization and Mapping (vSLAM) is a prevailing technology for many emerging robotic applications. Achieving real-time SLAM on mobile robotic systems with limited computational resources is challenging because the complexity of SLAM algorithms increases over time. This restriction can be lifted by offloading computations to edge servers, forming the emerging paradigm of edge-assisted SLAM. Nevertheless, the exogenous and stochastic input processes affect the dynamics of the edge-assisted SLAM system. Moreover, the requirements of clients on SLAM metrics change over time, exerting implicit and time-varying effects on the system. In this paper, we aim to push the limit beyond existing edge-assist SLAM by proposing a new architecture that can handle the input-driven processes and also satisfy clients’ implicit and time-varying requirements. The key innovations of our work involve a regional feature prediction method for importance-aware local data processing, a configuration adaptation policy that integrates data compression/decompression and task offloading, and an input-dependent learning framework for task scheduling with constraint satisfaction. Extensive experiments prove that our architecture improves pose estimation accuracy and saves up to 47% of communication costs compared with a popular edge-assisted SLAM system, as well as effectively satisfies the clients’ requirements.}
}


@article{DBLP:journals/tmc/HongYWLYWLWZ25,
	author = {Zhiqing Hong and
                  Heng Yang and
                  Haotian Wang and
                  Wenjun Lyu and
                  Yu Yang and
                  Guang Wang and
                  Yunhuai Liu and
                  Yang Wang and
                  Desheng Zhang},
	title = {Real-Time Abnormal Address Detection for Mobile Devices in Location-Based
                  Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6918--6933},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545875},
	doi = {10.1109/TMC.2025.3545875},
	timestamp = {Sun, 09 Nov 2025 16:59:25 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HongYWLYWLWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An address, a textual description of a geographical location, plays an important role in location-based services such as instant delivery. However, abnormal addresses (i.e., an address without detailed or accurate information) have led to significant costs. In real-world settings, abnormal address detection is not trivial because it needs to be completed in real-time to support massive online queries from mobile devices. In this study, we design FastAddr, a fast abnormal address detection framework, which detects abnormal addresses in real time. FastAddr consists of a novel contrastive address augmentation module and a lightweight multi-head attention model. We further design FastAddr+ to enhance FastAddr by utilizing large-scale spatial entities. A comprehensive three-phase evaluation is conducted. (i) We evaluate FastAddr on a real-world dataset and it yields the average F1 of 85.7% in 0.058 milliseconds, which outperforms the state-of-the-art models by 47.4% with a similar detection time. (ii) An offline A/B test shows that FastAddr outperforms the previous model significantly. (iii) We also conduct an online A/B test to compare FastAddr with the deployed model, which shows an improvement of F1 by more than 20%. Moreover, we conduct two case studies on real industry data, demonstrating both the efficiency and effectiveness of FastAddr.}
}


@article{DBLP:journals/tmc/EhsanNSMM25,
	author = {Muhammad Khurram Ehsan and
                  Neelma Naz and
                  Ali Hassan Sodhro and
                  Shahid Mumtaz and
                  Asad Mahmood},
	title = {{DT-RSSI:} Digital Twin-Replica of Sensing Statistics for {IRA} in
                  Intelligent NG-HetNetIs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6934--6944},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3544918},
	doi = {10.1109/TMC.2025.3544918},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/EhsanNSMM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent resource allocation maintains a better quality of service among devices in next-generation heterogeneous network infrastructures (NG-HetNetIs). NG-HetNetIs include industry 5.0 enabled infrastructures like Internet of Things (IoT), cognitive radio (CR) enabled B5G and 6G networks, unmanned aerial vehicles (UAVs), wireless sensor networks (WSNs) and autonomous vehicles (AVs). Digital twin (DT) joins hand with cognitive radio and resource aggregation technologies to provide the integrated framework for intelligent resource allocation in NG-HetNetIs. In NG-HetNetIs, the obtained statistics of measured radio activity as prior information play an instrumental role in enabling optimized resource allocation using context awareness. Unfortunately, the already available static approaches are inefficient to replicate (DT) the radio activity in a heterogeneous radio environment. To address the issue, static implementation framework is extended as dynamic radio activity characterization framework (DRAC) to have context awareness in NG-HetNetIs. The proposed DRAC replicates (DT) the wide sense stationarity of time and carrier aggregated radio activity due to its exploitation of more localized temporal and spectral information in NG-HetNets. The obtained localized statistics using DRAC can be exploited as appropriate prior knowledge and test statistics during the spectrum sensing phase of NG-HetNetIs for intelligent resource allocation instead of a single statistic obtained by the static approach.}
}


@article{DBLP:journals/tmc/ZhaoGTHSXT25,
	author = {Liang Zhao and
                  Shenglin Geng and
                  Xiongyan Tang and
                  Ammar Hawbani and
                  Yunhe Sun and
                  Lexi Xu and
                  Daniele Tarchi},
	title = {{ALANINE:} {A} Novel Decentralized Personalized Federated Learning
                  for Heterogeneous {LEO} Satellite Constellation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6945--6960},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3545429},
	doi = {10.1109/TMC.2025.3545429},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoGTHSXT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellite constellations have seen significant growth and functional enhancement in recent years, which integrates various capabilities like communication, navigation, and remote sensing. However, the heterogeneity of data collected by different satellites and the problems of efficient inter-satellite collaborative computation pose significant obstacles to realizing the potential of these constellations. Existing approaches struggle with data heterogeneity, varing image resolutions, and the need for efficient on-orbit model training. To address these challenges, we propose a novel decentralized PFL framework, namely, A Novel DecentraLized PersonAlized Federated Learning for HeterogeNeous LEO SatellIte CoNstEllation (ALANINE). ALANINE incorporates decentralized FL (DFL) for satellite image Super Resolution (SR), which enhances input data quality. Then it utilizes PFL to implement a personalized approach that accounts for unique characteristics of satellite data. In addition, the framework employs advanced model pruning to optimize model complexity and transmission efficiency. The framework enables efficient data acquisition and processing while improving the accuracy of PFL image processing models. Simulation results demonstrate that ALANINE exhibits superior performance in on-orbit training of SR and PFL image processing models compared to traditional centralized approaches. This novel method shows significant improvements in data acquisition efficiency, process accuracy, and model adaptability to local satellite conditions.}
}


@article{DBLP:journals/tmc/WangLDOHF25,
	author = {Qiwei Wang and
                  Chi Lin and
                  Haipeng Dai and
                  Mohammad S. Obaidat and
                  Kuei{-}Fang Hsiao and
                  Xin Fan},
	title = {Compromising Rechargeable Sensor Networks in Marine Environment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6961--6977},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546276},
	doi = {10.1109/TMC.2025.3546276},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLDOHF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Marine Wireless Rechargeable Sensor Networks (MWRSNs), enhanced by recent Wireless Power Transfer (WPT) technology, present a significant advancement in extending network life. Traditional methods improve network performance through algorithm optimization, but neglect charging security, exposing networks to potential attacks. This paper addresses this problem from an adversarial view and develops a novel attack for MWRSN through Denying of Charge (DoC) to maximize network destructiveness. We start by establishing a generalized on-demand charging model, essential for developing DoC tactics. Subsequently, we unveil the Collaborative DoC (CoDoC) algorithm, capable of manipulating and falsifying charging requests. Central to CoDoC is the Request Prediction Method (RPM), which forecasts the initiation of charging requests and facilitates rapid request surges to enhance the attack's efficacy. CoDoC is able to disguise the presence of the attack, which is able to escape from being detected by the base station. Theoretical analyses are provided to explore the features of the proposed scheme. To demonstrate the outperformed features of the proposed schemes, extensive simulations and test-bed experiments are conducted. Our analysis and extensive simulations demonstrate that CoDoC increases sensor node failures by 20% to 142% compared to traditional methods, highlighting its effectiveness in marine environments.}
}


@article{DBLP:journals/tmc/WanJXWZW25,
	author = {Yiyao Wan and
                  Jiahuan Ji and
                  Wenqing Xie and
                  Guangyu Wu and
                  Fuhui Zhou and
                  Qihui Wu},
	title = {A Multimodal Scale Normalization Framework for Vision-Radar Small
                  {UAV} Positioning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6978--6995},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549620},
	doi = {10.1109/TMC.2025.3549620},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WanJXWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed aerial vehicles (UAVs) positioning is of crucial importance in diverse applications. However, it is extremely challenging to realize the precise UAVs positioning over long distances due to the small size and dramatic scale variations associated with the high mobility in the wide area. To tackle this issue, a multimodal scale normalization framework is proposed for the scale-robust precise pixel-level UAV positioning. The framework exploits our proposed distance-aware image slicing and distance-aware scale normalization module. Moreover, a modal fusion-based scale normalization network is proposed that can accept arbitrary low-resolution UAV patches and produce the consistent high-resolution images at a uniform UAV instance scale with a single learnable model. The proposed framework is generic and can be directly used in the existing pixel-level positioning pipelines to improve the positioning performance and scale robustness. To verify the proposed framework in the real application, a practical vision-radar UAV positioning system is developed. Experimental results on the real-world dataset demonstrate the generality and effectiveness of our framework. Moreover, the ablation experiments also confirm the contribution of each module in the framework.}
}


@article{DBLP:journals/tmc/DaiTLLJD25,
	author = {Xingxia Dai and
                  Shujuan Tian and
                  Haolin Liu and
                  Zhetao Li and
                  Hongbo Jiang and
                  Qingyong Deng},
	title = {Joint Optimization of Offloading and Caching in Full-Duplex-Enabled
                  Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {6996--7011},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546263},
	doi = {10.1109/TMC.2025.3546263},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiTLLJD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing (EC) reduces task processing and content download delay by providing computation and caching resources directly to task offloading (TO) users and content request (CR) users. However, existing studies often focus exclusively on either TO users or CR users within EC networks, neglecting the interaction between these two groups. To address this gap, we investigate the offloading and caching decision-making in scenarios where TO and CR users coexist. Furthermore, we employ full-duplex (FD) technology to enhance spectral utilization for edge-end transmissions. Specifically, we jointly optimize offloading and caching in FD-enabled EC networks. To accomplish this, we decompose the formulated optimization problem into three sub-problems using the alternating optimization (AO) method. We then propose a three-subproblem alternating iterative delay minimization algorithm to effectively tackle the challenges of offloading and caching. Additionally, we analyze the convergence and complexity of our proposed algorithm. Finally, we conduct extensive simulations to evaluate the effectiveness of our approach. The simulation results demonstrate that the delay reduction achieved by our algorithm is between 24.78% and 89.23% greater than that of comparative algorithms.}
}


@article{DBLP:journals/tmc/PonomarenkoTimofeevGBHA25,
	author = {Aleksei A. Ponomarenko{-}Timofeev and
                  Olga Galinina and
                  Ravikumar Balakrishnan and
                  Nageen Himayat and
                  Sergey Andreev and
                  Yevgeni Koucheryavy},
	title = {Multi-Task Model Personalization for Federated Supervised {SVM} in
                  Heterogeneous Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7012--7027},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546550},
	doi = {10.1109/TMC.2025.3546550},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PonomarenkoTimofeevGBHA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated systems enable collaborative training on highly heterogeneous, non-i.i.d. data through model personalization, which can be facilitated by employing multi-task learning. However, multi-task learning algorithms are often implemented using methods like stochastic gradient descent, which may suffer from slow convergence in a multi-task federated setting. To accelerate the training procedure, we design an efficient iterative distributed method based on the alternating direction method of multipliers (ADMM) for support vector machines (SVMs), which tackles federated classification and regression. The proposed method utilizes efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To ensure data privacy, we introduce a randomization algorithm that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and participant hardware and data heterogeneity on the system performance. Our experiments confirm the advantages of the proposed ADMM-based personalized federated multi-task learning.}
}


@article{DBLP:journals/tmc/DengXC25,
	author = {Hongyu Deng and
                  Tianfan Xue and
                  He Henry Chen},
	title = {FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent
                  Objects},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7028--7041},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547371},
	doi = {10.1109/TMC.2025.3547371},
	timestamp = {Tue, 05 Aug 2025 22:51:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transparent objects are prevalent in everyday environments, but their distinct physical properties pose significant challenges for camera-guided robotic arms. Current research is mainly dependent on camera-only approaches, which often falter in suboptimal conditions, such as low-light environments. In response to this challenge, we present FuseGrasp, the first radar-camera fusion system tailored to enhance the transparent objects manipulation. FuseGrasp exploits the weak penetrating property of millimeter-wave (mmWave) signals, which causes transparent materials to appear opaque, and combines it with the precise motion control of a robotic arm to acquire high-quality mmWave radar images of transparent objects. The system employs a carefully designed deep neural network to fuse radar and camera imagery, thereby improving depth completion and elevating the success rate of object grasping. Nevertheless, training FuseGrasp effectively is non-trivial, due to limited radar image datasets for transparent objects. We address this issue utilizing large RGB-D dataset, and propose an effective two-stage training approach: we first pre-train FuseGrasp on a large public RGB-D dataset of transparent objects, then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as a byproduct, FuseGrasp can determine the composition of transparent objects, such as glass or plastic, leveraging the material identification capability of mmWave radar. This identification result facilitates the robotic arm in modulating its grip force appropriately. Extensive testing reveals that FuseGrasp significantly improves the accuracy of depth reconstruction and material identification for transparent objects. Moreover, real-world robotic trials have confirmed that FuseGrasp markedly enhances the handling of transparent items.}
}


@article{DBLP:journals/tmc/ZhangZSWCFLHLC25,
	author = {Haoyu Zhang and
                  Dongheng Zhang and
                  Ruiyuan Song and
                  Zhi Wu and
                  Jinbo Chen and
                  Liang Fang and
                  Zhi Lu and
                  Yang Hu and
                  Hui Lin and
                  Yan Chen},
	title = {{UMIMO:} Universal Unsupervised Learning for Mmwave Radar Sensing
                  With {MIMO} Array Synthesis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7042--7058},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546757},
	doi = {10.1109/TMC.2025.3546757},
	timestamp = {Mon, 02 Feb 2026 13:11:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZSWCFLHLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) radar sensing powered by deep learning is now emerging in numerous applications, which are predominantly trained in a supervised manner. However, due to the non-interpretable nature of mmWave signals, labeling the radar data has always been a difficult task. While there have been investigations on unsupervised pre-training for mmWave radar sensing, these methods are tailored to specific signal representations. In this paper, we propose UMIMO, an unsupervised learning framework combining the hardware nature of MIMO radar and deep learning techniques to resolve the challenge raised by the insufficient labeled data. UMIMO leverages the antenna arrays synthesized from multiple transmitting and receiving antennas in mmWave radar to construct positive samples for contrastive learning. To achieve this, we propose the constraints on angular resolution and grating lobes to generate effective signal representations with different synthetic arrays. We conduct experiments using UMIMO on three tasks: contactless ECG monitoring, 3D human pose estimation, and human silhouette generation. All experimental results demonstrate that UMIMO can effectively improve the performance of learning-based mmWave radar sensing in an unsupervised manner.}
}


@article{DBLP:journals/tmc/YiGWZWX25,
	author = {Rongjie Yi and
                  Liwei Guo and
                  Shiyun Wei and
                  Ao Zhou and
                  Shangguang Wang and
                  Mengwei Xu},
	title = {EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7059--7073},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546466},
	doi = {10.1109/TMC.2025.3546466},
	timestamp = {Tue, 18 Nov 2025 13:49:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YiGWZWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) such as GPTs and Mixtral-8x7B have revolutionized machine intelligence due to their exceptional abilities in generic ML tasks. Transiting LLMs from datacenters to edge devices brings benefits like better privacy and availability, but is challenged by their massive parameter size and thus unbearable runtime costs. To this end, we present EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs – a popular form of sparse LLM that scales its parameter size with almost constant computing complexity. EdgeMoE achieves both memory- and compute-efficiency by partitioning the model into the storage hierarchy: non-expert weights are held in device memory; while expert weights are held on external storage and fetched to memory only when activated. This design is motivated by a key observation that expert weights are bulky but infrequently used due to sparse activation. To further reduce the expert I/O swapping overhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth adaptation that reduces the expert sizes with tolerable accuracy loss; (2) expert preloading that predicts the activated experts ahead of time and preloads it with the compute-I/O pipeline. On popular MoE LLMs and edge devices, EdgeMoE showcase significant memory savings and speedup over competitive baselines.}
}


@article{DBLP:journals/tmc/LinWLHWWFW25,
	author = {Chi Lin and
                  Qiwei Wang and
                  Hengyi Li and
                  Shibo Hao and
                  Yi Wang and
                  Lei Wang and
                  Xin Fan and
                  Guowei Wu},
	title = {Wireless Charging for Uncertain Location Nodes},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7074--7091},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546316},
	doi = {10.1109/TMC.2025.3546316},
	timestamp = {Thu, 06 Nov 2025 14:15:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinWLHWWFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from Wireless Power Transfer (WPT) technology, Wireless Rechargeable Sensor Networks (WRSNs) effectively address the lifetime bottleneck of sensor nodes, enabling them to work perpetually. Most state-of-the-art studies assume that all WRSNs’ information is known or precise in advance. However, sensor nodes may be deployed randomly in a large-scale area, and some critical information (such as node location) may be unavailable or difficult to obtain precisely. In this work, we eliminate the effect of uncertain or imprecise node location and formalize the Maximizing Charging Energy utility for uncertain location nodes problem (i.e., MCE problem). With magnetic resonance coupling and beamforming technologies, we propose a novel node localization method to determine precise node location information. In addition, we present a reinforcement learning framework and a charging path scheduling method to maximize charging energy. To validate the effectiveness of our proposed scheme in real-world scenarios, we conduct test-bed experiments. The results demonstrate that our approach significantly improves charging efficiency by an average of 20.9% in a large-scale network, even when the locations of sensors are entirely unknown.}
}


@article{DBLP:journals/tmc/LiuZQYLC25,
	author = {Tao Liu and
                  Suiwen Zhang and
                  Xiaomei Qu and
                  Lijun Yang and
                  Chengjie Li and
                  Yihong Chen},
	title = {Joint Optimization for IRS-Assisted Self-Powered IoT in 5G mmWave
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7092--7106},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547790},
	doi = {10.1109/TMC.2025.3547790},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZQYLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Harvesting energy from ambient energy source is the key technology for self-powered Internet of Things (IoT) devices to maintain continuous operation without an external power supply. Motivated by the expansion and popularity of 5G networks, we propose a novel solution for IoT devices which are self-powered via harvesting energy from the millimeter-wave (mmWave) communications in 5G mmWave networks. For overcoming the high path loss in mmWave communications, directional narrow-beam transmission is adopted to provide sufficient link budget between transceivers through beamforming technology, which however makes IoT devices difficult to scavenge energy from the mmWave signals. Hence, we employ multiple intelligent reflecting surfaces (IRSs) to assist in energy harvesting at the IoT devices and data transmission at the 5G users. Considering beam codebook design for 5G mmWave networks, this paper jointly optimizes the Discrete Fourier transform (DFT) codebook-based transmit codevectors at the 5G base station (BS) and the phase shifts of IRS's reflective elements for minimizing BS's transmit power, while satisfying the Signal to Interference plus Noise Ratio (SINR) constraints at users and energy harvesting constraints of IoT devices. Nevertheless, owing to the intricate coupling of variables and discrete constraints, this joint optimization problem is extremely non-convex and non-linear. To address such challenges, we propose a penalty dual-decomposition (PDD)-based algorithm which combines the penalty-based augmented Lagrangian method and block coordinate descent method. It explores the structure of the mmWave channel and performs a double iterations in which the joint optimization problem is decomposed into several simplified subproblems. Simulation results reveal that the above algorithm enhances the energy efficiency as compared to other algorithms.}
}


@article{DBLP:journals/tmc/ChenYCKZC25,
	author = {Yunzhong Chen and
                  Jiadi Yu and
                  Yingying Chen and
                  Linghe Kong and
                  Yanmin Zhu and
                  Yichao Chen},
	title = {Sensing Metal Coil Vibration of Headsets for Eavesdropping on Online
                  Conversations With Out-of-Vocabulary Words Using {RFID}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7107--7120},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548980},
	doi = {10.1109/TMC.2025.3548980},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYCKZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most essential accessories, headsets have been widely used in common online conversations. The metal coil vibration patterns of headset speakers/microphones have been proven to be highly correlated with the speaker-produced/microphone-received sound. This paper presents an online conversation eavesdropping system, RFSpy, which uses only one RFID tag attached on a headset to alternately sense metal coil vibrations of headset speaker and microphone for eavesdropping on speaker-produced and microphone-received sound. In some accessible scenarios, assuming attackers secretly attach a small, battery-free RFID tag under one ear cushion of an eavesdropped user’s headset without being noticed. Meanwhile, RFID readers are camouflaged as decorations placed in/out of rooms to transmit and receive RF signals. When the eavesdropped user talks with other users online through the headset, RFSpy first activates the RFID tag to capture the metal coil vibration patterns of headset speaker and microphone upon RF signals. Then, RFSpy reconstructs sound spectrograms from the RF signal-based vibration patterns for not only trained words but also untrained (i.e., out-of-vocabulary) words utilizing designed SSR network. Finally, RFSpy converts the sound spectrograms to conversation content through sound recognition API. Extensive experiments demonstrate that RFSpy can eavesdrop on online conversations with out-of-vocabulary words effectively.}
}


@article{DBLP:journals/tmc/ShenSASY25,
	author = {Yizhou Shen and
                  Carlton Shepherd and
                  Chuadhry Mujeeb Ahmed and
                  Shigen Shen and
                  Shui Yu},
	title = {Privacy Preservation Strategies for Malware-Infected Edge Intelligence
                  Systems: {A} Bayesian Stochastic Game-Based Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7121--7135},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546910},
	doi = {10.1109/TMC.2025.3546910},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenSASY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malware in the Internet of Things (IoT) is prone to contaminating various IoT end-points through network communication and information transfer, leading to surreptitious privacy leakage and data theft. The existing privacy-preserving approaches including data masking, anonymization, and differential privacy always lack the consideration of strategic interactions among rational agents. Inspired by Bayesian games, we model incomplete stochastic games between IoT end-points and edge nodes in edge intelligence (EI)-enabled IoT systems to conduct probability analysis for predicting and defending privacy leakage caused by malware infection. It is notable that the posterior probability is defined based on the Bayes’ rule to reflect the statistical inference of incomplete privacy leakage information. Such a method can intrinsically characterize the actual situations of IoT end-points. Further, we propose a novel privacy preservation optimization approach named Bayesian advantage actor critic (BA2C) for the practical implementation of optimization decision in EI-enabled IoT privacy-preserving systems. Eventually, we conduct experimental simulations to understand the most effective parameters in decision-making among the successful detection rate, successful infection rate, and false alarm rate. We also compare traditional algorithms and validate the efficacy of the proposed approach.}
}


@article{DBLP:journals/tmc/ZhouTKHC25,
	author = {Yuanhang Zhou and
                  Fei Tong and
                  Chunming Kong and
                  Shibo He and
                  Guang Cheng},
	title = {Towards Efficient, Robust, and Privacy-Preserving Incentives for Crowdsensing
                  via Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7136--7151},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546941},
	doi = {10.1109/TMC.2025.3546941},
	timestamp = {Wed, 19 Nov 2025 07:42:09 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouTKHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive development of mobile devices, mobile crowdsensing (MCS) has emerged as a promising approach for large-scale sensing data collection. In the research of MCS, blockchain technology has been widely adopted to decentralize the traditional mobile crowdsensing and tackle the problem of single point of failure. Incentive mechanisms are devised to boost participation with fairness and truthfulness. However, to better determine the incentive strategy, participants’ privacy can be disclosed on top of the blockchain and obtained by adversaries during the transmission and execution of user data, leading to serious security issues. In this paper, we propose a two-stage incentive scheme with efficiency, robustness and privacy preservation considered based on the combination of blockchain technology and Trusted Execution Environment (TEE). Detailedly, we design two kinds of smart contracts, where on-chain public contracts support the procedure of general crowdsensing interactions, and off-chain private ones enabled by TEE complete the privacy-preserving computations, including an online incentive mechanism for worker recruitment decisions and a truth discovery algorithm for data aggregation. Recovery mechanism and hash check mechanism are introduced to avoid TEE provider failures and TEE providers’ attacks, respectively. Our scheme is proved to be theoretically secure in terms of private information protection, worker participation anonymity, and data aggregation privacy. Experimental results also verify the feasibility and superiority of our incentive scheme.}
}


@article{DBLP:journals/tmc/SunZDZNKWRGW25,
	author = {Mingjie Sun and
                  Jie Zheng and
                  Hongyang Du and
                  Haijun Zhang and
                  Dusit Niyato and
                  Jiawen Kang and
                  Jiacheng Wang and
                  Jie Ren and
                  Ling Gao and
                  Zheng Wang},
	title = {Trust Online Over-the-Air Computation for Wireless Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7152--7170},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547148},
	doi = {10.1109/TMC.2025.3547148},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunZDZNKWRGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using the wireless waveform superposition property, over-the-air computation (OAC) enables federated learning (FL) to achieve fast model aggregation. However, this computing paradigm is vulnerable to poisoning attacks due to the openness of a wireless channel over time, where malicious mobile devices can introduce cumulative errors for the global FL model in a time-varying wireless environment for each communication round. This article presents a trust online OAC (TO-OAC) scheme to minimize impacts on the global model introduced by malicious devices adjusting to dynamic attack and wireless channel fluctuations over time. TO-OAC achieves this by utilizing trustworthy security quantification of OAC for each FL training round. To optimize the cumulative training loss at the aggregation node with the long-term power and trust constraints of mobile devices, we propose a joint trust, power, and channel-aware algorithm to flexibly update local and global models in response to the dynamic changes in the wireless and secure environment. We analyze the performance limits for the aggregation of trust models, considering metrics for computation and communication through time. We then propose another trust online regularization over-the-air computation (TOR-OAC) as an improved version of the TO-OAC scheme to decrease convergence time while ensuring long-term trust and power limitation. Experimental results performed on real-life datasets show that the two proposed schemes (TO-OAC and TOR-OAC) outperform prior works, especially in noisy, time-varying wireless channels and malicious attacks.}
}


@article{DBLP:journals/tmc/XuTZSZRJZ25,
	author = {Yang Xu and
                  Yunlin Tan and
                  Cheng Zhang and
                  Peng Sun and
                  Yibang Zhang and
                  Ju Ren and
                  Hongbo Jiang and
                  Yaoxue Zhang},
	title = {Towards Privacy-Enhanced and Robust Clustered Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7171--7188},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547149},
	doi = {10.1109/TMC.2025.3547149},
	timestamp = {Sun, 14 Sep 2025 11:27:36 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuTZSZRJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustered federated learning (CFL) leverages data distribution similarities to cluster clients, facilitating personalized model training under data heterogeneity. However, most existing CFL schemes pose potential privacy risks for clients (e.g., gradient inversion attacks) as they rely on individual gradients for clustering. This also renders them incompatible with secure aggregation mechanisms that are widely employed in federated learning for privacy protection. Moreover, CFL introduces the risk of malicious clients dominating several clusters and conducting poisoning attacks therein, thereby threatening secure model training. To address these issues, we propose ProCFL, a Privacy-Enhanced and Robust CFL framework incorporating gradient-free clustering and peer validation. Specifically, we first design a new protocol for measuring data distribution similarity among clients without using their gradient information. Then, we transform the client clustering process into a weighted set covering problem and introduce a diversity-optimized clustering algorithm to achieve near-optimal clustering results while eliminating any need for prior knowledge. Furthermore, we develop a post-hoc detection mechanism that employs peer validation to identify and discard malicious client models. Extensive experimental evaluation of ProCFL validates its superior model robustness and accuracy performance compared to existing schemes.}
}


@article{DBLP:journals/tmc/LiuWLWQZL25,
	author = {Xiang Liu and
                  Weiwei Wu and
                  Minming Li and
                  Wanyuan Wang and
                  Yifan Qin and
                  Yingchao Zhao and
                  Junzhou Luo},
	title = {Budget-Feasible Diffusion Mechanisms for Mobile Crowdsourcing in Social
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7189--7205},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549751},
	doi = {10.1109/TMC.2025.3549751},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWLWQZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsourcing has emerged as a popular approach for organizations to leverage the collective intelligence of a crowd of users to obtain services. Considering users’ costs for providing services, it is vital for the requester to design incentive mechanisms to encourage users’ participation in crowdsourcing under the budget constraint. This aligns with the concept of budget-feasible mechanism design. Existing budget-feasible mechanisms often assume immediate user reachability and willingness of joining the crowdsourcing, which is unrealistic. To address this issue, a promising approach is to have participating users diffuse auction information to potential users in the social network. However, this brings another challenge in that participating users can be strategic and therefore hesitant to invite more potential competitors to join the crowdsourcing platform. In this paper, we focus on developing diffusion mechanisms that incentivize strategic users to actively diffuse auction information through the social network. This helps to attract more informed users and ultimately increases the value of the procured services. Specifically, we propose optimal budget-feasible diffusion mechanisms that simultaneously guarantee individual rationality, budget-feasibility, strong budget-balance, incentive-compatibility (i.e., users report real costs and diffuse auction information to all their neighbors) and approximation. Experiment results under real datasets further demonstrate the efficiency of proposed mechanisms.}
}


@article{DBLP:journals/tmc/DingGH25,
	author = {Ningning Ding and
                  Lin Gao and
                  Jianwei Huang},
	title = {Incentive Mechanism Design for Federated Learning With Dynamic Network
                  Pricing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7206--7222},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3546977},
	doi = {10.1109/TMC.2025.3546977},
	timestamp = {Tue, 05 Aug 2025 22:51:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DingGH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning protects users’ data privacy by sharing users’ local model parameters (instead of raw data) with a server. However, when massive users train a large machine learning model through federated learning, the dynamically varying and often heavy communication overhead can put significant pressure on the network operator. The operator may choose to dynamically change the network prices in response, which will eventually affect the payoffs of the server and users. This paper considers the under-explored yet important issue of the joint design of participation incentives (for encouraging users’ contribution to federated learning) and network pricing (for managing network resources). Due to heterogeneous users’ private information and multi-dimensional decisions, the optimization problems in Stage I of multi-stage games are non-convex. Nevertheless, we are able to analytically derive the corresponding optimal contract and pricing mechanism through proper transformations of constraints, variables, and functions, under three interaction structures of the participants. We show that the coordinated structure is better than the two uncoordinated structures, as it avoids the selfish behaviors of the network operator and the server; the vertically uncoordinated structure is better than the horizontally uncoordinated structure, as it avoids the interests misalignment between the server and the network operator. We also propose multi-period network pricing to reduce the implementation complexity of dynamic pricing. Numerical results based on real-world datasets show that our proposed mechanisms decrease the server's cost by up to 24.87% and increase the network operator's profit by up to 1245.25%, compared with the state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/HuangYMWXY25,
	author = {Shan Huang and
                  Haipeng Yao and
                  Tianle Mai and
                  Di Wu and
                  Jiaqi Xu and
                  F. Richard Yu},
	title = {Multi-Agent Moth-Flame Reinforcement Learning Based Broadcast Beam
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7223--7236},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547946},
	doi = {10.1109/TMC.2025.3547946},
	timestamp = {Mon, 25 Aug 2025 12:25:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangYMWXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, beamforming antenna array technologies are of utmost importance in 5G communication systems. These technologies are essential for optimizing the coverage and signal quality of the cellular network. However, the optimization of broadcast beams presents significant challenges due to the complex strategy profile space. Each beam can be configured with different widths and heights, making it difficult for conventional algorithms to handle. To address this issue, we propose a novel approach called Multi-Agent Moth-Flame Reinforcement Learning (MAMF-RL) algorithm for broadcast beam optimization. MAMF-RL combines reinforcement learning and moth-flame optimization algorithms to interactively search for the optimal broadcast beams. By decomposing the problem into multiple single-sector antenna configuration problems, MAMF-RL effectively reduces the algorithm complexity. We conducted experiments utilizing real data in an 18-sector wireless coverage area. To evaluate the performance of our proposed method, we compared it with traditional methods such as the particle swarm algorithm. The results demonstrate that our MAMF-RL model achieves an average coverage rate of 1.82% higher and a 13.74% lower overlapping coverage rate compared to traditional methods.}
}


@article{DBLP:journals/tmc/CuiDMLZF25,
	author = {Yue Cui and
                  Haichuan Ding and
                  Ying Ma and
                  Xuanheng Li and
                  Haixia Zhang and
                  Yuguang Fang},
	title = {Blockage-Resilient Integrated Sensing and Communication in mmWave
                  Networks: Multi-View Collaboration and Efficient Task Allocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7237--7251},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551099},
	doi = {10.1109/TMC.2025.3551099},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CuiDMLZF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated sensing and communication (ISAC) has emerged as a promising technology for future millimeter wave (mmWave) networks. However, the susceptibility of mmWave signals to blockages poses considerable challenges for ISAC as it can result in unreliable links and disrupted sensing. As a result, this paper investigates the blockage-resilient ISAC design that leverages the robustness offered by multi-base station (BS) collaboration. Given the dynamic blockages and the fluctuation in the targets’ radar cross section (RCS), the blockage-resilient multi-BS collaborative ISAC design is cast as a chance constrained integer programming (CCIP) by jointly considering the diverse deadlines of different sensing tasks and the spatial/temporal user-target pairing for dual-functional radar and communication (DFRC) waveform scheduling. To facilitate efficient solution finding, we develop a group concatenating assisted reinforcement learning (GCRL) algorithm, where we linearize the chance constraints via variable grouping and concatenation, enabling the RL agent to understand the problem structure with bipartite graphs so as to develop an efficient branching policy. Extensive experiments demonstrate the resilience of the obtained ISAC scheme to dynamic blockages.}
}


@article{DBLP:journals/tmc/ZhangTTSLLQHSHSY25,
	author = {Haojie Zhang and
                  Fuze Tian and
                  Yang Tan and
                  Lin Shen and
                  Jingyu Liu and
                  Jie Liu and
                  Kun Qian and
                  Yalei Han and
                  Gong Su and
                  Bin Hu and
                  Bj{\"{o}}rn W. Schuller and
                  Yoshiharu Yamamoto},
	title = {An AI-Assisted All-in-One Integrated Coronary Artery Disease Diagnosis
                  System Using a Portable Heart Sound Sensor With an On-Board Executable
                  Lightweight Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7252--7266},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3547842},
	doi = {10.1109/TMC.2025.3547842},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangTTSLLQHSHSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heart sounds play a crucial role in assessing Coronary Artery Disease (CAD). The advancement of Artificial Intelligence (AI) technologies has given rise to Computer Audition (CA)-based methods for CAD detection. However, previous research has focused primarily on analyzing and modeling heart sound data, overlooking practical application scenarios. In this work, we design a pervasive heart sound collection device used for high-quality heart sound data acquisition. Moreover, we introduce an on-board executable lightweight network tailored for the designed portable device, referred to as TYKDModel. Further, heart sound data from 41 CAD patients and 22 non-CAD healthy controls are collected using the developed device. Experimental results show that the TYKDModel exhibits low-computational complexity, with 52.16 K parameters and 5.03 M Floating-Point Operations (FLOPs). When deployed on the board, it requires only 1.10 MB of Random Access Memory (RAM) and 236.27 KB of Read-Only Memory (ROM), and takes around 1.72 seconds to perform a classification. Despite the low computational and spatial complexity, the TYKDModel achieves a notable classification accuracy of 85.2%, specificity of 88.6%, and sensitivity of 82.8% on the board. These results indicate the promising potential of AI-assisted all-in-one integrated system for the diagnosis of heart sound-assisted CAD.}
}


@article{DBLP:journals/tmc/NabiM25,
	author = {Ahmadun Nabi and
                  Sangman Moh},
	title = {Joint Offloading Decision, User Association, and Resource Allocation
                  in Hierarchical Aerial Computing: Collaboration of UAVs and {HAP}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7267--7282},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548668},
	doi = {10.1109/TMC.2025.3548668},
	timestamp = {Sat, 09 Aug 2025 12:15:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NabiM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, applications are becoming increasingly computation-intensive and delay-sensitive owing to the rapid growth of Internet of Things (IoT) devices among ground users (GUs). Mobile edge computing (MEC) presents crucial computational support, but conventional MEC services often fail in remote areas and in disaster scenarios. This study presents a hierarchical aerial computing platform leveraging uncrewed aerial vehicles (UAVs) and high-altitude platform (HAP) to meet the computation demands and latency requirements of various IoT applications for GUs. We propose a joint offloading decision, user association, and resource allocation (JOUR) scheme, utilizing binary offloading from GUs to UAVs and partial offloading from UAVs to HAP. The proposed scheme minimizes the energy consumption and latency while maximizing the load balancing. A matching game-based algorithm addresses the GUs offloading decision and GUs-UAVs association, followed by an enhanced soft actor-critic (ESAC) algorithm for UAV partial offloading decision, UAV computation resource allocation, and HAP computation resource allocation. Our simulation results demonstrate the effectiveness of the JOUR scheme in reducing the energy consumption and latency, while improving the load balancing and task completion rates. This demonstrates its potential for optimizing the hierarchical aerial computing platforms in dynamic IoT environments.}
}


@article{DBLP:journals/tmc/ZhaoFDLWWZWX25,
	author = {Cui Zhao and
                  Guotong Fang and
                  Han Ding and
                  Xinhui Liu and
                  Fei Wang and
                  Ge Wang and
                  Kun Zhao and
                  Zhi Wang and
                  Wei Xi},
	title = {Federated Multi-Source Domain Adaptation for mmWave-Based Human Activity
                  Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7283--7296},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549705},
	doi = {10.1109/TMC.2025.3549705},
	timestamp = {Thu, 06 Nov 2025 14:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoFDLWWZWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contactless mmWave-based human activity recognition (HAR) is essential for various applications, yet most existing approaches often assume consistent environments. Integrating domain adaptation offers a promising solution to this challenge. This prevailing paradigm works well when the source and target data are centralized on a single server while learning to adapt. However, in more universal and practical situations, such as personal health records, users’ biometric information, and financial issues, the raw data is typically protected by different privacy-preserving policies and is stored by multiple parties. Additionally, labeling RF signals in the target domain is a non-trivial and labor-intensive task for most end-users. To address these problems, this paper introduces FMDA, a federated multi-source domain adaptation framework for mmWave-based HAR. FMDA assesses the contribution of each source and performs weighted parameter aggregation for knowledge transfer. This facilitates unsupervised training of the target HAR model without requiring access to any source domain data. Moreover, the model is optimized by minimizing the generalization gaps between the source and target models, benefiting all participants during the learning process and enhancing overall performance. Extensive experiments demonstrate the effectiveness of FMDA. The results indicate that in the target domain, FMDA achieves comparable performance to supervised learning approaches, while also enhancing the efficacy of source domain models to varying degrees.}
}


@article{DBLP:journals/tmc/HaoYLZWR25,
	author = {Yijun Hao and
                  Shusen Yang and
                  Fang Li and
                  Yifan Zhang and
                  Shibo Wang and
                  Xuebin Ren},
	title = {Learning Adaptive Multi-Timescale Scheduling for Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7297--7311},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548533},
	doi = {10.1109/TMC.2025.3548533},
	timestamp = {Wed, 13 Aug 2025 17:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HaoYLZWR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC), resource scheduling is crucial to task requests’ performance and service providers’ cost, involving multi-layer heterogeneous scheduling decisions. Existing MEC schedulers typically adopt static-timescale scheduling, where scheduling decisions are updated regularly at fixed intervals for all layers. The inflexible updating timescales lead to poor performance in the production networks. In this paper, we propose EdgeTimer, an unprecedented approach that automatically and adaptively determines respective updating timescales of multiple scheduling layers to achieve a better trade-off between the operation cost and service performance. Specifically, we design (i) a three-layer hierarchical deep reinforcement learning (DRL) framework for efficient learning of tightly coupled policies, (ii) a tailored multi-agent DRL algorithm for decentralized scheduling, with the convergence strictly proved, and (iii) a lightweight system defender for deterministic reliability assurance. Furthermore, we apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Through extensive trace-driven experiments, we demonstrate that EdgeTimer can significantly decrease the operation cost for service providers without sacrificing the delay performance, thereby improving overall profits, compared with the state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/XuQCZLMZZ25,
	author = {Ziheng Xu and
                  Wei Quan and
                  Nan Cheng and
                  Yuming Zhang and
                  Mingyuan Liu and
                  Xiaoting Ma and
                  Xue Zhang and
                  Hongke Zhang},
	title = {HarmonyPath: Fine-Grained Flexible Multipath Transmission for Mobile
                  Differentiated Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7312--7327},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549505},
	doi = {10.1109/TMC.2025.3549505},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuQCZLMZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in mobile application services has led to diversified traffic and increased demands on network resources. Traditional multipath algorithms, designed for resource integration through subflow scheduling across paths, struggle with disharmonious transmission caused by terminal mobility and differentiated path resources. Especially when differentiated services are transmitted concurrently, disharmonious transmission can give rise to resource contention, causing a large number of subflows to congest a single path and leading to performance degradation. To mitigate these challenges, this paper introduces HarmonyPath, a fine-grained flexible multipath transmission mechanism that can ensure harmonious resource occupation. Specifically, HarmonyPath firstly employs an in-band telemetry protocol to gather path resource information, generating a network resource distribution map. Based on this map, it flexibly allocates path resources according to the network resource distribution and service requirements. Then, HarmonyPath establishes a collaborative matching model for service demands and path resources. Through matrix transformation and calculation, it rapidly generates and deploys the scheduling strategy. To further alleviate service contention, HarmonyPath employs heuristic algorithms to optimize the scheduling strategy and achieve precise multipath transmission. Experiments demonstrate that HarmonyPath surpasses traditional algorithms in the multipath transmission of differentiated services, offering flexible service resource guarantees and enhancing network resource utilization efficiency.}
}


@article{DBLP:journals/tmc/GuoFZLDYCPR25,
	author = {Jialin Guo and
                  Yongjian Fu and
                  Zhiwei Zhai and
                  Xinyi Li and
                  Yongheng Deng and
                  Sheng Yue and
                  Lili Chen and
                  Hao Pan and
                  Ju Ren},
	title = {{MASA:} Multimodal Federated Learning Through Modality-Aware and Secure
                  Aggregation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7328--7344},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548954},
	doi = {10.1109/TMC.2025.3548954},
	timestamp = {Fri, 21 Nov 2025 10:51:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoFZLDYCPR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising paradigm, federated learning has been applied to multimodal sensing tasks due to its deployment convenience. However, the recent advances in multimodal federated learning emphasize learning a high-quality multimodal model but overlook the model usage requirements of massive unimodal clients. Moreover, the privacy risk in model sharing and client data heterogeneity impact the efficacy of federated learning. In this paper, we propose a novel multimodal federated learning system named MASA. As a departure from existing approaches, MASA simultaneously enhances the model learning efficiency of both multimodal and unimodal clients while ensuring their data privacy. First, we employ a gated cross-modal distillation scheme to achieve performance-aware knowledge transfer across modality-heterogeneous clients. To enhance the system security, MASA integrates a lightweight split-shuffle mechanism to realize the anonymization and encryption of model aggregation. Moreover, to reach personalized collaboration while protecting privacy, MASA features an attention-based spontaneous client clustering mechanism to form client cluster structures securely and distributedly. We evaluate our MASA on four public multimodal datasets for human activity recognition. The results show that our MASA outperforms leading multimodal federated learning methods on the model performance of both multimodal and unimodal clients.}
}


@article{DBLP:journals/tmc/ZengJWL25,
	author = {Rongfei Zeng and
                  Chenyang Jiang and
                  Xingwei Wang and
                  Baochun Li},
	title = {A Joint Secure Mechanism of Multi-Task Learning for a {UAV} Team Under
                  {FDI} Attacks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7345--7359},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551537},
	doi = {10.1109/TMC.2025.3551537},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZengJWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A UAV team shows tremendous potential for various mobile scenarios. However, some evidences reveal their vulnerability to False Data Injection (FDI) attacks, which can significantly jeopardize the flight security or even lead to catastrophic incidents. Existing studies primarily focus on detecting or defending against FDI attacks at the trajectory control of individual UAVs, leaving a gap in a comprehensive secure mechanism that can simultaneously detect, localize, and compensate for such attacks across an entire UAV team. The complexity of developing such a solution is magnified by the multiple design goals, the inherent sophistication of UAV team, and practical attack assumptions. In this paper, we propose a joint secure framework based on multi-task deep learning to simultaneously detect FDI attacks, localize the compromised components, and compensate control signals to mitigate the impact of FDI attacks on promising UAV teams. Specifically, we design an all-in-one deep learning model framework with a temporal-spatial information extraction module and a hierarchical multi-task module to perform three tasks simultaneously. Moreover, we introduce an iterative learning method with experience replay to counteract knowledge decay during model training. Extensive experiments and real flight demonstrations are presented to validate the improved performance and the benefits of our proposed secure method.}
}


@article{DBLP:journals/tmc/LiHYZ25,
	author = {Fang Li and
                  Yijun Hao and
                  Shusen Yang and
                  Peng Zhao},
	title = {OACR{\textdollar}{\^{}}\{2\}{\textdollar}2: Online Admission Control
                  and Resource Reservation for 5G Slice Networks With Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7360--7376},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548767},
	doi = {10.1109/TMC.2025.3548767},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiHYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing architecture is expected to fulfill network applications with heterogeneous requirements through efficient slice admission control (SAC) policies. Existing SAC approaches entirely rely on current limited observations to make admission decisions, ignoring the potential impact of future demands. The short-sighted behaviors lead to poor service performance and infrastructure providers’ (InPs’) revenue in practice. In this paper, we propose OACR 2 ^{2} , an online SAC approach based on deep reinforcement learning (DRL) that can exploit predictable future requests to make more precise admission control decisions for the long-term revenue, and reserve proper resources accordingly. Specifically, we design three novel schemes: (i) a requirement predictor based on long short-term memory (LSTM) and a novel input-output way to predict future unforeseen requests, (ii) a DRL admission controller based on the partially observable Markov decision process model to make precise admission decisions without accurate future request information, with the convergence strictly proved, and (iii) a decision defender to guarantee decision reliability. Extensive experiments on real-world traces demonstrate that compared to the No-wait, Wait-queue, and Wait-earliest time approaches, OACR 2 ^{2}  improves InPs’ revenue and acceptance ratio by up to 40.9% and 16.7%, respectively, without sacrificing online inference time (within 0.9 milliseconds).}
}


@article{DBLP:journals/tmc/ChiCNSSY25,
	author = {Xuejian Chi and
                  Honglong Chen and
                  Zhichen Ni and
                  Haiyang Sun and
                  Peng Sun and
                  Dongxiao Yu},
	title = {{H-STEP:} Heuristic Stable Edge Service Entity Placement for Mobile
                  Virtual Reality Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7377--7388},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3548703},
	doi = {10.1109/TMC.2025.3548703},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChiCNSSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual reality (VR) technology, as a latency-sensitive application, can achieve real-time response to enhance the user’s quality of experience (QoE) on edge devices. However, edge servers, unlike internally managed cloud servers, are prone to hardware failures, software abnormalities, and network attacks. Most prior studies have focused on reducing service delay and improving user coverage through service entity (SE) placement, often neglecting the critical impact of edge server malfunctions on user QoE. In this work, we design a stable service entity placement framework that connects users on faulty servers to collaborative edge servers, ensuring seamless task completion. This framework presents two primary challenges: determining the grouping of collaborative edge services and the placement of SEs. To address these challenges, we introduce a heuristic stable service entity placement (H-STEP) scheme. This scheme first determines the grouping of collaborative edge servers using an iterative search algorithm and then places SEs on suitable edge servers via a fast non-dominated sorting genetic placement algorithm. This approach balances stability benefits with total cost, enhancing the system’s economic benefits. We theoretically analyze the performance of H-STEP and derive the performance gap between H-STEP and the optimal scheme. Extensive real-data-driven simulations demonstrate that H-STEP’s performance closely approximates that of the optimal scheme and surpasses existing schemes.}
}


@article{DBLP:journals/tmc/LiuDWNK25,
	author = {Guangyuan Liu and
                  Hongyang Du and
                  Jiacheng Wang and
                  Dusit Niyato and
                  Dong In Kim},
	title = {Contract-Inspired Contest Theory for Controllable Image Generation
                  in Mobile Edge Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7389--7405},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550815},
	doi = {10.1109/TMC.2025.3550815},
	timestamp = {Sun, 09 Nov 2025 17:01:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuDWNK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of immersive technologies has propelled the development of the Metaverse, where the convergence of virtual and physical realities necessitates the generation of high-quality, photorealistic images to enhance user experience. However, generating these images, especially through Generative Diffusion Models (GDMs), in mobile edge computing environments presents significant challenges due to the limited computing resources of edge devices and the dynamic nature of wireless networks. This paper proposes a novel framework that integrates contract-inspired contest theory, Deep Reinforcement Learning (DRL), and GDMs to optimize image generation in these resource-constrained environments. The framework addresses the critical challenges of resource allocation and semantic data transmission quality by incentivizing edge devices to efficiently transmit high-quality semantic data, which is essential for creating realistic and immersive images. The use of contest and contract theory ensures that edge devices are motivated to allocate resources effectively, while DRL dynamically adjusts to network conditions, optimizing the overall image generation process. Experimental results demonstrate that the proposed approach not only improves the quality of generated images but also achieves superior convergence speed and stability compared to traditional methods. This makes the framework particularly effective for optimizing complex resource allocation tasks in mobile edge Metaverse applications, offering enhanced performance and efficiency in creating immersive virtual environments.}
}


@article{DBLP:journals/tmc/LiCWYYC25,
	author = {Qingyang Li and
                  Yuanjiang Cao and
                  Qianru Wang and
                  Lina Yao and
                  Zhiwen Yu and
                  Jiangtao Cui},
	title = {FedHMIR: Unified Framework for Federated Human-Machine Synergy in
                  Personalization-Generalization Balancing Identity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7406--7422},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549925},
	doi = {10.1109/TMC.2025.3549925},
	timestamp = {Thu, 28 Aug 2025 13:43:11 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiCWYYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As device-free identity recognition (IR) gains popularity and the demand for the Internet of Things (IoT) continues to grow, a new-era IR system featuring multiple distributed recognition devices and edge servers faces two main challenges: model adaptability and balancing the personalization of devices with the generalization of the system. This research introduces FedHMIR, a federated framework designed to simultaneously address these challenges by harmonizing human-machine collaboration with personalization-generalization trade-offs. The proposed framework features a human-machine cooperative online internal update mechanism, leveraging reinforcement learning to maintain the adaptability of personalized local IR models. To counter overfitting and enhance the generalization of the overall IR system, an external update process incorporating a confidence index is introduced. Additionally, the framework employs asynchronous internal and external update procedures to effectively balance personalization and generalization between local and global models. Finally, extensive experiments on three diverse real-world datasets demonstrate the effectiveness and advantages of FedHMIR compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/XiaCWLG25,
	author = {Junxu Xia and
                  Geyao Cheng and
                  Wenfei Wu and
                  Lailong Luo and
                  Deke Guo},
	title = {Exploring Communication-Efficient Federated Learning via Stateless
                  in-Network Aggregation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7423--7439},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551368},
	doi = {10.1109/TMC.2025.3551368},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaCWLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an ambitious training paradigm, federated learning has garnered increasing attention in recent years, which enables collaborative training of a global model without accessing users’ private data. However, due to the simultaneous and constant model updates gathering from massive distributed clients, the central server generally becomes a performance bottleneck. Additionally, the stateful aggregation (retaining all the updates from each client) conducted by the central server further poses potential threats to privacy, since it may recover the raw data based on such model updates inversely. The state-of-the-art methodologies, however, fail to address these two problems concurrently and efficiently. To this end, we propose GAIN, a secure aggregation acceleration service for federated learning. At its core, GAIN leverages programmable switches deployed at the edge network to aggregate model updates in a stateless manner before transmitting them to the central server. Consequently, GAIN can accelerate the transmission and aggregation of model updates while eliminating the chance of recovering private data. We evaluate the performance of GAIN through FPGA-based experiments and large-scale simulations. The results show that GAIN can effectively reduce bandwidth overhead and achieve up to 4.11× training throughput acceleration while prioritizing privacy protection.}
}


@article{DBLP:journals/tmc/ZhaoZZZS25,
	author = {Shuangrui Zhao and
                  Xinghui Zhu and
                  Yuanyu Zhang and
                  Zhiwei Zhang and
                  Yulong Shen},
	title = {Joint {RIS} and Beamforming Design for Secure and Energy-Efficient
                  Two-Way Relay Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7440--7457},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549445},
	doi = {10.1109/TMC.2025.3549445},
	timestamp = {Fri, 26 Sep 2025 12:58:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZZZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper examines the enhancement of secrecy energy efficiency (SEE) in a reconfigurable intelligent surface (RIS)-assisted two-way relay (TWR) system. We first establish a theoretical model for the system's secrecy rate, energy consumption, and SEE, and formulate the SEE maximization problem through the joint design of the RIS phase shifts and beamforming matrix. Using techniques such as weighted minimum mean square error (WMMSE), alternating optimization, and the augmented Lagrange method, we then develop a theoretical framework that identifies locally optimal solutions for the RIS and beamforming settings under unit-modulus and power constraints. The proposed framework is also shown to be applicable to solving the system's secrecy rate maximization problem. To address the computational complexity involved in optimizing the RIS phase shifts, we further propose a suboptimal scheme leveraging the Newton's method, which significantly reduces the computational burden while achieving performance close to the optimal SEE. Extensive numerical results validate the effectiveness of the proposed schemes, showing significant SEE improvements compared to traditional channel-capacity-based secure transmission scheme.}
}


@article{DBLP:journals/tmc/LiYYYLLQLLH25,
	author = {Zhenhua Li and
                  Ruoxuan Yang and
                  Xinlei Yang and
                  Jing Yang and
                  Xingyao Li and
                  Hao Lin and
                  Feng Qian and
                  Yunhao Liu and
                  Zhi Liao and
                  Daqiang Hu},
	title = {A Four-Year Retrospective of Mobile Access Bandwidth Evolution: The
                  Inspiring, the Frustrating, and the Fluctuating},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7458--7474},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551595},
	doi = {10.1109/TMC.2025.3551595},
	timestamp = {Wed, 10 Sep 2025 14:09:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiYYYLLQLLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in mobile technologies (like WiFi 6 and 5G) do not seem to deliver the promised access bandwidth. To effectively characterize mobile access bandwidth in the wild, we work with a major commercial mobile bandwidth testing app to conduct a long-term (2020-2023) and large-scale (involving 4.76M users) measurement study in China, based on coarse-grained general statistics and fine-grained sampling diagnostics. Our study presents distinct facts as to WiFi, 5G, and 4G: in the past few years, the average WiFi download bandwidth exhibits a considerable rise (by 119.7% ), the average 5G download bandwidth constantly decreases (by a total of 20.2% ) despite the enormous infrastructure investments, while the average 4G download bandwidth first declines (by 22.1% ) and then increases (by 22.5% ). The situations of upload bandwidths are generally similar to those of download bandwidths, except that 5G upload bandwidths manifest N-shaped  ( ↗ ↘ ↗ ) (\\nearrow \\searrow \\nearrow )  fluctuations. Our cross-layer and cross-technology analysis reveals a variety of impact factors as well as their complicated interplay as the root causes, such as the bottlenecks in underlying infrastructure (e.g., communication devices and wired Internet access), the traffic offloading from one access technology to another, the influence of the COVID-19 pandemic, and the side effects of aggressively migrating radio resources from 4G to 5G. With the longitudinal, holistic picture of today's mobile access bandwidth, we finally provide multifold practical implications on closing the technology gaps.}
}


@article{DBLP:journals/tmc/ZhangCYWL25,
	author = {Jixian Zhang and
                  Peng Chen and
                  Xuelin Yang and
                  Hao Wu and
                  Weidong Li},
	title = {An Optimal Reverse Affine Maximizer Auction Mechanism for Task Allocation
                  in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7475--7488},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549504},
	doi = {10.1109/TMC.2025.3549504},
	timestamp = {Tue, 05 Aug 2025 22:51:20 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCYWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing service (MCS) providers recruit users to complete data collection tasks with an incentive mechanism. How to maximize the utility of service providers has long been a popular topic in MCS research. Applying the existing reverse auction mechanism to an MCS may result in excessively high payments, thereby reducing the utility of the MCS provider. The affine maximizer auction (AMA) mechanism increases the revenue of service providers and meets dominant-strategy incentive-compatible (DSIC) characteristics. However, the AMA mechanism is a forward auction mechanism and cannot be applied to MCSs. Inspired by the AMA mechanism, this paper innovatively proposes a reverse affine maximizer auction (RAMA) mechanism to solve the task allocation problem of MCSs, effectively improving the MCS provider utility. Specifically, we construct a RAMA theoretical model and prove that the mechanism satisfies DSIC characteristics. For the discrete MCS task allocation problem, we use the reverse virtual valuation combinatorial auction (RVVCA) mechanism, a subclass of RAMA, to design a random mechanism RVVCA t ^{t}  and prove that the RVVCA t ^{t}  has a logarithmic approximate ratio. For the differentiable MCS task allocation problem, we use the deep learning transformer framework to design RAMANet, which can fit an exponential number of allocation solutions and output the optimal allocation and payment. We experimentally compare the algorithms of the RAMA family we propose, which use affine maximization, with existing state-of-the-art algorithms, demonstrating that the proposed algorithms significantly improve MCS provider utility.}
}


@article{DBLP:journals/tmc/DenipitiyageSGSMSC25,
	author = {Dishanika Denipitiyage and
                  Bhanuka Silva and
                  Kavishka Gunathilaka and
                  Suranga Seneviratne and
                  Anirban Mahanti and
                  Aruna Seneviratne and
                  Sanjay Chawla},
	title = {Detecting and Characterising Mobile App Metamorphosis in Google Play
                  Store},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7489--7504},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550121},
	doi = {10.1109/TMC.2025.3550121},
	timestamp = {Thu, 25 Dec 2025 12:46:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DenipitiyageSGSMSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {App markets have evolved into highly competitive and dynamic environments for developers. While the traditional app life cycle involves incremental updates for feature enhancements and issue resolution, some apps deviate from this norm by undergoing significant transformations in their use cases or market positioning. We define this previously unstudied phenomenon as ‘app metamorphosis'. In this paper, we propose a novel and efficient multi-modal search methodology to identify apps undergoing metamorphosis and apply it to analyse two snapshots of the Google Play Store taken five years apart. Our methodology uncovers various metamorphosis scenarios, including re-births, re-branding, re-purposing, and others, enabling comprehensive characterisation. Although these transformations may register as successful for app developers based on our defined success score metric (e.g., re-branded apps performing approximately 11.3% better than an average top app), we shed light on the concealed security and privacy risks that lurk within, potentially impacting even tech-savvy end-users.}
}


@article{DBLP:journals/tmc/YeLLH25,
	author = {Wentao Ye and
                  Yuan Luo and
                  Bo Liu and
                  Jianwei Huang},
	title = {Efficient and Cost-Effective Vehicle Recruitment for {HD} Map Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7505--7518},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552396},
	doi = {10.1109/TMC.2025.3552396},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YeLLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high-definition (HD) map is a cornerstone of autonomous driving. The crowdsourcing paradigm is a cost-effective way to keep an HD map up-to-date. Current HD map crowdsourcing mechanisms aim to enhance HD map freshness within recruitment budgets. However, many overlook unique and critical traits of crowdsourcing vehicles, such as random arrival and heterogeneity, leading to either compromised map freshness or excessive recruitment costs. Furthermore, these characteristics complicate the characterization of the feasible space of the optimal recruitment policy, necessitating a method to compute it efficiently in dynamic transportation scenarios. To overcome these challenges, we propose an efficient and cost-effective vehicle recruitment (ENTER) mechanism. Specifically, the ENTER mechanism has a threshold structure and balances freshness with recruitment costs while accounting for the vehicles’ random arrival and heterogeneity. It also integrates the bound-based relative value iteration (RVI) algorithm, which utilizes the threshold-type structure and upper bounds of thresholds to reduce the feasible space and expedite convergence. Numerical results show that the proposed ENTER mechanism increases the HD map company's payoff by 23.40 % \\%  and 43.91 % \\%  compared to state-of-the-art mechanisms that do not account for vehicle heterogeneity and random arrivals, respectively. Furthermore, the bound-based RVI algorithm in the ENTER mechanism reduces computation time by an average of 18.91% compared to the leading RVI-based algorithm.}
}


@article{DBLP:journals/tmc/PapaioannouKTPP25,
	author = {Savvas Papaioannou and
                  Panayiotis Kolios and
                  Theocharis Theocharides and
                  Christos G. Panayiotou and
                  Marios M. Polycarpou},
	title = {Jointly-Optimized Trajectory Generation and Camera Control for 3D
                  Coverage Planning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7519--7537},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551362},
	doi = {10.1109/TMC.2025.3551362},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PapaioannouKTPP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work proposes a jointly optimized trajectory generation and camera control approach, enabling an autonomous agent, such as an unmanned aerial vehicle (UAV) operating in 3D environments, to plan and execute coverage trajectories that maximally cover the surface area of a 3D object of interest. Specifically, the UAV's kinematic and camera control inputs are jointly optimized over a rolling planning horizon to achieve complete 3D coverage of the object. The proposed controller incorporates ray-tracing into the planning process to simulate the propagation of light rays, thereby determining the visible parts of the object through the UAV's camera. This integration enables the generation of precise look-ahead coverage trajectories. The coverage planning problem is formulated as a rolling finite-horizon optimal control problem and solved using mixed-integer programming techniques. Extensive real-world and synthetic experiments validate the performance of the proposed approach.}
}


@article{DBLP:journals/tmc/LiuLW25,
	author = {Yanting Liu and
                  Rui Lu and
                  Dan Wang},
	title = {LVMScissor: Split and Schedule Large Vision Model Inference on Mobile
                  Edges via Salp Swarm Algorithm},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7538--7553},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550519},
	doi = {10.1109/TMC.2025.3550519},
	timestamp = {Tue, 05 Aug 2025 22:51:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Computer Vision, the Large Vision Models (LVM) based on Vision Transformer (ViT) achieve advanced performance on general complex visual tasks. However, deploying these resource-intensive models on edge devices with limited computational power and memory is challenging, especially for those mobile edge devices. Existing model compression works downgrade the prediction accuracy and fail to adapt to dynamic network bandwidth or hardware changes. Besides, the split inference for typical Deep Neural Networks (DNN) is inefficient for LVM’s large intermediate result size. To address the computation and bandwidth limitation of edge devices of LVM, we design a new split inference acceleration LVMScissor by leveraging model parallelism and meta-heuristic algorithm. We first implement an inter-layer ViT parallelism strategy. After modeling the parallelized ViT split problem into a Multi-task three-processor scheduling (MTS) problem, we propose LVM-MSSA, a scheduling algorithm based on a famous meta-heuristic algorithm, the Multi-objective Salp Swarm Algorithm (MSSA) to schedule model parallelism and split strategy. The evaluation results show that compared to state-of-the-art inference acceleration approaches, our solution is faster from  1.6 × 1.6\\times  to  6.92 × 6.92\\times  under variant LVMs, edge devices, datasets, and network traces.}
}


@article{DBLP:journals/tmc/GeLGHZC25,
	author = {Rui Ge and
                  Huanghuang Liang and
                  Zheng Gong and
                  Chuang Hu and
                  Xiaobo Zhou and
                  Dazhao Cheng},
	title = {Streamlining Data Transfer in Collaborative {SLAM} Through Bandwidth-Aware
                  Map Distillation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7554--7567},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549367},
	doi = {10.1109/TMC.2025.3549367},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GeLGHZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge intelligence offers a promising solution for Simultaneous Localization and Mapping (SLAM) in large-scale scenarios, where multiple robots collaboratively perceive the environment and upload their local maps to an edge server. However, maintaining mapping accuracy under constrained and dynamic communication resources remains a significant challenge for the practical deployment of robot swarms. Concurrent data uploads from multiple agents can exacerbate network congestion, leading to the loss of critical information, delayed updates, and, ultimately, the inconsistency of the generated maps. This paper presents Hermes, an edge-assisted collaborative mapping system designed for communication-constrained environments. Hermes streamlines data transfer through bandwidth-aware map distillation, ensuring only the most crucial messages are transmitted to the edge server. We quantify the importance of keyframes and landmarks based on their information entropy gain in pose estimation. By selectively sharing essential submaps, Hermes adaptively balances communication bandwidth and information richness during the mapping process. We implemented Hermes on heterogeneous platforms and conducted experiments using public datasets and self-collected campus data. Hermes exceeds SwarmMap by 50% in bandwidth utilization with similar accuracy and surpasses COVINS-G by 65% in trajectory error under highly constrained network resources.}
}


@article{DBLP:journals/tmc/CaiCZLTXCL25,
	author = {Qixuan Cai and
                  Ruikai Chu and
                  Kaixuan Zhang and
                  Xiulong Liu and
                  Xinyu Tong and
                  Xin Xie and
                  Jiancheng Chen and
                  Keqiu Li},
	title = {{AMRE:} Adaptive Multilevel Redundancy Elimination for Multimodal
                  Mobile Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7568--7583},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549422},
	doi = {10.1109/TMC.2025.3549422},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaiCZLTXCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given privacy and network load concerns, employing on-device multimodal neural networks (MNNs) for IoT data is a growing trend. However, the high computational demands of MNNs clash with limited on-device resources. MNNs involve input and model redundancies during inference, wasting resources to process redundant input components and run excess model parameters. Model Redundancy Elimination (MRE) reduces redundant parameters but cannot bypass inference for unnecessary input components. Input Redundancy Elimination (IRE) skips inference for redundant input components but cannot reduce computation for the remaining parts. MRE and IRE independently fail to meet the diverse computational needs of multimodal inference. To address these issues, we aim to combine the advantages of MRE and IRE to achieve a more efficient inference. We propose an adaptive multilevel redundancy elimination framework (AMRE), which supports both IRE and MRE. AMRE first establishes a collaborative inference mechanism for IRE and MRE. We then propose a multifunctional, lightweight policy model that adaptively controls the inference logic for each instance. Moreover, a three-stage training method is proposed to ensure the performance of collaborative inference in AMRE. We validate AMRE in three scenarios, achieving up to 52.91% lower latency, 56.79% lower energy cost, and a slight accuracy gain compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/ZengQL25,
	author = {Zhiyong Zeng and
                  Meng Qin and
                  Dandan Liang},
	title = {A Cybertwin-Enabled Multipath Transmission Scheme in Cloud Native
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7584--7599},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550129},
	doi = {10.1109/TMC.2025.3550129},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZengQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To address issues arising from the independence of the radio access network, the IP bearer network, and the data center network, a Cybertwin-based cloud native network (CCNN) is proposed. However, due to diverse application requirements and user demands, providing individualized transmission services that meet these requirements within CCNN remains a major challenge. This paper proposes a Cybertwin-enabled multipath transmission scheme (CMTS) to provide personalized quality-of-service (QoS)-guaranteed transmission services by dynamically creating network paths tailored to users’ demands. Specifically, by introducing Cybertwin, CMTS allows separate and dynamic scheduling of service and access network resources from different providers, decouples users from network providers, and enables more flexible use of heterogeneous network resources. Moreover, CMTS can make resource scheduling decisions on demand by leveraging the user’s personal information held by Cybertwin. To fully exploit the potential of multiple paths, we formulate network resource planning as an optimization problem aimed at minimizing link differences and propose a heuristic method for determining optimal policies. Finally, we present the first implementation of Cybertwin in CCNN, validate CMTS in both emulated and semi-physical environments, and conduct thorough evaluations with benchmarks. The results show that CMTS can deliver a personalized, QoS-guaranteed transmission service while achieving efficient resource utilization.}
}


@article{DBLP:journals/tmc/LiuYZX25,
	author = {Shuai Liu and
                  Helin Yang and
                  Mengting Zheng and
                  Liang Xiao},
	title = {Multi-UAV-Assisted {MEC} in Internet of Vehicles With Combined Multi-Modal
                  Semantic Communication Under Jamming Attacks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7600--7614},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550965},
	doi = {10.1109/TMC.2025.3550965},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuYZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication technology, which transmits only relevant semantic information, can significantly conserve communication resources and reduce service time. This technology is particularly promising for unpilotedaerial vehicle (UAV)-assisted mobile edge computing (MEC) in the internet of vehicles (IoV). However, integrating semantic communication with UAV-assisted vehicle MEC is susceptible to malicious jamming. This paper introduces a reliable communication method that combines multi-modal semantic communication with UAV-assisted vehicle MEC to minimize delays in communication and computation while maintaining semantic accuracy during jamming attacks. Our approach optimizes UAV trajectories, user associations, and channel selections, enabling the UAV to select optimal positions when associating with different modal users and reducing the impact of jammers during multi-modal task reception. Due to the non-convex nature of the optimization problem and the highly dynamic environment, we employ the semantic communication combined with the multi-agent twin delayed deep deterministic policy gradient (SC-MA-TD3) approach, a multi-agent deep reinforcement learning (DRL) strategy that fosters UAV cooperation for efficient resource allocation. Simulation results show that our approach outperforms existing approaches in reducing delays and enhancing semantic accuracy.}
}


@article{DBLP:journals/tmc/LiuGLWLFXMLY25,
	author = {Sicong Liu and
                  Bin Guo and
                  Shiyan Luo and
                  Yuzhan Wang and
                  Hao Luo and
                  Cheng Fang and
                  Yuan Xu and
                  Ke Ma and
                  Yao Li and
                  Zhiwen Yu},
	title = {CrowdHMTware: {A} Cross-Level Co-Adaptation Middleware for Context-Aware
                  Mobile {DL} Deployment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7615--7631},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549399},
	doi = {10.1109/TMC.2025.3549399},
	timestamp = {Sat, 23 Aug 2025 18:55:15 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuGLWLFXMLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There are many deep learning (DL) powered mobile and wearable applications today continuously and unobtrusively sensing the ambient surroundings to enhance all aspects of human lives. To enable robust and private mobile sensing, DL models are often deployed locally on resource-constrained mobile devices using techniques such as model compression or offloading. However, existing methods, either front-end algorithm level (i.e. DL model compression/partitioning) or back-end scheduling level (i.e. operator/resource scheduling), cannot be locally online because they require offline retraining to ensure accuracy or rely on manually pre-defined strategies, struggle with dynamic adaptability. The primary challenge lies in feeding back runtime performance from the back-end level to the front-end level optimization decision. Moreover, the adaptive mobile DL model porting middleware with cross-level co-adaptation is less explored, particularly in mobile environments with diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic context-adaptive DL model deployment middleware for heterogeneous mobile devices. It establishes an automated adaptation loop between cross-level functional components, i.e. elastic inference, scalable offloading, and model-adaptive engine, enhancing scalability and adaptability. Experiments with four typical tasks across 15 platforms and a real-world case study demonstrate that  C r o w d H M T w a r e {\\sf CrowdHMTware}  can effectively scale DL model, offloading, and engine actions across diverse platforms and tasks. It hides run-time system issues from developers, reducing the required developer expertise.}
}


@article{DBLP:journals/tmc/WangXYSQ25,
	author = {Shuai Wang and
                  Yanqing Xu and
                  Chaoqun You and
                  Mingjie Shao and
                  Tony Q. S. Quek},
	title = {Communication-Efficient Federated Learning by Quantized Variance Reduction
                  for Heterogeneous Wireless Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7632--7647},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551759},
	doi = {10.1109/TMC.2025.3551759},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangXYSQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has been recognized as a viable solution for local-privacy-aware collaborative model training in wireless edge networks, but its practical deployment is hindered by the high communication overhead caused by frequent and costly server-device synchronization. Notably, most existing communication-efficient FL algorithms fail to reduce the significant inter-device variance resulting from the prevalent issue of device heterogeneity. This variance severely decelerates algorithm convergence, increasing communication overhead and making it more challenging to achieve a well-performed model. In this paper, we propose a novel communication-efficient FL algorithm, named FedQVR, which relies on a sophisticated variance-reduced scheme to achieve heterogeneity-robustness in the presence of quantized transmission and heterogeneous local updates among active edge devices. Comprehensive theoretical analysis justifies that FedQVR is inherently resilient to device heterogeneity and has a comparable convergence rate even with a small number of quantization bits, yielding significant communication savings. Besides, considering non-ideal wireless channels, we propose FedQVR-E which enhances the convergence of FedQVR by performing joint allocation of bandwidth and quantization bits across devices under constrained transmission delays. Extensive experimental results are also presented to demonstrate the superior performance of the proposed algorithms over their counterparts in terms of both communication efficiency and application performance.}
}


@article{DBLP:journals/tmc/ZhangMZSYJCH25,
	author = {Guoming Zhang and
                  Xiaohui Ma and
                  Huiting Zhang and
                  Riccardo Spolaor and
                  Yanni Yang and
                  Xiaoyu Ji and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {UltraAdv: An Ultrasonic Adversarial Attack on Closed-Box Speech Recognition
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7648--7662},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555680},
	doi = {10.1109/TMC.2025.3555680},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangMZSYJCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attacks on speech recognition systems often use adversarial or inaudible commands. However, a challenge is that adversarial perturbations typically fall within the audible frequency range, making it difficult to achieve inaudibility. Additionally, the non-linear effects of loudspeakers often cause inaudible commands to become audible at higher power levels. Therefore, minimizing the power requirements of the attack is essential to maintain inaudibility. Another significant obstacle is the conversion of variable-length commands, especially longer ones, into shorter target commands. In this paper, we present UltraAdv, a method for generating long-range adversarial perturbations capable of compromising commands of arbitrary length in closed-box setting. By combining the ultrasonic signal with the normal one, rather than negating it as in DolphinAttack, we significantly improve the energy efficiency, thus enhancing its attack distance. We also propose a dynamically adjustable suppression-interference method based on automatic gain control to address the challenge of mismatched durations between long commands and target commands (length-independent). Experiments demonstrate that using a single perturbation, we achieve impressive success rates of 98.84% and 96.62% and 98.32% across a diverse set of 12,260 speeches on DeepSpeech, iFlytek, and Whisper. The attack range reaches up to 15 m, surpassing DolphinAttack's 5 m range at equivalent power.}
}


@article{DBLP:journals/tmc/LuoZSFXLY25,
	author = {Shouxi Luo and
                  Peidong Zhang and
                  Xin Song and
                  Pingzhi Fan and
                  Huanlai Xing and
                  Long Luo and
                  Hongfang Yu},
	title = {Domain-Specific Transport Protocols for In-Network Processing at the
                  Edge: {A} Case Study of Accelerating Model Synchronization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7663--7679},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552220},
	doi = {10.1109/TMC.2025.3552220},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoZSFXLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, cross-device federated learning (FL) is the key to achieving personalization services for mobile users and has been widely employed by companies like Google, Microsoft, and Alibaba in production. With the explosive growth in the number of participants, the central FL server, which acts as the manager and aggregator of cross-device model training, would get overloaded, becoming the system bottlenecks. Inspired by the emerging wave of edge computing, an interesting question arises: Could edge clouds help cross-device FL systems overcome the bottleneck? This article provides a cautiously optimistic answer by proposing INP, a FL-specific In-Network Processing framework to achieve the goal. As in-network processing has broken the end-to-end principle of the involved communication and lacks the support of transport protocols, the key is to design domain-specific transport protocols for INP. To fill the gap, we propose the novel Model Download Protocol of mdp and Model Upload Protocol of mup. With mdp and mup, edge cloud nodes along the paths in INP can easily eliminate duplicated model downloads and pre-aggregate associated gradient uploads for the central FL server, thus alleviating its bottleneck effect, and further accelerating the entire training progress significantly.}
}


@article{DBLP:journals/tmc/WuBHWLHC25,
	author = {Yuan Wu and
                  Shoudu Bai and
                  Qingyong Hu and
                  Bo Wang and
                  Min Li and
                  Xinrong Hu and
                  Yanjiao Chen},
	title = {Ubicon-BP: Towards Ubiquitous, Contactless Blood Pressure Detection
                  Using Smartphone},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7680--7692},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551315},
	doi = {10.1109/TMC.2025.3551315},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuBHWLHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blood pressure (BP) is a critical physiological parameter closely associated with severe diseases such as heart failure and kidney damage. Current methods either require additional or dedicated hardware, or closing touching to the devices, causing discomfort and inconvenience. Therefore, a convenient, contactless BP measurement solution is highly desired. In this work, we present Ubicon-BP, a ubiquitous, device-free, and contactless BP detection application. Ubicon-BP calculates BP based on the pulse transit time (PTT), a key feature that is medically proven correlated with BP. However, using smartphone sensors to contactless calculate PTT is non-trivial since it requires a micro-second level precision for cardiac event detection. To address this issue, we propose leveraging the acoustic sensors in smartphone to detect vibrations caused by heart valve movements, as well as camera sensors to measure finger pulses. To accurately measure heartbeat signal that are susceptible to motion, we first improve the sensing granularity of acoustic signals and then introduce the IQ-MVED model to eliminate motion interference. Furthermore, when recovering pulse signals from video signals, issues such as poor generalization performance arise. Consequently, we propose the TS-CAN and meta-learning models to obtain personalized pulse signals. Finally, we transform the extracted time-frequency features from the recovered heartbeats and pulse signals to the corresponding BP. Comprehensive testing involving 50 subjects reveal a standard deviation error of  4.27 mmHg  4.27 \\; \\text{mmHg}  for diastolic pressure and  6.36 mmHg  6.36 \\; \\text{mmHg}  for systolic pressure, respectively.}
}


@article{DBLP:journals/tmc/LiSLSCZYCH25,
	author = {Wenhao Li and
                  Riccardo Spolaor and
                  Chuanwen Luo and
                  Yuchao Sun and
                  Huashan Chen and
                  Guoming Zhang and
                  Yanni Yang and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {Acoustic Eavesdropping From Sound-Induced Vibrations With Multi-Antenna
                  mmWave Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7693--7708},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551317},
	doi = {10.1109/TMC.2025.3551317},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiSLSCZYCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic eavesdropping against private or confidential spaces is a significant threat in the realm of privacy protection. While the presence of soundproof material would weaken such an attack, current eavesdropping technology may be able to bypass these protections. Fortunately, existing studies either inadequately cover the full spectrum of human speech due to low-frequency responses or rely heavily on the prior knowledge used to train a model. To address these challenges, this paper introduces mmEcho, a new acoustic eavesdropping method that utilizes millimeter-wave signals to sense vibration induced by sound precisely. Through signal processing techniques such as the intra-chirp scheme and phase calibration algorithm, mmEcho achieves micrometer-level vibration extraction without requiring target-related data. To improve the range of eavesdropping attacks while reducing noise, we optimize radar signals by leveraging the widespread availability of multiple antennas on commercial off-the-shelf radars. We comprehensively evaluate the performance of mmEcho in different real-world settings. Experimental results demonstrate that, with the aid of multi-antenna technology, mmEcho can more effectively reconstruct the audio from the target at various distances, directions, sound insulators, reverberating objects, sound levels, and languages. Compared to existing methods, our approach provides better effectiveness without prior knowledge, such as the speech data from the target.}
}


@article{DBLP:journals/tmc/FuTDLZ25,
	author = {Haoyu Fu and
                  Fengsen Tian and
                  Guoqiang Deng and
                  Lingyu Liang and
                  Xinglin Zhang},
	title = {Reads: {A} Personalized Federated Learning Framework With Fine-Grained
                  Layer Aggregation and Decentralized Clustering},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7709--7725},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552982},
	doi = {10.1109/TMC.2025.3552982},
	timestamp = {Tue, 20 Jan 2026 14:46:52 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FuTDLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The heterogeneity of local data and client performance, along with real-world system risks, is driving the evolution of federated learning (FL) towards personalized, model-heterogeneous, and decentralized approaches. However, due to the differing structures of heterogeneous models, it is hard to use them to identify clients with similar data distributions and further enhance the personalization of local models. Therefore, how to deal with data heterogeneity to obtain superior personalized local models for clients, while simultaneously addressing model heterogeneity and system risks is a challenging problem. In this paper, we propose a novel personalized FL framework with fine-gRained layEr aggregAtion and Decentralized cluStering ( R e a d s {\\sf Reads} ), which integrates four key components: (1) deep mutual learning with privacy guarantee for model training and privacy preservation, (2) fine-grained layer similarity computation among heterogeneous model layers, (3) fully decentralized clustering for soft clustering of clients based on layer similarities, and (4) personalized layer aggregation for capturing common knowledge from other clients. Through  R e a d s {\\sf Reads} , clients obtain personalized models that accommodate model heterogeneity, while the system ensures robustness against a single point of failure. Extensive experiments demonstrate the efficacy of  R e a d s {\\sf Reads}  in achieving these goals.}
}


@article{DBLP:journals/tmc/GuoWZZCLYZ25,
	author = {Haotian Guo and
                  Feng Wang and
                  Wei Zhang and
                  Yifei Zhu and
                  Laizhong Cui and
                  Jiangchuan Liu and
                  Fei Richard Yu and
                  Lei Zhang},
	title = {Joint Adaptation for Mobile 360-Degree Video Streaming and Enhancement},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7726--7741},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555322},
	doi = {10.1109/TMC.2025.3555322},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoWZZCLYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tile-based streaming and super resolution (SR) are two representative technologies adopted to improve bandwidth efficiency of 360° video streaming. The former allows selective downloading of contents in the user viewport by splitting the video into multiple independently decodable tiles. The latter leverages client-side computation to enhance the received video to higher quality using advanced neural network models. In this work, we propose a Collaborated Streaming and Enhancement (CSE) adaptation framework for mobile 360° videos, which integrates super resolution with tile-based streaming to optimize the user experience with dynamic bandwidth and limited computing capability. To effectively enhance the tile-based video streaming through SR, we propose to adaptively group the tiles for quality enhancement adapting to the content similarity. We also identify and address several key design issues to integrate SR into tile-based video streaming including unified video quality assessment, computational complexity model for super resolution, and buffer analysis considering the interplay between transmission and enhancement. We further formulate the quality-of-experience (QoE) maximization problem for mobile 360° video streaming and propose a rate adaptation algorithm to make the best decisions for download and for enhancement based on the Lyapunov optimization theory. Extensive evaluation results validate the superiority of our proposed approach, which demonstrates stable performance with considerable QoE improvement, while enabling a trade-off between playback smoothness and video quality.}
}


@article{DBLP:journals/tmc/ZhanDDHLCH25,
	author = {Zijun Zhan and
                  Yaxian Dong and
                  Daniel Mawunyo Doe and
                  Yuqing Hu and
                  Shuai Li and
                  Shaohua Cao and
                  Zhu Han},
	title = {Vision Language Model-Empowered Contract Theory for {AIGC} Task Allocation
                  in Teleoperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7742--7756},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551597},
	doi = {10.1109/TMC.2025.3551597},
	timestamp = {Wed, 10 Dec 2025 08:08:42 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanDDHLCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating low-light image enhancement techniques, in which diffusion-based AI-generated content (AIGC) models are promising, is necessary to enhance nighttime teleoperation. Remarkably, the AIGC model is computation-intensive, thus necessitating the allocation of AIGC tasks to edge servers with ample computational resources. Given the distinct cost of the AIGC model trained with varying-sized datasets and AIGC tasks possessing disparate demand, it is imperative to formulate a differential pricing strategy to optimize the utility of teleoperators and edge servers concurrently. Nonetheless, the pricing strategy formulation is under information asymmetry, i.e., the demand (e.g., the difficulty level of AIGC tasks and their distribution) of AIGC tasks is hidden information to edge servers. Additionally, manually assessing the difficulty level of AIGC tasks is tedious and unnecessary for teleoperators. To this end, we devise a framework of AIGC task allocation assisted by the Vision Language Model (VLM)-empowered contract theory, which includes two components: VLM-empowered difficulty assessment and contract theory-assisted AIGC task allocation. The first component enables automatic and accurate AIGC task difficulty assessment. The second component is capable of formulating the pricing strategy for edge servers under information asymmetry, thereby optimizing the utility of both edge servers and teleoperators. The simulation results demonstrated that our proposed framework can improve the average utility of teleoperators and edge servers by  10.88 ∼ 12.43 % 10.88 \\sim 12.43\\%  and  1.4 ∼ 2.17 % 1.4\\! \\sim \\!2.17\\% , respectively.}
}


@article{DBLP:journals/tmc/PerezFGMFW25,
	author = {Pablo Fern{\'{a}}ndez P{\'{e}}rez and
                  Claudio Fiandrino and
                  Eloy P{\'{e}}rez G{\'{o}}mez and
                  Hossein Mohammadalizadeh and
                  Marco Fiore and
                  Joerg Widmer},
	title = {AIChronoLens: {AI/ML} Explainability for Time Series Forecasting in
                  Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7757--7772},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3554035},
	doi = {10.1109/TMC.2025.3554035},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PerezFGMFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting is increasingly considered a fundamental enabler for the management of next-generation mobile networks. While deep neural networks excel at short- and long-term forecasting, their complexity hinders interpretability, a crucial factor for production deployment. The existing EXplainable Artificial Intelligence (XAI) techniques, primarily designed for computer vision and natural language processing, struggle with time series data due to their lack of understanding of temporal characteristics of the input data. In this paper, we take the research on EXplainable Artificial Intelligence (XAI) for time series forecasting one step further by proposing AIChronoLens, a new tool that links legacy XAI explanations with the temporal properties of the input. AIChronoLens allows diving deep into the behavior of time series predictors and spotting, among other aspects, the hidden causes of forecast errors. We show that AIChronoLens’s output can be utilized for meta-learning to predict when the original time series forecasting model makes errors and fix them in advance, thereby improving the accuracy of the predictors. Extensive evaluations with real-world mobile traffic traces pinpoint model behaviors that would not be possible to identify otherwise and show how model performance can be improved by 32 % upon re-training and by up to 39 % with meta-learning.}
}


@article{DBLP:journals/tmc/ZhangXLC25,
	author = {Jie Zhang and
                  Jianqiang Xue and
                  Yanjiao Li and
                  Simon L. Cotton},
	title = {Leveraging Online Learning for Domain-Adaptation in Wi-Fi-Based Device-Free
                  Localization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7773--7787},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552538},
	doi = {10.1109/TMC.2025.3552538},
	timestamp = {Sat, 09 Aug 2025 12:15:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangXLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi-based device-free localization (DFL) will be an integral part of many emerging applications, such as smart healthcare and smart homes. One popular approach to DFL in Wi-Fi makes use of fingerprinting based on channel state information (CSI). Unfortunately, high-quality fingerprints cannot easily be obtained in many real-world environments due to the complicated, time-varying and multipath conditions which exist. Additionally, existing methods struggle to update the DFL models in a real-time manner to track changes of environment. To address these issues, an online data-driven modelling DFL framework is designed for robustness enhancement. Specifically, the raw CSI data is first augmented with the hidden layer parameters of an online deep neural network to strengthen the pair-to-pair mappings between signal variations and a target’s location. The radio map created with the augmented fingerprints can be updated with new sequential data collected from other domains, such as different times and layouts of the same environment. Subsequently, a novel online DFL model is established using these augmented fingerprints, which itself can be updated with new sequential data from other domains without the need for retraining. A forgetting mechanism is considered to mitigate the effects of outdated data on the localization performance. To validate our new framework, a comprehensive set of experiments have been performed in various environments for different scenarios. The experimental results verify the robustness and responsive tracking ability of the proposed online data-driven modelling DFL framework.}
}


@article{DBLP:journals/tmc/DouZLS25,
	author = {Jizhe Dou and
                  Haotian Zhang and
                  Yang Luo and
                  Guodong Sun},
	title = {Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {8},
	pages = {7788--7804},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551386},
	doi = {10.1109/TMC.2025.3551386},
	timestamp = {Tue, 05 Aug 2025 22:51:19 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DouZLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, there has been a growing interest in using chargers to extend the operational longevity of UAVs (drones). In this paper, we explore a charger-assisted drone application where a drone observes points of interest while a mobile charger moves to recharge its battery. We focus on the route and charging schedule of the drone and mobile charger to maximize observation utility in the shortest possible time, while ensuring continuous drone operation. In our problem, the drone and mobile charger cooperate to complete a task. Their discrete-continuous hybrid actions pose a major computational challenge. To address this issue, we present a hybrid-action deep reinforcement learning framework, called HaDMC, which uses a typical policy learning algorithm to generate latent continuous actions. We specifically design and train an action decoder. It involves two pipelines to convert the latent continuous actions into the original hybrid actions for the drone and mobile charger to directly interact with environment. We incorporate a mutual learning scheme into model training, emphasizing collaboration over individual actions. By extensive numerical experiments, we evaluate HaDMC and compare it with state-of-the-art approaches. The experimental results demonstrate the effectiveness and efficiency of our solution.}
}


@article{DBLP:journals/tmc/WeiLLJFWZ25,
	author = {Zhiqing Wei and
                  Haotian Liu and
                  Hujun Li and
                  Wangjun Jiang and
                  Zhiyong Feng and
                  Huici Wu and
                  Ping Zhang},
	title = {Integrated Sensing and Communication Enabled Cooperative Passive Sensing
                  Using Mobile Communication System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7805--7821},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3514113},
	doi = {10.1109/TMC.2024.3514113},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiLLJFWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated sensing and communication (ISAC) is a potential technology of the sixth-generation (6G) mobile communication system, which enables communication base station (BS) with sensing capability. However, the performance of single-BS sensing is limited, which can be overcome by multi-BS cooperative sensing. There are three types of multi-BS cooperative sensing, including cooperative active sensing, cooperative passive sensing, and cooperative active and passive sensing, where the multi-BS cooperative passive sensing has the advantages of low hardware modification cost and large sensing coverage. However, multi-BS cooperative passive sensing faces the challenges of synchronization offset mitigation and sensing information fusion. To address these challenges, a non-line of sight (NLoS) and line of sight (LoS) signal cross-correlation (NLCC) method is proposed to mitigate carrier frequency offset (CFO) and time offset (TO). Besides, a symbol-level fusion method of multi-BS sensing information is proposed. The discrete samplings of echo signals from multiple BSs are matched independently and coherently accumulated to improve sensing accuracy. Moreover, a low-complexity joint angle-of-arrival (AoA) and angle-of-departure (AoD) estimation method is proposed to reduce the computational complexity. Simulation results show that symbol-level multi-BS cooperative passive sensing scheme has an order of magnitude higher sensing accuracy than single-BS passive sensing. This work provides a reference for the research on multi-BS cooperative passive sensing.}
}


@article{DBLP:journals/tmc/PengCLYFBSLRG25,
	author = {Jinbo Peng and
                  Zhe Chen and
                  Zheng Lin and
                  Haoxuan Yuan and
                  Zihan Fang and
                  Lingzhong Bao and
                  Zihang Song and
                  Ying Li and
                  Jing Ren and
                  Yue Gao},
	title = {Sums: Sniffing Unknown Multiband Signals Under Low Sampling Rates},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7822--7835},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2024.3509861},
	doi = {10.1109/TMC.2024.3509861},
	timestamp = {Sat, 27 Sep 2025 08:12:39 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengCLYFBSLRG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz). Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency. However, to achieve a multiband signals sniffer is really a challenge. To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner. Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework. The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate. Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals. Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-the-art baselines in spectrum sensing, modulation classification, and demodulation. As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice.}
}


@article{DBLP:journals/tmc/HuangZPSYP25,
	author = {Chenpei Huang and
                  Hui Zhong and
                  Pavana Prakash and
                  Dian Shi and
                  Xu Yuan and
                  Miao Pan},
	title = {Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice
                  Assistant},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7836--7850},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3530962},
	doi = {10.1109/TMC.2025.3530962},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZPSYP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in speech and language processing have led to the rise of smart voice services like Alexa, Google Home, and Siri. However, these advancements also increase security risks due to sophisticated voice domain attacks. Instead of relying on acoustic clues to detect replayed or synthesized speech, we utilize microphones and motion sensors in head-wearable devices to authorize legitimate users through bone-conducted vibrations, enabling multi-factor authentication (MFA) for spoken voice. Our proposed two-stage authentication system, AirBone, captures air and bone conduction (AirBone) signals and exploits two authentication factors sequentially. The first stage, called temporal consistency scoring (TCS), employs signal processing to verify the recorded AC and BC signals are concurrent and originate from the same vocalization process. Statistical tools are employed to distinguish legitimate attempts against false-triggering or acoustic attacks. The second stage leverages deep learning to verify the user’s unique bone conduction patterns in the vibration domain. Specifically, we enhance the robustness through data augmentation with constant-Q transform and adversarial training, improving the model’s ability to detect impersonation and machine-induced vibrations. Thanks to these designs, AirBone authentication offers enhanced security via MFA with no extra cost of user effort. In addition, our experimental results demonstrate a  96.3 % 96.3\\%  overall accuracy, robustness against AirBone noise and room impulse responses, and  0.3 % 0.3\\%  Equal Error Rate (EER) against acoustic and cross-domain attacks.}
}


@article{DBLP:journals/tmc/LiuLTXCYL25,
	author = {Zijuan Liu and
                  Xiulong Liu and
                  Xinyu Tong and
                  Xin Xie and
                  Jiancheng Chen and
                  Ming Yang and
                  Keqiu Li},
	title = {MLiquID: Towards Mobile Liquid Sensing With {COTS} RFIDs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7851--7865},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3536842},
	doi = {10.1109/TMC.2025.3536842},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLTXCYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Liquid sensing in ubiquitous contexts plays an essential role in various scenarios. Recently, some wireless sensing systems have been proposed for liquid identification. However, existing works usually require specific equipment or capture the signals penetrating a target, limiting the deployability of liquid sensing. In large-scale scenarios, multiple devices are usually required to expand the coverage area due to the RFID reader antenna's reading range limitation. To enlarge the sensing range and make the liquid sensing method can be adopted in real moving scenarios, in this paper, we present Mobile Liquid IDentification (MLiquID), a liquid sensing system that can recognize the type of liquid in a mobile manner with commercial off-the-shelf (COTS) RFID devices. This mobile process leads to continuous variation in location, so the major challenge in this paper is how to extract signal features from the superimposed information of movement and material. The key insight is to regard movement as an opportunity to acquire data from different perspectives instead of a challenge to hinder feature extraction. We construct a Phase-RSS model by analyzing the influence of moving and liquid on the phase and RSS signals. First, we propose a method to calculate the distance from the tag to the reader antenna. Second, we explore an identification method to identify liquid type by extracting signal features Phase-RSS coefficient  C P − R C_{P-R}  and Maximum Response Distance (MRD). Experimental results demonstrate an average accuracy of 96.80% in identifying 10 common liquids, which shows the great potential of MLiquID for mobile liquid sensing.}
}


@article{DBLP:journals/tmc/GaoMYZGFG25,
	author = {Yucen Gao and
                  Li Ma and
                  Zhemeng Yu and
                  Songjian Zhang and
                  Hui Gao and
                  Jun Fang and
                  Xiaofeng Gao},
	title = {A Lightweight Encoder-Decoder Framework for Carpooling Route Planning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7866--7879},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549757},
	doi = {10.1109/TMC.2025.3549757},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoMYZGFG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Carpooling Route Planning (CRP) has become an important issue with the growth of low-carbon traffic systems. We investigate a novel, meaningful and challenging scenario for CRP in industry, called Multi-Candidate Carpooling Route Planning (MCRP) problem, where each passenger may have several potential positions to get on and off the car. We surprisingly notice that this problem can be easily generalized for similar services such as express, takeout, or crowdsensing services, which means MCRP is a new fundamental combinatorial optimization problem. Traditional graph search algorithms or indexing methods are usually time and space consuming or perform poorly, which are not suitable for solving the problem. In this paper, we propose an end-to-end encoder-decoder model to plan a route for each many-to-one carpooling order with various data-driven mechanisms such as graph partitioning and feature crossover. The encoder is a filter-integrated Graph Convolution Network with external information fusion combining a supervised pre-training classification task, while the latter mimics a pointer network with a rule-based mask mechanism and a domain feature crossover module. We validate the effectiveness and efficiency of our model based on both synthetic and real-world datasets.}
}


@article{DBLP:journals/tmc/WangWYLCS25,
	author = {Ru{-}Jun Wang and
                  Chih{-}Hang Wang and
                  De{-}Nian Yang and
                  Guang{-}Siang Lee and
                  Wen{-}Tsuen Chen and
                  Jang{-}Ping Sheu},
	title = {Optimizing Resource Block Allocation for Multicast in Beyond 5G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7880--7898},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3549590},
	doi = {10.1109/TMC.2025.3549590},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWYLCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {New radio (NR) and non-orthogonal multiple access (NOMA) offer scalable and efficient resource allocation in Beyond 5G (B5G) networks. NR implements mixed numerology with flexible frame structures for future compatibility, whereas NOMA allows users with different channel states to share an identical Physical Resource Block (PRB). Multi-connectivity enables a user to connect to multiple networks for reliability, and multicast conveys data to users simultaneously that request the same content. However, resource allocation in the NOMA-based mixed numerology system with multi-connectivity for multicast remains unexplored. The problem is challenging due to 1) the different shapes of PRBs in NR and 2) the shared locations of PRBs in a frame with NOMA. In this paper, we formulate a new optimization problem, named Multicast, Multi-connectivity, and Multi-Dimensional Resource Allocation Problem (M3DRAP), and prove its NP-hardness and inapproximability. We propose an approximation algorithm for general M3DRAP with the ideas of Multicast Inter-Numerology Relation, Layer Dissimilarity, Subgrouping Nonuniformity, and Segmentation Preference. To find the intrinsic properties of PRB allocation for multicast in NOMA-based networks, we consider a single B5G usage scenario (e.g., eMBB, URLLC, or mMTC) and propose another approximation algorithm. Simulations demonstrate our algorithms improve the weighted sum rate by over 50% and increase the user satisfaction ratio by 1.5x.}
}


@article{DBLP:journals/tmc/ZhaoHYZ25,
	author = {Jiayu Zhao and
                  Fannv He and
                  Yiyu Yang and
                  Yuqing Zhang},
	title = {Identifying Implementation Flaws of {SMS} {OTP} Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7899--7913},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3550883},
	doi = {10.1109/TMC.2025.3550883},
	timestamp = {Sat, 06 Sep 2025 20:29:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoHYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, the Short Message Service (SMS) One-Time Passwords (OTP) authentication is widely adopted in mobile applications. However, due to improper implementation by developers, significant security flaws exist in the SMS OTP authentication mechanisms of some apps. To provide a comprehensive and accurate assessment, we propose a new approach. First, we locate the SMS OTP authentication page through UI exploration. Then, using hooking technology, we conduct simulated attacks to verify the security of the SMS OTP authentication in the app, focusing on its susceptibility to brute-force attacks. This approach is applicable to apps with app-side or UI-layer protection measures, uncovering hidden implementation flaws beneath these protections. Technically, we employ dynamic analysis based on the ART virtual machine instrumentation to obtain runtime information of the app and generate vulnerability verification scripts, overcoming the challenges posed by code-packing in program analysis. We implemented a semi-automatic tool named AuthChecker and tested it on 950 popular apps, identifying 87 apps with security flaws that potentially allow attackers to achieve unauthorized account access. Our findings highlight the security issues in SMS OTP authentication of apps, promoting improvements in vulnerability patching and preventive strategies by developers.}
}


@article{DBLP:journals/tmc/HeCHGDZDZW25,
	author = {Zhixiang He and
                  Jing Chen and
                  Kun He and
                  Yangyang Gu and
                  Qiyi Deng and
                  Zijian Zhang and
                  Ruiying Du and
                  Qingchuan Zhao and
                  Cong Wu},
	title = {HeadSonic: Usable Bone Conduction Earphone Authentication via Head-Conducted
                  Sounds},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7914--7928},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551272},
	doi = {10.1109/TMC.2025.3551272},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeCHGDZDZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Earables (ear wearables) are rapidly emerging as a new platform encompassing a diverse of personal applications, prompting the development of authentication schemes to protect user privacy. Existing earable authentication methods are all specifically designed for air-conduction earphones, which are not suited for bone conduction earphones (BCEs) that rely on bone conduction mechanisms. In this paper, we propose HeadSonic, a usable BCE authentication system based on the unique head-conducted sounds, which can be acquired when the user wears the BCE device. Specifically, the system emits a millisecond-level sound to initiate the authentication session. The signal captured by the BCE microphone is propagated through the user's head, which is unique in density, geometry, and bone-tissue ratio. It operates implicitly, while maintaining robustness across different behaviors. Extensive experiments involving 60 subjects demonstrate that HeadSonic achieves a commendable balanced accuracy of 96.59%, proving its efficacy and resilience against replay and synthesis attacks. Our dataset and source codes are available at https://anonymous.4open.science/r/HeadSonic-1CE4.}
}


@article{DBLP:journals/tmc/ParkHTHSH25,
	author = {Yu Min Park and
                  Sheikh Salman Hassan and
                  Yan Kyaw Tun and
                  Eui{-}Nam Huh and
                  Walid Saad and
                  Choong Seon Hong},
	title = {Design Optimization of {NOMA} Aided Multi-STAR-RIS for Indoor Environments:
                  {A} Convex Approximation Imitated Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7929--7946},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552521},
	doi = {10.1109/TMC.2025.3552521},
	timestamp = {Wed, 08 Oct 2025 16:59:46 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ParkHTHSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-orthogonal multiple access (NOMA) enables multiple users to share the same frequency band, and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) provides 360-degree full-space coverage, optimizing both transmission and reflection for improved network performance and dynamic control of the indoor environment. However, deploying STAR-RIS indoors presents challenges in interference mitigation, power consumption, and real-time configuration. In this work, a novel network architecture utilizing multiple access points (APs), STAR-RISs, and NOMA is proposed for indoor communication. To address these, we formulate an optimization problem involving user assignment, access point (AP) beamforming, and STAR-RIS phase control. A decomposition approach is used to solve the complex problem efficiently, employing a many-to-one matching algorithm for user-AP assignment and K-means clustering for resource management. Additionally, multi-agent deep reinforcement learning (MADRL) is leveraged to optimize the control of the STAR-RIS. Within the proposed MADRL framework, a novel approach is introduced in which each decision variable acts as an independent agent, enabling collaborative learning and decision making. The MADRL framework is enhanced by incorporating convex approximation (CA), which accelerates policy learning through suboptimal solutions from successive convex approximation (SCA), leading to faster adaptation and convergence. Simulations demonstrate significant improvements in network utility compared to baseline approaches.}
}


@article{DBLP:journals/tmc/JiZJZLQY25,
	author = {Mingtao Ji and
                  Hehan Zhao and
                  Lei Jiao and
                  Sheng Zhang and
                  Xin Li and
                  Zhuzhong Qian and
                  Baoliu Ye},
	title = {Edge {AI} Inference as a Service via Dynamic Resources From Repeated
                  Auctions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7947--7964},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3554816},
	doi = {10.1109/TMC.2025.3554816},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiZJZLQY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enable edge AI providers to recruit edge devices and use them to deploy AI models and provision inference services, we conduct a comprehensive mathematical and algorithmic study on a novel incentive and optimization mechanism based on repeated auctions. We first model and formulate a time-cumulative social cost optimization problem to capture the challenges of the trade-off between cost and accuracy, the dependency between adjacent auctions, and the need of achieving desired economic properties. Then, to solve this intractable non-linear integer program in an online manner, we design a set of polynomial-time algorithms that work together. Our approach dynamically chooses and switches winning bids under careful control, incorporates online learning to overcome posterior inference accuracy and workload queue dynamics, and leverages randomization to strategically convert fractional decisions of model placement and query dispatch into integers. We also allocate payments to meet the necessary and sufficient conditions for the desired economic properties. Further, we rigorously prove the constant competitive ratio, the sub-linear regret and fit, and the truthfulness and individual rationality for our proposed approach. Finally, through extensive experiments using real devices, AI models, and data traces, we have validated the substantial advantages of our proposed approach compared to the baselines and the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/HeHL25,
	author = {Xiaoyang He and
                  Xiaoxia Huang and
                  Lanhua Li},
	title = {Contextual Bandits With Non-Stationary Correlated Rewards for User
                  Association in mmWave Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7965--7979},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3552717},
	doi = {10.1109/TMC.2025.3552717},
	timestamp = {Thu, 20 Nov 2025 14:38:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter wave (mmWave) communication has emerged as a key technology enabling ultra-low latency and high throughput in vehicular communication. Usually, an appropriate decision on user association requires timely channel information between vehicles and base stations (BSs), which is challenging given a fast-fading mmWave vehicular channel. In this paper, we propose a low-complexity semi-distributed contextual correlated upper confidence bound (SD-CC-UCB) algorithm to establish an up-to-date user association between vehicles and BSs without explicit measurement of channel state information (CSI). Under a contextual multi-arm bandits framework, SD-CC-UCB learns and predicts the transmission rate given the location and velocity of the vehicle, which can adequately capture the intricate channel condition for a prompt decision on user association. Further, SD-CC-UCB efficiently identifies the set of candidate BSs which probably support supreme transmission rates by leveraging the correlated distributions of transmission rates on different locations. To further refine the learning transmission rate to candidate BSs, each vehicle deploys the Thompson Sampling algorithm by taking the interference among vehicles and handover into consideration. Numerical results show that our proposed algorithm achieves the network throughput within 100%–103% of a benchmark algorithm which requires perfect instantaneous CSI, demonstrating the effectiveness of SD-CC-UCB in vehicular communications.}
}


@article{DBLP:journals/tmc/ZhangPCT25,
	author = {Ningbo Zhang and
                  Guangqian Peng and
                  Hao Chen and
                  Caitong Tang},
	title = {{PBN-CSMA/CA:} {A} Power Back-Off NOMA-Based {CSMA/CA} Protocol for
                  Ad Hoc Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7980--7993},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3551340},
	doi = {10.1109/TMC.2025.3551340},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangPCT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Carrier-sense multiple access with collision avoidance (CSMA/CA) is one of the fundamental medium access control (MAC) protocols for ad hoc networks. As a network’s size increases, its throughput degrades substantially because of packet collisions. To reduce the collision probability, combining non-orthogonal multiple access (NOMA) with CSMA/CA is a promising solution. However, existing NOMA-CSMA/CA protocols adopt distributed power selection and channel inversion power control, resulting in a high power collision probability and limiting the number of power levels. To address these issues, we propose a power back-off NOMA-based CSMA/CA (PBN-CSMA/CA) protocol for ad hoc networks. The proposed protocol achieves centralized power allocation, avoiding power collisions by employing the Zadoff-Chu (ZC) sequence and the power level allocation (PLA) frame. Additionally, power back-off (PB) control is used to set the transmission power, which expands the number of power levels and gives full play to the performance advantages of NOMA. To analyze the performance comprehensively, the closed-form expressions of the average outage probability, normalized saturation throughput, average packet delay and transmission energy consumption are theoretically analyzed. Both the analytical and simulation results demonstrate that the PBN-CSMA/CA protocol outperforms the existing NOMA-CSMA/CA and traditional CSMA/CA protocols, with significant throughput gains and delay reductions.}
}


@article{DBLP:journals/tmc/LuoXLCHZC25,
	author = {Ji Luo and
                  Zijian Xiao and
                  Zuxin Li and
                  Xuecheng Chen and
                  Chaopeng Hong and
                  Xiao{-}Ping Zhang and
                  Xinlei Chen},
	title = {SmartSpr: {A} Physics-Informed Mobile Sprinkler Scheduling System
                  for Reducing Urban Particulate Matter Pollution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {7994--8010},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555448},
	doi = {10.1109/TMC.2025.3555448},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoXLCHZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban particulate pollution presents considerable public health hazards, underscoring the need for effective control measures in various cities. This paper proposes SmartSpr, a physics-informed urban mobile sprinkler scheduling system designed for enhanced efficiency in reducing particulate pollution. SmartSpr incorporates a Physics-Informed Neural Network (PINN)-based model, enriched with Bayesian optimization, to accurately simulate the impact of mobile sprinklers on particulate matter (PM) dispersion. Building on this sprinkling effect model, a selective sprinkling strategy considering the replenish process is proposed. This strategy employs a sparsity-driven decoupling simulated annealing algorithm to refine sprinkler routes, prioritizing areas with substantial environmental benefits. Extensive field experiments and simulations have validated SmartSpr, demonstrating a 64.8% reduction in prediction error of SmartSpr's sprinkling model compared to the leading baseline and an 18% enhancement in pollutant reduction efficiency of the proposed scheduling algorithm.}
}


@article{DBLP:journals/tmc/XiongLYZWS25,
	author = {Xin Xiong and
                  Meng Li and
                  F. Richard Yu and
                  Haijun Zhang and
                  Kan Wang and
                  Pengbo Si},
	title = {Cloud-Edge-End Collaborative Computing-Enabled Intelligent Sharding
                  Blockchain for Industrial IoT Based on {PPO} Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8011--8024},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3554568},
	doi = {10.1109/TMC.2025.3554568},
	timestamp = {Thu, 27 Nov 2025 10:58:56 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiongLYZWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The security and reliability risks of industrial data have constrained the advancement of the Industrial Internet of Things (IIoT). Although blockchain can protect the security and reliability of industrial data through hash verification mechanisms, there are numerous challenges in the existing blockchain-enabled IIoT systems, such as the trilemma of scalability, decentralization and security, high computational power consumption of consensus protocols and limited computational resources of industrial devices. To address these problems, an intelligent sharding blockchain-enabled IIoT framework is proposed, in which the intelligent sharding based on the reputation mechanism and the adaptive switching for multi-consensus protocols are utilized to enhance the decentralization, security and scalability of blockchain. Considering higher requirement of computational power of the sharding blockchain, a cloud-edge-end collaborative computing framework is introduced, in which the parallel computational offloading and the Terahertz communication technology are utilized to enhance the cooperation of the cloud-edge-end networks. Furthermore, due to the highly dynamic nature of industrial devices and industrial data, we consider and design the optimization problem as a Markov decision process (MDP), which is solved via the Proximal Policy Optimization (PPO) algorithm. Simulation results show that our proposed scheme can minimize total delay and maximize transaction throughput while guaranteeing the safety as well as decentralization of blockchain-enabled IIoT systems.}
}


@article{DBLP:journals/tmc/XuWLWWZZ25,
	author = {Chao Xu and
                  Jessie Hui Wang and
                  Rui Li and
                  Hao Wu and
                  Jilong Wang and
                  Jun Zhang and
                  Kai Zheng},
	title = {Cost-Efficient {FEC} Scheme for Time-Sensitive Multi-Hop Transmissions
                  in Overlay Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8025--8038},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553380},
	doi = {10.1109/TMC.2025.3553380},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWLWWZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In pursuit of low latency, real-time communication (RTC) service providers usually use multi-hop overlay links worldwide to bypass congested links, especially for medium- and long-distance transmissions. In such multi-hop long-distance transmission scenarios, utilizing retransmission to recover lost packets can result in increased end-to-end latency. Therefore, Forward Error Correction (FEC) is viewed as a promising way to solve the loss problem. However, for multi-hop overlay transmission, existing FEC schemes either introduce a non-negligible processing delay at each hop or reduce the processing delay at the cost of a high coefficient overhead. In this work, we propose a multi-hop FEC scheme, i.e., FEC-OEM, which considers both processing delay and coefficient overhead. FEC-OEM is designed based on two observations we obtained from measurements. First, coefficient overhead can only be reduced through an implicit transmission way. Therefore, we design a modulation-based recoding module that enables implicit coefficient transmission and hop-by-hop recoding at the same time. Second, using on-the-fly computation is a promising way to reduce processing delay. Accordingly, we design an elimination method to make the modulation-based recoding can be carried out on-the-fly. Real-world experiments demonstrate that FEC-OEM can reduce the processing delay by up to 88% without increasing the coefficient overhead compared to state-of-the-art schemes. We also use FEC-OEM to transmit packets for applications with different loss tolerances, and the results show that FEC-OEM can improve the QoE more effectively than state-of-the-art coding schemes.}
}


@article{DBLP:journals/tmc/ShiYWTLWZV25,
	author = {Rongye Shi and
                  Xin Yu and
                  Yandong Wang and
                  Yongkai Tian and
                  Zhenyu Liu and
                  Wenjun Wu and
                  Xiao{-}Ping Zhang and
                  Manuela M. Veloso},
	title = {Symmetry-Informed {MARL:} {A} Decentralized and Cooperative {UAV}
                  Swarm Control Approach for Communication Coverage},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8039--8056},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553285},
	doi = {10.1109/TMC.2025.3553285},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShiYWTLWZV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed aerial vehicle-mounted base stations (UAV-MBSs) provide flexible wireless connectivity, extending communication coverage in underserved areas. Recently, multi-agent reinforcement learning (MARL) has shown great potential for cooperative UAV swarm control to support efficient communication coverage in dynamic and complex environments. However, existing MARL-based methods often suffer from low sample efficiency due to its trial-and-error training characteristics, limiting its ability to control large UAV swarms with continuous state-action space and partial observation. We notice that UAV swarm systems in communication coverage tasks exhibit a spatial symmetry property, e.g., a rotation in the spatial observation of a UAV results in a same rotation in its optimal action. Exploiting this property, we formulate the task as a symmetric decentralized partially observable Markov decision process and introduce symmetry-informed MARL, featuring a novel network called the symmetry-informed graph neural network (SiGNN) to serve as the policy/value networks. SiGNN leverages the inherent symmetry in multi-UAV systems by embedding the symmetry into the network structure, thereby enhancing the training efficiency to handle large swarms with continuous control. Theoretical analysis shows that the SiGNN strictly preserves symmetry properties, which guarantees the effectiveness of the approach. Experiments in simulation were conducted to handle communication coverage using up to 20 UAVs with continuous control. Experimental results demonstrate that SiGNN-based MARL outperforms advanced baselines, verifying its superior sample efficiency, scalability and robustness.}
}


@article{DBLP:journals/tmc/XiaXZMXR25,
	author = {Yun Xia and
                  Hai Xue and
                  Di Zhang and
                  Shahid Mumtaz and
                  Xiaolong Xu and
                  Joel J. P. C. Rodrigues},
	title = {Dynamic Pricing Based Near-Optimal Resource Allocation for Elastic
                  Edge Offloading},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8057--8070},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553188},
	doi = {10.1109/TMC.2025.3553188},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaXZMXR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC), task offloading can significantly reduce task execution latency and energy consumption of end user (EU). However, edge server (ES) resources are limited, necessitating efficient allocation to ensure the sustainable and healthy development for MEC system. In this paper, we propose a dynamic pricing mechanism based near-optimal resource allocation for elastic edge offloading. First, we construct a resource pricing model and accordingly develop the utility functions for both EU and ES, the optimal pricing model parameters are derived by optimizing the utility functions. In the meantime, our theoretical analysis reveals that the EU’s utility function reaches a local maximum within the search range, but exhibits barely growth with increased resource allocation beyond this point. To this end, we further propose the Dynamic Inertia and Speed-Constrained particle swarm optimization (DISC-PSO) algorithm, which efficiently identifies the near-optimal resource allocation. Comprehensive simulation results validate the effectiveness of DISC-PSO algorithm, demonstrating that it significantly outperforms existing schemes by reducing the average number of iterations to reach a near-optimal solution by 86.88%, increasing the EU utility function value by 0.13%, and decreasing the variance of results by 96.78%.}
}


@article{DBLP:journals/tmc/PengXCLCZAF25,
	author = {Jincheng Peng and
                  Huanlai Xing and
                  Xiangyi Chen and
                  Yang Li and
                  Yunhe Cui and
                  Danyang Zheng and
                  Laha Ale and
                  Li Feng},
	title = {Security Enhanced Computation Offloading for Collaborative Inference
                  at Semantic-Communication-Empowered Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8071--8088},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555298},
	doi = {10.1109/TMC.2025.3555298},
	timestamp = {Fri, 12 Sep 2025 07:39:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengXCLCZAF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication (SC) has emerged as a promising paradigm for upcoming intelligent applications, enabling mobile devices to collaboratively execute intelligent tasks with edge servers through computation offloading. However, few studies have addressed the problem of collaborative inference in SC networks. Traditional collaborative inference mechanisms may suffer performance decline in SC systems and are vulnerable to eavesdroppers. To address these issues, first, we present an encryptor that encrypts semantic information to avoid privacy leakage and a decryptor for restoration. Besides, we propose a novel SC-empowered edge computing framework enabling mobile devices to deploy a partial semantic encoder and offload the rest to edge servers. Based on this framework, we formulate the collaborative inference optimization problem, jointly optimizing delay, energy consumption, and privacy leakage. DNNPart is devised based on deep deterministic policy gradient to address the problem, which consists of a semantic attention mechanism that enables it to focus on important state variables, a hybrid action representation method that makes it adapt to mixed discrete and continuous action spaces, a dynamic model splitting algorithm that locates the optimal partition layer and adaptively splits the semantic coders. Integrated with these components, DNNPart iteratively optimizes the offloading strategy to find the optimal offloading strategy. Extensive simulations were conducted to verify the effectiveness of the proposed method by comparing it with baseline mechanisms.}
}


@article{DBLP:journals/tmc/ZengZYQGGL25,
	author = {Yue Zeng and
                  Junlong Zhou and
                  Baoliu Ye and
                  Zhihao Qu and
                  Song Guo and
                  Tianjian Gong and
                  Pan Li},
	title = {ExpertDRL: Request Dispatching and Instance Configuration for Serverless
                  Edge Inference With Foundation Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8089--8104},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553201},
	doi = {10.1109/TMC.2025.3553201},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZengZYQGGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence of the pre-training & fine-tuning paradigm enables machine learning models to quickly adapt to various downstream tasks by fine-tuning pre-trained foundation models (FMs), greatly facilitating various IoT applications that rely on model inference in dynamic edge serverless environments. Efficiently dispatching inference requests and configuring instances to batch inference requests can significantly enhance resource efficiency. However, existing serverless inference solutions are tailored for traditional models, make coarse-grained request dispatching and instance configuration decisions, fail to exploit the shared model backbone characteristics of the FM and capture delayed rewards in dynamic environments, and ignore communication latency between edge sites, resulting in high costs and constraint violations. In this paper, we leverage our insight that fine-grained batch inference requests can effectively exploit the shared model backbone feature of FM to save monetary costs. We propose an algorithm that incorporates deep reinforcement learning (DRL) and expert intervention for fine-grained request dispatching and instance configuration, where the DRL component outputs fractional solutions as guidance, while the expert intervention module integrates our insights—batching reduces monetary costs at the expense of increased inference latency, whereas higher configurations shorten inference latency. This module rounds fractional solutions and adjusts instance configurations to search for optimal solutions while satisfying constraints, with theoretical guarantees rigorously proved. Finally, we conducted our experiments on an OpenFaas-based platform and simulator, and extensive trace-driven evaluation results show that ExpertDRL can save costs by up to 85.14% and improve request acceptance ratio by up to 26.93%, compared to the state-of-the-art solution.}
}


@article{DBLP:journals/tmc/XiaoZLLWW25,
	author = {Yunpeng Xiao and
                  Dengke Zhao and
                  Xufeng Li and
                  Tun Li and
                  Rong Wang and
                  Guoyin Wang},
	title = {A Federated Learning-Based Data Augmentation Method for Privacy Preservation
                  Under Heterogeneous Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8105--8118},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553501},
	doi = {10.1109/TMC.2025.3553501},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoZLLWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning is an important distributed machine learning paradigm. This study proposes a privacy-preserving data augmentation model for federated learning of heterogeneous data, which is able to mitigate heterogeneity and augmenting the participant’s local data while protecting data privacy. First, to address the problem of global model bias due to heterogeneous data, this study proposes a distributed generative adversarial network FedEqGAN. The model introduces a multi-source data feature fusion mechanism, which can learn the features of each data source to generate synthetic data. Second, addressing the privacy leakage issue caused by the disclosure of data distribution information, this paper proposes an encryption algorithm for heterogeneous environments FedHE, which utilizes homomorphic encryption to protect local data distributions and aggregates local data information through KL dispersion in order to construct global data distributions. Finally, for the privacy leakage problem caused by uploading model parameters in federation training, this paper proposes a federation model parameter encryption algorithm DPFedMP. This algorithm dynamically injects Gaussian noise into the model parameters according to the difference of data distribution to realize differential privacy protection and update the global model. Experiments show that the method is applicable to heterogeneous data environment, significantly enhancing model performance while ensuring data security.}
}


@article{DBLP:journals/tmc/GuiZGLGDX25,
	author = {Linqing Gui and
                  Siyi Zheng and
                  Zhengxin Guo and
                  Zhetao Li and
                  Ming Gao and
                  Schahram Dustdar and
                  Fu Xiao},
	title = {RaliSense: Extending WiFi Respiratory Detection Range by Rapid Alignment
                  of Dynamic Components},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8119--8135},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553924},
	doi = {10.1109/TMC.2025.3553924},
	timestamp = {Wed, 29 Oct 2025 15:58:02 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuiZGLGDX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi based respiratory detection has attracted increasing attentions due to its ubiquity and convenience. In Non-Line-of-Sight (NLoS) scenarios, WiFi signals reflected from human target are blocked by obstacles and become much weaker, thus limiting the sensing range and hindering the practical deployment. The existing best respiratory detection system extended the sensing range by scaling and aligning dynamic components in WiFi signals. However, its dynamic component scaling causes the amplification of noise, while its dynamic component alignment increases computation complexity due to the traversal on all possible rotation angles. To address the above issues, in this paper we first build WiFi sensing range models for respiratory detection in NLoS scenario, find factors that limit the sensing range, and then propose a new respiratory detection system named RaliSense which can further rapidly extend the sensing range in NLoS scenario. The main idea of RaliSense is rapidly aligning dynamic components without amplifying noise, based on change direction vector and CSI ratio sum polarity of dynamic components. The proposed change direction vector is obtained by calculating the direction on which the noisy dynamic components have the maximum variance, and CSI ratio sum polarity is then obtained by summing the dynamic components which have been rotated by the change direction vector. According to the CSI ratio sum polarity, the rotation angle is quickly adjusted for aligning dynamic components. Extensive simulation and experiment results verify the effectiveness of our proposed sensing range models. The results also demonstrate that our proposed system RaliSense can effectively extend sensing range in NLoS scenario, achieving a 22.7% improvement over the best existing work but spending only a quarter of its computation time.}
}


@article{DBLP:journals/tmc/LiHLZM25,
	author = {Ziqi Li and
                  Jia Hu and
                  Xi Li and
                  Heli Zhang and
                  Geyong Min},
	title = {Dynamic {AP} Clustering and Power Allocation for CF-mMIMO-Enabled
                  Federated Learning Using Multi-Agent {DRL}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8136--8151},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3554081},
	doi = {10.1109/TMC.2025.3554081},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiHLZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is recognized as a pivotal paradigm for 6G, offering decentralized model training without compromising data privacy. Recent works have proposed deploying FL in cell-free massive MIMO (CF-mMIMO) networks for reliable model transmission between FL clients and the server. Nevertheless, the problem of simultaneous access point (AP) clustering (i.e., dynamically forming AP groups to facilitate client-server communication) and transmit power allocation has not been thoroughly investigated. Furthermore, most existing solutions do not simultaneously consider the fast decision-making requirements brought by user mobility and the scalability of solutions in large-scale networks. To address this gap, we propose DACPA, a multi-agent deep reinforcement learning (DRL)-based scheme that accounts for client mobility (walking speed) and heterogeneous computing capabilities. DACPA strategically assigns each client a customized AP cluster and corresponding transmit power configuration, thereby optimizing model update latency. Extensive simulation results demonstrate the superior performance of DACPA in terms of convergence stability, spectral efficiency, global model update latency, and average energy consumption.}
}


@article{DBLP:journals/tmc/ZhangZLYLM25,
	author = {Zhengyuan Zhang and
                  Dong Zhao and
                  Renhao Liu and
                  Yuxing Yao and
                  Xiangyu Li and
                  Huadong Ma},
	title = {{ACL:} Adaptive Edge-Cloud Collaborative Learning for Heterogeneous
                  Devices With Unlabeled Local Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8152--8166},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3553971},
	doi = {10.1109/TMC.2025.3553971},
	timestamp = {Tue, 20 Jan 2026 18:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZLYLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-cloud collaborative learning emerges as a promising paradigm for adapting pre-trained deep neural network (DNN) models to the ever-changing edge data environments and specific downstream tasks. However, the heterogeneity of edge devices and unlabeled local data hinder the effectiveness of existing collaborative learning approaches. To address the above issues, we propose ACL, a novel adaptive edge-cloud collaborative learning paradigm for heterogeneous devices with unlabeled local data. In ACL, we first use FedNAS, a neural architecture search algorithm designed for collaborative learning to generate a customized model on each participating device, and then a lightweight semi-supervised collaborative learning framework HSSCL is used to fine-tune the pre-trained DNN model. Compared with the SOTA collaborative learning approaches, ACL achieves significant accuracy improvement, averaging 31.5% for image classification and 15.5% for object detection. Furthermore, it reduces time overhead by 3.1-5.1× and memory overhead by 6.3-12.5×. We will release our models and tools.}
}


@article{DBLP:journals/tmc/LiW25,
	author = {Mengning Li and
                  Wenye Wang},
	title = {Synergizing Acoustic and Wi-Fi Signals for Device-Free Gesture Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8167--8179},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558139},
	doi = {10.1109/TMC.2025.3558139},
	timestamp = {Sat, 06 Sep 2025 20:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gesture recognition has significant applications in areas such as assisted living, e-health, and human-device interactions. Moving beyond conventional computer vision techniques, recent studies have increasingly adopted ubiquitous methods like Wi-Fi and acoustic signals, which provide a cost-effective solution for device deployment. In this paper, we explore these two ubiquitous techniques to enhance gesture recognition, focusing on overcoming challenges associated with multi-modal fusion. To harmonize information from these inherently different signal types, we propose a tailored fusion strategy specifically designed for Wi-Fi and acoustic signals. Traditional multi-modal fusion methods often lack a theoretical framework due to insufficient analysis of the fundamental characteristics of different signals. To address this gap, we introduce the concept of the Hybrid Zone, a novel theoretical framework that models the interaction and fusion of acoustic and Wi-Fi sensing signals. The Hybrid Zone offers a unified perspective on the interaction between acoustic and Wi-Fi sensing areas and delivers insights into the granular synthesis of their velocity profiles. Our experimental results demonstrate strong performance, achieving a gesture recognition accuracy rate of 94.69% .}
}


@article{DBLP:journals/tmc/YinZYW25,
	author = {Guolin Yin and
                  Junqing Zhang and
                  Xinping Yi and
                  Xuyu Wang},
	title = {Evasion Attacks and Countermeasures in Deep Learning-Based Wi-Fi Gesture
                  Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8180--8195},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557757},
	doi = {10.1109/TMC.2025.3557757},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YinZYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning-based Wi-Fi sensing has received massive interest thanks to the prevalence of Wi-Fi technology. While deep learning techniques provide promising results in Wi-Fi sensing, there are only very few studies on the vulnerabilities against Wi-Fi ensing. In this paper, we studied evasion attacks against deep learning-based Wi-Fi sensing and the countermeasure and conducted an extensive experimental evaluation using two publicly available datasets, namely SignFi and Widar. Accordingly, we proposed three white-box and two black-box attacks and revealed that even with an undetectable power change, evasion attacks can achieve a remarkable attack success rate (ASR) of 97.0% and 95.6% in white-box and black-box settings, respectively. These results highlight the urgent need for countermeasures against evasion attacks in Wi-Fi sensing systems. We introduced adversarial training and randomised smoothing, which notably improved the robustness of the Wi-Fi sensing model. The ASRs for white-box and black-box attacks were reduced to a minimum of around 6% and 2%, respectively. Moreover, randomised smoothing also introduced certifiable robustness, achieving 70.1% of samples certified for our model. The certification method provides an additional layer of reliability, ensuring that the model’s performance remains consistent and predictable even under adversarial conditions.}
}


@article{DBLP:journals/tmc/SongHGLCS25,
	author = {Shanshan Song and
                  Bingwen Huangfu and
                  Jiani Guo and
                  Jun Liu and
                  Junhong Cui and
                  Xuemin Shen},
	title = {A Digital Twin-Based Intelligent Network Architecture for Underwater
                  Acoustic Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8196--8213},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555640},
	doi = {10.1109/TMC.2025.3555640},
	timestamp = {Sun, 01 Feb 2026 13:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SongHGLCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater acoustic sensor networks (UASNs) drive toward strong environmental adaptability, intelligence, and multifunctionality. However, due to unique UASN characteristics, such as long propagation delay, dynamic channel quality, and high attenuation, existing studies present untimeliness, inefficiency, and inflexibility in real practice. Digital twin (DT) technology is promising for UASNs to break the above bottlenecks by providing high-fidelity status prediction and exploring optimal schemes. In this article, we propose a Digital Twin-based Network Architecture (DTNA), enhancing UASNs’ environmental adaptability, intelligence, and multifunctionality. By extracting real UASN information from local (node) and global (network) levels, we first design a layered architecture to improve the DT replica fidelity and UASN control flexibility. In local DT, we develop a resource allocation paradigm (RAPD), which rapidly perceives performance variations and iteratively optimizes allocation schemes to improve real-time environmental adaptability of resource allocation algorithms. In global DT, we aggregate decentralized local DT data and propose a collaborative Multi-agent reinforcement learning framework (CMFD) and a task-oriented network slicing (TNSD). CMFD patches scarce real data and provides extensive DT data to accelerate AI model training. TNSD unifies heterogeneous tasks’ demand extraction and efficiently provides comprehensive network status, improving the flexibility of multi-task scheduling algorithms. Finally, practical and simulation experiments verify the high fidelity of DT. Compared with the original UASN architecture, experiment results demonstrate that DTNA can: (i) improve the timeliness and robustness of resource allocation; (ii) greatly reduce the training time of AI algorithms; (iii) more rapidly obtain network status for multi-task scheduling at a low cost.}
}


@article{DBLP:journals/tmc/ChenKXWZHNM25,
	author = {Junlong Chen and
                  Jiawen Kang and
                  Minrui Xu and
                  Fan Wu and
                  Hongliang Zhang and
                  Huawei Huang and
                  Dusit Niyato and
                  Shiwen Mao},
	title = {Efficient Twin Migration in Vehicular Metaverses: Multi-Agent Split
                  Deep Reinforcement Learning With Spatio-Temporal Trajectory Generation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8214--8227},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558918},
	doi = {10.1109/TMC.2025.3558918},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenKXWZHNM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users’ immersive experiences in vehicular metaverses.}
}


@article{DBLP:journals/tmc/WangLZLHZL25,
	author = {Xiujun Wang and
                  Zhi Liu and
                  Xiaokang Zhou and
                  Yong Liao and
                  Han Hu and
                  Xiao Zheng and
                  Jie Li},
	title = {A Near-Optimal Category Information Sampling in {RFID} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8228--8244},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556869},
	doi = {10.1109/TMC.2025.3556869},
	timestamp = {Sat, 15 Nov 2025 16:51:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLZLHZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many RFID-enabled applications, objects are classified into different categories, and the information associated with each object's category (called category information) is written into the attached tag, allowing the reader to access it later. The category information sampling in such RFID systems, which is to randomly choose (sample) a few tags from each category and collect their category information, is fundamental for providing real-time monitoring and analysis in RFID. However, to the best of our knowledge, two technical challenges, i.e., how to guarantee a minimized execution time and reduce collection failure caused by missing tags, remain unsolved for this problem. In this paper, we address these two limitations by considering how to use the shortest possible time to sample a different number of random tags from each category and collect their category information sequentially in small batches. In particular, we first obtain a lower bound on the execution time of any protocol that can solve this problem. Subsequently, we present a near-OPTimal Category information sampling protocol (OPT-C) that solves the problem with an execution time close to the lower bound. Finally, extensive simulation results demonstrate the superiority of OPT-C over existing protocols, while real-world experiments further validate its practicality.}
}


@article{DBLP:journals/tmc/MishraG25a,
	author = {Rahul Mishra and
                  Hari Prabhat Gupta},
	title = {Fed-NL: {A} Federated Learning Approach to Suppress Noise in Participant
                  Datasets to Reduce Communication Rounds for Convergence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8245--8257},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558874},
	doi = {10.1109/TMC.2025.3558874},
	timestamp = {Sat, 06 Sep 2025 20:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MishraG25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning enables multiple participants to collaboratively train machine learning models without the need to share their private and limited data, thereby preserving privacy. When datasets used in federated learning contain noisy labels, it can lead to degraded performance and an increased number of communication rounds needed to achieve convergence. This, in turn, requires more time and energy to train the model. This paper proposes a federated learning approach to suppress the unequal distribution of the noisy labels in the dataset of each participant. The approach first estimates the noise ratio of the dataset for each participant and normalizes it using the server dataset. Next, the approach considers the influence of each participant and calculates the optimal weighted contributions for each one. The approach also considers bias in the server dataset and minimizes its impact on the participants. Further, the paper provides an expression to estimate the number of communication rounds required for convergence. Results demonstrate the superiority of the proposed approach over baselines in terms of communication rounds and performance.}
}


@article{DBLP:journals/tmc/WangSLLZ25,
	author = {Kang Wang and
                  Zhishu Shen and
                  Zhen Lei and
                  Xianhui Liu and
                  Tiehua Zhang},
	title = {Toward Multi-Agent Reinforcement Learning Based Traffic Signal Control
                  Through Spatio-Temporal Hypergraphs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8258--8271},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556243},
	doi = {10.1109/TMC.2025.3556243},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSLLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatio-temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent urban traffic management solutions.}
}


@article{DBLP:journals/tmc/ZhouWZ25,
	author = {Taolue Zhou and
                  Xiaohan Wu and
                  Xin{-}Ming Zhang},
	title = {Digital Twin Empowered mmWave Multi-Hop {V2X} Routing Scheme With
                  {UAV} Assistance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8272--8286},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556091},
	doi = {10.1109/TMC.2025.3556091},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In VANETs, millimeter wave (mmWave) is capable of providing ultra-high bandwidth, large data throughput, and very low transmission latency. However, the difficulty of mmWave transmission is further exacerbated by its limited transmission range and its physical properties of low diffraction and low penetration, coupled with real-time dynamic network topology. In future 6G networks, digital twins (DTs) are considered a promising enabling technology as it can provide seamless interaction between the virtual network world and the real world. In this paper, we propose a DT-empowered multi-hop V2X routing scheme using mmWave as the transmission link in urban scenarios. First, we propose a novel DT-assisted mmWave multi-hop relay network architecture, which uses the global traffic flow information possessed by the DTs to assist vehicles in selecting the optimal relay nodes. Second, the uncrewed aerial vehicles (UAVs) are deployed at intersections to improve the packet forwarding efficiency of the intersections. Finally, a reinforcement learning algorithm is used to make forwarding decisions between streets. In addition, we also consider vehicle density and network load as key indicators for optimal street selection. The experimental results indicate that our solution has significant advantages in terms of key indicators such as packet delivery rate and latency.}
}


@article{DBLP:journals/tmc/WangLLZL25,
	author = {Hongbo Wang and
                  Xin Li and
                  Jiachun Li and
                  Haojin Zhu and
                  Jun Luo},
	title = {VR-Fi: Positioning and Recognizing Hand Gestures via VR-Embedded Wi-Fi
                  Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8287--8300},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557561},
	doi = {10.1109/TMC.2025.3557561},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate gesture-based interactions are crucial for enhancing the immersive experience in VR (virtual reality) systems; they in turn necessitate gesture positioning and recognition in physical world. However, existing VR gesture recognition methods are predominantly vision-based, incurring high computational demands and raising privacy concerns. Meanwhile, Wi-Fi-based gesture recognition methods, deemed as promising complement to vision-based ones, typically lack gesture positioning capabilities. To this end, we propose VR-Fi, a gesture positioning and recognition system leveraging VR(-headset)-embedded Wi-Fi. To position gestures across different areas, VR-Fi innovates in a frequency-hopping bandwidth expansion (FHBE) technique to improve spatial resolution for locating a target. Additionally, VR-Fi innovates in neural models to process the FHBE-enhanced Wi-Fi CSI (channel state information) and enable the multi-task requirements of the joint positioning and recognition of hand gestures. Extensive experimental results demonstrate that VR-Fi achieves a positioning accuracy of 94.47%, a recognition accuracy of 92.13%, and a joint accuracy of 89.47%.}
}


@article{DBLP:journals/tmc/WangZCLW25,
	author = {Jiankun Wang and
                  Zenghua Zhao and
                  Jiayang Cui and
                  Jiafan Lu and
                  Bin Wu},
	title = {TrackLet: Data-Driven Inertial Tracking on Your Own {IMU} Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8301--8313},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557190},
	doi = {10.1109/TMC.2025.3557190},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZCLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Empowered by deep learning, the data-driven smartphone inertial tracking has attracted much attention due to its high accuracy and robustness. However, training a one-size-fits-all model requires a significant amount of inertial measurement unit (IMU) data with accurate labels, which incurs high costs of annotation and computation. In this work, we propose a new solution for IMU tracking that trains a specific model for each individual on their own IMU data. Individual IMU data can be opportunistically collected and automatically annotated with coarse-grained locations provided by smartphones (lite labels). To learn from noisy lite labels, we propose a lightweight and noise-resistant framework TrackLet for personalized data-driven IMU tracking. Two effective techniques are designed to combat the noise in lite labels, namely CNAL (Chained-Noise Adaptation Layer) and CoAdapt (Cooperatively Adaptive small-loss selection and weighting). Extensive experiment results demonstrate that TrackLet achieves high accuracy yet at a low cost, outperforming its state-of-the-art counterparts.}
}


@article{DBLP:journals/tmc/ZhangWGDMPDL25,
	author = {Qiyang Zhang and
                  Shangguang Wang and
                  Jinglong Guan and
                  Praveen Kumar Donta and
                  Xiao Ma and
                  Ranga Rao Venkatesha Prasad and
                  Schahram Dustdar and
                  Xuanzhe Liu},
	title = {SatCooper: Enhancing Cooperative Inference Analytics for Satellite
                  Service via Multi-Exit DNNs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8314--8328},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556457},
	doi = {10.1109/TMC.2025.3556457},
	timestamp = {Fri, 16 Jan 2026 20:27:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWGDMPDL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a key technology of intelligent satellite-enabled services in B5G or 6G networks, deploying Deep Neural Networks (DNN) models on satellites has been a notable trend, catering to the daily demand for extensive computing-intensive and latency-sensitive tasks. The computing resources are strategically deployed on satellites where sensor data is generated or collected, facilitating the fine-grained computational inference of DNN-based tasks. However, no prior study has comprehensively explored the crucial inference challenges – e.g., the trade-off between the number of tasks completed and accuracy and partitioning models in multi-exit models – in the resource-constrained space environment. Effective scheduling frameworks cater to various streams of inference tasks are scarce because inference performance may deviate from the ideal situation due to changes in task system status, such as task profiles and network state. To this end, we first formulate a gain-aware in-orbit computing inference problem to strike a proper trade-off between inference latency and the number of tasks completed by dynamically selecting optimal early exit points and model partitioning points. We propose an offline dynamic programming-based algorithm that provides an effective solution when comprehensive system details are to be predicted. We have developed an online learning-based method to schedule inference tasks with uncertain and dynamic system statuses in real-world situations. Our evaluation shows that, compared to baseline methods, the online learning-based algorithm can improve task gain by an average of 87.3% across various tasks.}
}


@article{DBLP:journals/tmc/DaiWCSW25,
	author = {Minghui Dai and
                  Tianshun Wang and
                  Shan Chang and
                  Zhou Su and
                  Yuan Wu},
	title = {Energy Minimization Oriented Hybrid Semantic Data Transmission in
                  Air-Ocean Integrated Networks: {A} Resource Allocation Design},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8329--8346},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556476},
	doi = {10.1109/TMC.2025.3556476},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiWCSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of new generation communication technologies, the future maritime information networks pave the way to promote the exploration of ocean resources. Moreover, the underwater data center (UDC) is considered to be a significant data storage and computing unit in future maritime networks for providing ocean services. However, the current deployment of UDC faces the critical issues, i.e., the long-distance underwater transmission is unreliable and the energy consumption and resources of underwater transmission are overloaded. To address the two critical issues of unreliable data transmission and high resource overheads, in this paper, we present a hybrid semantic data transmission architecture in air-ocean integrated networks, which can perceive the sea surface data accurately and transmit it to the UDC for processing. Specifically, in surface layer, uncrewed aerial vehicles (UAVs) perceive ocean environment and send data to the buoy via non-orthogonal multiple-access (NOMA) transmission to improve the channel utilization. In underwater layer, the buoy sends the collected data to UDC via semantic transmission, while the semantic fidelity metric is utilized to improve the transmission efficiency. A resource allocation problem for energy minimization is formulated to jointly optimize the semantic scaling factor, the NOMA decoding order, the communication and computing resource allocations. We exploit a decomposition approach to transform the problem into two sub-problems, where the optimal resource allocations are obtained by proposing efficient algorithms. Finally, we provide simulations to verify the effectiveness and efficiency of our proposed scheme. The results demonstrate that our proposal has the advantages of lower energy consumption compared to several baseline schemes.}
}


@article{DBLP:journals/tmc/LiuLZXL25,
	author = {Xiulong Liu and
                  Hankai Liu and
                  Jiaqi Zhang and
                  Xin Xie and
                  Keqiu Li},
	title = {Multi-User Behavioral Privacy Filtering for mmWave Radar Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8347--8361},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556674},
	doi = {10.1109/TMC.2025.3556674},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLZXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an advanced technology for non-contact sensing, mmWave radar enables fine-grained measurement of a wide variety of user behaviors. While creating intelligence and convenience, it also concerns behavioral privacy and security, as radar signals contain a wealth of behavioral information. Existing solutions are either incapable of customizable privacy protections or cannot cope with multi-person scenarios. This paper presents a Multi-user behavioral privacy Filter, MuFilter, a data masking system centered on the idea of dimensional signal interference. It determines the sensing signatures that need to be preserved or interfered with based on the sensing services that users want to enable and disable, thereby making targeted tampering on the radar signal. On this basis, we introduce the multi-person tracking technology to allow MuFilter to determine the number of users in unknown scenarios. Moreover, a subspace tampering technique is proposed to ensure that each tampering only affects the target user and not other users, thus supporting personalized privacy protection for multiple users. Experiments show that MuFilter can interfere with targeted behavioral signatures with a 100% success rate, while the degree of impact on other users’ signatures ranges from 0% to 3.85%.}
}


@article{DBLP:journals/tmc/FanCWC25,
	author = {Xueli Fan and
                  Jieming Chen and
                  Qixin Wang and
                  Edward Chung},
	title = {A {CAV} Cooperative Lane Change Protocol With {CTH} Safety Guarantee
                  on Dedicated Highways},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8362--8378},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558922},
	doi = {10.1109/TMC.2025.3558922},
	timestamp = {Tue, 04 Nov 2025 16:35:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanCWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autopiloting Connected and Autonomous Vehicles (CAVs) is an important application for mobile computing. A promising context to realize autopiloting CAVs is cooperative driving on dedicated highways. For such a context, an indispensable driving scenario is Cooperative Lane Change (CLC). Due to the safety concerns of this driving scenario, a verifiably safe solution is needed (at least, the solution design should be formally provably safe). However, this demand is complicated by the inherently unreliable wireless communications between the CAVs. In this paper, we focus on the well-adopted Constant Time Headway (CTH) safety rule. We propose a CLC protocol, and formally prove its guarantee of the CTH safety and liveness, even under arbitrary wireless packet losses. These theoretical claims are further confirmed by our simulations. The simulation results also show that our proposed protocol significantly improves lane change success rates (by  5.3 % ∼ + ∞ % 5.3\\% \\sim +\\infty \\% ) than other alternatives under adverse conditions. Furthermore, the sensitivity study results also show our protocol can tolerate reasonable disturbances.}
}


@article{DBLP:journals/tmc/KumarGT25,
	author = {Pankaj Kumar and
                  Nikita Goel and
                  Manoj Tolani},
	title = {Drone-Assisted {IRS} System in 5G and Beyond: Improving Reliability
                  and Enhancing the Network Life Span},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8379--8392},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556043},
	doi = {10.1109/TMC.2025.3556043},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KumarGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an drone-assisted intelligent reflecting surface (IRS) for device-to-device (D2D) communication in infrastructure-less scenarios. The aim of this paper is to enhance the reliability among D2D ground users (GUs) and extend the lifespan of 5G and beyond 5G (B5G) wireless communication system. This work may be applicable for packet delivery in bustling urban areas, especially where ground-to-ground (G2G) links are in deep fade. For modeling air-to-ground (A2G) links among GUs to IRS/drone and IRS/drone to GUs, we consider a height-dependent Nakagami- m m  channel model for small-scale fading and height-dependent path-loss exponent for modeling large-scale fading. The lifespan of the network is improved by proposing height-dependent energy harvesting (EH) at drone. We derive the cumulative distribution function (CDF) of the signal-to-noise ratio (SNR) whenever the signal reaches the receiving node, either via drone or via IRS. We also develop the expression of spectral efficiency and derive a closed-form expression of an outage probability by taking the combined effect of the signal for the proposed scenario using decode-and-forward (DF) and amplify-and-forward (AF) relaying at the drone. Additionally, the statistical parameters such as mean, variance, and probability density function (PDF) of total noise are derived, which is useful at the receiver node for estimating the bit error rate (BER). The analytical result is validated with simulation results, and the work is compared with the existing state-of-the-art.}
}


@article{DBLP:journals/tmc/LiD25a,
	author = {Hongbo Li and
                  Lingjie Duan},
	title = {Competitive Multi-Armed Bandit Games for Resource Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8393--8404},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3555971},
	doi = {10.1109/TMC.2025.3555971},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiD25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our theoretical study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new $N$-player $K$-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on the same arms and the time-varying nature of arm rewards make the policy analysis here more involved than the existing studies for myopic players. We explicitly analyze the threshold-based structures of the social optimum and the existing selfish policy, showing that the latter causes prolonged convergence times $\\Omega (\\frac{K}{\\eta ^{2}}\\ln ({\\frac{KN}{\\delta }}))$, while the socially optimal policy with coordinated communication reduces it to $\\mathcal {O}(\\frac{K}{N\\eta ^{2}}\\ln {(\\frac{K}{\\delta })})$. Based on the policy comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to the social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their diverse and time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for the social planner and ensures truthful reporting from players, thereby achieving the minimum $\\text{PoA}=1$ and the same convergence time as the social optimum.}
}


@article{DBLP:journals/tmc/LuLLXWLLY25,
	author = {Xinyu Lu and
                  Jiong Lou and
                  Jie Li and
                  Runhui Xu and
                  Chentao Wu and
                  Zhi Liu and
                  Yuan Luo and
                  Yang Yang},
	title = {Towards Bi-Level Supply/Demand Balanced Charging Systems via Online
                  Power Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8405--8422},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558550},
	doi = {10.1109/TMC.2025.3558550},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuLLXWLLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of transportation electrification, an increasing number of charging stations have been established, forming a city-scale charging system. These charging stations serve as intermediaries that connect supply and demand, drawing power from the grid and renewable energy sources to provide electricity to electric vehicles. Maintaining a delicate balance between supply and demand has emerged as a significant challenge for the charging system. On a macroscopic level, it impacts the power grid’s peak load and reliability, while locally, it influences electric vehicle detour events. To comprehensively model the spatio-temporal characteristics in the charging system, we partition the charging system by adopting a supply-demand-aware approach and propose OPS, an online power scheduling algorithm based on the regularization technique. OPS aims to achieve a bi-level balance between supply and demand while constraining the power output of the charging system. We substantiate the efficacy of OPS through rigorous theoretical proofs, demonstrating its comparability to the optimal solution. Furthermore, we conduct extensive evaluation experiments with real-world data sets to establish the feasibility of the proposed methodology in alleviating the supply-demand imbalance. The results indicate that OPS attains an empirical competitive ratio of less than 1.2.}
}


@article{DBLP:journals/tmc/ZhuHQLSBAT25,
	author = {Yuanwei Zhu and
                  Yakun Huang and
                  Xiuquan Qiao and
                  Xiaoli Liu and
                  Xiang Su and
                  Anna Brunstr{\"{o}}m and
                  {\"{O}}zg{\"{u}} Alay and
                  Sasu Tarkoma},
	title = {FPSelector: {A} Flexible Path Selector for Mobile Augmented Reality
                  Offloading},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8423--8440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556473},
	doi = {10.1109/TMC.2025.3556473},
	timestamp = {Thu, 06 Nov 2025 15:00:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHQLSBAT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Augmented Reality (MAR) applications pose unique challenges due to computation intensity, constrained device resources, and high interactive rendering requirements. The emergence of 5G and edge computing offers opportunities to offload computation to the edge and cloud, indirectly enhancing the computing capability and usage duration of MAR devices. However, existing general task offloading and multipath transmission techniques do not address the challenges in offloading path selection with multiple edges, dynamic resource competition awareness, and spatial computation with strong task dependencies. This paper contributes FPSelector, a flexible path selector for MAR offloading. We present a two-tier MAR-specific offloading scheme with multiple edge nodes. In offloading decisions, we design a reinforcement learning model to generate the selection policy for each packet of an AR data stream. This model incorporates an action masking mechanism, a comprehensive reward function, and state features complemented by a resource prediction module, making FPSelector aware of dynamic heterogeneous environments. Moreover, we propose an online learning strategy to facilitate real-time selection. To validate its efficacy, we compare FPSelector’s performance against leading schedulers under various scenarios, demonstrating a notable reduction of 9.9% and 9.6% in overall completion time for 4 K and 8 K video-based MAR applications compared to its closest competitor.}
}


@article{DBLP:journals/tmc/WengLZLSH25,
	author = {Wuwei Weng and
                  Jiguo Li and
                  Yichen Zhang and
                  Yang Lu and
                  Jian Shen and
                  Jinguang Han},
	title = {Efficient Registered Attribute Based Access Control With Same Sub-Policies
                  in Mobile Cloud Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8441--8453},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556279},
	doi = {10.1109/TMC.2025.3556279},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WengLZLSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ciphertext-policy attribute-based encryption (CP-ABE) has long been considered as a promising access control technology for cloud storage. However, CP-ABE depends on a central trusted authority to generate and distribute decryption keys, resulting in the key escrow issue. Most existing solutions only mitigate this problem but fail to resolve it entirely. Registered attribute-based encryption (RABE), a new cryptographic primitive, fundamentally addresses the key escrow problem by modifying the trust model, but its high computational overhead limits its practical application. Inspired by this challenge, we present an efficient registered attribute-based access control scheme designed for data encrypted with access policies containing the same sub-policy. In our scheme, users generate their own keys, while a key manager, who does not hold keys, replaces the central authority in managing users. Additionally, for data encrypted with the same sub-policy, the user’s initial decryption stores the relevant parameters, which can be used for subsequent decryptions to reduce computational overhead. The proposed scheme is proven to achieve semantic security. Performance analysis demonstrates that our scheme enhances decryption efficiency by roughly 41.4 % \\%  compared to existing RABE scheme, with a minimal storage trade-off, making it more practical for cloud storage application.}
}


@article{DBLP:journals/tmc/WangGLDXZ25,
	author = {Lei Wang and
                  Tao Gu and
                  Jingyu Li and
                  Haipeng Dai and
                  Chenren Xu and
                  Daqing Zhang},
	title = {Acoustic Sensing for Multi-User Heartbeat Monitoring Using Dualforming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8454--8474},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558418},
	doi = {10.1109/TMC.2025.3558418},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangGLDXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic sensing for heartbeat monitoring has emerged as a prevailing research topic in wireless sensing. However, existing acoustic sensing systems face two limitations: a restricted sensing range and operation limited to a single user, impeding large-scale deployment of its applications. In this paper, we present DF-Sense, a Dual Forming based multi-user acoustic Sensing system for heartbeat monitoring in home settings. Specifically, we design a novel sensing signal-to-noise ratio (SSNR) enhancement model, namely Dualforming, which leverages constructive superposition across multiple subcarriers and microphones. To facilitate Dualforming, we propose a novel MUltiple Subtle SIgnal Classification (MUS 2 ^{2} 2IC) method and a 2-D peak identification scheme to locate and identify multiple subjects with subtle motions. Additionally, we propose a phase change-based method to promptly identify body leaning and adaptively re-localize subjects, thereby avoiding the high computational cost. Finally, we propose an enhanced recursive least squares (RLS) filter to effectively reconstruct high-quality heartbeat waveforms from Channel Frequency Response (CFR) signals affected by limb movements. Experimental results show that DF-Sense achieves high precision measurement of instantaneous heart rates within a range of 10 m, sufficient for most daily space requirements, and can monitor heartbeat for up to 6 subjects in a 2-D space.}
}


@article{DBLP:journals/tmc/GongZLMYT25,
	author = {Taiyuan Gong and
                  Li Zhu and
                  Yang Li and
                  Shuomei Ma and
                  F. Richard Yu and
                  Tao Tang},
	title = {Edge Intelligence Enhanced Monte Carlo Tree Search for Virtually Coupled
                  Train Set Optimal Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8475--8491},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556143},
	doi = {10.1109/TMC.2025.3556143},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongZLMYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtually Coupled Train Set (VCTS) is an advanced train control technology enabling multiple trains to operate closely through wireless communication, enhancing capacity and operational flexibility. Traditional VCTS control algorithms struggle with complex dynamic models and local optimality, hindering real-time, long-term optimization. This paper proposes an Edge Intelligence (EI) enhanced Monte Carlo Tree Search (MCTS) framework for VCTS Optimal Control (M-VOC). MCTS is a heuristic search algorithm that identifies optimal operational solutions efficiently, focusing on long-term stability over local optimums. EI supports MCTS for real-time decision-making, and we introduce a model-based reinforcement learning algorithm to manage VCTS's complex dynamics. Our framework addresses VCTS control issues in real-time while optimizing long-term benefits. To meet computational and real-time demands, we propose a train-to-edge cooperative computing strategy using multi-intelligence reinforcement learning. Simulations demonstrate that our EI-enhanced MCTS strategy effectively provides cooperative control, ensuring virtually coupled trains operate safely, stably, and punctually with reduced intervals.}
}


@article{DBLP:journals/tmc/WuSWPHCY25,
	author = {Liantao Wu and
                  Peng Sun and
                  Zhibo Wang and
                  Xiaoyi Pang and
                  Jiahui Hu and
                  Honglong Chen and
                  Yang Yang},
	title = {An Incentive Framework for Task Offloading in Edge Computing Marketplaces
                  Under Price Competition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8492--8505},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556516},
	doi = {10.1109/TMC.2025.3556516},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuSWPHCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To efficiently execute tasks, computation resource requesters (CRRs) with limited resources can offload their tasks to nearby computation resource providers (CRPs) with spare computing capacity. These CRPs require appropriate incentives to compensate for their incurred costs when helping process the offloaded tasks. Although several mechanisms have been designed to incentivize CRPs, none of them have investigated the incentive mechanism considering price-setting and price-taking CRPs simultaneously. In this work, we propose an incentive framework for task offloading in the edge computing marketplace that includes both price-setting and price-taking CRPs. We model the CRR's interactions with both types of CRPs as a three-stage Stackelberg game to maximize the profit for both the CRR and CRPs. We prove the existence of a unique subgame perfect equilibrium (SPE) of the formulated game and further develop iterative algorithms for the CRR and price-setting CRPs to achieve the equilibrium. Through the designed algorithms, each CRP does not require complete information about the CRR and other CRPs. Extensive simulations demonstrate that offloading tasks to both price-setting and price-taking CRPs achieves higher profits for the CRR and price-setting CRPs compared to offloading tasks solely to price-setting CRPs. Additionally, the obtained SPE can achieve near-optimal social welfare.}
}


@article{DBLP:journals/tmc/LiuSX25,
	author = {Xiangyong Liu and
                  Xuesong Sun and
                  Zhiqiang Xu},
	title = {Mobile Parcels' Grasping Detection System by the Neuromorphic
                  Vision and Efficient Fusion Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8506--8519},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556735},
	doi = {10.1109/TMC.2025.3556735},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuSX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing popularity of online shopping has resulted in a surge of parcels that need to be sorted, which exerts great challenges to the sorting work. Robotic grasp can greatly improve the sorting efficiency. However, the dynamic grasp of moving parcels requires higher detection speed and grasping pose calculation accuracy. To address these requirements, this study proposes a new grasping system through the Neuromorphic vision (NeuroIV), which owns the advantages of low latency and lightweight computing. As a young field, Neuromorphic camera is rarely used in robotic grasp. In view of this, we present a novel parcel-grasping dataset. After that, a double channels’ down-sampling and grasping network (DCDG-Net) is designed, which can extract abundant features with ResNet and transformer branches, respectively. To mitigate the calculation burden introduced by the dual channels' network, we design a feature-vector multiplication to replace the dot-product multiplication, thereby reducing the computational load among different matrixes. Furthermore, channel and space attentions are fused to construct multidimensional network to suppress noisy features and highlight useful information. Finally, we have evaluated the proposed method in real-world scenarios. Together with qualitative and quantitative comparisons, this work provides a state-of-the-art grasping detection with the new NeuroIV dataset and network.}
}


@article{DBLP:journals/tmc/WangFZST25,
	author = {Yuhong Wang and
                  Shaohan Feng and
                  Yonghong Zeng and
                  Sumei Sun and
                  Peng Hui Tan},
	title = {Toward Real-Time Digital Twin of Physical Reality via Intelligent
                  Wireless Resource Allocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8520--8531},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557867},
	doi = {10.1109/TMC.2025.3557867},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangFZST25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enhanced Mobile Broadband (eMBB) and Ultra Reliable Low Latency Communication (URLLC) are two important wireless communication traffics to build a digital twin of physical reality. Therein, eMBB and URLLC traffics are to transmit high-quality sensed data and critical commands, respectively. To support these two important traffics, we develop an intelligent resource allocation mechanism. First, we model the time-frequency resource allocation as an optimization problem aiming to maximize the throughput for the eMBB traffics according to their urgency subject to the constraint on the successful transmission for the URLLC traffics. In this way, the amount of resources allocated to each traffic can be appropriately determined without causing waste in resource usage. Second, we propose a feasible low-complexity solution for the optimization problem by relaxing it and then applying linear programming. Third, to address the possible failure of the algorithm due to the relaxation, we propose a post-processing by puncturing the resource initially allocated to eMBB traffics and thereafter reallocating this resource to URLLC traffics. By such, the characteristic of the eMBB traffic, i.e., high throughput, and that of the URLLC traffic, i.e., low latency and ultra reliability, can be achieved. We perform system-level simulations on Matlab 5 G simulation platform to evaluate the performance of the proposed mechanism under different scenarios. Simulations show that the proposed mechanism achieves better performance compared to existing schemes regarding the total eMBB throughput and URLLC failure probability on all the scenarios.}
}


@article{DBLP:journals/tmc/RenXLLYLL25,
	author = {Yanzhi Ren and
                  Zhiliang Xia and
                  Siyi Li and
                  Hongbo Liu and
                  Jiadi Yu and
                  Shuai Li and
                  Hongwei Li},
	title = {User Authentication on Smart Speakers Leveraging Acoustic Imaging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8532--8548},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556404},
	doi = {10.1109/TMC.2025.3556404},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenXLLYLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The user authentication has drawn increasingly attention as the smart speaker becomes more prevalent. For example, smart speakers that can verify who is sending voice commands can mitigate various types of attacks, such as replay attack or impersonation attack. Existing user authentication solutions either cannot be applicable to smart speakers directly or require certain additional user-device interaction or pre-installed infrastructure, which may severely affect the user experience and create extra burdens to users. In this work, we propose a user authentication system utilizing acoustic images, which are derived from the smart speaker by emitting beep signals and sensing echoes from the user’s body with its microphone array, as the proof for user authentication. Given the acoustic samplings of the reflected beep signal, our system designs a distance estimation component by applying a correlation based technique on the beamformed signal to estimate the distance between the user and microphone array. Our image construction component then constructs a virtual imaging plane using the estimated distance and steers the array towards each grid of the plane to generate an acoustic image of the user. We also propose a transfer learning-based method to derive efficient features from the constructed images, and employ SVM classifiers for accurate user authentication. Moreover, to ensure the accuracy of our system, our user direction estimation scheme could further estimate the DoA of the user’s voice command to make sure that the user stands in front of the smart speaker during the authentication process. Our extensive experiments demonstrate that our system is robust and accurate across various scenarios.}
}


@article{DBLP:journals/tmc/ChenSLAWL25,
	author = {Yali Chen and
                  Sheng Sun and
                  Min Liu and
                  Bo Ai and
                  Yuwei Wang and
                  Yunhao Liu},
	title = {Energy-Efficient Over-the-Air Computation in UAV-Assisted IIoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8549--8563},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556382},
	doi = {10.1109/TMC.2025.3556382},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenSLAWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In remote industrial Internet of Things (IIoT) monitoring systems, the uncrewed aerial vehicle (UAV) serves as supplementary infrastructure to aggregate data from a large number of distributed sensors, and achieve industrial operation intelligence. In the wireless data aggregation process, using conventional orthogonal multiple access techniques face challenges such as scarce bandwidth, high communication latency and energy consumption. To tackle these issues, the over-the-air computation (AirComp) technique has emerged. It allows concurrent data transmissions from sensors, as well as integrates communication and computation processes, ultimately enabling fast data aggregation. However, the energy consumption issue remains unresolved. In this paper, we exploit spatial correlations among sensor measurements, and design an energy-efficient AirComp in UAV-assisted IIoT networks, where only a subset of sensors transmit data instead of all sensors. Then, we derive a closed-form expression for the mean square error (MSE) of each combination under a specific number of sensor transmissions. By jointly optimizing the UAV deployment and pre-coding coefficients of sensors, we formulate the problem of minimizing MSE for each combination of transmitted sensors. Furthermore, the MSE optimization algorithm is developed to output the average MSE of all combinations. Finally, we evaluate the average MSE and network lifetime performance of proposed scheme.}
}


@article{DBLP:journals/tmc/KommaTBYBK25,
	author = {Demba Komma and
                  Chien{-}Wei Tseng and
                  Andrea Bejarano{-}Carbo and
                  Mingyu Yang and
                  David T. Blaauw and
                  Hun{-}Seok Kim},
	title = {NBLoc: {A} Narrowband {RF} Localization System for Wide-Area Indoor
                  Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8564--8581},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556743},
	doi = {10.1109/TMC.2025.3556743},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KommaTBYBK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce NBLoc, a narrowband frequency hopping, long-range localization system designed for low-power Internet of Things (IoT) devices. Traditional high-accuracy localization systems typically require wide-bandwidth and power-demanding radio frequency (RF) circuits, leading to limitations such as short operational range and high power consumption to achieve decimeter-level accuracy. NBLoc overcomes these challenges by using narrowband symbols with a frequency-hopping mechanism across a wide bandwidth, enabling the localization of low-power tags over large areas. NBLoc features a novel custom-designed RF analog frontend (AFE) integrated circuit (IC), that eliminates the need for a conventional phase-locked loop, significantly reducing the cost and power consumption of the receiving tag. This advancement is enabled by NBLoc's thoughtful waveform design and specialized signal processing algorithms, which mitigate phase noise and uncertainty. Compared to previous solutions, NBLoc achieves lower power consumption and extended operational range due to its narrowband symbols while maintaining high localization accuracy by leveraging a 100 MHz localization bandwidth through frequency hopping. In NBLoc, system anchors transmit narrowband orthogonal symbols, hopping across the localization bandwidth in a predetermined pattern known to the tag. The tag, equipped with the custom low-power RF AFE IC, dynamically tunes its local oscillator (LO) frequency to match the hop pattern and capture these symbols, which are then used to estimate the channel impulse response (CIR). The tag calculates the time difference of arrival (TDOA) for each anchor pair from the CIRs, and determines its 2D location via multilateration. The system was implemented and tested using the low-power RF AFE IC in both line-of-sight (LOS) and non-line-of-sight (NLOS) environments, achieving decimeter-level accuracy across areas as large as 269 × 125 m 2 ^{2} .}
}


@article{DBLP:journals/tmc/XuSPWTH25,
	author = {Dianlei Xu and
                  Xiang Su and
                  Gopika Premsankar and
                  Huandong Wang and
                  Sasu Tarkoma and
                  Pan Hui},
	title = {Dynamic Hierarchical Reinforcement Learning Framework for Energy-Efficient
                  5G Base Stations in Urban Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8582--8599},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557280},
	doi = {10.1109/TMC.2025.3557280},
	timestamp = {Thu, 06 Nov 2025 15:00:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuSPWTH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The energy consumption of 5G base stations (BSs) is significantly higher than that of 4G BSs, creating challenges for operators due to increased costs and carbon emissions. Existing solutions address this issue by switching off BSs during specific periods or forming cooperation coalitions where some BSs deactivate while others serve users. However, these approaches often rely on fixed geographic configurations, making them unsuitable for urban areas with numerous BSs and mobile users. To tackle these challenges, we propose a hierarchical reinforcement learning (RL) framework for energy conservation in large-scale 5G networks. In the upper-layer, we propose a deep Q-network integrated with a graph convolutional network that dynamically groups BSs into coalitions from a macro perspective. This layer focuses on high-level coalition formation to optimize system-wide energy efficiency by considering the global state of the network. In the lower-layer, we combine attention mechanism with multi-agent RL and graph convolutional networks to design a scalable algorithm that maximizes local energy efficiency through optimizing the cooperation within each coalition. These two layers align global coalition dynamics with local intra-coalition cooperation to achieve system-wide energy optimization. Moreover, we accurately model large-scale urban 5G scenarios leveraging a high-fidelity network simulator, which enables our RL framework to learn from real-world feedback. Extensive experiments conducted with the simulator demonstrate that our proposed framework achieves remarkable energy savings of up to 75.6%, significantly outperforming baseline approaches. These findings highlight the effectiveness and superiority of our hierarchical RL optimization framework in addressing the energy consumption challenges faced by large-scale 5G networks.}
}


@article{DBLP:journals/tmc/YiCWZ25,
	author = {Peini Yi and
                  Wenchi Cheng and
                  Jingqing Wang and
                  Wei Zhang},
	title = {RIS-Assisted Seamless Connectivity in Wireless Multi-Hop Relay Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8600--8611},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557676},
	doi = {10.1109/TMC.2025.3557676},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YiCWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, reconfigurable intelligent surfaces (RIS) have garnered significant attention for their ability to control the phase shifts in reflected signals. By intelligently adjusting these phases, RIS can establish seamless direct paths between communication devices obstructed by obstacles, eliminating the need for forwarding and significantly reducing system overhead associated with relaying. This capability is crucial in multi-hop ad hoc networks requiring multiple relay steps. Consequently, the concept of incorporating multi-hop RIS into wireless multi-hop relay networks has emerged. In this paper, we propose a novel network model where each UAV communication node is equipped with a RIS, facilitating seamless connections in multi-hop relay wireless networks. We analyze the performance of this model by integrating RIS-assisted physical layer modeling into the seamless connection network framework and conducting a detailed comparative analysis of RIS-assisted and conventional connections. At the medium access layer, we introduce a RIS-DCF MAC protocol based on the IEEE 802.11 distributed coordination function (DCF), modeling the medium access process as a two-hop access scenario. Our results demonstrate that the seamless connections and diversity gain provided by RIS significantly enhance the performance of multi-hop relay wireless networks.}
}


@article{DBLP:journals/tmc/HuDZC25,
	author = {Bintao Hu and
                  Jianbo Du and
                  Jie Zhang and
                  Xiaoli Chu},
	title = {Computation Offloading and Resource Allocation in Mixed Cloud/Vehicular-Fog
                  Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8612--8624},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556315},
	doi = {10.1109/TMC.2025.3556315},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuDZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of vehicular user equipment (V-UE) in the Internet-of-Vehicles (IoV) systems, cloud computing alone cannot process all V-UE tasks, especially those latency-sensitive ones. Although static roadside fog nodes have been employed to offload computation from V-UEs, mobile fog nodes carried by vehicles that have the potential to further improve the performance of computation offloading for vehicular tasks have not been sufficiently studied for IoV systems. In this paper, we consider a mixed cloud/vehicular-fog computing (VFC) system that employs vehicle-carried fog nodes (V-FNs) in addition to cloud servers to offload tasks from V-UEs. To minimise the maximum service delay (which includes the transmission delay, queueing delay, and processing delay) among all V-UEs, we jointly optimise the offloading decisions of all V-UEs, the computation resource allocation at all V-FNs, the allocation of resource block (RB) and transmission power for all V-UEs while considering the mobility of V-UEs and V-FNs. The joint optimisation is solved by devising a fireworks algorithm-based offloading decision optimisation scheme, in conjunction with a bisection method-based V-FN computation resource allocation scheme and a clustering-based communication resource allocation scheme. Simulation results show that our proposed schemes outperform the benchmarks in terms of service the maximum delay among all V-UEs.}
}


@article{DBLP:journals/tmc/HuangLXXWHL25,
	author = {Tianze Huang and
                  Qing Li and
                  Chenren Xu and
                  Mengwei Xu and
                  Shangguang Wang and
                  Gang Huang and
                  Xuanzhe Liu},
	title = {From Earth to Orbit: Launch Sequence Optimization for {LEO} Mega-Constellations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8625--8641},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556227},
	doi = {10.1109/TMC.2025.3556227},
	timestamp = {Tue, 18 Nov 2025 13:49:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLXXWHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent emergence of Low Earth Orbit (LEO) mega-constellations, designed for high-speed broadband connections with low latency, has introduced new deployment challenges. Efficient launch sequence planning is crucial for rapid service rollout, performance enhancement, and service promotion. However, existing research predominantly focuses on the design and performance analysis of fully-deployed constellations and overlooks the evolving process from a partially-deployed constellation to a fully-deployed one. This paper explores the launch sequence optimization problem for mega-constellations, tailored to expedite service delivery and adapt to changing performance demands. To this end, (1) we identify critical network performance metrics for the constellation evolving process and construct a simulation toolchain capable of simulating and evaluating these metrics for any potential partially-deployed constellation. (2) Drawing upon three key observations on network availability, the number of visible satellites, and latency, we propose an algorithm that can construct a launch sequence for an arbitrary mega-constellation topology. Evaluation results show that this algorithm enables the early provision of services and maximizes network performance gains at each launch batch while catering to different user demands. For instance, our algorithm can achieve network performance nearly equivalent to that of Starlink when it initiated its service, without losing redundancy, while using 55% fewer satellites.}
}


@article{DBLP:journals/tmc/LiWSCD25,
	author = {Liang Li and
                  Haiqin Wu and
                  Jiachen Shen and
                  Zhenfu Cao and
                  Xiaolei Dong},
	title = {PGVMatch: Privacy-Preserving and Fine-Grained Crowdsourcing Task Matching
                  With Lightweight On-Chain Public Verifiability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8642--8655},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556249},
	doi = {10.1109/TMC.2025.3556249},
	timestamp = {Wed, 26 Nov 2025 13:39:04 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiWSCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure task matching has been a crucial research problem in crowdsourcing, requiring the alignment of workers’ preferences and requesters’ task requirements while ensuring user privacy and matching integrity. Recently, some researchers applied blockchain to crowdsourcing, either replacing the platform for decentralization or recording proofs for public verification to defend against malicious platforms. However, they still suffer from unitary coarse-grained matching models or expensive on-chain costs. To address these limitations, we propose PGVMatch, a privacy-aware and fine-grained crowdsourcing task-matching scheme with lightweight on-chain public verifiability. Our scheme is constructed on our newly proposed cryptographic primitive–Multi-authority Attribute-Based Keyword Search with Public Verifiability (MABKS-PV), which avoids access policy leakage and key escrow risks on a single authority, meanwhile adding constant-size proof generation and lightweight verification algorithms to a basic ABKS construction. In PGVMatch, requesters can select workers with fine-grained attribute demands, and workers can pick interested tasks with multi-keyword search, preserving dual-side privacy. The matching process is conducted off-chain, while constant-size proofs are recorded on-chain for efficient and public verification of matching integrity. Security analysis and extensive experiments on the Hyperledger Fabric blockchain demonstrate both the security and our superior performance. PGVMatch outperforms the existing scheme with the fastest matching result verification, achieving a 29% improvement in throughput and a 33% reduction in latency.}
}


@article{DBLP:journals/tmc/YoonCNGL25,
	author = {Hyungjun Yoon and
                  Hyeongheon Cha and
                  Hoang C. Nguyen and
                  Taesik Gong and
                  Sung{-}Ju Lee},
	title = {From Vision to Motion: Translating Large-Scale Knowledge for Data-Scarce
                  {IMU} Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8656--8667},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3556998},
	doi = {10.1109/TMC.2025.3556998},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YoonCNGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pre-training representations acquired via self-supervised learning could achieve high accuracy on even tasks with small training data. Unlike in vision and natural language processing domains, pre-training for IMU-based applications is challenging, as there are few public datasets with sufficient size and diversity to learn generalizable representations. To overcome this problem, we propose IMG2IMU that adapts pre-trained representation from large-scale images to diverse IMU sensing tasks. We convert the sensor data into visually interpretable spectrograms for the model to utilize the knowledge gained from vision. We further present a sensor-aware pre-training method for images that enables models to acquire particularly impactful knowledge for IMU sensing applications. This involves using contrastive learning on our augmentation set customized for the properties of sensor data. Our evaluation with four different IMU sensing tasks shows that IMG2IMU outperforms the baselines pre-trained on sensor data by an average of 9.6%p F1-score, illustrating that vision knowledge can be usefully incorporated into IMU sensing applications where only limited training data is available.}
}


@article{DBLP:journals/tmc/GaoWZLYPL25,
	author = {Jing Gao and
                  Die Wu and
                  Linglin Zhang and
                  Jingwen Li and
                  Jin Yang and
                  Jian Peng and
                  Tang Liu},
	title = {Utilizing Multipath Effects for Mobile Charging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8668--8682},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557899},
	doi = {10.1109/TMC.2025.3557899},
	timestamp = {Thu, 11 Sep 2025 20:24:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoWZLYPL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Wireless Rechargeable Sensor Networks (WRSNs) have emerged as a promising solution to address the energy limitations of wireless sensor networks. In practical applications of WRSNs, environmental objects are ubiquitous, reflecting radio waves and causing them to reach sensors via multiple paths. These multipath effects significantly impact the power intensity received by sensors. In this paper, we study a fundamental issue of charGing schEduling with mulTipath effectS (GETS), that is, how to schedule a mobile charger by comprehensively considering the multipath effects to maximize the overall charging utility. To this end, we first establish a charging model with environmental objects to investigate the impact of multipath effects on power distribution. Then, we propose a charging scheduling scheme that not only selects a series of sojourn locations for the MC (Mobile Charger) to maximize the total power received by nearby sensors but also construct a charging path that avoids environmental objects. We conduct extensive simulations as well as indoor and outdoor field experiments to evaluate the performance of our scheme. The results demonstrate that, on average, our scheme outperforms baseline algorithms by 48.87% .}
}


@article{DBLP:journals/tmc/QiuNLDZGWQDSCXTQ25,
	author = {Chenyang Qiu and
                  Guoshun Nan and
                  Ruiwen Liang and
                  Wendi Deng and
                  Yufan Zhang and
                  Yuchong Gao and
                  Di Wang and
                  Meng Qu and
                  Zhuoran Duan and
                  Qianlong Sun and
                  Qimei Cui and
                  Xiaodong Xu and
                  Xiaofeng Tao and
                  Tony Q. S. Quek},
	title = {Plugging and Breathing on the Air: {A} Practical Defense System for
                  Deep Learning-Based Wireless Semantic Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8683--8699},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558793},
	doi = {10.1109/TMC.2025.3558793},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiuNLDZGWQDSCXTQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning-based semantic communications (DLSC) leverage deep neural networks in transmitters and receivers, pushing the boundaries beyond Shannon limit. However, DLSC is extremely vulnerable to malicious physical-layer adversarial attacks due to the openness of wireless channels. Meanwhile, existing defense approaches still suffer from two challenges for robust DLSC. First, most methods require offline DLSC retraining to defend against various attacks, causing interruptions of online service. Second, they struggle to achieve effective defense in real-world time-varying channels, thus limiting DLSC reliability. We propose PBNet, integrating a pluggable protector and an adaptive protector to respectively address the above two challenges. First, the pluggable protector utilizes a novel denoising module to safeguard the transmitted signals, enabling hot-pluggable deployment without interrupting communication. Second, the adaptive protector leverages a novel alternating adaption strategy to achieve effective defense in time-varying channels, ensuring robust performances under real-world dynamic conditions. Evaluations involving symbols, images, texts, and speeches show the efficacy of our PBNet, which has respectively achieved an impressive 72.22% and 73.71% accuracy improvement in defending against unknown  l 0 l_{0} -norm and  l 2 l_{2} -norm attacks on image-based DLSC. Furthermore, we developed two real-world radio systems of PBNet to perform over-the-air signal generation, integrating hardware and software such as FPGA chips and GNU radio. We also implemented an interactive UI of PBNet based on QT5, aiming to demonstrate the effect of attacks and defense visually. This work achieves robust DLSC performances under various attacks and time-varying channels, taking a significant step towards the practical defense scheme for robust DLSC.}
}


@article{DBLP:journals/tmc/WuLZSSLZ25,
	author = {Haijie Wu and
                  Weiwei Lin and
                  Haotong Zhang and
                  Fang Shi and
                  Wangbo Shen and
                  Keqin Li and
                  Albert Y. Zomaya},
	title = {Container Scheduling Strategy Based on Image Layer Reuse and Sequential
                  Arrangement in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8700--8713},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557160},
	doi = {10.1109/TMC.2025.3557160},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuLZSSLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Mobile Edge Computing (MEC) scenarios, computational tasks are popularly deployed using containerization to isolate the runtime environment. To complete the execution of the task, the edge server first pulls the image, then instantiates and runs the container. Since it takes a lot of time for the edge server to download the image from the cloud, image reuse reduces the pulling latency significantly. However, the limited storage capacity of edge servers hinders image reuse. Recent works have enhanced reuse efficiency by leveraging the hierarchical structure of images and caching high-value layers. However, their efficiency remains limited due to the lack of multi-container collaboration. This paper proposes a novel container scheduling strategy based on image layer reuse and sequence arrangement (ILR-SA) for MEC scenarios, which achieves efficient scheduling by collaborating multiple containers. First, containers are greedily deployed into the edge cluster. Then, the execution sequence of containers is modeled as an optimal Hamiltonian path problem, efficiently solved by our proposed decomposition algorithm. Finally, an efficient image layer update strategy is used to achieve layer reuse. We conduct rigorous experiments to demonstrate that our proposed container scheduling strategy reduces the computational task completion time by up to 91.3% compared to existing approaches.}
}


@article{DBLP:journals/tmc/ZhangYGBYC25,
	author = {Jiale Zhang and
                  Longhao Yi and
                  Xiaofeng Gao and
                  Shahzad Sarwar Bhatti and
                  Ting Yuan and
                  Guihai Chen},
	title = {Task Scheduling Mechanism for Crowdsourcing in Mobile Social Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8714--8728},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558275},
	doi = {10.1109/TMC.2025.3558275},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangYGBYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularization of smart phones, mobile crowdsourcing emerged and gained growing attention in the recent years. Mobile users are now able to conduct complex tasks with the communication between each other. In this paper, we study the task scheduling problem in the mobile crowdsourcing systems based on the spontaneously formed mobile social networks (MSNs). We introduce two crowdsourcing task scheduling problems under this system model, with one problem aiming to minimize the total cost of some crowdsourcing tasks and the other focusing on minimizing the final completion time of the tasks belonging to the same project. We introduce a unified framework to solve the problems and propose two approximation algorithms for these two problems in the offline versions respectively and prove Their approximation ratios accordingly. Based on the two algorithms, we further design two online algorithms to deal with the workers’ dynamism and also analyze their competitive ratios. Finally, we verify the effectiveness and efficiency of the proposed methods through numerical experiments on real and synthetic datasets.}
}


@article{DBLP:journals/tmc/WangJYLLCL25,
	author = {Qipeng Wang and
                  Shiqi Jiang and
                  Yifan Yang and
                  Ruiqi Liu and
                  Yuanchun Li and
                  Ting Cao and
                  Xuanzhe Liu},
	title = {Efficient and Adaptive Diffusion Model Inference Through Lookup Table
                  on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8729--8746},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558203},
	doi = {10.1109/TMC.2025.3558203},
	timestamp = {Tue, 20 Jan 2026 10:38:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangJYLLCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Diffusion models have revolutionized image synthesis applications. Many studies focus on using approximate computation such as model quantization to reduce inference costs on mobile devices. However, due to their extensive model parameters and autoregressive inference fashion, the overhead of diffusion models remains high, which is challenging for mobile devices to handle. To reduce the inference overhead of diffusion models on mobile devices, we propose LUT-Diff, an algorithm-system co-design specifically tailored for mobile device diffusion model inference optimization. LUT-Diff optimizes using lookup tables and can efficiently generate a series of lookup table candidates for diffusion models without end-to-end training. During inference, LUT-Diff adaptively selects the best inference strategy based on the application/user’s latency budget. Additionally, LUT-Diff includes a parallel inference engine that rapidly completes model inference through CPU-GPU co-scheduling. Extensive experiments demonstrate that LUT-Diff can generate images comparable to the original model, with an up to 0.012 MSE in generated images. LUT-Diff can also achieve up to 9.1× inference acceleration and reduce the inference memory footprint by up to 70.9% compared to baseline methods. Moreover, LUT-Diff can save at least 3281× the learning cost of lookup tables.}
}


@article{DBLP:journals/tmc/ZhaoLDWZWXZ25,
	author = {Cui Zhao and
                  Qiumin Luo and
                  Han Ding and
                  Ge Wang and
                  Kun Zhao and
                  Zhi Wang and
                  Wei Xi and
                  Jizhong Zhao},
	title = {mm-Fall: Practical and Robust Fall Detection via mmWave Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8747--8760},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557504},
	doi = {10.1109/TMC.2025.3557504},
	timestamp = {Thu, 06 Nov 2025 14:55:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLDWZWXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Falls pose a significant risk to the health and well-being of older adults, driving the development of various fall detection systems. Existing solutions have explored wearable and vision sensors, while non-invasive RF-based approaches have raised a growing interest due to their convenience and privacy considerations. Despite major advancements in RF-based passive estimation, current approaches still face challenges in handling complex real-world scenarios. They often lack the ability to generalize to new domains (i.e., people, position, environment), and struggle to accurately detect and localize a fallen person in the presence of unknown activities from nearby objects (e.g., pet animal and robot vacuum cleaner) or persons. To address these challenges, we present mm-Fall, a novel mmWave-based non-invasive fall detection system that utilizes Range-Angle (RA) energy maps to separate and localize multiple moving targets, and further accurately estimate their states. Unlike previous approaches, mm-Fall is capable of working with new domains and effectively distinguishing falls from non-fall motions that may appear similar. Additionally, it performs well in challenging conditions, such as poor lighting and occluded scenarios. Our design of mm-Fall is evaluated in 13 environments with over 16 individuals performing 24+ types of motions. The results demonstrate an impressive average recall of 0.969 and precision of 0.996 in detecting falls, whether involving single or multiple moving targets simultaneously. The source codes and dataset of mmFall are available at https://github.com/iwantlatiao/mmFall.}
}


@article{DBLP:journals/tmc/WangZZJLWJS25,
	author = {Xinyu Wang and
                  Wangqiu Zhou and
                  Hao Zhou and
                  Shenyao Jiang and
                  Zhi Liu and
                  Xiaoyan Wang and
                  Yusheng Ji and
                  Qi Song},
	title = {Relip: Reliable In-Band Parallel Communication for Magnetic {MIMO}
                  Wireless Power Transfer System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8761--8779},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558524},
	doi = {10.1109/TMC.2025.3558524},
	timestamp = {Fri, 05 Dec 2025 07:48:05 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZJLWJS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In magnetic resonant coupling (MRC) based wireless power transfer (WPT) systems, receiver (RX) feedback communication is promising to enhance the capability and efficiency of the system. Although some studies have explored in-band implementations with low overhead costs, it has not been comprehensively investigated. In this paper, we propose  R e l i p \\mathtt {Relip} , a Reliable layer-level in-band parallel feedback communication mechanism for MIMO MRC-WPT systems, which addresses the impact of RX-RX couplings (i.e., non-negligible interference from strong couplings and positive effects of relay phenomenon), and provides a theoretical analysis of communication reliability. Technically, we first devise an On-Off based two-phase modulation mechanism to achieve RX identification and dependency detection under relay phenomenon. Then, we utilize observed channel decomposability to collect group-level power transfer channel conditions for eliminating the interference caused by strong RX-RX couplings. Furthermore, we perform RX selection to optimize the trade-off between communication reliability and time overhead. We design and implement the  R e l i p \\mathtt {Relip}  prototype and conduct extensive experiments. The results validate the effectiveness of our mechanism, i.e.,  R e l i p \\mathtt {Relip}  can provide  ≥ \\geq 99% average decoding accuracy for concurrent feedback communication of 14 devices, achieving an 18.31% improvement compared to the state-of-the-art solution.}
}


@article{DBLP:journals/tmc/TangZSDWNH25,
	author = {Xiao Tang and
                  Kexin Zhao and
                  Chao Shen and
                  Qinghe Du and
                  Yichen Wang and
                  Dusit Niyato and
                  Zhu Han},
	title = {Deep Graph Reinforcement Learning for UAV-Enabled Multi-User Secure
                  Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8780--8793},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558790},
	doi = {10.1109/TMC.2025.3558790},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TangZSDWNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While unmanned aerial vehicles(UAVs) with flexible mobility are envisioned to enhance physical layer security in wireless communications, the efficient security design that adapts to such high network dynamics is rather challenging. The conventional approaches extended from optimization perspectives are usually quite involved, especially when jointly considering factors in different scales such as deployment and transmission in UAV-related scenarios. In this paper, we address the UAV-enabled multi-user secure communications by proposing a deep graph reinforcement learning framework. Specifically, we reinterpret the security beamforming as a graph neural network (GNN) learning task, where mutual interference among users is managed through the message-passing mechanism. Then, the UAV deployment is obtained through soft actor-critic reinforcement learning, where the GNN-based security beamforming is exploited to guide the deployment strategy update. Simulation results demonstrate that the proposed approach achieves near-optimal security performance and significantly enhances the efficiency of strategy determination. Moreover, the deep graph reinforcement learning framework offers a scalable solution, adaptable to various network scenarios and configurations, establishing a robust basis for information security in UAV-enabled communications.}
}


@article{DBLP:journals/tmc/DingSWB25,
	author = {Ningning Ding and
                  Zhenyu Sun and
                  Ermin Wei and
                  Randall Berry},
	title = {Incentivized Federated Learning and Unlearning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8794--8810},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557857},
	doi = {10.1109/TMC.2025.3557857},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DingSWB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To protect users’ right to be forgotten in federated learning, federated unlearning aims at eliminating the impact of leaving users’ data on the global learned model. The current research in federated unlearning mainly concentrates on developing effective and efficient unlearning techniques. However, the issue of incentivizing valuable users to remain engaged and preventing their data from being unlearned is still under-explored, yet important to the unlearned model performance. This paper focuses on the incentive issue and develops an incentive mechanism for federated learning and unlearning. We first characterize the leaving users’ impact on the global model accuracy and the required communication rounds for unlearning. Building on these results, we propose a four-stage game to capture the interaction and information updates during the learning and unlearning process. A key contribution is to summarize users’ multi-dimensional private information into one-dimensional metrics to guide the incentive design. Interestingly, we prove that allowing federated unlearning can result in reduced payoffs for both the server and users, compared to a scenario without unlearning. Numerical results demonstrate the necessity of unlearning incentives for retaining valuable leaving users, and also show that our proposed mechanisms decrease the server's cost by up to 53.91% compared to state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/SunPSFLL25,
	author = {Yuxia Sun and
                  Siyi Pan and
                  Aoxiang Sun and
                  Zhixiao Fu and
                  Saiqin Long and
                  Zhetao Li},
	title = {FedLFP: Communication-Efficient Personalized Federated Learning on
                  Non-IID Data in Mobile Edge Computing Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8811--8823},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558406},
	doi = {10.1109/TMC.2025.3558406},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunPSFLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) facilitates computing and storage at edge nodes near user devices, reducing latency and optimizing bandwidth. Federated Learning (FL) complements MEC by enabling privacy-preserving collaborative model training across edge nodes without sharing raw data. However, in MEC environments, FL faces challenges such as communication inefficiency and data heterogeneity (Non-IID), which degrade model performance and hinder convergence. To address these issues, we propose FedLFP, a communication-efficient personalized federated learning approach using label-free prototypes for Non-IID data in MEC. FedLFP employs three key strategies: (1) a Label-Free Prototype strategy to reduce communication costs and mitigate privacy risks, (2) a centroid prototype and combined clustering weight strategy to improve global prototype quality by considering data quantity and confidence levels, and (3) a multifaceted weighted contrastive learning strategy to enhance local representation learning and global alignment. We evaluated FedLFP on Android malware recognition using the KronoDroid dataset and standard image classification tasks, with eight configurations representing practical Non-IID settings. Experimental results show that FedLFP consistently outperforms thirteen state-of-the-art FL methods in accuracy, communication and computational efficiency. Additionally, we provide theoretical guarantees for the convergence of FedLFP under Non-IID conditions.}
}


@article{DBLP:journals/tmc/SongHDG25,
	author = {Yuxiao Song and
                  Daojing He and
                  Minghui Dai and
                  Mohsen Guizani},
	title = {Cost-Efficient and Privacy-Preserving Distributed Learning: {A} Double
                  Layer-Based Auction Design},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8824--8840},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560550},
	doi = {10.1109/TMC.2025.3560550},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongHDG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of Artificial Intelligence of Things (AIoT) has enabled AI-powered services within wireless networks, relying on well-trained machine learning (ML) models. Distributed learning, such as federated learning (FL), allows smart devices (SDs) to collaborate on model training without sharing raw data, but privacy protection is still necessary to prevent potential information leakage from evolving attacks. Additionally, training efficiency is hampered by limited resources and selfishness of SDs. This paper considers a layered distributed learning scenario using a double-layer auction approach, where model users act as buyers, SDs act as data owners contributing their datasets, and edge layer nodes (ELNs) serve as model trainers providing computing resources. The differential privacy (DP) mechanism is utilized to add Gaussian noise to the trained models by the ELNs. Then, we formulate a joint optimization problem to optimize task assignment, data owners’ sensing durations, and model trainers’ local iterations and privacy budgets, aiming to maximize the utility of all participants while ensuring cost-effective and privacy-preserving distributed learning. We decompose the formulated problem into four sub-problems and design a layered algorithm to solve them and derive collaboration strategies. Simulation results validate the algorithm’s performance and demonstrate the advantages of our proposed approach compared to benchmark schemes.}
}


@article{DBLP:journals/tmc/OuyangLL25,
	author = {Jinhao Ouyang and
                  Yuan Liu and
                  Hang Liu},
	title = {A Two-Timescale Approach for Wireless Federated Learning With Parameter
                  Freezing and Power Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8841--8855},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557838},
	doi = {10.1109/TMC.2025.3557838},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OuyangLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables distributed devices to train a shared machine learning (ML) model collaboratively while protecting their data privacy. However, the resource-limited mobile devices suffer from intensive computation-and-communication costs of model parameters. In this paper, we observe the phenomenon that the model parameters tend to be stabilized long before convergence during training process. Based on this observation, we propose a two-timescale FL framework by joint optimization of freezing stabilized parameters and controlling transmit power for the unstable parameters to balance the energy consumption and convergence. First, we analyze the impact of model parameter freezing and unreliable transmission on the convergence rate. Next, we formulate a two-timescale optimization problem of parameter freezing percentage and transmit power to minimize the model convergence error subject to the energy budget. To solve this problem, we decompose it into parallel sub-problems and decompose each sub-problem into two different timescales problems using the Lyapunov optimization method. The optimal parameter freezing and power control strategies are derived in an online fashion. Experimental results demonstrate the superiority of the proposed scheme compared with the benchmark schemes.}
}


@article{DBLP:journals/tmc/ChenXLCM25,
	author = {Xing Chen and
                  Bohuai Xiao and
                  Xinyu Lin and
                  Zheyi Chen and
                  Geyong Min},
	title = {Multi-Agent Collaboration for Vehicular Task Offloading Using Federated
                  Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8856--8871},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3557898},
	doi = {10.1109/TMC.2025.3557898},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXLCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) distributes resources such as computing, storage, and bandwidth to the side close to users, which can provide low-latency services to in-vehicle users, thus promising a more efficient and safer driving environment. However, due to the dynamic scale of vehicle and the variability of resource requirements, it is a significant challenge to quickly obtain effective task offloading in large-scale vehicle scenarios. The existing studies generally adopt the centralized decision-making method, with long decision-making time and high computational overhead, which cannot effectively achieve good offloading decisions in large-scale scenarios. To address these problems, we propose a Multi-agent Collaborative Method for vehicular task offloading using Federated Deep Reinforcement Learning called MCM-FDRL. First, each vehicle as an agent, independently makes offloading decisions based on local information. Next, the offloading decision model of each vehicle is obtained through federated reinforcement learning training. At runtime, an effective vehicle offloading plan can be gradually developed through multi-agent collaboration. Using two real-world datasets, experiments show that the MCM-FDRL has good adaptability and scalability. Moreover, compared to the state-of-the-art methods, the task's average response time of the MCM-FDRL is reduced by 9.75%-64.90%, respectively.}
}


@article{DBLP:journals/tmc/MiaoXZWWYJ25,
	author = {Hao Miao and
                  Ronghui Xu and
                  Yan Zhao and
                  Senzhang Wang and
                  Jianxin Wang and
                  Philip S. Yu and
                  Christian S. Jensen},
	title = {A Parameter-Efficient Federated Framework for Streaming Time Series
                  Anomaly Detection via Lightweight Adaptation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8872--8885},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558964},
	doi = {10.1109/TMC.2025.3558964},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MiaoXZWWYJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of mobile sensing techniques, huge amounts of time series data are continuously generated and accumulated in various domains, fueling considerable real-world mobile computing applications. In this context, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal distribution in time series data. Existing approaches generally assume that all the time series data is available at a central location. However, with the increasing deployment of edge devices, we are witnessing a decentralized collection of time series data. To bridge the gap between decentralized data and centralized anomaly detection algorithms, we propose a Parameter-efficient Federated Anomaly Detection framework via Lightweight Adaptation (PeFAD-LA) that addresses growing privacy concerns. PeFAD-LA innovatively employs a pre-trained large language model (PLM or LLM) as the core of the client’s local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and the local model adaptation cost, we propose a parameter-efficient federated training module that requires clients to fine-tune only small-scale parameters and transmit them to the server for updates. Further, to handle anomalies in streaming time series, a lightweight adaptation module is employed to overcome concept drift. PeFAD-LA utilizes an anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A novel dual knowledge transfer mechanism is designed to harness the useful knowledge across clients and sequentially learned data. Extensive experiments on real data offer evidence of the effectiveness and efficiency of the proposed framework.}
}


@article{DBLP:journals/tmc/QinTTZK25,
	author = {Yeguang Qin and
                  Jie Tang and
                  Fengxiao Tang and
                  Ming Zhao and
                  Nei Kato},
	title = {Multi-Agent Reinforcement Learning in Adversarial Game Environments:
                  Personalized Anti-Interference Strategies for Heterogeneous {UAV}
                  Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8886--8898},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559123},
	doi = {10.1109/TMC.2025.3559123},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QinTTZK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing anti-jamming strategies for unmanned aerial vehicle (UAV) networks largely assume homogeneity among UAVs, neglecting the differences in hardware configurations, task requirements, and environmental adaptability. In the face of such heterogeneity, these strategies often fail to effectively counter intelligent jamming and co-channel interference. To address this issue, this paper proposes an intelligent anti-jamming framework designed specifically for the heterogeneous UAV network, allowing each UAV to autonomously adjust its transmission channel and power based on its hardware capabilities and task requirements in a distributed environment. This aims to optimize communication efficiency and reduce energy consumption. We formulate the anti-jamming problem as an adversarial game and confirm the existence of a unique equilibrium point within this model. Moreover, we introduce the novel Personalized Federated Soft Actor-Critic (PFSAC) algorithm, which combines the global model with local models to customize personalized anti-jamming strategies for each UAV, significantly enhancing network performance in complex jamming environments. Simulation results indicate that compared to other methods, our proposed algorithm significantly enhances the anti-jamming capability of heterogeneous UAV networks and performs better than them.}
}


@article{DBLP:journals/tmc/ShaoQGLXZ25,
	author = {Junming Shao and
                  Xiaoqi Qin and
                  Jian Gao and
                  Yanlin Li and
                  Liang Xin and
                  Ping Zhang},
	title = {Collaborate for Real-Time Gain: Semantic-Based Robotic Communication
                  in 3D Object Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8899--8912},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559584},
	doi = {10.1109/TMC.2025.3559584},
	timestamp = {Tue, 30 Sep 2025 09:14:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShaoQGLXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-agent collaborative perception has emerged as a promising solution to enhance autonomous vehicle perception capabilities, surpassing the limitations of single-agent systems. Utilizing vehicle-to-everything (V2X) communication, this approach inherently results in a trade-off between perception ability and communication costs. However, current research mainly focuses on optimizing this trade-off by selecting spatial critical information, often neglecting the temporal dimension. This paper proposes a novel collaborative perception framework that integrates semantic information across both spatial and temporal dimensions, thereby achieving reduced latency and enhanced perception performance. By leveraging a factorized hyperprior model for feature encoding and applying temporal fusion with Kalman filter predicting, the proposed approach enhances the accuracy and efficiency of object detection. The framework is evaluated on the OPV2V and Dair-V2X datasets, demonstrating significant improvements in the communication-perception trade-off compared to state-of-the-art methods.}
}


@article{DBLP:journals/tmc/DuanMQZCLH25,
	author = {Haihan Duan and
                  Tengfei Ma and
                  Yuyang Qin and
                  Runhao Zeng and
                  Wei Cai and
                  Victor C. M. Leung and
                  Xiping Hu},
	title = {DeRelayL: Sustainable Decentralized Relay Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8913--8929},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558544},
	doi = {10.1109/TMC.2025.3558544},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuanMQZCLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of Big Data, large-scale machine learning models have revolutionized various fields, driving significant advancements. However, large-scale model training demands high financial and computational resources, which are only affordable by a few technological giants and well-funded institutions. In this case, common users like mobile users, the real creators of valuable data, are often excluded from fully benefiting due to the barriers, while the current methods for accessing large-scale models either limit user ownership or lack sustainability. This growing gap highlights the urgent need for a collaborative model training approach, allowing common users to train and share models. However, existing collaborative model training paradigms, especially federated learning (FL), primarily focus on data privacy and group-based model aggregation. To this end, this paper intends to address this issue by proposing a novel training paradigm named decentralized relay learning (DeRelayL), a sustainable learning system where permissionless participants can contribute to model training in a relay-like manner and share the model. In detail, this paper presents the architecture and workflow of DeRelayL, designs incentive mechanisms to ensure sustainability, and conducts theoretical analysis and numerical simulations to demonstrate its effectiveness.}
}


@article{DBLP:journals/tmc/ZhangDLDDWZW25,
	author = {Rongyu Zhang and
                  Xize Duan and
                  Jiaming Liu and
                  Li Du and
                  Yuan Du and
                  Dan Wang and
                  Shanghang Zhang and
                  Fangxin Wang},
	title = {RepCaM++: Exploring Transparent Visual Prompt With Inference-Time
                  Re-Parameterization for Neural Video Delivery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8930--8944},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558259},
	doi = {10.1109/TMC.2025.3558259},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangDLDDWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, content-aware methods have been employed to reduce bandwidth and enhance the quality of Internet video delivery. These methods involve training distinct content-aware super-resolution (SR) models for each video chunk on the server, subsequently streaming the low-resolution (LR) video chunks with the SR models to the client. Prior research has incorporated additional partial parameters to customize the models for individual video chunks. However, this leads to parameter accumulation and can fail to adapt appropriately as video lengths increase, resulting in increased delivery costs and reduced performance. In this paper, we introduce RepCaM++, an innovative framework based on a novel Re-parameterization Content-aware Modulation (RepCaM) module that uniformly modulates video chunks. The RepCaM framework integrates extra parallel-cascade parameters during training to accommodate multiple chunks, subsequently eliminating these additional parameters through re-parameterization during inference. Furthermore, to enhance RepCaM’s performance, we propose the Transparent Visual Prompt (TVP), which includes a minimal set of zero-initialized image-level parameters (e.g., less than 0.1%) to capture fine details within video chunks. We conduct extensive experiments on the VSD4K dataset, encompassing six different video scenes, and achieve state-of-the-art results in video restoration quality and delivery bandwidth compression.}
}


@article{DBLP:journals/tmc/LiFTLYC25,
	author = {Zhenyu Li and
                  Yuchuan Fu and
                  Mengqiu Tian and
                  Changle Li and
                  F. Richard Yu and
                  Nan Cheng},
	title = {FedSTDN: {A} Federated Learning-Enabled Spatial-Temporal Prediction
                  Model for Wireless Traffic Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8945--8958},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559066},
	doi = {10.1109/TMC.2025.3559066},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiFTLYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Traffic Prediction (WTP) plays a significant role in achieving intelligent resource management forcommunication systems. However, WTP still faces challenges such as inaccurate prediction resulting from the complex spatial-temporal characteristics due to user mobility, high communication overhead caused by the complexity of the prediction model, and user privacy issues stemming from Centralized Learning (CL). To address the aforementioned issues, this paper proposes a WTP framework under the Federated Learning (FL) strategy called Federated Spatial-Temporal Dual-attention based Network (FedSTDN). Aiming at improving communication efficiency and simultaneously representing various wireless traffic patterns, a data augmentation-based clustering algorithm is adopted, which groups cells into different regions using a small augmented dataset, facilitating subsequent processing. To improve prediction performance, a local prediction model based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) is proposed to capture the short- and long-term dependencies of traffic. Additionally, a novel Kolmogorov-Arnold Network (KAN) layer is introduced to replace the traditional Multi-Layer Perceptron (MLP) layer, further enhancing prediction performance. Simulations on two different real-world datasets verify the effectiveness and efficiency of FedSTDN. Compared to the well-performing baseline, the proposed FedSTDN achieves up to 32.83% and 24.30% improvements in Mean Square Error (MSE) and Mean Absolute Error (MAE) on the Milan dataset, respectively. For the Trentino dataset, FedSTDN achieves up to 17.25% and 5.86% improvements in MSE and MAE, respectively.}
}


@article{DBLP:journals/tmc/ChenLXRS25,
	author = {Miaojiang Chen and
                  Anfeng Liu and
                  Neal N. Xiong and
                  Yingying Ren and
                  Houbing Herbert Song},
	title = {{ANNS:} An Intelligent Advanced Non-Convex Non-Smooth Scheme for IRS-Aided
                  Next Generation Mobile Communication Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8959--8973},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559099},
	doi = {10.1109/TMC.2025.3559099},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLXRS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enhancing the communication rate and quality has become the primary goal for the development of next-generation mobile communication networks, and traditional techniques such as MIMO and increasing the transmit power of the base station (BS) have not achieved a leapfrog effect. The emergence of Intelligent Reflective Surfaces (IRS) provides more reliable technical support for providing high-energy and high-rate communications. However, IRS-aided joint optimization with communication rate and quality of service constraints is a non-convex non-smooth optimization problem, and the optimal global solution cannot be obtained due to its computational complexity. In this paper, we propose an intelligent Advanced Non-convex Non-smooth Scheme (ANNS) in IRS-aided Next Generation Mobile Communication Networks for making the transmission rate of mission communication and communication quality effective. To ensure that the inequality constraints in the joint optimization problem are not violated and the equation constraints are satisfied, a hybrid deep reinforcement learning and data-experience-driven constraint security layer network is proposed, which maps the constraint violations into the safe feasible domain by mapping the constraint variables into the constraint variables mapping method, and the convergence of the algorithm is theoretically demonstrated. Experimental results show that the proposed ANNS performs superior optimization compared to SAC, DDPG, and A2C for solving non-convex non-smooth problems. The proposed ANNS has the potential to be generalized to other mobile computing applications with non-convex non-smooth characteristics.}
}


@article{DBLP:journals/tmc/HanSWC25,
	author = {Mingqi Han and
                  Xinghua Sun and
                  Xijun Wang and
                  Xiang Chen},
	title = {Foundation Model Enhanced Multiple Access in Heterogeneous Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8974--8987},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558942},
	doi = {10.1109/TMC.2025.3558942},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanSWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-generation multiple access techniques are crucial for providing low-latency and highly efficient data transmission services. Recently, Deep Reinforcement Learning (DRL) has emerged as a prevalent approach in the multiple access domain, aiming to facilitate user coordination and enhance transmission efficiency. However, current DRL approaches face challenges, including limited generalization ability, low sample efficiency, and the complexities associated with Partially Observable Markov Decision Processes (POMDP), which hinder their application in heterogeneous networks with varying numbers of nodes and configurations. In this paper, we propose a foundation model-based multiple access (FMA) algorithm. To address severe POMDP and sample inefficiency issues, we decompose the multiple access problem into two parts: a transmission decision part and a configuration estimation part. We leverage the strong generalization and inference capabilities of the foundation model, utilizing a Deep Learning (DL) approach instead of DRL for training, and adopt the Low-Rank Adaptation (LoRA) technique to fine-tune the foundation model for downstream multiple access tasks. Simulation results demonstrate that: 1) through the decomposition, the FMA approach exhibits sufficient generalization and inference abilities to adapt to various scenarios with various protocols, configurations, and numbers of heterogeneous nodes; 2) by incorporating expert knowledge, the FMA approach can significantly enhance network performance while ensuring certain fairness requirement for heterogeneous nodes.}
}


@article{DBLP:journals/tmc/WangJCKX25,
	author = {Weiyi Wang and
                  Yutao Jiao and
                  Jin Chen and
                  Jiawen Kang and
                  Yuhua Xu},
	title = {Autonomous and Incentivized Wireless Connection for Robust Mobile
                  Blockchain Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {8988--9003},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559007},
	doi = {10.1109/TMC.2025.3559007},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangJCKX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain has been widely implemented as a trusted platform. Previous works mainly focus on the computing capacity of devices while communication factors play a vital role in blockchain performance during dynamic wireless environments. High-speed movement causes frequent wireless connection interruptions and leads to severe performance degradation of blockchain. Besides, resource-constrained mobile devices are unwilling to selflessly contribute their energy and bandwidth for blockchain, hindering applications in dynamic mobile networks. This paper proposes a reverse auction mechanism to incentivize mobile devices to provide robust wireless connections. Devices submit their connection provision and expected rewards as bids. The mobile blockchain system uses smart contracts to autonomously execute the reverse auction to determine winners and allocate payments based on actual connections. We prove that the reverse auction mechanism is Individual Rationality (IR), Incentive Compatibility (IC), and Computational Efficiency (CE), and derive the approximation ratio 2 σ \\sigma  of the mechanism. Extensive simulation results demonstrate that the proposed mechanism decreases up to half the energy and bandwidth consumption, but achieves a similar TPS and stale rate compared to the selfless scheme, where devices contribute all wireless connections for nothing in return. The proposed auction mechanism achieves more than 96% of the optimal social welfare.}
}


@article{DBLP:journals/tmc/FuHLZDZC25,
	author = {Lele Fu and
                  Sheng Huang and
                  Yanyi Lai and
                  Chuanfu Zhang and
                  Hong{-}Ning Dai and
                  Zibin Zheng and
                  Chuan Chen},
	title = {Federated Domain-Independent Prototype Learning With Alignments of
                  Representation and Parameter Spaces for Feature Shift},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9004--9019},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560083},
	doi = {10.1109/TMC.2025.3560083},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FuHLZDZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning provides a privacy-preserving modeling schema for distributed data, which coordinates multiple clients to collaboratively train a global model. However, data stored in different clients may be collected from diverse domains, and the resulting feature shift is prone to the degraded performance of global model. In this paper, we propose a Federated Domain-Independent Prototype Learning (FedDP) method with Alignments of Representation and Parameter Spaces for Feature Shift. Concretely, FedDP aims to eliminate the domain-specific information and explore the pure representations via information bottleneck, thus integrating the local and global domain-independent prototypes, respectively. To align the cross-domain representation spaces, the global domain-independent prototypes serve as the supervised signals to enable local intra-class representations to approach them. Further, to mitigate the divergences of optimization directions between multiple clients induced by the feature shift, the global representations are yielded by the global model on the client-side and guide the learning of local representations, thus unifying the parameter spaces of multiple local models. We derive the theoretical lower bound of the optimization objective based on mutual information, which is transformed into a computable loss. The proposed FedDP can be applied in the scenarios of homogeneous and heterogeneous models. Extensive experiments are conducted on three challenging multi-domain datasets. The experimental results illustrate the superiority of FedDP compared with state-of-the-art federated learning methods.}
}


@article{DBLP:journals/tmc/ChenXJZW25,
	author = {Liqing Chen and
                  Shiguo Xu and
                  Chunhua Jin and
                  Hao Zhang and
                  Jian Weng},
	title = {Partially Hidden Policy Attribute-Based Multi-Keyword Searchable Encryption
                  With Verification and Revocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9020--9035},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558955},
	doi = {10.1109/TMC.2025.3558955},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data security is becoming increasingly critical as outsourced data services flourish. An effective solution for ensuring data confidentiality in the cloud is attribute-based searchable encryption (ABSE). However, most extant ABSE solutions have not been able to simultaneously achieve rich expressive queries, result validation, attribute revocation, and policy hiding, and there are problems such as inflexibility in query methods and access control capabilities, insufficient scalability and practicality, and privacy leakage. In particular, it should be noted that most ABSE schemes fail to promise the privacy security of access policies and are susceptible to offline keyword guessing attack. Hidden policy-ABSE (HP-ABSE) has been proposed to tackle the issue of explicit attribute values in access policies potentially compromising privacy. Nevertheless, in most existing HP-ABSE schemes, an adversary may initiate attribute values guessing attack to speculate on the values of attribute within access strategy. To this end, this paper proposes partially hidden policy attribute-based multi-keyword searchable encryption with verification and revocation (PHP-ABMKSE-VR). PHP-ABMKSE-VR scheme is practical, secure, as well as effective, making it appropriate for real-world scenarios like smart healthcare, as demonstrated by the experimental outcomes and safety analysis.}
}


@article{DBLP:journals/tmc/YangYKZD25,
	author = {Junlin Yang and
                  Jiadi Yu and
                  Linghe Kong and
                  Yanmin Zhu and
                  Hong{-}Ning Dai},
	title = {Toward Open-World-Aware User Authentication Based on Human Bodies
                  Using mmWave Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9036--9049},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562151},
	doi = {10.1109/TMC.2025.3562151},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangYKZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User authentication is evolving with expanded applications and innovative techniques. New authentication approaches utilize RF signals to sense specific human characteristics, offering a contactless and nonintrusive solution. However, these RF signal-based methods struggle with challenges in open-world scenarios, i.e., dynamic environments, daily behaviors with unrestricted postures, and identification of unauthorized users with security threats. In this paper, we present an open-world user authentication system, OpenAuth, which leverages a commercial off-the-shelf (COTS) mmWave radar to sense unrestricted human postures and behaviors for identifying individuals. First, OpenAuth utilizes a MUSIC-based neural network imaging model to eliminate environmental clutter and generate environment-independent human silhouette images. Then, the human silhouette images are normalized to consistent topological structures of human postures, ensuring robustness against unrestricted human postures. Next, fine-grained body features are extracted from these environment-independent and posture-independent human silhouette images using a metric learning model. To eliminate potential security threats that arise from unauthorized users, OpenAuth synthesizes data placeholders for enhancing unauthorized user identification. Finally, a k-NN-based authentication model is constructed to authenticate users’ identities. Experiments in real environments show that the proposed OpenAuth achieves an average authentication accuracy of 93.4% and false acceptance rate (FAR) of 1.8% in open-world scenarios.}
}


@article{DBLP:journals/tmc/TianYLYWF25,
	author = {Xiang Tian and
                  Jiguo Yu and
                  Chuanwen Luo and
                  Dongxiao Yu and
                  Guijuan Wang and
                  Bin Feng},
	title = {Robust Dynamic Broadcasting for Multi-Hop Wireless Networks Under
                  Time-Varying Connectivity and Dynamic {SINR}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9050--9067},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3558815},
	doi = {10.1109/TMC.2025.3558815},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TianYLYWF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Throughput-optimal dynamic broadcasting is an essential cornerstone for the efficient operation of Multi-hop Wireless Networks (MWNs). Most existing algorithms for this problem were developed assuming static interference environments and network connectivity. However, wireless interference environments and network connectivity are inherently time-varying in real-world scenarios, primarily due to uncontrollable interference sources and unreliable links. Such time-varying characteristics make these existing algorithms less robust. In this paper, we study the robust throughput-optimal dynamic broadcasting for MWNs with multi-dimensional time-varying characteristics in terms of interference environments, network connectivity, and data arrival. We model the time-varying link existence states using a random process and characterize the time-varying interference environments through a dynamic variant of the classical Signal-to-Interference-plus-Noise-Ratio (SINR) model. In this variant, the SINR model parameters are dynamically adjusted over time by an adversary. Based on this, we first design a Robust Throughput-optimal Dynamic Broadcast (RTDB) algorithm which makes efficient slot-based max-weight link scheduling, power allocation, and data forwarding decisions in each time slot. We then prove its throughput-optimality in time-varying acyclic directed MWNs under the dynamic SINR model. The effectiveness of RTDB is validated via numerous simulations.}
}


@article{DBLP:journals/tmc/ChenYLZ25,
	author = {Guojun Chen and
                  Xiaojing Yu and
                  Neiwen Ling and
                  Lin Zhong},
	title = {TypeFly: Low-Latency Drone Planning With Large Language Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9068--9079},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3561282},
	doi = {10.1109/TMC.2025.3561282},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in robot planning using large language models (LLMs) have demonstrated significant potential, primarily due to LLMs’ capabilities to understand natural language commands and generate executable plans in various languages. However, in time-sensitive and interactive applications involving mobile robots, particularly drones, the sequential token generation process inherent to LLMs introduces substantial latency, i.e. response time, during the control plan generation. In this paper, we present a system called TypeFly that tackles this latency problem using a combination of a novel programming language called MiniSpec and its runtime to reduce both the response time and generation time for the robot plan. That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, TypeFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpreting. Using a set of challenging drone tasks, we show that design choices made by TypeFly can reduce the average response time to 74% compared to existing works and provide a more consistent user experience, enabling responsive and intelligent LLM-based drone control.}
}


@article{DBLP:journals/tmc/GhaffarKMTSDA25,
	author = {Zahid Ghaffar and
                  Wen{-}Chung Kuo and
                  Khalid Mahmood and
                  Tayyaba Tariq and
                  Salman Shamshad and
                  Ashok Kumar Das and
                  Mohammed J. F. Alenazi},
	title = {A Lightweight and Robust Access Control Protocol for IoT-Based e-Healthcare
                  Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9080--9091},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3561084},
	doi = {10.1109/TMC.2025.3561084},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GhaffarKMTSDA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) devices are crucial components in e-healthcare networks. It enables remote patient health monitoring and facilitates seamless communication among medical sensors, wearable devices, and healthcare providers through public communication channels. Despite these advantages, the use of public communication among medical sensors in e-healthcare networks introduces critical challenges, such as vulnerability to impersonation, physical capture, and ephemeral secret leakage, particularly in resource-constrained environments. In recent years, various access control protocols have been developed to mitigate these risks. However, these protocols often fail to ensure robust security while incurring significant communication and computation overhead. To overcome these limitations, we propose a lightweight and robust access control protocol for IoT-based e-healthcare networks using chaotic maps. We propose a novel protocol that integrates a PUF-based mechanism to mitigate the challenges of physical tampering and cloning attacks in e-healthcare networks. It leverages the inherent uniqueness of PUF and enhances security through the high-entropy properties of chaotic maps. We analyze the proposed protocol informally, which confirms that it significantly bolsters efficiency and security. We also validate the security using the Random or Real (RoR) model. Moreover, we verify the security of the proposed protocol using Scyther. These analyses highlight that the proposed protocol offers robust resistance to numerous attacks, such as impersonation, physical capture, and ephemeral secret leakage. Moreover, we also compare it with existing and relevant protocols. The comparative analysis showcases its superior performance. Notably, the proposed authentication protocol significantly reduces 46.84% computational overhead and decreases 31.30% communication overhead, underscoring its enhanced performance and resource efficiency.}
}


@article{DBLP:journals/tmc/YangXGMKD25,
	author = {Wanting Yang and
                  Zehui Xiong and
                  Song Guo and
                  Shiwen Mao and
                  Dong In Kim and
                  M{\'{e}}rouane Debbah},
	title = {Efficient Multi-User Offloading of Personalized Diffusion Models:
                  {A} DRL-Convex Hybrid Solution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9092--9109},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560582},
	doi = {10.1109/TMC.2025.3560582},
	timestamp = {Thu, 23 Oct 2025 12:57:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangXGMKD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative diffusion models like Stable Diffusion are at the forefront of the thriving field of generative models today, celebrated for their robust training methodologies and high-quality photorealistic generation capabilities. These models excel in producing rich content, establishing them as essential tools in the industry. Building on this foundation, the field has seen the rise of personalized content synthesis as a particularly exciting application. However, the large model sizes and iterative nature of inference make it difficult to deploy personalized diffusion models broadly on local devices with heterogeneous computational power. To address this, we propose a novel framework for efficient multi-user offloading of personalized diffusion models. This framework accommodates a variable number of users, each with different computational capabilities, and adapts to the fluctuating computational resources available on edge servers. To enhance computational efficiency and alleviate the storage burden on edge servers, we propose a tailored multi-user hybrid inference approach. This method splits the inference process for each user into two phases, with an optimizable split point. Initially, a cluster-wide model processes low-level semantic information for each user’s prompt using batching techniques. Subsequently, users employ their personalized models to refine these details during the later phase of inference. Given the constraints on edge server computational resources and users’ preferences for low latency and high accuracy, we model the joint optimization of each user’s offloading request handling and split point as an extension of the Generalized Quadratic Assignment Problem (GQAP). Our objective is to maximize a comprehensive metric that balances both latency and accuracy across all users. To solve this NP-hard problem, we transform the GQAP into an adaptive decision sequence, model it as a Markov decision process, and develop a hybrid solution combining deep reinforcement learning with convex optimization techniques. Simulation results validate the effectiveness of our framework, demonstrating superior optimality and low complexity compared to traditional methods.}
}


@article{DBLP:journals/tmc/HuangMZWCQS25,
	author = {Yakun Huang and
                  Shengwei Meng and
                  Yuanwei Zhu and
                  Jun Wang and
                  Jacky Cao and
                  Xiuquan Qiao and
                  Xiang Su},
	title = {WebARNav: Mobile Web {AR} Indoor Navigation With Edge-Assisted Vision
                  Localization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9110--9125},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560296},
	doi = {10.1109/TMC.2025.3560296},
	timestamp = {Thu, 06 Nov 2025 15:00:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangMZWCQS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The gradual maturation of mobile augmented reality (AR) and localization technologies is enabling the development of immersive AR-enabled indoor localization and navigation systems. Existing indoor localization technologies (e.g., WiFi, infrared, Bluetooth) and navigation services do not provide intuitive 3D AR experiences and can be expensive to deploy. This paper introduces WebARNav, a cross-platform indoor localization system that provides user-friendly AR navigation services with low overhead and remarkable accuracy. First, we propose a lightweight location fusion framework for indoor navigation on the mobile web, which leverages accurate edge-supported vision localization to guide and correct lightweight pedestrian dead reckoning localization. Second, we improve the accuracy of localization using an attention-based feature extraction method and a dual-stream retrieval and co-visibility re-ranking technique for initial localization. Third, we significantly improve accuracy and speed up retrieval as users move by generating a topological map for traveling localization. We conducted extensive experiments on various indoor datasets to demonstrate localization accuracy and navigation experience. The study shows that WebARNav achieves a localization frequency of over 30 Hz and reduces the average trajectory error by 76% and 95% for single- and multi-floor office scenes, respectively, compared to the PDR-only method. The proposed traveling localization method also reduces the localization latency by 15.2%, 55.1%, and 98.6% in the baseline datasets, with an accuracy improvement of over 4%.}
}


@article{DBLP:journals/tmc/LiZPLZW25,
	author = {Guo Li and
                  Jiandian Zeng and
                  Zihao Peng and
                  Yuzhu Liang and
                  James Xi Zheng and
                  Tian Wang},
	title = {{E2EC:} Edge-to-Edge Collaboration for Efficient Real-Time Video Surveillance
                  Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9126--9140},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559919},
	doi = {10.1109/TMC.2025.3559919},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZPLZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In smart cities, Multi-Camera Multi-Target pedestrian tracking and Re-identification (MCMT-ReID) is essential for effective surveillance, particularly in real-time scenarios, as it demands significant computational resources. Current edge-cloud collaboration methods encounter issues such as high latency and potential data leakage due to the physical distance between cloud servers and cameras. To address these issues, we propose a novel Edge-to-Edge Collaboration (E2EC) system that fully utilizes collaboration between heterogeneous edge devices. E2EC partitions the MCMT-ReID task into two modular applications: Tracking and Re-identification (ReID), and employs a customized Kafka communication protocol to optimize data exchange efficiency. Moreover, E2EC dynamically orchestrates intermediate inference flows and transmits features instead of pedestrian detection frames to avoid data leakage. To enhance ReID accuracy, we introduce a real-time ReID Loop Confirmation (ReLC) algorithm, which continuously validates identities to boost reliability and accuracy. E2EC has been deployed and tested in a real-world campus environment to validate its effectiveness. Experimental results demonstrate that E2EC enhances the Rank-1 accuracy and mAP of pedestrian ReID by 36.88% and 46.00%, respectively. Furthermore, it achieves an increase of about 6.35%-12.66% in throughput and reduces latency by 35.01%-57.83% compared to baselines, ensuring real-time performance under dynamic workloads.}
}


@article{DBLP:journals/tmc/WangWCBC25,
	author = {Jiaxing Wang and
                  Jingjing Wang and
                  Jianrui Chen and
                  Lin Bai and
                  Jinho Choi},
	title = {An Efficient Frame Aggregation Scheme for Relay-Aided Internet of
                  Things Networks With Age of Information Constraints},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9141--9152},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560839},
	doi = {10.1109/TMC.2025.3560839},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWCBC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Things (IoT) networks, monitoring information collection is critical for intelligent decision-making, which is a significant challenge for the sensors deployed at remote locations. Relay can effectively improve the transmission quality and transmission range of sensors by means of multi-hop transmission. It is an effective method for remote data collection in IoT networks. However, the lifetime of the relay may be dramatically reduced due to the heavy resource overhead for frequent short packet delivery. In this paper, we present an efficient relay transmission scheme for IoT networks, in which the frame aggregation technology is employed at the relay to reduce the resource overhead by sharing a common frame header and tail. Meanwhile, for the delay caused by frame aggregation, we analyze the freshness of the sensing data in terms of age of information (AoI) and take it as a constraint for the frame aggregation system. Besides, the optimal frame aggregation period is determined based on the closed-form expressions derived for the average AoI and transmission efficiency. Simulation results show that the theoretical analysis closely matches the simulations, and the proposed method significantly improves transmission efficiency compared to the traditional decode-and-forward method.}
}


@article{DBLP:journals/tmc/ChenSJW25,
	author = {Zhenyu Chen and
                  Xinghua Sun and
                  Yili Jin and
                  Fangxin Wang},
	title = {Multi-Task Reinforcement Learning-Based Multiple Access for Dynamic
                  Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9153--9167},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559676},
	doi = {10.1109/TMC.2025.3559676},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenSJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of emergent applications, wireless networks require the provision of high throughput. Meanwhile, wireless scenarios exhibit highly dynamic characteristics, involving frequent changes in the network scale and traffic. To satisfy the high demand for new applications in dynamic wireless scenarios, a novel medium access control (MAC) protocol is required to allow stations to access the channel with high efficiency and adaptability. Based on multi-agent reinforcement learning (MARL), we propose a new MAC protocol, Multi-task Transformer-based Multiple Access (MTMA). Multi-task learning is applied to train a single actor to adapt to multiple wireless environments simultaneously. To improve the scalability, we propose a transformer-based critic network, which can scale to different wireless scenarios. Moreover, a novel network called “Generalization for N (Gen-N)” network is proposed to enhance the generalization ability. We conduct simulation experiments to demonstrate that MTMA: 1) achieves over 95% of upper bound of throughput while maximizing the fairness performance; 2) outperforms classic MAC protocol and MARL-based baselines in scenarios with saturated and light traffic; 3) can adapt to environmental changes quickly in dynamic scenarios; 4) can generalize to unseen scenarios during training. Finally, the ablation experiments are conducted to evaluate the effectiveness of components used in MTMA.}
}


@article{DBLP:journals/tmc/LiuLHXTL25,
	author = {Xiulong Liu and
                  Hankai Liu and
                  Yantao Han and
                  Xin Xie and
                  Xinyu Tong and
                  Keqiu Li},
	title = {MHTrack: mmWave-Based Mobile Hand Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9168--9183},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560074},
	doi = {10.1109/TMC.2025.3560074},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLHXTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-intrusive hand tracking with mmWave radar technology is important in various Human-Computer Interaction (HCI) scenarios. However, existing mmWave-based solutions require users to be stationary and restrict a fixed hand motion area, which limits application flexibility and user experience. This paper proposes a novel mmWave-based Mobile Hand Tracking (MHTrack) system, which tracks user’s hand gestures during walking. MHTrack focuses on tracking both absolute hand trajectory in the global coordinate system and relative hand trajectory to the body. Specifically, we propose a wake-up mechanism for hand motion capture, in which hand point cloud can be recognized even under body interference and noise. We propose a hand tracking strategy named local spatial update, which overcomes the sparsity and instability of point clouds, to obtain absolute hand trajectory. Subsequently, we propose a hand anchor correction method to suppress anchor offset and remove the impact of body movement from absolute hand trajectory, thereby obtaining relative hand trajectory. As a case study, we project the relative hand trajectory onto a 2D image and feed it into a gesture recognition model to recognize the gestures. We conduct extensive experiments to evaluate the performance of MHTrack. Results demonstrate a 3D hand trajectory tracking error of  3.6  3.6  cm in an area of  3.2 m × 4.8 m  3.2\\;{\\rm m}\\,\\times\\, 4.8\\;{\\rm m}  and a gesture recognition accuracy of 99% with 30 gesture classes.}
}


@article{DBLP:journals/tmc/ZhouLWMXDHLZSWC25,
	author = {Nan Zhou and
                  Yuxuan Liu and
                  Haoyang Wang and
                  Fanhang Man and
                  Jingao Xu and
                  Fan Dang and
                  Chaopeng Hong and
                  Yunhao Liu and
                  Xiao{-}Ping Zhang and
                  Yali Song and
                  Qiuhua Wang and
                  Xinlei Chen},
	title = {CatUA: Catalyzing Urban Air Quality Intelligence Through Mobile Crowd-Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9184--9201},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3560120},
	doi = {10.1109/TMC.2025.3560120},
	timestamp = {Fri, 12 Sep 2025 08:19:09 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLWMXDHLZSWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile air pollution sensing methods have emerged to collect air quality data with improved spatial and temporal resolutions. However, existing methodologies struggle to effectively process spatially mixed gas samples due to the highly dynamic fluctuations experienced by sensors, resulting in significant measurement deviations. We identify an opportunity to address this issue by exploring potential patterns within sensor measurements. To this end, we propose CatUA, a novel city-scale fine-grained air quality estimation system designed to deliver accurate mobile air quality data. First, we design AirBERT, a representation learning model specifically aimed at discerning mixed gas concentrations from sensor data. Second, we implement a Prompt-informed Training Strategy that leverages extensive unlabeled and minimal labeled city-scale data to enhance the performance of CatUA. Notably, the Auto-Prompt mechanism allows CatUA to conveniently acquire new knowledge tailored to specific downstream tasks. To ensure the practicality of CatUA, we have invested considerable effort in developing the software stack on our meticulously crafted Sensing Front-end, which has successfully gathered city-scale air quality data for over 1,200 hours. Experiments conducted on the collected data demonstrate that CatUA reduces sensing errors by 96.9% with a latency of only 44.9 ms, outperforming the state-of-the-art baseline by 42.6%.}
}


@article{DBLP:journals/tmc/ZhangLBXZ25,
	author = {Manman Zhang and
                  Peng Li and
                  Shanjun Bao and
                  He Xu and
                  Feng Zhu},
	title = {A Secondary Nondestructive Detection Method of Liquid Concentration
                  for {RFID} Tag Array With Mutual Coupling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9202--9221},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559487},
	doi = {10.1109/TMC.2025.3559487},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLBXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Frequency Identification (RFID) technology emerges as a crucial tool for passive sensing due to the lightweight, taggable, and easily deployable attributes of tags. However, RFID multi-tag systems face significant accuracy degradation due to tag mutual coupling interference. To address this problem, we propose the RF-Copy method, a tagged coupling interference suppression approach. The method constructs RSSI and phase models with coupled terms, simplifies and estimates these parameters based on the models, and develops an interference suppression algorithm to remove coupling effects and extract ‘clean’ signal fingerprints. Experimental results demonstrate that the decoupled outputs (0.318 dB RSSI, 0.623 radians phase) closely approach the interference-free baseline values (0.392 dB, 0.649 radians). We apply the RF-Copy method to evaluate the tag mutual coupling suppression effectiveness in nondestructive white wine and wine concentration recognition. This process generates ‘clean’ RSSI and phase values, which serve as input features for training a Gated Recurrent Unit (GRU) neural network integrated with an attention mechanism. Experimental results demonstrate that our method achieves unopened-package detection accuracies of 99.0% (white wine) and 99.6% (wine). Compared to other methods, the RF-Copy-GRU not only improves sensing accuracy but also demonstrates robustness against interference.}
}


@article{DBLP:journals/tmc/ChenSSY25,
	author = {Quan Chen and
                  Xiaoqin Song and
                  Tiecheng Song and
                  Yang Yang},
	title = {Vehicular Edge Computing Networks Optimization via DRL-Based Communication
                  Resource Allocation and Load Balancing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9222--9237},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559707},
	doi = {10.1109/TMC.2025.3559707},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenSSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the evolution of the Internet of vehicles (IoV), the increasing demand for vehicular computation tasks presents significant challenges, particularly in the context of constrained local computation resources and high processing delays. To mitigate these challenges, multi-access edge computing (MEC) offers a potential solution by leveraging edge servers for low-latency processing. However, it also encounters issues such as sub-channel competition and workload imbalance owing to the uneven distribution of vehicle densities. This paper introduces a novel IoV architecture that incorporates multi-task and multi-roadside unit (RSU) capabilities, enabling edge-to-edge collaboration for efficient task offloading among RSUs. The optimization problem is formulated with the objective of minimizing the overall task delay, which is further divided into two sub-problems: communication resource allocation and load balancing. Considering the non-deterministic polynomial (NP)-hard nature of these sub-problems, we propose a two-stage deep reinforcement learning-based communication resource allocation and load balancing (DRLCL) algorithm to address them sequentially. Based on realistic vehicle trajectories, comprehensive evaluation results demonstrate the superiority of the proposed algorithm in reducing system delay compared to existing state-of-the-art baselines, offering an effective approach for optimizing the performance of vehicular edge computing (VEC) networks.}
}


@article{DBLP:journals/tmc/ZhaoMTLXQ25,
	author = {Yiming Zhao and
                  Xuanqi Meng and
                  Xinyu Tong and
                  Xiulong Liu and
                  Xin Xie and
                  Wenyu Qu},
	title = {Baton: Compensate for Missing Wi-Fi Features for Practical Device-Free
                  Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9238--9254},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3559927},
	doi = {10.1109/TMC.2025.3559927},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoMTLXQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi contact-free sensing systems have attracted widespread attention due to their ubiquity and convenience. The integrated sensing and communication (ISAC) technology utilizes off-the-shelf Wi-Fi communication signals for sensing, which further promotes the deployment of intelligent sensing applications. However, current Wi-Fi sensing systems often require prolonged and unnecessary communication between transceivers, and brief communication interruptions will lead to significant performance degradation. This paper proposes Baton, the first system capable of accurately tracking targets even under severe Wi-Fi feature deficiencies. To be specific, we explore the relevance of the Wi-Fi feature matrix from both horizontal and vertical dimensions. The horizontal dimension reveals feature correlation across different Wi-Fi links, while the vertical dimension reveals feature correlation among different time slots. Based on the above principle, we propose the Simultaneous Tracking And Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi features over time and across different links, akin to passing a baton. We implement the system on commercial devices, and the experimental results show that our system outperforms existing solutions with a median tracking error of  0.46  0.46  m, even when the communication duty cycle is as low as 20.00%. Compared with the state-of-the-art, our system reduces the tracking error by 79.19% in scenarios with severe Wi-Fi feature deficiencies.}
}


@article{DBLP:journals/tmc/CaoZWYMY25,
	author = {Zijian Cao and
                  Dong Zhao and
                  Qiyue Wang and
                  Haitao Yuan and
                  Huadong Ma and
                  Shui Yu},
	title = {CrossTrace: Privacy-Aware Cross-System Trajectory Recovery via Hybrid
                  Split and Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9255--9272},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3561791},
	doi = {10.1109/TMC.2025.3561791},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoZWYMY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive urban-scale vehicle trajectories benefit various downstream applications. However, trajectories collected from existing sensing systems are often incomplete, necessitating the recovery of coarse-grained trajectories. Considering that mobility knowledge learned from a single system is less representative of all vehicles or covers only partial road segments, it becomes essential to combine diverse data from multiple systems to support trajectory recovery. Therefore, we learn the impacts of mobility intentions and dynamic traffic conditions on the movement of vehicles from trajectories aggregated across different systems to recover their travel routes on unobservable road intersections. Nonetheless, aggregating raw data across multiple systems raises privacy concerns. This data isolation compounds challenges in acquiring comprehensive mobility intentions and traffic conditions, thereby impairing recovery performance. In this paper, we propose CrossTrace, a two-stage framework for privacy-aware cross-system trajectory recovery: in the Traffic Condition Inference stage, a Split Learning pipeline with a multi-view graph neural network is utilized to infer complete traffic conditions for all road segments; in the Trajectory Recovery stage, a Federated Learning pipeline with dedicated modules is utilized to recover missing points by fusing inferred traffic conditions and mobility intentions. Extensive experiments on two large-scale trajectory datasets demonstrate that CrossTrace outperforms all alternative schemes.}
}


@article{DBLP:journals/tmc/MaHQM25,
	author = {Xiao Ma and
                  Shengfeng He and
                  Hezhe Qiao and
                  Dong Ma},
	title = {Toward Diverse Tiny-Model Selection for Microcontrollers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {9},
	pages = {9273--9288},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3561778},
	doi = {10.1109/TMC.2025.3561778},
	timestamp = {Thu, 11 Sep 2025 20:24:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaHQM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is challenging due to their constrained on-chip resources. Existing approaches mainly focus on compressing larger models, often compromising model accuracy as a trade-off. In this paper, we rethink the problem from the inverse perspective by directly constructing small/weak models, then enhancing their accuracy. Thus, we propose DiTMoS, a novel DNN training and inference framework featuring a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is built on a key insight: a combination of weak models can exhibit high diversity and the union of them can significantly raise the upper bound of overall accuracy. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to enhance the classifiers’ diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers. We further design a network slicing technique to eliminate the extra memory consumption incurred by feature aggregation. We deploy DiTMoS on the Nucleo STM32F767ZI board and evaluate its performance across three time-series datasets for human activity recognition, keyword spotting, and emotion recognition tasks. The experimental results show that: (a) DiTMoS improves accuracy by up to 13.4% compared to the best baseline; (b) network slicing successfully eliminates the memory overhead introduced by feature aggregation, with only a minimal increase in latency.}
}


@article{DBLP:journals/tmc/YaoWRXWXXZ25,
	author = {Su Yao and
                  Mu Wang and
                  Ju Ren and
                  Tianyu Xia and
                  Weiqiang Wang and
                  Ke Xu and
                  Mingwei Xu and
                  Hongke Zhang},
	title = {Multi-Agent Reinforcement Learning for Task Offloading in Crowd-Edge
                  Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9289--9302},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3531793},
	doi = {10.1109/TMC.2025.3531793},
	timestamp = {Tue, 04 Nov 2025 12:12:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YaoWRXWXXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Crowd-edge (CE) computing paradigm facilitates the utilization of the computational resources through simultaneously relying the edge computing and the collaboration among various mobile devices (MDs). Most existing works, focusing on offloading tasks from device to edge servers by centralized solutions, are unable to distribute tasks to massive MDs in CE. Meanwhile, designing a decentralized task offloading solution enabling task subscribers to individually make offloading decisions can be challenging given the randomness of crowd resource provisioning and limited knowledge of global status variations. In this paper, we propose a decentralized crowd-edge task offloading solution that enables users to optimally offload tasks to the CE in a distributed manner. Specifically, we formulate the corresponding problem as a stochastic optimization with partially observable status. By observing network and process delays at the crowd side, we further reform the optimization forms and provide a novel approximation policy, enabling users to optimize their offloading strategy based on local observations without interaction with each other. We then solve this task offloading problem by developing a Mixed Multi-Agent Proxy Policy Optimization algorithm (mixed MAPPO). Extensive testing, including numerical and system-level simulations, was conducted to validate the performance of the proposed algorithm in terms of task delay (including the processing delay and transmission delay), load rate, and resource utilization.}
}


@article{DBLP:journals/tmc/ZhaoLJY25,
	author = {Sifan Zhao and
                  Tongtong Liu and
                  Hai Jin and
                  Dezhong Yao},
	title = {SPViT: Accelerate Vision Transformer Inference on Mobile Devices via
                  Adaptive Splitting and Offloading},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9303--9318},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562721},
	doi = {10.1109/TMC.2025.3562721},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Vision Transformer (ViT), which benefits from utilizing self-attention mechanisms, has demonstrated superior accuracy compared to CNNs. However, due to the expensive computational costs, deploying and inferring ViTs on resource-constrained mobile devices has become a challenge. To resolve this challenge, we conducted an empirical analysis to identify performance bottlenecks in deploying ViTs on mobile devices and explored viable solutions. In this paper, we propose SPViT, an adaptive split and offloading method that accelerates ViT inference on mobile devices. SPViT executes collaborative inference of ViT across available edge devices. We introduce a fine-grained splitting technique for the vision transformer structure. Furthermore, we propose an algorithm based on the Auto Regression model to predict partition latency and adaptive offload partitions. Finally, we design offline and online optimization methods to minimize the computational and communication overhead on each device. Based on real-world prototype experiments, SPViT effectively reduces inference latency by 2.2x to 3.3x across four state-of-the-art models.}
}


@article{DBLP:journals/tmc/GongJAKM25,
	author = {Taesik Gong and
                  Si Young Jang and
                  Utku G{\"{u}}nay Acer and
                  Fahim Kawsar and
                  Chulhong Min},
	title = {Synergy: Towards On-Body {AI} via Tiny {AI} Accelerator Collaboration
                  on Wearables},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9319--9333},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564314},
	doi = {10.1109/TMC.2025.3564314},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongJAKM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0× improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.}
}


@article{DBLP:journals/tmc/LiWWLWLXD25,
	author = {Ming Li and
                  Jian Weng and
                  Jia{-}Si Weng and
                  Yi Li and
                  Yongdong Wu and
                  Dingcheng Li and
                  Guowen Xu and
                  Robert H. Deng},
	title = {IvyCross: {A} Privacy-Preserving and Concurrency Control Framework
                  for Blockchain Interoperability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9334--9351},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562875},
	doi = {10.1109/TMC.2025.3562875},
	timestamp = {Fri, 17 Oct 2025 07:35:07 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWWLWLXD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interoperability is a fundamental challenge for long-envisioned blockchain applications. A mainstream approach is using Trusted Execution Environment (TEE) to support interoperable off-chain execution. However, this incurs multiple TEE configured with non-trivial storage capabilities running on fragile concurrent processing environments, rendering current strategies based on TEE far from being practical. This paper aims to fill this gap and design a practical interoperability mechanism with simplified TEE as the underlying architecture. Specifically, we present IvyCross, a TEE-based framework that achieves low-cost, privacy-preserving, and race-free blockchain interoperability. IvyCross allows running arbitrary smart contracts across heterogeneous blockchains atop two distributed TEE-powered hosts. We design an incentive scheme based on smart contracts to stimulate the honest behavior of two hosts, bypassing the requirement of the number of TEE and large memory need. We examine the conditions to guarantee the uniqueness of Nash Equilibrium via Game Theory. Furthermore, an extended optimistic concurrency control protocol is designed to ensure the correctness of concurrent contracts execution. We formally prove the security of IvyCross in the Universal Composability (UC) framework and implement a prototype atop Bitcoin, Ethereum, and FISCO BOCS. Extensive experimental results on end-to-end performance and concurrency control demonstrate the efficiency and practicality of IvyCross.}
}


@article{DBLP:journals/tmc/LinWCLCGL25,
	author = {Zheng Lin and
                  Wei Wei and
                  Zhe Chen and
                  Chan{-}Tong Lam and
                  Xianhao Chen and
                  Yue Gao and
                  Jun Luo},
	title = {Hierarchical Split Federated Learning: Convergence Analysis and System
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9352--9367},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3565509},
	doi = {10.1109/TMC.2025.3565509},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinWCLCGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As AI models expand in size, it has become increasingly challenging to deploy federated learning (FL) on resource-constrained edge devices. To tackle this issue, split federated learning (SFL) has emerged as an FL framework with reduced workload on edge devices via model splitting; it has received extensive attention from the research community in recent years. Nevertheless, most prior works on SFL focus only on a two-tier architecture without harnessing multi-tier cloud-edge computing resources. In this paper, we intend to analyze and optimize the learning performance of SFL under multi-tier systems. Specifically, we propose the hierarchical SFL (HSFL) framework and derive its convergence bound. Based on the theoretical results, we formulate a joint optimization problem for model splitting (MS) and model aggregation (MA). To solve this rather hard problem, we then decompose it into MS and MA sub-problems that can be solved via an iterative descending algorithm. Simulation results demonstrate that the tailored algorithm can effectively optimize MS and MA in multi-tier systems and significantly outperform existing schemes.}
}


@article{DBLP:journals/tmc/LiuTLCYZYHH25,
	author = {Jiacheng Liu and
                  Feilong Tang and
                  Hao Liu and
                  Long Chen and
                  Yichuan Yu and
                  Yanmin Zhu and
                  Jiadi Yu and
                  Xiaofeng Hou and
                  Pheng{-}Ann Heng},
	title = {{BAT:} {A} Versatile Bipartite Attention-Based Approach for Comprehensive
                  Truth Inference in Mobile Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9368--9382},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563345},
	doi = {10.1109/TMC.2025.3563345},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuTLCYZYHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of smart mobile devices has catalyzed the growth of Mobile CrowdSourcing (MCS) as a distributed problem-solving paradigm. MCS platforms heavily rely on advanced truth inference techniques to extract reliable information from diverse and potentially noisy crowd-contributed data. Existing truth inference models often made simplified assumptions about workers or tasks, employing complex Bayesian models or stringent data aggregation methods. These approaches tend to be task-specific, primarily limited to categorical labeling, making adaptations to other mobile computing scenarios labor-intensive. To address these limitations, we introduce the Bipartite Attention-driven Truth (BAT), a versatile approach tailored for mobile computing environments. BAT utilizes an Attributed Bipartite Graph (ABG) to holistically model the MCS process, with workers and tasks as nodes connected by edges representing answer-specific attributes. The approach employs a bipartite graph neural network with an innovative attention mechanism to assess the importance of different answers. BAT extends beyond categorical tasks to support numerical ones by incorporating novel feature representations and model extensions. Theoretical analyses clarify the link between answer similarity and worker expertise. Extensive experiments using diverse real-world datasets demonstrate BAT's superior performance compared to state-of-the-art categorical and numerical truth inference models, highlighting its effectiveness in mobile computing scenarios.}
}


@article{DBLP:journals/tmc/AlShawabkaPPJM25,
	author = {Amani Al{-}Shawabka and
                  Philip Pietraski and
                  Sudhir B. Pattar and
                  Pedram Johari and
                  Tommaso Melodia},
	title = {SignCRF: Scalable Channel-Agnostic Data-Driven Radio Authentication
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9383--9394},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564556},
	doi = {10.1109/TMC.2025.3564556},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AlShawabkaPPJM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (“fingerprint”) the device itself based on variations introduced in the transmitted waveform. Key impediments in developing robust and scalable Radio Frequency Fingerprinting through Deep Learning (RFFDL) techniques that are practical in dynamic and mobile environments are the non-stationary behavior of the wireless channel and other impairments introduced by the propagation conditions. To date, the existing RFFDL-based techniques have only been able to demonstrate a desirable performance when the training and testing environment remains the same, which makes the solutions impractical. SignCRF brings to the RFFDL landscape what it has been missing so far: a scalable, channel-agnostic data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments that is independent of the dynamic nature of the environment or channel irregularities caused by mobility. SignCRF consists of: (i) a classifier developed in a base-environment with minimum channel dynamics, and finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator that is carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific “signature”; and (iii) a Max Rule module that selects the highest precision authentication technique between the baseline classifier and the environment translator per radio. We design, train, and validate the performance of SignCRF for multiple technologies in dynamic environments and at scale (100 LoRa and 20 WiFi devices, the largest datasets available in the literature). We assess the scalability of SignCRF across various testbed scales by validating our system using small, medium, and large-scale testbeds, with sizes of 5, 20, and 100 devices, respectively. We demonstrate that SignCRF can significantly improve the RFFDL performance by achieving as high as 100% correct authentication for WiFi devices and 80% correctly authenticated LoRa devices, a 5x and 8x improvement when compared to the state-of-the-art respectively. Furthermore, we show that SignCRF is resilient to adversarial actions by reducing the device recognition accuracy from 73% to 6%, which translates into zero mis-authentication of adversary radios that try to impersonate legitimate devices, which has not been achieved by any prior RFFDL techniques.}
}


@article{DBLP:journals/tmc/WangOGHC25,
	author = {Pu Wang and
                  Tao Ouyang and
                  Jie Gong and
                  Chao Hong and
                  Xu Chen},
	title = {Adaptive Dynamic Scaling and Request Routing Optimization in the Multi-Edge
                  Cluster Collaboration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9395--9412},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562209},
	doi = {10.1109/TMC.2025.3562209},
	timestamp = {Sun, 23 Nov 2025 13:28:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangOGHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid proliferation of mobile devices, a growing number of intelligent applications are being deployed at the network edge, placing immense strain on the processing capabilities of edge computing. Therefore, resource-constrained edge servers frequently experience overload due to highly dynamic workloads. To address this, one approach involves forwarding user requests to the cloud or other edge servers, albeit at the cost of increased transmission latency. Alternatively, dynamic scaling of edge clusters can be employed to enhance processing capacity, thereby mitigating latency but at the expense of additional service configuration and hosting expenses. By integrating their complementary benefits, we study the joint optimization problem of dynamic scaling and request routing within a multi-edge cluster collaborative framework, which fully exploits cluster resources to manage the temporal and spatial varying edge workloads. This collaborative framework aims to minimize overall request latency while satisfying an acceptable time-averaged budget cost. However, the complex coupling between scaling and routing decisions, along with the uncertainty of future system information (e.g., user request workloads) impedes the derivation of an optimal offline policy over the long term. Thus, considering the different decision granularities, we employ the two-timescale Lyapunov optimization technique to decouple the original problem into a series of independent online optimization problems with the current system state. In particular, we make cluster scaling decisions in each large timescale and request routing decisions in each small timescale. Given that the decoupled large-timescale subproblems involve NP-hard mixed-integer linear programming, we design an edge resource-aware greedy rounding algorithm to efficiently produce approximate optimal solutions. Finally, both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the superiority of our proposed algorithm over its counterparts.}
}


@article{DBLP:journals/tmc/ChenLCBL25,
	author = {Yile Chen and
                  Xiucheng Li and
                  Gao Cong and
                  Zhifeng Bao and
                  Cheng Long},
	title = {Semantic-Enhanced Representation Learning for Road Networks With Temporal
                  Dynamics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9413--9427},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562656},
	doi = {10.1109/TMC.2025.3562656},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLCBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of mobile devices and positioning technology has resulted in the generation of massive urban data, offering great opportunities to improve analytical abilities for urban infrastructure components. In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage mobile trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.}
}


@article{DBLP:journals/tmc/DengQWYFC25,
	author = {Bangchao Deng and
                  Bingqing Qu and
                  Pengyang Wang and
                  Dingqi Yang and
                  Benjamin Fankhauser and
                  Philippe Cudr{\'{e}}{-}Mauroux},
	title = {{REPLAY:} Modeling Time-Varying Temporal Regularities of Human Mobility
                  for Location Prediction Over Sparse Trajectories},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9428--9440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562669},
	doi = {10.1109/TMC.2025.3562669},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengQWYFC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-location prediction aims to forecast which location a user is most likely to visit given the user’s historical data. As a sequence modeling problem by nature, it has been widely addressed using Recurrent Neural Networks (RNNs). To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by integrating them into the RNN units as additional information, or utilizing them to search for informative historical hidden states to improve prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other time periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Under this circumstance, we propose REPLAY, learning to capture the time-varying temporal regularities for location prediction based on general RNN architecture. Specifically, REPLAY is designed on top of a flashback mechanism, where the spatiotemporal distances in sparse trajectories are used to search for the informative past hidden states; to accommodate the time-varying temporal regularities, REPLAY incorporates smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. We conduct a comprehensive evaluation, comparing REPLAY against a wide range of state-of-the-art methods. Experimental results show REPLAY significantly and consistently outperforms state-of-the-art methods by 7.7%–10.5% in the location prediction task, and the learnt bandwidths reveal interesting patterns of the time-varying temporal regularities.}
}


@article{DBLP:journals/tmc/HidalgoALGC25,
	author = {Ethan Sanchez Hidalgo and
                  Jose A. Ayala{-}Romero and
                  J. Xavier Salvat Lozano and
                  Andres Garcia{-}Saavedra and
                  Xavier P{\'{e}}rez Costa},
	title = {AegisRAN: {A} Fair and Energy-Efficient Computing Resource Allocation
                  Framework for vRANs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9441--9457},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564116},
	doi = {10.1109/TMC.2025.3564116},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HidalgoALGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The virtualization of Radio Access Networks (vRAN) is rapidly becoming a reality, driven by the increasing need for flexible, scalable, and cost-effective mobile network solutions. To mitigate energy efficiency concerns in vRAN deployments, two approaches are gaining attention: ($i$) sharing computing infrastructure among multiple virtualized base stations (vBSs); and ($ii$) relying upon general-purpose, low-cost CPUs. However, effectively realizing these approaches poses several challenges. In this paper, we first conduct a comprehensive experimental campaign on a vRAN platform to characterize the impact of computing and radio resource allocation on energy consumption and performance across various network contexts. This analysis reveals several key issues. First, determining the optimal allocation of computing resources is difficult because it depends on the context of each vBS (e.g., traffic load, channel quality) in a non-trivial and non-linear manner. Second, suboptimal resource assignment can lead to increased energy consumption or, even worse, degradation of users’ Quality of Service. Third, the high dimensionality of the solution space hinders the effectiveness of traditional optimization or learning methods. To tackle these challenges, we propose AegisRAN, a framework for optimizing computing resource allocation in vRAN. AegisRAN addresses the dual objective of minimizing energy consumption while maintaining high system reliability. Moreover, when computing resources are overbooked, our solution ensures a fair resource partition based on vBS performance. AegisRAN leverages a discrete soft actor-critic algorithm combined with several techniques, including multi-step decision-making, action masking, digital twin-based training, and a tailored reward signal that mitigates feedback sparsity. Our evaluations demonstrate that AegisRAN achieves near-optimal performance and offers high flexibility across diverse network contexts and varying numbers of vBSs, with up to 25% improvement in energy savings compared to baseline solutions in medium-scale scenarios.}
}


@article{DBLP:journals/tmc/GuoZW25,
	author = {Kaiyi Guo and
                  Qian Zhang and
                  Dong Wang},
	title = {EchoExpress: Facial Expression Recognition in the Wild via Acoustic
                  Sensing on Smart Glasses},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9458--9476},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3566341},
	doi = {10.1109/TMC.2025.3566341},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately recognizing facial expressions and emotions at any time and in any place can significantly improve people’s quality of life and mental well-being. However, existing methods lack the convenient capability for long-term monitoring in the wild environment. In this paper, we introduce EchoExpress, an in-the-wild emotion-related facial expression recognition system that works in an unobtrusive, low-power, and privacy-friendly way. EchoExpress uses two speakers and two microphones mounted on a glass-frame for transmitting and receiving mutually orthogonal wave signals. Concurrently, a unique attention mechanism dynamically extracts crucial features, enabling the capture of nuanced facial expressions and emotions. Furthermore, we introduce an open-set filtering mechanism with a specially designed loss function, which effectively filters out irrelevant actions, thereby reducing the risk of misidentification. Finally, a semi-supervised training method is employed to address the significant variability in wild expressions across different individuals. In extensive testing, EchoExpress achieves an accuracy of 84% in a laboratory environment and over 75% in real-world conditions. We believe that EchoExpress can serve as an unobtrusive and reliable way to monitor facial expressions.}
}


@article{DBLP:journals/tmc/HanLSZCANK25,
	author = {Hong Han and
                  Yang Lu and
                  Zihan Song and
                  Ruichen Zhang and
                  Wei Chen and
                  Bo Ai and
                  Dusit Niyato and
                  Dong In Kim},
	title = {SWIPTNet: {A} Unified Deep Learning Framework for {SWIPT} Based on
                  {GNN} and Transfer Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9477--9488},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563892},
	doi = {10.1109/TMC.2025.3563892},
	timestamp = {Tue, 13 Jan 2026 20:25:57 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanLSZCANK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the deep learning based approaches for simultaneous wireless information and power transfer (SWIPT). The quality-of-service (QoS) constrained sum-rate maximization problems are, respectively, formulated for power-splitting (PS) receivers and time-switching (TS) receivers and solved by a unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet). To improve the performance of SWIPTNet, we first propose a single-type output method to reduce the learning complexity and facilitate the satisfaction of QoS constraints, and then, utilize the Laplace transform to enhance input features with the structural information. Besides, we adopt the multi-head attention and layer connection to enhance feature extracting. Furthermore, we present the implementation of transfer learning to the SWIPTNet between PS and TS receivers. Ablation studies show the effectiveness of key components in the SWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in achieving near-optimal performance with millisecond-level inference speed which is much faster than the traditional optimization algorithms. We also show the effectiveness of transfer learning via fast convergence and expressive capability improvement.}
}


@article{DBLP:journals/tmc/LiZLX25,
	author = {Xingyu Li and
                  Wenzhe Zhang and
                  Linfeng Liu and
                  Jia Xu},
	title = {Exploring the Robustness: Hierarchical Federated Learning Framework
                  for Object Detection of {UAV} Cluster},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9489--9505},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562812},
	doi = {10.1109/TMC.2025.3562812},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of Unmanned Aerial Vehicle (UAV) cluster is an available solution for object detection missions. In the harsh environment, UAV cluster could suffer from some significant threats (e.g., forest fire hazards, electromagnetic interference, and ground-to-air attacks), which could lead to the destruction of UAVs and loss of data. To this end, we propose a Hierarchical Federated Learning Framework for Object Detection (HFL-OD) to enhance the robustness of UAV cluster conducting object detection missions. In HFL-OD, UAVs are grouped through a Three-Dimensional (3D) graph coloring method, and an intragroup backup mechanism is provided to prevent the data loss caused by the destruction of UAVs. Besides, a dynamic server selection mechanism deals with the potential destruction of servers (cluster server and group servers) by adaptively reassigning the server roles. To further improve the robustness and mission efficiency of UAV cluster, a two-tier federated learning framework is introduced to make a proper trade-off between object detection accuracy and communication/computational overhead. This framework is built on the concept of hierarchical federated learning by implementing both intragroup parameter aggregation and global parameter aggregation. Extensive simulations and comparisons demonstrate the superior performance of our proposed HFL-OD, i.e., the robustness of UAV cluster conducting object detection missions can be significantly improved, and the communication/computational overhead is effectively reduced.}
}


@article{DBLP:journals/tmc/XiaZLHM25,
	author = {Dan Xia and
                  Xiaolong Zheng and
                  Liang Liu and
                  Shanguo Huang and
                  Huadong Ma},
	title = {WiCast: Parallel Cross-Technology Transmission for Connecting Heterogeneous
                  IoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9506--9523},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564340},
	doi = {10.1109/TMC.2025.3564340},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaZLHM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-Technology Communication (CTC) is an emerging technique that enables direct interconnection among incompatible wireless technologies. However, for the downlink from WiFi to multiple IoT technologies, serially emulating and transmitting the data of each IoT technology has extremely low spectrum efficiency. In this paper, we propose WiCast, a parallel CTC that uses IEEE 802.11ax to emulate a composite signal that can be received by commodity BLE, ZigBee, and LoRa devices. By taking advantage of OFDMA in 802.11ax, WiCast uses a single Resource Unit (RU) for parallel CTC and sets other RUs free for high-rate WiFi users. But such a sophisticated composite signal is very easily distorted by emulation imperfections, dynamic channel noises, cyclic prefix, and center frequency offset. We propose a CTC link model that jointly models the emulation errors and channel distortions. Then we carve the emulated signal with elaborate compensations in both time and frequency domains. Based on the proposed CTC scheme, a unified Media Access Control approach is introduced to discover and synchronize the heterogeneous IoT devices. We implement a prototype of WiCast using USRP N210 platform along with commodity ZigBee, BLE, and LoRa devices. The extensive experiments demonstrate WiCast can achieve an efficient parallel transmission with the aggregated goodput up to  390.24 kbps  390.24 \\;\\text{kbps} .}
}


@article{DBLP:journals/tmc/WangYMZDCZZZ25,
	author = {Pei Wang and
                  Anlan Yu and
                  Xujun Ma and
                  Rong Zheng and
                  Jingfu Dong and
                  Zhaoxin Chang and
                  Duo Zhang and
                  Djamal Zeghlache and
                  Daqing Zhang},
	title = {WiCG: In-Body Cardiac Motion Sensing Based on a Mix-Medium Wi-Fi Fresnel
                  Zone Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9524--9538},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564843},
	doi = {10.1109/TMC.2025.3564843},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYMZDCZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cardiovascular diseases (CVDs) are a leading cause of mortality worldwide, highlighting the critical need for accurate and continuous heart health monitoring. Electrocardiograms (ECG), considered as the golden standard for diagnosing and monitoring heart-related conditions, offer precise measurements but require direct skin contact, limiting their practicality for long-term and everyday use. On the other hand, existing RF sensing techniques that analyze signals reflected off the skin struggle to distinguish micro cardiac motions of the heart due to weak motion amplitude and respiration interference at the chest wall. To overcome these limitations, we introduce WiCG, a novel contact-less cardiac motion monitoring system that employs 2.4 GHz Wi-Fi signals to penetrate the chest and detect subtle cardiac movements. A mix-medium Wi-Fi Fresnel zone model is developed to explain the enhanced phase sensitivity of in-body Wi-Fi signals, which is crucial for accurately detecting cardiac motions. By strategically positioning antennas near the heart, WiCG captures ventricular motions effectively. A novel cardiac Doppler method is proposed to suppress phase noise and interference from static paths and extract the time interval between the systole and diastole of the ventricular. Extensive experiments demonstrate that the proposed system can robustly estimate the R-R and Q-T intervals of human cardiac cycles across 21 subjects and different environments with an average accuracy of 99.22% and 92.8%, achieving performance comparable to ECG.}
}


@article{DBLP:journals/tmc/WangSSY25,
	author = {Shumo Wang and
                  Xiaoqin Song and
                  Tiecheng Song and
                  Yang Yang},
	title = {Joint Optimization of Beamforming and Trajectory for UAV-RIS-Assisted
                  {MU-MISO} Systems Using {GNN} and {SD3}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9539--9553},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563072},
	doi = {10.1109/TMC.2025.3563072},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In urban environments, direct communication links between a base station (BS) and user equipment (UEs) are often obstructed by buildings. To mitigate these blockages, we integrate uncrewed aerial vehicles (UAVs) and reconfigurable intelligent surfaces (RISs) to enhance system flexibility and improve transmission efficiency. This paper investigates an RIS-assisted multi-user multiple-input single-output (MU-MISO) downlink system, where the RIS is mounted on a UAV. To maximize the system rate while minimizing the UAV’s energy consumption and flight duration, we formulate a multi-objective optimization problem. To address this problem, we propose a hybrid algorithm that integrates the soft deep deterministic policy gradient (SD3) algorithm with a graph neural network (GNN) architecture, named SD3-GNN-RIS. The original problem is decomposed into two subproblems: joint active beamforming at the BS and passive beamforming at the RIS, optimized via a GNN-based approach, and three-dimensional (3D) UAV trajectory optimization, formulated as a Markov decision process and solved using the SD3 algorithm. Simulation results demonstrate the superior performance of the proposed algorithm compared to baseline methods in terms of system rate, energy efficiency, and UAV trajectory optimization.}
}


@article{DBLP:journals/tmc/MengTSW25,
	author = {Chuiyang Meng and
                  Ming Tang and
                  Mehdi Setayesh and
                  Vincent W. S. Wong},
	title = {Tackling Resource Allocation for Decentralized Federated Learning:
                  {A} GNN-Based Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9554--9569},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562834},
	doi = {10.1109/TMC.2025.3562834},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MengTSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized federated learning (DFL) enables clients to train a neural network model in a device-to-device (D2D) manner without central coordination. In practical systems, DFL faces challenges due to dynamic topology changes, time-varying channel conditions, and limited computational capability of the clients. These factors can affect the learning performance and efficiency of DFL. To address the aforementioned challenges, in this paper, we propose a graph neural network (GNN)–based algorithm to minimize the total delay and energy consumption on training and improve the learning performance of DFL in D2D wireless networks. In our proposed GNN, a multi-head graph attention mechanism is used to capture different features of clients and wireless channels. We design a neighbor selection module which enables each client to select a subset of its neighbors for the participation of model aggregation. We develop a decoder that enables each client to determine its transmit power and computational resource. Experimental results show that our proposed algorithm achieves a lower total delay and energy consumption on training when compared with five baseline schemes. Furthermore, by properly selecting a subset of neighbors for each client, our proposed algorithm achieves similar testing accuracy to the full participation scheme.}
}


@article{DBLP:journals/tmc/HanWYWNS25,
	author = {Zengyi Han and
                  En Wang and
                  Mohan Yu and
                  Jie Wang and
                  Yuuki Nishiyama and
                  Kaoru Sezaki},
	title = {HeadMon{\textdollar}{\^{}}\{+\}{\textdollar}+: Domain Adaptive Head
                  Dynamic-Based Riding Maneuver Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9570--9583},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562179},
	doi = {10.1109/TMC.2025.3562179},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanWYWNS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Micro-mobility has become a vital means of transportation in recent years, however, it has also resulted in a rise in traffic incidents. Timely tracking and predicting riders’ maneuvers hold the potential to ensure active protection and allow for sufficient time to avert accidents by issuing timely warnings and interventions. We contend that the rider's head dynamics can provide valuable information regarding their subsequent maneuvers. Riders’ traveling habits, however diverse, not to mention the rapidly varying riding environment. The above factors contribute to significant disruptions in the data source, and various micro-mobility forms further exacerbate the issue. We accordingly present HeadMon + ^{+} , which predicts the rider's subsequent maneuver by examining their head dynamics, and it can effectively adapt to various riding conditions and individuals. The system incorporates a deep learning framework with an advanced domain adversarial network. By single-time pre-training, HeadMon + ^{+}  is capable of adapting to new data domains, including human subjects, and riding conditions for robust maneuver prediction. Based on our evaluation, we have found that the maneuver prediction of HeadMon + ^{+}  has an overall precision of 94% with a prediction time gap of 4 seconds. HeadMon + ^{+} 's low cost and rapid response capability make it easily deployed and then contribute to enhancing safe riding.}
}


@article{DBLP:journals/tmc/WangFGS25,
	author = {Yuhui Wang and
                  Junaid Farooq and
                  Hakim Ghazzai and
                  Gianluca Setti},
	title = {Joint Positioning and Computation Offloading in Multi-UAV {MEC} for
                  Low Latency Applications: {A} Proximal Policy Optimization Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9584--9598},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562806},
	doi = {10.1109/TMC.2025.3562806},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangFGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC) has emerged as a proven solution for reducing communication latency and enhancing user experience in delay-sensitive applications by offloading computation-intensive tasks to edge servers. In future networks, uncrewed aerial vehicles (UAVs), with their flexible deployment and reliable communication capabilities, have the potential to be deployed as aerial MEC servers in areas lacking cellular infrastructure. However, the joint optimization of UAV placement and task offloading poses significant challenges due to the interdependence between communication latency, computational demands, and the resource limitations of UAVs. In this paper, we propose a novel joint optimization framework utilizing proximal policy optimization (PPO) to simultaneously address UAV placement and computation offloading in UAV-enabled MEC networks. The framework dynamically adapts to changing network conditions, minimizing end-to-end latency while balancing computational loads and energy consumption. Extensive simulations demonstrate that the proposed PPO-based approach achieves superior performance compared to conventional optimization methods, with significant improvements in system latency, resource utilization, and network resilience. This work contributes scalable, adaptive solutions for UAV-assisted MEC networks in dynamic environments, enabling robust support for mission-critical and latency-sensitive applications.}
}


@article{DBLP:journals/tmc/ZhouZHQQJLCG25,
	author = {Pengzhan Zhou and
                  Yijun Zhai and
                  Yuepeng He and
                  Fang Qu and
                  Zhida Qin and
                  Xianlong Jiao and
                  Fulin Luo and
                  Chao Chen and
                  Songtao Guo},
	title = {FedRAV: Hierarchically Federated Region-Learning for Traffic Object
                  Classification of Personalized Autonomous Vehicles With Guaranteed
                  Efficiency},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9599--9618},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564402},
	doi = {10.1109/TMC.2025.3564402},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZHQQJLCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging federated learning enables distributed autonomous vehicles to train equipped deep learning models collaboratively without exposing their raw data, providing great potential for utilizing explosively growing autonomous driving data. However, considering the complicated traffic environments and driving scenarios, deploying federated learning for autonomous vehicles is inevitably challenged by non-independent and identically distributed (Non-IID) data of vehicles, which may lead to failed convergence and low training accuracy. In this paper, we propose a novel hierarchically Federated Region-learning framework of Autonomous Vehicles (FedRAV) that adaptively divides a large area containing vehicles into sub-regions based on the defined region-wise distance, and achieves personalized vehicular models and regional models. Specifically, the architecture employs a designated hypernetwork to learn personalized mask vectors per vehicle used in the linear combination of models shared by vehicles in the same region. This approach ensures that the updated vehicular model adopts the beneficial models while discarding the unprofitable ones. We validate our FedRAV framework against existing federated learning algorithms on four real-world autonomous driving datasets in various heterogeneous settings. Extensive experiment results demonstrate that FedRAV framework achieves superior performance than the state-of-the-art algorithms, and improves the accuracy by 9.36%.}
}


@article{DBLP:journals/tmc/LiWZZHZH25,
	author = {Tianyu Li and
                  Xingwei Wang and
                  Rongfei Zeng and
                  Liang Zhao and
                  Ammar Hawbani and
                  Yuxin Zhang and
                  Min Huang},
	title = {Enhancing Edge-Cloud Collaboration With Blockchain-Assisted Digital
                  Twin Intelligence Offloading Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9619--9635},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562189},
	doi = {10.1109/TMC.2025.3562189},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWZZHZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Edge-Cloud Collaborative (ECC) has emerged as an efficient and promising technique to empower various computation-intensive applications in Digital Twin Network (DTN). The integration of ECC with JointCloud and DTN serves to bridge the gap between data analysis and physical states. In ECC, a reliable and optimal task offloading scheme is required to maximize resource utilization and provide satisfying services to End Users (EU). However, existing offloading schemes still face significant challenges, such as the instability and complexity of network topologies, the intricacies of massive data, and the lack of trust among EU. In this paper, we propose an enhancinG edge-clOud collaboraTion wiTh blockchain-assistEd digital twin intelligence offloadiNg scheme (GOTTEN) which transmits large-scale tasks generated by DTs to Edge Station (ES) or Cloud Station (CS) in dynamic DTN scenarios. We first formulate this resource allocation and task offloading problem and provide an appropriate initial solution which guarantees that tasks generated by DTs can be accurately mapped to physical entities, while optimizing block allocation and reducing the decision space of task offloading. Then, we employ the Lagrange Multiplier based Distributed Island model-enhanced Genetic Algorithm (LM-DIGA) to transform our formulated problem into a convex form and achieve an optimal resource allocation under a specific scheme. Additionally, our proposed architecture also leverages blockchain verification mechanisms to enhance system stability, strengthening privacy protection for DT data as well. Finally, extensive simulation results demonstrate that, compared with seven baselines, our proposed scheme achieves a 10 percent the total system delay and privacy overhead with regard to other schemes in ECC.}
}


@article{DBLP:journals/tmc/GaoWSSSS25,
	author = {Yang Gao and
                  Wenjun Wu and
                  Ao Sun and
                  Yang Sun and
                  Teng Sun and
                  Pengbo Si},
	title = {QoS-Aware Intelligence Information Sharing Requests Scheduling in
                  IoV: CPO-Based Modeling and Solution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9636--9649},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3565898},
	doi = {10.1109/TMC.2025.3565898},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoWSSSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the accelerated development of autonomous driving and large language model, blockchain-supported data interaction and artificial intelligence (AI)-assisted performance optimization is the current mainstream research in the Internet of Vehicles (IoV). However, the trial-and-error behavior of the AI algorithm during the training process is a threat to road safety. Therefore, this paper proposes a general constrained policy optimization (CPO)-based modeling and solution for high-dimensional constrained optimization problems. We focus on intelligent driving information sharing in blockchain-enhanced IoV and optimize the service rewards in the sharing requests scheduling problem while ensuring the frequency resource limitation, service quality constraint, and road safety constraint. The constrained state space (CSS) is innovatively proposed to abstract the environment mathematically with the definition of constraint hyperplanes and distance. Accordingly, the constrained Markov Decision process (CMDP) and the optimization problem are formulated. With the practical implementation of the CPO theory, the constrained sharing requests scheduling (CSRS) algorithm is proposed. Ablation experiments are deep reinforcement learning-based methods without using the CSS-based constraint modeling or without using the CPO-based constrained problem solving process. Results show the effectiveness of CSS and CSRS algorithm in improving the policy training efficiency, and the testing results shows excellent generalization ability.}
}


@article{DBLP:journals/tmc/CaoLLZZ25,
	author = {Qian Cao and
                  Zhihui Li and
                  Haitao Li and
                  Shirui Zhou and
                  Yunxiang Zhang},
	title = {Participant Recruitment of Vehicular Crowdsensing Along Freeways for
                  Traffic Accident Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9650--9663},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562565},
	doi = {10.1109/TMC.2025.3562565},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLLZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular crowdsensing provides a new approach for freeway traffic accident detection. However, the uncertainty on traffic accidents and Mobile Users (MUs) brings great challenges for participant recruitment in constructing the deterministic representation of sensing tasks and estimating the participants. To address the challenges, a participant recruitment method for freeway traffic accident detection is proposed. In the method, to deal with the non-deterministic sensing tasks and MUs, the temporal-spatial distribution of accident risk is estimated by optimal transport theory to represent sensing tasks, and the probability distributions of MUs’ trip distance and requested rewards are used to estimate MUs. Then the participant recruitment problem is converted into an optimal coverage problem for accident risk under the macro statistical characteristics of MUs. The participant recruitment model is established to determine the participants by maximizing the coverage rate of accident risk with the budget constraint. And a greedy heuristic strategy is used to solve the model. Simulation experiments are carried out to validate the proposed method. The results show the proposed method is effective and reliable in freeway traffic accident detection.}
}


@article{DBLP:journals/tmc/WuZTG25,
	author = {Jiaqi Wu and
                  Xinyi Zhuang and
                  Ming Tang and
                  Lin Gao},
	title = {QoE-Aware Offloading and Resource Allocation for MEC-Empowered {AIGC}
                  Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9664--9682},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563027},
	doi = {10.1109/TMC.2025.3563027},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZTG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Artificial Intelligence-Generated Content (AIGC) has emerged as a transformative paradigm, enabling the autonomous creation of diverse content. By offloading model inference tasks to the network edge that is closer to mobile users (MUs), Mobile Edge Computing (MEC) has the potential to significantly enhance the performance of AIGC services. In practice, however, it is challenging to optimally manage MEC-empowered AIGC services, due to the lack of well-defined AIGC-specific metrics, as well as the dynamic workload and computation-intensive nature of AIGC services. In this paper, we first define a novel AIGC metric based on extensive real data experiments, and then study the joint task offloading and resource allocation problem in a generic MEC-empowered AIGC network, where MUs can offload model inference tasks to local or remote Base Stations (BSs), aiming at maximizing their Quality of Experience (QoE). The problem is challenging due to the fast and randomly changing of environments, as well as the necessity for real-time, asynchronous decision-making. To tackle these challenges, we propose two deep reinforcement learning algorithms based on the Proximal Policy Optimization (PPO) framework: Single-Layer PPO (SL-PPO) and Multi-Layer PPO (ML-PPO), designed for slow-changing and fast-changing environments, respectively. In the SL-PPO algorithm, both task offloading and resource allocation decisions are made simultaneously when tasks arrive. In the ML-PPO algorithm, the task offloading decision is made immediately when tasks arrive, while the resource allocation decision is deferred until tasks are scheduled for processing or transmission in the corresponding queues. Simulation results show that (i) both algorithms outperform existing methods in the literature, and can increase the average utility by up to 47% and 48.8%; (ii) both algorithms can effectively manage the trade-off between latency and energy consumption.}
}


@article{DBLP:journals/tmc/LiZLWXB25,
	author = {Yajun Li and
                  Jumin Zhao and
                  Dengao Li and
                  Hejun Wu and
                  Shuang Xu and
                  Ruiqin Bai},
	title = {Salix-Leaf: Find Main Veins of Signal Clusters for Practical Parallel
                  Decoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9683--9694},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3562590},
	doi = {10.1109/TMC.2025.3562590},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLWXB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parallel decoding of backscatter improves communication throughput by enabling concurrent transmission of backscatter tags. In practical applications of parallel decoding, it is extremely difficult to distinguish collided signals in superclusters where multiple signal clusters overlap. Existing methods are usually effective for superclusters with uniformly distributed signals. Nevertheless, there are many more scenarios in which signals in superclusters tend to gather unevenly, and existing methods cannot work. Such uneven clustering of signals occurs due to the following two possible causes: (1) signal-strength-differences (SSDs) among tags; or (2) cluster drifting (CD) driven by interferences from other objects within communication environments. This paper proposes a novel scheme called Salix-Leaf, which aims to identify the main veins of signal clusters to address this problem of superclusters with unevenly distributed signals. Salix-Leaf identifies the main vein of each signal cluster for fine-grained clustering so that the direction of the main veins can be used to verify the accuracy of clustering. In addition, Salix-Leaf employs a supercluster decomposer that divides signals into different segments for clustering analysis, enhancing robustness and practicability. Experimental results show that Salix-Leaf achieves a 1.2-fold increase in throughput and a 25% reduction in bit error rate (BER) compared to the state-of-the-art.}
}


@article{DBLP:journals/tmc/WuLWCBW25,
	author = {Chenrui Wu and
                  Zexi Li and
                  Fangxin Wang and
                  Hongyang Chen and
                  Jiajun Bu and
                  Haishuai Wang},
	title = {Toward Universal Personalization in Federated Learning via Collaborative
                  Foundation Generative Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9695--9708},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564880},
	doi = {10.1109/TMC.2025.3564880},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuLWCBW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized federated learning (PFL) enhances the performance of customized client models through collaborative training without compromising data privacy and ownership. Some previous PFL methods rely on rich prior knowledge about the types of data heterogeneity (such as class imbalance or feature skew), which greatly limits their application ranges. In this paper, we study the Universal Personalization in Federated Learning (UniPFL), the problem that has no prior knowledge about the types of data heterogeneity. In real-world PFL scenarios, UniPFL is potential because the data distributions of clients are usually heterogeneous and unknown to the server, where quantity imbalance, class imbalance, feature skew, or hybrid heterogeneity are possible contingencies. To address UniPFL, we propose FedFD, a novel framework with local data augmentation and global concept fusion, which is based on the recent advances in the foundation generative models (e.g., diffusion models, BLIP-2). On the client side, FedFD utilizes a diffusion model to assist local training by generating augmented data samples, and is then efficiently fine-tuned to be personalized. On the server side, we customize the aggregation strategies based on model similarities to learn both personalized models and diverse feature concepts. Extensive experiments show that FedFD reaches the state-of-the-art on (1) CIFAR-10 and CIFAR-100 for class imbalance; (2) DomainNet and Office-10 for feature skew, and (3) hybrid heterogeneity with both class and feature shifts.}
}


@article{DBLP:journals/tmc/ZhouPLW25,
	author = {Changjun Zhou and
                  Chenglin Pan and
                  Minglu Li and
                  Pengfei Wang},
	title = {Federated Unlearning With Fast Recovery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9709--9725},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563265},
	doi = {10.1109/TMC.2025.3563265},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouPLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent federated unlearning studies mainly focus on removing the target client's contributions from the global model permanently. However, the requirement for accommodating temporary user exits or additions in federated learning has been neglected. In this paper, we propose a novel recoverable federated unlearning scheme, named RFUL, which allows users to remove or add their local model to the global one at any time easily and quickly. It mainly consists of two main components, i.e., knowledge unlearning and knowledge recovery. In knowledge unlearning, the target contributions can be eliminated by training with mislabeled target data, while preserving the non-target contributions through distillation using the original model. In knowledge recovery, the forgotten contributions can be restored by training the target data using classification loss, while the non-target contributions are maintained through feature distillation and parameter freezing on the classifier. Both knowledge unlearning and recovery processes only require the participation of target data, guaranteeing the algorithm's practicality in federated learning systems. Extensive experiments demonstrate the significant efficacy of RFUL. For knowledge unlearning, RFUL matches state-of-the-art methods using only target data, achieving a runtime speedup of 3.3 to 8.7 times compared to retraining across various datasets. For knowledge recovery, RFUL exceeds state-of-the-art incremental learning methods by 5.02% to 29.97% in accuracy and achieves a runtime speedup of 1.8 to 4.4 times compared to retraining on different datasets.}
}


@article{DBLP:journals/tmc/LiuZLDFQW25,
	author = {Yijing Liu and
                  Long Zhang and
                  Xiaoqian Li and
                  Hongyang Du and
                  Gang Feng and
                  Shuang Qin and
                  Jiacheng Wang},
	title = {Trusted Clustering Based Federated Learning in Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9726--9742},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3566492},
	doi = {10.1109/TMC.2025.3566492},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZLDFQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is integral to advancing edge intelligence by enabling collaborative machine learning. In FL-empowered edge networks, computing nodes first train local models and then send them to an or multiple aggregation node(s) for global model collaboration. However, the trustworthiness of both local and global models in conventional FL frameworks is compromised due to inadequate model security and transparency. Distributed ledger technique (DLT) can address this issue by leveraging multi-nodes trust capabilities to support distributed consensus. However, model training and consensus performance of DLT may significantly degrade due to instability and resource constraints of edge networks. Sharding technique provides an effective approach by dividing the ledger into smaller and manageable shards. In this paper, to improve model training and consensus performance, we propose a trusted FL framework by incorporating sharding DLT into FL frameworks. We construct a theoretical model to investigate the relationship between model training performance, consensus efficiency, and capacity of edge nodes regarding storage, computing and communications. Based on the theoretical model, we propose a trusted clustering scheme to aggregate local models. Numerical results show that our proposed scheme significantly improves network throughput for transmitting models while guaranteeing model learning performance in comparison with some classical baselines.}
}


@article{DBLP:journals/tmc/SunHLPYS25,
	author = {Yiyong Sun and
                  Jiajun He and
                  Zhidi Lin and
                  Wenqiang Pu and
                  Feng Yin and
                  Hing Cheung So},
	title = {Hybrid Data-Driven {SSM} for Interpretable and Label-Free mmWave Channel
                  Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9743--9759},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564260},
	doi = {10.1109/TMC.2025.3564260},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunHLPYS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate prediction of mmWave time-varying channels is essential for mitigating the issue of channel aging in highly dynamic scenarios. Existing channel prediction methods have limitations: classical model-based methods often struggle to track highly nonlinear channel dynamics due to limited expert knowledge, while emerging data-driven methods typically require substantial labeled data for effective training and often lack interpretability. To address these issues, this paper proposes a novel hybrid method that integrates a data-driven neural network into a conventional model-based workflow based on a state-space model (SSM), implicitly tracking complex channel dynamics from data without requiring precise expert knowledge. Additionally, a novel unsupervised learning strategy is developed to train the embedded neural network solely with unlabeled data. Theoretical analyses and ablation studies are conducted to interpret the enhanced benefits gained from the hybrid integration. Numerical simulations based on the 3GPP mmWave channel model corroborate the superior prediction accuracy of the proposed method, compared to state-of-the-art methods that are either purely model-based or data-driven. Furthermore, extensive experiments validate its robustness against various challenging factors, including among others severe channel variations.}
}


@article{DBLP:journals/tmc/XuGLMH25,
	author = {Hongli Xu and
                  Xianjun Gao and
                  Jianchun Liu and
                  Qianpiao Ma and
                  Liusheng Huang},
	title = {FedACS: An Adaptive Client Selection Framework for Communication-Efficient
                  Federated Graph Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9760--9773},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563404},
	doi = {10.1109/TMC.2025.3563404},
	timestamp = {Fri, 17 Oct 2025 07:35:07 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuGLMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated graph learning (FGL) has been proposed to collaboratively train the increasing graph data with graph neural networks (GNNs) in a recommendation system. Nevertheless, implementing an efficient recommendation system with FGL still faces two primary challenges, i.e., limited communication bandwidth and non-IID local graph data. Existing works typically reduce communication frequency or transmission amount, which may suffer significant performance degradation under non-IID settings. Furthermore, some researchers propose to share the underlying structure among clients, which brings massive communication cost. To this end, we propose an efficient FGL framework, named FedACS, which adaptively selects a subset of clients for model training, to alleviate communication overhead and non-IID issues simultaneously. In FedACS, the global GNN model learns significant hidden edges and the structure of graph data among selected clients, enhancing recommendation efficiency. This capability distinguishes it from the traditional FL client selection methods. To optimize the client selection process, we introduce a multi-armed bandit (MAB) based algorithm to select participating clients according to the resource budgets and the training performance (i.e., RMSE). Experimental results indicate that FedACS improves RMSE by 5.4% over baselines with the same resource budget and reduces communication costs by up to 70.7% to achieve the same RMSE performance.}
}


@article{DBLP:journals/tmc/HuHLLRJCWH25,
	author = {Jinbin Hu and
                  Jiawei Huang and
                  Zhaoyi Li and
                  Yijun Li and
                  Shuying Rao and
                  Wenchao Jiang and
                  Kai Chen and
                  Jian{-}Xin Wang and
                  Tian He},
	title = {Proactive Transport With High Link Utilization Using Opportunistic
                  Packets in Cloud Data Centers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9774--9790},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563182},
	doi = {10.1109/TMC.2025.3563182},
	timestamp = {Wed, 19 Nov 2025 15:04:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuHLLRJCWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the stringent demanding low latency and high throughput of cloud datacenter applications, recent receiver-driven transport protocols transmit only one packet once receiving each credit packet from the receiver to achieve ultra-low queueing delay. However, the round-trip time variation and the highly dynamic background traffic significantly deteriorate the performance of receiver-driven transport protocols, resulting in under-utilized bandwidth. This article designs a simple yet effective solution called RPO, which retains the advantages of receiver-driven transmission while efficiently utilizing the available bandwidth. Specifically, RPO rationally uses low-priority opportunistic packets to ensure high network utilization without increasing the queueing delay of high-priority normal packets. Furthermore, to tackle the queueing buildup due to line-rate transmission in the first RTT, we design a selective dropping mechanism called SDM to help the majority of small flows complete within only one RTT by prioritizing the first-RTT bursty packets over the packets triggered by grants. We implement RPO in Linux hosts with DPDK. The experimental results show that RPO significantly improves the network utilization by up to 35% over the state-of-the-art schemes, without introducing additional queueing delay. Moreover, RPO integrated with SDM reduces the AFCT of small flows by up to 45% compared with RPO integrated with Aeolus.}
}


@article{DBLP:journals/tmc/HuFFDCFK25,
	author = {Senkang Hu and
                  Zhengru Fang and
                  Zihan Fang and
                  Yiqin Deng and
                  Xianhao Chen and
                  Yuguang Fang and
                  Sam Tak{-}Wu Kwong},
	title = {AgentsCoMerge: Large Language Model Empowered Collaborative Decision
                  Making for Ramp Merging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9791--9805},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564163},
	doi = {10.1109/TMC.2025.3564163},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuFFDCFK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent’s own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflection guided training paradigm to further enhance the decision-making capability of the framework. Extensive experiments are conducted to evaluate the performance of our proposed method, demonstrating its superior efficiency and effectiveness for multi-agent collaborative decision-making under various ramp merging scenarios.}
}


@article{DBLP:journals/tmc/ZhangGLYYL25,
	author = {Yuanyuan Zhang and
                  Runwei Guan and
                  Lingxiao Li and
                  Rui Yang and
                  Yutao Yue and
                  Eng Gee Lim},
	title = {radarODE: An ODE-Embedded Deep Learning Model for Contactless {ECG}
                  Reconstruction From Millimeter-Wave Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9806--9821},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3563945},
	doi = {10.1109/TMC.2025.3563945},
	timestamp = {Tue, 14 Oct 2025 19:49:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGLYYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radar-based cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses purely data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model that considers the fine-grained cardiac feature sensed by radar, and a novel deep learning framework called radarODE is designed to extract both temporal and morphological features for generating ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the dataset, the proposed radarODE achieves better performance compared with the benchmark in terms of missed detection rate, root mean square error, Pearson correlation coefficient with improvements of 9%, 16% and 19%, respectively. The validation results imply that radarODE is capable of recovering ECG signals from radar signals with high fidelity and can potentially be implemented in real-life scenarios.}
}


@article{DBLP:journals/tmc/DuDNL25,
	author = {Baoxia Du and
                  Hongyang Du and
                  Dusit Niyato and
                  Ruidong Li},
	title = {Task-Oriented Semantic Communication in Large Multimodal Models-Based
                  Vehicle Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9822--9836},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564543},
	doi = {10.1109/TMC.2025.3564543},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuDNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12 dB and 33.1% at 10 dB, respectively.}
}


@article{DBLP:journals/tmc/WangGWYSZZQL25,
	author = {Jingyu Wang and
                  Lingqi Guo and
                  Jianyu Wu and
                  Caijun Yan and
                  Haifeng Sun and
                  Lei Zhang and
                  Zirui Zhuang and
                  Qi Qi and
                  Jianxin Liao},
	title = {Hierarchical Index Retrieval-Driven Wireless Network Intent Translation
                  With {LLM}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9837--9851},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564937},
	doi = {10.1109/TMC.2025.3564937},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangGWYSZZQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intent-Based Networking (IBN) represents an emerging network management concept that is designed to fulfill user service requirements through automation. At its core, IBN is capable of translating user intent into network policies, thereby enabling automated configuration and management. However, the application of IBN has been limited by challenges associated with automation and intelligence. The recent widespread adoption of Large Language Model (LLM) has partially mitigated these issues. Nonetheless, hardware heterogeneity and high dynamic networks remain significant challenges for IBN: (i) Devices from different vendors are challenging to manage uniformly; (ii) Aligning service demands with rapidly changing network status is difficult. To address these challenges, we propose LIT, a framework of LLM-empowered Intent Translation with manual guidance. LIT incorporates Retrieval-Augmented Generation (RAG) to reference hardware manuals and enhance the generation results of LLMs. To reduce noise from retrieval results, we optimized the general RAG process. Additionally, LIT introduces MoE (Mixture of Experts) to adjust parameter values according to network status by synthesizing results from multiple expert models. Experiments demonstrate that LIT alleviates the challenges faced by IBN, achieving a 57.5% improvement in F1 score compared to the baseline.}
}


@article{DBLP:journals/tmc/TangZJXWZ25,
	author = {Jine Tang and
                  Wentao Zhao and
                  Jiahao Jin and
                  Yong Xiang and
                  Xiaofei Wang and
                  Zhangbing Zhou},
	title = {Adaptive Search and Collaborative Offloading Under Device-to-Device
                  Joint Edge Computing Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9852--9867},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567549},
	doi = {10.1109/TMC.2025.3567549},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TangZJXWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) and Device-to-Device (D2D) peer offloading are two promising paradigms in the mobile Internet of Things (IoT). In this paper, we study the collaborative task offloading with redundant data and codes in large-scale IoT networks, where computing resource-starved IoT devices can offload their tasks to MEC servers via cellular links or to nearby peer devices (PDs) with idle resources through D2D links for execution. IoT tasks usually consist of a series of dependent and parallel subtasks, and the difficulties in current research are (i) how to eliminate redundancy in data or codes between subtasks, and (ii) how to leverage previous experience to adaptively search a set of collaborative MEC servers and PDs for matching offloading of dependent and parallel subtasks. From this, we propose a redundancy-aware adaptive search offloading (RASO) method based on the deep Q-network (DQN). Specifically, we first design a fine-grained task recombination scheme by judging the consistency of subtask data and codes. After that, we organize the global devices into a spatial index MP-tree to reduce the search solution space, and propose a fast adaptive search method based on the DQN combined with MP-tree, where optimal path-guiding parameters training of inner and outer layers is involved to efficiently help achieve collaborative devices to complete specific tasks with the same type. After finding the collaborative MEC servers and PDs along MP-tree for a certain task, a centralized stable matching algorithm is further developed to give a decision of offloading each of its divided dependent and parallel subtasks to the matched one, thereby optimizing offloading delay and energy consumption. Extensive simulation results show that compared to other counterpart solutions, our proposed method has improved task offloading performance in terms of delay and energy consumption.}
}


@article{DBLP:journals/tmc/GuoZHCS25,
	author = {Bicheng Guo and
                  Conghao Zhou and
                  Shibo He and
                  Ji{-}Ming Chen and
                  Xuemin Shen},
	title = {Mixture-of-Experts as Continual Knowledge Adapter for Mobile Vision
                  Understanding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9868--9882},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567179},
	doi = {10.1109/TMC.2025.3567179},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoZHCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continual machine learning in the context of limited computational resources and data availability is critical in the connected digital world. Current intelligent applications predominantly rely on deep learning models requiring labor/computation-intensive training. These models often struggle to adapt effectively to new data while preserving performance on previously learned knowledge. In this paper, we introduce a lightweight method for continual knowledge adaptation that can address these challenges. To prevent disruption of the existing services, we propose a Mixture-of-Experts (MoE) adapter that integrates seamlessly with the existing vision model to encode new data. The weights of the original model are kept fixed during the adaptation process, ensuring the preservation of previously learned knowledge. The MoE technique enables scaling up the parameters of the adapter while maintaining a relatively low computation, making it fit for constrained devices in mobile computation scenarios. Furthermore, to enhance learning efficiency and accelerate convergence with new data, we implement a knowledge fusion mechanism that facilitates interaction between the existing knowledge and the information extracted from new data. The timing of employing the fusion module is further investigated. We find that it is conducive in scenarios where the task’s performance requirements are enhanced. The MoE adapter and knowledge fusion module are integrated at each stage with minimal trainable parameters, efficiently optimizing resource usage. Extensive experiments and ablation studies validate the effectiveness of the proposed method. Specifically, the proposed method prevents an accuracy drop of 43.02% on the previous data compared to the continual train method, while achieving an accuracy of 44.81% on the new data, which is even 0.34% higher than fully training a new model.}
}


@article{DBLP:journals/tmc/WuJCZSWW25,
	author = {Yihao Wu and
                  Peipei Jiang and
                  Jianhao Cheng and
                  Lingchen Zhao and
                  Chao Shen and
                  Cong Wang and
                  Qian Wang},
	title = {Sonicumos: An Enhanced Active Face Liveness Detection System via Ultrasonic
                  and Video Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9883--9901},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3565689},
	doi = {10.1109/TMC.2025.3565689},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuJCZSWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sonicumos is an enhanced behavior-based face liveness detection system that combines ultrasonic and video signals to sense the 3D head gestures. As face authentication becomes increasingly prevalent, the need for a reliable liveness detection system is paramount. Traditional behavior-based liveness detection methods (e.g., eye-blinking, nodding, etc.), which are widely deployed in mission-critical scenarios like finance and banking applications today, are prone to advanced media-based facial forgery attacks. Sonicumos aims to incorporate the traditional behavior-based method for active liveness detection without introducing extra user burden. By employing ultrasonic signals, Sonicumos capitalizes on the head gestures, significantly raising the security bar. Our approach utilizes the frequency-modulated continuous-wave (FMCW) ultrasonic radar for robust 3D gesture recognition compatible with face authentication. We also propose a new dual-feature fusion network that integrates audio and video features at the feature level to increase detection accuracy and resilience against numerous attacks. Our prototype has been tested on seven off-the-shelf Android/iOS smartphones, achieving an overall detection accuracy of 95.83% at an equal error rate (EER) of 4.96% when dealing with 3D impersonation attacks.}
}


@article{DBLP:journals/tmc/ZhaoCDNLYZTZ25,
	author = {Xiyu Zhao and
                  Qimei Cui and
                  Ziqiang Du and
                  Wei Ni and
                  Weicai Li and
                  Xi Yu and
                  Ji Zhang and
                  Xiaofeng Tao and
                  Ping Zhang},
	title = {Enhancing Convergence, Privacy and Fairness for Wireless Personalized
                  Federated Learning: Quantization-Assisted Min-Max Fair Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9902--9918},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3566421},
	doi = {10.1109/TMC.2025.3566421},
	timestamp = {Wed, 12 Nov 2025 16:29:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoCDNLYZTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized federated learning (PFL) offers a solution to balancing personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). Little attention has been given to wireless PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is another challenge resulting from communication bottlenecks in WPFL. This paper exploits quantization errors to enhance the privacy of WPFL and proposes a novel quantization-assisted Gaussian differential privacy (DP) mechanism. We analyze the convergence upper bounds of individual PL models by considering the impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and imperfect communication channels on the FL of WPFL. By minimizing the maximum of the bounds, we design an optimal transmission scheduling strategy that yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by revealing the nested structure of this problem to decouple it into subproblems solved sequentially for the client selection, channel allocation, and power control, and for the learning rates and PL-FL weighting coefficients. Experiments validate our analysis and demonstrate that our approach substantially outperforms alternative scheduling strategies by 87.08%, 16.21%, and 38.37% in accuracy, the maximum test loss of participating clients, and fairness (Jain's index), respectively.}
}


@article{DBLP:journals/tmc/YangPWXXL25,
	author = {Yu Yang and
                  Kai Peng and
                  Shangguang Wang and
                  Xiaolong Xu and
                  Peiyun Xiao and
                  Victor C. M. Leung},
	title = {Fairness-Aware Incentive Mechanism for Multi-Server Federated Learning
                  in Edge-Enabled Wireless Networks With Differential Privacy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9919--9933},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564301},
	doi = {10.1109/TMC.2025.3564301},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangPWXXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a distributed machine learning method, federated learning (FL) can collaboratively train a global model with multiple devices without sharing the original data, thus protecting certain privacy. However, due to the strong heterogeneity of edge nodes (ENs) participating in FL, the quality of data uploaded to the parameter server (PS) varies significantly. Without an appropriate incentive mechanism, low-quality contributors may receive disproportionately high rewards, while high-quality contributors may lack sufficient motivation, leading to inefficient participation and suboptimal global model performance. Consequently, it is critical to develop an effective incentive mechanism to promote fairness for the FL process. To address the issues of existing FL incentive mechanisms lacking privacy protection performance analysis, we propose a fairness-aware incentive mechanism for multi-server FL in edge-enabled wireless differential privacy (DP) networks. Specifically, the wireless channel noise is used to provide DP protection for the local model gradients uploaded by ENs. Next, the interaction between the PSs and ENs is modeled as a Stackelberg game. Furthermore, we solve the Stackelberg game process using backward induction and theoretically propose optimal strategies for both the PSs and ENs. Finally, extensive numerical simulations using real datasets demonstrate the superior performance of our theoretical analysis of the proposed scheme.}
}


@article{DBLP:journals/tmc/LangSDR25,
	author = {Natalie Lang and
                  Nir Shlezinger and
                  Rafael G. L. D'Oliveira and
                  Salim El Rouayheb},
	title = {Compressed Private Aggregation for Scalable and Robust Federated Learning
                  Over Massive Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9934--9950},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564390},
	doi = {10.1109/TMC.2025.3564390},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LangSDR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an emerging paradigm that allows a central server to train machine learning models using remote users’ data. Despite its growing popularity, Federated learning (FL) faces challenges in preserving the privacy of local datasets, its sensitivity to poisoning attacks by malicious users, and its communication overhead, especially in large-scale networks. These limitations are often individually mitigated by local differential privacy (LDP) mechanisms, robust aggregation, compression, and user selection techniques, which typically come at the cost of accuracy. In this work, we present compressed private aggregation (CPA), allowing massive deployments to simultaneously communicate at extremely low bit rates while achieving privacy, anonymity, and resilience to malicious users. CPA randomizes a codebook for compressing the data into a few bits using nested lattice quantizers, while ensuring anonymity and robustness, with a subsequent perturbation to hold LDP. CPA-aided FL is proven to converge in the same asymptotic rate as FL without privacy, compression, and robustness considerations, while satisfying both anonymity and LDP requirements. These analytical properties are empirically confirmed in a numerical study, where we demonstrate the performance gains of CPA compared with separate mechanisms for compression and privacy, as well as its robustness in mitigating the harmful effects of malicious users.}
}


@article{DBLP:journals/tmc/LiSCSD25,
	author = {Zhetao Li and
                  Weifan Shi and
                  Young{-}June Choi and
                  Hiroo Sekiya and
                  Qingyong Deng},
	title = {Location and Reward Privacy-Preserving Based Secure Task Allocation
                  in Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9951--9964},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3564404},
	doi = {10.1109/TMC.2025.3564404},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiSCSD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online multi-task allocation has become an essential research topic in Mobile Crowdsensing (MCS). Most existing studies merely focus on minimizing the total distance that workers need to travel, but ignore considering the total task rewards, which could lead to a reduction in the willingness of workers to complete tasks. In this paper, to incentivize workers to participate in tasks and protect their privacy, we propose a Location and Reward Privacy-Preserving based Secure Task Allocation(LRPP-STA) scheme. First, we design a secure distance computation method to obtain the distance from the workers to the tasks under location privacy preserving. Second, considering fixed reward for the task, we propose a Fixed Rewarding Secure Task Allocation(FR-STA) scheme, where a secure utility calculation method is proposed to calculate the encrypted utility of the worker upon completing tasks under rewards privacy preserving, along with the path planning for workers to maximize the total utility of the system through an Extended Maximum-Utility Flow model(EMUF). Third, considering the situation of dynamic task reward adjusted by requesters based on the supply and demand relationship as well as the urgency of the task, we propose a Dynamic Rewarding Secure Task Allocation(DR-STA) scheme to optimize the task allocation for workers while improving requesters satisfaction. Finally, we theoretically analyze the security of location and reward privacy-preserving scheme, and conduct extensive experiments with real-world datasets to verify that the secure task allocation scheme is effective in improving the total utility of workers compared to other baseline online tasking schemes.}
}


@article{DBLP:journals/tmc/DuHSWH25,
	author = {Lei Du and
                  Ru Huo and
                  Chuang Sun and
                  Shuo Wang and
                  Tao Huang},
	title = {Collaborative Video Processing of Multiple Cameras in Smart Transportation:
                  Content Analysis and Resource Allocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9965--9979},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567062},
	doi = {10.1109/TMC.2025.3567062},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuHSWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of smart transportation, the collaborative processing of video data sourced from multiple cameras plays a pivotal role in promoting efficient traffic management and augmenting safety measures. Nevertheless, the exponential surge in surveillance cameras deployment has concurrently engendered a rapid increase in the magnitude of video analysis tasks and data volume. To address these challenges, we propose a comprehensive framework for collaborative video processing. Primarily, a collaborative content analysis approach is proposed, and which employs a Transformer-based ReID (Re-identification) algorithm to construct key stickers. These key stickers are optimized with cross-cameras correlations and serve as the foundational structure for subsequent online video compression. Subsequently, we propose a collaborative resource allocation approach, and which involves the formulation of a queue model designed for the orchestration of online camera analysis tasks. In addition, we have devised an enhanced deep reinforcement learning algorithm to fine-tune the task scheduling configuration of multiple cameras, with guidance from the queue model. Extensive experiments and simulations were conducted to evaluate the proposed framework. The results demonstrate its effectiveness in achieving accurate and real-time analysis of video data in smart transportation scenarios.}
}


@article{DBLP:journals/tmc/GuoA25,
	author = {Hongzhi Guo and
                  Ian F. Akyildiz},
	title = {Task-Oriented Mulsemedia Communication Using Unified Perceiver and
                  Conformal Prediction in 6G Wireless Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9980--9994},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567880},
	doi = {10.1109/TMC.2025.3567880},
	timestamp = {Tue, 28 Oct 2025 16:51:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing prominence of eXtended Reality (XR), holographic-type communications, and metaverse demands truly immersive user experiences by using many sensory modalities, including sight, hearing, touch, smell, taste, etc. Additionally, the widespread deployment of sensors in areas such as agriculture, manufacturing, and smart homes is generating diverse sensory data. A new media format known as multisensory media (mulsemedia) has emerged, which incorporates many sensory modalities beyond the traditional visual and auditory media. 6G wireless systems are envisioned to support the Internet of Senses, making it crucial to explore effective data fusion and communication strategies for mulsemedia. In this paper, we introduce a task-oriented multi-task mulsemedia communication system named MuSeCo, which is developed using unified Perceiver models and Conformal Prediction. This unified model can accept any sensory input and efficiently extract latent semantic features, making it adaptable for deployment across various Artificial Intelligence of Things (AIoT) devices. Conformal Prediction is employed for modality selection and combination, enhancing task accuracy while minimizing data communication overhead. The model is trained using six sensory modalities across four classification tasks. Simulations and experiments demonstrate that it can effectively fuse sensory modalities, significantly reduce end-to-end communication latency and energy consumption, and maintain high accuracy in communication-constrained systems.}
}


@article{DBLP:journals/tmc/SunZML25,
	author = {Haitong Sun and
                  Haijun Zhang and
                  Hui Ma and
                  Victor C. M. Leung},
	title = {Joint Scheduling, Computing, and Load Balancing for Time Sensitive
                  Traffic in SDN-Enabled Space-Air-Ground Integrated 6G Networks: {A}
                  Federated Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {9995--10008},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567289},
	doi = {10.1109/TMC.2025.3567289},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunZML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) constellations and Unmanned Aerial Vehicle (UAV) networks enable wide coverage for the sixth generation (6G) mobile communication. However, it is a challenge to achieve high scheduling success rate, ultra-low latency, and efficient load balance in the Space-Air-Ground Integrated 6G Network (SAGGIN). This paper addresses the following issue: How to effectively and orderly transmit time-sensitive traffic in SAGGIN under strict deadlines, limited computational ability, and restrained link capacity? Specifically, this paper uses Software-Defined Networking (SDN) and designs a joint optimization method to enhance the traffic transmission ability of SAGGIN. Considering response time, computing cost, and link capacity in SAGGIN, the scheduling, computing, and load balance issues are modeled as a multi-objective optimization problem that minimizes the worst-case response time and computing cost of data frames while maximizing the network flow. Then, this paper leverages a Federated Reinforcement Learning (FRL) scheme to solve the problem. Results show that the FRL could achieve great scheduling, computing, and load balance performance. Specifically, our method can successfully schedule 80% of the traffic at most when the current network load is around 90%. Furthermore, the computational delay could reduce around 50%.}
}


@article{DBLP:journals/tmc/ZhangCZLWW25,
	author = {Rongyu Zhang and
                  Xiaowei Chi and
                  Wenyi Zhang and
                  Guiliang Liu and
                  Dan Wang and
                  Fangxin Wang},
	title = {Unimodal Training-Multimodal Prediction: Cross-Modal Federated Learning
                  With Hierarchical Aggregation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10009--10023},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567535},
	doi = {10.1109/TMC.2025.3567535},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCZLWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal learning has significantly advanced the extraction of features from varied data sources, enhancing model performance. Federated learning (FL) complements this by enabling collaborative training while maintaining data privacy. The fusion of these two fields, multimodal federated learning, offers considerable promise. Yet, standard methods often incorrectly assume that each node in the FL network has a full complement of multimodal data, which is rare in real-world applications. In our study, we present a novel architecture designed to surmount these challenges, termed the Unimodal Training - Multimodal Prediction (UTMP) framework, positioned within the multimodal federated learning paradigm. Our proposed model, the HA-Fedformer, is a transformer-based model crafted to facilitate unimodal training on the client-side using exclusively unimodal datasets and to execute multimodal inference by synthesizing insights from multiple clients. Our HA-Fedformer model effectively handles non-IID data through a novel uncertainty-aware aggregation technique and layer-wise Markov Chain Monte Carlo sampling in local encoders. It also resolves misaligned language sequences via cross-modal decoder aggregation, capturing correlations between decoders trained on different modalities. Our comprehensive evaluations conducted on widely recognized sentiment analysis benchmarks demonstrate the superiority of the HA-Fedformer. The results show that our model achieves a substantial uplift in performance.}
}


@article{DBLP:journals/tmc/XuHCQDC25,
	author = {Xiaolong Xu and
                  Yuhao Hu and
                  Guangming Cui and
                  Lianyong Qi and
                  Wanchun Dou and
                  Zhipeng Cai},
	title = {{CADEC:} {A} Combinatorial Auction for Dynamic Distributed {DNN} Inference
                  Scheduling in Edge-Cloud Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10024--10041},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567459},
	doi = {10.1109/TMC.2025.3567459},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuHCQDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Neural Network (DNN) Inference, as a key enabler of intelligent applications, is often computation-intensive and latency-sensitive. Combining the advantages of cloud computing (abundant computing resources) and edge computing (fast transmission), edge-cloud collaborative DNN inference is a powerful solution to these problems. However, in edge-cloud networks with heterogeneous resources, how to obtain reasonable decisions on server selection, model partition and resource allocation for efficient distributed DNN inference is a hard challenge. Furthermore, it is non-trivial to design suitable resource prices to maximize the social welfare. These challenges even escalate in dynamic edge-cloud networks where decisions should be generated as soon as each user arrives without future information. Therefore, we design a combinatorial auction for dynamic distributed DNN inference scheduling, named CADEC. CADEC first constructs a bid set for each user based on convex optimization theory for optimal solution searching. Next, prices of resources in the edge-cloud network are adjusted according to changes in supply-demand relationship, and whether to admit the request of each user is decided. Finally, the dynamic distributed inference scheduling decisions are generated through the primal-dual algorithm to maximize the social welfare. Theoretical analysis shows the good competitive ratio and polynomial time complexity of CADEC. Results of simulation experiments present that CADEC improves social welfare by up to 224% compared with state-of-the-art distributed DNN inference schemes.}
}


@article{DBLP:journals/tmc/LiJWWHSW25,
	author = {Haoyang Li and
                  Wanchun Jiang and
                  Jie Wang and
                  Ying Wang and
                  Jiawei Huang and
                  Danfeng Shan and
                  Jian{-}xin Wang},
	title = {Teaching to Fish Rather Than Giving a Fish: The Concentrator Method
                  of Teaching Classic Congestion Control With Learning-Based module},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10042--10054},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567582},
	doi = {10.1109/TMC.2025.3567582},
	timestamp = {Thu, 13 Nov 2025 11:12:00 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiJWWHSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, Congestion Control (CC) algorithms are expected to satisfy the diverse demands of applications running over diverse networks. To achieve this goal, the combinations, aiming to inherit both the advantages of classic CC in terms of convergence, overhead, and explainability, and the advantages of learning-based CC on adapting to diverse networks and demands, become a hot topic. In this paper, we reveal the existing combination works are either giving a fish or teaching to fish. Based on the insight of their essential issues, we develop the Concentrator method of teaching to fish. According to this method, we propose Seagull. Specifically, Seagull captures the network characteristics and application demands in a coarse-grained manner via an online learning module. Moreover, this module guides to customize the rate adjustment rules of the classic CC module for fine-grained system evolution. Replacing the assumption on networks by the captured characteristics, the classic CC module of Seagull can fulfill the specified application demands. Real-world experimental results show Seagull respectively outperforms Orca, PCC-Vivace, and CUBIC by  49.3 % , \xa0 30.4 % 49.3\\%,\\ 30.4\\% , and 24.9% in terms of throughput over the Internet, and improves the video quality of experience (QoE) by  12.9 ∼ 33.5 % 12.9\\sim 33.5\\%  compared to CUBIC over cellular links.}
}


@article{DBLP:journals/tmc/LiuNLGZ25,
	author = {Bin Liu and
                  Wei Ni and
                  Ren Ping Liu and
                  Y. Jay Guo and
                  Hongbo Zhu},
	title = {Delay-Sensitive Goods Delivery and In-Situ Sensing Using a Multi-Task
                  Drone},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10055--10068},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570437},
	doi = {10.1109/TMC.2025.3570437},
	timestamp = {Fri, 21 Nov 2025 20:31:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuNLGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones are evolving into highly capable and adaptable devices, prompting the development of advanced control frameworks. This paper introduces a novel online control framework tailored for a multi-task drone, explicitly addressing the simultaneous execution of in-situ sensing and goods delivery. To tackle this complex scenario, a finite-horizon Markov decision process (FH-MDP) is formulated to ensure not only the prompt delivery of goods but also the minimization of energy consumption and the maximization of the drone's reward for in-situ sensing. A significant contribution lies in establishing the monotonicity and subadditivity of the FH-MDP. This mathematical foundation provides evidence for the existence of an optimal, monotone, deterministic Markovian policy. The crux of the optimal policy revolves around flight distance- and time-related thresholds, determining the precise points at which the drone should switch its optimal action. This unique feature empowers the multi-task drone to make real-time decisions, such as adjusting flight speed or engaging in in-situ sensing, by comparing its current state with these predefined thresholds. This process can be accomplished with a linear complexity, ensuring efficiency in decision-making. The optimality of our approach is rigorously demonstrated through numerical validation, where it is compared against a computationally expensive, dynamic programming-based alternative. Under the considered simulation settings, our approach reduces drone energy consumption by a substantial 19.8% compared to existing benchmarks. This not only highlights the practical effectiveness of the proposed framework but also underscores its potential for significant advancements in the field of drone operations and energy efficiency.}
}


@article{DBLP:journals/tmc/SrinivasanSAC25,
	author = {Suresh Srinivasan and
                  Sam Shippey and
                  Ehsan Aryafar and
                  Jacob Chakareski},
	title = {{FBDT:} Sum-Throughput Achieving Transport Layer Solution for Multi-RAT
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10069--10084},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569453},
	doi = {10.1109/TMC.2025.3569453},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SrinivasanSAC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging mobile applications give rise to new bandwidth-hungry and latency-sensitive traffic classes that challenge existing wireless systems. Addressing them requires innovative approaches such as simultaneous data transmission across multiple Radio Access Technologies (RATs), e.g., WiFi and WiGig. However, existing transport layer multi-RAT traffic aggregation schemes, e.g., multi-path TCP, suffer from Head-of-Line (HoL) blocking and sub-optimal traffic splitting across the RATs that severely penalize their performance. In this paper, we investigate the design of FBDT, a novel multi-path transport layer solution that for the first time can achieve the sum of the throughput rates across the individual RATs network paths, despite their channel conditions’ dynamics. We have implemented FBDT in the Linux kernel and show substantial improvement in throughput relative to state-of-the-art schemes, e.g, 2.5x gain in a dual-RAT scenario (WiFi and WiGig) when the client is mobile. Second, we extend FBDT to more than two radios and demonstrate that its throughput performance scales linearly with the number of RATs, in contrast to multi-path TCP, whose performance degrades with an increase in the number of RATs. We evaluate the performance of FBDT on different traffic classes and demonstrate: (i) 2-3 times shorter file download times, (ii) up to 10 times shorter streaming times and 10 dB higher video quality for progressive download video applications, and (iii) up to 9 dB higher viewport quality for interactive mobile VR applications, when our viewport quality maximization framework is employed along with FBDT.}
}


@article{DBLP:journals/tmc/WangXHSRW25,
	author = {Weizheng Wang and
                  Qipeng Xie and
                  Zhaoyang Han and
                  Chunhua Su and
                  Joel J. P. C. Rodrigues and
                  Kaishun Wu},
	title = {Secure Enhanced IoT-WLAN Authentication Protocol With Efficient Fast
                  Reconnection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10085--10098},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569593},
	doi = {10.1109/TMC.2025.3569593},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangXHSRW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing integration of Internet of Things (IoT) devices in Wireless Local Area Networks (WLANs) necessitates robust and efficient authentication mechanisms. While existing IoT authentication protocols address certain security concerns, they often fail to provide comprehensive protection against threats such as perfect forward secrecy violations, insider attacks, and key compromise impersonation, or impose significant computational and communication overhead on resource- constrained IoT systems. This paper presents a novel Extensible Authentication Protocol (EAP) based scheme for IoT-WLAN environments that addresses these security challenges while maintaining cost-effectiveness. Our approach utilizes elliptic curve cryptography and incorporates advanced features including perfect forward secrecy, strong identity protection, and explicit key confirmation. We provide a thorough security analysis using informal heuristics, formal methods (Random Oracle Model and BAN Logic), and automated verification with ProVerif. Performance evaluations demonstrate that our protocol achieves lower communication, storage, and computational costs compared to state-of-the-art solutions, with an average 79.6% reduction in computation time. A detailed comparison with existing schemes highlights the efficiency and enhanced security features of our proposed authentication mechanism for IoT-WLAN deployments.}
}


@article{DBLP:journals/tmc/ZhouLQ25,
	author = {Longyu Zhou and
                  Supeng Leng and
                  Tony Q. S. Quek},
	title = {VerDT: {A} Versatile Digital Twins Framework for UAVs-Based Industrial
                  Cyber-Physical Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10099--10117},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567284},
	doi = {10.1109/TMC.2025.3567284},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of cyber-physical systems, Digital Twins (DT)-powered network autonomy is emerging to embrace the fifth-generation industrial revolution. In this context, Unscrewed Aerial Vehicles (UAVs)-based low-altitude networks are expected to be the engines that drive industrial development. As an attractive industry application, UAVs-based intelligent logistics has been widely investigated to achieve a fully automated distribution manner without the aid of a workforce. However, it is difficult to perform real-time DT implementations due to limited computing resources and the high mobility of UAVs. To address the mentioned problems, we propose a Versatile DT (VerDT) framework operating at the edge. It can enable a double DT cooperation manner with a resource scheduling model and a path planning model for real-time and accurate logistics distributions. The resource scheduling model can implement the integration of computing and communication resources among UAVs for feasible cooperative distribution decisions. With the decisions, the path planning model can imitate to derive positions and velocities of UAVs for low-latency distribution performance with energy saving. Experiment results demonstrate the efficiency of our VerDT framework. Compared to state-of-the-art logistics distribution solutions, our solution reduces the distribution latency by 63.9% while improving the successful distribution ratio by 10.9%.}
}


@article{DBLP:journals/tmc/ZhouLMYWZG25,
	author = {Huan Zhou and
                  Yadong Lu and
                  Geyong Min and
                  Zhiwen Yu and
                  Liang Wang and
                  Yao Zhang and
                  Bin Guo},
	title = {QoS-Oriented Joint Resource and Trajectory Optimization in NOMA-Enhanced
                  {AAV-MEC} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10118--10134},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3575451},
	doi = {10.1109/TMC.2025.3575451},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLMYWZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous Aerial Vehicle (AAV)-assisted Mobile Edge Computing (MEC) has received extensive attention because it provides resilient computation services for multiple Mobile Users (MUs). However, due to the increasing scale of offloaded tasks, the uncertain mobility of MUs, and the limited energy budget of AAV and MUs, it is extremely challenging to achieve satisfactory Quality-of-Service (QoS). Non-Orthogonal Multiple Access (NOMA), a promising technology to serve multiple MUs with limited communication resources, has great potential to be integrated with MEC. To this end, this paper proposes a QoS-oriented NOMA-enhanced AAV-MEC system, which aims to capture the potential gains of uplink NOMA and enable more MUs to benefit from edge computing servers in resource-constrained AAV-assisted MEC environments. This synergy reduces MUs’ uplink energy consumption but poses new challenges in resource allocation and AAV trajectory design. To address these challenges, we define a new metric called System Overhead Ratio (SOR) to reflect the system’s QoS, and then consider a joint optimization problem of resource allocation, transmission power control, and AAV trajectory design, with the goal of minimizing the SOR. Given the NP-hard nature of the optimization problem, we propose a Lyapunov and convex optimization-based Low-complexity Online Resource allocation and Trajectory optimization method (LORT) to solve it, and further analyze the convergence and complexity of LORT. Finally, extensive simulations show that the proposed method surpasses other benchmarks, reducing the SOR by approximately  10 % 10\\% - 25 %  25\\%  under various scenarios.}
}


@article{DBLP:journals/tmc/LiYFLKLN25,
	author = {Yang Li and
                  Quan Yuan and
                  Xiaoyuan Fu and
                  Guiyang Luo and
                  Jiawen Kang and
                  Jinglin Li and
                  Dusit Niyato},
	title = {{ROTR:} Role-Transformable Multi-Agent Resource Allocation for Nonstationary
                  Vehicular Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10135--10152},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567652},
	doi = {10.1109/TMC.2025.3567652},
	timestamp = {Tue, 14 Oct 2025 19:49:04 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiYFLKLN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient wireless resource allocation is essential for supporting multi-vehicle cooperation. The service data exchanged among intelligent vehicles is typically diverse, with varying transmission requirements that shift according to applications and traffic conditions, leading to major fluctuation in communication situations. Existing multi-agent reinforcement learning based resource allocation methods are often inefficient in handling such nonstationary communication situations due to their rigid cooperation patterns. To this end, we propose a ROle-TRansformable multi-agent resource allocation method, named ROTR. This method adopts a hierarchical decision-making process, where a high-level agent at a base station (BS) dynamically plans and distributes cooperation roles (CRs) and cooperation behaviors (CBs) in response to fluctuating communication situations. The Low-level agents within the transmitting vehicles (TVs) perform role transformations based on the assigned CRs and subsequently receive behavioral guidance according to CBs, enabling dynamic adjustments in cooperation patterns to adapt to variable communication situations and make resource allocation decisions. Additionally, we introduce a non-BS-assisted mode based on policy distillation, which enables a seamless transition to independent operation without the BS, relying solely on local states to generate CRs and CBs, thereby facilitating global resource cooperation. Extensive simulation experiments demonstrate that the proposed framework optimizes resource efficiency in nonstationary vehicular communications.}
}


@article{DBLP:journals/tmc/DingPZLLCX25,
	author = {Dian Ding and
                  Hao Pan and
                  Yongzhao Zhang and
                  Yijie Li and
                  Yu Lu and
                  Yi{-}Chao Chen and
                  Guangtao Xue},
	title = {TouchHBC: Touch-Based Human Body Communication via Leakage Current},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10153--10168},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569282},
	doi = {10.1109/TMC.2025.3569282},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DingPZLLCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wearable devices, including smartwatches, are increasingly popular among consumers due to their user-friendly services. However, transmitting sensitive data like social media messages and payment QR codes via commonly used low-power Bluetooth exposes users to privacy breaches and financial losses. This study introduces TouchHBC, a secure and reliable communication scheme leveraging a smartwatch’s built-in electrodes. This system establishes a touch-based human communication system utilizing a laptop’s leakage current. As the transmitting device, the laptop modulates this current via the CPU. Simultaneously, the smartwatch, equipped with built-in electrodes, captures the current traversing the human body and decodes it. The modulation and decoding processes involve techniques such as amplitude modulation, variational mode decomposition, channel estimation, and retransmission mechanisms. TouchHBC facilitates communication between laptops and smartwatches. Real-world tests demonstrate that our prototype achieves a throughput of  19.83 \xa0 bps  19.83\\ \\text{bps} . Moreover, TouchHBC offers the potential for enhanced interaction, including improved gaming experiences through vibration feedback and secure touch login for smartwatch applications by synchronizing with a laptop. Furthermore, the system can be integrated with high-throughput communication protocols such as Bluetooth, enhancing its scalability while maintaining a strong foundation of security.}
}


@article{DBLP:journals/tmc/LiZSWL25,
	author = {Yang Li and
                  Xing Zhang and
                  Yukun Sun and
                  Wenbo Wang and
                  Bo Lei},
	title = {Spatiotemporal Non-Uniformity-Aware Online Task Scheduling in Collaborative
                  Edge Computing for Industrial Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10169--10185},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567615},
	doi = {10.1109/TMC.2025.3567615},
	timestamp = {Sat, 13 Dec 2025 17:10:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZSWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing mitigates the shortcomings of cloud computing caused by unpredictable wide-area network latency and serves as a critical enabling technology for the Industrial Internet of Things (IIoT). Unlike cloud computing, mobile edge networks offer limited and distributed computing resources. As a result, collaborative edge computing emerges as a promising technology that enhances edge networks’ service capabilities by integrating computational resources across edge nodes. This paper investigates the task scheduling problem in collaborative edge computing for IIoT, aiming to optimize task processing performance under long-term cost constraints. We propose an online task scheduling algorithm to cope with the spatiotemporal non-uniformity of user request distribution in distributed edge networks. For the spatial non-uniformity of user requests across different factories, we introduce a graph model to guide optimal task scheduling decisions. For the time-varying nature of user request distribution and long-term cost constraints, we apply Lyapunov optimization to decompose the long-term optimization problem into a series of real-time subproblems that do not require prior knowledge of future system states. Given the NP-hard nature of the subproblems, we design a heuristic-based hierarchical optimization approach incorporating enhanced discrete particle swarm and harmonic search algorithms. Finally, an imitation learning-based approach is devised to further accelerate the algorithm's operation, building upon the initial two algorithms. Comprehensive theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed schemes.}
}


@article{DBLP:journals/tmc/MengWZSLLZ25,
	author = {Qianhe Meng and
                  Han Wang and
                  Chong Zhang and
                  Yihang Song and
                  Songfan Li and
                  Li Lu and
                  Hongzi Zhu},
	title = {Embedding Chips Over the Air: Rethink IoT Architecture for Ubiquitous
                  Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10186--10199},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567635},
	doi = {10.1109/TMC.2025.3567635},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MengWZSLLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale IoT sensor deployment calls for inexpensive, low-power sensor nodes that still perform long-range, large-scale networking at the system level. However, current sensor nodes are constructed according to the ’one-size-fits-all’ embedded design, where the processor and RF transceiver are indispensable but underutilized in low-duty cycles, resulting in overwhelmingly significant unit price and run-time power. In this paper, we propose a novel processor-sharing IoT architecture that converts the vast majority of sensor nodes from embedded computers to low-end RF peripherals. The conventional full-fledged sensor nodes are smashed into the air, and the scattered chips are scaled well with negligible overheads through a virtual I 2 ^{2} C bus called RFBus. Specifically, RFBus interface is designed to be backward compatible with the I 2 ^{2} C bus interface, and thus, RFBus network inherits versatile link layer services transparently from the well-established I 2 ^{2} C link layer protocol. We design RFBus with joint consideration of system-level performance and deployment costs and evaluate the prototypes both indoors and outdoors. The result indicates that the proposed architecture achieves 6.09 × (indoor) and 6.69 × (outdoor) energy saving and reduces the unit price of sensor nodes by 23.5% (indoor) and 33.5% (outdoor).}
}


@article{DBLP:journals/tmc/LuLLWX25,
	author = {Xiaozhen Lu and
                  Lixin Liu and
                  Zhibo Liu and
                  Qihui Wu and
                  Liang Xiao},
	title = {Blockchain-Based Intelligent Trusted Computational Resource Allocation
                  for Low-Altitude Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10200--10213},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3568614},
	doi = {10.1109/TMC.2025.3568614},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuLLWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In low-altitude networks, unmanned aerial vehicles (UAVs) can offer services such as logistics, intelligence surveillance, and environmental monitoring, aided by base stations (BSs) with substantial computational resources. However, BSs must defend against malicious UAVs that may overload resources or launch denial-of-service attacks. In this paper, we formulate a blockchain-enabled access control model, which uses the UAV identities (IDs) and trajectories, positive and negative interactions with the BS to evaluate the reputations of UAVs. In the blockchain, the elected miner generates blocks containing UAV IDs, coordinates, interactions, and reputation values. To defend against malicious UAVs, this paper formulates a trusted computational resource allocation optimization problem, solved by safe reinforcement learning (RL) with a three-level hierarchical structure. Specifically, this method uses the designed structure to optimize the BS access control, resource allocations, and block size. In particular, we design an E-network to evaluate the long-term risk resulting from the chosen policy, which is used to refine the policy distribution for safe exploration. A modified reward function accounts for immediate risks, preventing short-term dangerous explorations that could lead to illegal access or computational failures. We prove the Lyapunov asymptotic stability of the proposed system and derive the reward upper bound. Simulation results show that our scheme can converge to the upper bound, outperform the benchmark, and validate the effectiveness via ablation experiments.}
}


@article{DBLP:journals/tmc/ZhangWSS25,
	author = {Songge Zhang and
                  Wen Wu and
                  Lingyang Song and
                  Xuemin Shen},
	title = {Efficient Model Training in Edge Networks With Hierarchical Split
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10214--10229},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569407},
	doi = {10.1109/TMC.2025.3569407},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an efficient model training scheme, named Group-based Hierarchical Split Learning (GHSL), which can accelerate the artificial intelligence (AI) training process in edge networks in a “first-sequential-then-parallel” manner. Specifically, the proposed scheme hierarchically splits an AI model into a user-side and server-side model, while dividing a number of users into multiple groups. Users in each group train user-side models with the interaction of the shared server-side model sequentially; different groups perform the above training process parallelly; the AI models of each group are aggregated into a global model. We also carry out the convergence analysis for the proposed scheme over non-independent and identically distributed data, which reveals that the convergence rate depends on user grouping. Furthermore, we propose a data-driven two-stage user grouping algorithm to minimize the overall training delay, taking user resource heterogeneity and the black-box training process into account. The proposed algorithm first utilizes the Gaussian process regression approach to determine the number of groups, and then employs the coalition game theory to determine the optimal user grouping decision. Comprehensive simulation results demonstrate that the proposed scheme can reduce training delay, user-side computational workload, and communication overhead by up to 19%, 53%, and 54%, respectively, comparing to state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/LiuZSLLDN25,
	author = {Jingwei Liu and
                  Zihan Zhou and
                  Rong Sun and
                  Lei Liu and
                  Rongxing Lu and
                  Schahram Dustdar and
                  Dusit Niyato},
	title = {TraCemop: Toward Federated Learning With Traceable Contribution Evaluation
                  and Model Ownership Protection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10230--10246},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569547},
	doi = {10.1109/TMC.2025.3569547},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZSLLDN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) allows multiple clients to collaboratively train machine learning models without the need to share their local private data. As a result, it can effectively address the issue of data fragmentation. Nevertheless, insufficient evaluation of individual contributions and the lack of protections for both the intellectual property rights (IPR) of models and client privacy can greatly reduce clients’ motivations in federated training. To address these challenges, this paper introduces the Traceable Contribution Evaluation and Model Ownership Protection (TraCemop) framework for federated learning, which allows each client to swiftly assess the contributions of others in each round, with integrated support for the traceability of evaluation results. To safeguard the intellectual property of models, a collective watermark is embedded in the global model. Additionally, a secure mechanism for verifying model ownership is also available in case of disputes. Security analysis indicates that TraCemop is capable of resisting data reconstruction attacks as well as various types of model copyright infringements. Finally, we evaluate the proposed framework using two commonly-used datasets, and the experimental results show a significant improvement in the efficiency of contribution evaluation compared to existing methods. Meanwhile, IPR infringement tests on TraCemop reveal that the proposed framework is resilient against malicious efforts to monopolize model ownership.}
}


@article{DBLP:journals/tmc/LiuLWLS25,
	author = {Nan Liu and
                  Tom H. Luan and
                  Yuntao Wang and
                  Yiliang Liu and
                  Zhou Su},
	title = {QoE-Oriented Cooperative {VR} Rendering and Dynamic Resource Leasing
                  in Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10247--10263},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569695},
	doi = {10.1109/TMC.2025.3569695},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLWLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of the Metaverse has ushered in a new era of social networking, offering users deeply engaging spaces to connect and participate in social activities. However, rendering these virtual environments is resource-intensive. With many users accessing simultaneously and requiring diverse Metaverse services, optimizing Metaverse resources to deliver the best quality-of-experience (QoE) for users is a significant challenge. In this paper, we propose a cooperative virtual reality (VR) rendering and dynamic resource leasing mechanism to address this issue. Specifically, we first introduce a cooperative VR scene pre-rendering framework between users and Planets (i.e., edge servers hosting users), and establish a new user QoE metric named EdgeVRQoE which considers both rendering delay and visual quality. We formulate the multidimensional rendering resources (e.g., GPU, CPU, and outbound bandwidth) leasing problem between Planets and users as a double-layer decision problem, and devise a hybrid action multi-agent reinforcement learning-based dynamic resource auction mechanism to efficiently allocate limited resources of Planets in a distributed and adaptive manner. Extensive simulations demonstrate that our proposed scheme outperforms the representatives in user QoE and resource utilization efficiency. Particularly, the proposed scheme shows at least an 18-fold improvement in QoE over other schemes, demonstrating its capability in providing immersive Metaverse experiences.}
}


@article{DBLP:journals/tmc/LiangYCCYDNPY25,
	author = {Ruihuai Liang and
                  Bo Yang and
                  Pengyu Chen and
                  Xuelin Cao and
                  Zhiwen Yu and
                  M{\'{e}}rouane Debbah and
                  Dusit Niyato and
                  H. Vincent Poor and
                  Chau Yuen},
	title = {{GDSG:} Graph Diffusion-Based Solution Generator for Optimization
                  Problems in {MEC} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10264--10277},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3568248},
	doi = {10.1109/TMC.2025.3568248},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiangYCCYDNPY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optimization is crucial for the efficiency and reliability of multi-access edge computing (MEC) networks. Many optimization problems in this field are NP-hard and do not have effective approximation algorithms. Consequently, there is often a lack of optimal (ground-truth) data, which limits the effectiveness of traditional deep learning approaches. Most existing learning-based methods require a large amount of optimal data and do not leverage the potential advantages of using suboptimal data, which can be obtained more efficiently. To illustrate this point, we focus on the multi-server multi-user computation offloading (MSCO) problem, a common issue in MEC networks that lacks efficient optimal solution methods. In this paper, we introduce the graph diffusion-based solution generator (GDSG), designed to work with suboptimal datasets while still achieving convergence to the optimal solution with high probability. We reformulate the network optimization challenge as a distribution-learning problem and provide a clear explanation of how to learn from suboptimal training datasets. We develop GDSG, a multi-task diffusion generative model that employs a graph neural network (GNN) to capture the distribution of high-quality solutions. Our approach includes a straightforward and efficient heuristic method to generate a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the GNN architecture to achieve better generalization. Moreover, the proposed GDSG can achieve nearly 100% task orthogonality, which helps prevent negative interference between the discrete and continuous solution generation training objectives. We demonstrate that this orthogonality arises from the diffusion-related training loss in GDSG, rather than from the GNN architecture itself. Finally, our experiments show that the proposed GDSG outperforms other benchmark methods on both optimal and suboptimal training datasets. Regarding the minimization of computation offloading costs, GDSG achieves savings of up to 56.62% on the ground-truth training set and 41.06% on the suboptimal training set compared to existing discriminative methods.}
}


@article{DBLP:journals/tmc/HeZMH25,
	author = {Junyi He and
                  Meng Zhang and
                  Qian Ma and
                  Jianwei Huang},
	title = {Trading Fresh IoT Data With Strategic Users},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10278--10294},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571452},
	doi = {10.1109/TMC.2025.3571452},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeZMH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The immense value of IoT data in real-time applications has led to the rise of fresh IoT data trading. Existing research often neglects strategic users who optimally time their data purchases, significantly affecting market demand and revenue. This paper studies a fresh data market with strategic users arriving stochastically and having heterogeneous data valuations. Strategic users decide purchase timing based on data freshness and price, while the platform optimizes its data pricing policy to maximize profit. We first examine a dynamic pricing policy, offering a price menu to each arriving user. This analysis is technically challenging due to the varied integer programming problems faced by heterogeneous users, making direct price optimization infeasible. To address this, we adopt a mechanism design approach, analytically deriving the optimal dynamic pricing policy. To reduce implementation complexity, we also study two simpler pricing policies: single and two-price pricing. In a two-period refreshing model, we derive the optimal single and two-price pricing policies analytically. Our findings reveal that the optimal two-price policy significantly outperforms the single pricing policy, guaranteeing at least 96% of the revenue achieved by the optimal dynamic pricing policy in a two-period refreshing model. Surprisingly, despite having more purchasing options, strategic users may be worse off than if they were myopic due to higher prices. The platform actually benefits from strategic users, generating up to five times more profit with strategic users than with myopic users, even while reducing data refresh frequency.}
}


@article{DBLP:journals/tmc/WangWLDWNPF25,
	author = {Changheng Wang and
                  Zhiqing Wei and
                  Lizhe Liu and
                  Qiao Deng and
                  Yingda Wu and
                  Yangyang Niu and
                  Yashan Pang and
                  Zhiyong Feng},
	title = {Decentralized Federated Averaging via Random Walk},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10295--10311},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569423},
	doi = {10.1109/TMC.2025.3569423},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWLDWNPF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a communication-efficient distributed machine learning method that allows multiple devices to collaboratively train models without sharing raw data. FL can be categorized into centralized and decentralized paradigms. The centralized paradigm relies on a central server to aggregate local models, potentially resulting in single points of failure, communication bottlenecks, and exposure of model parameters. In contrast, the decentralized paradigm, which does not require a central server, provides improved robustness and privacy. The essence of federated learning lies in leveraging multiple local updates for efficient communication. However, this approach may result in slower convergence or even convergence to suboptimal models in the presence of heterogeneous and imbalanced data. To address this challenge, we study decentralized federated averaging via random walk (DFedRW), which replaces multiple local update steps on a single device with random walk updates. Traditional Federated Averaging (FedAvg) and its decentralized versions commonly ignore stragglers, which reduces the amount of training data and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial random walk updates, ensuring that each computation contributes to the model update. To further improve communication efficiency, we also propose a quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves convergence upper bound of order  O ( 1 k 1 − q ) \\mathcal {O}(\\frac{1}{k^{1-q}})  under convex conditions. Furthermore, we propose a sufficient condition that reveals when quantization balances communication and convergence. Numerical analysis indicates that our proposed algorithms outperform (decentralized) FedAvg in both convergence rate and accuracy, achieving a 38.3% and 37.5% increase in test accuracy under high levels of heterogeneities, without increasing communication costs for the busiest device.}
}


@article{DBLP:journals/tmc/GongHJHS25,
	author = {Hao Gong and
                  Baoqi Huang and
                  Bing Jia and
                  Lifei Hao and
                  Zhenwei Shi},
	title = {Jointly Optimizing the Energy and Time for Multi-UAV 3-D Coverage
                  of Terrestrial Regions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10312--10329},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3568788},
	doi = {10.1109/TMC.2025.3568788},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongHJHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-rotor uncrewed aerial vehicles(UAVs) have been widely employed in various sensing tasks, e.g., environmental monitoring and disaster rescuing, many of which often require full coverage of terrestrial regions by UAVs. Efforts have been devoted to minimizing one of two objectives, i.e., energy consumptions and time costs of UAVs fulfilling such tasks, whereas it is still challenging to jointly optimize both objectives due to their complicated interdependent relationship. Therefore, this paper deals with the tasks of sensing terrestrial regions with multiple UAVs, and focuses on the three-dimensional (3-D) coverage problem by formulating a multi-objective optimization problem of jointly minimizing both objectives. Specifically, in order to optimize energy consumption effectively, an advanced closed-form energy consumption model for multi-rotor UAVs is developed based on a rigorous theoretical analysis by introducing the influences of torque and acceleration, which are often ignored by existing heuristic models. Moreover, considering the NP-hardness of the problem, an innovative swarm intelligence optimization framework is established by leveraging a multitasking learning pattern to exploit cross-task knowledge transfer and adopting an improved multi-objective salp swarm algorithm. Therein, two novel operators, i.e., a variable characteristic-guided hybrid solution initialization operator and a large-scale search-space-oriented multi-mechanism solution update operator, are designed to handle continuous, discrete and even high-dimensional variables involved. Real-world experiments validate the proposed energy model due to the reduction of power consumption estimation error by up to 59% compared to baselines, and besides, extensive simulations demonstrate that the proposed algorithm significantly outperforms the benchmarks in terms of both energy consumptions and time costs.}
}


@article{DBLP:journals/tmc/HeWDYXY25,
	author = {Yuanyuan He and
                  Meiqi Wang and
                  Xianjun Deng and
                  Peng Yang and
                  Qiao Xue and
                  Laurence T. Yang},
	title = {Personalized Local Differential Privacy for Multi-Dimensional Range
                  Queries Over Mobile User Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10330--10344},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3568511},
	doi = {10.1109/TMC.2025.3568511},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeWDYXY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-dimensional range queries performed on the mobile user data records become increasingly important and popular in the fields of e-commerce, social media, transportation logistics, etc. Meanwhile, mobile users usually have different privacy requirements for different attributes of the records. A straightforward and effective approach is to first get low-dimensional range query outcomes by using existing LDP mechanisms at different privacy levels, and then derive high-dimensional range query results at each level, and finally aggregate the results from all levels. However, it incurs low utility of the query results, since the non-fixed privacy budgets and the correlation between dimensions (attributes) detrimentally impact the utility of LDP methods, ultimately rendering them ineffective in practice. In this paper, we propose a new Personalized LDP approach for Multi-dimensional Range queries (PLDP-MR) over mobile user data, consisting of the user grouping, data perturbing, data re-perturbing, and range query results aggregating steps. First, PLDP-MR offers flexible dual grouping based on user-selected privacy levels and relevant attributes to obtain the corresponding one-dimensional and two-dimensional grids. PLDP-MR optimizes the grid granularity to minimize errors from perturbing users’ attribute data with different LDP noises at non-fixed privacy levels. Furthermore, PLDP-MR carefully re-perturbs the LDP-noisy data from mobile users at lower privacy levels (i.e., having the higher utility) to achieve LDP with higher privacy levels and supplement the data volume of the corresponding groups. Thus, the data utility is effectively improved without additional privacy losses. Finally, PLDP-MR aggregates the frequencies in all the one-dimensional and two-dimensional grids related to the multi-dimensional range query at all query intervals and all privacy levels to derive the final query result with considering the correlation between attributes. The aggregations use maximum entropy optimization and maximum likelihood methods to further enhance its utility. The privacy and utility of PLDP-MR are analyzed, and extensive experiments demonstrate its effectiveness.}
}


@article{DBLP:journals/tmc/LeiWYXCDZ25,
	author = {Chengjia Lei and
                  Shaohua Wu and
                  Yi Yang and
                  Jiayin Xue and
                  Dawei Chen and
                  Pengfei Duan and
                  Qinyu Zhang},
	title = {Joint Partitioning, Allocation, and Transmission Optimization for
                  Federated Learning in Satellite Constellations via Multi-Task {MARL}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10345--10362},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3568470},
	doi = {10.1109/TMC.2025.3568470},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LeiWYXCDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Orbital edge computing (OEC) is crucial for supporting space intelligence applications within satellite networks. However, individual satellites face resource constraints, and implementing distributed processing techniques, such as federated learning (FL), across multiple satellites introduces significant scheduling complexity. To address these challenges, we first model the key factors influencing complex satellite networks, including satellite constellations, regional resource demands, inter-satellite communication and routing, energy consumption, and battery aging—a novel aspect invoked by OEC operations. We propose an adaptive aggregation method to fundamentally improve communication efficiency in OEC-based FL. To enhance scheduling performance, we formulate a unified optimization problem that jointly considers data partitioning, resource allocation, and aggregation transmission tasks within a decentralized partially observable Markov decision process (Dec-POMDP) framework. Furthermore, we introduce an episodic-phase-recalling reward shaping (EPRS) method to correlate the influences across these phases. Inspired by multi-task learning, we propose an efficient multi-agent reinforcement learning (MARL) algorithm featuring a multi-head actor-critic (MH-AC) network structure and task-equalized adaptation (TEA) technology, designed to optimize latency, energy consumption, network traffic, and battery aging. Extensive experiments validate the effectiveness of the proposed method, showing a 29.9% reduction in total training time, an 11.5% reduction in network traffic, and superior overall performance compared to rule-based methods.}
}


@article{DBLP:journals/tmc/TangLNQHLNJ25,
	author = {Wenzheng Tang and
                  Erwu Liu and
                  Wei Ni and
                  Xinyu Qu and
                  Butian Huang and
                  Kezhi Li and
                  Dusit Niyato and
                  Abbas Jamalipour},
	title = {Game-Theoretic Incentive Mechanism for Blockchain-Based Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10363--10376},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3567355},
	doi = {10.1109/TMC.2025.3567355},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TangLNQHLNJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain-based federated learning (BFL) has gained attention for its potential to establish decentralized trust. While existing research primarily focuses on personalized frameworks for various applications, essential aspects including incentive mechanisms—critical for ensuring stable system operation—remain under-explored. To bridge this gap, we propose a game-theoretic incentive mechanism designed to foster active participation in BFL tasks. Specifically, we model a BFL system comprising a model owner (MO), i.e., task publisher, multiple miners, and training terminals, framing their interactions through two-tier Stackelberg games. In the first-tier game, the MO designs reward strategies to incentivize training terminals to contribute more data, enhancing model accuracy. The second-tier game introduces a multi-leader multi-follower Stackelberg game, enabling miners to set model packaging prices based on competitors’ strategies and anticipated user behavior. By deriving the Stackelberg equilibrium, we identify optimal strategies for all participants, leading to an incentive mechanism balancing individual interests with overall performance. Compared to its benchmarks, our incentive mechanism offers 5.8% and 53.4% higher utilities in the two games compared to its alternatives, accelerating convergence and improving accuracy.}
}


@article{DBLP:journals/tmc/KumariP25,
	author = {Shilpi Kumari and
                  Ajay Pratap},
	title = {Maximizing Service Provider's Profit in Multi-UAV 5G Network
                  Via Deep Reinforcement Learning and Graph Coloring},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10377--10388},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571804},
	doi = {10.1109/TMC.2025.3571804},
	timestamp = {Tue, 14 Oct 2025 19:49:04 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KumariP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The current 5G network is expected to have a densely populated architecture comprising radio-enabled Service Provider (SP) and heterogeneous User Equipment (UE). Addressing the real-time service demands of UEs with strict deadlines is a critical challenge. Uncrewed Aerial Vehicle (UAV) assisted service provisioning is emerging as an efficient solution for timely service transfers. Therefore, SPs are interested in offering UAV-assisted service transmission to get profited by deploying UAVs. However, this introduces challenges like optimizing the locations of UAVs and Power Level (PL) along with interference management within limited available radio resources. Hence, we proposed a novel framework for multi-UAV-assisted service provisioning, consisting of Base Station (BS), UAVs, and heterogeneous UEs in 5G network. We formulate the SP’s profit maximization problem, optimizing UAVs’ location, PL, and resource allocation while considering service latency, interference management, and UAVs’ energy constraints collectively as an optimization problem. Furthermore, we propose a semi-centralized sub-optimal solution utilizing Multi-agent Deep Reinforcement Learning (MaDRL) and a Graph Coloring-based approach. Extensive simulation analysis demonstrates the proposed algorithm’s effectiveness, achieving an average of 99.05% profit compared to the optimal value.}
}


@article{DBLP:journals/tmc/WuYLGACJN25,
	author = {Sijun Wu and
                  Liang Yang and
                  Junjie Li and
                  Hongzhi Guo and
                  Ishtiaq Ahmad and
                  Daniel Benevides da Costa and
                  Hongbo Jiang and
                  Dusit Niyato},
	title = {DRL-Based Pricing-Driven for Task Offloading and Dynamic Resource
                  in Vehicle Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10389--10404},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569817},
	doi = {10.1109/TMC.2025.3569817},
	timestamp = {Tue, 28 Oct 2025 16:53:26 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuYLGACJN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle Edge Computing (VEC) assists vehicles in performing latency-sensitive tasks by deploying resources near the vehicle. Designing an incentive mechanism for vehicles and VEC is crucial for realizing an intelligent transmission system. Considering the rationality of resource allocation, we model the utility functions of the VEC and the vehicle, which are used as optimization objectives. Specifically, the VEC allocates resources through pricing to maximize revenue under resource-constrained conditions, and the vehicle weighs payments against energy consumption to determine offloading and resource allocation. Given the vehicle movement and the variable channel state, we use the Deep Reinforcement Learning (DRL) algorithm to solve these optimization problems. To reduce the learning difficulty of the DRL algorithm in complex VEC scenarios with multiple optimization variables, we propose a Pricing-Driven Resource Allocation (PDRA) algorithm that performs mobility-aware task offloading and calculates the optimal values of the optimization variables in the utility function of the vehicle to reduce the decision dimension. Furthermore, we also propose a DRL-based Pricing-Driven Dynamic Resource Allocation (DPDDRA) algorithm to achieve efficient resource allocation. Extensive experimental results show that the proposed algorithms can reduce the learning difficulty while maximizing VEC and vehicle revenue in complex VEC scenarios.}
}


@article{DBLP:journals/tmc/GuoTXZLLWL25,
	author = {Kaiwen Guo and
                  Hui Tang and
                  Tianyi Xu and
                  Hao Zhou and
                  Mengxia Lyu and
                  Zhi Liu and
                  Xiaoyan Wang and
                  Xiangyang Li},
	title = {{XHGA:} Expanding the Capabilities of Cross-Modal Wrist-Worn Devices
                  for Multi-Task Hand Gesture Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10405--10422},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569841},
	doi = {10.1109/TMC.2025.3569841},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoTXZLLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hand gesture applications (HGA) are essential for human-machine interaction. Although the existing solutions achieve good performance in specific tasks, they still face challenges when users navigate through different application contexts, i.e., requiring multi-task ability to support newly arrived HGA tasks. In this paper, we propose a novel wrist-worn multi-task HGA system named XHGA, which can implement modal-domain combination, data-domain adaptation and label-domain extension to ensure the performance in multi-task scenarios. The system introduces a novel two-stage training strategy, i.e., task-agnostic stage to align cross-modal features from unlabeled arbitrary gestures through contrastive learning, and task-related stage to learn modality contributions with limited labeled data in specific tasks through self-attention mechanism, while achieves multi-objective recognition simultaneously by employing an adaptive loss function weighting method. Extensive experiments demonstrate that XHGA can achieve an average accuracy of 92.7% with only using 15 labeled data per gesture under three HGA tasks. Compared with the state-of-the-art multi-modal approach, XHGA reduces 82.7% training time, and 47.7% storage, with about 5% improvements in accuracy.}
}


@article{DBLP:journals/tmc/ZhuSZJK25,
	author = {Jingyang Zhu and
                  Yuanming Shi and
                  Yong Zhou and
                  Chunxiao Jiang and
                  Linling Kuang},
	title = {Hierarchical Learning and Computing Over Space-Ground Integrated Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10423--10440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569887},
	doi = {10.1109/TMC.2025.3569887},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuSZJK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space-ground integrated networks hold great promise for providing global connectivity, particularly in remote areas where large amounts of valuable data are generated by Internet of Things (IoT) devices, but lacking terrestrial communication infrastructure. The massive data is conventionally transferred to the cloud server for centralized artificial intelligence (AI) models training, raising huge communication overhead and privacy concerns. To address this, we propose a hierarchical learning and computing framework, which leverages the low-latency characteristic of low-earth-orbit (LEO) satellites and the global coverage of geostationary-earth-orbit (GEO) satellites, to provide global aggregation services for locally trained models on ground IoT devices. Due to the time-varying nature of satellite network topology and the energy constraints of LEO satellites, efficiently aggregating the received local models from ground devices on LEO satellites is highly challenging. By leveraging the predictability of inter-satellite connectivity, modeling the space network as a directed graph, we formulate a network energy minimization problem for model aggregation, which turns out to be a Directed Steiner Tree (DST) problem. We propose a topology-aware energy-efficient routing (TAEER) algorithm to solve the DST problem by finding a minimum spanning arborescence on a substitute directed graph. Extensive simulations under real-world space-ground integrated network settings demonstrate that the proposed TAEER algorithm significantly reduces energy consumption and outperforms benchmarks.}
}


@article{DBLP:journals/tmc/ZhangCZPLXCL25,
	author = {Lei Zhang and
                  Peng Chen and
                  Cong Zhang and
                  Cheng Pan and
                  Tao Long and
                  Weizhen Xu and
                  Laizhong Cui and
                  Jiangchuan Liu},
	title = {Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree
                  Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10441--10455},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571186},
	doi = {10.1109/TMC.2025.3571186},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCZPLXCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to minimize transmission and computation overhead for mobile terminals. To improve viewport prediction accuracy, we utilize both spatial information through a saliency prediction model and temporal information through a modified LSTM model. Different computations introduced by the neural network models are distributed across the network to keep the computation light on mobile devices. To better adapt to the content dynamics in live streaming, we employ the model-agnostic meta-learning (MAML) method for video saliency prediction. The learned saliency prediction model with optimized initialization via offline meta-training can be fast fine-tuned online using a few samples. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results demonstrate that our approach achieves real-time prediction for live video streaming and surpasses existing methods in prediction accuracy on mobile terminals, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. Compared to baseline methods, MFVP achieves a 4.7–28.7% improvement in accuracy and demonstrates faster adaptability to dynamic content changes, enabling rapid fine-tuning and adjustment. When integrated into a streaming system and paired with our adaptive bitrate allocation algorithm, MFVP enhances overall video quality by 5.6–12.9% and reduces quality fluctuations by 33.3–50.9%.}
}


@article{DBLP:journals/tmc/DengD25,
	author = {Yong Deng and
                  Min Dong},
	title = {Design and Optimization of Heterogeneous Coded Distributed Computing
                  With Nonuniform File Popularity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10456--10473},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570907},
	doi = {10.1109/TMC.2025.3570907},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies MapReduce-based heterogeneous coded distributed computing (CDC) where, besides different computing capabilities at workers, input files to be accessed by computing jobs have nonuniform popularity. We propose a file placement strategy that can handle an arbitrary number of input files. Furthermore, we design a nested coded shuffling strategy that can efficiently manage the nonuniformity of file popularity to maximize the coded multicasting opportunity. We then formulate the joint optimization of the proposed file placement and nested shuffling design variables to optimize the proposed CDC scheme. To reduce the high computational complexity in solving the resulting mixed-integer linear programming (MILP) problem, we propose a simple two-file-group-based file placement approach to obtain an approximate solution. Both numerical studies and experimental tests show that the optimized CDC scheme outperforms other alternatives. Also, the proposed two-file-group-based approach achieves nearly the same performance as the conventional branch-and-cut method in solving the MILP problem but with substantially lower computational complexity that is scalable over the number of files and workers. For computing jobs with aggregate target functions that commonly appear in machine learning applications, we propose a heterogeneous compressed CDC (C-CDC) scheme to further improve the shuffling efficiency. The C-CDC scheme uses a local data aggregation technique to compress the data to be shuffled for the shuffling load reduction. We again optimize the proposed C-CDC scheme and explore the two-file-group-based low-complexity approach to find an approximate solution. Numerical studies show that the proposed C-CDC scheme provides a considerable shuffling load reduction over the CDC scheme, and the two-file-group-based file placement approach maintains good performance.}
}


@article{DBLP:journals/tmc/HeFFWZYC25,
	author = {Qiang He and
                  Zheng Feng and
                  Hui Fang and
                  Xing{-}Wei Wang and
                  Liang Zhao and
                  Keping Yu and
                  Kim{-}Kwang Raymond Choo},
	title = {Blockchain-Based Edge Computing Service With Dynamic Entry and Exit
                  Mechanism},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10474--10491},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570646},
	doi = {10.1109/TMC.2025.3570646},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeFFWZYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread application of 5G and artificial intelligence (AI) technology, the Internet of Things (IoT) has been expanding and integrated into various aspects of our daily lives. However, this also poses challenges such as the ubiquitous demand for communication and computing resources, and data privacy issues. Considering its flexible deployment, high security, and ease of scalability, blockchain-enabled edge computing IoT network (BECIN) has become a promising solution to provide secure and fast communication and computing services. However, existing research on computation offloading in edge computing largely overlooks the stochastic arrival of computational tasks and the potential variability in the number, locations, and resource provisions of edge computing service providers. Therefore, we propose a dynamic, self-adjusting BECIN framework aimed at providing long-term stable, efficient, and secure edge computing data offloading services for ground users in a specific region. This framework supports the dynamic entry and exit of edge computing service providers. Additionally, we introduce a novel dynamic Dueling DDQN approach to update the offloading and resource management policies based on changes in resource provisioning. Experimental results demonstrate the feasibility and superior performance of our framework on system cost and system latency.}
}


@article{DBLP:journals/tmc/ZhangYCWCLYXLZLJS25,
	author = {Yongzhao Zhang and
                  Yuqiao Yang and
                  Zhiwei Chen and
                  Zhongjie Wu and
                  Ting Chen and
                  Jun Li and
                  Jie Yang and
                  Guowen Xu and
                  Wenhao Liu and
                  Xiaosong Zhang and
                  Jingwei Li and
                  Yu Jiang and
                  Zhuo Su},
	title = {A Practical DoS Attack on Commercial {UWB} Ranging Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10492--10509},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569972},
	doi = {10.1109/TMC.2025.3569972},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangYCWCLYXLZLJS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-wideband (UWB) ranging systems are increasingly deployed in critical, security-sensitive applications due to their precise positioning and secure ranging capabilities. In this work, we introduce a practical DoS attack via reactive jamming, referred to as UWBAD+, which targets commercial UWB ranging systems by exploiting the vulnerabilities of the normalized cross-correlation process. This allows UWBAD+ to selectively and effectively disrupt ranging sessions without requiring prior knowledge of the victim devices’ configurations, leading to potentially severe consequences, such as property loss, unauthorized access, or vehicle theft. The enhanced effectiveness and low detectability of UWBAD+ stem from the following: (i) it can rapidly sniff the physical layer structures of unknown UWB systems, even in the presence of multiple UWB devices operating simultaneously; (ii) it blocks each ranging session efficiently by employing field-level jamming, thus exerting a significant impact on commercial UWB ranging systems; and (iii) its compact, reactive, and selective design based on COTS UWB chips, which makes it both affordable and less noticeable. We successfully executed real-world attacks on commercial UWB ranging systems produced by the three largest UWB chip vendors in the market, including Apple, NXP, and Qorvo. We disclosed our findings to Apple, relevant Original Equipment Manufacturers (OEMs), and the Automotive Security Research Group. As of the time of writing, the involved OEM has acknowledged this vulnerability in their automotive systems and has issued a  $ 5 , 000 {\\$} 5,000  bounty as a reward.}
}


@article{DBLP:journals/tmc/WangQJWY25,
	author = {Qianru Wang and
                  Li Ping Qian and
                  Wei Jiang and
                  Yuan Wu and
                  Xiaoniu Yang},
	title = {Integrated Communication and Computation Resource Allocation for the
                  Compressive Sensing Based Image Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10510--10522},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570447},
	doi = {10.1109/TMC.2025.3570447},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangQJWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The data compression based transmission has been envisioned as a promising solution to improve the data transmission efficiency with the limited radio resources in the future sixth-generation (6G) wireless networks. In this paper, we propose an integrated communication and computation resource allocation system for image transmission based on compressive sensing (CS), which consists of several camera devices and a base station (BS). The device side first compresses the images, after which the compressed images are transmitted using non-orthogonal multiple access (NOMA) transmission, and finally the BS restores the received compressed images. Due to the limited energy supply, the total system energy consumption is minimized by jointly optimizing the image sampling rate, the image data transmission power, the number of floating point operations per second (FLOPS), the time of image compression and the time of data transmission under the constraints of latency and the peak signal-to-noise ratio (PSNR). Due to the non-convexity of the proposed problem, after a series of equal substitutions we convexify the problem. Then, the Karush–Kuhn–Tucker (KKT) condition and the gradient descent method are used to obtain the optimal solution of the target problem. After simulation experiments, it is concluded that the proposed CS-based image transmission scheme effectively reduces the total energy consumption by a factor of 2.7 compared with frequency division multiple access (FDMA), and the total latency by 180% compared with the original image transmission.}
}


@article{DBLP:journals/tmc/GengLQLLHP25,
	author = {Jiaxiang Geng and
                  Boyu Li and
                  Xiaoqi Qin and
                  Yixuan Li and
                  Liang Li and
                  Yanzhao Hou and
                  Miao Pan},
	title = {FedEx: Expediting Federated Learning Over Heterogeneous Mobile Devices
                  by Overlapping and Participant Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10523--10536},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3572516},
	doi = {10.1109/TMC.2025.3572516},
	timestamp = {Wed, 26 Nov 2025 13:39:04 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GengLQLLHP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training latency is critical for the success of numerous intrigued applications ignited by federated learning (FL) over heterogeneous mobile devices. By revolutionarily overlapping local gradient transmission with continuous local computing, FL can remarkably reduce its training latency over homogeneous clients, yet encounter severe model staleness, model drifts, memory cost and straggler issues in heterogeneous environments. To unleash the full potential of overlapping, we propose, FedEx, a novel federated learning approach to expedite FL training over mobile devices under data, computing and wireless heterogeneity. FedEx redefines the overlapping procedure with staleness ceilings to constrain memory consumption and make overlapping compatible with participation selection (PS) designs. Then, FedEx characterizes the PS utility function by considering the latency reduced by overlapping, and provides a holistic PS solution to address the straggler issue. FedEx also introduces a simple but effective metric to trigger overlapping, in order to avoid model drifts. Experimental results show that compared with its peer designs, FedEx demonstrates substantial reductions in FL training latency over heterogeneous mobile devices with limited memory cost.}
}


@article{DBLP:journals/tmc/WuLTJHC25,
	author = {Wenhan Wu and
                  Huanghuang Liang and
                  Tianyu Tu and
                  Jiawei Jiang and
                  Chuang Hu and
                  Dazhao Cheng},
	title = {Mimir: Data-Free Federated Unlearning Through Client-Specific Prompt
                  Generation for Personalized Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10537--10556},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570018},
	doi = {10.1109/TMC.2025.3570018},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuLTJHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated unlearning (FU) has become an important area of research due to an increasing need for federated learning (FL) applications to comply with emerging data privacy regulations such as GDPR. It facilitates the removal of certain clients’ data from an already trained FL model while preserving the performance on the remaining client without the need to retrain from scratch. Existing FU methods typically require clients to have access to their training data or historical model updates, which may be impractical in real-world scenarios due to privacy constraints and changes in data availability. Moreover, FU methods may cause catastrophic unlearning, where removing a client’s data from heterogeneous, non-IID settings can negatively impact the model’s performance on data from retained clients. To address the aforementioned issues and leverage the capabilities of personalized federated learning (pFL) in handling non-IID data distributions, this paper introduce Mimir, a novel data-free federated unlearning framework designed for pFL settings. Mimir integrates both learning and unlearning phases by utilizing personalized prompts for each client. We design a distillation structure based on Generative Adversarial Networks (GANs) for client-level unlearning that does not require access to original data or historical updates. By leveraging client-specific prompts generated during the pFL phase, Mimir adapts to heterogeneous data distributions and mitigates catastrophic unlearning on the retained data. We demonstrate the effectiveness of Mimir through extensive experiments on benchmark datasets, showing its ability to forget target client data while preserving model accuracy on the remaining clients.}
}


@article{DBLP:journals/tmc/WuWZPZYLZ25,
	author = {Fan Wu and
                  Shanshan Wang and
                  Jieyu Zhou and
                  Haoye Pan and
                  Conghao Zhou and
                  Wang Yang and
                  Feng Lyu and
                  Yaoxue Zhang},
	title = {Multi-Variate Time Series Prediction of Traffic and Users for Dynamic
                  {RRH-BBU} Mapping in {C-RAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10557--10572},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570851},
	doi = {10.1109/TMC.2025.3570851},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuWZPZYLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular operators face significant challenges in cutting operating expenses while maintaining the quality of service (QoS) for users due to growing network traffic and dynamic user connections. These challenges are addressed by the cloud radio access network (C-RAN) architecture, which includes a centralized pool of baseband units (BBUs) and distributes them from remote radio heads (RRHs). The key to improving C-RAN performance is to dynamically allocate large-scale RRHs to different BBUs in real time. In this paper, we propose a user behavior-aware RRH-BBU mapping framework to improve the performance of large-scale C-RANs by predicting RRH traffic and users in advance. First, we propose a Multivariate RRH time series Prediction Model (MRPM) that captures the spatio-temporal patterns in the data to predict the traffic volume and the number of users of RRHs, which represent key indicators of RRH connection states. Second, we formulate the RRH-BBU mapping as a Markov decision process problem to optimize cost and QoS by considering BBU utilization, BBU energy consumption, RRH migration frequency, and BBU load balancing. Third, we propose a prediction-based RRH-BBU mapping scheme (PB-RBM) to find the optimal RRH-BBU mapping strategy by leveraging the prediction information of MRPM. In the PB-RBM algorithm, we employ an A3C algorithm to learn the mapping policy and group the RRHs based on a defined popularity metric to reduce the state and action space of the reinforcement learning algorithm. Finally, extensive experiments are conducted on a real-world dataset, and our algorithm is compared with several matching algorithms, such as ACKTR, heuristic, etc., to demonstrate its superiority, especially reducing 17.5% in RMSE compared to the best-performing baseline.}
}


@article{DBLP:journals/tmc/MaGLFLZY25,
	author = {Ke Ma and
                  Bin Guo and
                  Sicong Liu and
                  Cheng Fang and
                  Siqi Luo and
                  Zimu Zheng and
                  Zhiwen Yu},
	title = {AdaShift: Anti-Collapse and Real-Time Deep Model Evolution for Mobile
                  Vision Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10573--10589},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3572215},
	doi = {10.1109/TMC.2025.3572215},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaGLFLZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As computational hardware advance, integrating deep learning (DL) models into mobile devices has become ubiquitous for visual tasks. However, “data distribution shift” in live sensory data can lead to a degradation in the accuracy of mobile DL models. Conventional domain adaptation methods, constrained by their dependence on pre-compiled static datasets for offline adaptation, exhibit fundamental limitations in real-time practicality. While modern online adaptation methodologies enable incremental model evolution, they remain plagued by two critical shortcomings: computational latency from excessive resource demands on mobile devices that compromise temporal responsiveness, and accuracy collapse stemming from error accumulation through unreliable pseudo-labeling processes. To address these challenges, we introduce AdaShift, an innovative cloud-assisted framework enabling real-time online model adaptation for vision-based mobile systems operating under non-stationary data distributions. Specifically, to ensure real-time performance, the adaptation trigger and plug-and-play adaptation mechanisms are proposed to minimize redundant adaptation requests and reduce per-request costs. To prevent accuracy collapse, AdaShift introduces a novel anti-collapse parameter restoration mechanism that explicitly recovers knowledge, ensuring stable accuracy improvements during model evolution. Through extensive experiments across various vision tasks and model architectures, AdaShift demonstrates superior accuracy and 100ms-level adaptation latency, achieving an optimal balance between accuracy and real-time performance compared to baselines.}
}


@article{DBLP:journals/tmc/PreverDBPTLM25,
	author = {Pietro Brach del Prever and
                  Salvatore D'Oro and
                  Leonardo Bonati and
                  Michele Polese and
                  Maria Tsampazi and
                  Heiko Lehmann and
                  Tommaso Melodia},
	title = {{PACIFISTA:} Conflict Evaluation and Management in Open {RAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10590--10605},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570632},
	doi = {10.1109/TMC.2025.3570632},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PreverDBPTLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms making autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts generated by O-RAN applications that control RAN parameters. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA’s ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems. We use PACIFISTA to demonstrate that users can experience a 16% throughput loss even in the case of xApps with similar goals, and that applications with conflicting goals might cause severe instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify conflicting applications and maintain performance degradation below a tolerable threshold.}
}


@article{DBLP:journals/tmc/WangLFWQW25,
	author = {Tianshun Wang and
                  Peichun Li and
                  Panpan Feng and
                  Xin Wei and
                  Li Ping Qian and
                  Yuan Wu},
	title = {Compression Meets Security: Low-Complexity Linear Collaborative Federated
                  Learning With Enhanced Accuracy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10606--10621},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569669},
	doi = {10.1109/TMC.2025.3569669},
	timestamp = {Wed, 15 Oct 2025 19:22:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLFWQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has been regarded as a promising paradigm for enabling distributed model training over resource-limited edge devices. Although FL maintains data locality and enhances model generalization, it faces challenges such as model leakage and pressure from frequent model updates. Some existing schemes, such as differential privacy and model encryption, can partially alleviate these issues while sacrificing the accuracy of the modeling training or increasing the computational overheads in training. To address this issue, we design a low-complexity linear collaborative FL (LCFL) framework to enhance the privacy and accuracy of FL. Specifically, we propose the collaborative secrecy transmission (CST) algorithm by integrating a variant of Shamir’s secret-sharing with the model segmentation, which can compresses and encrypts the local models for FL. The decoding complexity of the CST algorithm is only  O ( N 3 ) O(N^{3})  under the compression ratio of  N N , which reduces the communication overhead and computational complexity. We conduct a quantitative analysis of the model error induced by the CST algorithm and derive its closed-form upper bound. Within LCFL, we formulate an optimization problem to maximize the global model accuracy in wireless FL by optimizing compression ratios, bandwidth allocation, and transmit-powers. Subsequently, we propose a low-complexity algorithm to solve this problem effectively. Numerical simulations demonstrate the efficacy of LCFL in improving FL’s accuracy and security, and the results validate the efficiency of the proposed optimization scheme for wireless FL.}
}


@article{DBLP:journals/tmc/YanYSMFA25,
	author = {Li Yan and
                  Bin Yang and
                  Haiying Shen and
                  Shohaib Mahmud and
                  Natasha Zhang Foutz and
                  Joshua Anton},
	title = {MobiRescue: Optimal Dispatching of Rescue Teams Under Flooding Disasters},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10622--10639},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3569757},
	doi = {10.1109/TMC.2025.3569757},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanYSMFA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective dispatching of rescue teams under flooding disasters is crucial. However, previous methods are either incapable of handling flooding disaster situations, or cannot accurately estimate the distribution of rescue requests and accordingly adjust the search of the rescue teams. We propose MobiRescue, a human Mobility based Rescue team dispatching system, which aims to maximize the total number of rescued people, minimize the rescue delay and the number of serving rescue teams. We studied a city-scale human mobility dataset collected under the Hurricane Florence, and observed that several natural and demographic factors are closely related to impact severity, and road segment passability must be considered. Accordingly, we first propose a Support Vector Machine based method to predict the distribution of rescue requests considering the disaster-related factors. Then, we design an Euler path based method to determine the search paths for rescue team dispatching. Subsequently, we develop a Reinforcement Learning based method to guide the search of the rescue teams. Finally, we design a multi-objective optimization problem based method to adapt to the changed road segment passability. Our experiments demonstrate that compared with the other methods, MobiRescue increases the total number of timely served rescue requests by 43.4% in average.}
}


@article{DBLP:journals/tmc/ChiHS25,
	author = {Genghui Chi and
                  Qinlong Huang and
                  Caiqun Shi},
	title = {Privacy-Preserving and Autonomous-Path Access Delegation for Mobile
                  Cloud Healthcare Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10640--10653},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570769},
	doi = {10.1109/TMC.2025.3570769},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChiHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile cloud healthcare systems are gaining popularity due to their remote data collection through mobile devices and flexible data access through cloud services. The collected electronic health records (EHRs) are generally encrypted on mobile devices before being uploaded to the cloud, and accessed only by specific users. This incurs inconvenience when re-sharing EHRs with new receivers, especially in heterogeneous scenarios. Although cross-domain proxy re-encryption (CD-PRE) schemes have been studied to transform ciphertexts between different cryptosystems, they can neither support EHRs’ controlled multi-hop delegation between trusted delegatees chosen by the delegator nor prevent EHRs’ privacy inference through delegatees’ information. To this end, we present CAP-PRE for mobile cloud healthcare systems, which is a multi-hop CD-PRE scheme that supports lightweight encryption and re-encryption key generation on mobile devices, as well as privacy-preserving and autonomous-path access delegation. In CAP-PRE, the delegator creates a delegation path that includes preferred delegatees, and generates corresponding re-encryption keys which enables the cloud server to convert the collected ciphertext to an inner product ciphertext for privacy-preserving EHR re-sharing and pass the access rights along the delegation path for controlled multi-hop delegation. We finally prove the security of CAP-PRE and show the better performance of CAP-PRE with extensive experiments.}
}


@article{DBLP:journals/tmc/LinXCL25,
	author = {Zhiping Lin and
                  Liang Xiao and
                  Hongyi Chen and
                  Zefang Lv},
	title = {Collaborative Perception Against Data Fabrication Attacks in Vehicular
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10654--10667},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571013},
	doi = {10.1109/TMC.2025.3571013},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinXCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative perception in vehicular networks enables the connected autonomous vehicle (CAV) to gather sensing data, such as feature maps of light detection and ranging (LiDAR) point clouds, from neighboring CAVs to achieve higher perception accuracy, which has performance degradation against data fabrication attacks that share falsified sensing data with random probability. In this paper, we exploit the spatial consistency check to detect the potentially manipulated regions in LiDAR point clouds and measure the inconsistency degree of the received sensing data based on the number of conflict regions, which is the basis for determining the falsified sensing data if the inconsistency degree exceeds the threshold of the hypothesis test. The reinforcement learning (RL)-based collaborative vehicular perception scheme against data fabrication attacks is further proposed to choose CAVs based on the inconsistency degrees, the data quality measured by the confidence scores, the channel gains and the CAV reputations, which enhances the utility as the weighted sum of perception accuracy, speed and minimum latency requirement for data transmission. In addition, the multi-layer perceptron-based neural networks extract the perception features of sensing data from historical experiences, such as the data quality of received feature maps, as well as compress the RL state that linearly increases with the network scales and the spatial granularity of LiDAR point clouds for faster learning. Experimental results based on 10 CAVs equipped with LiDAR sensors and NVIDIA computational units to detect 20 vehicles against data fabrication attacks show that our proposed scheme outperforms the benchmarks in terms of perception accuracy and speed.}
}


@article{DBLP:journals/tmc/YuXZHZ25,
	author = {Shiming Yu and
                  Xianjin Xia and
                  Ziyue Zhang and
                  Ningning Hou and
                  Yuanqing Zheng},
	title = {FDLoRa: Scaling Downlink Concurrent Transmissions With Full-Duplex
                  LoRa Gateways},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10668--10682},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3572130},
	doi = {10.1109/TMC.2025.3572130},
	timestamp = {Tue, 14 Oct 2025 19:49:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuXZHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unlike traditional data collection applications which primarily rely on uplink transmissions, emerging applications (e.g., device actuation, firmware update, packet reception acknowledgment) increasingly demand robust downlink transmission capabilities. Current LoRaWAN systems struggle to support these applications due to the inherent asymmetry between downlink and uplink capabilities. While uplink transmissions can handle multiple packets simultaneously, downlink transmissions are restricted to a single logical channel at a time, significantly limiting the deployment of applications that require substantial downlink capacity. To address this challenge, FDLoRa introduces an innovative in-band full-duplex LoRa gateway design, featuring novel solutions to mitigate self-interference (i.e., the strong downlink interference to ultra-weak uplink reception). This approach enables full-spectrum in-band downlink transmissions without compromising the reception of weak uplink packets. Building on the capabilities of full-duplex gateways, FDLoRa presents a new downlink framework that supports concurrent downlink transmissions across multiple logical channels of available gateways. Evaluation results show that FDLoRa enhances downlink capacity by 5.7× compared to LoRaWAN in a three-gateway testbed and achieves 2.58× higher downlink concurrency per gateway than the current leading solutions.}
}


@article{DBLP:journals/tmc/HanMBCZ25,
	author = {Rui Han and
                  Jiahao Ma and
                  Lin Bai and
                  Jinho Choi and
                  Wei Zhang},
	title = {Communication-Efficient Multi-Server Federated Learning via Over-the-Air
                  Computation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10683--10695},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573600},
	doi = {10.1109/TMC.2025.3573600},
	timestamp = {Sun, 04 Jan 2026 13:45:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanMBCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Thanks to the Internet of Things (IoT), there has been explosive growth in edge devices, which generate a tremendous amount of data that holds invaluable potential. However, conventional data mining and machine learning (ML) paradigms require transmitting raw data to data centers for further use, which puts a heavy burden on communication networks and is exposed to high privacy risks. Federated learning allows for the training of ML models using distributed datasets, which can be applied to protect data privacy and alleviate transmission burdens. Meanwhile, the technique of over-the-air (OTA) computation can be utilized to exploit the superposition property of wireless communication channels. Motivated by this, in this paper, we propose a co-phase OTA approach for communication-efficient uploading in multi-server federated learning, which does not require expansion of the uplink channel bandwidth when the numbers of users and models increase. Besides, the digital OTA with randomized transmission is proposed to overcome the disadvantages of analog OTA, where the performance analyses of analog OTA and digital OTA are deduced, respectively. Simulation results show that a lower cost function can be obtained by digital OTA while requiring fewer iterations for convergence than that in analog OTA as more users can upload.}
}


@article{DBLP:journals/tmc/GongH25,
	author = {Jie Gong and
                  Jiajie Huang},
	title = {Age-Energy Analysis in Multi-Source Systems With Wake-Up Control and
                  Packet Management},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10696--10709},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571419},
	doi = {10.1109/TMC.2025.3571419},
	timestamp = {Sun, 23 Nov 2025 13:28:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GongH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, there has been an increasing focus on real-time mobile applications, such as news updates and weather forecast. In these applications, data freshness is of significant importance, which can be measured by age-of-synchronization (AoS). At the same time, the reduction of carbon emission is increasingly required by the communication operators. Thus, how to reduce energy consumption while keeping the data fresh becomes a matter of concern. In this paper, we study the age-energy trade-off in a multi-source single-server system, where the server can turn to sleep mode to save energy. We adopt the stochastic hybrid system (SHS) method to analyze the average AoS and power consumption with three wake-up policies including N-policy, single-sleep policy and multi-sleep policy, and three packet preemption strategies, including Last-Come-First-Serve with preemption-in-Service (LCFS-S), LCFS with preemption-only-in-Waiting (LCFS-W), and LCFS with preemption-and-Queueing (LCFS-Q). The trade-off performance is analyzed via both closed-form expressions and numerical simulations. It is found that N-policy attains the best trade-off performance among all three sleep policies. Among packet management strategies, LCFS-S is suitable for scenarios with high requirements on energy saving and small arrival rate difference between sources. LCFS-Q is suitable for scenarios with high requirements on information freshness and large arrival rate difference between sources.}
}


@article{DBLP:journals/tmc/FanMWBLL25,
	author = {Wenhao Fan and
                  Qingcheng Meng and
                  Guan Wang and
                  Hengwei Bian and
                  Yabin Liu and
                  Yuan'an Liu},
	title = {Satellite Edge Intelligence: DRL-Based Resource Management for Task
                  Inference in LEO-Based Satellite-Ground Collaborative Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10710--10728},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570799},
	doi = {10.1109/TMC.2025.3570799},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanMWBLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distinguished from terrestrial edge intelligence, satellite edge intelligence has unique characteristics, including the rapid mobility of satellites, limitations in computing and energy resources, and differences in the artificial intelligence models deployed on user devices, satellites, and ground cloud servers. In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource management scheme for task inference in Low Earth Orbit (LEO)-based satellite-ground collaborative networks. In our approach, the task of a user can be inferred by the user device itself, the edge server of the current satellite via user-to-satellite transmission, the edge server of a neighboring satellite via satellite-to-satellite transmission, or a ground cloud server via satellite-to-cloud transmission. Our scheme jointly optimizes task offloading, computing resource allocation, and communication resource allocation to minimize the total system cost, which encompasses trade-offs among the task inference delays for all tasks, the energy consumption of system, and the task inference accuracies for all tasks, while ensuring that the transmit power budgets of all satellites and the satellite coverage time constraints for each user are met. A DRL-based algorithm combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm and two numerical methods is designed to solve the optimization problem efficiently. We prove the convergence of our algorithm and demonstrate the superiority of our scheme by performing extensive simulations in 4 scenarios with 4 reference schemes.}
}


@article{DBLP:journals/tmc/ZuoYFHSQZ25,
	author = {Shiyuan Zuo and
                  Xingrun Yan and
                  Rongfei Fan and
                  Han Hu and
                  Hangguan Shan and
                  Tony Q. S. Quek and
                  Puning Zhao},
	title = {Federated Learning Resilient to Byzantine Attacks and Data Heterogeneity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10729--10742},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571058},
	doi = {10.1109/TMC.2025.3571058},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZuoYFHSQZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses federated learning (FL) in the context of malicious Byzantine attacks and data heterogeneity. We introduce a novel Robust Average Gradient Algorithm (RAGA), which uses the geometric median for aggregation and allows flexible round number for local updates. Unlike most existing resilient approaches, which base their convergence analysis on strongly-convex loss functions or homogeneously distributed datasets, this work conducts convergence analysis for both strongly-convex and non-convex loss functions over heterogeneous datasets. The theoretical analysis indicates that as long as the fraction of the data from malicious users is less than half, RAGA can achieve convergence at a rate of  O ( 1 / T 2 / 3 − δ ) \\mathcal {O}({1}/{T^{2/3- \\delta }})  for non-convex loss functions, where  T T  is the iteration number and  δ ∈ ( 0 , 2 / 3 ) \\delta \\in (0, 2/3) . For strongly-convex loss functions, the convergence rate is linear. Furthermore, the stationary point or global optimal solution is shown to be attainable as data heterogeneity diminishes. Experimental results validate the robustness of RAGA against Byzantine attacks and demonstrate its superior convergence performance compared to baselines under varying intensities of Byzantine attacks on heterogeneous datasets.}
}


@article{DBLP:journals/tmc/ShengBCGX25,
	author = {Biyun Sheng and
                  Yan Bao and
                  Hui Cai and
                  Linqing Gui and
                  Fu Xiao},
	title = {I Sense You Fast: Simultaneous Action and Identity Inference by Slimming
                  Multi-Branch RadarNet},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10743--10759},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570757},
	doi = {10.1109/TMC.2025.3570757},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShengBCGX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing connection between internet and human society, millimeter-wave radar based action recognition and user authentication exhibit remarkable prospects in security scenarios. Existing solutions usually focus on one of the tasks and mainly emphasize accuracy without reducing the inference time. In this paper, we propose a dual-task based Polymorphic Lightweight (PolyLite) RadarNet framework, in which the shared features are fed into two split streams for different tasks under joint supervision. The polymorphic concept here means that the trained network with parallel designs can be slimmed as a single-branch structure for inference. By this design strategy, we can not only efficiently extract spatial-temporal features during the training stage but also largely improve the response speed for simultaneously testing human activities and identities. Specifically, we design triple-view (TRIview) video-like data as the input by successively concatenating the range-velocity and range-angle matrices. Then a PolyLite module with linear and lightweight designs in each branch is integrated into our RadarNet framework to learn discriminative representations. Experimental results demonstrate that our approach is able to reach the accuracy over 98 % {\\%}  within 0.21 ms inference time. Especially, untrained intruders can also be successfully identified by a simple matching computation.}
}


@article{DBLP:journals/tmc/ZhangPLFWDX25,
	author = {Junru Zhang and
                  Cheng Peng and
                  Zhidan Liu and
                  Lang Feng and
                  Yuhan Wu and
                  Yabo Dong and
                  Duanqing Xu},
	title = {DI2SDiff++: Activity Style Decomposition and Diffusion-Based Fusion
                  for Cross-Person Generalization in Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10760--10777},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3572220},
	doi = {10.1109/TMC.2025.3572220},
	timestamp = {Mon, 19 Jan 2026 17:29:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangPLFWDX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing domain generalization (DG) methods for cross-person sensor-based activity recognition tasks often struggle to capture both intra- and inter-domain style diversity, leading to significant domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity, termed Diversified Intra- and Inter-domain distributions via activity Style-fused Diffusion modeling (DI2SDiff). In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random style representations from the same class to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible combinations among existing styles to generate a broad spectrum of new style instances. We further extend DI2SDiff into DI2SDiff++ by enhancing the diversity of style guidance. Specifically, DI2SDiff++ integrates a multi-head style conditioner to provide multiple distinct, decomposed substyles and introduces a substyle-fused sampling strategy that allows cross-class substyle fusion for broader guidance. Empirical evaluations on a wide range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have been proven significant and valuable, enabling DI2SDiff and DI2SDiff++ to surpass state-of-the-art DG methods in various cross-person activity recognition tasks.}
}


@article{DBLP:journals/tmc/HeLADDN25,
	author = {Changpeng He and
                  Yang Lu and
                  Bo Ai and
                  Octavia A. Dobre and
                  Zhiguo Ding and
                  Dusit Niyato},
	title = {{ICGNN:} Graph Neural Network Enabled Scalable Beamforming for {MISO}
                  Interference Channels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10778--10791},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570648},
	doi = {10.1109/TMC.2025.3570648},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeLADDN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the graph neural network (GNN)-enabled beamforming design for interference channels. We propose a model termed interference channel GNN (ICGNN) to solve a quality-of-service constrained energy efficiency maximization problem. The ICGNN is two-stage, where the direction and power parts of beamforming vectors are learned separately but trained jointly via unsupervised learning. By formulating the dimensionality of features independent of the transceiver pairs, the ICGNN is scalable with the number of transceiver pairs. Besides, to improve the performance of the ICGNN, the hybrid maximum ratio transmission and zero-forcing scheme reduces the output ports, the feature enhancement module unifies the two types of links into one type, the subgraph representation enhances the message passing efficiency, and the multi-head attention and residual connection facilitate the feature extracting. Furthermore, we present the over-the-air distributed implementation of the ICGNN. Ablation studies validate the effectiveness of key components in the ICGNN. Numerical results also demonstrate the capability of ICGNN in achieving near-optimal performance with an average inference time less than 0.1 ms. The scalability of ICGNN for unseen problem sizes is evaluated and enhanced by transfer learning with limited fine-tuning cost. The results of the centralized and distributed implementations of ICGNN are illustrated.}
}


@article{DBLP:journals/tmc/XuWLWX25,
	author = {Sheng Xu and
                  Linlong Wu and
                  Xianliang Li and
                  Xinyu Wu and
                  Tiantian Xu},
	title = {{AOA} Sensor Placement for Anchor-Assisted Target Localization in
                  GNSS-Denied Environment: Formulation, Bounds and Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10792--10806},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570768},
	doi = {10.1109/TMC.2025.3570768},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWLWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Target localization technology is widely applied in various applications, such as rescue missions, robot navigation, and the Internet of Things. However, in some scenarios, the positions of sensors are unknown due to the load limitation of the sensor carriers and environmental interferences, resulting in the instability of the global navigation satellite system (GNSS). This paper focuses on optimal angle-of-arrival (AOA) sensor placement using multiple position-unknown sensors for target localization accuracy improvement. To guarantee the uniqueness of the target coordinate, at least two anchors are needed. The anchors are some static benchmark objects in the environment with priori known positions. Firstly, a new optimization problem for AOA target localization accuracy improvement incorporating position-unknown sensors and anchors is formulated. Secondly, the optimal theoretical localization accuracies of the unknown sensors and target are derived by minimizing the trace of the Cramér-Rao lower bounds (CRLBs). Thirdly, a mixture optimization method, including a geometrical initialization and the new proposed simultaneous perturbation stochastic approximation and adaptive momentum estimation (SPSA-Adam) algebraic algorithm, is developed. Then, the correctness of the new theoretical findings and the effectiveness of the proposed sensor placement optimization method are verified by simulation examples.}
}


@article{DBLP:journals/tmc/YaoLLWZW25,
	author = {Junmei Yao and
                  Chaoyang Liu and
                  Sheng Luo and
                  Lu Wang and
                  Tingting Zhang and
                  Kaishun Wu},
	title = {Toward Integrated Sensing and Communication: Interference-Resistance
                  Design for WiFi Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10807--10822},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570752},
	doi = {10.1109/TMC.2025.3570752},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YaoLLWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi has been widely used for local area networking of devices and Internet access in the past two decades. Many researchers exploit WiFi signals for target sensing through analyzing the Channel State Information (CSI) of signals affected by the target movement. With the development of 6G Integrated Sensing and Communication (ISAC), some researchers further consider using communication data packets for WiFi sensing. However, all the current works do not analyze the impact of ubiquitous interference on WiFi sensing performance. In this paper, we propose IRSensing, an interference-resistance design to improve the CSI quality under interference in the ISAC scenario, aiming to improve the WiFi sensing performance. IRSensing exploits the overall WiFi packet for CSI optimization. It first measures the interference level of each subcarrier based on variance analysis, then proposes a CSI optimization method based on maximal ratio combining to improve the CSI quality. It finally proposes a practical CSI enhancement process to adapt to complex interference situations in actual networks. We implement IRSensing on a hardware testbed and evaluate its performance under different settings. Experiment results show that it can significantly decrease the activity detection error rate by up to 80% and improve the classification accuracy by up to 15 % \\% .}
}


@article{DBLP:journals/tmc/DuJLWZ25,
	author = {Zhuang Du and
                  Jian Jiao and
                  Hao Liu and
                  Ye Wang and
                  Qinyu Zhang},
	title = {Multi-Attribute Consistency Segment Resilient Routing for {LEO} Satellite
                  Mega Constellations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10823--10839},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570670},
	doi = {10.1109/TMC.2025.3570670},
	timestamp = {Sun, 02 Nov 2025 12:34:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuJLWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low earth orbit (LEO) satellite mega constellations are regarded to provide pervasive intelligent services in the upcoming sixth generation network via the inter-satellite links (ISL). However, the inherent challenges of LEO satellites including limited onboard resources and failure-prone topology, create substantial hurdles for multi-attribute services routing in mega constellations. In this paper, we propose a multi-attribute consistency segment resilient (MCSR) routing algorithm, and a segmentation approach is designed to partition the mega constellation into non-intersecting segment routing domains (SRDs) through joint optimization of intra- and inter-SDRs update time, which leads to the potential of balancing network load and minimizing routing convergence time. Then, we utilize the multi-attribute consistency to determine the dominant paths of ISLs within and between SRDs for multi-attribute services. Furthermore, we develop a resilient rerouting strategy that utilizes the ephemeris to manage periodic ISL handovers, and selects a reserved/recalculated candidate path from the dominant paths for ISL random failures. Thus, our MCSR routing can converge to an optimal path for multi-attribute services from the dominant paths under ISL failures in mega constellations. Finally, we develop a testbed and simulation results validate the advantages of MCSR routing in handling multi-attribute services and rerouting capability in response to failures.}
}


@article{DBLP:journals/tmc/ZhangLHZC25,
	author = {Yingqian Zhang and
                  Chao Li and
                  Shibo He and
                  Xiangliang Zhang and
                  Jiming Chen},
	title = {Toward Generalized Urban Computing: Pretraining a Spatial-Temporal
                  Model for Diverse Urban Tasks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10840--10852},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573373},
	doi = {10.1109/TMC.2025.3573373},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLHZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban computing leverages data analysis to improve urban areas’ efficiency and sustainability, tackling tasks like traffic management, crime forecasting, and air quality predictions. Current models, while efficient, often struggle with tasks beyond their initial training due to limited flexibility. Typically, new tasks require developing specialized models, which may not perform optimally with limited data. To overcome these challenges, we propose the development of a universal pretrained model that understands a city’s various aspects comprehensively. This model serves as a robust foundation, ready to be quickly adjusted for different urban tasks as they arise, even if they occur in different cities. Unlike language models, urban computing models must handle unique spatial-temporal dynamics, making standard pretraining techniques inadequate. Our approach includes a spatial-temporal module with multi-graph convolution and temporal attention mechanisms, capturing the necessary spatial-temporal patterns during pretraining. We also integrate a prompt-tuning module within this framework, which can be adapted for new predictive tasks. The results of extensive experiments on four urban predictive tasks across two cities demonstrate the effectiveness of our model.}
}


@article{DBLP:journals/tmc/JiaCDWLNH25,
	author = {Ziye Jia and
                  Can Cui and
                  Chao Dong and
                  Qihui Wu and
                  Zhuang Ling and
                  Dusit Niyato and
                  Zhu Han},
	title = {Distributionally Robust Optimization for Aerial Multi-Access Edge
                  Computing via Cooperation of UAVs and HAPs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10853--10867},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571023},
	doi = {10.1109/TMC.2025.3571023},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiaCDWLNH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an extensive increment of computation demands, the aerial multi-access edge computing (MEC), mainly based on uncrewed aerial vehicles (UAVs) and high altitude platforms (HAPs), plays significant roles in future network scenarios. In detail, UAVs can be flexibly deployed, while HAPs are characterized with large capacity and stability. Hence, in this paper, we provide a hierarchical model composed of an HAP and multi-UAVs, to provide aerial MEC services. Moreover, considering the errors of channel state information from unpredictable environmental conditions, we formulate the problem to minimize the total energy cost with the chance constraint, which is a mixed-integer nonlinear problem with uncertain parameters and intractable to solve. To tackle this issue, we optimize the UAV deployment via the weighted K-means algorithm. Then, the chance constraint is reformulated via the distributionally robust optimization (DRO). Furthermore, based on the conditional value-at-risk mechanism, we transform the DRO problem into a mixed-integer second order cone programming, which is further decomposed into two subproblems via the primal decomposition. Moreover, to alleviate the complexity of the binary subproblem, we design a binary whale optimization algorithm. Finally, we conduct extensive simulations to verify the effectiveness and robustness of the proposed schemes by comparing with baseline mechanisms.}
}


@article{DBLP:journals/tmc/TongXYXLQ25,
	author = {Xinyu Tong and
                  Xiaoqiang Xu and
                  Aiwen Yu and
                  Xin Xie and
                  Xiulong Liu and
                  Wenyu Qu},
	title = {{STAGR:} Simultaneous Tracking and Gait Recognition With Commodity
                  Wi-Fi},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10868--10885},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3570993},
	doi = {10.1109/TMC.2025.3570993},
	timestamp = {Fri, 30 Jan 2026 19:34:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TongXYXLQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location-based services and identification hold promise for future smart home applications. Through them, we can provide customized services for specific users in current locations. Recent studies have demonstrated that Wi-Fi signals can be leveraged to achieve device-free tracking and gait recognition. Despite their good performance, these two technologies are not effectively integrated for the following reasons: First, the device-free tracking method might yield tracking results that conflict with human gait. Second, extracting gait features relies on knowing or accurately estimating the user’s trajectory. Consequently, gait recognition and tracking are inherently linked, but there has been no effective approach to integrate these two techniques. In this paper, we present STAGR, a system capable of Simultaneous Tracking And Gait Recognition. The main contribution of our technique is that we establish a theoretical model that reveals how to transform path-dependent spectra into path-independent spectra directly. Specifically, we conduct a preliminary study to demonstrate the need for simultaneous tracking and gait recognition. Second, we propose a novel method to extract path-independent gait features, which can significantly save execution time compared with the learning-based method. Third, we design a polar-coordinate filtering method to retain the gait features while correcting the trajectory. We implement a prototype STAGR system and conduct extensive experiments to verify the proposed mechanism. The experimental results show that we can realize simultaneous tracking and gait recognition. The median tracking error is  0.45 m  0.45\\;{\\rm{m}} , while the recognition accuracy is 95.3% for 6 users.}
}


@article{DBLP:journals/tmc/ChenZZCQ25,
	author = {Ning Chen and
                  Songwei Zhang and
                  Xiaobo Zhou and
                  Song Cao and
                  Tie Qiu},
	title = {Fast Robustness Enhancement for Dynamic IIoT Topology With Adaptive
                  Bayesian Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10886--10899},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571431},
	doi = {10.1109/TMC.2025.3571431},
	timestamp = {Tue, 14 Oct 2025 19:49:03 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZZCQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In resource-constrained and dynamic Industrial Internet of Things (IIoT) environments, ensuring robust and adaptable network topologies remains a significant challenge. Existing reinforcement learning-based approaches tackle topology optimization but face scalability issues due to high computational complexity and latency under strict time constraints. To address these challenges, we propose FRED-ABL (Fast Robustness Enhancement for Dynamic IIoT topology optimization with Adaptive Bayesian Learning), a novel paradigm that delivers lightweight topology solutions within a constrained time frame. FRED-ABL introduces an innovative topology structure compression method leveraging auxiliary continuous coding, enabling lossless representation of network structures as model inputs. It further defines a new robustness performance metric that integrates considerations of node failures and connection capabilities, serving as a comprehensive evaluation function. By developing an adaptive Bayesian learning model, FRED-ABL efficiently maps the relationship between topology structures and robustness metrics, enabling rapid optimization while significantly reducing computational overhead. Extensive experiments demonstrate that FRED-ABL consistently outperforms state-of-the-art methods, delivering superior robustness and optimization efficiency even in large-scale IIoT deployments.}
}


@article{DBLP:journals/tmc/WangZWWWDSHH25,
	author = {Fei Wang and
                  Tingting Zhang and
                  Xilei Wu and
                  Pengcheng Wang and
                  Xin Wang and
                  Han Ding and
                  Jingang Shi and
                  Jinsong Han and
                  Dong Huang},
	title = {You Can Wash Hands Better: Accurate Daily Handwashing Assessment With
                  a Smartwatch},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10900--10913},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571805},
	doi = {10.1109/TMC.2025.3571805},
	timestamp = {Thu, 27 Nov 2025 10:47:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZWWWDSHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hand hygiene is among the most effective daily practices for preventing infectious diseases such as influenza, malaria, and skin infections. While professional guidelines emphasize proper handwashing to reduce the risk of viral infections, surveys reveal that adherence to these recommendations remains low. To address this gap, we propose UWash, a wearable solution leveraging smartwatches to evaluate handwashing procedures, aiming to raise awareness and cultivate high-quality handwashing habits. We frame the task of handwashing assessment as an action segmentation problem, similar to those in computer vision, and introduce a simple yet efficient two-stream UNet-like network to achieve this goal. Experiments involving 51 subjects demonstrate that UWash achieves 92.27% accuracy in handwashing gesture recognition, an error of  < <  0.5 seconds in onset/offset detection, and an error of  < <  5 points in gesture scoring under user-dependent settings. The system also performs robustly in user-independent and user-independent-location-independent evaluations. Remarkably, UWash maintains high performance in real-world tests, including evaluations with 10 random passersby at a hospital 9 months later and 10 passersby in an in-the-wild test conducted 2 years later. UWash is the first system to score handwashing quality based on gesture sequences, offering actionable guidance for improving daily hand hygiene.}
}


@article{DBLP:journals/tmc/GuCCHJFDW25,
	author = {Yangyang Gu and
                  Jing Chen and
                  Congrui Chen and
                  Kun He and
                  Ju Jia and
                  Yebo Feng and
                  Ruiying Du and
                  Cong Wu},
	title = {CSIPose: Unveiling Human Poses Using Commodity WiFi Devices Through
                  the Wall},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10914--10926},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3571469},
	doi = {10.1109/TMC.2025.3571469},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuCCHJFDW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of WiFi devices and the development of WiFi sensing have alerted people to the threat of WiFi sensing-based privacy leakage, especially the privacy of human poses. Existing work on human pose estimation is deployed in indoor scenarios or simple occlusion (e.g., a wooden screen) scenarios, which are less privacy-threatening in attack scenarios. To reveal the risk of leakage of the pose privacy to users from commodity WiFi devices, we propose CSIPose, a privacy-acquisition attack that passively estimates dynamic and static human poses in through-the-wall scenarios. We design a three-branch network based on transfer learning, auto-encoder, and self-attention mechanisms to realize the supervision of video frames over CSI frames to generate human pose skeleton frames. Notably, we design AveCSI, a unified framework for preprocessing and feature extraction of CSI data corresponding to dynamic and static poses. This framework uses the average of CSI measurements to generate CSI frames to mitigate the instability of passively collected CSI data, and utilizes a self-attention mechanism to enhance key features. We evaluate the performance of CSIPose across different room layouts, subjects, devices, subject locations, and device locations. Evaluation results emphasize the generalizability of CSIPose. Finally, we discuss measures to mitigate this attack.}
}


@article{DBLP:journals/tmc/FanYBL25,
	author = {Wenhao Fan and
                  Yang Yu and
                  Chenhui Bao and
                  Yuan'an Liu},
	title = {Vehicular Edge Intelligence: DRL-Based Resource Orchestration for
                  Task Inference in Vehicle-RSU-Edge Collaborative Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10927--10944},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3572296},
	doi = {10.1109/TMC.2025.3572296},
	timestamp = {Wed, 19 Nov 2025 07:42:09 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanYBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular edge intelligence, distinct from traditional edge intelligence, exhibits unique characteristics, including the mobility of vehicles, uneven spatial and temporal distribution of vehicles, and variability in the AI models deployed on vehicles, Roadside Units (RSUs), and edge servers (ESs). In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource orchestration scheme for task inference in vehicle-RSU-edge collaborative networks. In our approach, vehicles’ inference tasks can be processed on the vehicles, RSUs, or ESs, encompassing a total of 9 possible scenarios based on the cross-RSU mobility of vehicles. The scheme jointly optimizes task processing decision-making, transmission power allocation, computational resource allocation, and transmission rate allocation. The objective is to minimize the total cost, which involves a trade-off between task processing latency, energy consumption and inference error rate across all vehicle tasks. We design a DRL algorithm that decomposes the original optimization problem into sub-problems and efficiently solves them by combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm with multiple numerical methods. We analyzed the complexity and convergence of the algorithm. Specifically, we demonstrated its low complexity and fast, stable convergence, which prove its effectiveness in solving the problem. And we demonstrate the superiority of our scheme by comparing it with 5 benchmark schemes across 6 different scenarios.}
}


@article{DBLP:journals/tmc/YeODZQOCGLC25,
	author = {Shengyuan Ye and
                  Bei Ouyang and
                  Jiangsu Du and
                  Liekang Zeng and
                  Tianyi Qian and
                  Wenzhong Ou and
                  Xiaowen Chu and
                  Deke Guo and
                  Yutong Lu and
                  Xu Chen},
	title = {Resource-Efficient Collaborative Edge Transformer Inference With Hybrid
                  Model Parallelism},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10945--10962},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574695},
	doi = {10.1109/TMC.2025.3574695},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YeODZQOCGLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users’ privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy+, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy+ introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity and memory-aware parallelism planning for fully exploiting the resource potential. To mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments, Galaxy+ devises a tile-based fine-grained overlapping of communication and computation. Furthermore, a fault-tolerant re-scheduling mechanism is developed to address device-level resource dynamics, ensuring stable and low-latency inference. Extensive evaluation based on prototype implementation demonstrates that Galaxy+ remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving a  1.2 × 1.2\\times  to  4.24 × 4.24\\times  end-to-end latency reduction. Besides, Galaxy+ can adapt to device-level resource dynamics, swiftly rescheduling and restoring inference in the presence of unexpected straggler devices.}
}


@article{DBLP:journals/tmc/ZhaoSHLTX25,
	author = {Liang Zhao and
                  Lu Sun and
                  Ammar Hawbani and
                  Zhi Liu and
                  Xiongyan Tang and
                  Lexi Xu},
	title = {Dual Dependency-Aware Collaborative Service Caching and Task Offloading
                  in Vehicular Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10963--10977},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573379},
	doi = {10.1109/TMC.2025.3573379},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoSHLTX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although some studies in recent years have focused on the coexistence of service and task dependencies in the collaborative optimization of service caching and task offloading in Vehicle Edge Computing (VEC), the challenges brought by dual dependencies have not been fully addressed. Therefore, this paper proposes a more comprehensive joint optimization method for service caching and task offloading under dual dependencies. First, this paper proposes a service criticality prediction method based on the Gated Graph Recurrent Network (GGRN) to perceive complex task dependencies and accurately capture the service requirements of critical task types. Based on this, a hierarchical active-passive hybrid caching strategy is designed, which aims to satisfy diverse service demands while reducing the additional overhead caused by remote service requests. Second, a global task priority computation method based on application heterogeneity has been developed to prevent cascading delays in task chains. Finally, this paper formulates a joint optimization problem for service caching and task offloading in a three-layer VEC system, models it as a markov decision process, and applies a proximal policy optimization-driven collaborative optimization algorithm named COHCTO. Simulation results show that COHCTO achieves multi-objective optimization across metrics such as delay, energy consumption, caching hit rate, and application success rate under conditions different from those of other algorithms.}
}


@article{DBLP:journals/tmc/LiDZW25,
	author = {Chunlin Li and
                  Chaoyue Deng and
                  Yong Zhang and
                  Shaohua Wan},
	title = {Federated Meta-Learning Based Computation Offloading Approach With
                  Energy-Delay Tradeoffs in UAV-Assisted {VEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10978--10991},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573278},
	doi = {10.1109/TMC.2025.3573278},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiDZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) provides an applicable solution for computation offloading in Unmanned Aerial Vehicle(UAV)-assisted Vehicular Edge Computing (VEC) by preserving privacy. However, the heterogeneity of clients brings challenges to the generalization of models. Therefore, we propose a federated meta-learning (FML) framework to solve computation offloading for UAV-assisted VEC. In this paper, we are concerned with computation offloading of temporary hotspot regions due to traffic congestion. First, we construct a computation offloading problem with energy-delay tradeoffs and convert the problem to a Markov Decision Process (MDP). Then, we use FML to train personalized models for different vehicles while enhancing the generalization, we propose a Graph neural network-based FL Probabilistic Embedding for Actor-critic RL (GFL-PEARL) algorithm. We model the context as a Directed Acyclic Graph (DAG) and use GNN to reconstruct the inference network of the PEARL algorithm to extract the correlation between contexts fully. We dynamically adjust the task priority during the FML training process to improve the sampling efficiency. Finally, we verify the performance of the algorithm through simulation and physical experiments. Experimental results show that our algorithm can reduce average cost and task overtime rate by 31% and 56% respectively compared with the benchmarks.}
}


@article{DBLP:journals/tmc/HeJCLCGM25,
	author = {Xiaoming He and
                  Yunzhe Jiang and
                  Huajun Cui and
                  Yinqiu Liu and
                  Mingkai Chen and
                  Maher Guizani and
                  Shahid Mumtaz},
	title = {QoE-Driven Proactive Caching With {DRL} in Sustainable Cloud-to-Edge
                  Continuum},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {10992--11004},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3577197},
	doi = {10.1109/TMC.2025.3577197},
	timestamp = {Thu, 29 Jan 2026 17:25:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeJCLCGM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud-assisted edge computing scenarios can intelligently cache and update the content periodically, thereby enhancing users’ overall perception of service, which is called quality of experience (QoE). To maximize QoE in cloud-to-edge continuum, we formulate a multi-objective optimization problem, which optimizes the cache hit ratio while simultaneously minimizing traffic load and time latency. Particularly, we present an innovative algorithm named Hyperdimensional Transformer with Priority Experience Playback-based Agent Deep network (HT-PAD), which provides a complete solution for prediction and decision-making for proactive caching. First, to improve the prediction accuracy of cached content, we use the encoding layer in hyperdimensional (HD) computing to extract the information features. Second, HD-Transformer, as the prediction part of HT-PAD, is proposed to make predictions based on user preferences, historical information, and popular information. HD-Transformer uses deep neural networks to predict user preferences and process time series data by combining hyperdimensional computation with the Transformer. Third, to avoid errors in the prediction content, we employ PER-MADDPG as the decision-making part of HT-PAD, which consists of Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Prioritized Experience Replay (PER). We use MADDPG to improve content decision-making and utilize PER to select appropriate training samples for PER-MADDPG. Finally, our experiments show that our proposed approach achieves strong performance in terms of edge hit ratio, latency, and traffic load, thus improving QoE.}
}


@article{DBLP:journals/tmc/LiXPLLZ25,
	author = {Xiaoxiao Li and
                  Yong Xie and
                  Cong Peng and
                  Entao Luo and
                  Xiong Li and
                  Zhili Zhou},
	title = {EPREAR:An Efficient Attribute-Based Proxy Re-Encryption Scheme With
                  Fast Revocation for Data Sharing in AIoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11005--11018},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573288},
	doi = {10.1109/TMC.2025.3573288},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiXPLLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Artificial Intelligence of Things (AIoT) is driving human society from “information” to “intelligence”, and the information technology industry is undergoing tremendous changes. However, AIoT data faces security threats such as leakage and illegal access when assisted by third parties. Therefore, some scholars use attribute-based proxy re-encryption (ABPRE) for secure sharing of data. However, the existing ABPRE schemes suffer from high computational overhead and inefficient attribution revocation, which seriously hinders practical application. To solve these problems, in this paper, we propose an efficient attribute-based proxy re-encryption scheme with fast attribute revocation (EPREAR). We design a non-interactive zero-knowledge proof protocol based on blockchain to ensure the verifiability of the key during attribute revocation. Furthermore, we devise a boundless encryption and decryption mechanism to enable the system’s encryption and decryption with a fixed computation overhead, regardless of the size of the attribute set. And EPREAR possesses the ability to add infinite attributes without re-initializing the system. Finally, we perform theoretical and experimental analyses that show EPREAR has excellent computational performance. As a consequence, it has better application value in AIoT.}
}


@article{DBLP:journals/tmc/LeiRYWZ25,
	author = {Kaijia Lei and
                  Xuebin Ren and
                  Shusen Yang and
                  Xiaocheng Wang and
                  Fangyuan Zhao},
	title = {FedDSV: Shapley Value-Based Contribution Estimation in Federated Learning
                  With Dynamic Participation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11019--11033},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574784},
	doi = {10.1109/TMC.2025.3574784},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LeiRYWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) succeeds in collaborative and privacy-preserving ML model training among multiple distributed data owners. To maintain a healthy FL ecosystem, it is crucial to estimate the contributions of all participants fairly. Due to provable fairness, Shapley value (SV) is widely used for contribution estimation in FL. However, current studies focus on static scenarios with fixed participants and neglect the dynamic settings with the random joining or leaving of participants in practice. This paper fills the gap by proposing FedDSV, a novel contribution estimation framework for FL with dynamic participation. FedDSV supports flexible weighting mechanisms and is compatible with the SV fairness properties in dynamic scenarios. To reduce the computational complexity, we propose a Monte Carlo variant sampling method (SMC), which can adapt well to dynamic scenarios and approximate the true SVs. To evaluate the effectiveness and efficiency of our proposed approaches, extensive experiments under different settings (e.g., frequency switching, low-quality detection, etc.) are conducted on both i.i.d and non-i.i.d. distributions. Experimental results demonstrate that FedDSV can reflect the real utility contribution of data sources for dynamic FL, and SMC can approximate the exact dynamic SVs with larger similarities in a much shorter time than the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/FanQGF25,
	author = {Tianqi Fan and
                  Sen Qiu and
                  Wei Gong and
                  Yuguang Fang},
	title = {Multi-Source Domain Generalization for CSI-Based Human Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11034--11045},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573457},
	doi = {10.1109/TMC.2025.3573457},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanQGF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain generalization remains a key challenge in human activity recognition based on channel state information (CSI). Different domains correspond to distinct data distributions, deviating from the typical assumption of independent and identically distributed (i.i.d.) data, which leads to significant performance degradation when models are applied to unseen domains. To address this issue, we propose a novel domain generalization model that integrates meta-learning initialization and an adaptive channel grouping attention mechanism. First, a meta-learning strategy is employed to acquire well-initialized parameters from multiple source domain tasks, enabling the model to implicitly enhance its cross-domain generalization ability. Second, an adaptive grouping attention mechanism is designed in the feature extraction stage to effectively capture the sensitivity differences of different subcarriers to human activities. Meanwhile, a random masking training mechanism is introduced to simulate real-world domain variations and improve model robustness. In addition, a domain adversarial training framework based on the gradient reversal layer (GRL) is adopted to mitigate domain-specific feature dependency, further enhancing the model’s generalization capability. We evaluate our proposed method on both a self-collected dataset, which includes human activity data from nine volunteers across six different environments, and a public CSI dataset. The experimental results demonstrate that our method significantly outperforms existing approaches in domain generalization performance, verifying its effectiveness and practical applicability.}
}


@article{DBLP:journals/tmc/WuWLGLJ25,
	author = {Mengru Wu and
                  Haonan Wu and
                  Weidang Lu and
                  Lei Guo and
                  Inkyu Lee and
                  Abbas Jamalipour},
	title = {Security-Aware Designs of Multi-UAV Deployment, Task Offloading and
                  Service Placement in Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11046--11060},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574061},
	doi = {10.1109/TMC.2025.3574061},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuWLGLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution to support wireless devices’ computation-intensive services in the absence of terrestrial infrastructures. Nevertheless, the heterogeneous nature of MEC services and the security vulnerability of wireless channels present significant challenges to achieving efficient and secure computation offloading. In this paper, we investigate a multi-UAV-assisted MEC network in which wireless devices need to process diverse computation tasks. The devices can perform local computing or offload their computation tasks to UAV servers that have pre-cached relevant service programs in the presence of eavesdroppers. To facilitate secure service provisioning, we propose a cooperative jamming-based scheme in which a UAV jammer transmits jamming signals to interfere with eavesdroppers during devices’ computation offloading processes. Taking into account UAV servers’ constrained caching spaces and secure offloading requirements, we minimize the total task completion delay of devices by jointly optimizing multi-UAV deployment, task offloading decisions, service placement, UAV jammer’s transmit power, and devices’ transmit power. To tackle the formulated mixed-integer nonlinear programming problem, we design an optimization-embedding multi-agent twin delayed deep deterministic policy gradient (OE-MATD3) algorithm. Specifically, the MATD3 approach is leveraged to deal with optimization variables concerning UAVs, while a closed-form solution for devices’ transmit power is derived and guides MATD3-based decision-making. Simulation results demonstrate that the proposed scheme outperforms baselines in terms of devices’ task completion delay.}
}


@article{DBLP:journals/tmc/LiZDCZZCG25,
	author = {Yunzhe Li and
                  Hongzi Zhu and
                  Zhuohong Deng and
                  Yunlong Cheng and
                  Zimu Zheng and
                  Liang Zhang and
                  Shan Chang and
                  Minyi Guo},
	title = {A Scene-Aware Model Adaptation Scheme for Cross-Scene Online Inference
                  on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11061--11075},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574766},
	doi = {10.1109/TMC.2025.3574766},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZDCZZCG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmannedaerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).}
}


@article{DBLP:journals/tmc/MuruganandhamPGBRC25,
	author = {Divyadharshini Muruganandham and
                  Suyash Pradhan and
                  Jerry Gu and
                  Torsten Braun and
                  Debashri Roy and
                  Kaushik R. Chowdhury},
	title = {{SMART:} Sim2Real Meta-Learning-Based Training for mmWave Beam Selection
                  in {V2X} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11076--11091},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3576203},
	doi = {10.1109/TMC.2025.3576203},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MuruganandhamPGBRC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twins (DT) offer a low-overhead evaluation platform and the ability to generate rich datasets for training machine learning (ML) models before actual deployment. Specifically, for the scenario of ML-aided millimeter wave (mmWave) links between moving vehicles to roadside units, we show how DT can create an accurate replica of the real world for model training and testing. The contributions of this paper are twofold: First, we propose a framework to create a multimodal Digital Twin (DT), where synthetic images and LiDAR data for the deployment location are generated along with RF propagation measurements obtained via ray-tracing. Second, to ensure effective domain adaptation, we leverage meta-learning, specifically Model-Agnostic Meta-Learning (MAML), with transfer learning (TL) serving as a baseline validation approach. The proposed framework is validated using a comprehensive dataset containing both real and synthetic LiDAR and image data for mmWave V2X beam selection. It also enables the investigation of how each sensor modality impacts domain adaptation, taking into account the unique requirements of mmWave beam selection. Experimental results show that models trained on synthetic data using transfer learning and meta-learning, followed by minimal fine-tuning with real-world data, achieve up to 4.09× and 14.04× improvements in accuracy, respectively. These findings highlight the potential of synthetic data and meta-learning to bridge the domain gap and adapt rapidly to real-world beamforming challenges.}
}


@article{DBLP:journals/tmc/ZhangLYYMZ25,
	author = {Weiting Zhang and
                  Peixi Liao and
                  Dong Yang and
                  Qiang Ye and
                  Shiwen Mao and
                  Hongke Zhang},
	title = {Toward Deterministic Satellite-Terrestrial Integrated Networks via
                  Resource Adaptation and Differentiated Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11092--11109},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574740},
	doi = {10.1109/TMC.2025.3574740},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLYYMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite-terrestrial integrated network (STIN) is a full-scale communication paradigm, which can support joint information processing and seamless service provision by leveraging satellites’ wide coverage and terrestrial networks’ high capacity. The existing STIN operates with insufficient synergy in transmission scheduling, impacting resource allocation efficiency and transmission delay optimization, particularly in complex transmission scenarios. In this paper, we design Deterministic STIN (DetSTIN), a novel architecture for STIN, along with two algorithms tailored for transmission scheduling to collaboratively optimize resource adaptation and service flow scheduling. Specifically, the DetSTIN enables the smooth interconnection and integration of heterogeneous networks by providing layered deterministic services. Besides, a genetic-based resource adaptation algorithm is designed for fixed-mobile-satellite heterogeneous networks to reduce resource allocation overhead while maintaining the network performance. Furthermore, we propose a deep reinforcement learning-based differentiated scheduling algorithm to solve the routing-queue two-dimensional decision problem to differentially optimize transmission delay of service flows, thus obtaining higher transmission scheduling benefit. By addressing resource adaptation and differentiated scheduling synergistically, the proposed solution achieves reduced resource allocation overhead and increased transmission scheduling benefit, ultimately leading to increased network operation revenue of the DetSTIN. Simulation results demonstrate that the proposed solution delivers effective performance across various flow proportions, and as the number of flows increases, the network operation revenue exhibits a noticeable improvement, compared with benchmark algorithms.}
}


@article{DBLP:journals/tmc/YanZFHSZL25,
	author = {Xingrun Yan and
                  Shiyuan Zuo and
                  Rongfei Fan and
                  Han Hu and
                  Li Shen and
                  Puning Zhao and
                  Yong Luo},
	title = {Sequential Federated Learning in Hierarchical Architecture on Non-IID
                  Datasets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11110--11124},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573928},
	doi = {10.1109/TMC.2025.3573928},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanZFHSZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a real federated learning (FL) system, communication overhead for passing model parameters between the clients and the parameter server (PS) is often a bottleneck. Hierarchical federated learning (HFL) that poses multiple edge servers (ESs) between clients and the PS can partially alleviate communication pressure but still needs the aggregation of model parameters from multiple ESs at the PS. To further reduce communication overhead, we remove the central PS, so that each iteration only completes model training by transmitting the global model between two adjacent ES. We call this serial learning method Sequential FL (SFL). For the first time, we introduced SFL into HFL and proposed a novel algorithm adapted to this combined framework, called Fed-CHS. Convergence results are derived for strongly convex and non-convex loss functions under various data heterogeneity setups, which show comparable convergence performance with the algorithms for HFL or SFL solely. Experimental results provide evidence of the superiority of our proposed Fed-CHS on both communication overhead saving and test accuracy over baseline methods.}
}


@article{DBLP:journals/tmc/HuangZMWC25,
	author = {Wenjie Huang and
                  Zhiwei Zhao and
                  Geyong Min and
                  Yang Wang and
                  Zheng Chang},
	title = {{MMTO:} Multi-Vehicle Multi-Hop Task Offloading in MEC-Enabled Vehicular
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11125--11136},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3576154},
	doi = {10.1109/TMC.2025.3576154},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZMWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC)-enabled vehicular networks have emerged as a promising approach to enhancing the performance and efficiency of the Internet-of-Vehicles (IoV) applications. By leveraging some vehicles to act as transmission relays, multi-hop task offloading addresses the problem of intermittent connectivity between vehicles and edge servers to cope with the issues of network congestion or obstacles. However, two critical issues, i.e., uncooperative behaviors of selfish vehicles and network resource dynamics, resulting from multi-vehicle concurrent offloading are not fully considered in the existing work. To fill this gap, this paper proposes a novel and efficient task offloading scheme, namely MMTO, that exploits the multi-hop computational resources to maximize the system-wide profit, and supports incentive compatibility of vehicular users and concurrent offloading. Specifically, an iterative hierarchical estimation algorithm is designed to estimate the offloading delay and energy cost in order to iteratively optimize the offloading decisions. An energy-efficient routing approach is then proposed to schedule the transmission paths for the offloading vehicles. Furthermore, an effective reward-driven auction-based incentive mechanism is designed for incentivizing relayers and calculators to engage in collaboration. Both simulation and field experiments are conducted; extensive results demonstrate that MMTO outperforms the state-of-the-art approaches in terms of the system-wide profit improvement and overall task delay reduction.}
}


@article{DBLP:journals/tmc/GaoYKLZ25,
	author = {Shuangwei Gao and
                  Peng Yang and
                  Yuxin Kong and
                  Feng Lyu and
                  Ning Zhang},
	title = {Characterizing and Scheduling of Diffusion Process for Text-to-Image
                  Generation in Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11137--11150},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3574065},
	doi = {10.1109/TMC.2025.3574065},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoYKLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Artificial Intelligence-Generated Content (AIGC) technology is transforming content creation by enabling diverse customized and quality services. However, the limited computing resources on mobile devices hinder the provisioning of AIGC services at scale, pose challenges in guaranteeing user-satisfied content quality requirement. To address these challenges, we first investigate the characteristics of prompt category and inference models in Text-to-Image (T2I) diffusion process. It is observed that, model size, denoising steps, and computing resource, are three deciding factors to image generation utility. Based on this insight, we first design an edge-assisted AIGC service system to efficiently process multi-user T2I generative requests, employing a multi-flow queuing model to capture multi-user dynamics and characterize the impact of diffusion scheduling on service latency. The system schedules the diffusion process of T2I generation across edge-deployed models, balancing service quality and computing resource. To maximize generation utility under resource constraints, we propose a Monte Carlo Tree Search-based diffusion scheduling algorithm embedded with adaptive computing resource allocation subroutine. This algorithm ensures that, resource allocation dynamically adapts to scheduling decisions in real time, enabling an effective trade-off between service quality and latency. Extensive experimental comparison against baseline approaches demonstrates that, the proposed system can enhance the generation utility by up to 7.3 % \\% , achieving a 2.9 % \\%  improvement in quality score and a 33.3 % \\%  reduction in service latency.}
}


@article{DBLP:journals/tmc/FengYZHFL25,
	author = {Jiyuan Feng and
                  Xu Yang and
                  Dongyi Zheng and
                  Weihong Han and
                  Binxing Fang and
                  Qing Liao},
	title = {Overcoming Catastrophic Forgetting in Federated Continual Graph Learning
                  for Resource-Limited Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11151--11163},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573964},
	doi = {10.1109/TMC.2025.3573964},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengYZHFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Graph Learning (FGL) enables multiple clients to collaboratively learn node representations from private subgraph data, such as user transactions or social networks. Local models are trained on clients and then aggregated by a central server, supporting large-scale graph learning without sharing raw data. However, most existing FGL methods assume that the number of nodes in the graph remains constant, while real-world scenarios often evolve, with new nodes and edges continually added and older ones removed due to limited device memory. We define this setting as Federated Continual Graph Learning (FCGL). In FCGL, global model aggregation may cause interference occur inter-task and inter-client, therefore, FCGL suffers from the global catastrophic forgetting, as the global model adapts to newly added nodes, it loses knowledge acquired from earlier graph data of clients. To address this, we propose GRE-FL, a generative replay framework, which can mitigate global catastrophic forgetting by generating a global summary graph at the server to preserve critical information from historical nodes. It also improves performance by equipping local models with a gating graph attention network for better feature extraction. Experiments show that GRE-FL achieves strong performance across multiple datasets.}
}


@article{DBLP:journals/tmc/ShengLCZLX25,
	author = {Biyun Sheng and
                  Jiabin Li and
                  Hui Cai and
                  Yiping Zuo and
                  Li Lu and
                  Fu Xiao},
	title = {mmZeAR: Zero-Effort Cross-Category Action Recognition With mmWave
                  Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11164--11179},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573168},
	doi = {10.1109/TMC.2025.3573168},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShengLCZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the widespread application of radio frequency (RF) signal-based human action recognition, traditional solutions can only recognize seen categories and the perception scope is restrained by the limited activity classes. When a novel category emerges, the model needs to be optimized again on additionally collected samples at the cost of computation and labor burden. To address this challenge, we develop the mmZeAR system, which learns semantic knowledge from available vision data as class attributes and then transforms the classification into a matching problem. Specifically, we build the attribute space by fusing the coarse-grained video classification features and fine-grained angle change features of 3D joint skeletons. Then we design an efficient feature extraction backbone named TriSqN, which integrates triple radar heatmaps into the final representations by sufficiently exploring the heterogeneous and complementary characteristics. Finally, a projection network is developed between semantic attributes and radar features to construct indirect relationships between samples and labels. By implementing mmZeAR on millimeter wave (mmWave) radar signal datasets, our extensive experiments have demonstrated its remarkable recognition accuracy in novel category recognition with zero effort and achieved state-of-the-art performance.}
}


@article{DBLP:journals/tmc/XuLZLWFN25,
	author = {Shanfeng Xu and
                  Zhipeng Liu and
                  Le Zhao and
                  Ziyi Liu and
                  Xinyi Wang and
                  Zesong Fei and
                  Arumugam Nallanathan},
	title = {Joint Trajectory and Beamforming Optimization for AAV-Relayed Integrated
                  Sensing and Communication With Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11180--11192},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3573702},
	doi = {10.1109/TMC.2025.3573702},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLZLWFN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate joint trajectory and beamforming design for autonomous aerial vehicle (AAV)-relayed integrated sensing and communication (ISAC) systems with mobile edge eomputing (MEC) under the clutter environment. Due to the limited on-board computing capability, the AAV has to offload sensing echoes to the base station (BS) for efficient processing. A novel relay-based ISAC-then-offload frame structure is considered. We aim to maximize the throughput of the BS-AAV-user relaying link while ensuring sensing accuracy and efficient sensing data offloading. The non-convex problem is solved using an alternating optimization algorithm based on successive convex approximation (SCA). Simulation results illustrate that our proposed algorithm achieves near-optimal communication performance while guaranteeing sensing accuracy, addressing the balance between the communication and sensing performance. Furthermore, we evaluate the impact of critical system parameters including sensing constraints, power control factor, and AAV flight duration on communication performance, and explore the trade-offs between energy efficiency and spectral efficiency under varying sensing data intensity and offloading duration.}
}


@article{DBLP:journals/tmc/LiZLSLY25,
	author = {Zhao Li and
                  Lijuan Zhang and
                  Siwei Le and
                  Kang G. Shin and
                  Jia Liu and
                  Zheng Yan},
	title = {Distributed Modulation Exploiting {IRS} for Secure Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11193--11208},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579960},
	doi = {10.1109/TMC.2025.3579960},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLSLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the broadcast nature of wireless communications, users’ data transmitted wirelessly is susceptible to security/privacy threats. The conventional modulation scheme “loads” all of the user’s transmitted information onto a physical signal. Then, as long as an adversary overhears and processes the signal, s/he may access the user’s information, hence breaching communication privacy. To counter this threat, we propose IRS-DMSC, a Distributed Modulation based Secure Communication (DMSC) scheme by exploiting Intelligent Reflecting Surface (IRS). Under IRS-DMSC, two sub-signals are employed to realize legitimate data transmission. Of these two signals, one is directly generated by the legitimate transmitter (Tx), while the other is obtained by modulating the phase of the direct signal and then reflecting it at the IRS in an indirect way. Both the direct and indirect signal components superimpose on each other at the legitimate receiver (Rx) to produce a waveform identical to that obtained under traditional centralized modulation (CM), so that the legitimate Rx can employ the conventional demodulation method to recover the desired data from the received signal. IRS-DMSC incorporates the characteristics of wireless channels into the modulation process, and hence can fully exploit the randomness of wireless channels to enhance transmission secrecy. However, due to the distribution and randomization of legitimate transmission, it becomes difficult or even impossible for an eavesdropper to wiretap the legitimate user’s information. Furthermore, in order to address the problem of decoding error incurred by the difference of two physical channels’ fading, we develop Relative Phase Calibration (RPC) and Constellation Point Calibration (CPC), to improve decoding correctness at the legitimate Rx. Our method design, experiment, and simulation have shown the proposed IRS-DMSC to prevent eavesdroppers from intercepting legitimate information while maintaining good performance of the legitimate transmission.}
}


@article{DBLP:journals/tmc/XuSPWL25,
	author = {Qichao Xu and
                  Zhou Su and
                  Haixia Peng and
                  Yuan Wu and
                  Ruidong Li},
	title = {Blockchain-Empowered Game Theoretical Incentive for Secure Bandwidth
                  Allocation in UAV-Assisted Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11209--11223},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579505},
	doi = {10.1109/TMC.2025.3579505},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuSPWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the promising unmanned aerial vehicle (UAV)-assisted wireless networks (UAWNs) have emerged by advocating the UAVs to provide wireless transmission services. However, owing to the ever-growing volume of data traffic and the untrusted network operation environment, efficiently and securely assigning limited bandwidth for high-quality wireless communication between UAVs and mobile users poses a significant challenge. To address this challenge, we propose a novel secure UAV-bandwidth allocation scheme to provision reliable wireless transmission services for mobile users in UAWNs. Specifically, we first introduce a novel blockchain-empowered framework for secure bandwidth allocation, designed to automate payment processes and deter malicious activities through the immutable logging of transactional and behavioral data. Wherein, a smart contract is designed to regulate the honest behaviors of both mobile users and UAVs during bandwidth allocation with a distributed manner. Besides, a delegated proof-of-stake (DPoS) with reputation consensus protocol is presented to ensure the authenticity and efficiency of the decision-making process. Further, we apply the Stackelberg game theory to model the dynamic of the bandwidth allocation between mobile users and UAVs. In this game, the UAVs act as game leaders to determine the bandwidth price, while each mobile user acts as a game follower, making decision on the bandwidth request. We utilize the backward induction method to derive the optimal strategies of both parties, culminating in the identification of the Stackelberg equilibrium of the formulated game. Finally, extensive simulations are carried out to show the superiority of the proposed scheme over conventional schemes in terms of security, efficiency, and fairness in bandwidth allocation.}
}


@article{DBLP:journals/tmc/RenDZL25,
	author = {Meixuan Ren and
                  Haipeng Dai and
                  Linglin Zhang and
                  Tang Liu},
	title = {Adaptive Charging With Beam Steering},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11224--11240},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579692},
	doi = {10.1109/TMC.2025.3579692},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenDZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the maturation of wireless power transfer technology, Wireless Rechargeable Sensor Networks (WRSNs) have been able to provide a continuous energy supply by scheduling a Mobile Charger (MC). However, traditional charging modes suffer from fixed charging areas that lack the ability to adapt to variable sensor distributions. This inflexibility yields a gap between energy supply and utilization, resulting in relatively low charging efficiency. To address this issue, we propose an adaptive charging mode that utilizes beam steering to dynamically adjust the charging area, thereby catering to different sensor distributions encountered during the charging process. First, we build a dual-symmetric steering charging model to describe the characteristics of dynamic beam steering, enabling precise manipulation of the charging area. Then, we develop a charging power discretization based on steering angle and charging distance to obtain a finite feasible charging strategy set for MC. We reformalize charging utility maximization under energy constraints as a submodular function maximization problem, and propose an approximate algorithm to solve it. Lastly, simulations and field experiments demonstrate that our scheme outperforms other algorithms by 43.9% on average.}
}


@article{DBLP:journals/tmc/PanLSWGWNY25,
	author = {Hongyang Pan and
                  Yanheng Liu and
                  Geng Sun and
                  Qingqing Wu and
                  Tierui Gong and
                  Pengfei Wang and
                  Dusit Niyato and
                  Chau Yuen},
	title = {Cooperative UAV-Mounted RISs-Assisted Energy-Efficient Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11241--11258},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579597},
	doi = {10.1109/TMC.2025.3579597},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PanLSWGWNY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative reconfigurable intelligent surfaces (RISs) are promising technologies for 6G networks to support a great number of users. Compared with the fixed RISs, the properly deployed RISs may improve the communication performance with less communication energy consumption, thereby improving the energy efficiency. In this paper, we consider a cooperative uncrewed aerial vehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs are carried and enhanced by UAVs to serve multiple ground users (GUs) simultaneously such that achieving the three-dimensional (3D) mobility and opportunistic deployment. Specifically, we formulate an energy-efficient communication problem based on multi-objective optimization framework (EEComm-MOF) to jointly consider the beamforming vector of base station (BS), the location deployment and the discrete phase shifts of UAV-RIS system so as to simultaneously maximize the minimum available rate over all GUs, maximize the total available rate of all GUs, and minimize the total energy consumption of the system, while the transmit power constraint of BS is considered. To comprehensively solve EEComm-MOF which is an NP-hard and non-convex problem with constraints, a non-dominated sorting genetic algorithm-II with a continuous solution processing mechanism, a discrete solution processing mechanism, and a complex solution processing mechanism (INSGA-II-CDC) is proposed. Simulations results demonstrate that the proposed INSGA-II-CDC can solve EEComm-MOF effectively and outperforms other benchmarks under different parameter settings. Moreover, the stability of INSGA-II-CDC and the effectiveness of the improved mechanisms are verified. Finally, the implementability analysis of the algorithm is given.}
}


@article{DBLP:journals/tmc/GaoWYD25,
	author = {Zhen Gao and
                  Gang Wang and
                  Lei Yang and
                  Yu Dai},
	title = {Transfer Learning for Joint Trajectory Control and Task Offloading
                  in Large-Scale Partially Observable UAV-Assisted {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11259--11276},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579748},
	doi = {10.1109/TMC.2025.3579748},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoWYD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing joint trajectory control and task offloading (JTCTO) algorithms offer ultra-low latency services for smart devices (SDs) in uncrewed aerial vehicle (UAV)-assisted mobile edge computing (MEC). However, these JTCTO algorithms typically require large training datasets to learn the optimal policies, leading to low learning efficiency. Additionally, most existing JTCTO algorithms are difficult to scale to environments with more than a few UAVs, as their complexity increases exponentially with the number of UAVs. In this paper, we propose a decentralized JTCTO algorithm based on the Policy Transfer and Mean Field-based Multi-Agent Actor-Critic (PTMF-MAAC). First, a novel policy transfer algorithm is proposed to determine which UAV’s JTCTO strategy is helpful for each UAV and when to terminate the strategy to accelerate the learning efficiency of the UAV. Second, we propose a partially observable mean field algorithm that significantly reduces the model space by replacing the influence of all other UAVs on a particular UAV with an average value, thereby adapting to large-scale UAV scenarios. Experiments have shown that compared to the baseline, PTMF-MAAC reduces the system cost by 18.44% ∼ \\sim 28.57% and improves the model learning efficiency and adaptability to partially observable large-scale UAV-assisted MEC.}
}


@article{DBLP:journals/tmc/LiLLLGD25,
	author = {Renjie Li and
                  Yeming Li and
                  Jiamei Lv and
                  Hailong Lin and
                  Yi Gao and
                  Wei Dong},
	title = {Combating {BLE} Weak Links by Combining {PHY} Layer Symbol Extension
                  and Link Layer Coding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {10},
	pages = {11277--11291},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3579934},
	doi = {10.1109/TMC.2025.3579934},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLLLGD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) technology supports various Internet-of-Things (IoT) applications. However, because of their limited transmission power and channel interference, their performance is deficient over weak links. Extending physical layer symbols or using error correction code to the link layer is effective somehow. Introducing excessive BLE bits to both respectively can also decrease the network throughput. To optimize the BLE technology performance, we propose CPL, a combining PHY and link layer optimization technology that adaptively allocates BLE bits to both the physical layer and link layer. Then we propose the Cross-Layer BLE Bits Dynamic Allocation Model that unifies the gain of BLE bits in different layers. Finally, we propose an Interference-Aware Controlled CFO Fine-Tuning Method that calibrates the model according to different interference patterns. We implement CPL on Commercial-Off-The-Shelf (COTS) BLE chips and SDR. The experiment results show that under various interference conditions, CPL achieves 50× and 32.16% throughput improvement over RSBLE and Symphony. CPL reduces energy consumption by 60.42% to 97.95% compared to RSBLE, and 11.04% to 25.15% compared to Symphony.}
}


@article{DBLP:journals/tmc/HouXWDJRN25,
	author = {Xiangwang Hou and
                  Tianyu Xing and
                  Jingjing Wang and
                  Jun Du and
                  Chunxiao Jiang and
                  Yong Ren and
                  Dusit Niyato},
	title = {Age of Information-Aware Multi-Objective Optimization for Heterogeneous
                  {UAV-USV-UUV} Networks in Underwater Target Hunting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11292--11304},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581836},
	doi = {10.1109/TMC.2025.3581836},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HouXWDJRN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater target hunting (UTH) is a critical and complex mission involving the search, tracking, and hunting of targets in an underwater environment. However, the unpredictable trajectories and flexibility of these targets, along with complex underwater environments, significantly impede the efficiency and success of traditional methods that depend solely on unmanned underwater vehicles (UUVs). Consequently, this paper presents the “3U network”, a novel heterogeneous framework integrating unmanned aerial vehicles (UAVs), unmanned surface vehicles (USVs), and UUVs for UTH. Within this framework, a UAV identifies the target, a USV acts as a communication relay, and a swarm of UUVs hunts the target. Moreover, to improve the timeliness of target detection, we incorporate the age of information (AoI) concept into the UAV’s search strategy. Additionally, we develop a constrained multi-objective optimization problem to minimize energy consumption and mission duration by optimizing vehicles’ trajectories, considering mobility limitations, safety, and connectivity constraints. Furthermore, to tackle this problem, we design an AoI- and energy-aware multi-vehicle twin-delayed deep deterministic policy gradient algorithm (AE-MVTD3) to optimize control policies for heterogeneous vehicles. The experimental results show that the proposed method performs effectively across diverse complex scenarios.}
}


@article{DBLP:journals/tmc/VillaKKHSMBKDBJMPK25,
	author = {Davide Villa and
                  Imran Khan and
                  Florian Kaltenberger and
                  Nicholas Hedberg and
                  R{\'{u}}ben Soares da Silva and
                  Stefano Maxenti and
                  Leonardo Bonati and
                  Anupa Kelkar and
                  Chris Dick and
                  Eduardo Baena and
                  Josep Miquel Jornet and
                  Tommaso Melodia and
                  Michele Polese and
                  Dimitrios Koutsonikolas},
	title = {{X5G:} An Open, Programmable, Multi-Vendor, End-to-End, Private 5G
                  {O-RAN} Testbed With {NVIDIA} {ARC} and OpenAirInterface},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11305--11322},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3580764},
	doi = {10.1109/TMC.2025.3580764},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/VillaKKHSMBKDBJMPK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Fifth generation (5G) cellular systems transition to softwarized, programmable, and intelligent networks, it becomes fundamental to enable public and private 5G deployments that are (i) primarily based on software components while (ii) maintaining or exceeding the performance of traditional monolithic systems and (iii) enabling programmability through bespoke configurations and optimized deployments. This requires hardware acceleration to scale the Physical (PHY) layer performance, programmable elements in the Radio Access Network (RAN) and intelligent controllers at the edge, careful planning of the Radio Frequency (RF) environment, as well as end-to-end integration and testing. In this paper, we describe how we developed the programmable X5G testbed, addressing these challenges through the deployment of the first 8-node network based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air (ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent Controller (RIC). The Aerial Software Development Kit (SDK) provides the PHY layer, accelerated on Graphics Processing Unit (GPU), with the higher layers from the OAI open-source project interfaced with the PHY through the Small Cell Forum (SCF) Functional Application Platform Interface (FAPI). An E2 agent provides connectivity to the O-RAN Software Community (OSC) near-real-time RIC. We discuss software integration, network infrastructure, and a digital twin framework for RF planning. We then profile the performance with up to 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with iPerf and video streaming applications, as well as up to 25 emulated User Equipments (UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in uplink.}
}


@article{DBLP:journals/tmc/GuoLJGSC25,
	author = {Xiuzhen Guo and
                  Boya Liu and
                  Nan Jing and
                  Chaojie Gu and
                  Yuanchao Shu and
                  Jiming Chen},
	title = {Enabling Cross-Band Backscatter Communication With Twaltz},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11323--11336},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581900},
	doi = {10.1109/TMC.2025.3581900},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoLJGSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequency switching is a fundamental capability for wireless communication systems. However, this capability is significantly constrained in backscatter systems. The difficulty is to generate tunable high-frequency modulation signals on a backscatter tag at an acceptable power budget. In this paper, we present Twaltz, a new design paradigm for backscatter communication that enables frequency switching across large frequency bands. By exploiting a low-power semiconductor device, i.e., tunnel diode, and carefully addressing its physical features, Twaltz generates oscillation signals up to 1.2 GHz while maintaining micro-watt level power consumption. Twaltz further facilitates on-tag oscillation signal stabilization and programmable oscillation frequency tuning. We prototype Twaltz on a PCB board, demonstrating its efficiency in cross-band communication for LoRa backscatter, and verifying its performance in concurrent transmission, channel hopping, and data transmission.}
}


@article{DBLP:journals/tmc/ChenXFWCL25,
	author = {Jinyu Chen and
                  Wenchao Xu and
                  Yunfeng Fan and
                  Haozhao Wang and
                  Quan Chen and
                  Jing Li},
	title = {Fast Multimodal Edge Inference via Selective Feature Distillation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11337--11350},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3580102},
	doi = {10.1109/TMC.2025.3580102},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXFWCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inferring user status at the edge is essential for delivering personalized services, such as detecting emotional states. However, deploying large-scale models directly on user devices is impractical due to substantial computational overhead and the scarcity of labeled data. Conversely, uploading raw data to the cloud for processing raises significant privacy concerns and incurs prohibitive communication costs. To address this challenge, we propose a privacy-preserving multimodal inference framework that leverages large-scale public data while safeguarding sensitive information and optimizing computational efficiency. Specifically, we first train a teacher model in the cloud using publicly available data. Through a feature distillation process, the knowledge from this teacher model is transferred to a lightweight encoder deployed at the user end. This transfer is tailored to the user’s data, ensuring that only relevant knowledge is distilled. To accommodate varying communication constraints, we introduce a feature compression mechanism that significantly reduces communication overhead without compromising inference accuracy. Extensive experiments on emotion recognition tasks demonstrate that the proposed framework effectively balances privacy preservation, resource efficiency, and inference accuracy, facilitating seamless collaboration between cloud and edge devices.}
}


@article{DBLP:journals/tmc/HuangSLXW25,
	author = {Jianping Huang and
                  Feng Shan and
                  Junzhou Luo and
                  Runqun Xiong and
                  Wenjia Wu},
	title = {{ASSUME:} An Optimal Algorithm to Minimize {UAV} Energy by Altitude
                  and Speed Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11351--11368},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581929},
	doi = {10.1109/TMC.2025.3581929},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangSLXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed aerial vehicles (UAVs) are being widely employed in wireless communication applications, e.g., collecting data from ground nodes (GNs). Minimizing UAV energy in these applications is crucial due to the limited energy supply onboard. Unlike previous studies that assume UAVs fly at a fixed altitude and simplify the energy consumption model of UAVs, we consider the impact of varying UAV altitudes on the ground-to-air communication and utilize a general communication model for GN. Furthermore, we conduct real-world flight tests and introduce a practical speed-related flight energy consumption model of UAVs. This paper focuses on the UAV altitude-speed scheduling and GN transmission switching (UASS-GTS) problem, specifically in scenarios where the UAV flies straight for monitoring applications such as power transmission lines, roads, and water/oil/gas pipes. However, minimizing energy consumption presents challenges due to the tight coupling of altitude scheduling and speed scheduling. To tackle this, first, we develop the looking before crossing algorithm for speed scheduling. We then extend this algorithm by integrating altitude scheduling to propose the Altitude-Speed Scheduling of UAV for Minimizing Energy (ASSUME) algorithm, using a dynamic programming method. The ASSUME algorithm is theoretically proven to be optimal. Additionally, based on ASSUME, we propose an offline-inspired online heuristic algorithm to handle agnostic situations where GN information is not available unless flies close. Simulations indicate that the ASSUME algorithm saves an average of 26.1%–62.7% energy compared to the baseline methods, and the performance gap between the online algorithm and the offline optimal algorithm ASSUME is 22.8%.}
}


@article{DBLP:journals/tmc/GaoCLW25,
	author = {Demin Gao and
                  Yongrui Chen and
                  Ye Liu and
                  Honggang Wang},
	title = {Seamless Physical-Layer Cross-Technology Communication from ZigBee
                  to LoRa via Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11369--11385},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3580396},
	doi = {10.1109/TMC.2025.3580396},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoCLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa, designed for Low-Power, Wide-Area Networks (LPWANs), is widely used in the Internet of Things (IoT). In contrast, Wireless Personal Area Network (WPAN) technologies like ZigBee struggle to connect directly to LPWANs due to their limited communication range and differing modulation schemes. ZigBee uses Offset Quadrature Phase-Shift Keying (OQPSK) modulation, while LoRa employs Chirp Spread Spectrum (CSS) modulation, complicating cross-technology communication. To address this challenge, we propose a novel approach for seamless physical-layer cross-technology communication between ZigBee and LoRa networks, bridging the gap between short-range and long-range communication technologies. We introduce ZigRa, a communication method that leverages neural networks for efficient modulation translation between ZigBee’s IEEE 802.15.4 standard and LoRa’s CSS modulation. The core of ZigRa is a deep learning model that adapts and optimizes the transformation of ZigBee signals into ultra-narrowband single-tone sinusoidal signals, which can be reliably detected by LoRaWAN base stations. Our solution enables ZigBee devices to seamlessly connect to LoRa-based LPWANs, overcoming modulation mismatches and providing long-range connectivity. Extensive evaluations with both USRP hardware and commercial devices demonstrate that ZigRa achieves a frame reception rate exceeding 85% at distances up to 500 meters, significantly enhancing the interoperability and coverage of heterogeneous IoT networks.}
}


@article{DBLP:journals/tmc/FangYLHZZXYWLZ25,
	author = {Zhihan Fang and
                  Guang Yang and
                  Wenjun Lyu and
                  Zhiqing Hong and
                  Shuxin Zhong and
                  Weijian Zuo and
                  Yuelei Xie and
                  Yu Yang and
                  Guang Wang and
                  Yunhuai Liu and
                  Desheng Zhang},
	title = {Cellular Infrastructure Sharing for Network Robustness: {A} Citywide
                  Empirical Study},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11386--11400},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3580605},
	doi = {10.1109/TMC.2025.3580605},
	timestamp = {Mon, 10 Nov 2025 08:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FangYLHZZXYWLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Individual cellular networks have been very robust to random cell tower failures due to redundant cell tower deployments. However, a large-scale clustered failure with multiple cell towers can lead to the loss of services of a cellular network. Recently, off-the-shelf smartphones can support multiple network standards, so cellular network infrastructure sharing is a promising direction to improve the service robustness under potential large-scale clustered tower failures. The existing work on cellular network robustness is usually limited to large-scale studies of individual networks or small-scale studies of multiple networks. In this work, we conduct the first investigation, to our knowledge, on cross-network infrastructure sharing benefits for enhancing robustness with a full cellular penetration rate. Our work is based on all cellular networks in Shenzhen, China, covering over 10 million cellular users. Specifically, we design a new metric to quantify cellular network robustness with or without cross-network sharing under both random and clustered cell tower failures. We further study the impact of different factors on robustness, including the number of networks, spatiotemporal dynamics, contextual factors, and a case study at two key transportation hubs. We provide a set of lessons learned based on our study, along with discussions of the results.}
}


@article{DBLP:journals/tmc/ZhaoAPY25,
	author = {Xiaopeng Zhao and
                  Zhenlin An and
                  Qingrui Pan and
                  Lei Yang},
	title = {Frequency-Aware Neural Radio-Frequency Radiance Fields},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11401--11415},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583580},
	doi = {10.1109/TMC.2025.3583580},
	timestamp = {Tue, 14 Oct 2025 19:49:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoAPY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although Maxwell discovered the physical laws of electromagnetic waves about 160 years ago, accurately modeling the propagation of RF signals in large and complex electrical environments remains a persistent challenge. This complexity arises from the interactions between the RF signal and various obstacles, including reflection and diffraction. Inspired by the success of neural networks in mapping the optical field in computer vision, we introduce the neural radio-frequency radiance field, or NeRF 2 ^{2} 2. This represents a continuous volumetric scene function that effectively models RF signal propagation. Remarkably, after only a sparse amount of training with signal measurements, NeRF 2 ^{2} 2 can accurately predict the nature and origin of signals received at any location, assuming the transmitter’s position is known. Additionally, we propose the frequency-aware NeRF 2 ^{2} 2 to enhance channel prediction performance for wideband signals using an RF prism module. Compared to the vanilla NeRF 2 ^{2} 2, the frequency-aware NeRF 2 ^{2} 2 achieves a 4 dB improvement in SNR for FDD OFDM channel estimation and is nearly 3.5 × faster. Functioning as a physical-layer neural network, NeRF 2 ^{2} 2 also supports application-layer artificial neural networks (ANNs) by generating synthetic training datasets. Our empirical results demonstrate that augmented sensing enhances the accuracy of AoA estimation, achieving an approximate 50% improvement.}
}


@article{DBLP:journals/tmc/LiangSNI25,
	author = {Chengsi Liang and
                  Yao Sun and
                  Dusit Niyato and
                  Muhammad Ali Imran},
	title = {Knowledge Graph Fusion Based Semantic Communication Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11416--11429},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583605},
	doi = {10.1109/TMC.2025.3583605},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiangSNI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication (SemCom), a paradigm that emphasizes conveying the meaning of information, faces challenges in precise reasoning in semantic coding models. Knowledge graphs (KGs) offer a potential solution by providing structured triples (entities and relations), enabling inference via entity attributes and relational logic. Several key challenges exist in leveraging KGs within SemCom. The first challenge lies in developing methods to create semantic representations aligning and integrating source data and KG information. Second, reconstructing the original data using KGs becomes challenging particularly under poor communication conditions. Moreover, integrating KGs with source data inevitably increases the transmission overhead. In this paper, we propose a novel SemCom framework named KG-SemCom with sophisticated KG-based semantic encoding and decoding designs to solve these challenges. This framework aligns KG entities with message tokens, and then encodes messages into a semantic fusion of contextual and knowledge-based information. Furthermore, KG-SemCom can utilize the KG and contextual relationships to assist in predicting incomplete or distorted messages during the decoding process. Finally, simulation results demonstrate that KG-SemCom achieves higher accuracy and greater robustness compared to existing benchmarks without incorporating KGs, especially in challenging communication environments.}
}


@article{DBLP:journals/tmc/SunYSWDNHK25,
	author = {Geng Sun and
                  Minghua Yuan and
                  Zemin Sun and
                  Jiacheng Wang and
                  Hongyang Du and
                  Dusit Niyato and
                  Zhu Han and
                  Dong In Kim},
	title = {Online Collaborative Resource Allocation and Task Offloading for Multi-Access
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11430--11448},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3580365},
	doi = {10.1109/TMC.2025.3580365},
	timestamp = {Thu, 23 Oct 2025 12:57:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunYSWDNHK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC) is emerging as a promising paradigm to provide flexible computing services close to user devices (UDs). However, meeting the computation-hungry and delay-sensitive demands of UDs faces several challenges, including the resource constraints of MEC servers, inherent dynamic and complex features in the MEC system, and difficulty in dealing with the time-coupled and decision-coupled optimization. In this work, we first present an edge-cloud collaborative MEC architecture, where the MEC servers and cloud collaboratively provide offloading services for UDs. Moreover, we formulate an energy-efficient and delay-aware optimization problem (EEDAOP) to minimize the energy consumption of UDs under the constraints of task deadlines and long-term queuing delays. Since the problem is proved to be non-convex mixed integer nonlinear programming (MINLP), we propose an online joint communication resource allocation and task offloading approach (OJCTA). Specifically, we transform EEDAOP into a real-time optimization problem by employing the Lyapunov optimization framework. Then, to solve the real-time optimization problem, we propose a communication resource allocation and task offloading optimization method by employing the Tammer decomposition mechanism, convex optimization method, bilateral matching mechanism, and dependent rounding method. Simulation results demonstrate that the proposed OJCTA can achieve superior system performance compared to the benchmark approaches.}
}


@article{DBLP:journals/tmc/XieCWM25,
	author = {Mingyue Xie and
                  Zheng Chang and
                  Li Wang and
                  Geyong Min},
	title = {Blockchain-Assisted Lightweight Cross-Domain Authentication for Multi-UAV
                  Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11449--11464},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582833},
	doi = {10.1109/TMC.2025.3582833},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieCWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution of future network and control technologies has enabled unmanned aerial vehicles (UAVs) to collaborate across diverse geographical areas and task domains, enhancing task execution efficiency through data and resource sharing. In response to the increasing demand for cross-domain task allocation and operations for UAVs, establishing robust authentication mechanisms within trusted domains has become a critical foundation for ensuring secure cross-domain access. Despite significant progress in UAV identity authentication and cross-domain access, challenges persist, such as cumbersome and inefficient processes, UAV resource limitations, and establishing trust relationships across different domains. To address these challenges, this paper introduces a dual blockchain-assisted trusted authentication scheme for UAVs’ cross-domain access. Our approach utilizes a certificateless signcryption algorithm for lightweight UAV authentication, thereby eliminating the need for certificate management. Then, an efficient credit-based trust model is designed to measure the trustworthiness of data-in-transit and cross-domain entities. Furthermore, blockchain technology is introduced to store the relevant information of UAVs and credibility to assist cross-domain authentication. Theoretical security analysis and extensive simulations have been conducted, demonstrating the effectiveness and efficiency of our proposed scheme.}
}


@article{DBLP:journals/tmc/JiangGCMLW25,
	author = {Yang Jiang and
                  Shengnan Guo and
                  Hanyang Chen and
                  Xiaowei Mao and
                  Youfang Lin and
                  Huaiyu Wan},
	title = {Proactive-XLight: Proactive Traffic Signal Control With Pluggable
                  and Reliable Traffic Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11465--11479},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581938},
	doi = {10.1109/TMC.2025.3581938},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangGCMLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic signal control (TSC) plays a crucial role in the intelligent transportation system. Among existing TSC approaches, Proactive TSC (PTSC) predicts future traffic states at intersections and proactively adjusts control policies. It is evident that PTSC methods are highly effective in alleviating both current and future traffic congestion at intersections. However, existing PTSC methods focus on point estimation prediction while neglecting prediction reliability. Additionally, they fail to adaptively coordinate between current and future traffic states for optimal control. To address these limitations, we propose an innovative Proactive-Plugin that can be combined with existing TSC methods to enhance the accuracy and robustness of traffic signal control policies. This plugin enhances two critical aspects: 1) Prediction reliability is achieved through Fine-grained Traffic Uncertainty Quantification. This module generates probabilistic forecasts along with confidence intervals to explicitly indicate the credibility of the predictions. 2) Coordination adaptiveness is enabled by a Current-Future Tradeoff Integration mechanism. This mechanism dynamically adjusts the relative influence of current traffic states and probabilistic forecasts on control policies. To further ensure robustness, we design a multi-task joint optimization to reduce the negative impact of inaccurate predictions during training. Experimental results on six real-world datasets demonstrate consistent improvements in traffic efficiency, validating the effectiveness of our approach.}
}


@article{DBLP:journals/tmc/LingSWHW25,
	author = {Tao Ling and
                  Siping Shi and
                  Hao Wang and
                  Chuang Hu and
                  Dan Wang},
	title = {An Efficient On-Device Federated Learning System Through the Interplay
                  of Client Selection and Batch Size With Watermarked Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11480--11493},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3585033},
	doi = {10.1109/TMC.2025.3585033},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LingSWHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables edge devices to collaboratively train a global model using local data. However, the increasing prevalence of watermarks in datasets presents a new challenge to efficient FL. While watermarks assert data ownership and copyright, they introduce complexities that can lead to shortcut learning problems and mislead utility measurements for client selection. These issues are further exacerbated by batch size variations in efficient FL frameworks, ultimately undermining their time-to-accuracy performance. We introduce LotusFL, an FL system designed to address the challenges posed by watermarked datasets in efficient FL. Specifically, it tackles the increased time-to-accuracy due to erroneous client selection and the accuracy degradation observed with larger batch sizes. LotusFL first estimates the characteristics of watermarks through statistical estimation and then adjusts the batch size using this estimated watermark information to balance the negative impact of the watermark against device idle waiting time. Additionally, its client selection mechanism, based on historical information, avoids the misleading utility signals from watermarks. This mechanism, working in conjunction with batch size adjustment, aims to accurately predict device runtime and identify potentially valuable devices. We evaluated LotusFL through a real-world deployment on 40 edge devices. Compared to state-of-the-art efficient FL frameworks, LotusFL achieves superior performance, enhancing accuracy by up to 8.2% and reducing training time by 1.97×.}
}


@article{DBLP:journals/tmc/ZhangZDNWSSK25,
	author = {Ruichen Zhang and
                  Changyuan Zhao and
                  Hongyang Du and
                  Dusit Niyato and
                  Jiacheng Wang and
                  Suttinee Sawadsitang and
                  Xuemin Shen and
                  Dong In Kim},
	title = {Embodied AI-Enhanced Vehicular Networks: An Integrated Vision Language
                  Models and Reinforcement Learning Method},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11494--11510},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582864},
	doi = {10.1109/TMC.2025.3582864},
	timestamp = {Tue, 13 Jan 2026 20:25:57 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZDNWSSK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating vision language models (VLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model’s focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36% compared to DDPG and accelerates convergence by reducing required steps by up to 47% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4% improvement in QoE when scaling from 4 to 8 vehicles.}
}


@article{DBLP:journals/tmc/DimceZBCD25,
	author = {Sigrid Dimce and
                  Anatolij Zubow and
                  Alireza Bayesteh and
                  Giuseppe Caire and
                  Falko Dressler},
	title = {Reconsidering Sparse Sensing Techniques for Channel Sounding Using
                  Splicing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11511--11526},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581446},
	doi = {10.1109/TMC.2025.3581446},
	timestamp = {Tue, 14 Oct 2025 19:49:03 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DimceZBCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-band splicing offers a promising solution to extend existing band-limited communication systems to support high-precision sensing applications. This technique involves performing narrow-band measurements at multiple center frequencies, which are then combined to effectively increase the bandwidth without changing the sampling rate. In this paper, we introduce a mmWave channel sounder based on multi-band splicing, leveraging the sparse nature of wireless channels through compressed sensing and sparse recovery techniques for channel reconstruction. We focus on three sparse recovery methods: the widely used grid-based orthogonal matching pursuit (OMP) algorithm as a baseline, our newly developed two-stage mmSplicer algorithm, which extends the OMP method by introducing an additional stage for improving its performance for our application, and our adaptation of sparse reconstruction by separable approximation (SpaRSA), named Net-SpaRSA, optimized for wireless applications. All three algorithms are integrated into an experimental OFDM-based IEEE 802.11ac system. Our analysis centers on evaluating the performance of these algorithms under limited number of narrow-band measurements, demonstrating that accurate CIR estimation is achievable even using only 50% of the full wideband spectrum. Additionally, we analyze and compare the computational complexity of these algorithms to assess their practical feasibility.}
}


@article{DBLP:journals/tmc/LiZSLLLY25,
	author = {Zhao Li and
                  Lijuan Zhang and
                  Kang G. Shin and
                  Jia Liu and
                  Yicheng Liu and
                  Pintian Lyu and
                  Zheng Yan},
	title = {Parasitic Communication: Opportunistic Utilization of Interference
                  Using Asymmetric Demodulation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11527--11540},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581798},
	doi = {10.1109/TMC.2025.3581798},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZSLLLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of wireless communication technologies, interference has become a key impediment to the improvement of wireless data transmission performance. Traditional interference management (IM) suppresses or adjusts interference at the cost of additional communication resources without exploiting interference effectively. Moreover, wirelessly transmitted data is susceptible to eavesdropping. To address these issues cooperatively, we propose Opportunistic Parasitic Communication with Asymmetric Demodulation(OPC-AD). In particular, we consider the interference experienced by the intended/target communication (i.e., parasitic) receiver (Rx) as the host signal. The target communication constructs a selection signal carrying parasitic indication information based on the data it intends to send and the data decoded by its Rx using asymmetric demodulation from the host signal, and then sends it to its Rx. This signal is used to instruct the parasitic Rx to extract the desired information from the host signal. OPC-AD allows for the exploitation of the interference (i.e., host signal) for data transmission to an interfered Rx. Using AD can also ensure the privacy of the host communication. Since the parasitic communication is concealed within the host signal, eavesdroppers cannot compromise the confidentiality of the parasitic transmission without precisely decoding the selection signal. Furthermore, considering more practical situations, we extend the OPC-AD design to cover a broader range of realistic scenarios. Our experimental results validate the applicability of OPC-AD, while our in-depth simulations demonstrate that parasitic communication can effectively thwart eavesdropping and achieve higher spectral efficiency (SE) than other existing IM methods, particularly in strong interference environments.}
}


@article{DBLP:journals/tmc/WuSZWZHG25,
	author = {Jiasheng Wu and
                  Shaojie Su and
                  Wenjun Zhu and
                  Xiong Wang and
                  Jingjing Zhang and
                  Xingqiu He and
                  Yue Gao},
	title = {PHandover: Parallel Handover in Mobile Satellite Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11541--11554},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582245},
	doi = {10.1109/TMC.2025.3582245},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuSZWZHG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The construction of Low Earth Orbit satellite constellations has recently spurred tremendous attention from both academia and industry. 5G and 6G standards have specified the LEO satellite network as a key component of the mobile network. However, due to the satellites’ fast traveling speed, ground terminals usually experience frequent and high-latency handover, which significantly deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a parallel handover mechanism for the mobile satellite network which can considerably reduce the handover latency. The main idea is to use plan-based handovers instead of measurement-based handovers to avoid interactions between the access and core networks, hence eliminating the significant time overhead in the traditional handover procedure. Specifically, we introduce a novel network function named Satellite Synchronized Function (SSF), which is designed for being compliant with the standard 5G core network. Moreover, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments and results demonstrate that our proposed handover scheme can considerably reduce the handover latency by 21× compared to the standard NTN handover scheme and two other existing handover schemes, along with significant improvements in network stability and user-level performance.}
}


@article{DBLP:journals/tmc/FengSCLZLX25,
	author = {Xingyu Feng and
                  Zehua Sun and
                  Zhuangzhuang Chen and
                  Chengwen Luo and
                  Zhangbing Zhou and
                  Victor C. M. Leung and
                  Weitao Xu},
	title = {LLM-CoSen: Revisiting Collaborative Sensing With Large Language Models
                  (LLMs)},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11555--11567},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583345},
	doi = {10.1109/TMC.2025.3583345},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengSCLZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative sensing has emerged as a novel sensing paradigm, entailing multi-sensor data sharing and multimodal modeling to collaboratively understand sensing behaviors. However, current solutions, i.e., data-level and decision-level fusion methods, fall short of generality, expert knowledge, and holistic/chronic perspective. In this paper, we propose LLM-CoSen to revisit collaborative sensing with Large Language Models (LLMs). Specifically, LLM-CoSen designs a semantic-level fusion approach for inference results for collaborative sensing. Such an approach is characterized by its generality, making it applicable to any heterogeneous devices, and its expert knowledge incorporation, which provides chronic, holistic, and insightful perspectives on the inference results. Regarding inference absence challenges, we propose a personalized model design method to constrain inference time, and a voting-based two-pass prompt engineering strategy for token completion. Regarding inference error challenges, we propose an accuracy restoration strategy for personalized models, and a two-level error estimator coupled with self-correction. Experimental results of human digital system use case on four corresponding benchmark datasets show LLM-CoSen can decrease inference absence by 72.83% and inference errors by 7.65% on average.}
}


@article{DBLP:journals/tmc/TianZXLPS25,
	author = {Bo Tian and
                  Bowen Zhao and
                  Yang Xiao and
                  Yang Liu and
                  Qingqi Pei and
                  Yulong Shen},
	title = {{RAPOO:} An Efficient Privacy-Preserving Facial Expression Recognition
                  via Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11568--11581},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581687},
	doi = {10.1109/TMC.2025.3581687},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TianZXLPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facial expression recognition is a technology that involves analyzing and interpreting human facial expressions to determine individual expressions or states. Mobile crowdsensing (MCS), a promising sensing paradigm, makes it easy to capture facial images and benefits facial expression recognition. Existing inference models for facial expression recognition usually rely on facial feature vectors or facial images, increasing privacy concerns about expression. For this reason, this paper proposes a privacy-preserving facial expression recognition scheme through MCS, named RAPOO, which falls in a client-server architecture. Roughly speaking, a user captures facial images using mobile devices and requests a recognition service provided by a cloud computing center. To protect the privacy of expressions, our approach focuses on designing secure computation protocols required by facial expression recognition necessarily, such as secure vector distance calculation and secure top- k k  query. These protocols enable facial expression recognition over encrypted data directly. To speed up the recognition and store encrypted feature vectors, a  k k -D tree data structure is introduced. The security analysis confirms that RAPOO effectively preserves the confidentiality of personal expressions. Extensive experimental evaluations show that our solution obtains a three-order-of-magnitude speedup in terms of computational overhead compared with the state-of-the-art.}
}


@article{DBLP:journals/tmc/ZhaoLGH25,
	author = {Yifeng Zhao and
                  Chenyi Liang and
                  Zhibin Gao and
                  Lianfen Huang},
	title = {Robust and Asynchronous Multi-Node Cooperative Vehicular Fog Computing
                  Enhanced IoV},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11582--11595},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581397},
	doi = {10.1109/TMC.2025.3581397},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLGH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Vehicular Fog Computing (VFC) provides low-latency computing service to support emerging intelligent transportation applications in Internet of Vehicles (IoV). Multi-node cooperative VFC can utilize Connected and Autonomous Vehicles (CAVs) to implement cooperative intelligence. Due to the mobility of vehicles, service migration is necessary when task offloading service providers change. This paper proposes an Asynchronous Task Offloading Scheme (ATO-S) that allows each CAV to choose an independent optimization period and provides robust task offloading services under unknown vehicle mobility probability distribution. To the best of our knowledge, this is the first work to investigate asynchronous and robust multi-node cooperative task offloading in dynamic VFC-enhanced IoV scenarios. Furthermore, we formulate the long-term energy consumption minimization problem of VFC and transfer it into each time slot problem by Lyapunov optimization. Then we design Asynchronous Task Offloading Algorithm (ATO-A) to jointly optimizing CAVs matching, communication and computation resource allocation, and transmission power based on multiple mathematical techniques and hybrid heuristic algorithm. Extensive simulations based on real-world traffic scenario are conducted by varying multiple crucial parameters. Simulation results demonstrate the energy efficiency and task queue stability achieved by ATO-A, and service robustness achieved by ATO-S, in comparison with benchmark solutions.}
}


@article{DBLP:journals/tmc/TianWLHZZSL25,
	author = {Yu Tian and
                  Lei Wang and
                  Chi Lin and
                  Bin Han and
                  Lupeng Zhang and
                  Zhiyi Zhou and
                  Yu Sun and
                  Bingxian Lu},
	title = {Zero-Knowledge Neighbor Discovery for Underwater Optical Wireless
                  Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11596--11613},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581371},
	doi = {10.1109/TMC.2025.3581371},
	timestamp = {Thu, 01 Jan 2026 19:11:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TianWLHZZSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neighbor discovery poses significant challenges in Underwater Optical Wireless Sensor Networks (UOWSNs) due to the unique characteristics of directional transceivers, line-of-sight communication, and mobility induced by water currents. Traditional methods typically rely on prerequisites and prior knowledge, such as centralized coordination, time synchronization, and neighbor-related information, which are often unavailable or impractical in underwater environments. In this paper, we make the first attempt to address the issue of Robust and Efficient Neighbor Discovery (termed the REND problem) in UOWSNs with zero-knowledge. Here, zero-knowledge refers to the capability that enables sensors to identify neighbors in dynamic underwater optical channel conditions without prerequisites or prior knowledge. We design a zero-knowledge distributed directional neighbor discovery scheme inspired by gear meshing. We then propose a deterministic algorithm for the REND problem based on theoretical analysis. Additionally, to further reduce the discovery delay for the periodic REND problem, we develop a greedy-based approximation algorithm with a performance guarantee. Finally, extensive simulations demonstrate that the proposed scheme reduces the discovery delay by 34.9% on average and achieves an additional 54.4% reduction for periodic neighbor discovery. Furthermore, test-bed experiments are carried out to verify the applicability of our zero-knowledge scheme in real-world scenarios.}
}


@article{DBLP:journals/tmc/ZhuHLWH25,
	author = {Zhiyu Zhu and
                  Jinhui Hou and
                  Jiading Li and
                  Jinjian Wu and
                  Junhui Hou},
	title = {Modeling State Shifting via Local-Global Distillation for Event-Frame
                  Gaze Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11614--11627},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581317},
	doi = {10.1109/TMC.2025.3581317},
	timestamp = {Fri, 10 Oct 2025 10:59:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHLWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper tackles the problem of passive gaze estimation using both event and frame (or 2D image) data. Considering the inherently different physiological structures, it is intractable to accurately estimate gaze purely based on a given state. Thus, we reformulate gaze estimation as the quantification of the state shifting from the current state to several prior registered anchor states. Specifically, we propose a two-stage learning-based gaze estimation framework that divides the whole gaze estimation process into a coarse-to-fine approach involving anchor state selection and final gaze location. Moreover, to improve the generalization ability, instead of learning a large gaze estimation network directly, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion techniques to iteratively remove inherent noise in event data. Extensive experiments demonstrate the effectiveness of the proposed method, which surpasses state-of-the-art methods by a large margin of 15 % \\% .}
}


@article{DBLP:journals/tmc/ShakeriSGC25,
	author = {Maryam Shakeri and
                  Abolghasem Sadeghi{-}Niaraki and
                  Jens Grubert and
                  Soo{-}Mi Choi},
	title = {Personalized {AR} Content and {POI} Recommendation in Mobile Cultural
                  Heritage Systems Using Semantic Relatedness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11628--11640},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587655},
	doi = {10.1109/TMC.2025.3587655},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShakeriSGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a mobile augmented reality (AR) system designed to enhance user experiences at cultural heritage (CH) sites by providing personalized content and points of interest (POI) recommendations. Applying semantic relatedness, we address the heterogeneity of CH POIs to improve personalization accuracy. Our system constructs a POI-based co-occurrence graph to model semantic relationships, enriching users’ visited AR content items for better recommendations. The proposed method integrates content-filtering-based recommendations with this graph and recommends personalized POIs and AR content items. A user study demonstrated that our system outperforms conventional methods by 9.4% in recommendation precision and recall, significantly enhancing user engagement and attention focus during CH tours.}
}


@article{DBLP:journals/tmc/ZhangWHZXRZ25,
	author = {Yongmin Zhang and
                  Wei Wang and
                  Rui Huang and
                  Junfan Zhou and
                  Yang Xu and
                  Ju Ren and
                  Yaoxue Zhang},
	title = {Collaborative Edge and Cloud Computing: Optimal Configuration and
                  Computation Management},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11641--11656},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3584524},
	doi = {10.1109/TMC.2025.3584524},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWHZXRZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) plays an increasingly important role in the rapidly increasing mobile applications by providing high-quality computing services. The majority of current research has focused on designing efficient computing task offloading schemes to ensure the effectiveness of the MEC system. However, the configuration and resource management of the MEC system, which are crucial for its scattered features, have not received due attention. This paper investigates the configuration and computation resource management problem for the MEC system by formulating a profit maximization problem. To address this problem, we first analyze the relationship among mobile users’ offloading decisions, the configuration and computation management of the MEC system, and the service quality. Then, we design an optimal configuration and computation management scheme for the MEC system, which can not only maintain the efficiency of computing processes but also make a good trade-off between profitability and service quality. In such a way, the total expected profit of the MEC system can be maximized. Numerical evaluations show that the proposed optimal configuration and computation management scheme can efficiently improve the total profit of the MEC system.}
}


@article{DBLP:journals/tmc/JohariSTBS25,
	author = {Seyed Soheil Johari and
                  Nashid Shahriar and
                  Massimo Tornatore and
                  Raouf Boutaba and
                  Aladdin Saleh},
	title = {Anomaly Detection and Localization in {NFV} Systems by Utilizing Masked-Autoencoder
                  and {XAI}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11657--11674},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582195},
	doi = {10.1109/TMC.2025.3582195},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JohariSTBS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of Network Functions Virtualization (NFV) systems into mobile edge and core networks has heightened the need for effective anomaly detection and localization methods. The complexity of NFV demands robust mechanisms for network resilience, security, and performance. Machine Learning approaches have demonstrated promising solutions in crafting adaptive and efficient mechanisms for detecting and localizing potential anomalies within NFV systems. Particularly, Unsupervised Learning (UL) methods have garnered significant attention for their potential to detect anomalies without the need for labeled data. However, UL methods are susceptible to even minor levels of anomalous samples in the training data, termed contamination, which can severely compromise their performance. This paper proposes a novel approach using the Noisy-Student technique for anomaly detection. It addresses data contamination by combining a density-estimation teacher model for pseudo-labeling with a weakly-supervised student model based on a Masked Autoencoder trained on the pseudo-labeled data. For anomaly localization, we introduce a heuristic tailored for our anomaly detection model and two Explainable Artificial Intelligence (XAI)-based approaches applicable to any detection model. Extensive experiments on three NFV datasets demonstrate superior performance, with up to a 20% improvement in anomaly detection and up to a 22% improvement in localization, in terms of F1-score.}
}


@article{DBLP:journals/tmc/YeTLDQW25,
	author = {Yuxiao Ye and
                  Yuxuan Tian and
                  Chi Harold Liu and
                  Linkang Dong and
                  Guangpeng Qi and
                  Dapeng Oliver Wu},
	title = {AoI-Aware Air-Ground Mobile Crowdsensing by Multi-Agent Curriculum
                  Learning With Collaborative Observation Augmentation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11675--11687},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583499},
	doi = {10.1109/TMC.2025.3583499},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YeTLDQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By harnessing the capabilities of unmanned aerial and ground vehicles (UAVs and UGVs), equipped with high-precision sensors, air-ground mobile crowdsensing (AG-MCS) has proven to be effective for data collection in urban environments. In this paper, by optimizing the metric of age-of-information (AoI) that measures the freshness of collected data, we consider the problem of AoI-Aware AG-MCS (A3G-MCS), where UGVs dispatch UAVs from multiple UGV stops to collect data from point-of-interests (PoIs). We propose a novel multi-agent curriculum learning framework called “MACL(MCS)”, that explicitly balances the individual and team goals of both UAV/UGV controllers to facilitate the exploration of policy towards globally-optimal performance. It is further enhanced by a UAV/UGV collaborative observation augmentation (COA) module for improved inter-controller communication. Extensive results reveal that MACL(MCS) consistently outperforms five baselines, and achieves comparable performance to exact method with better scalability and efficiency. It also showcases strong generalization capability towards real-world scenarios on both TSPLIB and Purdue, KAIST and NCSU datasets.}
}


@article{DBLP:journals/tmc/HeHJCX25,
	author = {Yu He and
                  Guangjie Han and
                  Jinfang Jiang and
                  Xin Cheng and
                  Pengfei Xu},
	title = {{CADTR:} Context-Aware Trust Routing Algorithm Based on Priority Sampling
                  {DDPG} for UASNs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11688--11702},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581512},
	doi = {10.1109/TMC.2025.3581512},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeHJCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The underwater acoustic sensor network (UASN) is a pivotal paradigm within the underwater Internet of Things, where multi-hop forwarding-based underwater data routing is essential for information acquisition. However, the dynamic nature of underwater network topology and the instability of underwater acoustic communication pose significant challenges to achieving efficient and reliable data transmission. In light of unreliable underwater environments and potential malicious attacks, studying trusted routing strategies for UASNs is crucial. This study introduces a context-aware trust routing scheme (CADTR) based on deep reinforcement learning (DRL), which integrates real-time environmental state perception with AI-driven routing decisions, thereby enhancing the reliability and robustness of data routing in dynamic and potentially hostile underwater scenarios. First, a unified trust evidence framework is developed to strengthen the support of evidence experience for subsequent trust decisions by mapping multi-dimensional trust evidence to a unified scale. This framework is tightly coupled with the DRL agent, allowing the agent to evaluate and update trust levels based on real-time evidence. Second, a dynamic topology perception model and an underwater acoustic communication perception model are constructed to enable real-time perception of the interactive experience context. These models provide continuous input to the DRL agent, enabling it to adapt to topological changes and communication conditions dynamically. This facilitates priority experience sampling during the training process of the routing decision model, indirectly boosting model training efficiency and decision accuracy. Finally, the DRL agent learns optimal routing policies by interacting with the environment, leveraging the trust evidence and perception models to make informed decisions. Experimental results demonstrate that the proposed CADTR algorithm significantly improves the overall performance of the routing strategy in terms of packet delivery rate, energy utilization efficiency, and data transmission delay compared to the benchmark algorithms.}
}


@article{DBLP:journals/tmc/ChengWFZBJ25,
	author = {Shuang Cheng and
                  Zhaoyang Wang and
                  Fangzheng Feng and
                  Yu Zhang and
                  Ting Bi and
                  Tao Jiang},
	title = {IFresher: Information Freshening for Mobile Augmented Reality With
                  Multi-Agent Reinforcement Learning in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11703--11716},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581523},
	doi = {10.1109/TMC.2025.3581523},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChengWFZBJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose the IFresher framework to improve the timeliness of multi-agent mobile augmented reality (MAR) systems. Existing works have made strides in accuracy-latency trade-offs, but fail to directly address real-time task responsiveness and multi-agent contention challenges. To bridge this gap, we introduce the concept of the age of analytics information (AoAI), which quantifies the combined impact of video analytics (VA) accuracy, transmission delay, and computational efficiency. By deriving a closed-form expression for AoAI, IFresher establishes a central control mechanism that jointly optimizes bandwidth allocation and video configuration to minimize AoAI while ensuring accuracy. Due to the mixed-integer nonlinear characteristics of the problem and the fact that each agent only has local observations, the problem is reformulated into a decentralized partially observable Markov decision process (Dec-POMDP). We propose a multi-agent reinforcement learning (MARL) algorithm, named convex-embedded transformer QMIX (CTQMIX), using the centralized training and decentralized execution (CTDE) framework for agent collaboration. Specifically, the convex optimization ensures optimal bandwidth distribution, and the transformer captures temporal dependencies between observations and actions across time steps to improve decision-making in dynamic environments. Evaluations with real-world experiments show that the CTQMIX outperforms state-of-the-art (SOTA) algorithms.}
}


@article{DBLP:journals/tmc/LiuZWC25,
	author = {Haibo Liu and
                  Zhenzhe Zheng and
                  Fan Wu and
                  Guihai Chen},
	title = {From Non-IID to {IID:} Mobility-Aware Hierarchical Federated Learning
                  With Client-Edge Association Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11717--11730},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3585538},
	doi = {10.1109/TMC.2025.3585538},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying federated learning (FL) in wireless network with hierarchical client-edge-cloud architecture enables large-scale distribution collaboration without long-distance communication latency. However, the ongoing edge dynamics with uncertain client mobility and imbalanced data distributions, poses great challenge for collaboration efficiency of FL. In this work, we first model the client mobility with a Markov chain, and formulate the minimization of performance degradation as a client-edge association control problem. With the analysis of client mobility patterns, we propose ALPHA, a new client-edge association control framework for mobility-aware FL, to reshape the edge-level data distributions close to i.i.d in both offline and online mobility scenarios. In the offline scenario with deterministic client mobility trajectories, we leverage alternating optimization theory to transform the client-edge association control problem into a weighted bipartite b-matching problem, and derive an efficient solution with linear relaxation and dependent rounding techniques. As for the online scenario, where each client arrives at different edge access points (APs) in an online manner, we design a fast and simple online subgradient projection algorithm with a bounded regret to make an online decision on client-edge association. Extensive experiment results on three public datasets and a real-world mobility trajectory dataset show that ALPHA has a superior learning performance with 1.40× – 2.89× convergence speedup compared to state-of-the-art solutions.}
}


@article{DBLP:journals/tmc/QiuCLANW25,
	author = {Yu Qiu and
                  Min Chen and
                  Weifa Liang and
                  Lejun Ai and
                  Dusit Niyato and
                  Gang Wei},
	title = {Privacy-Enhanced Healthcare Monitoring Service Refreshment in Human
                  Digital Twin-Assisted Fabric Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11731--11747},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582084},
	doi = {10.1109/TMC.2025.3582084},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiuCLANW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human digital twin bridges humans with digital avatars in the fabric metaverse, assisting users and healthcare professionals with real-time visualization, analysis, and prediction of personal data sensed by fabric sensors. The human digital twin-assisted healthcare monitoring (HHM) service refreshment refers to sending personal health data to corresponding services hosted on nearby edge servers and receiving the results to update local digital avatars continuously. However, the malicious nature and resource limitations of edge servers may lead to user privacy leaks and refreshment timeout, thereby impacting diagnostics. In this paper, we investigate a novel privacy-enhanced HHM service refreshment maximization problem in the fabric metaverse by considering privacy data encryption, model compression, and personalized user requirements. To this end, we first formulate the above issue as an Integer Linear Programming (ILP) problem, and prove its NP-hardness. Then, a resource scheduler named Wiper is designed, consisting of a shallow-deep distiller and an agile refresher library. To enable efficient inference while preserving user privacy, the former replaces violation modules in existing models with approximations and conducts shallow distillation on model layers to meet operation type and depth limits of homomorphic encryption, and then deep distillation on model parameters to decrease end-to-end refreshment delay. Finally, to satisfy user requirements on accuracy and delay during encrypted refreshments while maximizing the throughput of HHM services in offline and online situations with different problem scales, a series of HHM service refreshment algorithms are merged into the latter, including exact, performance-guaranteed approximation, and residual diffusion reinforcement learning algorithms. Theoretical analyses and experiments demonstrate that our algorithms are promising compared with baseline algorithms.}
}


@article{DBLP:journals/tmc/HeCLW25,
	author = {Tongyue He and
                  Junxin Chen and
                  Chi Lin and
                  Wei Wang},
	title = {Mobile Phone-Based Digital Biomarkers Empowered by Knowledge Distillation
                  for Diagnosis of Parkinson's Disease},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11748--11763},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581525},
	doi = {10.1109/TMC.2025.3581525},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeCLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile phones have evolved from basic communication tools to feature-rich mobile devices. These ubiquitous and portable devices, equipped with inertial sensors and high-speed network access, create opportunities for remote health monitoring, especially for movement disorders such as Parkinson’s disease (PD). Inertial sensors (gyroscopes and accelerometers) endow smartphones with a natural ability to monitor movement disorders. Based on this, we develop a novel vision-based time-series feature augmentation framework for remote diagnosis and severity grading of PD using mobile phone walking records. Specifically, preprocessed time-series data is encoded into RGB images for the teacher model, while the time-series data is input into the student model, with the teacher guiding the student’s learning. The teacher model is based on MobileNetV2 and incorporates spatial and channel relation-aware attention mechanisms to capture important features and filter out irrelevant information. The inter-modal feature fusion module combines attention and CNN to emphasize both global and local features. The student model utilizes a simple CNN to directly extract features from time-series data and perform classification. For the three-level classification task, the teacher model achieves accuracies of 0.887, 0.886, and 0.896 across the three datasets, while the distillation student model reaches 0.779, 0.828, and 0.827, generally surpassing state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/LiuTGLF25,
	author = {Liang Liu and
                  Siyuan Tan and
                  Songtao Guo and
                  Guiyan Liu and
                  Hao Feng},
	title = {Joint Dynamic {VNF} Placement and Delay and Jitter Aware Multicast
                  Routing in NFV-Enabled SDNs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11764--11778},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581905},
	doi = {10.1109/TMC.2025.3581905},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuTGLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For reliability, security and scalability, Multicast Request flows (MRs) need to traverse a Service Function Chain (SFC) that consists of series of Virtual Network Functions (VNFs) such as firewalls, encoder-decoder in Network Function Virtualization-enabled Software-Defined Networks (NFV-enabled SDNs). There are typically multiple identical VNF-instances in the network, it brings significant challenges when dynamically choosing or placing the requisite VNF-instances to construct a Service Function Tree (SFT) consisting of SFCs for fulfilling the MRs’s routing. This paper investigates the Delay and Jitter Aware Dynamic SFT Embedding and Routing Problem (DJA-DSERP) considering VNF placement, network resources, delay and jitter constraints as well as network load balance in NFV-Enabled SDNs. First, we formulate DJA-DSERP as an integer linear programming model and prove it to be NP-hard. Then, an auxiliary edge-weighted graph and an Optimal Link Selection Function (OLSF) are devised, and SFT Embedding Algorithm (SFT-EA) is proposed to address the problem aiming at minimizing the resource consumption costs while satisfying multiple QoS constraints and network load balance. Furthermore, we theoretically prove the effectiveness of the OLSF and the SFT-EA. Simulation results demonstrate that the SFT-EA exhibits superior performance compared to existing algorithms in terms of throughput, traffic acceptance rate, and network load balance.}
}


@article{DBLP:journals/tmc/GoelJV25,
	author = {Shefali Goel and
                  Vinod Kumar Jain and
                  Abhishek Verma},
	title = {LiSec-RTF: Reinforcing {RPL} Resilience Against Routing Table Falsification
                  Attack in 6LoWPAN},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11779--11792},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581561},
	doi = {10.1109/TMC.2025.3581561},
	timestamp = {Mon, 20 Oct 2025 15:33:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GoelJV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent node’s routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed “Licenses.” LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.}
}


@article{DBLP:journals/tmc/FengZZBJ25,
	author = {Fangzheng Feng and
                  Yu Zhang and
                  Xinkun Zheng and
                  Ting Bi and
                  Tao Jiang},
	title = {{LOCA:} Long-Term Optimization Based on Chunk-Level Analysis in Edge-Assisted
                  Massive Mobile Live Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11793--11807},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581922},
	doi = {10.1109/TMC.2025.3581922},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengZZBJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents an edge-assisted massive mobile live streaming (MMLS) framework named LOCA, integrating chunk-level analysis and long-term optimization to design resource allocation, bitrate adaptation, and source selection strategies. The proposed method ensures sustained real-time video delivery while minimizing latency and communication costs. First, a chunk-level analysis of the entire process of video streaming is introduced, aiming at modeling fetch queue waiting time and rebuffering duration in each time slot. By embedding this mathamatical model into consideration, a long-term optimization is formulated to minimize rebuffering and communication overhead while maintaining high video qualities for massive users. Leveraging Lyapunov optimization, we transform this problem into a computationally tractable form. Further simplification via linearization achieves near-optimal solutions by adopting the mixed-integer linear programming method with enhanced computational efficiency. Simulation results demonstrate superior stability and long-term performance compared to the state-of-the-art and baseline methods, validating the framework’s efficacy in MMLS scenarios.}
}


@article{DBLP:journals/tmc/ChenCZTJWC25,
	author = {Zhen Chen and
                  Gaojie Chen and
                  Xiu Yin Zhang and
                  Jie Tang and
                  Shi Jin and
                  Kai{-}Kit Wong and
                  Jonathon A. Chambers},
	title = {Joint Power Allocation and Phase Shifts Design for Distributed RIS-Assisted
                  Multiuser Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11808--11819},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582750},
	doi = {10.1109/TMC.2025.3582750},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenCZTJWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed reconfigurable intelligent surfaces (RISs) provide rich macro-diversity coverage due to different locations of the RISs, which is beneficial to combat coverage holes. However, the system performance relies on the effective coordination of multiple RISs. In particular, distributed RIS-assisted power allocation and the phase shifts of RISs should be jointly designed under nonlinear scheduling constraints. Thus, the resource allocation scheme for distributed RIS-assisted multiuser system is a crucial challenge. To tackle these issues, joint power allocation, phase shifts and communication scheduling design for distributed RIS-assisted systems is investigated in this paper, where all RISs simultaneously and cooperatively serve multiple users. To overcome the formulated nonconvex optimization problem, the original problem is decoupled into three subproblems and solved in an iterative manner. Specifically, we first consider the subproblem of power allocation, which can be solved via maximizing the ergodic achievable rate. By applying the ergodic rate, an approximate closed-form solution is formed for the power allocation. Subsequently, the phase shifts are optimized using the minimization-maximization optimization methods. Finally, a communication scheduling scheme is presented to address the scheduling variables. Numerical simulations are conducted to demonstrate that the considered solution outperforms the existing benchmark and achieves a near-optimal spectral efficiency.}
}


@article{DBLP:journals/tmc/AramiRM25,
	author = {Mohammad Ali Arami and
                  Erfan Rasti and
                  Abbas Mohammadi},
	title = {A User-Centric Energy-Saving Method for Dynamic 5G Heterogeneous Networks
                  Using Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11820--11832},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582623},
	doi = {10.1109/TMC.2025.3582623},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AramiRM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy consumption (EC) represents a significant challenge for 5G and 6G mobile networks, necessitating a primary focus on optimizing energy savings (ES). This paper illustrates the practical benefits of a user-centric deep reinforcement learning (DRL) models in achieving a green cellular network. The primary objective is to optimize energy usage in a heterogeneous network (HetNet). The optimization of power consumption (PC) in such networks is a non-convex and NP-hard problem. To address this challenge, we propose using reinforcement learning (RL). Due to the extensive state and action space, classical RL approaches are unsuitable. Therefore, the adoption of DRL methods, notably the deep Q-network (DQN) and deep deterministic policy gradient (DDPG) methods, is necessary. The proposed approach entails a user-centric connection establishment, whereby small base stations (SBSs) are switched to an on mode. The mode switching determined by the DRL methods is controlled by an anti-abrupt transition mechanism, which prevents unnecessary oscillations in the network. The results are benchmarked against existing approaches, specifically genetic algorithm (GA) and particle swarm optimization (PSO) for ES. The proposed methods outperform both GA and PSO optimization techniques in terms of ES and significantly reduce time consumption, enhancing its practical implementation feasibility.}
}


@article{DBLP:journals/tmc/ZhengSLWWNJ25,
	author = {Xiaoya Zheng and
                  Geng Sun and
                  Jiahui Li and
                  Jiacheng Wang and
                  Qingqing Wu and
                  Dusit Niyato and
                  Abbas Jamalipour},
	title = {{UAV} Swarm-Enabled Collaborative Post-Disaster Communications in
                  Low Altitude Economy via a Two-Stage Optimization Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11833--11851},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583510},
	doi = {10.1109/TMC.2025.3583510},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengSLWWNJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The low-altitude economy (LAE), as a new economic paradigm, plays an indispensable role in cargo transportation, healthcare, infrastructure inspection, and especially post-disaster communications. Specifically, uncrewed aerial vehicles (UAVs), as one of the core technologies of the LAE, can be deployed to provide communication coverage, facilitate data collection, and relay data for trapped users, thereby significantly enhancing the efficiency of post-disaster response efforts. However, conventional UAV self-organizing networks exhibit low reliability in long-range cases due to their limited onboard energy and transmit ability. Therefore, in this paper, we design an efficient and robust UAV-swarm enabled collaborative self-organizing network to facilitate post-disaster communications. Specifically, a ground device transmits data to UAV swarms, which then use collaborative beamforming (CB) technique to form virtual antenna arrays and relay the data to a remote access point (AP) efficiently. Then, we formulate a rescue-oriented post-disaster transmission rate maximization optimization problem (RPTRMOP), aimed at maximizing the transmission rate of the whole network. Given the challenges of solving the formulated RPTRMOP by using traditional algorithms, we propose a two-stage optimization approach to address it. In the first stage, the optimal multi-path traffic routing and the theoretical upper bound on the transmission rate of the network are derived. In the second stage, we transform the formulated RPTRMOP into a variant named V-RPTRMOP based on the obtained optimal multi-path traffic routing, aimed at rendering the actual transmission rate closely approaches its theoretical upper bound by optimizing the excitation current weight and the placement of each participating UAV via a diffusion model-enabled particle swarm optimization (DM-PSO) algorithm. Simulation results show the effectiveness of the proposed two-stage optimization approach in improving the transmission rate of the constructed network, which demonstrates the great potential for post-disaster communications. Moreover, the robustness of the constructed network is also validated via evaluating the impact of three unexpected situations on the system transmission rate.}
}


@article{DBLP:journals/tmc/CuiC25,
	author = {Yue Cui and
                  Man Hon Cheung},
	title = {The Price of Forgetting: Incentive Mechanism Design for Machine Unlearning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11852--11864},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582904},
	doi = {10.1109/TMC.2025.3582904},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CuiC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data protection policies (e.g., GDPR) enforce the right to be forgotten and require companies to perform machine unlearning once users request data removal. This process incurs costs for server and degrades model performance, impacting users’ satisfaction. In this paper, we propose the first incentive mechanism for machine unlearning, where server compensates users to retain their data. We characterize server’s major unlearning costs, accuracy degradation and consumed time, in data redemption amount through experiments on three datasets and two unlearning algorithms. We model server-users interaction as a two-stage Stackelberg game. In Stage I, server optimizes compensation unit prices to minimize costs. In Stage II, users jointly decide data redemption amounts as a non-cooperative game. By restricting the feasible set of Stage I to Nash Equilibrium of Stage II, we formulate a challenging non-convex bilevel optimization problem. We propose an iterative algorithm to compute optimal unit prices in Stage I and equilibrium data redemption amounts in Stage II by characterizing bilevel problem’s convexity. We prove the distributed convergence of best response updates to the unique Nash equilibrium by showing Stage II is a submodular game. Experimental results show that our mechanism minimizes server cost and maximizes social welfare over two practical baselines.}
}


@article{DBLP:journals/tmc/WeiYZL25,
	author = {Xinyuan Wei and
                  Weijie Yuan and
                  Kecheng Zhang and
                  Fan Liu},
	title = {OTFS-Assisted {ISAC} System: Delay Doppler Channel Estimation and
                  SDR-Based Implementation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11865--11878},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582421},
	doi = {10.1109/TMC.2025.3582421},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiYZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Orthogonal Time-Frequency Space (OTFS) modulation is an emerging technique that characterizes wireless channels and transmits information in the delay-Doppler domain. This work focuses on estimating fundamental sensing parameters, i.e., the delay and Doppler shifts of individual propagation paths, which serve as critical enablers for downstream positioning techniques, such as time-difference-of-arrival (TDOA)-based localization. Specifically, we propose a parameter-inherited (PI) channel estimation method that integrates sparse Bayesian learning (SBL) with unitary approximate message passing (UAMP), achieving low computational complexity and high estimation robustness. To accelerate the convergence of the UAMP-based iterative estimation, we explore the strategy of initializing parameters by inheriting prior estimates from adjacent OTFS transmission blocks. Furthermore, the overall computational burden is significantly reduced by employing large-scale matrix operations via two-dimensional fast Fourier transform (2D FFT). The proposed algorithms are implemented and evaluated on a software-defined radio (SDR)-based ISAC platform. Experimental results demonstrate that the proposed dual-functional system outperforms existing benchmarks in both communication quality and sensing parameter accuracy.}
}


@article{DBLP:journals/tmc/ChiLDNFW25,
	author = {Yang Chi and
                  Chi Lin and
                  Jing Deng and
                  Kaiwen Ning and
                  Xin Fan and
                  Guowei Wu},
	title = {Edge Computing Underwater Optical Wireless Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11879--11895},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581690},
	doi = {10.1109/TMC.2025.3581690},
	timestamp = {Thu, 06 Nov 2025 14:15:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChiLDNFW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater Optical Wireless Sensor Networks (UOWSNs) play important roles in resource exploration and maritime rescue. However, they face significant challenges in real-time data transmission due to the limited propagation range of optical signals (typically 10–100 m), frequent link disconnections caused by node mobility, and the extended distances to onshore servers. Traditional cloud computing solutions, designed for stable terrestrial networks with stationary edge servers and continuous connectivity, experience high latency (3-15 s) in UOWSNs, rendering them unsuitable for real-time applications in underwater environments. To address this issue, we propose a cloud-edge-end architecture tailored for UOWSNs, which can not only combat unique underwater environmental interference on link connection and topological changes but also guarantee robust and real-time communication. We develop a dynamic link-stability-based task offloading path selection (DLS-TOPS) algorithm for maximizing network resource profits. Afterward, we propose an online primal-dual task offloading (OPD-TO) algorithm for minimizing task completion time. Simulation results indicate that the proposed method significantly improves the real-time performance and resource profits of the network, reducing the total task completion time by more than 50% compared to baseline algorithms. We implemented a UOWSN with a cloud-edge-end architecture using commercial off-the-shelf and verified the applicability and effectiveness of the proposed scheme in emergency detection through testbed experiments.}
}


@article{DBLP:journals/tmc/QiangCM25,
	author = {Xianke Qiang and
                  Zheng Chang and
                  Geyong Min},
	title = {AIGC-Assisted Federated Learning for Vehicular Edge Intelligence:
                  Vehicle Selection, Resource Allocation and Model Augmentation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11896--11909},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581983},
	doi = {10.1109/TMC.2025.3581983},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiangCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To leverage the vast amounts of onboard data while ensuring privacy and security, federated learning (FL) is emerging as a promising technology for supporting a wide range of vehicular applications. Although FL has great potential to improve the vehicular edge intelligence(VEI), challenges arise due to vehicle mobility, wireless channel instability, and data heterogeneity. To mitigate the issue of heterogeneous data across vehicles in FL, artificial intelligence-generated content (AIGC) can be employed as an innovative data synthesis technique to enhance FL model performance. In this paper, we propose AIGC-assisted Federated Learning for Vehicular Edge Intelligence (GenFV). We further propose a weighted policy using the Earth Mover’s Distance (EMD) to measure data distribution heterogeneity and introduce a convergence analysis for GenFV. Subsequently, we analyze system delay and formulate a mixed-integer nonlinear programming (MINLP) problem to minimize system delay. To solve this MINLP NP-hard problem, we propose a two-scale algorithm. At large communication scale, we implement label sharing and vehicle selection based on mobility and data heterogeneity. At the small computation scale, we optimally allocate bandwidth, transmission power and amount of generated data. Extensive experiments show that GenFV significantly improves the performance and robustness of FL in dynamic, resource-constrained environments, outperforming other schemes and confirming the effectiveness of our approach.}
}


@article{DBLP:journals/tmc/ZhuHYHZL25,
	author = {Yinan Zhu and
                  Haiyan Hu and
                  Baichen Yang and
                  Qianyi Huang and
                  Qian Zhang and
                  Wei Li},
	title = {FreshSpec: Sashimi Freshness Monitoring With Low-Cost Multispectral
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11910--11926},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581714},
	doi = {10.1109/TMC.2025.3581714},
	timestamp = {Fri, 10 Oct 2025 10:59:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHYHZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Monitoring sashimi freshness, i.e., histamine levels, in showcases poses a critical challenge for sushi restaurants and fresh food stores. Current histamine monitoring methods involve labor-intensive chemical experiments or expensive devices, making affordable on-site monitoring difficult. This paper proposes FreshSpec, a low-cost and automatic spectral imaging system capable of precisely monitoring histamine levels in sashimi with minimal human intervention. The low concentration of histamine, combined with the potential for other ingredients to mask its spectral characteristics, complicates precise histamine level predictions using coarse or redundant spectral data from low-cost devices. To address this issue, FreshSpec employs an innovative feature-wise spectral reconstruction (SR) framework that effectively eliminates irrelevant and redundant data while preserving critical histamine-related spectral features. Specifically, we redefine the SR reconstruction target by utilizing features derived from the encoder of the spectral foundation model that is enhanced to focus on histamine-related spectral features. Furthermore, inspired by the monotonic accumulation properties of histamine over time, we propose a histamine regression model with unsupervised continual adaptation to new sashimi samples during practical deployment. Experimental results from 240 samples of salmon, tuna, and snapper demonstrate that FreshSpec achieves an R2 of 0.9319 and an RMSE of 3.101 mg/100 g, comparable to laboratory spectral imaging systems, while outperforming baseline schemes with a 46.95% RMSE reduction and a 0.1631 R2 improvement.}
}


@article{DBLP:journals/tmc/FanWXZLHH25,
	author = {Weibei Fan and
                  Donglai Wang and
                  Fu Xiao and
                  Yiping Zuo and
                  Mengjie Lv and
                  Lei Han and
                  Sun{-}Yuan Hsieh},
	title = {Dynamic Topology and Resource Allocation for Distributed Training
                  in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11927--11941},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581510},
	doi = {10.1109/TMC.2025.3581510},
	timestamp = {Thu, 16 Oct 2025 09:58:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanWXZLHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC), edge servers and mobile terminals use federated learning distributed architecture to build a deep model, so that terminals can cooperate in training without sharing data. Distributed training requires network virtualization to provide high bandwidth and low latency characteristics to support large-scale parallel computing. Traditional virtual network embedding (VNE) relies on a static network topology, which lacks flexibility and incurs high resource costs during model training. To improve the efficiency of embedding distributed training tasks, we propose a novel Node Selection and Dynamic Topology resource allocation scheme for VNE of distributed training, NSDT-VNE, based on reconfigurable network topology. This algorithm divides the underlying network into static and dynamic topologies, enhancing low latency for small flows while providing high bandwidth for large flows as needed. Additionally, we introduce a two-phase coordinated alternating optimization algorithm that optimizes embedding decisions at both computational and topological levels, ensuring optimal node selection. Overall, NSDT-VNE follows demand-aware network design principles, allowing continuous optimization of the underlying topology. Compared to state-of-the-art heuristic and reinforcement learning-based virtual network algorithms, NSDT-VNE achieves superior performance, with request acceptance rates improving by 6.67% to 25.68% and embedding revenue increasing by approximately 7% to 32%.}
}


@article{DBLP:journals/tmc/FanCJZBH25,
	author = {Chunyang Fan and
                  Jie Cui and
                  Hulin Jin and
                  Hong Zhong and
                  Irina Bolodurina and
                  Debiao He},
	title = {Robust Intrusion Detection System for Vehicular Networks: {A} Federated
                  Learning Approach Based on Representative Client Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11942--11956},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582237},
	doi = {10.1109/TMC.2025.3582237},
	timestamp = {Wed, 15 Oct 2025 19:22:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanCJZBH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of network technology has allowed numerous vehicular applications to be deployed in vehicles, thereby enriching the driving experience of users. However, the openness of vehicular networks enables attackers to launch network attacks on vehicles through network ports, leading to the destruction of vehicular networks. To develop an intrusion detection system suitable for distributed vehicular networks, researchers have utilized federated learning to train detection models. Nevertheless, most federated learning-based vehicular intrusion detection systems seldom consider rapidly updating the detection model and fail to detect unknown attacks effectively. In this study, we propose a federated learning-based vehicular intrusion detection system that fully considers the traffic characteristics of multiple network regions and selects representative clients to participate in model aggregation, thereby accelerating the convergence of the global model. Furthermore, to enhance the robustness of the detection system, we utilize extreme value theory and multilayer activation vectors to construct an unknown attack discriminator that can determine whether a network flow is an unknown attack. Comprehensive experiments on three open datasets demonstrate that the proposed intrusion detection system can quickly update and effectively identify known/unknown attacks in open vehicular networks.}
}


@article{DBLP:journals/tmc/ChenWHLJLL25,
	author = {Hao Chen and
                  Penghao Wang and
                  Jingyang Hu and
                  Feng Li and
                  Hongbo Jiang and
                  Minglu Li and
                  Chao Liu},
	title = {Wi-GR: Wi-Fi-Based Gait Recognition Using Multi-Part Velocity Profile},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11957--11971},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581549},
	doi = {10.1109/TMC.2025.3581549},
	timestamp = {Thu, 25 Dec 2025 12:46:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWHLJLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, with increasing user demands for convenience, privacy, and personalized experiences, gait recognition has been widely studied across various domains, such as indoor intrusion detection and smart homes. Although computer vision solutions are extensively researched for their visual intuitiveness, Wi-Fi sensing is emerging as a new research focus due to its ability to preserve privacy. However, previous studies have primarily relied on abstract features with limited interpretability or required multiple Wi-Fi links. To address these issues, we propose Wi-GR, which utilizes a Wi-Fi link to extract robust and highly interpretable gait features for user recognition. First, we construct a multi-path gait signal model to establish a clear relationship between Channel State Information (CSI) and gait motion. Then, we design a gait signal separation and enhancement method to mitigate the effects of external non-target reflections and internal multi-part reflections, which significantly impact the extraction and interpretability of gait features. Finally, fine-grained gait features that visualize gait patterns are generated using MUSIC-based and GAN-based multi-part velocity profile generation algorithms, tailored for single-person and multi-person scenarios, respectively. Numerous experiments have demonstrated that Wi-GR achieves single-person recognition accuracies of 95.3%, 94.0%, and 93.2% for 30 persons in the meeting room, corridor, and lobby, respectively, and an average accuracy of 88.3% for two-person recognition.}
}


@article{DBLP:journals/tmc/MaWWZZWW25,
	author = {Jiaming Ma and
                  Binwu Wang and
                  Pengkun Wang and
                  Zhengyang Zhou and
                  Yudong Zhang and
                  Xu Wang and
                  Yang Wang},
	title = {MobiMixer: {A} Multi-Scale Spatiotemporal Mixing Model for Mobile
                  Traffic Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11972--11986},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3585007},
	doi = {10.1109/TMC.2025.3585007},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaWWZZWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding mobile traffic data and predicting future trends are essential for wireless operators and service providers to allocate resources efficiently and manage energy effectively. Despite the strong performance of existing models, accurately forecasting mobile traffic remains a challenge due to limited spatial and temporal modeling capabilities and high computational complexity. This paper introduces MobiMixer, a lightweight and efficient multi-scale spatiotemporal mixing model. Its core concept is to integrate multi-scale information from both spatial and temporal dimensions to improve performance on mobile traffic data. We develop a hierarchical interaction module that incorporates super nodes to enable global high-level feature interactions among nodes with common patterns. Additionally, we employ a dynamic time warping strategy to decouple mobile traffic sequences into stable and seasonal components, which are then modeled at different scales using a multi-scale temporal mixing module. We conduct extensive experiments on mobile traffic datasets collected from four international cities. Compared with 21 state-of-the-art benchmark models, MobiMixer demonstrates highly competitive performance, achieving a maximum improvement of 48.49% on the Milan mobile dataset. The model achieves an improvement in training efficiency of up to 10.69 times and reduces memory usage by 33.01%.}
}


@article{DBLP:journals/tmc/ChenZY25,
	author = {Xingran Chen and
                  Yi Zhuang and
                  Kun Yang},
	title = {Age of Computing: {A} Metric of Computation Freshness in Communication
                  and Computation Cooperative Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {11987--12000},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582741},
	doi = {10.1109/TMC.2025.3582741},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In communication and computation cooperative networks (3CNs), timely computation is crucial but not always guaranteed. There is a strong demand for a computational task to be completed within a given deadline. The time taken involves processing time, transmission time, and the impact of the deadline. However, a measure of such timeliness in 3CNs is lacking. To address this gap, we propose the novel concept of Age of Computing (AoC) to quantify computation freshness in 3CNs. Built on task timestamps, AoC serves as a practical metric for dynamic and complex real-world 3CNs. We evaluate AoC under two types of deadlines: (i) soft deadline, tasks can be fed back to the source if delayed beyond the deadline, but with additional latency; (ii) hard deadline, tasks delayed beyond the deadline are discarded. We investigate AoC in two distinct networks. In point-to-point, time-continuous networks, tasks are processed sequentially using a first-come, first-served discipline. We derive a general expression for the time-average AoC under both deadlines. Utilizing this expression, we obtain a closed-form solution for M/M/1-M/M/1 systems under soft deadlines and propose an accurate approximation for hard deadlines. These results are further extended to G/G/1-G/G/1 systems. Additionally, we introduce the concept of computation throughput, derive its general expression and an approximation, and explore the trade-off between freshness and throughput. In the multi-source, time-discrete networks, tasks are scheduled for offloading to a computational node. For this scenario, we develop AoC-based Max-Weight policies for real-time scheduling under both deadlines, leveraging a Lyapunov function to minimize its drift.}
}


@article{DBLP:journals/tmc/DaiCWS25,
	author = {Minghui Dai and
                  Shan Chang and
                  Yixuan Wang and
                  Zhou Su},
	title = {Energy-Efficient Multi-Access Edge Computing for Heterogeneous Satellite-Maritime
                  Networks: {A} Hybrid Harvesting-and-Offloading Design},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12001--12018},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581607},
	doi = {10.1109/TMC.2025.3581607},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiCWS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low earth orbit (LEO) constellation integrated maritime networks have recently attracted much interest due to the rapid development of maritime applications and services. LEO satellites have the advantages of wide coverage to provide seamless connection for maritime wireless devices. However, due to the limited battery and computing capacity of uncrewed aerial vehicles (UAVs) for ocean information perception and processing, the computing-intensive and delay-sensitive oceanic data suffer from long latency and high energy consumption, which degrades the efficiency of maritime services. In this paper, to enhance the perception and offloading endurance of UAVs in maritime networks, we propose an energy efficient multi-access edge computing scheme for heterogeneous satellite-maritime networks, with the objective of minimizing the cumulative transmitted energy for UAVs. Specifically, we first present a heterogeneous satellite-maritime network framework in which LEO satellites and uncrewed surface vehicles (USVs) equipped with edge servers can process workloads simultaneously. Next, considering the limited battery supply of UAVs, we propose a hybrid harvesting-and-offloading scheme for resource allocation, where UAVs first harvest energy from solar power and radio frequency power from USV, and then UAVs determine the offloading strategy for task processing. Moreover, a joint optimization problem is formulated to optimize the offloading decision, the time scheduling, and the transmitting power. We also exploit a vertical architecture to solve the formulated problem. Regarding each decomposed sub-problem, we propose efficient algorithms to derive the corresponding solutions. Finally, we provide numerical results to validate the performance of our proposed algorithms in comparison with several benchmark algorithms.}
}


@article{DBLP:journals/tmc/SuWYXMXZG25,
	author = {Shan Su and
                  Liang Wang and
                  Zhiwen Yu and
                  Xiaofang Xia and
                  Lianbo Ma and
                  Fei Xiong and
                  Yao Zhang and
                  Bin Guo},
	title = {Crowdsensing for Emergency Response in Unknown Environments: {A} Rapid
                  Strategic Sensing Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12019--12034},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583816},
	doi = {10.1109/TMC.2025.3583816},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SuWYXMXZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating Unmanned Aerial Vehicles (UAVs) and autonomous vehicles within the crowdsensing paradigm offers a promising approach to collecting environment-relevant data over large spatial areas, particularly in disaster-stricken or high-risk regions. However, deploying crowdsensing systems in emergency response scenarios presents substantial challenges. The lack of prior environmental knowledge complicates the selection of optimal sensing locations and strategy optimization, often relying on costly trial-and-error methods. Additionally, real-time decision-making is critical in such scenarios, requiring the rapid identification of optimal deployment strategies. Yet, the absence of prior knowledge further complicates the assessment of the optimality of these strategies. This gap remains inadequately addressed in existing research. To address this, we present the first framework that frames these challenges as a rapid online strategy optimization problem for mobile agent-based crowdsensing systems operating in unknown environments during emergency response scenarios. We propose  D G a p − U C B {\\sf DGap\\!-\\! UCB} , a novel approach within the multi-armed bandit (MAB) framework, which efficiently identifies the optimal sensing strategy with high-confidence guarantees. Leveraging the Upper-Confidence Bound (UCB) technique,  D G a p − U C B {\\sf DGap \\!-\\! UCB}  iteratively refines strategy selection based on reward feedback. To accelerate learning, we introduce a gap-confidence pair  ( Δ t , δ t ) (\\Delta _{t}, \\delta _{t}) -based Quick Stopping Criterion, enabling rapid and high-confidence identification of the optimal strategy. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of  D G a p − U C B {\\sf DGap\\!-\\!UCB}  over state-of-the-art techniques.}
}


@article{DBLP:journals/tmc/JinWYLC25,
	author = {Haijia Jin and
                  Jun Wu and
                  Weijie Yuan and
                  Fan Liu and
                  Yuanhao Cui},
	title = {Co-Design of Sensing, Communications, and Control for Low-Altitude
                  Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12035--12048},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581616},
	doi = {10.1109/TMC.2025.3581616},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JinWYLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of Internet of Things (IoT) services and the evolution toward the sixth generation (6G) have positioned aerial drones as critical enablers of low-altitude wireless networks (LAWNs). This work investigates the co-design of integrated sensing, communication, and control ( S C 2 \\mathbf {SC^{2}} ) for multi-aerial drone cooperative systems with finite blocklength (FBL) transmission. In particular, the aerial drones continuously monitor the state of the field robots and transmit their observations to the robot controller to ensure stable control while cooperating to localize an unknown sensing target (ST). To this end, a weighted optimization problem is first formulated by jointly considering the control and localization performance in terms of the linear quadratic regulator (LQR) cost and the determinant of the Fisher information matrix (FIM), respectively. The resultant problem, optimizing resource allocations, the aerial drones’ deployment positions, and multi-user scheduling, is non-convex. To circumvent this challenge, we first derive a closed-form expression of the LQR cost with respect to other variables. Subsequently, the non-convex optimization problem is decomposed into a series of sub-problems by leveraging the alternating optimization (AO) approach, in which the difference of convex functions (DC) programming and projected gradient descent (PGD) method are employed to obtain an efficient near-optimal solution. Furthermore, the convergence and computational complexity of the proposed algorithm are thoroughly analyzed. Extensive simulation results are presented to validate the effectiveness of our proposed approach compared to the benchmark schemes and reveal the trade-off between control and sensing performance.}
}


@article{DBLP:journals/tmc/YangWLBYC25,
	author = {Nuocheng Yang and
                  Sihua Wang and
                  Yuchen Liu and
                  Christopher G. Brinton and
                  Changchuan Yin and
                  Mingzhe Chen},
	title = {Graph Neural Networks for the Optimization of Collaborative Federated
                  Learning Energy Efficiency},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12049--12060},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582911},
	doi = {10.1109/TMC.2025.3582911},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangWLBYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper delves into the design of an energy efficient collaborative federated learning (CFL) methodology using which mobile devices exchange their FL model with a subset of their neighbors without reliance on a parameter server based on the distributed graph neural network (GNN) method. Each device is unable to send its FL model to every neighboring device due to device mobility and wireless resource limitations. To reduce the energy consumption of FL model transmission, each device must choose a subset of devices with which to share its FL model. This problem is formulated as an optimization problem to meet the constraints of delay and training loss while minimizing the energy consumption for model transmission. However, the formulated problem is difficult to solve since the device mobility patterns, and the relationship between the device connection scheme and CFL performance are unknown. To address this challenge, we analytically characterize the relationship between dynamic device connections and the performance of CFL methodology. Based on the analysis, a GNN based algorithm is proposed to enable each device to select a subset of its neighbors and the transmit power in a decentralized method. Compared to standard optimization methods that must determine device connections in a centralized manner, the GNN based method enables each device to use its neighboring devices’ location and connection information to individually determine a subset of devices to transmit the local model. Given the device connections, the optimal transmit power of each device can be determined by convex optimization. Simulation results show that the proposed method can reduce the energy consumption for model transmission and training loss by up to 46 % \\%  and 2 % \\% , respectively.}
}


@article{DBLP:journals/tmc/LiZLS25,
	author = {Yi Li and
                  Hanying Zhao and
                  Yiman Liu and
                  Yuan Shen},
	title = {An Enhanced Stereo {UWB} Bearing Scheme via Network Ambiguity Resolution
                  and Online Phase Calibration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12061--12075},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583257},
	doi = {10.1109/TMC.2025.3583257},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-wideband (UWB) is a prominent technology for wireless localization, mainly attributed to its superior ranging performance enabled by the large signal bandwidth. However, its bearing capability remains underdeveloped due to practical issues such as phase deviations, antenna coupling, and phase ambiguity. This paper presents a high-accuracy stereo ultra-wideband (UWB) bearing scheme through network ambiguity resolution and online phase calibration. Specifically, we propose a sparse variational Gaussian process regression-based calibration technique to eliminate phase deviations and a range-assisted network solution to resolve phase ambiguities. Building on these techniques, we present an online angle estimation scheme that performs real-time phase calibration, ambiguity resolution, and calibration model updates, significantly reducing calibration complexity in large-scale networks. Real-world experiments on 4-element stereo UWB platforms achieve root mean square errors of 2.3 ∘ ^\\circ  and 1.1 ∘ ^\\circ  for azimuth and elevation angles, respectively. The success rate for ambiguity resolution exceeds 96%, a 20% improvement over existing methods.}
}


@article{DBLP:journals/tmc/LiuHJYZQ25,
	author = {Yu Liu and
                  Jingyang Hu and
                  Hongbo Jiang and
                  Kehua Yang and
                  Wei Zhang and
                  Zheng Qin},
	title = {{CSID:} Enhancing Wi-Fi Based Gait Recognition via Adversarial Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12076--12087},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583946},
	doi = {10.1109/TMC.2025.3583946},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHJYZQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of Wi-Fi sensing, wireless-based gait recognition has become increasingly important as it supports a wide range of applications (person identification, disease diagnosis, etc.). However, two serious challenges limit the universal deployment of such Wi-Fi vision schemes: i) the limited bandwidth of Wi-Fi severely restricts the granularity of gait recognition, and ii) users’ non-gait behaviors (e.g., stopping and turning) interfere with the extraction of gait-related features. In this paper, we propose CSID, which can achieve robust gait recognition under the limited bandwidth conditions of commercial Wi-Fi devices. Specifically, we use a neural network to generate super-resolution spectrograms of channel state information (CSI), overcoming the limitation of insufficient Wi-Fi bandwidth. To overcome the challenge of non-gait behavior interference, considering the human-incomprehensible nature of Wi-Fi spectrograms, we adopt cross-domain adversarial training and further extract gait features that are independent of the interference behaviors by learning domain-independent representations. We conducted a large number of experiments in different indoor environments, and the average person identification rate of the CSID system reached 91.6%. These results demonstrate that the CSID system is promising and could be used as a complement to visual person identification systems in the future.}
}


@article{DBLP:journals/tmc/XieDLRLL25,
	author = {Jianhang Xie and
                  Chuntao Ding and
                  Xiaqing Li and
                  Shenyuan Ren and
                  Yidong Li and
                  Zhichao Lu},
	title = {NestQuant: Post-Training Integer-Nesting Quantization for On-Device
                  {DNN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12088--12102},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582583},
	doi = {10.1109/TMC.2025.3582583},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieDLRLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1 K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models.}
}


@article{DBLP:journals/tmc/ShenYCZWLH25,
	author = {Leming Shen and
                  Qiang Yang and
                  Kaiyan Cui and
                  Yuanqing Zheng and
                  Xiao{-}Yong Wei and
                  Jianwei Liu and
                  Jinsong Han},
	title = {Hierarchical and Heterogeneous Federated Learning via a Learning-on-Model
                  Paradigm},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12103--12120},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581534},
	doi = {10.1109/TMC.2025.3581534},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenYCZWLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) collaboratively trains a shared global model without exposing clients’ private data. In practical FL systems, clients (e.g., smartphones and wearables) typically have disparate system resources. Traditional FL, however, adopts a one-size-fits-all solution, where a homogeneous large model is sent to and trained on each client. This method results in an overwhelming workload for less capable clients and starvation for others. To tackle this, we propose FedConv, a client-friendly FL framework, minimizing the system overhead on resource-constrained clients by providing heterogeneous customized sub-models. FedConv features a novel learning-on-model paradigm that learns the parameters of heterogeneous sub-models via convolutional compression. To aggregate heterogeneous sub-models, we propose transposed convolutional dilation to convert them back to large models with a unified size while retaining personalized information. The compression and dilation processes, transparent to clients, are tuned on the server using a small public dataset. We further propose a hierarchical and clustering-based local training strategy for enhanced performance. Extensive experiments on six datasets show that FedConv outperforms state-of-the-art FL systems in terms of model accuracy (by more than 35% on average), computation and communication overhead (with 33% and 25% reduction, respectively).}
}


@article{DBLP:journals/tmc/SongGZZQX25,
	author = {Fuyuan Song and
                  Yunlong Gao and
                  Mingyang Zhao and
                  Chuan Zhang and
                  Zheng Qin and
                  Bin Xiao},
	title = {High-Dimensional and Secure Spatial Keyword Query With Arbitrary Ranges
                  in Mobile Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12121--12136},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581562},
	doi = {10.1109/TMC.2025.3581562},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongGZZQX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial keyword query has emerged as a critical service in mobile cloud, enabling cloud servers to retrieve spatio-textual objects within a mobile user’s query range that contain specified query keywords. Numerous secure spatial keyword query schemes have been developed to enable geometric range queries and keyword searches on encrypted spatial data. However, spatial keyword queries are typically designed for searching high-dimensional spatial data across arbitrary geographic ranges. Most of them fail to handle arbitrary geometric range queries and efficient spatial keyword query over high-dimensional encrypted data. To address these issues, we propose a high-dimEnsional and Privacy-preserving Spatial Keyword Query (EPSKQ) scheme with arbitrary geometric ranges over encrypted spatial data, leveraging Hilbert curve encoding and Enhanced Matrix-based Inner Product Encryption (EMIPE). In EPSKQ, spatial locations and multi-keywords are encoded into compact vectors, and arbitrary geometric range queries are transformed into range intersection tests. To reduce computational overhead, we employ vector bucketing technique to partition large-size vectors into several small-size sub-vectors. Furthermore, we design a novel index structure called Hilbert Binary tree (HB-tree) to optimize range intersection tests. Based on HB-tree, we propose an enhanced spatial keyword query scheme, named EPSKQ+, which further improves query performance. Security analysis demonstrates that both EPSKQ and EPSKQ+ achieve semantic security against indistinguishability under chosen-plaintext attack (IND-CPA). Extensive experimental evaluations show that the proposed EPSKQ and EPSKQ+ schemes significantly outperform state-of-the-art schemes in terms of computational and communication costs, with EPSKQ+ being 9× and 3× faster than the state-of-the-art schemes in the index build and query phase, respectively.}
}


@article{DBLP:journals/tmc/PengLWL25,
	author = {Kai Peng and
                  Chengfang Ling and
                  Shangguang Wang and
                  Victor C. M. Leung},
	title = {An Online Computation Offloading Approach With Dual Stability Guarantee
                  for Heterogeneous Tasks in MEC-Enabled IIoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12137--12148},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581600},
	doi = {10.1109/TMC.2025.3581600},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of information and the continuous expansion of applications, Industrial Internet of Things (IIoT) is facing huge data processing and storage pressure. With mobile edge computing (MEC) technology, the computing power network connects the geographically distributed computing nodes and then coordinates the allocation and scheduling of resources, transmits data, and eventually relieves the pressure of the industrial site. However, the rigorous demands of IIoT for real-time and stability pose some daunting challenges. To this end, we propose an online computation offloading approach with dual stability guarantee, named OCODSG. Specifically, the Lyapunov function is used to optimize the stability of the virtual queue, and the system stability is optimized based on the network jitter measurement. Moreover, the Dueling Double Deep Q Network (D3QN) algorithm based on deep reinforcement learning (DRL) is used for model autonomous training, while Gaussian noise is added to the network parameter space to encourage exploration and enhance algorithm robustness. Finally, experimental results on both simulated and real datasets demonstrate that OCODSG improves service efficiency and system stability.}
}


@article{DBLP:journals/tmc/HeYZSML25,
	author = {Hengtao He and
                  Xianghao Yu and
                  Jun Zhang and
                  Shenghui Song and
                  Ross D. Murch and
                  Khaled B. Letaief},
	title = {Cell-Free Massive {MIMO} Detection: {A} Distributed Expectation Propagation
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12149--12161},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583019},
	doi = {10.1109/TMC.2025.3583019},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeYZSML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell-free massive MIMO is one of the core technologies for next-generation wireless networks. It is expected to bring enormous benefits, including ultra-high reliability, data throughput, energy efficiency, and uniform coverage. However, the radically distributed architecture of cell-free massive MIMO necessitates new paradigms for transceiver design, especially by exploiting efficient distributed processing algorithms. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO and THz ultra-massive MIMO systems, which consists of two modules: a nonlinear module at the central processing unit (CPU) and a linear module at each access point (AP). The turbo principle in iterative channel decoding is utilized to compute and pass the extrinsic information between the two modules. An analytical framework is provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Furthermore, a distributed iterative channel estimation and data detection (ICD) algorithm is developed to handle the practical scenario with imperfect channel state information (CSI). Simulation results will show that the proposed method outperforms existing detectors for cell-free massive MIMO systems in terms of the bit-error rate and the developed theoretical analysis can be utilized as an asymptotic lower bound. Finally, it is shown that with imperfect CSI, the proposed ICD algorithm can significantly improve the system performance and reduce the pilot overhead.}
}


@article{DBLP:journals/tmc/DengLLLX25,
	author = {Qingyong Deng and
                  Mengyao Li and
                  Zhetao Li and
                  Haolin Liu and
                  Yong Xie},
	title = {End-Edge Collaborative Optimization of Microservice Caching in D2D-Assisted
                  Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12162--12176},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582579},
	doi = {10.1109/TMC.2025.3582579},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DengLLLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Employing the caching resources of end users via Device-to-Device (D2D) communication to assist the edge server in microservice caching is promising to further alleviate the network congestion of the Internet of Things (IoT). However, significant extra energy consumption prevents the caching system from maximizing cache utility if all end users cache simultaneously. In this paper, we propose two novel end-edge collaborative microservice caching algorithms in D2D-assisted networks. First, we construct a D2D caching sharing link graph from the aspects of physical and social attributes of end users and introduce the Entropy-based Partitioning Around Medoid (EPAM) algorithm to identify critical users. Second, to address the challenges posed by unknown time-varying user preferences, we model the end-edge collaborative caching problem as a Multi-Agent Multi-Armed Bandit (MAMAB) problem, thus developing two caching decision schemes, i.e, Edge-Centric Scheme (ECS) and User-Centric Scheme (UCS), to accommodate different decision sequences. The simulation results show that the EPAM-ECS and EPAM-UCS have at least 29.2% and 39.3% improvement compared with other baseline algorithms.}
}


@article{DBLP:journals/tmc/ZhangZYWW25,
	author = {Yue Zhang and
                  Guopeng Zhang and
                  Kun Yang and
                  Yao Wen and
                  Kezhi Wang},
	title = {Enhancing Collaborative Machine Learning in Resource-Limited Networks
                  Through Knowledge Distillation and Over-the-Air Computation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12177--12192},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582683},
	doi = {10.1109/TMC.2025.3582683},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZYWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional collaborative machine learning (CML) faces significant challenges in resource-constrained environments, such as emergency scenarios with limited power, bandwidth, and computing resources, leading to increased communication delays and energy consumption. To address these issues, this paper introduces Air-CoKD, a novel CML framework designed to reduce resource consumption and training latency while preserving model performance. Air-CoKD leverages knowledge distillation (KD) to minimize data transmission by avoiding the direct sharing of model parameters. It also integrates over-the-air computation (AirComp) to aggregate local logits, optimizing bandwidth utilization. To address the dimensional differences in local logits caused by the unbalanced device data class, Air-CoKD employs orthogonal frequency division multiplexing (OFDM) to transmitting local logits for different target classes. To handle aggregation errors introduced by AirComp, we conduct a detailed analysis of error bounds. Specifically, we convert the Kullback–Leibler (KL) divergence, used in KD loss function, into a quadratic upper bound for precise error quantification and effective optimization. Based on these insights, we propose a strategy to manage bandwidth constraints, transmission power limits, and device energy budgets within Air-CoKD. Extensive simulations demonstrate that Air-CoKD surpasses state-of-the-art methods, effectively balancing training efficiency and model performance. The framework proves to be a robust solution for CML in resource-constrained networks.}
}


@article{DBLP:journals/tmc/YangLZQW25,
	author = {Haining Yang and
                  Jinlu Liu and
                  Pingyuan Zhang and
                  Jing Qin and
                  Huaxiong Wang},
	title = {Toward Efficient Verifiable Data Streaming Without Cryptographic Accumulator},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12193--12205},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582087},
	doi = {10.1109/TMC.2025.3582087},
	timestamp = {Tue, 14 Oct 2025 19:49:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangLZQW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Verifiable data streaming (VDS) enables the client to incrementally store a sequence of ordered data on an untrusted cloud server, and verify the validity of the retrieved data. Moreover, the client can replace a data with another value. The common security problem caused by updating operation is the cloud server may use old authentication information to make expired data pass the verification. To solve this problem, the known approaches use the cryptographic accumulator that actually influences the performance of VDS scheme. The main concerns can be generalized as how to design a VDS scheme without cryptographic accumulator, in such a way that further optimizes the performance of VDS scheme. We put forward the idea to convert the standard digital signature relevant to the updated data into chameleon digital signature whose non-transferability is the key to solve the problem. This is the first attempt to securely authenticate the dynamic data without cryptographic accumulator. In the proposed VDS scheme, the client’s local storage overhead, computation overheads of the cloud server in responding to a query and updating the data are constant. As the experimental results shown, the proposed VDS scheme outperforms the scheme in terms of the efficiency.}
}


@article{DBLP:journals/tmc/LiuWGGZWY25,
	author = {Sicong Liu and
                  Fengmin Wu and
                  Yuan Gao and
                  Bin Guo and
                  Zimu Zhou and
                  Hongkai Wen and
                  Zhiwen Yu},
	title = {AdaFlowLite: Scalable and Non-Blocking Inference on Asynchronous Mobile
                  Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12206--12220},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582060},
	doi = {10.1109/TMC.2025.3582060},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWGGZWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has driven the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival time of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slow data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to opportunistic inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a computational approach to control this affinity in open-world mobile environments.  A d a F l o w L i t e {\\sf AdaFlowLite}  pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an multi-modal lightweight Swin Transformer (MMLST),  A d a F l o w L i t e {\\sf AdaFlowLite}  facilitates real-time and flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that  A d a F l o w L i t e {\\sf AdaFlowLite}  significantly reduces inference latency by up to 80.4% and enhances accuracy by up to 62.1%, while achieving nearly a 50% reduction in energy consumption, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.}
}


@article{DBLP:journals/tmc/ChenYZKWN25,
	author = {Ruoyang Chen and
                  Changyan Yi and
                  Fuhui Zhou and
                  Jiawen Kang and
                  Yuan Wu and
                  Dusit Niyato},
	title = {Federated Digital Twin Construction via Distributed Sensing: {A} Game-Theoretic
                  Online Optimization With Overlapping Coalitions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12221--12238},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582755},
	doi = {10.1109/TMC.2025.3582755},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYZKWN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel federated framework for constructing the digital twin (DT) model, referring to a living and self-evolving visualization model empowered by artificial intelligence, enabled by distributed sensing under edge-cloud collaboration. In this framework, the DT model to be built at the cloud is regarded as a global one being split into and integrating from multiple functional components, i.e., partial-DTs, created at various edge servers (ESs) using feature data collected by associated sensors. Considering time-varying DT evolutions and heterogeneities among partial-DTs, we formulate an online problem that jointly and dynamically optimizes partial-DT assignments from the cloud to ESs, ES-sensor associations for partial-DT creation, and as well as computation and communication resource allocations for global-DT integration. The problem aims to maximize the constructed DT’s model quality while minimizing all induced costs, including energy consumption and configuration costs, in long runs. To this end, we first transform the original problem into an equivalent hierarchical game with an upper-layer two-sided matching game and a lower-layer overlapping coalition formation game. After analyzing these games in detail, we apply the Gale-Shapley algorithm and particularly develop a switch rules-based overlapping coalition formation algorithm to obtain short-term equilibria of upper-layer and lower-layer subgames, respectively. Then, we design a deep reinforcement learning-based solution, called DMO, to extend the result into a long-term equilibrium of the hierarchical game, thereby producing the solution to the original problem. Simulations show the effectiveness of the introduced framework, and demonstrate the superiority of the proposed solution over counterparts.}
}


@article{DBLP:journals/tmc/YangYSSJT25,
	author = {Yihuai Yang and
                  Bin Yang and
                  Shikai Shen and
                  Yumei She and
                  Xiaohong Jiang and
                  Tarik Taleb},
	title = {Covert Communications for Intelligent Reflecting Surface-Enabled {D2D}
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12239--12251},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583779},
	doi = {10.1109/TMC.2025.3583779},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangYSSJT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we explore covert communications in a device-to-device (D2D) network consisting of an intelligent reflecting surface (IRS), a base station, a cellular user, a D2D pair, and an adversary warden. With the help of the IRS, the D2D pair attempts to perform covert communication, while the warden also tries to detect the very existence of such a transmission. To investigate the covert performance under the scenario, we derive the detection error probability at Warden, the optimal detection threshold for minimizing the probability, and the transmission outage probabilities for D2D and cellular communications, respectively. We further jointly optimize the transmission powers of the cellular user and the D2D transmitter, the reflection phase shifts, and the amplitudes of the IRS reflecting elements to improve covert communication performance. Finally, we provide numerical results to reveal the impact of system parameters on the covert performance and also to exhibit the merits of IRS-enabled D2D networks for achieving covert communications.}
}


@article{DBLP:journals/tmc/WuSYCLW25,
	author = {Jun Wu and
                  Yuye Shi and
                  Weijie Yuan and
                  Qingqing Cheng and
                  Buyi Li and
                  Xinyuan Wei},
	title = {SDR-Empowered Environment Sensing Design and Experimental Validation
                  Using {OTFS-ISAC} Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12252--12263},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3581716},
	doi = {10.1109/TMC.2025.3581716},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuSYCLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the system design and experimental validation of integrated sensing and communication (ISAC) for environmental sensing, which is expected to be a critical enabler for next-generation wireless networks. We advocate exploiting orthogonal time frequency space (OTFS) modulation for its inherent sparsity and stability in delay-Doppler (DD) domain channels, facilitating a low-overhead environment sensing design. Moreover, a comprehensive environmental sensing framework is developed, encompassing DD domain channel estimation, target localization, and experimental validation. In particular, we first explore the OTFS channel estimation in the presence of fractional delay and Doppler shifts. Given the estimated parameters, we propose a three-ellipse positioning algorithm to localize the target’s position, followed by determining the mobile transmitter’s velocity. Additionally, to evaluate the performance of our proposed design, we conduct extensive simulations and experiments using a software-defined radio (SDR)-based platform with universal software radio peripheral (USRP). The experimental validations demonstrate that our proposed approach outperforms the benchmarks in terms of localization accuracy and velocity estimation, confirming its effectiveness in practical environmental sensing applications.}
}


@article{DBLP:journals/tmc/TianWZDLZ25,
	author = {Fengsen Tian and
                  Mingzi Wang and
                  Yu Zhang and
                  Guoqiang Deng and
                  Lingyu Liang and
                  Xinglin Zhang},
	title = {A Pricing Game for Federated Learning Supporting Lightweight Local
                  Model Training},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12264--12281},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583013},
	doi = {10.1109/TMC.2025.3583013},
	timestamp = {Tue, 20 Jan 2026 14:46:52 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TianWZDLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pervasive distribution of data across clients with privacy concerns and heterogeneous performance in edge networks presents a significant opportunity to enhance AI model performance. Federated learning (FL) enables a model owner (MO) to recruit these clients, offering compensation for their contributions, and to improve model quality by aggregating knowledge from their locally trained models. However, several challenges arise in this process. Clients may decline participation if they do not achieve positive utility. Moreover, due to constraints in memory, computing, and communication resources, some clients can only train lightweight models that represent partial versions of the global model. Importantly, the MO’s pricing for client contributions and the proportions of local model training are interdependent, collectively influencing client utilities and participation decisions. To address these challenges, we first model the utility functions of both the MO and the clients, accommodating the support for lightweight local models. We then formulate their interactions as a Stackelberg game and theoretically prove the existence of a Nash equilibrium. Based on this equilibrium, we derive optimal collaboration strategies for both the MO and the clients. Additionally, we design an efficient approximation algorithm to enable the MO to maximize its utility by selecting suitable clients to participate in FL. Finally, extensive experiments validate our theoretical findings, demonstrating the superior performance and effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/tmc/JinDHWLL25,
	author = {Yili Jin and
                  Xize Duan and
                  Kaiyuan Hu and
                  Fangxin Wang and
                  Xue (Steve) Liu and
                  Jiangchuan Liu},
	title = {Video Conferencing With Predictive Generation and Collaborative Computation
                  Across Mobile Headsets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12282--12296},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582284},
	doi = {10.1109/TMC.2025.3582284},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JinDHWLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual Reality (VR) has emerged as a transformative platform for remote collaboration, but its adoption for video conferencing is hindered by challenges related to facial expression reconstruction and computational resource constraints, especially on economical mobile VR headsets. This paper introduces a novel system for VR video conferencing that addresses these challenges through two key modules: Predictive Generation and Collaborative Computation. Predictive Generation leverages multimodal inputs, including voice, head motion, and eye blinks, to synthesize realistic facial animations with low latency, eliminating the need for high-precision hardware. Collaborative Computation enhances computational efficiency by employing a game-theoretic framework for resource sharing among users. Experimental evaluations demonstrate that our system delivers immersive and realistic VR video conferencing experiences with superior facial expression reconstruction and efficient resource utilization. Our approach makes VR video conferencing more accessible and practical for a broader audience across mobile headsets.}
}


@article{DBLP:journals/tmc/YangFWWY25,
	author = {Jian Yang and
                  Shuai Feng and
                  Yatong Wang and
                  Xinghang Wu and
                  Mu Yan},
	title = {OpenRFI: Open-Set Radio Frequency Fingerprint Identification via Test-Time
                  Fine-Tuning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12297--12313},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582980},
	doi = {10.1109/TMC.2025.3582980},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangFWWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of low-cost mobile edge devices, security and reliability have become crucial for mobile edge computing networks, especially for high-stakes applications. To facilitate it, Radio Frequency Fingerprint Identification (RFFI) has emerged as a promising physical layer security paradigm, offering a non-cryptographic and lightweight solution. However, most existing Deep Learning (DL) based RFFI methods operate under a closed-set assumption, limiting their ability to recognize devices not seen during training and posing a risk of misclassification. Addressing the open-set RFFI problem is critical for real-world deployments, where the system must handle both known and unknown devices, ensuring robust security in dynamic environments. In this paper, we propose OpenRFI, a novel test-time fine-tuning-based RFFI framework, consisting of two sequential stages: pre-training and test-time fine-tuning. During the pre-training stage, we design a data augmentation module, a feature extraction module, and an efficient hybrid loss function to minimize intra-class feature distances and tighten decision boundaries, enhancing the model’s ability to distinguish between different classes. In the test-time fine-tuning stage, we introduce a fine-tuning dataset construction module and a full-parameter fine-tuning module to dynamically adapt to the test environment and capture information from unknown samples, further improving open-set recognition. We theoretically establish the performance boundary of the fine-tuning dataset construction method, providing insights into its robustness and scalability. Extensive numerical results based on an open source dataset demonstrate the effectiveness of the proposed OpenRFI framework in comparison with existing baselines.}
}


@article{DBLP:journals/tmc/CuiTWJ25,
	author = {Hanshuai Cui and
                  Zhiqing Tang and
                  Yuan Wu and
                  Weijia Jia},
	title = {Layer-Aware Cost-Effective Container Updates With Edge-Cloud Collaboration
                  in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12314--12328},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583153},
	doi = {10.1109/TMC.2025.3583153},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CuiTWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containers have become popular for deploying applications in Edge Computing (EC) for their seamless integration and easy deployment. Frequent container updates are essential to enhance performance and introduce new challenges for cutting-edge applications such as large language models and digital twins. However, traditional container update methods result in substantial download costs and task interruptions, which are unacceptable for latency-sensitive tasks in resource-constrained EC. Existing work has largely overlooked the layered structure of container images. By leveraging this layered structure, duplicate downloads can be reduced, and various layers can be transferred from other edges, reducing burden on the remote cloud. In this paper, we model the layer-aware container update problem with edge-cloud collaboration to minimize update and scheduling costs. We present the Layer-aware Edge-cloud collaborative Container Update (LECU) algorithm based on reinforcement learning to make container update decisions. Moreover, a task scheduling algorithm is devised to schedule tasks affected by container updates to other edges, minimizing the impact of task interruptions. We implement our LECU algorithm on an edge system with real-world data traces to demonstrate its effectiveness and conduct larger-scale simulations to evaluate its scalability. Results demonstrate that our algorithms reduce container update and task scheduling costs by 14% and 19%, respectively, compared to baselines.}
}


@article{DBLP:journals/tmc/SunXYP25,
	author = {Lan Sun and
                  Songpengcheng Xia and
                  Jiarui Yang and
                  Ling Pei},
	title = {Suite-IN++: {A} FlexiWear BodyNet Integrating Global and Local Motion
                  Features From Apple Suite for Robust Inertial Navigation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12329--12344},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583055},
	doi = {10.1109/TMC.2025.3583055},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunXYP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.}
}


@article{DBLP:journals/tmc/MaoNHW25,
	author = {Changlin Mao and
                  Haocheng Ni and
                  Jianping Han and
                  Yingxiao Wu},
	title = {mmOrbit: Micrometer-Level Vibration and Rotor Orbit Measurement via
                  Synchronized Dual mmWave Radars},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12345--12358},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3582545},
	doi = {10.1109/TMC.2025.3582545},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaoNHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we introduce mmOrbit, a mmWave-based rotor orbit measurement system that can estimate rotor orbit by analyzing machinery surface vibration. To measure the 2D rotor orbit, two synchronized mmWave radars are deployed to build the orbit from different viewpoints. The existing literature shows that the micro-displacement measurement accuracy of mmWave radar is not enough to meet the application requirements of micron-level resolution without appropriate fine-gained processing methods. Therefore, we propose a three-step vibration displacement extraction algorithm with sliding table to extract mechanical vibration micro-displacement and increase measurement precision. Accurate mechanical vibration displacements in two vertical directions are used to estimate the 2D rotor orbit. Our extensive experiments indicate that mmOrbit can accurately measure mechanical vibration micro-displacement with an error of 4.4  μ m \\mu \\text{m}  for the  80 th 80\\text{th}  percentile. Furthermore, mmOrbit can estimate 2D rotor orbit with high precision, showing an orbit eccentricity error of 7%, an orbit direction error of  7 ∘ 7^\\circ , and a disjoint area proportion of 17% for the  80 th 80\\text{th}  percentile. The imbalance detection experiment verifies the accuracy and dependability of our measured rotor orbit.}
}


@article{DBLP:journals/tmc/WangXLLMZFC25,
	author = {Xiaojing Wang and
                  Binbin Xie and
                  Guanghui Lv and
                  Boyang Liu and
                  Chenhao Ma and
                  Renjie Zhao and
                  Chao Feng and
                  Xiaojiang Chen},
	title = {RISensing: Leveraging Reconfigurable Intelligent Surfaces to Empower
                  Wi-Fi Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12359--12374},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3583916},
	doi = {10.1109/TMC.2025.3583916},
	timestamp = {Mon, 20 Oct 2025 17:02:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangXLLMZFC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi technology has emerged as a promising solution for contact-free sensing owing to the pervasiveness of Wi-Fi signals in indoor environments. However, Wi-Fi sensing faces several fundamental issues, including limited sensing range and unstable orientation-dependent sensing performance, hindering the widespread adoption of Wi-Fi sensing in real-life scenarios. In this paper, we propose RISensing, a novel system that leverages Reconfigurable Intelligent Surfaces (RIS) to address these two fundamental issues of Wi-Fi sensing and bring Wi-Fi sensing one step closer to real-world adoption. Unlike prior Wi-Fi sensing works which typically rely on a single target reflection signal to capture the target movement, RISensing utilizes two target reflection signals, i.e., the direct target reflection signal and RIS-based target reflection signal, to boost the sensing capability. RISensing characterizes the RIS-based target reflection signal, and constructively combines it with the direct target reflection. We evaluate the sensing performance of RISensing in various environments, including corridor, office and lab. Extensive experiments demonstrate RISensing can improve the sensing range of Wi-Fi from 4 m to 23 m, and effectively mitigate the orientation-dependent issue.}
}


@article{DBLP:journals/tmc/ZouCWZHW25,
	author = {Yongpan Zou and
                  Weiyu Chen and
                  Yunshu Wang and
                  Canlin Zheng and
                  Wenfeng He and
                  Kaishun Wu},
	title = {UltraWrite: {A} Lightweight Continuous Gesture Input System With Ultrasonic
                  Signals on {COTS} Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12375--12390},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586259},
	doi = {10.1109/TMC.2025.3586259},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZouCWZHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advantages of device ubiquity, natural interaction and privacy preservation, acoustic-based gesture input has received widespread attention. Researchers have proposed various techniques for different applications. However, the existing work has shortcomings of heavy data-collection overhead, non-continuous input, and performance degradation in cross-user scenarios. To overcome these shortcomings, we propose  U l t r a W r i t e \\mathsf{UltraWrite} , an acoustic-based gesture input system that only needs extremely low data-collection overhead, supports continuous input, and achieves high cross-user recognition accuracy. The key idea of our solution is to synthesize training data of continuous gestures from isolated ones, build a lightweight continuous gesture recognition model based on connectionist temporal classification (CTC) mechanism, and design a novel decoupled model training strategy to improve its cross-user recognition capability. We have implemented prototype systems on commercial devices and conducted comprehensive experiments to evaluate their performance. The results show that  U l t r a W r i t e \\mathsf{UltraWrite}  achieves an average top-1 word accuracy of 99.3% and top-1 word error rate of 0.34%. In addition, we have also evaluated  U l t r a W r i t e ′ s \\mathsf{UltraWrite’s}  robustness to the sensing distance, angle, background noise, and device. The results reveal that  U l t r a W r i t e \\mathsf{UltraWrite}  possesses strong robustness to these factors.}
}


@article{DBLP:journals/tmc/ZhangQZJ25,
	author = {Songwei Zhang and
                  Tie Qiu and
                  Xiaobo Zhou and
                  Yusheng Ji},
	title = {A Quantum-Driven Efficient Learning Model for Enhancing Robustness
                  of IoT Topology},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12391--12405},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586749},
	doi = {10.1109/TMC.2025.3586749},
	timestamp = {Tue, 14 Oct 2025 19:49:06 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangQZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The robustness of Internet of Things (IoT) topologies measures a network structure’s tolerance to random failures, or attacks, which is crucial for stable network communication. Research on optimizing network topology robustness has shifted from empirical rules and heuristics to machine learning, which can extract the features of robust network from topology data, thereby reducing the complexity of traditional topology optimization. However, machine learning approaches typically require a large number of parameters, resulting in high costs associated with parameter tuning and inference. To address these issues, this paper combines parameterized quantum circuits, and proposes a Quantum-Driven efficient Learning Model (QDLM) for enhancing robustness of IoT topology. This model leverages quantum exponential states to significantly reduce the number of training parameters while preserving learning performance. For inputs, QDLM integrates arithmetic encoding and quantum state encoding based on topological adjacency matrix, reducing the number of neurons. In training phase, parameterized quantum rotation gates and controlled quantum gates are used to achieve efficient training. A quantum measurement method is designed to ensure the output topology is a connected graph with the required number of edges. Compared to existing topology learning models, QDLM achieves an order-of-magnitude reduction in training parameters while maintaining topology learning effectiveness.}
}


@article{DBLP:journals/tmc/JiaZLLM25,
	author = {Juncheng Jia and
                  Weipeng Zhu and
                  Bing Luo and
                  Xiaodong Lin and
                  Liuchen Ma},
	title = {Fed{\textdollar}n{\textdollar}nP: Federated Unlearning With Multiple
                  Client Set Partitions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12406--12423},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586441},
	doi = {10.1109/TMC.2025.3586441},
	timestamp = {Tue, 11 Nov 2025 11:35:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiaZLLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has garnered increased attention in the field of distributed machine learning and privacy computing. In the FL setup, effective and efficient unlearning algorithms are required to remove the impact of specific training data from the trained model, called federated unlearning. However, traditional machine unlearning algorithms face limitations in FL systems because the client data is private and even non-IID. In this paper, we propose a new federated unlearning algorithm called Fed n n P. Our approach involves dividing the client set into subsets using multiple different partitions. We then train constituent models for each client subset within these partitions using existing FL algorithms and aggregate the results of constituent models for predictions. With multiple partitions, Fed n n P limits the influence of the data to be erased within its belonging subsets, while it also improves the accuracy of the aggregated prediction. Based on the multiple-partition framework, we design partition creation methods to effectively enhance the prediction accuracy. Furthermore, we propose a cost reduction method to reduce the cost of training/retraining. Our extensive experiments on various datasets and model architectures demonstrate that Fed n n P improves prediction accuracy while well-controls the additional cost.}
}


@article{DBLP:journals/tmc/LiWWCZ25,
	author = {Shuyang Li and
                  Qiang Wu and
                  Ran Wang and
                  Long Chen and
                  Hongke Zhang},
	title = {Efficient Multipath Differential Routing and Traffic Scheduling in
                  Ultra-Dense {LEO} Satellite Networks: {A} {DRL} With Stackelberg Game
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12424--12440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586262},
	doi = {10.1109/TMC.2025.3586262},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWWCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth orbit satellite networks (LSNs) are envisioned as key enablers of 6G by offering ubiquitous, low-latency connectivity. Their mesh topology enables multipath differential routing, which improves bandwidth utilization and reduces transmission delay. However, the growing demand for data and the dynamic, self-organizing nature of LSNs pose significant challenges for joint multipath routing and traffic scheduling under strict latency and energy constraints. To address these challenges, this paper proposes a multipath routing optimization (MRO) and traffic scheduling method tailored for multipath differential routing. Specifically, a dynamic multi-attribute graph model is developed to precisely capture the dynamic properties of LSNs. Building on this model, a MRO algorithm, integrated with a Stackelberg game framework, is introduced. The MRO algorithm employs a decomposition-based approach to identify multiple optimal paths that minimize delay and energy consumption, while the Stackelberg game framework ensures efficient traffic distribution across these paths. Numerical results demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving cumulative reward improvements of 26.77% to 43.8% across four real-world network topologies and exhibiting better Pareto front coverage. Furthermore, by leveraging the rapid convergence properties of the Stackelberg game model, the proposed method enhances network throughput by 12% to 43% and reduces transmission time by 14% to 49%.}
}


@article{DBLP:journals/tmc/WeiDGLZ25,
	author = {Qingwen Wei and
                  Shuping Dang and
                  Zhihui Ge and
                  Xiangcheng Li and
                  Zhenrong Zhang},
	title = {Performance Analysis of Direct Acyclic Graph-Based Ledgers in Low-to-High
                  Load Regime},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12441--12455},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586668},
	doi = {10.1109/TMC.2025.3586668},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiDGLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Direct acyclic graph (DAG)-based ledgers and distributed consensus algorithms have been proposed for use in the Internet of Things (IoT). The DAG-based ledgers have many advantages over single-chain blockchains, such as low resource consumption, low transaction fee, high transaction throughput, and short confirmation delay. However, the scalability of the DAG consensus has not been comprehensively verified on a large scale. This paper explores the scalability of DAG consensus within the low-to-high load regime (L2HR) using the tangle model, where L2HR characterizes the transition from a phase of low network load to another phase of high network load. In particular, we determine the average number of tips in the tangle in L2HR when adopting the uniform random tip selection (URTS) and rigorously prove that using the tangle model, the average number of tips at the end of L2HR converges to a constant. We also analyze the probability that a transaction in L2HR becomes an abandoned tip, the approximate average time required for the network load to transition from low load regime (LR) to high load regime (HR), and the average time required for a tip being approved for the first time in L2HR. All analytics are verified by numerical simulations.}
}


@article{DBLP:journals/tmc/LinXFCL25,
	author = {Zhijian Lin and
                  Yang Xiao and
                  Yi Fang and
                  Hongbing Chen and
                  Xiaoqiang Lu},
	title = {HybridRDN: Delay-Optimal Computation Offloading for Autonomous Vehicle
                  Fleets Based on {RSMA}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12456--12470},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586638},
	doi = {10.1109/TMC.2025.3586638},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinXFCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate-splitting multiple access (RSMA), space division multiple access (SDMA), and non-orthogonal multiple access (NOMA) have gained significant popularity and are extensively utilized across various domains. However, it is still unclear whether hybrid RSMA-SDMA-NOMA (HybridRDN) would seamlessly combine the advantages of RSMA, SDMA, and NOMA to contribute to the computation offloading of autonomous vehicle systems. To address the above issue, this paper introduces a novel HybridRDN-assisted computation offloading fleet (COF) scheme tailored for autonomous vehicle systems. First, we propose a stochastic-geometry-aided method to model the offloading framework. Afterwards, the task vehicles (TVs) ingeniously employ the proposed HybridRDN scheme to offload tasks to the resource vehicles (RVs) in each COF to relieve their computational burden. Diverging from the sole optimization of the task segmentation ratio or the transmission rate, a joint optimization problem involving the transmission weighting factor, the HybridRDN precoding matrix, the common rate, and the task segmentation ratio, is formulated, which aims to minimize the average delay of the COF system while approaching the rate performance of the ideal HybridRDN. Furthermore, a delay-optimal alternating optimization algorithm (DOAOA) is developed to obtain the solution for the optimization problem. Experimental results validate the plausibility and superiority of the proposed framework compared to the state-of-the-art schemes.}
}


@article{DBLP:journals/tmc/WangDXLLC25,
	author = {Zixiao Wang and
                  Qi Dong and
                  Tianzhang Xing and
                  Zhidan Liu and
                  Zhenjiang Li and
                  Xiaojiang Chen},
	title = {Enabling Effective {OOD} Detection via Plug-and-Play Network for Mobile
                  Visual Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12471--12486},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586625},
	doi = {10.1109/TMC.2025.3586625},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangDXLLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile devices have increasingly integrated with numerous deep learning-based visual applications, such as object classification and recognition models. While these models perform well in controlled environments, their effectiveness declines in real-world environment due to out-of-distribution (OOD) data not seen during training. Existing methods for detecting OOD data often compromise normal data recognition and require extensive training on unattainable OOD data. To address these issues, we propose  P O D \\mathtt {POD} , a framework designed to enhance mobile visual applications by providing high-precision OOD detection without affecting original model performance. In the offline phase,  P O D \\mathtt {POD}  generates OOD detectors from any classification model by analyzing model’s neuron responses to various data types. In the online phase, it continuously adjusts decision boundaries by integrating results from both the original model and the detector. Evaluated on two public datasets and one self-collected dataset across various popular classification models,  P O D \\mathtt {POD}  significantly improves OOD detection performance while maintaining the accuracy of original models.}
}


@article{DBLP:journals/tmc/ZhangWLZL25,
	author = {Hefan Zhang and
                  Zhiyuan Wang and
                  Wenhao Lu and
                  Shan Zhang and
                  Hongbin Luo},
	title = {Source Routing for {LEO} Mega-Constellations Based on Bloom Filter},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12487--12504},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586626},
	doi = {10.1109/TMC.2025.3586626},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-earth-orbit (LEO) mega-constellations with inter-satellite links (ISLs) are becoming the Internet backbone in space. Satellites within LEO often need the capability to enforce data forwarding paths. For example, they may need to bypass the satellites over the untrusted areas for the data of mission-critical applications or minimize latency for the data of time-sensitive applications. However, typical source/segment routing techniques (e.g., SRv6) suffer from scalability issue, since they record source-route-style forwarding information via the list-based structure. This results in great payload and forwarding overhead. To overcome this drawback, we propose a source/segment routing architecture for LEO mega-constellations, which is named as Link-identified Routing (LiR). LiR leverages in-packet bloom filter (BF) to record source-route-style forwarding information. BF could efficiently record multiple elements via a probabilistic data structure, but overlooks the order of the encoded elements. To address this, LiR identifies each unidirectional ISL, and represents the path by encoding ISL identifiers into BF. We investigate how to optimize BF configuration and ISL encoding policy to address false positives caused by BF. We implement LiR in Linux kernel and develop a container-based emulator for performance evaluation. Results show that LiR significantly outperforms SRv6 in terms of packet forwarding and data delivery efficiency.}
}


@article{DBLP:journals/tmc/LiuYC25,
	author = {Kanghuai Liu and
                  Jihong Yu and
                  Lin Chen},
	title = {On Information Collection in Multi-Tagged {COTS} {RFID} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12505--12516},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586660},
	doi = {10.1109/TMC.2025.3586660},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of target object information collection in multi-tagged COTS RFID systems. Unlike its single-tagged peers, the multi-tagged COTS RFID scenario poses new challenges in devising information collection algorithms: 1) Tags attached to the same object carry identical information. Hence, reusing single-tagged information collection algorithms leads to unnecessary redundancy; 2) Multi-tagged RFID systems are often deployed in applications where tags are vulnerable to damage. Such faulty tags may severely degrade the performance of information collection; 3) Most state-of-the-art information collection algorithms rely heavily on the hashing operation that is not seamlessly supported by the C1G2 standard, rendering these solutions inefficient and impractical. To tackle these technical challenges, this paper makes three contributions. First, we develop an efficient and compact tag pseudo-ID design, enabling the reader to select a single tag from each target object to collect information with only one Select command. Second, we construct a robust fault-handling mechanism capable of recognizing faulty tags without executing the entire slot. Third, armed with the above two techniques, we develop a novel information collection algorithm by leveraging the functionality offered by C1G2 to optimize the information collection sequence, thus minimizing the overall execution time. Empirical experiments on a COTS RFID system prototype demonstrate that our algorithm outperforms the best existing solution by 35–50% on average.}
}


@article{DBLP:journals/tmc/SunZ25,
	author = {Haibin Sun and
                  Yongzheng Zhang},
	title = {{RF-DEGO:} {A} Range Free Localization Algorithm for Non Uniform Node
                  Distributions and Obstacle Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12517--12532},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586636},
	doi = {10.1109/TMC.2025.3586636},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range-free localization algorithms have attracted considerable attention for outdoor wireless sensor network (WSN) positioning because they are less susceptible to environmental factors when estimating inter node distances and require only a few beacon nodes with known locations to rapidly determine all node positions. Among these, the connectivity based DV Hop algorithm has become widely used due to its simplicity and ease of implementation. However, its localization accuracy is limited and it is easily degraded by non uniform node distributions and obstacle environments. To address these shortcomings, this paper proposes a novel range free localization algorithm (RF-DEGO). First, a new distance estimation formula is derived from node connectivity and the probability distribution of distances. Next, the estimated distances are corrected using the local node density along communication paths, and paths identified as detouring around obstacles receive a further correction. Finally, an enhanced hierarchical Grey Wolf Optimization algorithm computes the node positions. Extensive simulation experiments under various network scenarios and parameter settings show that the proposed algorithm outperforms several existing localization methods in both accuracy and computation time, demonstrating superior overall performance and strong competitiveness.}
}


@article{DBLP:journals/tmc/LiuLXXLQ25,
	author = {Jun Liu and
                  Yunming Liao and
                  Hongli Xu and
                  Yang Xu and
                  Jianchun Liu and
                  Chen Qian},
	title = {Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12533--12549},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586644},
	doi = {10.1109/TMC.2025.3586644},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLXXLQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA)1, but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8× and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.}
}


@article{DBLP:journals/tmc/WangWHD25,
	author = {Yundi Wang and
                  Xiaoyu Wang and
                  He Huang and
                  Haipeng Dai},
	title = {Practical Optimizing {UAV} Trajectory in Wireless Charging Networks:
                  An Approximated Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12550--12566},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586457},
	doi = {10.1109/TMC.2025.3586457},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWHD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicles (UAVs) can be easily deployed as auxiliary base stations due to their convenience and flexibility. However, limited battery capacity becomes a bottleneck. Promising wireless power transfer (WPT) technologies can provide a continuous power supply for UAVs. Many of the recent works treat the UAV battery capacity as a constraint, which hinders the assurance of continuous UAV operation. Furthermore, most studies employ intelligent path-planning algorithms that lack explicit performance guarantees. In this paper, we study the problem of  P ractical  O ptimizing UAV  T rajectory in  W ireless  C harging  N etworks (POTWCN), which involves planning the trajectory of the wireless-powered UAV in the practical environment with obstacles by selecting candidate passing positions and determining the access order in the charging network. The goal is to maximize the benefit, i.e., balancing the total task completion time and the number of charging stations visited, so as to minimize path length and flight time, and ensure energy constraints with performance bound. To solve this problem, we first formalize the problem and prove its submodularity. Then, we propose the obstacle-aware weighted graph generation algorithm (OWGGA) to deal with the obstacles in the environment, which forms an obstacle-avoidance path using tangents and arcs between two hovering positions and the blocking obstacles. Next, we propose a dynamic charging station selection algorithm (ACSA), which maximizes the UAV’s energy utilization by limiting the number of charging stations that can be included. In the algorithm, we introduce the Christofides algorithm and use the path length calculated by OWGGA as the edge weights of the graph. Subsequently, considering the UAV’s energy constraints, we iteratively solve the UAV trajectory planning problem by adding the charging station with a maximized marginal benefit to the path. We prove that the proposed algorithm achieves an approximation ratio $1 - 1/e$ as well as the path length is at most $3\\pi /4$ times the optimal solution. Simulation results show that our algorithm reduces the flight distance by 38.01% and the task completion time by 34.00% on average.}
}


@article{DBLP:journals/tmc/ZhanDDHLCFH25,
	author = {Zijun Zhan and
                  Yaxian Dong and
                  Daniel Mawunyo Doe and
                  Yuqing Hu and
                  Shuai Li and
                  Shaohua Cao and
                  Lei Fan and
                  Zhu Han},
	title = {Distributionally Robust Contract Theory for Edge {AIGC} Services in
                  Teleoperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12567--12579},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586606},
	doi = {10.1109/TMC.2025.3586606},
	timestamp = {Wed, 10 Dec 2025 08:08:42 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanDDHLCFH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, we employ contract theory to model information asymmetry while utilizing DRO to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7% to 10.74% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory.}
}


@article{DBLP:journals/tmc/YangWXWZ25,
	author = {Dingcheng Yang and
                  Kangqing Wu and
                  Yu Xu and
                  Fahui Wu and
                  Tiankui Zhang},
	title = {Achievable Rate Maximization for Multi-IRS Assisted {AAV-NOMA} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12580--12594},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586768},
	doi = {10.1109/TMC.2025.3586768},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangWXWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution towards Internet of Things (IoT) in the forthcoming sixth generation (6G) is facing massive amounts of transmitted data and harsh wireless transmission environment, which severely degrade the quality of communication. To overcome these difficulties, a novel multiple intelligent reflecting surfaces (IRSs) assisted autonomous aerial vehicle (AAV) network framework with non-orthogonal multiple access (NOMA) is proposed in this article, where the AAV applies the NOMA scheme to deliver the information to the ground users assisted by multiple IRSs. We aim to maximize the achievable rate of the considered network while guaranteeing the minimum communication rate of each user, by jointly optimizing the multi-IRS phase shifts, AAV transmit power, AAV trajectory, and NOMA decoding order. To handle the coupled variables and integer constraints, we decompose the original problem into three subproblems based on the block coordinate descent (BCD) framework. Specifically, we first obtain the multi-IRS phase shifts by applying the semidefinite relaxation (SDR) technique. Next, the AAV transmit power allocation is derived by exploiting the concave convex procedure (CCCP) method. The AAV trajectory and NOMA decoding order are finally obtained by invoking the penalty-based method and the successive convex approximation (SCA) technique. Based on these, an alternating optimization algorithm is proposed. The numerical results show that: 1) the NOMA scheme enhances the utilization of the spectrum and enhances the access capacity of the communication system; 2) the multi-IRS cooperative structure increases the reflective channels and effectively improves the air-ground transmission environment, thus enhancing the system achievable rate; 3) the proposed multi-IRS assisted AAV NOMA algorithm achieves a significant network rate improvement compared to other benchmark schemes.}
}


@article{DBLP:journals/tmc/LiuZLX25,
	author = {Linfeng Liu and
                  Wenzhe Zhang and
                  Xingyu Li and
                  Jia Xu},
	title = {On the Robust Topology Recovery of {UAV} Swarm for Detection and Localization
                  of Electronic Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12595--12610},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586447},
	doi = {10.1109/TMC.2025.3586447},
	timestamp = {Tue, 14 Oct 2025 19:49:05 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {At present, Unmanned Aerial Vehicle (UAV) swarm has been extensively applied in various fields. In the application of detection and localization of electronic signals, some UAVs could become disabled due to some abnormal events (e.g. electromagnetic interference and battery electricity exhaustion), and the topology connectivity of UAV swarm could be impaired, i.e., the topology of UAV swarm could be partitioned. For the topology recovery issue, we first propose Robust Topology Recovery Algorithm of UAV swarm (RTRA) to recover the topology connectivity of UAV swarm and enhance the topology robustness (reduce the number of potential topology recoveries in future) by relocating some UAVs to new positions with shortest flight distance. Furthermore, we note that the relocated UAVs are easy to exhaust the battery electricity and fail due to the extra flight movements for the topology recoveries, which affects the topology robustness. To this end, we present Cascading Robust Recovery Topology Algorithm of UAV swarm (CRTRA), which adopts a cascading movement strategy to share the flight movements among multiply relocated UAVs, thus avoiding the battery electricity exhaustion of the relocated UAVs. Extensive simulations and comparisons demonstrate that our proposed CRTRA can effectively recover the topology connectivity of UAV swarm while enhancing the topology robustness and shortening the flight distance of relocated UAVs, and CRTRA is especially suitable for some missions such as the detection and localization of electronic signals where UAVs are prone to fail.}
}


@article{DBLP:journals/tmc/HanHDZYT25,
	author = {Weijia Han and
                  Chuan Huang and
                  Yanjie Dong and
                  Yangyingzi Zhang and
                  Yuxiang Yue and
                  Wei Teng},
	title = {A Novel Information-Theoretical Framework for Quantifying Coding Performance
                  in Scalable Mobile Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12611--12625},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586587},
	doi = {10.1109/TMC.2025.3586587},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanHDZYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, scalable video coding (SVC) has gained significant recognition in mobile video streaming because it can adapt bitstreams to time-varying transmission conditions. However, the coding performance of SVC, which is determined by its coding structure, has not been thoroughly studied. To address this issue, we propose analyzing the redundancy, reduction, distortion, and mutuality of video information within the video coding processes. This analysis facilitates the development of a novel information-theoretical framework for quantifying coding performance, which includes an information theory (IT)-based quantification method and a graphical representation system. The representation system accurately delineates the coding reference structure for encoding each video frame, while the proposed method utilizes mutual information to quantify the achievable coding performance of SVC under the delineated structure. To demonstrate the significance of our research, we apply the proposed framework to encode a basic coding unit, showcasing its effectiveness in improving SVC schemes. Consequently, our framework not only provides an efficient approach for quantifying the coding performance of SVC but also serves as an invaluable tool for optimizing SVC in various applications.}
}


@article{DBLP:journals/tmc/HeXC25,
	author = {Tianlang He and
                  Zhiqiu Xia and
                  S.{-}H. Gary Chan},
	title = {Elevator, Escalator, or Neither? Classifying Conveyor State Using
                  Smartphone Under Arbitrary Pedestrian Behavior},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12626--12639},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586618},
	doi = {10.1109/TMC.2025.3586618},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowing a pedestrian’s conveyor state of “elevator,” “escalator,” or “neither” is fundamental to many applications such as indoor navigation and people flow management. Previous studies on classifying the conveyor state often rely on specially designed body-worn sensors or make strong assumptions on pedestrian behaviors, which greatly strangles their deployability. To overcome this, we study the classification problem under arbitrary pedestrian behaviors using the inertial navigation system (INS) of the commonly available smartphones (including accelerometer, gyroscope, and magnetometer). This problem is challenging, because the INS signals of the conveyor states are entangled by the arbitrary and diverse pedestrian behaviors. We propose ELESON, a novel and lightweight deep-learning approach that uses phone INS to classify a pedestrian to elevator, escalator, or neither. Using causal decomposition and adversarial learning, ELESON extracts the motion and magnetic features of conveyor state independent of pedestrian behavior, based on which it estimates the state confidence by means of an evidential classifier. We curate a large and diverse dataset with 36,420 instances of pedestrians randomly taking elevators and escalators under arbitrary unknown behaviors. Our extensive experiments show that ELESON is robust against pedestrian behavior, achieving a high accuracy of over 0.9 in F1 score, strong confidence discriminability of 0.81 in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements fit for common smartphone deployment.}
}


@article{DBLP:journals/tmc/ZhaoZZJCWLZTH25,
	author = {Qinglin Zhao and
                  Lixin Zhang and
                  Haojie Zhang and
                  Hua Jiang and
                  Kunbo Cui and
                  Zhongqing Wu and
                  Jingyu Liu and
                  Mingqi Zhao and
                  Fuze Tian and
                  Bin Hu},
	title = {{LSNN} Model: {A} Lightweight Spiking Neural Network-Based Depression
                  Classification Model for Wearable {EEG} Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12640--12654},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586591},
	doi = {10.1109/TMC.2025.3586591},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZZJCWLZTH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Depression detection via wearable Electroencephalogram (EEG) sensor-assisted diagnosis system demands computationally efficient models compatible with resource-constrained edge devices. Spiking Neural Networks (SNNs) offer inherent advantages for processing the spatio-temporal patterns of EEG through event-driven neuromorphic computing. In this study, we innovatively present LSNNet, a lightweight SNN model specifically designed for wearable EEG sensors. The model exhibits low computational complexity with 7.18 K parameters and 67.68 M Floating-Point Operations (FLOPs). It requires only 246.88 KB of Random Access Memory (RAM) and 57.33 KB of Read-Only Memory (ROM) for on-board execution, and has been validated on both the single-core STM32U535CET6 and the multi-core GAP8 microcontrollers. Despite its minimal computational and memory requirements, LSNNet achieves impressive performance metrics, with a classification accuracy of 89.2%, specificity of 92.4%, and sensitivity of 86.4% in independent tests conducted on EEG data collected from 73 depressed patients and 108 healthy controls using our three-lead EEG sensor. Especially, when running on the GAP8 microcontroller, the LSNNet model has a low power consumption of 21.43 mW and a satisfactory inference time of 0.63 s while maintaining a classification accuracy of 87.5% (only with a reduction of 1.98% ). These results underscore the potential of integrating wearable EEG sensors with the LSNNet model for depression detection in the Internet of Things (IoT) era.}
}


@article{DBLP:journals/tmc/LvNCJWC25,
	author = {Chengfei Lv and
                  Chaoyue Niu and
                  Yu Cai and
                  Xiaotang Jiang and
                  Fan Wu and
                  Guihai Chen},
	title = {ARSys: An Efficient and Cross-Platform Development, Deployment, and
                  Runtime System for Mobile Augmented Reality},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12655--12671},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586797},
	doi = {10.1109/TMC.2025.3586797},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LvNCJWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Augmented reality (AR) offers users immersive experiences to interact with digital contents in their physical space. However, practical AR applications are challenged by the tight coupling of algorithm and engineering during the development and deployment phases as well as the execution requirements of hybrid AR subtasks on heterogeneous and resource-constraint mobile devices. In this work, we build an end-to-end, cross-platform, and efficient AR system, called ARSys. The infrastructure in ARSys adopts the new principle of integrated design, unifies and refines AR fundamental capabilities, supports streaming media processing, model inference, and real-time rendering by exposing high-performance tensor compute engine to top, and constructs a Python multi-instance virtual machine as the cross-platform AR task execution container. The runtime mechanism of ARSys schedules AR tasks in a pipeline parallelism way and allocates subtasks to hardware backends by optimizing the slowest node. The development workbench and the deployment platform in ARSys allow the decoupling of algorithms written in Python from engineering components in C/C++ and further support remote debugging and quick validation of AR algorithms. We extensively evaluate ARSys in practical AR applications across high-end, mid-end, and low-end Android and iOS devices, demonstrating higher development, deployment, and runtime efficiency than existing MediaPipe-oriented framework. ARSys has been integrated into Mobile Taobao for production use.}
}


@article{DBLP:journals/tmc/GaoWYY25,
	author = {Zhen Gao and
                  Gang Wang and
                  Lei Yang and
                  Chenhao Ying},
	title = {{CSMAAC:} Multi-Agent Reinforcement Learning Based Flight Control
                  in Partially Observable Multi-UAV Assisted Crowd Sensing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12672--12691},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586429},
	doi = {10.1109/TMC.2025.3586429},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoWYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile crowd sensing systems, existing flight control methods enable uncrewed aerial vehicles (UAVs) to provide high-quality data collection services for various applications. However, due to limited communication range, UAVs typically collect data under partial observability, hindering optimal performance without global environmental information. Additionally, many methods fail to enforce critical safety constraints. This paper proposes a communication-assisted safe multi-agent actor-critic-based UAV flight control method (CSMAAC). First, we propose an independent prediction communication partner model to address the partial observability problem. Based on the UAV’s local observation, causal inference is used to obtain prior communication information between UAVs through a feed-forward neural network to help UAVs determine potential communication partners. Second, we utilize a critic-network to predict and quantify inter-UAV influence and determine the necessity of communication. By exchanging necessary information inter-UAV, UAVs can perceive global information, thereby solving the UAV’s partial observability problem and reducing communication overhead. Moreover, we propose a similarity enhancement mechanism to improve the learning efficiency of the model by enhancing the connection between UAV observations and the policies of other UAVs. Finally, we introduce a safety layer to Actor-Network to ensure safe UAV flight. The simulation results show that the proposed method outperforms the baselines.}
}


@article{DBLP:journals/tmc/WuZJ25,
	author = {Haixing Wu and
                  Jiameng Zheng and
                  Shunfu Jin},
	title = {Adaptive Computation Offloading Scheme Based on a Collaborative Architecture
                  With Heterogeneous {MEC} Nodes: {A} {DRL} Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12692--12710},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586623},
	doi = {10.1109/TMC.2025.3586623},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has become an effective paradigm to support computation-intensive applications by providing services in close proximity to user devices (UDs). In MEC networks, computation offloading technology is devoted to balancing system load and prolonging UDs’ battery life. However, most existing studies on computation offloading take the impractical assumption of the MEC scenario with homogeneous users, ignoring security requirement from certain users. Moreover, with users mobility and task arrivals correlation, most existing computing offloading approaches suffer from inefficient or suboptimal decision making in practical MEC environments. To tackle these issues, by integrating task arrivals correlation within a time slot and environment dynamics between time slots, we propose an adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes. First, considering additional security requirement from very important people (VIP) users, we present a novel collaborative architecture by separating edge/cloud servers into public and private nodes. Then, with the architecture, we develop a dynamic computation offloading (DCO) algorithm to realize adaptive computation offloading scheme in MEC environment with mobile users. Particularly, the algorithm involves three stages. 1) By extending Poisson process into Markovian arrival process (MAP), we construct an MAP-based system model to capture the behavior of time-dependent task arrivals and then analyze the system model to derive the system delay in steady state. 2) For the purpose of minimizing the system delay in each time slot, we formulate a computation offloading problem in MEC environment with mobile users. 3) Under a deep reinforcement learning (DRL) framework, by taking the system delay as environmental feedback, we solve the formulated problem and provide offloading decisions in each time slot. We evaluate the performance of DCO algorithm by comparing it with other benchmark algorithms in various application scenarios. Results demonstrate that the proposed DCO algorithm outperforms the compared algorithms in response performance.}
}


@article{DBLP:journals/tmc/ZhangGLCZL25,
	author = {Yiyi Zhang and
                  Peng Guo and
                  Xuefeng Liu and
                  Chao Cai and
                  Kui Zhang and
                  Jiang Liu},
	title = {Reducing Transmission Cost of Distributed Principal Components Analysis
                  in Wireless Networks With Accuracy Guaranteed},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12711--12725},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586615},
	doi = {10.1109/TMC.2025.3586615},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGLCZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a classic data processing tool, Principal Component Analysis (PCA) has been widely applied in various data analysis applications. To mitigate the high computational complexity of PCA on Big Data, distributed PCA methods have been extensively studied, which disperse the computational tasks across multiple computation units while guaranteeing the accuracy. For the scenarios of distributed PCA in wireless networks, as the data is originally dispersed across different locations, it is further required to reduce the communication cost of distributed PCA in networks, which however has been seldom studied. Reducing the communication cost of distributed PCA in wireless networks requires not only appropriately partitioning the computation of PCA, ensuring accuracy, but also effectively assigning the partitioned computations and routing strategies to the nodes. In this paper, we propose CD-PCA, a communication-efficient distributed PCA (CD-PCA) scheme. This scheme implements a transmission-benefit equipartition strategy for the network to facilitate high-accuracy distributed computation and designs novel routing strategies for nodes to execute the distributed PCA within each partitioned region. Extensive simulation results demonstrate that the proposed CD-PCA scheme can reduce transmission costs by over 30% on average compared to related methods and baseline approaches.}
}


@article{DBLP:journals/tmc/HuangLM25,
	author = {Hebin Huang and
                  Junbin Liang and
                  Geyong Min},
	title = {Joint {DNN} Model Deployment, Selection, and Configuration for Heterogeneous
                  Inference Services Toward Edge Intelligence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {11},
	pages = {12726--12741},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586793},
	doi = {10.1109/TMC.2025.3586793},
	timestamp = {Wed, 15 Oct 2025 19:22:24 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge intelligence is an emerging paradigm in edge computing that deploys Deep Neural Network (DNN) models on edge servers with limited storage and computation capacities to provide inference services for high mobility and real-time applications, such as autonomous driving or smart surveillance, with varying accuracy and delay requirements. Adapting application configurations (e.g., image resolution or video frame rate) while selecting different DNN models and deployment locations can provide high-accuracy, low-delay inference services that meet user requirements. However, the configurations and DNN models of various inference services are highly heterogeneous. As balancing inference accuracy, resource cost, and delay is a multi-objective programming problem, it is a great challenge to obtain the optimal solution. To address this challenge, we propose a novel online framework to jointly optimize the configuration adaption, DNN model selection, and deployment for heterogeneous inference services. Specifically, we first formulate this joint optimization problem as an integer linear programming problem and prove it is NP-hard. Then, we further model the problem as a Partial Observable Markov Decision Process (POMDP) and solve it by developing a Heterogeneous-Agent Reinforcement Learning (HARL) based algorithm, named Heterogeneous Inference Service ProvidER (HISPER). It allows agents to have different action spaces corresponding to different types of configurations and DNN models. Finally, extensive experiments demonstrate that the proposed algorithm outperforms other state-of-the-art counterparts.}
}


@article{DBLP:journals/tmc/YunKLL25,
	author = {Sinwoong Yun and
                  Dongsun Kim and
                  Sungjin Lee and
                  Jemin Lee},
	title = {i-CU: Intelligent Cache Replacement and Content Update for Data Freshness
                  in Cloud-Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12742--12755},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3589609},
	doi = {10.1109/TMC.2025.3589609},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YunKLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the demand on time-sensitive contents increases, data freshness recently becomes an important performance metric in the cache-enabled networks. Therefore, in this paper, we design the joint cache replacement and content update algorithm in the cloud-edge networks considering the data freshness at both the cloud server and the edge server. We define a fresh content acquisition with cache hit (FACH) ratio as a performance metric, which shows the portion of users obtaining the requesting content from the edge server while satisfying the freshness constraint. To maximize the FACH ratio, we propose the reinforcement learning (RL)-based algorithm, named the intelligent Cache replacement and content Update(i-CU) algorithm. In the proposed algorithm, we newly suggest the score-based action decision to reduce the action space while guaranteeing the constraints of the problem. In the simulation results, we develop and evaluate the i-CU algorithm for various datasets, which verifies that the i-CU algorithm can achieve the higher FACH ratio compared to the existing baselines under the various network parameters.}
}


@article{DBLP:journals/tmc/KangLGWHZYZNZ25,
	author = {Jiawen Kang and
                  Jiana Liao and
                  Runquan Gao and
                  Jinbo Wen and
                  Huawei Huang and
                  Maomao Zhang and
                  Changyan Yi and
                  Tao Zhang and
                  Dusit Niyato and
                  Zibin Zheng},
	title = {Efficient and Trustworthy Block Propagation for Blockchain-Enabled
                  Mobile Embodied {AI} Networks: {A} Graph Resfusion Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12756--12770},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587006},
	doi = {10.1109/TMC.2025.3587006},
	timestamp = {Mon, 12 Jan 2026 19:08:43 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KangLGWHZYZNZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By synergistically integrating mobile networks and embodied artificial intelligence (AI), mobile embodied AI networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of messages or vulnerability to malicious tampering, potentially causing severe accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the validator trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.}
}


@article{DBLP:journals/tmc/PetrucciZPBB25,
	author = {Luca Petrucci and
                  Samuele Zanini and
                  Ivan Palam{\`{a}} and
                  Nicola Blefari{-}Melazzi and
                  Stefania Bartoletti},
	title = {Localization in 5G and Beyond: {A} Multi-Objective Approach for Accuracy,
                  Latency, and Resilience},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12771--12783},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588712},
	doi = {10.1109/TMC.2025.3588712},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PetrucciZPBB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of localization capabilities within the cellular architecture through dedicated 5G network functions has notably enhanced cellular positioning accuracy and enabled new location-based services. However, this architectural shift requires placing measurement acquisition and computation at the network edge and core, resulting in distributed computational resources and increased latency and security risks. As a result, minimizing latency, ensuring resilience against security threats, and achieving high accuracy become critical performance indicators for location-based services. This paper examines both 3GPP-standardized and O-RAN-based 5G architectures, detailing the key functions, interfaces, and parameters influencing the localization process, from measurement acquisition to position estimation. We define performance indicators to evaluate localization services and develop a system model that quantifies the costs associated with latency, accuracy, computation, and resilience to security threats. By jointly considering these factors, we formulate a multi-objective optimization problem that guides the selection of an optimal system configuration to simultaneously satisfy multiple localization requirements. We validate our approach through a case study of an end-to-end 5G system using both simulations and experimental data. Specifically, we evaluate various algorithms and implementations across standardized channels and scenarios. Furthermore, we conduct experimental measurements using Software-Defined Radios (SDRs) and open-source 5G platforms to assess operational latency with commercial-off-the-shelf (COTS) devices.}
}


@article{DBLP:journals/tmc/LiuWZQZWD25,
	author = {Zhicheng Liu and
                  Yilan Wang and
                  Yunfeng Zhao and
                  Chao Qiu and
                  Cheng Zhang and
                  Xiaofei Wang and
                  Mianxiong Dong},
	title = {Enabling Real-Time Video Detection With Adaptive and Distributed Scheduling
                  in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12784--12801},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588142},
	doi = {10.1109/TMC.2025.3588142},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWZQZWD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time video detection is essential for many mobile visual applications, which brings the heavy computational burden of deep neural networks. Mobile edge computing offers a promising solution by deploying computational resources near mobile devices. However, achieving efficient video detection on mobile devices requires addressing challenges such as different performance requirements, diverse computing and network conditions, and system dynamics. We propose a real-time video detection framework in mobile edge computing, where multiple video streams from mobile devices are processed while balancing key performance metrics with consideration of grouping. A joint optimization problem of task scheduling, model selection, and resource provisioning is formulated for the system, where decisions are made on two timescales. To this end, we propose a window controller to unify decision-making at the time-slot level. We design an online scheduling algorithm based on multi-agent deep reinforcement learning to enable adaptive and distributed scheduling, while a masking-enhanced attention mechanism enables efficient explicit information exchange between mobile devices. Experimental evaluations across different numbers of mobile devices demonstrate that, in terms of average reward, the proposed algorithm outperforms local processing by 14.600%, fixed offloading by 10.007%, and four learning-based scheduling baselines by an average of 2.267%.}
}


@article{DBLP:journals/tmc/ZhuSWZWXL25,
	author = {Weihao Zhu and
                  Long Shi and
                  Kang Wei and
                  Yipeng Zhou and
                  Zhe Wang and
                  Zehui Xiong and
                  Jun Li},
	title = {Randomized {DP-DFL:} Towards Differentially Private Decentralized
                  Federated Learning via Randomized Model Interaction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12802--12817},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588537},
	doi = {10.1109/TMC.2025.3588537},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuSWZWXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional federated learning (FL) frameworks rely on a central server for model coordination among distributed mobile terminals (MTs). The centralization faces two critical challenges, i.e., single point of failure and potential privacy leakage. Differentially private decentralized FL (DP-DFL) has been proposed to address these challenges, wherein the MTs exchange models in a decentralized manner and maintain the differential privacy (DP) guarantee by adding noise to local models before model interaction. However, existing DP-DFL frameworks confront difficulty in achieving the expected privacy and convergence performance, simultaneously. To address this issue, we propose a novel DP-DFL framework (called randomized DP-DFL) that employs a randomized model interaction scheme to lower the model exposure frequency and hence reduce privacy budget consumption. Specifically, the scheme includes two sequential steps, i.e., randomized terminal assignment and randomized model transmission. In Step 1), the model interaction phase of DFL is further divided into several sequential sub-stages. MTs are randomly assigned to each sub-stage. In Step 2), each MT sequentially transmits either a model previously received from its neighbors or its own local model according to the assigned sub-stage order. The proposed scheme enhances the MTs’ privacy of DFL since the exposure probabilities of the MTs’ local models are significantly reduced via these two randomized steps. Besides, we theoretically analyze the convergence and privacy performance of randomized DP-DFL. In particular, properly tuning the number of sub-stages in randomized DP-DFL can achieve an optimal balance between privacy and convergence. Experimental results show that randomized DP-DFL consistently outperforms traditional frameworks. Compared with baselines, randomized DP-DFL reduces 40.9% privacy loss under the same target accuracy while improving 9.5% learning accuracy under the same privacy loss on EMNIST and CIFAR-10, respectively.}
}


@article{DBLP:journals/tmc/ZhangLYWWF25,
	author = {Enqi Zhang and
                  Lei Liang and
                  Lizhao You and
                  Zhaorui Wang and
                  Deqing Wang and
                  Liqun Fu},
	title = {High-Rate Uncoordinated Concurrent Random Access in Underwater Acoustic
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12818--12832},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586911},
	doi = {10.1109/TMC.2025.3586911},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLYWWF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncoordinated random-access protocols are well-suited for underwater acoustic (UWA) networks due to their simplicity and low overhead. However, their performance is hindered by severe collisions and the challenging characteristics of UWA channels such as rich multipath and Doppler effect. Existing UWA physical layer waveforms struggle to resolve collisions while maintaining high data rates. This paper introduces ZCMod, a high-rate waveform allowing uncoordinated concurrent random access in UWA networks. ZCMod employs a Zadoff–Chu (ZC) sequence-based modulation that assigns unique ZC sequences to users to minimize inter-user interference and encodes multiple bits through cyclic shifts of the sequences to improve data rates. ZCMod further addresses the unique challenges of UWA channels via two new designs: 1) a shape-based demodulation approach that estimates the data-induced shift of channel response shape between the preamble and data symbols to handle rich multipath, and 2) an auxiliary modulation approach that modulates each data symbol with two ZC sequences, one for extracting current channel response shape and the other for data modulation, to handle the fast time-varying channel. Experimental results in a lake and a swimming pool and extensive simulation results show that a) ZCMod achieves around 100% higher throughput compared with the state-of-the-art (SOTA) approaches in quasi-static channels, and b) ZCMod maintains comparable throughput in fast time-varying channels as in quasi-static conditions, where the SOTA approaches experience significant degradation.}
}


@article{DBLP:journals/tmc/YangCD25,
	author = {Kang Yang and
                  Yuning Chen and
                  Wan Du},
	title = {Generative Diffusion Model-Assisted Efficient Fingerprinting for In-Orchard
                  Localization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12833--12851},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587206},
	doi = {10.1109/TMC.2025.3587206},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Precise robot localization at the tree level is essential for smart agriculture applications such as precision disease management and targeted nutrient distribution. Existing methods fail to achieve the required accuracy. We propose OrchLoc, a fingerprinting-based localization solution that achieves tree-level precision using a single Long Range (LoRa) gateway. Our approach utilizes channel state information (CSI) across eight channels as a localization fingerprint. To minimize labor-intensive site surveys for fingerprint database construction and maintenance, we develop a CSI generative model (CGM) that learns the relationship between CSI vectors and their corresponding locations. The CGM is fine-tuned using CSI data from static agricultural LoRa sensor nodes, enabling continuous fingerprint database updates. Extensive experiments in two orchards demonstrate that OrchLoc effectively achieves accurate tree-level localization with minimal overhead, improving robot navigation.}
}


@article{DBLP:journals/tmc/ZhangWWXYYC25,
	author = {Guanghui Zhang and
                  Ziming Wang and
                  Huaren Wei and
                  Mengbai Xiao and
                  Hui Yuan and
                  Dongxiao Yu and
                  Xiuzhen Cheng},
	title = {A Novel Spatial-Temporal Learning Method for Enhancing Generalization
                  in Adaptive Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12852--12866},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588135},
	doi = {10.1109/TMC.2025.3588135},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWXYYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive video streaming has become a fundamental technology for video delivery. With the rise of deep reinforcement learning (DRL), streaming vendors are increasingly adopting DRL-driven adaptive bitrate (ABR) algorithms. In real-world deployments, most ABR approaches are developed with the aim of maintaining good performance across a wide variety of network environments. However, contrary to this expectation, our empirical findings show that even when trained on extensive real-world network trace data, these DRL-based ABR algorithms achieve only 43.1% to 48.9% of Quality-of-Experience (QoE) under highly diverse network conditions, which falls significantly short of the 100% optimum. We termed this problem as “ABR Under-Generalization”. To overcome this problem, we introduce BETA – a novel DRL-based ABR framework that incorporates both spatial and temporal learning mechanisms: 1) Spatially, BETA features a detector that flags the network conditions likely to cause poor performance, then trains specialized ABR models tailored for those conditions and 2) Temporally, BETA enhances its learning by incorporating multi-step decision experiences at each training epoch, enabling the trained model to account for long-term environmental dynamics. Comprehensive evaluations show that BETA outperforms state-of-the-art ABR algorithms, yielding average QoE gains of 19.4% to 50.9%, and achieving improvements of up to 244.1% under severely fluctuating network conditions.}
}


@article{DBLP:journals/tmc/FanBLNZHH25,
	author = {Yanbo Fan and
                  Yuanguo Bi and
                  Yufei Liu and
                  Dusit Niyato and
                  Liang Zhao and
                  Qiang He and
                  Ammar Hawbani},
	title = {{GATO:} Global Transmission Optimization for SAGIN-Assisted IoRT Data
                  Collection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12867--12884},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587634},
	doi = {10.1109/TMC.2025.3587634},
	timestamp = {Fri, 26 Dec 2025 20:52:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanBLNZHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 6G networks, the increasing demand for global coverage has catalyzed a significant expansion in the deployment of the Internet of Remote Things (IoRT). Supported by Space-Air-Ground Integrated Networks (SAGIN), IoRT can efficiently transmit data from areas lacking infrastructure. In this paper, we explore a key yet challenging issue faced by SAGIN: can we exploit the mobility and cooperation of SAGIN heterogeneous nodes to enhance overall transmission performance in all-weather? We approach this issue in three steps. First, to ensure the all-weather transmission, we propose a layered SAGIN transmission design that combines the advantages of radio-frequency (RF) and free-space-optics (FSO) transmissions. Second, we formulate an overall transmission performance optimization problem. We aim to maximize the total upload data amount of SAGIN by jointly optimizing the satellite selection, the trajectories of high-altitude-platforms (HAPs) and uncrewed aerial vehicles (UAVs), and the UAVs’ transmit power. However, the formulated optimization problem falls into a mixed-integer nonlinear programming (MINLP), which is challenging to address. Third, to solve this problem, we propose a global transmission optimization (GATO) strategy by employing block coordinate descent and successive convex approximation technologies. Finally, extensive experimental results demonstrate that the proposed strategy can significantly improve SAGIN’s overall transmission performance.}
}


@article{DBLP:journals/tmc/HuangWWGW25,
	author = {Ziyao Huang and
                  Weiwei Wu and
                  Kui Wu and
                  Guanyu Gao and
                  Jianping Wang},
	title = {Minimizing Age of Semantic Information for Analytics-Oriented Video
                  Streaming Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12885--12902},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588474},
	doi = {10.1109/TMC.2025.3588474},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWWGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video streaming systems are critical for intelligent applications to transmit video data from end devices to servers for real-time analysis. In contrast to traditional human-centric streaming systems, which prioritize user-perceived metrics, machine-centric streaming systems are designed to continuously provide fresh and accurate information for analytics purposes. Although numerous studies have investigated policies to optimize streaming performance, most of them employ the segment-by-segment streaming framework from human-centric systems. Through comprehensive theoretical analysis and experimentation, we uncover that the segmented streaming approach is sub-optimal for machine-centric streaming systems compared to the straightforward frame-by-frame streaming approach. Furthermore, instead of relying on conventional frame-level metrics, we introduce a novel metric called the Age of Semantic Information (AoSI) to evaluate the performance of analytics-oriented streaming systems. This metric balances the quantity and timeliness of the semantic information. Consequently, we propose a compression ratio adaption method tailored to optimize AoSI performance for frame-by-frame streaming systems. This method leverages a deep learning (DL)-based predictor to discover the dynamic, latent relationships between compression and inference accuracy. Evaluated on actual streaming prototypes and real-world datasets, our method significantly surpasses both segmented and frame-by-frame baseline methods in terms of worst-case and average AoSI performance.}
}


@article{DBLP:journals/tmc/WangGYZMX25,
	author = {Shuo Wang and
                  Keke Gai and
                  Jing Yu and
                  Liehuang Zhu and
                  Weizhi Meng and
                  Bin Xiao},
	title = {{EASTER:} Embedding Aggregation-Based Heterogeneous Models Training
                  in Vertical Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12903--12917},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3589426},
	doi = {10.1109/TMC.2025.3589426},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangGYZMX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical Federated Learning (VFL) allows collaborative machine learning without sharing local data. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization of participants’ local knowledge aggregation. To address this challenge, this paper proposes a novel approach called Embedding Aggregation-based Heterogeneous Models Training in Vertical Federated Learning (EASTER). EASTER focuses on aggregating the local embeddings of each participant’s knowledge during forward propagation. We propose an embedding protection method based on lightweight blinding factors, which injects the blinding factors into the local embedding of the passive party. However, the passive party does not own the sample labels, so the local model’s gradient cannot be calculated locally. To overcome this limitation, we propose a new method in which the active party assists the passive party in computing its local heterogeneous model gradients. Theoretical analysis and extensive experiments demonstrate that EASTER can simultaneously train multiple heterogeneous models and outperform some recent methods in model performance. For example, compared with the state-of-the-art method, the model accuracy of EASTER was improved by 7.22% under the CIFAR-10 dataset.}
}


@article{DBLP:journals/tmc/SunYSJT25,
	author = {Ranran Sun and
                  Bin Yang and
                  Yulong Shen and
                  Xiaohong Jiang and
                  Tarik Taleb},
	title = {On Joint Covert and Secure Communications in D2D-Enabled Cellular
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12918--12934},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3589011},
	doi = {10.1109/TMC.2025.3589011},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunYSJT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper explores the joint covert and secure communications in a device-to-device (D2D)-enabled cellular system (DCS) consisting of a base station BS, an eavesdropper Eve, and two user equipments UE and UR. To conduct secure communications with UE against Eve, BS works either under the cellular mode using direct transmission or under the D2D mode replying through UR, while UR is greedy since it opportunistically transmits its own covert message to UE against the detection from BS. To understand the fundamental performance of secrecy rate and covert rate in DCS, we first develop theoretical models to depict the detection probability/secrecy rate of BS and covert rate of UR under different modes (i.e., underlay, overlay, or cellular). Based on these models, we further explore the secrecy rate maximization (SRM) for BS subject to the constraints of detection probability at BS and transmit power at both BS and UR, as well as the covert rate maximization (CRM) for UR subject to the constraints of covertness requirement and covert transmit power. Finally, we employ the Newton-based searching method to solve the SRM/CRM problems and illustrate via numerical results the achievable secrecy rate and covert rate of BS and UR under various DCS scenarios.}
}


@article{DBLP:journals/tmc/YangWCSJK25,
	author = {Peng Yang and
                  Ting Wang and
                  Haibin Cai and
                  Yuanming Shi and
                  Chunxiao Jiang and
                  Linling Kuang},
	title = {Brain-Inspired Decentralized Satellite Learning in Space Computing
                  Power Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12935--12949},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587796},
	doi = {10.1109/TMC.2025.3587796},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangWCSJK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring. However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data. To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on-board data processing. Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks. To tackle this issue, we propose to employ spiking neural networks (SNNs) for on-board data processing, which is supported by the neuromorphic computing architecture. The extreme sparsity in its computation enables a high energy efficiency. Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum. We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed. We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance. Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks.}
}


@article{DBLP:journals/tmc/NiSCZWXZW25,
	author = {Tao Ni and
                  Zehua Sun and
                  Yongliang Chen and
                  Yihe Zhou and
                  Jiayimei Wang and
                  Weitao Xu and
                  Qingchuan Zhao and
                  Cong Wang},
	title = {When Good Becomes Evil: Exploring Crosstalk Attack Surfaces on Multi-Port
                  {USB} Chargers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12950--12967},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587292},
	doi = {10.1109/TMC.2025.3587292},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NiSCZWXZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-port chargers, designed to simultaneously charge multiple mobile devices such as smartphones, have gained significant popularity, with millions of units sold in recent years. However, this multi-device charging feature introduces security and privacy risks. If not properly designed and implemented, these chargers can enable communication between connected devices because they are inherently interconnected, which leads to crosstalk voltage leakages. Despite their widespread use, these risks have not been thoroughly investigated. We have identified novel attack surfaces in the circuit design of multi-port chargers that allow an adversary who shares the multi-port charger with the target victim in close proximity to exploit one port to ($i$i) recognize fine-grained user activities of other devices being charged, ($ii$ii) eavesdrop on secret audio transmission from USB-C audio pins, and (iii) inject malicious audio commands into built-in voice assistants of charging devices (e.g., Siri, Google Assistant). In this paper, we design and implement XPorTheft, a novel system to analyze and demonstrate the uncovered security and privacy threats in multi-port chargers. Specifically, it leverages changes in voltage signals in one neighbor port to monitor voltage changes in the charging port induced by user activities in various user interfaces, such as recognizing running apps and detecting keystrokes. Moreover, XPorTheft can also achieve audio transmission eavesdropping and launch inaudible audio injection attacks from the neighbor port to the charging mobile device via the USB-C interface. We extensively evaluate the effectiveness of XPorTheft using five commercial multi-port chargers and five mobile devices. The evaluation results show its high effectiveness in recognizing the launch of 20 mobile apps (88.7%) and revealing unlocking passcodes (98.8%), as well as eavesdropping on the audios of numeric digits (97.1%) and alphabetic characters (98.0%). Furthermore, XPorTheft achieves 100% success rates in inaudible audio injection attacks on three commercial voice assistants. In addition, our study also shows that XPorTheft is resilient to various impact factors and presents the potential to attack multiple victims.}
}


@article{DBLP:journals/tmc/LiZLLL25,
	author = {Songfan Li and
                  Yanbo Zhang and
                  Jansen Christian Liando and
                  Li Lu and
                  Mo Li},
	title = {LoRaMirror: Illuminating Shadowed Spots in Urban {LPWAN} With Reflective
                  Smart Surfaces},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12968--12982},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587057},
	doi = {10.1109/TMC.2025.3587057},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZLLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of low-power wide-area networks (LPWAN) in urban environments faces a critical challenge with signal blockage caused by dense obstacles like buildings, resulting in shadowed spots where end nodes have difficulty reaching the gateway. This paper proposes LoRaMirror, a reflective smart surface design, to essentially eliminate these shadowed spots and improve overall communication in urban LoRaWAN. LoRaMirror is different from existing smart surface designs, as it addresses unprecedented challenges posed by LPWAN’s unique application scenario of extremely long communication distances, extremely low data transmission rates, and extremely wide coverage. LoRaMirror is prototyped with a 16-antenna multi-layer array and the experimental results show significant performance gains in real world practice.}
}


@article{DBLP:journals/tmc/XuGLZZDJCW25,
	author = {Changfu Xu and
                  Jianxiong Guo and
                  Yuzhu Liang and
                  Haodong Zou and
                  Jiandian Zeng and
                  Haipeng Dai and
                  Weijia Jia and
                  Jiannong Cao and
                  Tian Wang},
	title = {Enhancing QoE in Collaborative Edge Systems With Feedback Diffusion
                  Generative Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12983--12998},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587744},
	doi = {10.1109/TMC.2025.3587744},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuGLZZDJCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative edge computing is a promising approach for delivering low-delay services to computation-intensive Internet of Things applications. Deep Reinforcement Learning (DRL) has become an effective way to solve task scheduling decisions in edge systems due to its adaptive learning ability to interact with the environment. However, current DRL-based task scheduling methods still face several challenges, such as limited exploration, sample inefficiency, and performance instability, which can lead to degraded user Quality of Experience (QoE). To address these challenges, we observe that diffusion models, famous for their performance in image generation, exhibit strong exploration, data efficiency, and performance stability. This inspires us to propose FDEdge, a novel feedback diffusion generative scheduling method for enhancing user QoE in collaborative edge systems. We first design an innovative Feedback Diffusion (FDN) model by leveraging historical action probability information during the denoising process. We then incorporate the FDN model into DRL, forming an effective and efficient framework for task scheduling in collaborative edge systems. We also present a probability derivation to ensure the FDEdge’s rationality. Extensive experimental results demonstrate that our FDEdge method significantly reduces service delays by 45.42% to 87.57% and speeds up training episode durations by  2.5\\times 2.5\\times  times for a higher QoE than state-of-the-art methods.}
}


@article{DBLP:journals/tmc/ChenLFZL25,
	author = {Gong Chen and
                  Wenzhong Li and
                  Yuchu Fang and
                  Yi Zhang and
                  Sanglu Lu},
	title = {CrowdNet: Adaptive Collaborative Inference for Dynamic Mobile Intelligent
                  Service},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {12999--13014},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588100},
	doi = {10.1109/TMC.2025.3588100},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLFZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks (DNNs) such as convolutional networks and Transformers are increasingly deployed to provide intelligent service. However, enabling large-scale DNNs in an infrastructure-less mobile crowd encounters the challenges of high resource consumption, poor performance, and low service availability. This paper proposes CrowdNet, a novel device-to-device (D2D) collaborative inference framework for dynamic mobile environments. CrowdNet introduces a CellNet architecture as its core component, designed as lightweight DNNs ideal for deployment and operation on resource-constrained mobile devices. The CellNets can perform inference tasks either independently or collaboratively to ensure robustness against network disruptions. A topology expansion method is utilized to create an inference flow from the physical communication topology, enabling the distributed operation of inference tasks. To handle the dynamic participation of mobile devices, CrowdNet employs fine-tuning adaptation for flexible assembly and collaborative inference. A reinforcement learning (RL)-based approach is introduced to optimize inference topology. Trained with a multi-objective optimization strategy, CrowdNet can enhance overall performance while maintaining individual CellNet functionality. Extensive experiments based on mobile network testbed and real-world datasets validate the effectiveness of CrowdNet on various intelligent tasks, exhibiting remarkable performance gains and robustness compared to state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/FanLHHJQYCW25,
	author = {Xiubin Fan and
                  Zhongming Lin and
                  Yuming Hu and
                  Zhiqing Hong and
                  Tianrui Jiang and
                  Feng Qian and
                  Zhimeng Yin and
                  S.{-}H. Gary Chan and
                  Dapeng Oliver Wu},
	title = {DeMo: Experiences of Deploying a Large-Scale Indoor Delivery Monitoring
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13015--13033},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588900},
	doi = {10.1109/TMC.2025.3588900},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanLHHJQYCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The delivery of goods to numerous indoor stores poses significant safety risks, with heavy, high-stacked packages on delivery trolleys posing a potential hazard to passersby. This paper reports our experiences of developing and operating DeMo, a practical system for real-time monitoring of indoor delivery. DeMo employs sensors attached to trolleys, utilizing Inertial Measurement Unit (IMU) and Bluetooth Low Energy (BLE) readings to detect delivery violations, such as speeding and the use of non-designated delivery paths, and ensure accurate matching of each delivery to its intended destination store. Unlike typical indoor localization applications, DeMo addresses unique challenges, including sensor placement and the complex electromagnetic characteristics encountered in underground settings. Specifically, DeMo adapts the classical logarithmic radio signal model to facilitate fingerprint-free localization, significantly reducing deployment and maintenance costs. DeMo has been operating since May 2020, covering more than 200 shops with 74,537 deliveries (6193.2 km) across 12 subway stations in Hong Kong. DeMo’s 4-year operation witnessed a significant violation rate drop, from 19% (May 2020) to 0.9% (Mar 2024).}
}


@article{DBLP:journals/tmc/MoriniMRFCD25,
	author = {Marcello Morini and
                  Eugenio Moro and
                  Chiara Rubaltelli and
                  Ilario Filippini and
                  Antonio Capone and
                  Danilo De Donno},
	title = {Exploring Upper-6 GHz and mmWave in Urban 5G Networks: {A} Direct
                  on-Field Comparison},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13034--13047},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587904},
	doi = {10.1109/TMC.2025.3587904},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MoriniMRFCD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for mobile bandwidth is driving 5G networks toward the use of high-frequency spectrum, particularly the upper-6 GHz and mmWave bands. While these bands offer vast bandwidth potential, their propagation characteristics raise critical deployment challenges. This paper presents the first direct, on-field comparative evaluation of 5G standalone (SA) macro-cell deployments operating in these two bands, conducted in Milan, Italy. We show that the upper-6 GHz band can deliver wide-area urban coverage (up to 600 meters) with stable gigabit-level downlink throughput, even in Non-Line of Sight (NLoS) scenarios. mmWave, traditionally deemed unsuitable for NLoS, exhibits strong performance via urban reflections, achieving up to 1.3 Gbps in downlink and 250 Mbps in uplink. Furthermore, outdoor-to-indoor connectivity at mmWave frequencies proves viable through glass facades, challenging pessimistic assumptions about penetration losses. These findings, derived from synchronized deployments and extensive measurements, provide new insights into the complementary roles of these bands and the practical feasibility of their integration into future 5G networks.}
}


@article{DBLP:journals/tmc/XuCWZHDQ25,
	author = {Bin Xu and
                  Longgang Cheng and
                  Qing Wen and
                  Zhensheng Zou and
                  Xiaoxuan Hu and
                  Zhenjiang Dong and
                  Jin Qi},
	title = {Heterogeneous Federated Learning Driven by Multi-Knowledge Distillation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13048--13061},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586921},
	doi = {10.1109/TMC.2025.3586921},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuCWZHDQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a fully heterogeneous federated learning environment, the client has significant differences in model structure and local data distribution (Non-IID), and the joint learning of the client model is blocked due to the limited communication content available for interaction in a fully heterogeneous scenario. In this context, the global knowledge constructed by the server through the simple aggregation of the client logits is essentially a fuzzy representation containing a lot of noise and information loss, which is difficult to effectively guide the client model update. To solve these problems, this paper proposes a heterogeneous federated learning framework (FedMkd) based on multi-knowledge distillation fusion to cope with multiple challenges in heterogeneous environments. The FedMkd framework uses a class-grained logits interaction architecture (CLIA) and introduces an efficient knowledge sharing mechanism. It innovatively integrates two knowledge distillation methods: 1) Temperature-Adaptive Knowledge Distillation (TAKD), which provides differentiated temperatures for teacher and student models by adaptively adjusting the distillation temperature, maximizing knowledge transfer between them; 2) Class-related Knowledge Distillation (CRKD), which introduces batch-level sample correlation loss to reduce over-reliance on specific samples or classes and improve the model’s understanding of overall data features. We conducted a large number of experiments on four public datasets. The results show that in a variety of data and model heterogeneous scenarios, FedMkd still performs better than the comparison method when the communication overhead is reduced by more than one order of magnitude.}
}


@article{DBLP:journals/tmc/XiangLZWZG25,
	author = {Chaocan Xiang and
                  Zhenghan Li and
                  Qianyuan Zhang and
                  Xuangou Wu and
                  Xiao Zheng and
                  Yulan Guo},
	title = {EagleEye: Balancing Latency, Accuracy, and Power on Edge-Assisted
                  UAVs for Urban Crowd Surveillance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13062--13077},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586860},
	doi = {10.1109/TMC.2025.3586860},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiangLZWZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed Aerial Vehicles (UAVs) equipped with cameras provide a promising way for large-scale urban crowd surveillance due to their convenient deployment and flexible mobility. However, UAVs are constrained by limited power and computing resources, hindering existing work in achieving efficient UAV-based crowd surveillance, i.e., long flight time, high accuracy, and low latency. To this end, we propose EagleEye, a low-power, high-precision, and low-latency crowd surveillance system empowered by edge-assisted UAVs. It leverages lightweight devices on UAV-side to compress video information edge-independently, then transmits key video information instead of raw high-definition videos. Furthermore, we propose a novel spatio-temporal Compressive-Sensing-based video feature compression algorithm to achieve efficient, low-latency video compression. It can reduce video volumes greatly while minimizing the loss of crowd-surveillance-related information. Specifically, inspired by the Compressive Sensing theory, we compress the video content from both the temporal and spatial perspectives by accounting for inter-frame redundancy and intra-frame information saliency, respectively. Finally, we implement a prototype system and conduct extensive experiments based on four large-scale datasets with over 20,000 frames. The experimental results demonstrate that EagleEye can reduce transmission latency by 31.4% only with no more than 4% of accuracy loss in urban crowd detection.}
}


@article{DBLP:journals/tmc/KroepMHWCGPCBP25,
	author = {H. J. C. Kroep and
                  Pavlos Makridis and
                  J. Huidobro and
                  Koen W{\"{o}}sten and
                  Deepak Choudhary and
                  Nithish K. Gnani and
                  Tamma V. Prabhakar and
                  Stijn Coppens and
                  Kilian van Berlo and
                  R. Venkatesha Prasad},
	title = {Utilizing Operator Intent for Haptic Teleoperation Under High Latencies},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13078--13089},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591197},
	doi = {10.1109/TMC.2025.3591197},
	timestamp = {Sun, 01 Feb 2026 13:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KroepMHWCGPCBP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Haptic teleoperation is a promising technology with applications in telemaintenance and disaster management. However, it faces significant challenges when the application is subjected to a high network latency and environments with moving objects. This work aims to extend Model Mediated Teleoperation (MMT) to overcome challenges in supporting dynamic environments. Instead of striving for perfect model alignment, we acknowledge the inevitable mismatch between the remote environment and its model at the operator. We propose a set of design principles and an accompanying framework for designing MMT solutions that prioritize operator intent. Our approach is exemplified through an application where an operator, located 8000 km away (The Netherlands – India) and subjected to an average of 179 ms end-to-end latency, guides a robot arm to draw on a whiteboard whose position is actively altered. We evaluate the effectiveness of our approach through a user study. We show a 3-point improvement on a 7-point Likert scale when users utilize our approach to teleoperate over significant network latency of up to 1 s.}
}


@article{DBLP:journals/tmc/YuanZDZSLXL25,
	author = {Mu Yuan and
                  Lan Zhang and
                  Di Duan and
                  Liekang Zeng and
                  Miao{-}Hui Song and
                  Zichong Li and
                  Guoliang Xing and
                  Xiang{-}Yang Li},
	title = {Mitigating Tail Latency for On-Device Inference With Load-Balanced
                  Heterogeneous Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13090--13105},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588430},
	doi = {10.1109/TMC.2025.3588430},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuanZDZSLXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serving machine learning models on edge, mobile, and embedded devices places stringent requirements on inference latency. From operating a real enterprise service, we observed that even a fully optimized model could lead to severe violations of latency objectives when the load surges. A straightforward and mature approach is to auto-scale multiple models to balance the load. However, unlike cloud clusters, edge or mobile devices usually cannot afford to deploy multiple model replicas. Therefore, in this paper, we explore a new idea: in addition to the original model, we deploy one (or more) heterogeneous model(s) with much smaller resource overhead on the device, and perform load balancing among all models. We overcame the technical challenges posed by performance dynamics and developed InferRouter based on queuing theory. We implement and evaluate InferRouter on three real on-device inference systems, covering mobile sensing, video analytics, and natural language processing applications. Experimental results show that compared with strong baselines, InferRouter can decrease 85.2% P99 latency (5.8x faster) and improve 5.9% accuracy on the mobile workload. For a traffic video analytics task, InferRouter achieves 55.1% higher accuracy with zero deadline misses. InferRouter also shows its advantages in saving resources compared with auto-scaling and offloading approaches.}
}


@article{DBLP:journals/tmc/YeLW25,
	author = {Chang{-}Lin Ye and
                  Ming{-}Chun Lee and
                  Chen{-}Yuan Wu},
	title = {Mixed-Timescale Service Caching, Computing, and Communication Optimization
                  for Low-Latency High-Reliability Edge-Cloud Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13106--13124},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587713},
	doi = {10.1109/TMC.2025.3587713},
	timestamp = {Sun, 07 Dec 2025 22:17:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YeLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the mixed-timescale joint optimization of service caching, computing, and communication for edge-cloud networks. By jointly considering the caching update, user association, task offloading, and computing and communication resource allocation in different timescales with queueing dynamics, we first formulate a long-term optimization problem whose goal is to minimize the network energy consumption while stabilizing the edge-cloud network. By exploiting the Lyapunov optimization techniques, we decompose the overall problem into large-, medium-, and small-timescale problems, and their individual solution approaches are provided. To further improve the latency and reliability, an end-to-end latency measure is proposed along with the use of extreme value theory to derive an enhancement approach. Performance analysis of our approaches is provided and computer simulations are conducted to evaluate the proposed approaches. Results show that our approaches have better latency performance as compared to reference schemes. Furthermore, results also show that our proposed enhancement approach can effectively improve the latency and reliability.}
}


@article{DBLP:journals/tmc/ZhaoGLLZL25,
	author = {Yunming Zhao and
                  Wei Gong and
                  Minghui Liwang and
                  Li Li and
                  Baoxian Zhang and
                  Cheng Li},
	title = {A Generalizable Prompt-Based Prototypical Framework for CSI-Based
                  Few-Shot and Cross-Domain Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13125--13141},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587702},
	doi = {10.1109/TMC.2025.3587702},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoGLLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensing systems for human activity recognition (HAR) have made great strides in recent years. However, current HAR models face challenges in generalization due to a limited number of training samples (i.e., few-shot problem) and pattern differences of the collected channel state information (CSI) data across diverse domains (i.e., cross-domain problem). To address the above problems, in this paper, we propose a prompt-based prototypical framework called Wi-Prompt. Our Wi-Prompt framework consists of the following three modules: Prompt generation module, prototypical representation module, and activity recognition module, which works as follows. Prompt generation module is developed to extract prior knowledge from samples in the source domains, providing insightful guidance for establishing class prototypes in the target domains. Prototypical representation module effectively captures the most representative prototype vector of each class with a temporal convolutional network (TCN)-attention model. Activity recognition module determines the activity class of new sample by comparing the Euclidean distance between its corresponding prototype vector and the prototype vector of each class. The greatest advantage of Wi-Prompt is its utilization of prompt-based prototype representation, which eliminates the need for prior domain-specific knowledge about the original CSI samples, making it highly adaptable to a wide variety of CSI datasets collected across different domains. Extensive experiment results based on real-world traces show that our proposed Wi-Prompt outperforms state-of-the-art models under various cross-domain scenarios.}
}


@article{DBLP:journals/tmc/GuoWRXLQSCGYZ25,
	author = {Yifan Guo and
                  Zhu Wang and
                  Zhihui Ren and
                  Wei Xu and
                  Yangqian Lei and
                  Qian Qin and
                  Zhuo Sun and
                  Chao Chen and
                  Bin Guo and
                  Zhiwen Yu and
                  Daqing Zhang},
	title = {MultiScanner: Enabling Simultaneous Detection of Multiple Liquids
                  With mmWave Radar Based on a Composite Reflection Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13142--13159},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587079},
	doi = {10.1109/TMC.2025.3587079},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoWRXLQSCGYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional liquid detection approaches are often time-intensive and invasive, typically requiring the opening of containers for examination. While recent initiatives have proposed several innovative solutions, including camera-based and vibration sensor-based techniques, these approaches still face limitations in terms of convenience. The development of radio frequency (RF) technology, particularly millimeter-wave (mmWave) radar, offers a promising solution for non-invasive and contactless liquid detection. In particular, during the past few years, a number of radar-based sensing systems have been developed to detect or identify liquids. However, little work has been done on the simultaneous detection of multiple liquids. To fill this gap, we design a novel composite reflection model, which overcomes the detection challenges due to composite interference and environmental reflections, by utilizing the consistency and uniqueness of the reflection signals from multiple liquid targets. Based on the proposed model, we develop a system named MultiScanner, which is able to detect different types of liquids in multi-target scenarios, exhibiting high location independence without the need for extensive data training. Extensive experiments validate the effectiveness of MultiScanner, achieving up to 95.91% accuracy in detecting 10 hazardous-normal liquid combinations in 2-target scenarios. Moreover, even in more complex 5-target scenarios, an detection accuracy of 86.49% can be obtained. To the best of our knowledge, this is the first study that uses RF signals for multi-liquid detection.}
}


@article{DBLP:journals/tmc/JiangZZWLJ25,
	author = {Shenyao Jiang and
                  Hao Zhou and
                  Wangqiu Zhou and
                  Xinyu Wang and
                  Zhenjiang Li and
                  Yusheng Ji},
	title = {FreAuth\({}^{\mbox{+}}\): {A} Robust Frequency Feature-Based Device
                  Authentication Mechanism for Magnetic Wireless Power Transfer System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13160--13176},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587937},
	doi = {10.1109/TMC.2025.3587937},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiangZZWLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device authentication is critical for preventing unauthorized access and ensuring streamlined operation in magnetic wireless power transfer (WPT) systems. However, existing authentication techniques often suffer from security vulnerabilities and lack compatibility with low-cost receiver devices, limiting their practical applications. In this paper, we introduce FreAuth+ (FREquency feature-based AUTHentication), a novel authentication mechanism tailored specifically for magnetic-based WPT systems. Our approach involves acquiring impedance solely at the transmitter side without requiring active cooperation from the receiver. Using a dual-frequency interleaved subtraction technique, we eliminate ideal receiver impedance to reveal subtle frequency characteristics. These frequency features are then normalized to mitigate the effects of device positioning, and a temperature compensation method is applied to account for thermal variations. This process enables the generation of a robust hardware fingerprint based on frequency characteristics. For authentication, we employ a discrete Fréchet distance-based algorithm for effective fingerprint matching. We also develop a prototype of FreAuth+ and conduct extensive experiments. The results demonstrate its reliability, achieving 96.23% authentication performance across more than 60 devices with an average response time of 1.6 seconds. Furthermore, FreAuth+ exhibits excellent stability, effectively mitigating interference caused by device payload, device positioning, and temperature variations.}
}


@article{DBLP:journals/tmc/ChenYZL25,
	author = {Shutong Chen and
                  Jingwen Yin and
                  Ruichao Zhong and
                  Fangming Liu},
	title = {DeVA: An Edge-Assisted Video Analytics Framework for Depth Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13177--13190},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588864},
	doi = {10.1109/TMC.2025.3588864},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-assisted video analytics frameworks, which offload vision-based tasks to edge servers, offer a promising approach to enhance accuracy while minimizing network resource overhead. However, these frameworks often overlook depth estimation, a critical task for applications like augmented reality and intelligent surveillance. Depth estimation, which calculates the distance between objects and the camera, generates depth images with unique characteristics, making existing approaches impractical or inefficient for video analytics in this context. In this work, we present DeVA, an edge-assisted video analytics framework for depth estimation that ensures accuracy with minimal network resource overhead. We examine the impact of various video analytics configurations, including resolution and quantization parameter (QP), on accuracy. Additionally, we analyze the region of interest (RoI) for depth estimation and propose methods for tracking RoI areas locally on the device. DeVA features an adaptive video encoding mechanism that dynamically adjusts the resolution for offloaded video and optimizes QPs for RoI and non-RoI areas. We implement DeVA and evaluate its performance using public video datasets. The results show that DeVA reduces 57.12% of the bandwidth overhead while keeping depth estimation errors within acceptable limits, demonstrating a great balance between accuracy and network resource usage.}
}


@article{DBLP:journals/tmc/HuangHCYZZ25,
	author = {Xiaowen Huang and
                  Tao Huang and
                  Peng Cheng and
                  Jinhong Yuan and
                  Shuguang Zhao and
                  Guanglin Zhang},
	title = {Optimizing Task Migration for Public and Private Services in Vehicular
                  Edge Networks: {A} Dual- Layer Graph Neural Network Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13191--13208},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3589245},
	doi = {10.1109/TMC.2025.3589245},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangHCYZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the vehicular edge networks (VEN), task migration is complicated by issues like vehicle movement, diverse resource allocation, and integrating sensing with communication technologies. This paper presents a task migration strategy to optimize task flow under limited resources in PMN-assisted VEN. Vehicles can send public and private tasks to roadside units (RSUs), constrained by bandwidth, computational power, and storage space. Public tasks aim at data collection for road transportation management, while private tasks cover a spectrum of services from work to entertainment. To address the limitations imposed by resource scarcity and meet the demands of task migration, we have developed a dual-layer graph neural network (GNN) that leverages vehicle mobility patterns. In particular, the first layer of GNN acquires vehicle information and the latest surrounding information, and sends it to the nearby RSU. Considering the variety of tasks and multi-dimensional resource constraints, the second GNN layer forecasts RSU resource availability and vehicular trajectories. Subsequently, a task-based maximum flow algorithm (T-MFA) is proposed to refine task migration paths and resource allocation strategies to maximize task flow. Simulation experiments validate the efficacy of the proposed algorithm, demonstrating its capability to achieve optimal task migration by accommodating differences in tasks, resources, and capacities.}
}


@article{DBLP:journals/tmc/LiLZJHWX25,
	author = {Lulu Li and
                  Yafei Li and
                  Shaohui Zhang and
                  Yuanyuan Jin and
                  Shuo He and
                  Ke Wang and
                  Mingliang Xu},
	title = {Efficient Cooperative Mechanism for Distributed Multi-Agent Traffic
                  Signal Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13209--13225},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3587257},
	doi = {10.1109/TMC.2025.3587257},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLZJHWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic signal control (TSC) provides a cost-effective approach to alleviating urban traffic congestion without requiring modifications to physical road infrastructure. As a classic multi-agent coordination problem, TSC can be optimized with multi-agent reinforcement learning (MARL). However, understanding the cooperative strategies among traffic signals is challenging due to the spatiotemporal dynamics inherent in distributed traffic signal controllers. Previous MARL approaches for TSC often rely on imprecise and redundant cooperation mechanisms, leading to excessive communication costs and inefficient training. In this study, we propose a novel dynamic and controllable cooperation MARL algorithm (DCoo) to optimize traffic signals coordination for alleviating traffic congestion. DCoo introduces a collaboration scheduling module (CSM) to obtain spatiotemporal collaborative information and incorporates a cooperative hyper-decision network (HDN) to optimize decisions by leveraging the processed information. In particular, the CSM first captures crucial temporal information for cooperation by analyzing the relationships between historical and current observations of intersections. CSM then frames the selection of spatial information as a binary classification problem to effectively expand the scope of collaboration among intersections. Additionally, HDN integrates an attention mechanism and permutation invariance to enhance the scalability of DCoo. We conduct comparative experiments on three real-world traffic datasets, and the results demonstrate that DCoo outperforms comparable algorithms in both training and testing phases.}
}


@article{DBLP:journals/tmc/LiWYMLG25,
	author = {Qun Li and
                  Zunliang Wang and
                  Haipeng Yao and
                  Tianle Mai and
                  Zhipei Li and
                  Mohsen Guizani},
	title = {Dynamic Routing Mechanism for Load Distribution in {UAV} Swarm Networks
                  With Edge Caching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13226--13242},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3589569},
	doi = {10.1109/TMC.2025.3589569},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiWYMLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of the UAV swarm network has made its widespread application across a multitude of domains. However, the inherently dynamic nature of the network often gives rise to intermittent connectivity issues, leading to a significant reduction in data transmission capacity. To address this challenge, this study explores the integration of Information-centric Network (ICN) with the delay-tolerant network (DTN). This design aims to enhance message delivery rates by caching content data packets in UAV nodes. Building upon this architecture, we study the congestion control and load balancing problem. We design an on-demand collaborative communication routing algorithm. In our design, we first propose a routing decision model that incorporates multiple routing metrics to capture the dynamic evolution patterns of network nodes, effectively controlling local congestion issues. Subsequently, we employ Lyapunov optimization techniques to achieve network load balancing. By integrating the Lyapunov drift function, we ensure the stability of the feasible solution space within the model. Additionally, considering the high communication overhead caused by the sparse communication characteristics of DTN, we deploy a Multi-Agent Incentivized Communication (MAIC) algorithm to optimize routing scheduling strategies. Within the MAIC framework, each agent develops unique models for its teammates to generate customized information and minimize network information redundancy. Simulation results demonstrate that this algorithm effectively ensures congestion control and load balancing within the UAV swarm network while maintaining communication overhead in routing computations at a minimal level.}
}


@article{DBLP:journals/tmc/SinghalWMCLC25,
	author = {Chetna Singhal and
                  Yashuo Wu and
                  Francesco Malandrino and
                  Sharon L. G. Contreras and
                  Marco Levorato and
                  Carla{-}Fabiana Chiasserini},
	title = {Resource-Efficient Sensor Fusion at the Edge via System-Wide Dynamic
                  Gated Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13243--13257},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3586882},
	doi = {10.1109/TMC.2025.3586882},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SinghalWMCLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-generation mobile systems will support multiple AI-based applications, each leveraging heterogeneous sensors and data sources through deep neural network (DNN) architectures collaboratively executed within the network. In this context, to minimize the cost of the AI inference task subject to requirements on latency, quality, and – crucially – reliability of the inference process, it is vital to optimize (i) the set of sensors/data sources and (ii) the DNN architecture, (iii) the network nodes executing sections of the DNN, and (iv) the resources to use. To achieve these goals, we leverage dynamic gated neural networks with branches, and propose a novel algorithmic strategy called Quantile-constrained Inference (QIC), based upon quantile-Constrained policy optimization. QIC makes joint, high-quality, swift decisions on all the above aspects of the system, with the aim to minimize inference energy cost. We remark that this is the first contribution connecting gated dynamic DNNs with infrastructure-level decision making. We evaluate QIC using a dynamic gated DNN with stems and branches for optimal sensor fusion and inference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera data, and real-world wireless measurements. Our results confirm that QIC closely matches the optimum and outperforms existing approaches in reducing energy consumption (compute, communication, and total) and application requirements failure by over 70%.}
}


@article{DBLP:journals/tmc/ChenXXSWLJ25,
	author = {Ciyuan Chen and
                  Zhuqing Xu and
                  Runqun Xiong and
                  Dian Shen and
                  Weizheng Wang and
                  Junzhou Luo and
                  Xiaohua Jia},
	title = {Enhancing Link Performance for Mobile LoRa Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13258--13275},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3588923},
	doi = {10.1109/TMC.2025.3588923},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXXSWLJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa, as a typical representative of Low Power Wide Area Networks (LPWAN), has been widely used to connect massive IoT devices. However, in mobile applications, there is significant packet loss in LoRa transmission due to link performance degradation. Existing studies take little account of end-devices’ movement, particularly when the movement pattern is unknown. We propose LMLoRa to enhance the Link Performance for Mobile LoRa networks in general scenarios for both single-gateway and multi-gateway applications. The key observation is that, due to LoRa’s unique feature, repeating the original packet content enables the use of smaller, more energy-saving transmission parameters, which not only enhances link performance but also reduces energy consumption. Technically, we propose a link performance estimation model based on packet content repetition for both single-gateway and multi-gateway mobile networks. Then, we propose the corresponding channel frequency selection model to avoid transmission collisions. Finally, we design low-overhead communication mechanisms to operate the system. To evaluate the performance of LMLoRa in various scenarios, we design and implement real-world testbeds and a simulation platform for both single-gateway and multi-gateway scenarios. Extensive results show that LMLoRa improves packet delivery ratio by an average of 33.4 \\% \\%  to 69.2 \\% \\%  compared with the state-of-the-art.}
}


@article{DBLP:journals/tmc/DaiZKCJL25,
	author = {Longbao Dai and
                  Fanzi Zeng and
                  Haoran Kong and
                  Jiang{-}hao Cai and
                  Hongbo Jiang and
                  Keqin Li},
	title = {Throughput-Aware Cooperative Task Offloading in Dynamic Mobile Edge
                  Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13276--13292},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3592450},
	doi = {10.1109/TMC.2025.3592450},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiZKCJL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the commercialization of fifth-generation (5G) mobile communication technology and the rapid proliferation of mobile devices (MDs), demand for data computation is surging. This growth increases the reliance of MDs on low latency and high throughput. For this purpose, Mobile Edge Computing (MEC) enhances the user’s data processing capability by offloading computation tasks to servers at the network edge. However, achieving high efficiency in task offloading is challenging due to factors such as decision complexity, network dynamics, and user data privacy protection. Additionally, energy causal constraints and the coupling between offloading proportions and resource distribution cannot be ignored. In this paper, we first establish a dynamic task offloading problem to optimize the long-term throughput of the system. Using perturbed Lyapunov optimization, we transform MD delay and energy threshold constraints into the stability control of corresponding virtual queues. Then, we propose the Lyapunov-guided federated deep reinforcement learning (DRL) online task offloading algorithm called LyFOTO, which combines a federated learning (FL) framework and an Actor-Critic (AC) model. Under favorable communication conditions, the LyFOTO algorithm adaptively boosts system throughput; under poorer conditions, it properly delays task offloading, without violating queue backlog constraints. Through mathematical analysis, we discuss the performance of the LyFOTO algorithm. Simulation experiments validate that LyFOTO effectively balances system throughput and device battery energy. Finally, Comparative results show that LyFOTO outperforms other benchmark algorithms in maximizing system throughput while ensuring task backlog and energy threshold constraints.}
}


@article{DBLP:journals/tmc/HuangJHL25,
	author = {Zhemin Huang and
                  Zhong{-}Ping Jiang and
                  Zhu Han and
                  Yong Liu},
	title = {Robust Lyapunov Optimization for {LEO} Satellite Networks Routing
                  Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13293--13308},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591766},
	doi = {10.1109/TMC.2025.3591766},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangJHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) satellite networks are emerging as crucial components of space-air-ground integrated networks (SAGINs), extending beyond terrestrial capabilities to provide global data transmission services for the Internet of Things (IoT) and mobile devices. The proliferation of connected devices has led to increased data volumes and highly variable, bursty traffic patterns, thus posing significant challenges for network stability and necessitating effective routing control mechanisms. Traditional Lyapunov optimization methods have been fundamental in network optimization, offering stability guarantees under the assumption that traffic flows remain strictly within the network’s capacity region. However, this assumption is often violated in LEO satellite networks due to their dynamic and bursty nature, thereby rendering conventional approaches inadequate for ensuring stability. To address this challenge, we propose a robust Lyapunov optimization framework tailored for LEO satellite networks. Our method relaxes the strict requirements of traditional Lyapunov optimization by allowing the network to tolerate finite violations of the capacity region while still ensuring overall system stability. This approach demonstrates that, for a stabilizable network system, it is not necessary for traffic to remain within the capacity region at every time slot. We validate the effectiveness of the proposed robust Lyapunov optimization through extensive simulations under various traffic conditions and LEO satellite network configurations. The results confirm that LEO satellite networks can maintain stability despite finite violations of the capacity region, ensuring reliable performance amid dynamic and bursty traffic demands.}
}


@article{DBLP:journals/tmc/WangMFLLRZL25,
	author = {Xiang Wang and
                  Lingxiao Ma and
                  Ziyan Fu and
                  Xiangyu Li and
                  Yuanchun Li and
                  Ju Ren and
                  Yaoxue Zhang and
                  Yunxin Liu},
	title = {Squeezer: Efficient Multi-DNN Inference for Edge Video Analytics via
                  Cross-Model Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13309--13321},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3592647},
	doi = {10.1109/TMC.2025.3592647},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangMFLLRZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video analytics at the edge is becoming increasingly prevalent in many scenarios, such as smart campuses and intelligent factories. These applications often consist of multiple subtasks, which necessitates the optimization for multi-DNN (Deep Neural Network) inference. Due to limited consideration over cross-model scheduling, current practices cannot fully leverage available computing resources, leading to suboptimal performance. To address this, we propose Squeezer, a multi-DNN serving framework that holistically schedules multiple DNN models on an edge server with a single GPU. Squeezer decouples the cross-model scheduling into a two-layered approach, which involves (1) balanced operator grouping which partitions operators of multiple DNN models into groups, significantly reducing the scheduling complexity and (2) kernel scheduler which orchestrates parallel execution within each group by considering the interplay among kernels running in parallel, thereby enabling cross-model optimizations in multi-DNN inference. Performance evaluation results demonstrate that Squeezer outperforms state-of-the-art baselines, achieving up to  1.91\\times 1.91\\times  improvement in system throughput.}
}


@article{DBLP:journals/tmc/ZhuZMSLWCSLWL25,
	author = {Andong Zhu and
                  Sheng Zhang and
                  Lingkun Meng and
                  Xiaohang Shi and
                  Xiangyu Li and
                  Dongxu Wang and
                  Ke Cheng and
                  Hesheng Sun and
                  Sanglu Lu and
                  Jie Wu and
                  Yu Liang},
	title = {End-to-End Coordinated Spatio-Temporal Redundancy Elimination for
                  Fast Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13322--13338},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591307},
	doi = {10.1109/TMC.2025.3591307},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuZMSLWCSLWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge video analytics typically rely on conventional encoding standards to transmit device visual data for server-side inference. Unfortunately, general-purpose compression solutions retain unnecessary visual data that does not contribute to accuracy, resulting in significant latency throughout Video Analytics Pipeline (VAP). While previous approaches have made partial progress, they cannot systematically eliminate VAP redundancy due to uncoordinated subsystem-level optimization. Achieving complete redundancy elimination presents a major challenge, as a lack of spatio-temporal coordination risks offsetting latency gains with computational overhead (associated with redundancy elimination). Crucio overcomes these limitations with an end-to-edge framework that integrates temporally adaptive frame filtering and coordinated video compression. It leverages redesigned asymmetric autoencoders to synchronize inter-frame temporal compression with intra-frame spatial feature extraction. Additionally, Crucio employs a one-pass decoding mechanism for encoded critical frames and dynamically adjusts batching scales to minimize latency. Empirical results demonstrate Crucio’s superiority, outperforming existing solutions (e.g., DDS, Reducto, and STAC) by over a 31% reduction in end-to-end latency at 0.9 accuracy thresholds.}
}


@article{DBLP:journals/tmc/AsvadiA25,
	author = {Sepehr Asvadi and
                  Farid Ashtiani},
	title = {Age of Information in a Fully-Prioritized Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13339--13350},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3592939},
	doi = {10.1109/TMC.2025.3592939},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AsvadiA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the age of information (AoI) in a network consisting of multiple information streams with different priorities sharing a common server. In this network, the transmission of a packet is interrupted whenever a higher priority packet arrives. Regarding the behavior of AoI, for each stream, we consider a single buffer to enqueue the incoming packets based on a quasi-blocking (QB) policy. With the assumption of Poisson packet arrivals, we formulate the AoI and PAoI moment generating functions for each stream, while no assumption is considered for the service times of the packets. We also introduce a new semantic queueing policy. In this respect, we define the semantic (i.e., significance) of a packet as a linear combination of its age and remaining transmission time. When this metric is lower for a packet, the packet is more significant. In this policy, the decision to replace the available packet of a stream with a new arriving one is made based on the significance of these packets. This decision minimizes the sum of the next local peak and end-to-end delay in the AoI function. We investigate the superiority of this policy to the traditional ones in our numerical results.}
}


@article{DBLP:journals/tmc/LiuLLYGJ25,
	author = {Yandi Liu and
                  Guowei Liu and
                  Le Liang and
                  Hao Ye and
                  Chongtao Guo and
                  Shi Jin},
	title = {Deep Reinforcement Learning-Based User Scheduling for Collaborative
                  Perception},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13351--13365},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3594222},
	doi = {10.1109/TMC.2025.3594222},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLLYGJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stand-alone perception systems in autonomous driving suffer from limited sensing ranges and occlusions at extended distances, potentially resulting in catastrophic outcomes. To address this issue, collaborative perception is envisioned to improve perceptual accuracy by using vehicle-to-everything (V2X) communication to enable collaboration among connected and autonomous vehicles and roadside units. However, due to limited communication resources, it is impractical for all units to transmit sensing data such as point clouds or high-definition video. As a result, it is essential to optimize the scheduling of communication links to ensure efficient spectrum utilization for the exchange of perceptual data. In this work, we propose a deep reinforcement learning-based V2X user scheduling algorithm for collaborative perception. Given the challenges in acquiring perceptual labels, we reformulate the conventional label-dependent objective into a label-free goal, based on characteristics of 3D object detection. Incorporating both channel state information (CSI) and semantic information, we develop a double deep Q-Network (DDQN)-based user scheduling framework for collaborative perception, named SchedCP. Simulation results verify the effectiveness and robustness of SchedCP compared with traditional V2X scheduling methods. Finally, we present a case study to illustrate how our proposed algorithm adaptively modifies the scheduling decisions by taking both instantaneous CSI and perceptual semantics into account.}
}


@article{DBLP:journals/tmc/ZhangHMGXL25,
	author = {Han Zhang and
                  Yu Han and
                  Lingxin Meng and
                  Guan Gui and
                  Wei Xiang and
                  Yun Lin},
	title = {{MFFGCN:} Multimodal Feature Fusion Graph Convolution Network for
                  Radio Map Estimation With Uneven Spatial Sampling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13366--13382},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590335},
	doi = {10.1109/TMC.2025.3590335},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHMGXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio map estimation (RME) is a crucial method for analyzing spectrum space utilization and network coverage, serving as an essential tool for the mobile communication. However, physical constraints, security, privacy, and other issues often render some areas inaccessible, resulting in extremely sparse and unevenly distributed measurement data. To address these challenges, we propose a multimodal feature fusion graph convolution network (MFFGCN). The model incorporates a dual-encoder architecture with an adaptive multi-feature fusion module to exploit environmental information and learn the shadowing effects of radio-signal propagation. We then convert the coarse estimation into regional feature patches and construct a graph over these patches. A graph neural network aggregates contextual information among them, thereby alleviating the impact of uneven spatial sampling. Extensive experiments on open datasets demonstrate that our method achieves state-of-the-art performance, effectively reducing the effects of uneven sampling.}
}


@article{DBLP:journals/tmc/ZhangWKN25,
	author = {Fuyao Zhang and
                  Dan Wang and
                  Jiawen Kang and
                  Dusit Niyato},
	title = {Ground-Assisted {LEO} Satellite Federated Learning: Dynamic, Efficient,
                  Distributed Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13383--13396},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3593252},
	doi = {10.1109/TMC.2025.3593252},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWKN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread deployment of Low Earth Orbit (LEO) satellites, they generate a vast amount of data. This data has been instrumental in supporting machine learning (ML) in various terrestrial services to address global challenges such as monitoring climate change and natural disasters. However, many national regulations restrict the direct transmission of satellite data to ground stations (GSs). Therefore, ground-assisted satellite federated learning (FL) has emerged as a paradigm to safeguard data privacy by transferring model parameters instead of raw data for collaborative training. At present, the existing ground-assisted satellite FL methods encounter practical challenges: 1) The dynamic environment of LEO satellites results in continuous changes in the types of data collected by satellites, making it difficult for traditional FL models to adapt to these changes. This can lead to a deterioration in model accuracy over extended periods of model training. 2) Communication between satellites and GS is affected by atmospheric interference and weather factors, resulting in increased transmission delays and affecting the real-time efficiency of the FL system. In response to these challenges, we propose a dynamic, efficient, and distributed ground-assisted LEO satellite federated learning (DEDFL) framework to improve model accuracy and reduce satellite communication delays. In DEDFL, we design a Balanced Class Memory Extraction and an information playback strategy that enables the onboard FL model to adapt to changing satellite data types, thus achieving a performance balance across different classes. Additionally, we propose an adaptive fine coding method for parameter adoption prior to satellite transmission, effectively reducing the delay caused by satellites and ground-specific environmental variations. Experimental results demonstrate that the DEDFL method offers better accuracy and communication efficiency than other baseline algorithms.}
}


@article{DBLP:journals/tmc/KimJKP25,
	author = {Taeyun Kim and
                  Daeyoung Jung and
                  Yujin Kim and
                  Sangheon Pack},
	title = {Cost-Aware Neural Adaptive Scaling for vRAN Resource Allocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13397--13407},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590472},
	doi = {10.1109/TMC.2025.3590472},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KimJKP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although virtualized radio access networks (vRANs) offer flexibility and scalability, current scaling methods in vRANs tend to overlook the costs associated with energy consumption and service interruptions during the reconfiguration of containerized network functions (cNFs), leading to inefficient resource utilization. Moreover, traditional vertical and horizontal scaling approaches exacerbate these issues by requiring cNFs to be stopped and restarted or by deploying cNFs with fixed resource sizes. To address these challenges, we propose a cost-aware neural adaptive scaling (CNAS) framework, which adjusts cNF resource allocation dynamically, minimizing service disruptions and avoiding overprovisioning. The cost model incorporates energy consumption during the activation, allocation, and termination of cNFs and servers, as well as the operational cost of maintaining active cNFs and servers. We then formulate the integer linear programming (ILP) problem to minimize total costs. Due to the NP-hard complexity of this problem, two heuristic algorithms are applied: one utilizes dynamic programming to establish the cost-aware resource allocation, while the other uses a greedy approach to handle cNFs and servers following the determined resource allocation. Trace-driven simulation results demonstrate that CNAS can reduce the scaling costs by 53.4% and the total costs by 21.3% compared to the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/HuangHHJJW25,
	author = {Chung{-}ju Huang and
                  Yuanpeng He and
                  Xiao Han and
                  Wenpin Jiao and
                  Zhi Jin and
                  Leye Wang},
	title = {UniTrans: {A} Unified Vertical Federated Knowledge Transfer Framework
                  for Enhancing Edge Healthcare Collaboration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13408--13425},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590813},
	doi = {10.1109/TMC.2025.3590813},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangHHJJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-hospital collaboration has the potential to mitigate disparities in medical resources across different regions. However, strict privacy regulations prohibit the direct sharing of sensitive patient information between hospitals. Vertical federated learning (VFL) provides a novel privacy-preserving machine learning paradigm designed to maximizes data utility across multiple hospitals. Nevertheless, traditional VFL methods primarily benefit patients with overlapping data, leaving non-overlapping patients without guaranteed improvements in distributed healthcare prediction services. While some existing knowledge transfer techniques attempt to improve prediction performance for non-overlapping patients, they fail to adequately address scenarios where overlapping and non-overlapping patients originate from different domains, resulting in challenges such as feature and label heterogeneity. To address these issues, we propose UniTrans, a unified vertical federated knowledge transfer framework for edge healthcare collaboration. Our framework consists of three key steps. First, we extract the federated representation of overlapping patients by employing an effective vertical federated representation learning method to model multi-party joint features online. Next, each hospital learns a local knowledge transfer module offline, enabling the domain-adaptive transfer of knowledge from the federated representation of overlapping patients to the enriched representation of local non-overlapping patients. Finally, hospitals utilize these enriched local representations to enhance performance across various downstream medical prediction tasks. Extensive experiments on real-world medical datasets demonstrate the effectiveness and scalability of UniTrans in both intra-domain and cross-domain knowledge transfer.}
}


@article{DBLP:journals/tmc/LuoZTWZ25,
	author = {Wei Luo and
                  Deyu Zhang and
                  Ying Tang and
                  Fan Wu and
                  Yaoxue Zhang},
	title = {EdgeOAR: Real-Time Online Action Recognition on Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13426--13440},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591188},
	doi = {10.1109/TMC.2025.3591188},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoZTWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the challenges of Online Action Recognition (OAR), a framework that involves instantaneous analysis and classification of behaviors in video streams. OAR must operate under stringent latency constraints, making it an indispensable component for real-time feedback for edge computing. Existing methods, which typically rely on the processing of entire video clips, fall short in scenarios requiring immediate recognition. To address this, we designed EdgeOAR, a novel framework specifically designed for OAR on edge devices. EdgeOAR includes the Early Exit-oriented Task-specific Feature Enhancement Module (TFEM), which comprises lightweight submodules to optimize features in both temporal and spatial dimensions. We design an iterative training method to enable TFEM learning features from the beginning of the video. Additionally, EdgeOAR includes an Inverse Information Entropy (IIE) and Modality Consistency (MC)-driven fusion module to fuse features and make better exit decisions. This design overcomes the two main challenges: robust modeling of spatio-temporal action representations with limited initial frames in online video streams and balancing accuracy and efficiency on resource-constrained edge devices. Experiments show that on the UCF-101 dataset, our method EdgeOAR reduces latency by 99.23% and energy consumption by 99.28% compared to state-of-the-art (SOTA) method. And achieves an adequate accuracy on edge devices.}
}


@article{DBLP:journals/tmc/MaoCZNYGL25,
	author = {Qichao Mao and
                  Jiujun Cheng and
                  MengChu Zhou and
                  Zhangkai Ni and
                  Guiyuan Yuan and
                  Shangce Gao and
                  Chuanhuang Li},
	title = {Contributed Perception-Based Dynamic Evolution Method for Autonomous
                  Vehicle Groups in Open Scenes},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13441--13458},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590653},
	doi = {10.1109/TMC.2025.3590653},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaoCZNYGL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately handling dynamic evolution events is a significant challenge for autonomous vehicle groups (AVGs) in open scenes, which can be affected by complex road conditions and various interference factors. Existing work on the dynamic evolution of AVGs in open scenes concentrates on semi-centralized groups, assessing communication links as the sole criterion. However, there lack the mathematical analysis of and methods for the dynamic evolution of events in distributed AVGs with cooperative perception. To address this issue, we propose a contributed perception-based dynamic evolution method designed for distributed AVGs. This method ensures that group members can continuously and timely exchange valid perceptual information. First, we investigate the impact of external interference on the contributed perception of vehicle groups to understand the drivers behind their dynamic evolution. Second, we define a range of vehicle group evolution behaviors and corresponding handling methods in response to external interference. Lastly, we introduce group states and perceptibility to delineate the evolution dynamics. Simulation results demonstrate the superiority of our proposed method over existing ones in terms of average group contribution, accessibility, persistence, timeliness, and perceptibility.}
}


@article{DBLP:journals/tmc/AyoubiMMTFS25,
	author = {Reza Agahzadeh Ayoubi and
                  Eugenio Moro and
                  Marouan Mizmizi and
                  Dario Tagliaferri and
                  Ilario Filippini and
                  Umberto Spagnolini},
	title = {Optimal Planning for Heterogeneous Smart Radio Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13459--13474},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3593191},
	doi = {10.1109/TMC.2025.3593191},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AyoubiMMTFS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart Radio Environment (SRE) is a central paradigm in 6 G and beyond, where integrating Smart Radio Environment (SRE) components into the network planning process enables optimized performance for high-frequency Radio Access Network (RAN). This paper presents a comprehensive planning framework utilizing realistic urban scenarios and channel models to analyze diverse SRE components, including Reconfigurable Intelligent Surface (RIS), Network-Controlled Repeater (NCR), and advanced technologies like Simultaneous Transmitting and Reflecting RIS (STAR-RIS) and Trisectoral NCR (3SNCR). We propose two optimization strategies—Full Coverage Minimum Cost (FCMC) and Maximum Budget-Constrained Coverage (MBCC)—that address key cost and coverage objectives by considering both physical characteristics and scalable costs of each component, influenced by factors such as NCR amplification gain and RIS dimensions. Extensive numerical results demonstrate the significant impact of these models in enhancing network planning efficiency for high-density urban environments.}
}


@article{DBLP:journals/tmc/ZhangWZCCH25,
	author = {Xinran Zhang and
                  Dan Wang and
                  Yifei Zhu and
                  Weilong Chen and
                  Zheng Chang and
                  Zhu Han},
	title = {Zero-Trust Based Robust Federated Learning Against Betrayal Behaviors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13475--13487},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591632},
	doi = {10.1109/TMC.2025.3591632},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWZCCH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to its advantage of protecting data privacy and reducing communication overhead, Federated Learning (FL) is becoming a promising machine learning paradigm. However, resource limitations and unstable communication connections on the participating client end can lead to unintentional failures that degrade FL performance. Moreover, as FL systems scale and interconnect increasingly, they face growing exposure to intentional network risks. Furthermore, the assumption of continued trust in historically benign clients introduces vulnerabilities to potential internal betrayal within FL systems. In this paper, we enhance the robustness of FL by incorporating the zero-trust principle, which eliminates implicit trust in clients and mitigates unintentional failures, intentional attacks, and strategic betrayal risks. The framework incorporates dynamic client selection and aggregation weight allocation through trustworthiness evaluation and sustained skepticism toward each potential betrayal behavior. Specifically, a Dirichlet-based trust evaluation technique is presented to update clients’ trustworthiness with evolving observations. Then, to reduce potential betrayal loss, we formulate a min-max optimization problem that minimizes the worst-case betrayal loss. Next, we transform the formulation into a convex programming problem for solution. Extensive simulations are conducted to demonstrate the efficacy of the zero-trust based FL in the accurate trust assessment and the system’s betrayal-aware robustness enhancement.}
}


@article{DBLP:journals/tmc/ZhaoH25,
	author = {Ping Zhao and
                  Guang{-}Da Hu},
	title = {Private and Effective Range Counting Query Over Evolving Data in Internet
                  of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13488--13505},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590918},
	doi = {10.1109/TMC.2025.3590918},
	timestamp = {Sun, 07 Dec 2025 22:17:51 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Range counting query is the fundamental task for data analysis and data mining in Internet of Things (IoT). However, it poses a threat to the data privacy of data contributors, which is exacerbated by evolving data in particular. Several studies focus on range counting query on a timestamp over the finite evolving data, and thus are not applicable to the longitudinal range counting query on the infinite evolving data. To this end, we propose the Private and Effective Range counting query over Evolving data (PERE) that supports both the finite evolving data and the infinite evolving data, and is applicable for both the range counting query on a specific timestamp and the longitudinal range counting query. Specifically, we first design a Private Infinite Update Framework for IoT evolving data while providing meaningful privacy protection. The framework is coupled with a general and practical data evolution paradigm. Then, we propose a Optimized Frequency Perturbation consisting of an enhanced frequency oracle protocol and random sampling attribute. On this basis, we further propose a novel Adaptive Interval Merging mechanism that dynamically considers all potential interval consolidation possibilities and the reasonable selection of intervals for merging, to balance non-uniform error and noise error. Thereafter, we further reduce the estimated error in query results by Frequency Adjustment that consists of Norm-Sub and weighted average process. Last, we theoretically prove that the proposed PERE satisfies Local Differential Privacy (LDP), that the query results of PERE are unbiased, and that the variance of the query results is desirable. Furthermore, the extensive experiments on multiple real-world and synthetic datasets validate the effectiveness of PERE, as well as its advantages over the state-of-the-art works in answering range counting queries of evolving data.}
}


@article{DBLP:journals/tmc/LiuHXXXZHC25,
	author = {Ruixuan Liu and
                  Ming Hu and
                  Zeke Xia and
                  Xiaofei Xie and
                  Jun Xia and
                  Pengyu Zhang and
                  Yihao Huang and
                  Mingsong Chen},
	title = {FedGraft: Memory-Aware Heterogeneous Federated Learning via Model
                  Grafting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13506--13519},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591537},
	doi = {10.1109/TMC.2025.3591537},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHXXXZHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although Federated Learning (FL) is good at collaborative learning among devices without compromising their data privacy, it suffers from the problem of large-scale deployment in Mobile Edge Computing (MEC) applications. This is mainly because the varying memory sizes of edge devices inevitably result in limited sizes of their hosting models. According to the Cannikin Law, when dealing with heterogeneous devices with different memory sizes, the learning capability of existing homogeneous FL schemes is greatly restricted by the weakest device. Worse still, although existing heterogeneous FL methods enable a MEC application to involve numerous devices equipped with heterogeneous models, their knowledge aggregation processes require either extra training data or architecture similarity of models. To address the above issues, this paper presents a novel FL method named FedGraft that enables effective knowledge sharing among heterogeneous device models of different sizes without imposing unrealistic assumptions. In FedGraft, all the device models are grafted to a common rootstock based on our proposed model partitioning and grafting mechanism, facilitating knowledge sharing among heterogeneous models on top of a tree-like global model. Meanwhile, using our proposed device selection strategy, the reassembled submodels extracted from the global model can be reasonably dispatched to corresponding devices with sufficient memory, thus enhancing the overall FL performance. Comprehensive experimental results show that, compared with state-of-the-art heterogeneous FL methods, FedGraft can improve inference accuracy by up to 17% in various memory-constrained scenarios.}
}


@article{DBLP:journals/tmc/HuangWMMW25,
	author = {Haojun Huang and
                  Qifan Wang and
                  Geyong Min and
                  Wang Miao and
                  Dapeng Oliver Wu},
	title = {QoS Prediction for Component Services in 5G via Graph-Based Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13520--13534},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591783},
	doi = {10.1109/TMC.2025.3591783},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWMMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The accurate prediction of Quality of Service (QoS) in terms of response time, packet loss rate, latency and throughput for component services is essential for 5G to fulfill specific Service Level Agreements (SLAs). However, most current efforts failed to fully exploit the time-varying mobility features of users and parallel iteration multi-rules to perform QoS prediction for component services, incurring poor prediction accuracy. Therefore, in this paper, we are devoted to accurate QoS Prediction of Component Services (QPCS) for 5G via Graph-based Deep Reinforcement Learning (GDRL) to tackle this issue. Towards this end, the QoS prediction is modeled as GDRL-based QoS tensor factorization by designing a Spatio-Temporal-Recurrent-based Graph Attention Network (STR-GAT) and introducing it into Deep Deterministic Policy Gradient (DDPG) to factorize QoS tensor with multiple available rules in parallel. Specifically, a low-rank QoS tensor and an adjacency tensor are established, which include partial QoS observations of component services in each Base Station (BS), along with some missing elements, and evolving spatial information of users across these BSs, respectively. Then, the novel STR-GAT is designed by introducing spatio-temporal relations into conventional GAT to fully derive the mobility features of users to explore potential actions, while the derivative DDPG is adopted to perform tensor factorization with multiple available rules in parallel. Furthermore, the action smoothing and hierarchical-based replay buffer with priority-based and random sampling are designed and introduced into DDPG to stabilize training process and accelerate model convergence. Experimental simulation results on real-world datasets validate the superiorities of QPCS compared with the state-of-the-art approaches in predicting the QoS of component services in 5G.}
}


@article{DBLP:journals/tmc/WangCPDJS25,
	author = {Hongjun Wang and
                  Jiyuan Chen and
                  Tong Pan and
                  Zheng Dong and
                  Renhe Jiang and
                  Xuan Song},
	title = {Evaluating the Generalization Ability of Spatiotemporal Model in Urban
                  Scenario},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13535--13548},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590606},
	doi = {10.1109/TMC.2025.3590606},
	timestamp = {Sun, 07 Dec 2025 22:17:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangCPDJS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatiotemporal neural networks have shown great promise in urban scenarios by effectively capturing temporal and spatial correlations. However, urban environments are constantly evolving, and current model evaluations are often limited to traffic scenarios and use data mainly collected only a few weeks after training period to evaluate model performance. The generalization ability of these models remains largely unexplored. To address this, we propose a Spatiotemporal Out-of-Distribution (ST-OOD) benchmark, which comprises six urban scenario: bike-sharing, 311 services, pedestrian counts, traffic speed, traffic flow, ride-hailing demand, and bike-sharing, each with in-distribution (same year) and out-of-distribution (next years) settings. We extensively evaluate state-of-the-art spatiotemporal models and find that their performance degrades significantly in out-of-distribution settings, with most models performing even worse than a simple Multi-Layer Perceptron (MLP). Our findings suggest that current leading methods tend to over-rely on parameters to overfit training data, which may lead to good performance on in-distribution data but often results in poor generalization. We also investigated whether dropout could mitigate the negative effects of overfitting. Our results showed that a slight dropout rate could significantly improve generalization performance on most datasets, with minimal impact on in-distribution performance. However, balancing in-distribution and out-of-distribution performance remains a challenging problem. We hope that the proposed benchmark will encourage further research on this critical issue.}
}


@article{DBLP:journals/tmc/LiJLAWXLH25,
	author = {Shaoran Li and
                  Nan Jiang and
                  Chengzhang Li and
                  Shiva Acharya and
                  Yubo Wu and
                  Weijun Xie and
                  Wenjing Lou and
                  Y. Thomas Hou},
	title = {Real-Time {MU-MIMO} Beamforming With Limited Channel Samples in 5G
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13549--13566},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3592929},
	doi = {10.1109/TMC.2025.3592929},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiJLAWXLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {MU-MIMO beamforming is a key technology for 5G networks, relying on Channel State Information (CSI). However, in practice, the estimated CSI in reality is prone to uncertainty. Further, a MU-MIMO beamforming solution must be derived within a millisecond to be useful for real-time 5G applications. We present ReDBeam—a real-time data-driven beamforming solution for MU-MIMO using limited CSI data samples. The main novelties of ReDBeam are a parallel algorithm and an optimized GPU implementation. ReDBeam delivers a MU-MIMO beamforming solution within 1 millisecond to meet the probabilistic data rate requirements from the users, and minimize a base station’s power consumption. Through extensive experiments, we show that ReDBeam consistently meets the stringent 1-millisecond real-time requirement and is orders of magnitude faster than other state-of-the-art algorithms. ReDBeam conclusively demonstrates that MU-MIMO beamforming with data rate requirements can be achieved in real-time using only limited CSI data samples.}
}


@article{DBLP:journals/tmc/MaXWBFNGYL25,
	author = {Shiyuan Ma and
                  Lei Xie and
                  Chuyu Wang and
                  Yanling Bu and
                  Long Fan and
                  Jingyi Ning and
                  Qing Guo and
                  Baoliu Ye and
                  Sanglu Lu},
	title = {Multi-Modal Based 3D Localization via the Channel Adjustment LED-Tag},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13567--13585},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590801},
	doi = {10.1109/TMC.2025.3590801},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaXWBFNGYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of intelligent systems like assisted driving and robotics, all-weather target identification and 3D localization systems have become crucial for reliable obstacle avoidance and navigation. However, vision-based methods struggle to provide accurate target locations under low light or bad weather. Radar-based solutions like mmWave radar and LiDAR are robust but hindered by high costs and challenges in recognizing target identities at scale. In this paper, we propose a low-cost, all-weather target identification and 3D localization system based on LED-tags, which system can address the needs of intelligent systems for obstacle avoidance in complex environments. We explore the backscatter communication of LED devices and design a dual-modal LED-Tag, which includes two features: a backscatter RF signal detectable by RF devices and visual light spot information detectable by cameras, both sharing the same ID. To enhance the limited backscatter capability, we propose a multi-branch parallel model that enhances the signal strength using beamforming synthesis and a channel adjustment mechanism to improve robustness in complex environments, ensuring accurate 3D localization. For multi-target identification, we design an LED-tag encoding system, assigning each tag a unique encoding sequence. Each target’s identity can be recognized with our customized ID decoding method, which leverages prior information and time-domain sampling characteristics. Extensive experimental results show that the backscatter communication and target detection range of LED-tags can reach 15 m. Moreover, the system achieves an average localization error of 7.3 cm within a 5 m range, demonstrating the system’s excellent performance in terms of practicality and accuracy.}
}


@article{DBLP:journals/tmc/LiXWZWCDW25,
	author = {Borui Li and
                  Tiange Xia and
                  Weilong Wang and
                  Jingyuan Zhang and
                  Shuai Wang and
                  Chenhong Cao and
                  Zheng Dong and
                  Shuai Wang},
	title = {FluidEdge: Expediting Serverless Machine Learning Inference via Bottleneck-Aware
                  Auto-Scaling on Edge SoCs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13586--13599},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3592334},
	doi = {10.1109/TMC.2025.3592334},
	timestamp = {Thu, 20 Nov 2025 07:44:44 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiXWZWCDW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile applications based on machine learning (ML) are increasingly relying on offloading to the edge devices for low-latency, resource-efficient computation. Applying serverless computing for these ML applications on the edge offers a promising solution for handling dynamic workloads while meeting user-specified latency service-level objectives (SLOs). However, existing serverless frameworks, with their coarse-grained data parallelism and rigid model partitioning, are inadequate for ML inference on widely adopted edge System-on-Chip (SoC) devices. This paper presents FluidEdge, an edge-native serverless inference framework. FluidEdge identifies bottleneck operators in ML models and addresses them through a novel fine-grained intra-function latency-sensitive auto-scaling approach that dynamically scales inference bottlenecks during online serving. Additionally, it employs inter-function scaling to further prevent latency SLO violations and leverages the unified memory of edge SoCs for efficient data sharing during inference. Experimental results demonstrate that FluidEdge achieves a 37.4% latency improvement and 67.3% -87.6% SLO violation reduction compared to best-performed state-of-the-art serverless inference frameworks.}
}


@article{DBLP:journals/tmc/WangHSGXL25,
	author = {Yiqian Wang and
                  Jianping Huang and
                  Feng Shan and
                  Yuming Gao and
                  Runqun Xiong and
                  Junzhou Luo},
	title = {Optimizing Joint Speed and Altitude Schedule for {UAV} Data Collection
                  in Low-Altitude Airspace},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13600--13614},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591698},
	doi = {10.1109/TMC.2025.3591698},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangHSGXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-altitude airspace in major cities across the world is increasingly congested with uncrewed aerial vehicles (UAVs) and other aircraft. Emerging technologies, innovative business models, and supportive government policies are driving the growth of the low-altitude economy, where UAVs play a crucial role. Given the limited on-board energy of UAVs, this paper investigates the Joint UAV Speed and Altitude Scheduling (JUSAS) problem for data collection from sensors deployed along power transmission lines, bridges, highways, railways, water/gas/oil pipelines, or rivers/coasts. Distinct from existing work, the paper focuses on jointly optimizing UAV speed and altitude scheduling while determining the wireless sensor collection order. It accounts for the altitude-specific sensor transmission range model and the complexities of overlapping range relationships. We first propose the Slowest Segment First (SSF) policy to obtain an optimal UAV speed scheduling for fixed-altitude scenarios. Building upon this, we then reformulate JUSAS as a shortest-path-type problem using our novel flight scheduling graph, solved efficiently through the SSF-based Ant Colony Optimization (SSF-ACO) algorithm. To handle practical scenarios without prior sensor information along the path, we develop SSF-ACO-Online for real-time scheduling. Extensive simulations demonstrate that SSF-ACO significantly outperforms four other algorithms (i.e., SSF-Only, SSF-GA, SSF-PSO, and SSF-SA) in energy efficiency, and reduces 13.11% energy consumption on average. SSF-ACO-Online achieves comparable performance with energy consumption 1.24% higher than offline counterpart in average.}
}


@article{DBLP:journals/tmc/ZhangWWHML25,
	author = {Zhuangzhuang Zhang and
                  Libing Wu and
                  Zhibo Wang and
                  Jiahui Hu and
                  Chao Ma and
                  Qin Liu},
	title = {Cost-Efficient and Secure Federated Learning for Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13615--13632},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590799},
	doi = {10.1109/TMC.2025.3590799},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWHML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the collaborative machine learning nature of Federated Learning (FL), it enables the training of machine learning models on large-scale distributed datasets in edge computing environments. Nevertheless, the application of FL in edge computing still faces three crucial challenges: resource constraint, privacy leakage, and Byzantine failures. Unfortunately, current approaches lack the ability to effectively balance these three challenges. In this paper, we propose FedEdge, a cost-efficient and secure FL for edge computing. FedEdge contains two main mechanisms: adaptive compression perturbation and dynamic update filtering. The adaptive compression perturbation mechanism reduces the communication overhead, provides different levels of privacy protection for edge nodes, and prevents Byzantine attacks. The dynamic update filtering mechanism is used to further filter Byzantine attacks and limit the impact of adaptive compression perturbation on the global model performance. The experimental results on the MNIST, CIFAR-10, CIFAR-100, and CelebA datasets demonstrate the effectiveness of FedEdge against free-riders, label-flipping, and sign-flipping attacks. Theoretical analysis also demonstrate that FedEdge can still converge even when the majority of edge nodes are malicious.}
}


@article{DBLP:journals/tmc/ZhaoZCDHNX25,
	author = {Chenbin Zhao and
                  Ruifeng Zhu and
                  Jing Chen and
                  Ruiying Du and
                  Kun He and
                  Jianting Ning and
                  Yang Xiang},
	title = {{EP-GSPR:} An Efficient Privacy-Preserving Graph Shortest Path Retrieval
                  Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13633--13647},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591097},
	doi = {10.1109/TMC.2025.3591097},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZCDHNX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The continuous development of mobile terminal applications, online maps, and other navigation services have become widely used, simultaneously giving rise to significant security risks. To address the issues of privacy leakage and low efficiency in traditional graph shortest path retrieval schemes, an efficient privacy-preserving graph shortest path retrieval scheme is proposed, called EP-GSPR. Specifically, this scheme addresses the privacy security problems in the existing graph shortest path retrieval solutions by ensuring the bilateral privacy protection of the user’s query location and the database privacy of the cloud server. Throughout the retrieval process, the cloud server cannot obtain the user’s location information, and the user cannot access any database information other than the retrieval results. To overcome the performance bottlenecks in existing schemes, a progressive iterative retrieval framework is designed as the fundamental modular, called Pirf, achieving sub-linear retrieval costs and low storage overhead on the cloud server side. Finally, the security analyses demonstrate the EP-GSPR scheme achieves the bilateral privacy-preserving in terms of user and server sides. The comprehensive experiment evaluations also state the efficiency and practicality of the proposed scheme.}
}


@article{DBLP:journals/tmc/YangWFZGC25,
	author = {Jin Yang and
                  Qiong Wu and
                  Zhiying Feng and
                  Zhi Zhou and
                  Deke Guo and
                  Xu Chen},
	title = {Quality-of-Service Aware {LLM} Routing for Edge Computing With Multiple
                  Experts},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13648--13662},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590969},
	doi = {10.1109/TMC.2025.3590969},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangWFZGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.}
}


@article{DBLP:journals/tmc/WangZLXWGW25,
	author = {Xinjie Wang and
                  Yifan Zhang and
                  Xinpu Liu and
                  Ke Xu and
                  Jianwei Wan and
                  Yulan Guo and
                  Hanyun Wang},
	title = {GCFI-Net: Global-Local Cross-Spatial-Channel Feature Interaction Network
                  for Point Cloud Geometry Compression},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13663--13677},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590775},
	doi = {10.1109/TMC.2025.3590775},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZLXWGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficiently compressing large-scale point cloud data under limited bandwidth and computing resource conditions has become a critical issue to be addressed in mobile computing platforms. Although the octree structure can efficiently represent large-scale and complex point clouds, existing octree-based Point Cloud Geometry Compression (PCGC) approaches typically focus on exploiting either spatial or channel features individually, neglecting the interaction across spatial-channel dimensions. In addition, current approaches are also limited to small-scale point clouds due to reliance on global Transformer or local convolutional neural network (CNN). To solve these issues, we introduce GCFI-Net, a global-local cross-spatial-channel feature interaction network for predicting the occupancy probability distribution of each octree node in this paper. In the GCFI-Net, we propose a Multiscale Convolutional Fusion-based Spatial Interaction (MCFSI) module to capture global context and model spatial interactions, and a Global-Local Cross-Channel Interaction (GLCCI) module with dual pathways to integrate global and local cross-channel information. Additionally, we propose a Multiscale-enhanced Spatial and Channel Interaction (MSCI) module to aggregate features from ancestor and sibling nodes, which further enhances the octree node representation ability. Extensive experiments on large-scale sparse LiDAR and dense human body point clouds demonstrate that the proposed GCFI-Net achieves superior compression performance with fewer parameters compared to state-of-the-art PCGC methods.}
}


@article{DBLP:journals/tmc/XingZWC25,
	author = {Rui Xing and
                  Zhenzhe Zheng and
                  Fan Wu and
                  Guihai Chen},
	title = {User Context Generation for Large Language Models From Mobile Sensing
                  Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13678--13695},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591561},
	doi = {10.1109/TMC.2025.3591561},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XingZWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) exhibit remarkable capabilities in natural language understanding and generation. However, the accuracy of the inference depends deeply on the contexts of queries, especially for personal services. Abundant mobile sensing data collected by sensors embedded in smart devices can proactively capture real-time user contexts. However, raw sensing data are low-quality (e.g., existing missing data and data redundancy) and are incapable of providing accurate contexts. In this work, we present ConGen, a user context generation framework for LLMs, aiming at prompting users’ contexts through their implicit mobile sensing information. ConGen integrates two components: refined data completion and multi-granularity context compression. Specifically, the refined data completion couples data-centric feature selection by leveraging the eXplainable AI (XAI) method into the data imputation model to generate fewer but more informative features for efficient and effective context generation. Additionally, we implement multi-granularity context compression, reducing timestep- and context-level data redundancy while further elevating context quality. Experiment results show that ConGen can generate more accurate context, surpassing competitive baselines by 1.3%-8.3% in context inference on all four datasets. Moreover, context compression significantly reduces redundancy to  1/70\\sim 1/40 1/70\\sim 1/40  of the original data amount, and further improves the context accuracy. Finally, the enhanced performance of LLMs, as demonstrated by both quantitative and qualitative evaluations of prompting ConGen-generated user contexts, underscores the effectiveness of ConGen.}
}


@article{DBLP:journals/tmc/HeWWGZNX25,
	author = {Jialing He and
                  Jiacheng Wang and
                  Ning Wang and
                  Shangwei Guo and
                  Liehuang Zhu and
                  Dusit Niyato and
                  Tao Xiang},
	title = {Preventing Non-Intrusive Load Monitoring Privacy Invasion: {A} Precise
                  Adversarial Attack Scheme for Networked Smart Meters},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13696--13709},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3590765},
	doi = {10.1109/TMC.2025.3590765},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeWWGZNX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart grid, through networked smart meters employing the non-intrusive load monitoring (NILM) technique, can considerably discern the usage patterns of residential appliances. However, this technique also incurs privacy leakage. To address this issue, we propose an innovative scheme based on adversarial attack in this paper. The scheme effectively prevents NILM models from violating appliance-level privacy, while also ensuring accurate billing calculation for users. To achieve this objective, we overcome two primary challenges. First, as NILM models fall under the category of time-series regression models, direct application of traditional adversarial attacks designed for classification tasks is not feasible. To tackle this issue, we formulate a novel adversarial attack problem tailored specifically for NILM and providing a theoretical foundation for utilizing the Jacobian of the NILM model to generate imperceptible perturbations. Leveraging the Jacobian, our scheme can produce perturbations, which effectively misleads the signal prediction of NILM models to safeguard users’ appliance-level privacy. The second challenge pertains to fundamental utility requirements, where existing adversarial attack schemes struggle to achieve accurate billing calculation for users. To handle this problem, we introduce an additional constraint, mandating that the sum of added perturbations within a billing period must be precisely zero. Experimental validation on real-world power datasets REDD and U.K.-DALE demonstrates the efficacy of our proposed solutions, which can significantly amplify the discrepancy between the output of the targeted NILM model and the actual power signal of appliances, and enable accurate billing at the same time. Additionally, our solutions exhibit transferability, making the generated perturbation signal from one target model applicable to other diverse NILM models.}
}


@article{DBLP:journals/tmc/YeXGN25,
	author = {Licheng Ye and
                  Zehui Xiong and
                  Lin Gao and
                  Dusit Niyato},
	title = {An Overlapping Coalition Game Approach for Collaborative Block Mining
                  and Edge Task Offloading in MEC-Assisted Blockchain Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13710--13724},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3591822},
	doi = {10.1109/TMC.2025.3591822},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YeXGN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) is a promising technology that enhances the efficiency of mobile blockchain networks, by enabling miners, often acted by mobile users (MUs) with limited computing resources, to offload resource-intensive mining tasks to nearby edge computing servers. Collaborative block mining can further boost mining efficiency by allowing multiple miners to form coalitions, pooling their computing resources and transaction data together to mine new blocks collaboratively. Therefore, an MEC-assisted collaborative blockchain network can leverage the strengths of both technologies, offering improved efficiency, security, and scalability for blockchain systems. While existing research in this area has mainly focused on the single-coalition collaboration mode, where each miner can only join one coalition, this work explores a more comprehensive multi-coalition collaboration mode, which allows each miner to join multiple coalitions. To analyze the behavior of miners and the edge computing service provider (ECP) in this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the ECP, as the leader, determines the prices of computing resources for all MUs. In Stage II, each MU decides the coalitions to join, resulting in an overlapping coalition formation (OCF) game; Subsequently, each coalition decides how many edge computing resources to purchase from the ECP, leading to an edge resource competition (ERC) game. We derive the closed-form Nash equilibrium for the ERC game, based on which we further propose an OCF-based alternating algorithm to achieve a stable coalition structure for the OCF game and develop a near-optimal pricing strategy for the ECP’s resource pricing problem. Simulation results show that the proposed multi-coalition collaboration mode can improve the system efficiency by  12.64\\% \\sim 17.63\\% 12.64\\% \\sim 17.63\\% , compared to the traditional single-coalition collaboration mode.}
}


@article{DBLP:journals/tmc/WangCZZKS25,
	author = {Zhenning Wang and
                  Yue Cao and
                  Huan Zhou and
                  Xiaokang Zhou and
                  Jiawen Kang and
                  Houbing Song},
	title = {{DRAM:} Digital Twin-Driven Double-Layer Reverse Auction Method for
                  Multi-Platform Vehicular Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13725--13742},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3594488},
	doi = {10.1109/TMC.2025.3594488},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangCZZKS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, For-Hire Vehicles (FHVs) have emerged as major players in Vehicular CrowdSensing (VCS). However, the heterogeneity of tasks issued by Data Requesters (DRs) and the heterogeneity of sensors equipped on FHVs under different Vehicle Platforms (VPs) bring difficulties to task allocation and execution. It can be concluded that it is important to reasonably analyze the relationship among DRs, VPs, and FHVs, as well as to motivate VPs and FHVs to complete sensing tasks. Therefore, taking advantage of the real-time simulation and intelligent decision-making of Digital Twins (DT), this paper proposes a DT-driven Double-layer Reverse Auction Method (DRAM). In the first layer, the reverse auction is established between each DR and VPs, and in the second layer, the reverse auction is established between each VP and FHVs. Meanwhile, we also introduce a sensing fairness index to ensure the sensing balance of different sub-regions and consider it in the DRAM process. Here, the idea of backward induction is used to solve the above problems, with the goal of minimizing the overhead of winning VP and the average overhead of all DRs. Finally, the effectiveness of the DRAM proposed in this paper is verified based on the real data set. Compared with the baseline method, DRAM can reduce the average overhead of DR by about 4%-25%. Meanwhile, in terms of sensing fairness, it can be improved by up to 55%.}
}


@article{DBLP:journals/tmc/WangZZL25,
	author = {Yehui Wang and
                  Baoxian Zhang and
                  Jinkai Zhang and
                  Cheng Li},
	title = {Efficient Privacy-Preserving Federated Learning via Homomorphic Encryption-Enabled
                  Over-the-Air Computation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {24},
	number = {12},
	pages = {13743--13759},
	year = {2025},
	url = {https://doi.org/10.1109/TMC.2025.3610887},
	doi = {10.1109/TMC.2025.3610887},
	timestamp = {Fri, 26 Dec 2025 20:52:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables collaborative model training across devices, but data exchanges pose privacy risks. Homomorphic Encryption (HE) is widely used to enhances privacy in FL but incurs significant communication and computation latency. Prior work reduced this latency using compressions, but sacrificed learning accuracy and overlooked the impact of the number of participating devices on latency. Over-the-air computation (AirComp) leverages wireless channels’ superposition property to achieve high spectral efficiency and efficient aggregation irrespective of device number. In this paper, we propose HEAirFed, integrating AirComp with the state-of-the-art HE scheme CKKS for efficient privacy-preserving FL. In HEAirFed, we develop a ciphertext-oriented wireless communication module to ensure homomorphic operations leverage AirComp’s superposition property, enabling correct decryption. We further build a rigorous error analysis model, derive the worst-case upper bound of approximation error, and characterize this bound’s impact on the convergence guarantee of HEAirFed, measured by the optimality gap with bounded approximation error. Then, we minimize this gap and derive a near-optimal solution in semi-closed form. Extensive experimental results on real-world datasets validate the ciphertext-oriented design’s necessity, the error analysis’s correctness, and demonstrate that HEAirFed achieves a substantial reduction in communication and aggregation latency compared to baseline, with minimal learning accuracy loss.}
}
