@article{DBLP:journals/tmc/ZemaNPG24,
	author = {Nicola Roberto Zema and
                  Enrico Natalizio and
                  Luigi Di Puglia Pugliese and
                  Francesca Guerriero},
	title = {3D Trajectory Optimization for Multimission UAVs in Smart City Scenarios},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {1--11},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3215705},
	doi = {10.1109/TMC.2022.3215705},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZemaNPG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a definite possibility that, in a recent future, Unmanned Aerial Vehicles (UAVs) will form the backbone of any smart city in terms of automation and networking. One approach to extend the UAVs’ resources spectrum is to provide a mean for them to opportunistically recharge and connect to otherwise unreachable networks: provide Training and Recharge Areas (TRAs). In these dedicated areas, the UAVs could dock to Energy and Data Dispensers (EDD) devices to resupply their batteries and exploit a high-speed connection. To autonomously move through the smart city while accomplishing a set of given tasks but, at the same time, consider visiting the EDDs, is part of a tridimensional trajectory planning problem that needs to be addressed. In this paper, we formally define the combinatorial optimization problem representing the trajectory planning. We consider the case in which more than one UAV can be connected with the same EDD at the same time, by properly addressing the assignment of the bandwidth. Through simulative investigation, realistic values for the solution of the optimization problem are found. The behavior of the proposed model is compared with an ”online” approach that does not require the same resources and knowledge and whose evaluation and comparison with the ”offline” approach are performed through network simulation.}
}


@article{DBLP:journals/tmc/MegasHOSC24,
	author = {Vasileios Megas and
                  Sandra Hoppe and
                  Mustafa {\"{O}}zger and
                  Dominic Schupke and
                  Cicek Cavdar},
	title = {A Combined Topology Formation and Rate Allocation Algorithm for Aeronautical
                  Ad Hoc Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {12--28},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3217924},
	doi = {10.1109/TMC.2022.3217924},
	timestamp = {Tue, 26 Mar 2024 21:21:10 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MegasHOSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the problem of providing internet connectivity to aircraft flying above the ocean without using satellite connectivity given the lack of ground network infrastructure in the relevant oceanic areas. Is it possible to guarantee a minimum flow rate to each aircraft flying over an ocean by forming an aeronautical ad hoc network and connecting that network to internet via a set of limited number of ground base stations at the coast as anchor points? We formulated the problem as mixed-integer-linear programming (MILP) to maximize the number of aircraft with flow data rate above a certain threshold. Since this multi-commodity flow problem is at least NP-complete, we propose a two-phase heuristic algorithm to efficiently form topology and assign flows to each aircraft by maximizing the minimum flow. The performance of the heuristic algorithm is evaluated over the North Atlantic Corridor, heuristic performs only 8% less than the optimal result with low densities. In high network densities, the connectivity percentage changes from 70% to 40% under 75 Mbps data rate threshold. Furthermore, the connectivity percentage is investigated for different network parameters such as altitude and compared to upper and lower bounds and a baseline algorithm.}
}


@article{DBLP:journals/tmc/LaboniSSRRH24,
	author = {Nadia Motalib Laboni and
                  Sadia Jahangir Safa and
                  Selina Sharmin and
                  Md. Abdur Razzaque and
                  Md. Mustafizur Rahman and
                  Mohammad Mehedi Hassan},
	title = {A Hyper Heuristic Algorithm for Efficient Resource Allocation in 5G
                  Mobile Edge Clouds},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {29--41},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3213410},
	doi = {10.1109/TMC.2022.3213410},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LaboniSSRRH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emergence of intelligent devices and mobile edge clouds (MECs) in 5G networks has exponentially increased the number of applications that demand low latency services. However, their resource heterogeneity, limited computing power and storage including congestion in the ultra-dense 5G network, make the real-time services challenging. Existing works are limited either by addressing application delay requirements or computational load balancing. This article develops an efficient resource allocation framework for selecting optimal servers and routing paths in the 5G MEC network by jointly optimizing latency, computational, and network load variances. First, we formulate the above multi-objective problem as a mixed-integer non-linear programming problem. Further, we adopt a hyper-heuristic (AWSH) algorithm by leveraging the combined powers of Ant Colony, Whale, Sine-Cosine, and Henry Gas Solubility Optimization algorithms. The proposed AWSH algorithm works at the higher level, and it explores and exploits one of the three lower-level heuristics in each iteration to efficiently capture the dynamically varying environmental parameters and thereby address the resource allocation problem. Their collaborative effort helps to achieve a global optimum in allocating resources of 5G MEC network. Simulation results prove the superiority of the AWSH algorithm compared to state-of-the-art solutions in terms of service latency, successful offloading ratio, and load balancing.}
}


@article{DBLP:journals/tmc/DingLLWG24,
	author = {Chuntao Ding and
                  Yidong Li and
                  Zhichao Lu and
                  Shangguang Wang and
                  Song Guo},
	title = {A Resource-Efficient Feature Extraction Framework for Image Processing
                  in IoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {42--55},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3218402},
	doi = {10.1109/TMC.2022.3218402},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DingLLWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extracting features from image data on Internet of Things (IoT) devices to reduce the amount of data that needs to be uploaded to cloud/edge servers has received increasing attention. However, most of the existing related approaches suffer from two major limitations, (i) low performance and high network traffic, and (ii) a lot of storage resource consumption. To this end, we propose a resource-efficient feature extraction framework for image processing in IoT devices. The proposed framework consists of the edge-assisted extractor generation method and the NestE method. The extractor generated by the edge-assisted extractor generation method can extract the features required by the application, which can not only avoid the IoT device uploading useless feature data but also improve application performance. The proposed NestE generates a nonredundant subextractor by splitting the extractor into multiple subextractors, removing redundant subextractors, and nesting small-capacity subextractors in large-capacity subextractors in a parameter-sharing manner. Compared with deploying multiple independent subextractors on IoT devices, deploying the nonredundant multifunctional extractor can save considerable storage resources and switching overhead. Extensive experimental results show that the proposed framework reduces the storage footprint by approximately 90.7% and switching overhead by approximately 92.4% compared with deploying independent subextractors when using the classical principal component analysis algorithm.}
}


@article{DBLP:journals/tmc/TongDZHP24,
	author = {Fei Tong and
                  Bowen Ding and
                  Yujian Zhang and
                  Shibo He and
                  Yuyang Peng},
	title = {A Single-Anchor Mobile Localization Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {56--69},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221957},
	doi = {10.1109/TMC.2022.3221957},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TongDZHP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is necessary for rescuers to localize a target trapped in a an unknown area resulting from various natural disasters or human warfare. The global navigation satellite systems and existing wireless and cellular infrastructures may have been partially or totally constrained and not available for localization in the target area. In this article, we propose a simple yet effective single-anchor mobile localization scheme, called TSAL as a potential solution in the case where traditional localization methods fail. By building three Cartesian coordinate systems and carrying out distance and/or steering-angle measurement with the off-the-shelf approaches while the target node is moving, utilizing just one of existing normally-functioning cellular Base Stations (BSs) as the only anchor or redeploying only one BS is enough to localize the target. In addition, TSAL also works with multiple anchor nodes and we propose a corresponding scheme based on TSAL, called TML, which can obtain more accurate localization. Theoretical analyses, extensive simulations and real-world experiments are conducted for evaluating the proposed schemes, with the effects of a set of parameter settings investigated, which shows the high availability and effectiveness of our schemes.}
}


@article{DBLP:journals/tmc/AlimpertisMBBP24,
	author = {Emmanouil Alimpertis and
                  Athina Markopoulou and
                  Carter T. Butts and
                  Evita Bakopoulou and
                  Konstantinos Psounis},
	title = {A Unified Prediction Framework for Signal Maps: Not All Measurements
                  are Created Equal},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {70--89},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221773},
	doi = {10.1109/TMC.2022.3221773},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AlimpertisMBBP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Signal maps are essential for the planning and operation of cellular networks. However, the measurements needed to create such maps are expensive, often biased, not always reflecting the performance metrics of interest, and posing privacy risks. In this paper, we develop a unified framework for predicting cellular performance maps from limited available measurements. Our framework builds on a state-of-the-art random-forest predictor, or any other base predictor. We propose and combine three mechanisms that deal with the fact that not all measurements are equally important for a particular prediction task. First, we design quality-of-service functions (QQ), including signal strength (RSRP) but also other metrics of interest to operators, such as number of bars, coverage (improving recall by 76%-92%) and call drop probability (reducing error by as much as 32%). By implicitly altering the loss function employed in learning, quality functions can also improve prediction for RSRP itself where it matters (e.g., MSE reduction up to 27% in the low signal strength regime, where high accuracy is critical). Second, we introduce weight functions (W) to specify the relative importance of prediction at different locations and other parts of the feature space. We propose re-weighting based on importance sampling to obtain unbiased estimators when the sampling and target distributions are different. This yields improvements up to 20% for targets based on spatially uniform loss or losses based on user population density. Third, we apply the Data Shapley framework for the first time in this context: to assign values (\\phi) to individual measurement points, which capture the importance of their contribution to the prediction task. This can improve prediction (e.g., from 64% to 94% in recall for coverage loss) by removing points with negative values and storing only the remaining data points (i.e., as low as 30%), which also has the side-benefit of helping privacy. We evaluate our methods and demonstrate significant improvement in prediction performance, using several real-world datasets.}
}


@article{DBLP:journals/tmc/LinLZW24,
	author = {Zhiqi Lin and
                  Xuxun Liu and
                  Huan Zhou and
                  Jie Wu},
	title = {Adaptive Time-Varying Routing for Energy Saving and Load Balancing
                  in Wireless Body Area Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {90--101},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3213471},
	doi = {10.1109/TMC.2022.3213471},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinLZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Routing plays an essential role in ensuring normal and lasting operation of wireless body area networks (WBANs). However, existing routing schemes cause inefficient and unbalanced energy dissipation, which contributes to premature death of some nodes and high temperature within a small area of the body. In this article, we propose an adaptive time-varying routing (ATVR) protocol to address these issues. Unlike in conventional routing solutions, in our protocol a node may act as different roles (source node or relay node) and select different paths in disparate periods. This dynamic routing pattern helps to achieve a globally optimal routing solution. In ATVR, a node evaluation function and a path evaluation function are designed to reflect the node state and the path state respectively. Then, the path selection problem is transformed into a Hitchcock transportation problem, in which the nodes with worse node state act as source nodes (i.e., producers) and the nodes with better node state act as relay nodes (i.e., consumers). Then, this Hitchcock transportation problem is addressed by the AlphaBeta algorithm, in which the paths with less energy consumption and less path loss are selected to forward data. The experimental results show that our protocol has better performance in terms of energy consumption, network lifetime, and node temperature.}
}


@article{DBLP:journals/tmc/LiuLHLRK24,
	author = {Qingyu Liu and
                  Chengzhang Li and
                  Y. Thomas Hou and
                  Wenjing Lou and
                  Jeffrey H. Reed and
                  Sastry Kompella},
	title = {Aion: {A} Bandwidth Conserving Scheduler With Data Freshness Guarantee},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {102--116},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3215934},
	doi = {10.1109/TMC.2022.3215934},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLHLRK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a bandwidth minimization problem with Age of Information (AoI) constraints—a fundamental problem that has not been studied in AoI research. The problem is of critical importance in bandwidth-limited IoT environment while, at the same time, there is an expectation of AoI requirement on the application side. We present a novel polynomial-time algorithm called Aion that can construct a scheduler to satisfy AoI constraints with strong theoretical guarantee in terms of minimizing required bandwidth. Specifically, we prove that the bandwidth required by Aion is minimum if the AoI constraint vector meets a special mathematical structure called Fractional Consecutively Divisible (FCD). In the general case when the given AoI constraint vector is not FCD, we show that the bandwidth required by Aion is tightly upper bounded by a factor of the minimum. We validate the performance of Aion through a large number of simulations and all results confirm our theoretical findings. The results from this paper lay a foundation for future research on bandwidth minimization with AoI guarantee.}
}


@article{DBLP:journals/tmc/MuZLZSD24,
	author = {Phil K. Mu and
                  Jinkai Zheng and
                  Tom H. Luan and
                  Lina Zhu and
                  Zhou Su and
                  Mianxiong Dong},
	title = {{AMIS-MU:} Edge Computing Based Adaptive Video Streaming for Multiple
                  Mobile Users},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {117--134},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3225414},
	doi = {10.1109/TMC.2022.3225414},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MuZLZSD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for online high-quality video streaming has brought huge challenges to the traditional client-server video streaming systems due to the high feedback delay, rigorous bandwidth requirement, and the lack of a mechanism of centralized resource management between users. In this work, we propose AMIS-MU, an edge computing-based mobile video streaming system that optimizes the watching experience of users via playback adaptation and channel resource allocation. AMIS-MU fully explores the power of edge servers from three perspectives. First, by pre-caching videos from the cloud, AMIS-MU analyzes video contents at the edge, and achieves a nearly imperceptible content-based playback speed adaptation. Second, as the edge server controls the channel resources of users in a centralized fashion, AMIS-MU adaptively updates the channel configuration to optimize the overall watching experience. Last, the plenty of computational power available at the edge enables a more intelligent playback control by using deep reinforcement learning (DRL). We propose a novel usage of DRL which significantly reduces the complexity of the cross-layer joint optimization problem and solve the non-convex channel resource allocation problem by Lyapunov optimization. Experiments show that AMIS-MU outperforms other existing algorithms in terms of average QoE and fairness.}
}


@article{DBLP:journals/tmc/ShiCLGQQ24,
	author = {Tuo Shi and
                  Zhipeng Cai and
                  Jianzhong Li and
                  Hong Gao and
                  Tie Qiu and
                  Wenyu Qu},
	title = {An Efficient Processing Scheme for Concurrent Applications in the
                  IoT Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {135--149},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3219983},
	doi = {10.1109/TMC.2022.3219983},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiCLGQQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the large volume of IoT data, conventional sensor network based and the cloud base IoT systems cannot handle latency-sensitive and resource-consuming IoT applications. Sensor networks do not have enough computation resources and also suffer from a limited network lifetime. On the other hand, the cloud based IoT system is far away from the users and the physical world, and cannot satisfy the real-time requirement of IoT applications. We adopt the IoT edge network to address these challenges and process IoT applications in modern IoT systems. The IoT edge network is an emerging computing architecture in the IoT. Compared to the sensor nodes in conventional sensor networks, the edge servers have more computation resources. Compared to the remote cloud, the edge servers are closer to the users and the physical world. However, processing IoT applications in the edge network still remains challenging. First, how to process concurrent IoT applications has not been fully investigated. Second, the inner relationship between the network resource and the application latency has not been deeply analyzed. Third, the function conflict problem in edge servers has not been taken seriously. To solve the above challenges, we propose the Energy and Latency Efficient Processing Plan for Concurrent IoT Applications Problem which aims to construct an application processing plan by jointly considering the concurrency, the energy-latency relationship, and the function conflict problems. We prove that such a problem is NP-Hard, and algorithms are proposed accordingly. Furthermore, we also estimate the performance of the proposed algorithms by numerical results.}
}


@article{DBLP:journals/tmc/ZhangJGC24,
	author = {Jiale Zhang and
                  Tianxiang Jiang and
                  Xiaofeng Gao and
                  Guihai Chen},
	title = {An Online Fairness-Aware Task Planning Approach for Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {150--163},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3229112},
	doi = {10.1109/TMC.2022.3229112},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangJGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial crowdsourcing, a human-centric paradigm for performing spatial tasks, has drawn rising attention. Task assignment and worker scheduling are basic issues in spatial crowdsourcing, which has a high demand on the timeliness of tasks and fairness among workers. In this article, we propose Utility-Fairness Index to evaluate the performance of the crowdsourcing platform and introduce the Fairness-Aware Task Planning problem that maximizes the utility while well-maintaining fairness during task assignment. We prove that its offline version is NP-hard, and clarify that there is no deterministic online algorithm with a constant competitive ratio for it. Two greedy-based online algorithms, Traversal Search and Advanced Nearest Neighbor are designed to solve the problem. We make optimization on running time with recurrence method to reach linear complexity, and use Gaussian Mixture Model to reveal the distribution regularity of tasks in these algorithms. Experiments on both synthetic and real datasets validate the efficiency and effectiveness of our algorithms.}
}


@article{DBLP:journals/tmc/TanXL24,
	author = {Haijun Tan and
                  Ning Xie and
                  Alex X. Liu},
	title = {An Optimization Framework for Active Physical-Layer Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {164--179},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3217055},
	doi = {10.1109/TMC.2022.3217055},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TanXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper concerns the problem of the parameter optimization of an active Physical-Layer Authentication (PLA) scheme, which is crucial for minimizing the distortion on the base signal carrying the original message caused by embedding a tag. An inappropriate parameter may significantly lower the efficiency of the entire period of message transmission. In this paper, we propose an optimization framework for an active PLA scheme and further propose a new systematic metric, defined as Secure Authentication Efficiency (SAE). In the proposed framework, we minimize the aforementioned distortion by tuning three parameters, i.e., the Probability of Message Transmission (PMT), the Probability of Message Outage (PMO), and the Probability of Secure Authentication (PSA). The proposed optimization framework deepens the understanding of the correlation between the parameters of an active PLA scheme and the conditions of a wireless channel, and allows us to systematically optimize the parameters of the active PLA scheme. Then, we establish an objective function by maximizing the SAE with both PMT and PMO constraints. We implement our approach and conduct extensive performance comparisons. Our experimental results show that when the SNR at the receiver is more than 15 dB, our approach achieves an average SAE of greater than 80%, whereas the prior scheme without parameter optimization degenerates to zero SAE under some conditions, e.g., high SNR at adversary or high communication rate.}
}


@article{DBLP:journals/tmc/LiLPFXX24,
	author = {Yafei Li and
                  Yifei Li and
                  Yun Peng and
                  Xiaoyi Fu and
                  Jianliang Xu and
                  Mingliang Xu},
	title = {Auction-Based Crowdsourced First and Last Mile Logistics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {180--193},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3219881},
	doi = {10.1109/TMC.2022.3219881},
	timestamp = {Wed, 18 Dec 2024 13:51:13 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLPFXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The booming of mobile internet and crowdsourcing technology has offered great opportunities for first and last mile logistics (FLML) service. Unlike the traditional FLML service that separates the parcel collection in the first mile from the parcel delivery in the last mile, a new type of crowdsourced FLML service integrates parcel collection and parcel delivery services as a whole, which can significantly improve the efficiency of FLML service. Briefly, in a crowdsourced FLML service, the platform assigns the customers’ triggered pick-up parcels to the couriers who are delivering drop-off parcels in terms of the real-time status of couriers (e.g., capacity, location, and schedule). Existing works solving the crowdsourced FLML problem only consider the utility maximization for the platform but ignore the incentive to the utilities of couriers. Inspired by this, in this paper, we investigate a novel type of crowdsourced FLML problem, namely Auction-based Crowdsourced FLML (ACF), where the platform assigns the couriers with suitable pick-up parcels based on the preferences of couriers with the goal of maximizing the social welfare of the platform and couriers. To solve the ACF problem, we present a novel auction model named Multi-attribute Reverse Vickrey (MRV), where the couriers bid on parcels according to their preferences for parcels. Based on the MRV model, we present three efficient assignment algorithms to assign parcels to couriers. In addition, we give theoretical analysis for our proposed algorithms. Extensive experiments examine the efficiency and effectiveness of our solutions.}
}


@article{DBLP:journals/tmc/HuYLLZCW24,
	author = {Miao Hu and
                  Wenzhuo Yang and
                  Zhenxiao Luo and
                  Xuezheng Liu and
                  Yipeng Zhou and
                  Xu Chen and
                  Di Wu},
	title = {AutoFL: {A} Bayesian Game Approach for Autonomous Client Participation
                  in Federated Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {194--208},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3227014},
	doi = {10.1109/TMC.2022.3227014},
	timestamp = {Thu, 13 Feb 2025 15:46:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuYLLZCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given that devices (i.e., clients) participating in federated edge learning (FEL) are autonomous and resource-constrained in nature, it is critical to design effective incentive mechanisms to encourage client participation so as to improve the performance of FEL. In this article, we aim to boost the FEL training efficiency by answering how much compute resource should clients autonomously contribute to maximize their utilities. To this end, we develop AutoFL, an autonomous client participation decision framework for federated learning at the network edge without assuming that each client possesses complete information. We first model the problem of autonomous client participation as a Bayesian game with incomplete information, where each player in the game is associated with a set of types according to network conditions. We optimize an individual client's decision based on the dynamics of the population estimated following the Bayes rule. We prove that AutoFL can converge to a unique Bayesian Nash equilibrium point. Empirical results on three real datasets show that AutoFL achieves a higher model accuracy with only 15.5-24.5% model aggregation time per global training round, and its energy cost saving on mobile devices is 82.2-86.8% compared to the state-of-the-art algorithms. Moreover, we can achieve a 2.75-3.2x long-term fairness compared to classical solutions.}
}


@article{DBLP:journals/tmc/HeLWZH24,
	author = {Lijun He and
                  Jiandong Li and
                  Yanting Wang and
                  Jiangbin Zheng and
                  Liang He},
	title = {Balancing Total Energy Consumption and Mean Makespan in Data Offloading
                  for Space-Air-Ground Integrated Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {209--222},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3222848},
	doi = {10.1109/TMC.2022.3222848},
	timestamp = {Fri, 16 Aug 2024 08:05:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeLWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the data offloading problem in space-air-ground integrated networks (SAGINs) by jointly optimizing task scheduling and power control to balance the total energy consumption and mean makespan. We consider a mixed integer nonlinear programming problem to minimize a normalized weighted combination of these two conflicting objectives. We first propose an approximation algorithm to find a high-quality solution, which is shown to be at most \\frac{1}{2} from the optimum to this problem for given power allocation. We further show that optimal power allocation can be obtained in closed form under the assumption that satellite-ground links have low signal-to-noise ratio (SNR). Thus, the proposed approximation algorithm can be directly utilized to obtain a constant-factor solution to the studied problem in low-SNR scenarios. To extend our solution to more general scenarios, we further propose an efficient hybird algorithm based on a genetic framework. Our simulation results demonstrate the near-optimality and correctness of the proposed algorithms, and they unveil the interplay between total energy consumption and mean makespan in SAGINs as well.}
}


@article{DBLP:journals/tmc/ZhouTH24,
	author = {Yuanhang Zhou and
                  Fei Tong and
                  Shibo He},
	title = {Bi-Objective Incentive Mechanism for Mobile Crowdsensing With Budget/Cost
                  Constraint},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {223--237},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3229470},
	doi = {10.1109/TMC.2022.3229470},
	timestamp = {Wed, 24 Jan 2024 17:49:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, mobile crowdsensing (MCS) has been widely adopted as an efficient method for large-scale data collection. In MCS systems, insufficient participation and unstable data quality have become two crucial issues that prevent crowdsensing from further development. Thus designing a valid incentive mechanism is essentially significant. Most of the existing works on incentive mechanism design focus on single-objective optimization with various constraints. However, in the real-world crowdsensing, it is common that several objectives to be optimized exist. Furthermore, constraints on budget or cost are often seen in MCS systems as the feasibility of implementing incentive mechanism is indispensable. This paper studies a bi-objective optimization scenario of MCS to simultaneously optimize total value function and coverage function with budget/cost constraint through a set of problem transformations. Then a budget- or cost-feasible bi-objective incentive mechanism is further proposed to solve the aforementioned bi-objective optimization problem through the combination of binary search and greedy heuristic solution under budget or cost constraint, respectively. Through both rigorous theoretical analysis and extensive simulations, the obtained results demonstrate that the mechanisms achieve computation efficiency, individual rationality, truthfulness, and budget or cost feasibility, while one mechanism obtains an approximation.}
}


@article{DBLP:journals/tmc/FanZLSLWL24,
	author = {Wenhao Fan and
                  Liang Zhao and
                  Xun Liu and
                  Yi Su and
                  Shenmeng Li and
                  Fan Wu and
                  Yuan'an Liu},
	title = {Collaborative Service Placement, Task Scheduling, and Resource Allocation
                  for Task Offloading With Edge-Cloud Cooperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {238--256},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3219261},
	doi = {10.1109/TMC.2022.3219261},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanZLSLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an edge-cloud cooperative computing network, the task offloading performance can be further improved by the edge-cloud and edge-edge cooperation, in which the tasks can be offloaded from an edge server to the cloud server or another edge server. Such edge-cloud cooperative task offloading can jointly utilize the resources of all the edge servers and the cloud server. This paper proposes a collaborative service placement, task scheduling, computing resource allocation, and transmission rate allocation scheme for a multi-task and multi-service scenario with edge-cloud cooperation. The objective of our optimization problem is to minimize the total task processing delay while guaranteeing long-term task queuing stability. Considering the high complexity of the original optimization problem, we transform the problem into a deterministic problem for each time slot based on the Lyapunov optimization. Then, we design an iterative algorithm to obtain the whole solution to the problem efficiently based on a hybrid method using multiple numerical techniques. Further, considering the inherent difference in the optimization periods of the service placement, resource allocation, and task scheduling sub-problems, we design a multi-timescale algorithm to solve the sub-problems with different optimization periods. The complexity of the proposed algorithms is analyzed, and extensive simulations are conducted by varying multiple crucial parameters. The superiority of our scheme is demonstrated in comparison with 4 other schemes.}
}


@article{DBLP:journals/tmc/PengTZLQLL24,
	author = {Yan Peng and
                  Xiaogang Tang and
                  Yiqing Zhou and
                  Jin{-}Tao Li and
                  Yanli Qi and
                  Ling Liu and
                  Hai Lin},
	title = {Computing and Communication Cost-Aware Service Migration Enabled by
                  Transfer Reinforcement Learning for Dynamic Vehicular Edge Computing
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {257--269},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3225239},
	doi = {10.1109/TMC.2022.3225239},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PengTZLQLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the high mobility of vehicles, service migration is inevitable in vehicular edge computing (VEC) networks. Frequent service migrations incur prohibitive migration cost including the computing cost (e.g., increased computing delay) and communication cost (e.g., occupied backhaul bandwidth). Yet existing service migration schemes are usually designed without considering the impact of the computing cost. This paper considers the impact of computing and communication cost jointly, and proposes a computing and communication cost-aware service migration scheme for VEC networks (i.e., CA-migration). Taking the service delay as a QoS metric for VEC networks, this paper formulates a migration optimization problem aiming to maximize the services’ satisfaction degree of delay (i.e., the probability that the service delay is smaller than the service delay requirement), where both the communication cost and computing cost affect the services’ satisfaction degree. Since the optimization problem is a constrained non-linear integer programming problem, it is difficult to solve. Moreover, the VEC networks are highly dynamic. Thus, a fast transfer reinforcement learning (fast-TRL) method combining transfer learning and reinforcement learning is proposed to provide an adaptive service migration scheme in dynamic VEC networks. Simulation results show that compared with existing schemes, the proposed CA-migration scheme can increase the satisfaction degree by up to 30%, and needs 25% less training time to obtain the optimal service migration policy.}
}


@article{DBLP:journals/tmc/ChenZWZSC24,
	author = {Jinbo Chen and
                  Dongheng Zhang and
                  Zhi Wu and
                  Fang Zhou and
                  Qibin Sun and
                  Yan Chen},
	title = {Contactless Electrocardiogram Monitoring With Millimeter Wave Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {270--285},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3214721},
	doi = {10.1109/TMC.2022.3214721},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZWZSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The electrocardiogram (ECG) has always been an important biomedical test to diagnose cardiovascular diseases. Current approaches for ECG monitoring are based on body attached electrodes leading to uncomfortable user experience. Therefore, contactless ECG monitoring has drawn tremendous attention, which however remains unsolved. In fact, cardiac electrical-mechanical activities are coupling in a well-coordinated pattern. In this paper, we achieve contactless ECG monitoring by breaking the boundary between the cardiac mechanical and electrical activity. Specifically, we develop a millimeter-wave radar system to contactlessly measure cardiac mechanical activity and reconstruct ECG without any contact in. To measure the cardiac mechanical activity comprehensively, we propose a series of signal processing algorithms to extract 4D cardiac motions from radio frequency (RF) signals. Furthermore, we design a deep neural network to solve the cardiac related domain transformation problem and achieve end-to-end reconstruction mapping from RF input to the ECG output. The experimental results show that our contactless ECG measurements achieve timing accuracy of cardiac electrical events with median error below 14ms and morphology accuracy with median Pearson-Correlation of 90% and median Root-Mean-Square-Error of 0.081mv compared to the groudtruth ECG. These results indicate that the system enables the potential of contactless, continuous and accurate ECG monitoring.}
}


@article{DBLP:journals/tmc/WuDLXXX24,
	author = {Sixu Wu and
                  Haipeng Dai and
                  Linfeng Liu and
                  Lijie Xu and
                  Fu Xiao and
                  Jia Xu},
	title = {Cooperative Scheduling for Directional Wireless Charging With Spatial
                  Occupation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {286--301},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3214979},
	doi = {10.1109/TMC.2022.3214979},
	timestamp = {Tue, 02 Jan 2024 12:25:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuDLXXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Power Transfer (WPT) technology has been developed rapidly in recent years. The cooperative charging model and corresponding scheduling methods have been proposed to save the charging cost in paid charging service. However, the state-of-the-art methods ignore the spatial occupation issue of rechargeable devices. Moreover, the cooperative charging scheduling in directional wireless charging has not been studied yet. This paper studies the cooperative scheduling for directional wireless charging with spatial occupation. We formulate the Cooperative Charging Scheduling with Spatial occupation (CCSS) problem of Mobile Rechargeable Sensor Devices (MRSDs) for optimizing the total cost of whole charging system. We first investigate the properties of optimal arrangement of MRSDs in charging group and calculate the tight intervals of charging angles of MRSDs. We show that it is sufficient to bound the error by conducting angle discretization for only two MRSDs in each charging group. Then, a (\\ln n+1)(1+\\varepsilon)\n-approximation algorithm of the CCSS problem is proposed based on greedy approach, where n\nis the number of MRSDs, and \\varepsilon\nis the discretization error. The results of extensive simulations and field experiments demonstrate that our algorithm can reduce at most 42.5% total cost comparing with the benchmark algorithms.}
}


@article{DBLP:journals/tmc/HaoLWZ24,
	author = {Yujiao Hao and
                  Xijian Lou and
                  Boyu Wang and
                  Rong Zheng},
	title = {CROMOSim: {A} Deep Learning-Based Cross-Modality Inertial Measurement
                  Simulator},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {302--312},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230370},
	doi = {10.1109/TMC.2022.3230370},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HaoLWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of wearable devices, inertial measurement unit (IMU) data has been utilized in monitoring and assessing human mobility such as human activity recognition (HAR) and human pose estimation (HPE). Training deep neural network (DNN) models for these tasks require a large amount of labelled data, which are hard to acquire in uncontrolled environments. To mitigate the data scarcity problem, we design CROMOSim, a cross-modality sensor simulator that simulates high fidelity virtual IMU sensor data from motion capture systems or monocular RGB cameras. It utilizes a skinned multi-person linear model (SMPL) for 3D body pose and shape representations to enable simulation from arbitrary on-body positions. Then a DNN model is trained to learn the functional mapping from imperfect trajectory estimations in a 3D SMPL body tri-mesh due to measurement noise, calibration errors, occlusion and other modelling artifacts, to IMU data. We evaluate the fidelity of CROMOSim simulated data and its utility in data augmentation on various HAR and HPE datasets. Extensive empirical results show that the proposed model achieves a 6.7% improvement over baseline methods in a HAR task.}
}


@article{DBLP:journals/tmc/ZhangGSLY24,
	author = {Zhouyangzi Zhang and
                  Bin Guo and
                  Wen Sun and
                  Yan Liu and
                  Zhiwen Yu},
	title = {Cross-FCL: Toward a Cross-Edge Federated Continual Learning Framework
                  in Mobile Edge Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {313--326},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3223944},
	doi = {10.1109/TMC.2022.3223944},
	timestamp = {Wed, 14 Aug 2024 08:21:03 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGSLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) in mobile edge computing (MEC) systems has recently been studied extensively. In ubiquitous environments, there are usually cross-edge devices that learn a series of tasks across multiple independent edge FL systems. Due to the differences in the scenarios and tasks of different FL systems, cross-edge devices will forget past tasks after learning new tasks, which is unacceptable for devices that pay system costs to participate in FL. Continual learning (CL) is a viable solution to this problem, which aims to train a model to learn a series of tasks without forgetting old knowledge. Currently, there is no work to investigate the problem of CL in a cross-edge FL scenario. In this paper, we propose Cross-FCL, a Cross-edge Federated Continual Learning framework. Specifically, it enables devices to retain the knowledge learned in the past when participating in new task training through a parameter decomposition based FCL model. Then various cross-edge strategies are introduced, including biased global aggregation and local optimization, to trade off memory and adaptation. We conducted experiments on a real-world dataset and other public datasets. Extensive experiments demonstrate that Cross-FCL achieves best accuracy on IID and highly non-IID tasks with a low storage cost compared to other baselines.}
}


@article{DBLP:journals/tmc/ShiKHCFY24,
	author = {Yishuo Shi and
                  Wen{-}Hsing Kuo and
                  Chih{-}Wei Huang and
                  Yen{-}Cheng Chou and
                  Shih{-}Hau Fang and
                  De{-}Nian Yang},
	title = {Cross-Layer Video Synthesizing and Antenna Allocation Scheme for Multi-View
                  Video Provisioning Under Massive {MIMO} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {327--340},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3227770},
	doi = {10.1109/TMC.2022.3227770},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiKHCFY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the growing need for bandwidth starving Multi-View Videos (MVV) in virtual reality, TV, and education, effectively allocating the resources of next-generation wireless technologies for MVV streams becomes increasingly crucial. To achieve high utility for MVV users, this article proposes a cross-layer resource allocation mechanism to leverage video synthesizing schemes (such as Depth-Image-Based Rendering (DIBR) for efficient MVV streaming with massive MIMO). First, we formulate a new problem, antenna allocation with video synthesis (AAVS), and prove its NP-hardness. Then, we design an approximation algorithm named Utility-based Multi-View Synthesis (UMVS) with the analytical performance provided, and dynamic scenarios are addressed by augmenting UMVS with deep reinforcement learning. Data-driven simulation results show that UMVS outperforms existing antenna allocation schemes by at least 10%, and the DRL extension provides an additional 6% improvement in system utility under congested scenarios.}
}


@article{DBLP:journals/tmc/ChenXXJQ24,
	author = {Suo Chen and
                  Yang Xu and
                  Hongli Xu and
                  Zhida Jiang and
                  Chunming Qiao},
	title = {Decentralized Federated Learning With Intermediate Results in Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {341--358},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221212},
	doi = {10.1109/TMC.2022.3221212},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXXJQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Federated Learning (FL) permits all workers (e.g., mobile devices) to cooperatively train a model using their local data at the network edge. In order to avoid the possible bottleneck of conventional parameter server architecture, the decentralized federated learning (DFL) is developed on the peer-to-peer (P2P) communication. In DFL, model exchanging among workers is usually regarded as an atomic operation, which largely affects the total bandwidth consumption during model training. Given the limited communication resource on workers, model exchanging will pose a great challenge when meeting with the large-scale models. Herein, we propose to let workers exchange the intermediate results, instead of the entire model, with each other. We provide theoretical analysis of DFL based on intermediate result exchanging, which reveals the relationship between the training performance and the exchanging interval (i.e., the number of local updating iterations) of intermediate results. According to the convergence bound, we propose an adaptive exchanging interval (or frequency) algorithm called Fed-IR, which optimizes the trade-off between communication cost and training performance. Extensive simulation results show that compared with the model exchanging methods, our proposed algorithms can save communication traffic of around 42%\\sim\n81% while still achieving the similar accuracy.}
}


@article{DBLP:journals/tmc/YangZ24,
	author = {Qiang Yang and
                  Yuanqing Zheng},
	title = {DeepEar: Sound Localization With Binaural Microphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {359--375},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3222821},
	doi = {10.1109/TMC.2022.3222821},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The binaural microphone, which refers to a pair of microphones with artificial human-shaped ears, is widely used in hearing aids and spatial audio recording to improve sound quality. It is crucial for such devices to find the voice direction in many applications such as binaural sound enhancement. However, sound localization with two microphones remains challenging, especially in multi-source scenarios. Most previous work utilized microphone arrays to deal with the multi-source localization problem. Extra microphones yet have space constraints for deployment in many scenarios (e.g., hearing aids). Inspired by the fact that humans have evolved to locate multiple sound sources with only two ears, we propose DeepEar, a binaural microphone-based sound localization system. To this end, we design a multisector-based neural network to locate multiple sound sources simultaneously, where each sector is a discretized region of the space for different angle of arrivals. DeepEar fuses explicit hand-crafted features and implicit latent sound representatives to facilitate sound localization. More importantly, the trained DeepEar model can adapt to new environments with a minimum amount of extra training data. The experiment results show that DeepEar substantially outperforms the state-of-the-art binaural deep learning approach by a large margin in terms of sound detection accuracy and azimuth estimation error.}
}


@article{DBLP:journals/tmc/LiuLXW24,
	author = {Zhidan Liu and
                  Jiancong Liu and
                  Xiaowen Xu and
                  Kaishun Wu},
	title = {DeepGPS: Deep Learning Enhanced {GPS} Positioning in Urban Canyons},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {376--392},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3208240},
	doi = {10.1109/TMC.2022.3208240},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Global Positioning System (GPS) has benefited many novel applications, e.g., navigation, ride-sharing, and location-based services, in our daily life. Although GPS works well in most places, its performance in urban canyons is well-known poor, due to the signal reflections of non-line-of-sight (NLOS) satellites. Tremendous efforts have been made to mitigate the impacts of NLOS signals, while previous works heavily rely on precise proprietary 3D city models or other third-party resources, which are not easily accessible. In this paper, we present DeepGPS, a deep learning enhanced GPS positioning system that can correct GPS estimations by only considering some simple contextual information. DeepGPSfuses environmental factors, including building heights and road distribution around GPS's initial position, and satellite statuses to describe the positioning context, and exploits an encoder-decoder network model to implicitly learn the complex relationships between positioning contexts and GPS estimations from massive labeled GPS samples. As a result, the well-trained model can accurately predict the correct position for each erroneous GPS estimation given its positioning context. We further improve the model with a novel constraint mask to filter out invalid candidate locations, and enable continuous localization with a simple mobility model. A prototype system is implemented and experimentally evaluated using a large-scale bus trajectory dataset and real-field GPS measurements. Experimental results demonstrate that DeepGPSsignificantly enhances GPS performance in urban canyons, e.g., on average effectively correcting 90.1% GPS estimations with accuracy improvement by 64.6%.}
}


@article{DBLP:journals/tmc/LiGLCXXZ24,
	author = {Jing Li and
                  Song Guo and
                  Weifa Liang and
                  Quan Chen and
                  Zichuan Xu and
                  Wenzheng Xu and
                  Albert Y. Zomaya},
	title = {Digital Twin-Assisted, SFC-Enabled Service Provisioning in Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {393--408},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3227248},
	doi = {10.1109/TMC.2022.3227248},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiGLCXXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) has been identified as a desirable computing paradigm that provides efficient and effective services for various applications, while meeting stringent service delay requirements. Orthogonal to the MEC computing paradigm, Network Function Virtualization (NFV) technology is another enabling technology that provides the network resource management with great flexibility and scalability, where the instances of Virtual Network Functions (VNFs) are deployed in edge servers as Service Function Chains (SFCs) for SFC-enabled services. Although reliable service provisioning in MEC environments is fundamentally important, the deployed VNF instances usually are not reliable, which can be affected by their software implementation, their execution duration, the workload among edge servers, and so on. Empowered by digital twin techniques, the states of VNF instances can be maintained by their digital twins in a real-time manner and their reliability can be accurately predicted through their digital twins. In this paper, we study digital twin-assisted, SFC-enabled reliable service provisioning in MEC networks by exploiting the dynamics of VNF instance reliability. We concentrate on two novel optimization problems of reliable service provisioning: the service cost minimization problem, and the dynamic service admission maximization problem. We first show their NP-hardness. We then formulate an Integer Linear Program (ILP) solution, and devise an approximation algorithm with a constant approximation ratio for the service cost minimization problem. We third provide an ILP solution to the offline version of the dynamic service admission maximization problem. Built upon this offline ILP solution, we also develop an online algorithm with a provable competitive ratio for the problem, by adopting the primal-dual dynamic updating technique. We finally evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform their comparison benchmarks, and improve the performance of their comparison counterparts by no less than $10.2 \\%$.}
}


@article{DBLP:journals/tmc/ZhangWWSCL24,
	author = {Lei Zhang and
                  Ximing Wu and
                  Feng Wang and
                  Andy Sun and
                  Laizhong Cui and
                  Jiangchuan Liu},
	title = {Edge-Based Video Stream Generation for Multi-Party Mobile Augmented
                  Reality},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {409--422},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232543},
	doi = {10.1109/TMC.2022.3232543},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWSCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of mobile devices and the continuous advancement of mobile network technology, running online augmented reality (AR) on lightweight mobile devices is much more desirable than on heavy and expensive head-mounted devices that are difficult to satisfy users. Mobile edge computing can assist in supporting AR applications running on mobile devices, which copes with compute-intensive and delay-sensitive requirements. However, subject to the limited and heterogeneous edge resources, offloading tasks to edge devices is not easy, especially if the application requires multi-party interaction. It is challenging to develop a credible task placement scheme that satisfies user experience with flexible use of edge resources. This article focus on the task offloading placement problem for AR overlay rendering in multi-party mobile augmented reality system. We first present our observations about performance bottlenecks of edge devices and explain the necessity of splitting the AR overlay rendering pipeline. We then formulate a joint optimization problem of task placement decisions, aiming to maximize the user experience of quality and minimize the service cost. We develop a novel decision approach based on deep reinforcement learning (DRL) to address this complex problem. Finally, we verify the effectiveness and superiority of the proposed method through extensive evaluation experiments.}
}


@article{DBLP:journals/tmc/YuWHWXR24,
	author = {Xiangbin Yu and
                  Guangying Wang and
                  Xu Huang and
                  Kezhi Wang and
                  WeiYe Xu and
                  Yun Rui},
	title = {Energy Efficient Resource Allocation for Uplink RIS-Aided Millimeter-Wave
                  Networks With {NOMA}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {423--436},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3222392},
	doi = {10.1109/TMC.2022.3222392},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuWHWXR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, energy efficiency (EE) is maximized for the reconfigurable intelligent surface (RIS) aided millimeter-Wave (mmWave) networks with non-orthogonal multiple access (NOMA) and multiple mobile devices. To this end, we first propose the EE optimization, under the constraints of maximum power, minimal rate of devices and constant modulus of beamforming (BF) vectors. Then, the joint resource allocation scheme of power allocation (PA) and BF is designed. Specifically, given PA, an effective iterative algorithm based on the majorization-minimization, concave-convex procedure and block coordinate descent (BCD) is presented to obtain closed-form solutions of suboptimal passive BF (PBF) and analog BF (ABF) for each iteration. Then, given PBF and ABF, an effective iterative algorithm based on the successive convex approximation, BCD and Dinkelbach methods is derived to achieve suboptimal closed-form PA for each iteration. By incorporating these two algorithms into the BCD method, a joint optimization algorithm for EE maximization is presented. As a result, joint resource allocation of PA, PBF and ABF is attained. Besides, the convergence and complexity of the algorithms are analyzed. For comparison, the benchmark scheme based on the multidimensional search method and artificial bee colony algorithm is also presented. Simulation results show that the proposed joint scheme is effective and higher EE can be obtained with lower complexity.}
}


@article{DBLP:journals/tmc/LiXLGFL24,
	author = {Han Li and
                  Ke Xiong and
                  Yang Lu and
                  Bo Gao and
                  Pingyi Fan and
                  Khaled B. Letaief},
	title = {Energy-Efficient Coordinated Beamforming in Multi-Pair {MISO} Networks
                  With {CDI} and Eavesdroppers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {437--452},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3217121},
	doi = {10.1109/TMC.2022.3217121},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiXLGFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the energy-efficient coordinated beamforming design for multi-pair multiple-input single-output (MISO) networks with passive eavesdroppers. To be practical, it is assumed that only channel distribution information (CDI) of the network is known by the transmitters/sources, and the dynamic energy consumption model (DECM) is employed. In order to achieve a green network design, an energy efficiency (EE) maximization problem is formulated subjecting to the individual available power constraints, the rate outage probability constraints, and the information leakage probability constraints. To solve the formulated non-convex problem, semidefinite relaxation (SDR) and first-order lower bound are applied to transform the problem, and then an efficient algorithm is proposed based on successive convex approximation (SCA) and Dinkelbach's approaches. The proposed algorithm is theoretically proved to converge to a stationary point of the considered problem. Further, a distributed version of the proposed algorithm is designed, with which each transmitter is able to optimize its own beamforming vector with local CDI. Moreover, the computational complexities and the signaling overheads of the two developed algorithms are analyzed and compared. Simulation results show that both algorithms achieve good EE performance, and the EE performance achieved by the distributed algorithm is very similar to that achieved by the centralized one. Additionally, it is shown that similar to the conventional scenarios without eavesdroppers, the achieved system EE also has a saturation point w.r.t. the available power of the transmitters, and by employing our proposed algorithms, the network security is significantly enhanced.}
}


@article{DBLP:journals/tmc/YinZSC24,
	author = {Guolin Yin and
                  Junqing Zhang and
                  Guanxiong Shen and
                  Yingying Chen},
	title = {FewSense, Towards a Scalable and Cross-Domain Wi-Fi Sensing System
                  Using Few-Shot Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {453--468},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221902},
	doi = {10.1109/TMC.2022.3221902},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YinZSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wi-Fi sensing can classify human activities because each activity causes unique changes to the channel state information (CSI). Existing WiFi sensing suffers from limited scalability as the system needs to be retrained whenever new classes are added, which causes overheads of data collection and retraining. Cross-domain sensing may fail because the mapping between activities and CSI variations is destroyed when a different environment or user (domain) is involved. This paper proposed a few-shot learning-based WiFi sensing system, named FewSense, which can recognise novel classes in unseen domains with only a few samples. Specifically, a feature extractor was pre-trained offline using the source domain data. When the system was applied in the target domain, a few samples were used to fine-tune the feature extractor for domain adaptation. Inference was made by computing the cosine similarity. FewSense can further boost the classification accuracy by collaboratively fusing inference from multiple receivers. We evaluated the performance of FewSense using three public datasets, i.e., SignFi, Widar, and Wiar. The results show that FewSense with five-shot learning recognised novel classes in unseen domains with an accuracy of 93.9%, 96.5%, and 82.7% on the SignFi, Widar, and Wiar datasets, respectively. Our collaborative sensing model improved system performance by an average of 29.2%.}
}


@article{DBLP:journals/tmc/HuangZWCLL24,
	author = {Shaoyuan Huang and
                  Heng Zhang and
                  Xiaofei Wang and
                  Min Chen and
                  Jianxin Li and
                  Victor C. M. Leung},
	title = {Fine-Grained Spatio-Temporal Distribution Prediction of Mobile Content
                  Delivery in 5G Ultra-Dense Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {469--482},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3226448},
	doi = {10.1109/TMC.2022.3226448},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZWCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The 5G networks have extensively promoted the growth of mobile users and novel applications, and with the skyrocketing user requests for a large amount of popular content, the consequent content delivery services (CDSs) have been bringing a heavy load to mobile service providers. As a key mission in intelligent networks management, understanding and predicting the distribution of CDSs benefits many tasks of modern network services such as resource provisioning and proactive content caching for content delivery networks. However, the revolutions in novel ubiquitous network architectures led by ultra-dense networks (UDNs) make the task extremely challenging. Specifically, conventional methods face the challenges of insufficient spatio precision, lacking generalizability, and complex multi-feature dependencies of user requests, making their effectiveness unreliable in CDSs prediction under 5G UDNs. In this article, we propose to adopt a series of encoding and sampling methods to model CDSs of known and unknown areas at a tailored fine-grained level. Moreover, we design a spatio-temporal-social multi-feature extraction framework for CDSs hotspots prediction, in which a novel edge-enhanced graph convolution block is proposed to encode dynamic CDSs networks based on the social relationships and the spatio features. Besides, we introduce the Long-Short Term Memory (LSTM) to further capture the temporal dependency. Extensive performance evaluations with real-world measurement data collected in two mobile content applications demonstrate the effectiveness of our proposed solution, which can improve the prediction area under the curve (AUC) by 40.5% compared to the state-of-the-art proposals at a spatio granularity of 76m, with up to 80% of the unknown areas.}
}


@article{DBLP:journals/tmc/YangXWCHMBHLJL24,
	author = {Chengxu Yang and
                  Mengwei Xu and
                  Qipeng Wang and
                  Zhenpeng Chen and
                  Kang Huang and
                  Yun Ma and
                  Kaigui Bian and
                  Gang Huang and
                  Yunxin Liu and
                  Xin Jin and
                  Xuanzhe Liu},
	title = {{FLASH:} Heterogeneity-Aware Federated Learning at Scale},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {483--500},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3214234},
	doi = {10.1109/TMC.2022.3214234},
	timestamp = {Mon, 03 Mar 2025 22:25:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangXWCHMBHLJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) becomes a promising machine learning paradigm. The impact of heterogeneous hardware specifications and dynamic states on the FL process has not yet been studied systematically. This paper presents the first large-scale study of this impact based on real-world data collected from 136k smartphones. We conducted extensive experiments on our proposed heterogeneity-aware FL platform namely FLASH, to systematically explore the performance of state-of-the-art FL algorithms and key FL configurations in heterogeneity-aware and -unaware settings, finding the following. (1) Heterogeneity causes accuracy to drop by up to 9.2% and convergence time to increase by 2.32×. (2) Heterogeneity negatively impacts popular aggregation algorithms, e.g., the accuracy variance reduction brought by q-FedAvg drops by 17.5%. (3) Heterogeneity does not worsen the accuracy loss caused by gradient-compression algorithms significantly, but it compromises the convergence time by up to 2.5×. (4) Heterogeneity hinders client-selection algorithms from selecting wanted clients, thus reducing effectiveness. e.g., the accuracy increase brought by the state-of-the-art client-selection algorithm drops by 73.9%. (5) Heterogeneity causes the optimal FL hyper-parameters to drift significantly. More specifically, the heterogeneity-unaware setting favors looser deadline and higher reporting fraction to achieve better training performance. (6) Heterogeneity results in non-trivial failed clients (more than 10%) and leads to participation bias (the top 30% of clients contribute 86% of computations). Our FLASH platform and data have been publicly open sourced.}
}


@article{DBLP:journals/tmc/JiangHOLMCSLJBP24,
	author = {Xiaopeng Jiang and
                  Han Hu and
                  Thinh On and
                  Phung Lai and
                  Vijaya Datta Mayyuri and
                  An M. Chen and
                  Devu M. Shila and
                  Adriaan Larmuseau and
                  Ruoming Jin and
                  Cristian Borcea and
                  NhatHai Phan},
	title = {FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {501--519},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3223578},
	doi = {10.1109/TMC.2022.3223578},
	timestamp = {Sun, 06 Oct 2024 21:41:31 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangHOLMCSLJBP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the problem of non-Independent and Identically Distributed data. A sentiment analysis model is also used to demonstrate that FLSys effectively supports concurrent models. This article reports our experience and lessons learned from conducting extensive experiments using simulations, Android/Linux emulations, and Android phones that demonstrate FLSys achieves good model utility and practical system performance.}
}


@article{DBLP:journals/tmc/CaoLJCZLL24,
	author = {Hangcheng Cao and
                  Daibo Liu and
                  Hongbo Jiang and
                  Chao Cai and
                  Tianyue Zheng and
                  John C. S. Lui and
                  Jun Luo},
	title = {HandKey: Knocking-Triggered Robust Vibration Signature for Keyless
                  Unlocking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {520--534},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3216868},
	doi = {10.1109/TMC.2022.3216868},
	timestamp = {Wed, 10 Jul 2024 16:47:46 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLJCZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Door lock is regarded as a critical line of defending the privacy and security of personal areas. However, for inner doors in environments like factories, existing locking mechanisms can be poor in user-friendliness and high in cost. For instance, mechanical locks require carrying keys that inevitably compromise user experiences, while smart locks always require non-trivial sensors. Therefore, inner doors urgently require a lightweight unlocking scheme that can properly balance user-friendliness, cost, and security. To this end, we propose HandKey as a keyless unlocking scheme to supplement existing lock systems. HandKey relies on two principles: the simplicity of hand knocking doors and the uniqueness of vibration triggered by the knocking force. In other words, a door and a hand knocking it jointly form a unique physical system that generates hand-dependent and user-specific vibration signatures uniquely representing a user identity. In designing HandKey, we first analyze the vibration mechanism behind it and the impacts of gestures and door materials on vibration signatures. Then we innovatively construct a signal processing and deep learning-based pipeline to extract signatures robust to variable knocking behaviors for representing user identity. Finally, we implement a HandKey prototype and use extensive evaluation to demonstrate its security and effectiveness.}
}


@article{DBLP:journals/tmc/LiuSQWCHS24,
	author = {Zhicheng Liu and
                  Jinduo Song and
                  Chao Qiu and
                  Xiaofei Wang and
                  Xu Chen and
                  Qiang He and
                  Hao Sheng},
	title = {Hastening Stream Offloading of Inference via Multi-Exit DNNs in Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {535--548},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3218724},
	doi = {10.1109/TMC.2022.3218724},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuSQWCHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the primary driver of intelligent mobile applications, deep neural networks (DNNs) have gradually deployed to millions of mobile devices, producing massive latency-sensitive and computation-intensive tasks daily. Mobile edge computing facilitates the deployment of computing resources at the edge, which enables fine-grained offloading of DNN inference tasks from mobile devices to edge nodes. However, most existing studies have not systematically considered three crucial performance aspects: scheduling multiple streams of DNN inference tasks, leveraging multi-exit models to hasten task processing, and partitioning inference models for partial offloading. To this end, this paper proposes an adaptive inference framework in mobile edge computing, which can dynamically select the exit point and partition point for multiple inference task streams. We design a dynamic programming algorithm to obtain an efficient solution under the ideal condition that task arrival information is known. Further, we design a learning-based algorithm for online scheduling, whose training efficiency is improved based on historical experience initialization and priority experience replay. Experimental results show that compared with the Greedy algorithm, the online algorithm improves the performance on two environmental parameters by an average of 5.9% and 32%, respectively.}
}


@article{DBLP:journals/tmc/YeF24,
	author = {Xiaowen Ye and
                  Liqun Fu},
	title = {Joint {MCS} Adaptation and {RB} Allocation in Cellular Networks Based
                  on Deep Reinforcement Learning With Stable Matching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {549--565},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3213837},
	doi = {10.1109/TMC.2022.3213837},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YeF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Joint modulation-coding scheme (MCS) adaptation and resource block (RB) allocation is an effective approach to guarantee different quality of service (QoS) requirements of all UEs under dynamic network environments. In this article, we consider a fifth generation (5G) cellular network with time-varying wireless channels, in which the BS serves multiple user equipments (UEs) under limited available RBs. We aim to minimize the total RB consumption subject to the rigorous constraints of each UE's QoS requirement. To attain this objective, this paper puts forth an online learning technique, referred to as integrated Deep Reinforcement learning and stable Matching (DeepRM), in the sense that the MCS adaptation and RB allocation decisions are conducted without acquiring the real-time channel quality indicator (CQI) feedback. DeepRM is a closed-loop framework, where the output of deep reinforcement learning (DRL) is imported into the stable matching to guide optimal RB allocation whilst the output of stable matching is fed into the DRL framework to assist efficient MCS decision-making. Specifically, in DeepRM, we first develop a powerful DRL algorithm, termed as Action-and-Reward Branching Deep Q-network (ARBDQ), by incorporating the action branch architecture into conventional DRL and modifying the traditional deep neural network training mechanism, to perform judicious MCS decisions on different links in parallel. Then, a new many-to-one stable matching algorithm, called adaptive deferred acceptance, is exploited to dynamically adjust the RB quota of each UE in a computationally efficient fashion. Simulation results demonstrate that compared with ACO-HM, OLLA-ADA, and ARBDQ-Random algorithms, DeepRM induces much less RB consumption while guaranteeing the QoS requirements of all UEs in various network scenarios. Furthermore, under miscellaneous QoS requirement, number of UEs, and CQI reporting period setups, DeepRM is more robust than other baselines.}
}


@article{DBLP:journals/tmc/ZhouTYDS24,
	author = {Jianshan Zhou and
                  Daxin Tian and
                  Yaqing Yan and
                  Xuting Duan and
                  Xuemin Shen},
	title = {Joint Optimization of Mobility and Reliability-Guaranteed Air-to-Ground
                  Communication for UAVs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {566--580},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3228870},
	doi = {10.1109/TMC.2022.3228870},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouTYDS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aerial unmanned vehicles (UAVs) play a significant role in improving the connectivity and coverage of terrestrial communication networks. However, UAV-assisted air-to-ground (A2G) data transmissions usually encounter several fundamental challenges, such as terminal mobility, random nature in channel fading and contention, resource constraints, and application-specific transmission requirements. To tackle these challenges, we formulate a bi-level optimization problem that jointly considers the control of the UAV mobility and transmission power and the scheduling of A2G data transmissions. The objective is to optimize energy consumption and maximize A2G transmission reliability. Particularly, we first theoretically characterize the A2G transmission reliability from a probabilistic perspective concerning the effects of channel fading, channel access contention, and application requirements. We then derive a closed-form expression for the optimal expected transmission reliability. Using the closed-form reliability, we transform the bi-level optimization into a mathematically-tractable optimal control problem and propose an efficient iterative algorithm to solve it. Simulation results show that our approach provides a comprehensive improvement in terms of both energy utilization and A2G transmission reliability, in particular, with a reduction of more than 12.1% in energy consumption and an increase of 7.53% in reliability on average, compared to several baselines.}
}


@article{DBLP:journals/tmc/MeterizYildiranYKM24,
	author = {{\"{U}}lk{\"{u}} Meteriz{-}Yildiran and
                  Necip Fazil Yildiran and
                  Joongheon Kim and
                  David Mohaisen},
	title = {Learning Location From Shared Elevation Profiles in Fitness Apps:
                  {A} Privacy Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {581--596},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3218148},
	doi = {10.1109/TMC.2022.3218148},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MeterizYildiranYKM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The extensive use of smartphones and wearable devices has facilitated many useful applications. For example, with Global Positioning System (GPS)-equipped smart and wearable devices, many applications can gather, process, and share rich metadata, such as geolocation, trajectories, elevation, and time. For example, fitness applications, such as Runkeeper and Strava, utilize the information for activity tracking and have recently witnessed a boom in popularity. Those fitness tracker applications have their own web platforms and allow users to share activities on such platforms or even with other social network platforms. To preserve the privacy of users while allowing sharing, several of those platforms may allow users to disclose partial information, such as the elevation profile for an activity, which supposedly would not leak the location of the users. In this work, and as a cautionary tale, we create a proof of concept where we examine the extent to which elevation profiles can be used to predict the location of users. To tackle this problem, we devise three plausible threat settings under which the city or borough of the targets can be predicted. Those threat settings define the amount of information available to the adversary to launch the prediction attacks. Establishing that simple features of elevation profiles, e.g., spectral features, are insufficient, we devise both natural language processing (NLP)-inspired text-like representation and computer vision-inspired image-like representation of elevation profiles, and we convert the problem at hand into text and image classification problem. We use both traditional machine learning- and deep learning-based techniques and achieve a prediction success rate ranging from 59.59% to 99.80%. The findings are alarming, highlighting that sharing elevation information may have significant location privacy risks.}
}


@article{DBLP:journals/tmc/CaoLJL24,
	author = {Hangcheng Cao and
                  Daibo Liu and
                  Hongbo Jiang and
                  Jun Luo},
	title = {MagSign: Harnessing Dynamic Magnetism for User Authentication on IoT
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {597--611},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3216851},
	doi = {10.1109/TMC.2022.3216851},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User authentication is a critical module to achieve security and privacy protections, especially for pervasive Internet of Things (IoT) deployments. However, existing methods on IoT devices are significantly short of implementability thanks to the lack of device uniformity and protocol openness. For instance, password becomes useless for devices void of text entry interfaces. Biometrics may not scale well as they require both non-trivial sensors and cumbersome user involvement. Proximity-based methods exploiting shared ambient contexts are vulnerable to co-located malicious attacks. Therefore, a low-cost authentication scheme widely implementable on heterogeneous IoT devices is urgently demanded. To this end, we propose MagSign that leverages two fundamental capabilities owned by common IoT devices: the ubiquity of magnetic induction sensors and the power of screens to change magnetic field. Essentially, MagSign controls screen contents of an authorized device (possessed by a user) to generate specific currents in its electronic components that in turn induce a magnetic signature. This signature, sensed by a nearby device, allows the user to be authenticated and hence to unlock that device. In designing MagSign, we explore critical parameters employable to magnetic signature generation by analyzing electronic components’ workflow. Moreover, we innovatively encode binary sequences into magnetic intensity transitions, so that a sequence issued from a trusted server can be converted into a magnetic signature. Different from existing proximity-based approaches relying on shared static environment information, magnetic signature is directly derived from a server-issued sequence, allowing for dynamic signature generation that effectively thwarts potential attacks. The comprehensive experiments show MagSign has a false acceptance rate (FAR) of 0.38% and a false rejection rate (FRR) of 3.13%.}
}


@article{DBLP:journals/tmc/LinHDYWWZ24,
	author = {Chi Lin and
                  Shibo Hao and
                  Haipeng Dai and
                  Wei Yang and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Maximizing Charging Efficiency With Fresnel Zones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {612--629},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3216817},
	doi = {10.1109/TMC.2022.3216817},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinHDYWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefitting from the discovery of wireless power transfer (WPT) technology, the wireless rechargeable sensor network (WRSN) has become a promising way for lifetime extension for wireless sensor networks. In practical WRSN scenarios, obstacles can be found almost everywhere. Most state-of-the-art researches believe that obstacles will always degrade signal strength, and omit the influence of obstacles for simplifying the computation process. However, overlooking the positive impacts of obstacles on signal propagation is inconsistent with the intrinsic features of electromagnetic waves. To address this issue, in this paper, we explore the wireless signal propagation process and provide a theoretical charging model to enhance the charging efficiency by leveraging obstacles. Through utilizing the concept of the Fresnel Zone model, we re-formalize the wireless charging model and discretize the charging area and charging time to determine the best charging locations as well as charging duration. We model the charging Efficiency Maximization with Obstacles (EMO) problem as a submodular function maximization problem and propose a cost-efficient algorithm to solve it. Finally, test-bed experiments and extensive simulations are both conducted to verify that our schemes outperform baseline algorithms by 33.46\\%\non average in charging efficiency improvement.}
}


@article{DBLP:journals/tmc/HuDE24,
	author = {Jie Hu and
                  Vishwaraj Doshi and
                  Do Young Eun},
	title = {Minimizing File Transfer Time in Opportunistic Spectrum Access Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {630--644},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3212926},
	doi = {10.1109/TMC.2022.3212926},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuDE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the file transfer problem in opportunistic spectrum access (OSA) model, which has been widely studied in throughput-oriented applications for max-throughput strategies and in delay-related works that commonly assume identical channel rates and fixed file sizes. Our work explicitly considers minimizing the file transfer time for a given file in a set of heterogeneous-rate Bernoulli channels, showing that max-throughput policy doesn't minimize file transfer time in general. We formulate a mathematical framework for static extend to dynamic policies by mapping our file transfer problem to a stochastic shortest path problem. We analyze the performance of our proposed static and dynamic optimal policies over the max-throughput policy. We propose a mixed-integer programming formulation as an efficient alternative way to obtain the dynamic optimal policy and show a huge reduction in computation time. Then, we propose a heuristic policy that takes into account the performance-complexity tradeoff and consider the online implementation with unknown channel parameters. Furthermore, we present numerical simulations to support our analytical results and discuss the effect of switching delay on different policies. Finally, we extend the file transfer problem to Markovian channels and demonstrate the impact of the correlation of each channel.}
}


@article{DBLP:journals/tmc/DuanLWCLDS24,
	author = {Sijing Duan and
                  Feng Lyu and
                  Huaqing Wu and
                  Wenxiong Chen and
                  Huali Lu and
                  Zhe Dong and
                  Xuemin Shen},
	title = {{MOTO:} Mobility-Aware Online Task Offloading With Adaptive Load Balancing
                  in Small-Cell {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {645--659},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3220720},
	doi = {10.1109/TMC.2022.3220720},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuanLWCLDS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing is a promising computing paradigm enabling mobile devices to offload computation-intensive tasks to nearby edge servers. However, within small-cell networks, the user mobilities can result in uneven spatio-temporal loads, which have not been well studied by considering adaptive load balancing, thus limiting the system performance. Motivated by the data analytics and observations on a real-world user association dataset in a large-scale WiFi system, in this paper, we investigate the mobility-aware online task offloading problem with adaptive load balancing to minimize the total computation costs. However, the problem is intractable directly without prior knowledge of future user mobility behaviors and spatio-temporal computation loads of edge servers. To tackle this challenge, we transform and decompose the original task offloading optimization problem into two sub-problems, i.e., task offloading control (ToC) and server grouping (SeG). Then, we devise an online control scheme, named MOTO (i.e., Mobility-aware Online Task Offloading), which consists of two components, i.e., Long Short Term Memory based algorithm and Dueling Double DQN based algorithm, to efficiently solve the ToC and SeG sub-problems, respectively. Extensive trace-driven experiments are carried out and the results demonstrate the effectiveness of MOTO in reducing computational costs of mobile devices and achieving load balancing when compared to the state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/LiuHWXWWWQ24,
	author = {Bingyi Liu and
                  Weizhen Han and
                  Enshu Wang and
                  Shengwu Xiong and
                  Libing Wu and
                  Qian Wang and
                  Jianping Wang and
                  Chunming Qiao},
	title = {Multi-Agent Attention Double Actor-Critic Framework for Intelligent
                  Traffic Light Control in Urban Scenarios With Hybrid Traffic},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {660--672},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3233879},
	doi = {10.1109/TMC.2022.3233879},
	timestamp = {Wed, 08 Jan 2025 08:46:44 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHWXWWWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world urban environments, hybrid and disorder traffic brings new challenges for the intelligent traffic light control system (ITLCS). Apart from coordinating traffic flows around intersections, the ITLCS is responsive to ensuring high priority vehicles pass through intersections quickly. To this end, we formulate the multiple intersections’ decision-making problem as a Semi-Markov game and propose a multi-agent attention double actor-critic (MAADAC) framework to solve this game, integrating the options framework with graph attention networks (GATs). Specifically, the options framework empowers agents to learn to make a long sequence of satisfactory decisions, such as keeping a reasonable phase for a short period to ensure high priority vehicles pass through intersections quickly. Besides, we adopt GATs to capture graph-structure mutual influences among agents. We set up a simulator based on real-world city road networks and conduct extensive experiments to evaluate the performance of MAADAC. The experimental results show that MAADAC can reduce high priority vehicles’ waiting time in the interval of 18.16%-38.14% versus the density of vehicles in real-world urban scenarios over several state-of-the-art approaches. Also, our framework can guarantee the passing efficiency of high priority vehicles under various traffic conditions with the change in the proportion of high priority vehicles.}
}


@article{DBLP:journals/tmc/WuKC24,
	author = {Sau{-}Hsuan Wu and
                  Chun{-}Hsien Ko and
                  Hsi{-}Lu Chao},
	title = {On-Demand Coordinated Spectrum and Resource Provisioning Under an
                  Open {C-RAN} Architecture for Dense Small Cell Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {673--688},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3215488},
	doi = {10.1109/TMC.2022.3215488},
	timestamp = {Sat, 13 Jan 2024 17:37:21 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile networks have seen more than a thousand times traffic increases in the past decade. Network operators face an ever increasing demand and cost to deploy denser networks in order to maintain the quality of services. In view of this paradigm shift, an open cloud radio access network (C-RAN) service model is studied in this work. Integrating the cross-layer functions of spectrum resource sharing, channel and power allocation, and interference management in a C-RAN architecture, we explore the feasibility of providing a scalable yet efficient broadband wireless service with dense small cell networks (DSN). The proposed methods and architecture can be applied to DSN that support the functions of (further enhanced) inter-cell interference cancelation (feICIC/ICIC) techniques of 3GPP standard, and have the potential to provide a cost-effective solution for high-quality DSN. Simulation results across an area of 100 km^{2}\nshow that the proposed scheme can offer an aggregate downlink throughput of 21 Gbps over a maximum channel bandwidth of 20 MHz, which is 5 times the throughput with a typical ICIC method. Moreover, the service satisfaction degree can reach 85% under the proposed C-RAN architecture, making it a promising and cost-effective solution for future broadband wireless services.}
}


@article{DBLP:journals/tmc/AhaniY24,
	author = {Ghafour Ahani and
                  Di Yuan},
	title = {Optimal Content Caching and Recommendation With Age of Information},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {689--704},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3213782},
	doi = {10.1109/TMC.2022.3213782},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AhaniY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Content caching at the network edge is an effective way of mitigating backhaul load and improving user experience. Caching efficiency can be enhanced by content recommendation and by keeping the information fresh. By content recommendation, a requested content that is not in the cache can be alternatively satisfied by a related cached content recommended by the system. Information freshness can be quantified by age of information (AoI). This article has the following contributions. First, we address optimal scheduling of cache updates for a time-slotted system accounting for content recommendation and AoI, and to the best of our knowledge, there is no work that has jointly taken into account these aspects. Next, we rigorously prove the problem's NP-hardness. Then, we derive an integer linear formulation, by which the optimal solution can be obtained for small-scale scenarios. On the algorithmic side, our contributions include the development of an effective algorithm based on Lagrangian decomposition, and efficient algorithms for solving the resulting subproblems. Our algorithm computes a bound that can be used to evaluate the performance of any suboptimal solution. We conduct simulations to show the effectiveness of our algorithm.}
}


@article{DBLP:journals/tmc/TutuncuogluD24,
	author = {Feridun T{\"{u}}t{\"{u}}nc{\"{u}}oglu and
                  Gy{\"{o}}rgy D{\'{a}}n},
	title = {Optimal Service Caching and Pricing in Edge Computing: {A} Bayesian
                  Gaussian Process Bandit Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {705--718},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221465},
	doi = {10.1109/TMC.2022.3221465},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TutuncuogluD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the emergence of function-as-a-service (FaaS) as a programming abstraction for edge computing, we consider the problem of caching and pricing applications for edge computation offloading in a dynamic environment where Wirelesss Devices (WDs) can be active or inactive at any point in time. We model the problem as a single leader multiple-follower Stackelberg game, where the service operator is the leader and decides what applications to cache and how much to charge for their use, while the WDs are the followers and decide whether or not to offload their computations. We show that the WDs’ interaction can be modeled as a player-specific congestion game and show the existence and computability of equilibria. We then show that under perfect and complete information the equilibrium price of the service operator can be computed in polynomial time for any cache placement. For the incomplete information case, we propose a Bayesian Gaussian Process Bandit algorithm for learning an optimal price for a cache placement and provide a bound on its asymptotic regret. We then propose a Gaussian process approximation-based greedy heuristic for computing the cache placement. We use extensive simulations to evaluate the proposed learning scheme, and show that it outperforms state of the art algorithms by up to 50% at little computational overhead.}
}


@article{DBLP:journals/tmc/YarkinaGMK24,
	author = {Natalia Yarkina and
                  Anna Gaydamaka and
                  Dmitri Moltchanov and
                  Yevgeni Koucheryavy},
	title = {Performance Assessment of an {ITU-T} Compliant Machine Learning Enhancements
                  for 5G {RAN} Network Slicing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {719--736},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3228286},
	doi = {10.1109/TMC.2022.3228286},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YarkinaGMK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a technique introduced by 3GPP to enable multi-tenant operation in 5G systems. However, the support of slicing at the air interface requires not only efficient optimization algorithms operating in real time but also its tight integration into the 5G control plane. In this paper, we first present a priority-based mechanism enabling defined performance isolation among slices competing for resources. Then, to speed up the resource arbitration process, we propose and compare several supervised machine learning (ML) techniques. We show how to embed the proposed approach into the ITU-T standardized ML architecture. The proposed ML enhancement is evaluated under realistic traffic conditions with respect to the performance criteria defined by GSMA while explicitly accounting for 5G millimeter wave channel conditions. Our results show that ML techniques are able to provide suitable approximations for the resource allocation process ensuring slice performance isolation, efficient resource use, and fairness. Among the considered algorithms, polynomial regressions show the best results outperforming the exact solution algorithm by 5–6 orders of magnitude in terms of execution time and both neural network and random forest algorithms in terms of accuracy (by 20–40 %), sensitiveness to workload variations and training sample size. Finally, ML algorithms are generally prone to service level agreements (SLA) violation under high load and time-varying channel conditions, implying that an SLA enforcement system is needed in ITU-T's 5G ML framework.}
}


@article{DBLP:journals/tmc/OkegbileCA24,
	author = {Samuel Dayo Okegbile and
                  Jun Cai and
                  Attahiru Sule Alfa},
	title = {Practical Byzantine Fault Tolerance-Enhanced Blockchain-Enabled Data
                  Sharing System: Latency and Age of Data Package Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {737--753},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3223306},
	doi = {10.1109/TMC.2022.3223306},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/OkegbileCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data timeliness, privacy, and security are key enablers for data-sharing systems to support time-sensitive and mission-critical systems and applications. While blockchain-enabled data sharing frameworks can offer reliable security and privacy when properly implemented, the timeliness of data and the related latency are important issues that can limit the adoption of blockchain in large-scale mission-critical applications. This paper thus carried out a performance analysis of the blockchain-enabled data-sharing framework from latency and data age perspectives to investigate the suitability of blockchain technology in data sharing systems. To achieve this, the uniqueness of such systems such as transactions validation latency, transaction generation rate, waiting time, blockchain-appending rate, and overall communication latency were jointly studied. The communication latency was characterized following the spatiotemporal modeling approach. We further adopted the practical Byzantine fault tolerance (PBFT) consensus protocol due to its well discussed suitability in large-scale data sharing applications and captured the validation stages of such a PBFT scheme using the Erlang distribution of order\nk\n. Simulations results show that various influential system parameters must be carefully considered when adopting blockchain technology in time-sensitive data sharing applications. This will guide the adoption of blockchain technology in various data sharing applications and systems.}
}


@article{DBLP:journals/tmc/ChenMGC24,
	author = {Ziya Chen and
                  Qian Ma and
                  Lin Gao and
                  Xu Chen},
	title = {Price Competition in Multi-Server Edge Computing Networks Under {SAA}
                  and {SIQ} Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {754--768},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3227675},
	doi = {10.1109/TMC.2022.3227675},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenMGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of edge computing, many business entities deploy their own edge servers to compete for users, which forms multi-server edge computing networks. However, no prior work studies the competition among heterogeneous edge servers and how the competition affects users’ selfish computation offloading behaviors in such a network from an economic perspective. In this paper, we model the interactions between edge servers and users as a two-stage game. In Stage I, edge servers with heterogeneous marginal costs set their service prices to compete for users, and in Stage II, each user selfishly offloads its task to one of the edge servers or the remote cloud. Analyzing the equilibrium of the two-stage game is challenging due to edge servers’ heterogeneity and the congestion effect caused by resource sharing among users. We first investigate the equilibrium when edge servers follow the serve-as-arrive (SAA) model (i.e., serving all offloaded tasks simultaneously), and then extend our analysis to the serve-in-queue (SIQ) model (i.e., serving offloaded tasks one by one following the M/M/1 queue rule). Under the SAA model, we prove that users’ selfish computation offloading game in Stage II is a potential game and admits a unique Nash equilibrium (NE), for which we derive the explicit expressions. Furthermore, for edge servers’ price competition game in Stage I, we characterize the conditions for the uniqueness of the NE and derive its explicit expression. Under the SIQ model, we derive the unique NE of users’ selfish computation offloading game, and show that the NE of edge servers’ price competition game may not always exist. We compare the equilibrium under the two service models and show that at equilibrium, edge servers with low marginal costs can achieve higher profits under the SIQ model when edge servers’ computation capacity is large or the delay incurred on the cloud is moderate; however, edge servers with high marginal costs can obtain higher profits under the SAA model in most cases.}
}


@article{DBLP:journals/tmc/ChenZWHS24,
	author = {Ying Chen and
                  Jie Zhao and
                  Yuan Wu and
                  Jiwei Huang and
                  Xuemin Shen},
	title = {QoE-Aware Decentralized Task Offloading and Resource Allocation for
                  End-Edge-Cloud Systems: {A} Game-Theoretical Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {769--784},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3223119},
	doi = {10.1109/TMC.2022.3223119},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZWHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the limited computing resource and battery capability at the mobile devices, the computation-intensive tasks generated by mobile devices can be offloaded to edge servers or cloud for processing. In this paper, we study the multi-user task offloading problem in an end-edge-cloud system, in which all user devices compete for the limited communication and computing resources. Particularly, we first formulate the offloading problem with the goal of maximizing the Quality of Experience (QoE) of the users subject to resource constraints. Since each user focuses on maximizing its own QoE, we reformulate the problem as a Multi-User Task Offloading Game (MUTO-Game). We then identify an important property that for any device, both the communication interference and the degree of computing resource competition can be upper bounded. Based on the property, we further theoretically prove that there exists at least one Nash Equilibrium offloading strategy in the MUTO-Game. We propose the Game-based Decentralized Task Offloading (GDTO) approach to obtain the Nash Equilibrium offloading strategy. Finally, we analyze the upper bound for the convergence time and characterize the performance guarantee of the obtained offloading strategy for the worst case. A series of experimental results are presented, in comparison with both the centralized optimal approach and the approximate approaches.}
}


@article{DBLP:journals/tmc/ChikhaMAJ24,
	author = {Wassim Ben Chikha and
                  Marie Masson and
                  Zwi Altman and
                  Sana Ben Jemaa},
	title = {Radio Environment Map Based Inter-Cell Interference Coordination for
                  Massive-MIMO Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {785--796},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3222763},
	doi = {10.1109/TMC.2022.3222763},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChikhaMAJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive-MIMO (M-MIMO) allows to schedule users with high gain narrow beams and to reduce intra-cell inter-beam interference. Close to cell edge, users may experience high interference from beams of neighboring cells which degrades their performance. This paper introduces low complexity inter-cell interference coordination between neighboring cells for M-MIMO systems implementing Grid of Beams (GoB) using Radio Environment Maps (REMs). The REMs are designed using the Kriging with the Covariance Tapering spatial interpolation technique and are created for the serving and the interfering beams. The interference coordination is introduced as constraints to the schedulers that avoid simultaneous scheduling of users served by highly interfering beams from the neighboring cells. The coordination decision is based on information retrieved from REMs and its quality depends on the REMs’ precision. Around 72 percent performance gain in terms of mean user throughput is achieved by the REM based coordination with respect to a baseline solution without coordination, and around 40 percent gain with respect to a state of the art solution that implements coordination using information at beam level resolution.}
}


@article{DBLP:journals/tmc/MaZLLWWG24,
	author = {Zijing Ma and
                  Shigeng Zhang and
                  Jia Liu and
                  Xuan Liu and
                  Weiping Wang and
                  Jianxin Wang and
                  Song Guo},
	title = {RF-Siamese: Approaching Accurate {RFID} Gesture Recognition With One
                  Sample},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {797--811},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3217487},
	doi = {10.1109/TMC.2022.3217487},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaZLLWWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing accurate sensing in diverse environments is a challenging issue in wireless sensing technologies. Existing solutions usually require collecting a large number of samples to train a classifier for every environment, or further assume similar sample distribution between different environments such that a model trained in one environment can be transferred to another. In this paper, we propose RF-Siamese, an RFID-based gesture sensing approach that achieves comparable accuracy to existing solutions but requires only a few samples in each eivironment. RF-Siamese leverages Siamese networks to distinguish different gestures with only a small number of samples and is enhanced by several novel designs to achieve high accuracy in diverse environments. First, the network structure and parameters (e.g., loss function and distance metric) are carefully designed to be suitable for RFID gesture recognition. Second, a permutation-based dataset generation strategy is proposed to make full use of the collected samples to enhance the recognition accuracy. Third, a template matching method is proposed to extend the Siamese network to classify multiple gestures. Extensive experiments on commercial RFID devices demonstrate that RF-Siamese achieves a high accuracy of 0.93 with only one sample of each gesture when recognizing 18 different gestures, while state-of-the-art approaches based on transfer learning and meta learning achieve an accuracy of only 0.59 and 0.70, respectively.}
}


@article{DBLP:journals/tmc/ZhangGGG24,
	author = {Zhenxiao Zhang and
                  Zhidong Gao and
                  Yuanxiong Guo and
                  Yanmin Gong},
	title = {Scalable and Low-Latency Federated Learning With Cooperative Mobile
                  Edge Networking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {812--822},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3216837},
	doi = {10.1109/TMC.2022.3216837},
	timestamp = {Mon, 03 Mar 2025 22:25:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables collaborative model training without centralizing data. However, the traditional FL framework is cloud-based and suffers from high communication latency. On the other hand, the edge-based FL framework that relies on an edge server co-located with mobile base station for model aggregation has low communication latency but suffers from degraded model accuracy due to the limited coverage of edge server. In light of high-accuracy but high-latency cloud-based FL and low-latency but low-accuracy edge-based FL, this paper proposes a new FL framework based on cooperative mobile edge networking called cooperative federated edge learning (CFEL) to enable both high-accuracy and low-latency distributed intelligence at mobile edge networks. Considering the unique two-tier network architecture of CFEL, a novel federated optimization method dubbed cooperative edge-based federated averaging (CE-FedAvg) is further developed, wherein each edge server both coordinates collaborative model training among the devices within its own coverage and cooperates with other edge servers to learn a shared global model through decentralized consensus. Experimental results based on benchmark datasets show that CFEL can largely reduce the training time to achieve a target model accuracy compared with prior FL frameworks.}
}


@article{DBLP:journals/tmc/YangZX24,
	author = {Jianfei Yang and
                  Han Zou and
                  Lihua Xie},
	title = {SecureSense: Defending Adversarial Attack for Secure Device-Free Human
                  Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {823--834},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3226742},
	doi = {10.1109/TMC.2022.3226742},
	timestamp = {Sun, 21 Jan 2024 13:01:05 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks have empowered accurate device-free human activity recognition, which has wide applications. Deep models can extract robust features from various sensors and generalize well even in challenging situations such as data-insufficient cases. However, these systems could be vulnerable to input perturbations, i.e., adversarial attacks. We empirically demonstrate that both black-box Gaussian attacks and modern adversarial white-box attacks can render their accuracies to plummet. In this paper, we first point out that such phenomenon can bring severe safety hazards to device-free sensing systems, and then propose a novel learning framework, SecureSense, to defend common attacks. SecureSense aims to achieve consistent predictions regardless of whether there exists an attack on its input or not, alleviating the negative effect of distribution perturbation caused by adversarial attacks. Extensive experiments demonstrate that our proposed method can significantly enhance the model robustness of existing deep models, overcoming possible attacks. The results validate that our method works well on wireless human activity recognition and person identification systems. To the best of our knowledge, this is the first work to investigate adversarial attacks and further develop a novel defense framework for wireless human activity recognition in mobile computing research.}
}


@article{DBLP:journals/tmc/HuXHCDP24,
	author = {Menglan Hu and
                  Mai Xiao and
                  Yi Hu and
                  Chao Cai and
                  Tianping Deng and
                  Kai Peng},
	title = {Software Defined Multicast Using Segment Routing in {LEO} Satellite
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {835--849},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3215976},
	doi = {10.1109/TMC.2022.3215976},
	timestamp = {Mon, 22 Jul 2024 14:29:43 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuXHCDP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging low earth orbit (LEO) broadband satellite networks are creating new opportunities to enable superior video distribution. With numerous satellites deployed, broadband constellations are capable of distributing videos across the globe by efficient multicasting techniques. However, existing work only studied IP multicast for broadband constellations, which suffer from limited scalability and tree performance. With recent breakthroughs in software defined networking, novel software defined multicasting (SDM) techniques manage to achieve efficient data transfer through intelligent and granular management, outperforming traditional IP Multicast. This paper leverages software defined multicasting in the promising broadband constellations to empower satellite-based Internet video distribution. Based on rectilinear Steiner trees, this paper proposes a novel software defined multicasting framework for broadband satellite networks. In addition, this paper designs simple, agile, and scalable multicast segment routing protocols implementing source routing and equal cost multipath routing. The proposed protocols also adapt to frequent member updates and network failures with efficient tree recovery and local rerouting mechanisms. Comprehensive experiments demonstrate the effectiveness and efficiency of our approach when compared with traditional algorithms.}
}


@article{DBLP:journals/tmc/LuoAD24,
	author = {Fucai Luo and
                  Saif Al{-}Kuwari and
                  Yong Ding},
	title = {{SVFL:} Efficient Secure Aggregation and Verification for Cross-Silo
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {850--864},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3219485},
	doi = {10.1109/TMC.2022.3219485},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuoAD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-silo federated learning (FL) allows organizations to collaboratively train machine learning (ML) models by sending their local gradients to a server for aggregation, without having to disclose their data. The main security issues in FL, that is, the privacy of the gradient and the trained model, and the correctness verification of the aggregated gradient, are gaining increasing attention from industry and academia. A popular approach to protect the privacy of the gradient and the trained model is for each client to mask their own gradients using additively homomorphic encryption (HE). However, this leads to significant computation and communication overheads. On the other hand, to verify the aggregated gradient, several verifiable FL protocols that require the server to provide a verifiable aggregated gradient were proposed. However, these verifiable FL protocols perform poorly in computation and communication. In this paper, we propose SVFL, an efficient protocol for cross-silo FL, that supports both secure gradient aggregation and verification. We first replace the heavy HE operations with a simple masking technique. Then, we design an efficient verification mechanism that achieves the correctness verification of the aggregated gradient. We evaluate the performance of SVFL and show, by complexity analysis and experimental evaluations, that its computation and communication overheads remain low even on large datasets, with a negligible accuracy loss (less than 1\\%\n). Furthermore, we conduct experimental comparisons between SVFL and other existing FL protocols to show that SVFL achieves significant efficiency improvements in both computation and communication.}
}


@article{DBLP:journals/tmc/YuZZL24,
	author = {Xiaojing Yu and
                  Zhijun Zhou and
                  Lan Zhang and
                  Xiang{-}Yang Li},
	title = {ThumbUp: Secure Smartwatch Controller for Smart Homes Using Simple
                  Hand Gestures},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {865--878},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3216927},
	doi = {10.1109/TMC.2022.3216927},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of creative applications and intelligent gadgets requires a secure and straightforward interface with human users. We propose, design, and implement ThumbUp, a smartwatch-based two-factor real-time identification and authentication system in which smartwatch users can identify and authenticate themselves using some simple hand and finger movements, such as thumb-up. ThumbUp leverages the signal from the Inertial Measurement Unit (IMU) in in Commercial-Off-The-Shelf (COTS) smart devices to discover the unique pattern generated by each user's simple gestures using a carefully constructed deep learning model. Smart homes provide a comfortable, safe, and efficient living environment, epecially help the sick and aged. We propose strategies for convenient and reliable control in smart homes with gesture command recognition. We build an Auto-Encoder-based filter that reconstructs the raw data to improve the representation of gesture features. Moreover, we adopt the random forest method to analyse the contextual command correlation. And we employ the authentication system based on smartwatch for personalized command feedback and ensure that illegals cannot use the device. We implement our system and undertake rigorous studies to determine its usefulness and efficiency over a three-month period with 65 users. It achieves a 97% accuracy for user classification and an EER of 0.014 for authentication task with a single simple gesture. And our method achieves 91% accuracy for command recognition and 96% command accuracy with contextual informations. Additionally, we conduct a study of user acceptability of our system and explain how gesture proficiency influences authentication accuracy.}
}


@article{DBLP:journals/tmc/CaoLCLW24,
	author = {Yetong Cao and
                  Fan Li and
                  Huijie Chen and
                  Xiaochen Liu and
                  Yu Wang},
	title = {Tongue-Jaw Movement Recognition Through Acoustic Sensing on Smartphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {879--894},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3222594},
	doi = {10.1109/TMC.2022.3222594},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLCLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Past tongue-jaw movement interaction systems typically require dedicated hardware and are uncomfortable to use, limiting their scalability and generalizability. This paper introduces CanalScan, the first system that recognizes tongue-jaw movements using commodity speakers and microphones mounted on ubiquitous off-the-shelf devices (e.g., smartphones). What inspires us is that tongue-jaw movements always cause ear canal deformations, and we find that for different tongue-jaw movements, dynamic features of ear canal deformations present unique patterns on acoustic reflections in the ear canal. Specifically, CanalScan first sends an acoustic signal to the ear canal, then parses the reflection signals for tongue-jaw movements recognition. To eliminate the impacts of body movements, we develop a body movement noise filtering method and a dynamic segmentation method to identify and separate the tongue-jaw movements-associated ear canal deformations from other types of body movements. We further propose a sensor position detection method and a data transformation mechanism to reduce the impacts of diversities in-ear canal shapes and relative positions between sensors and the ear canal. CanalScan explores twelve unique and consistent features and applies a random forest classifier to distinguish tongue-jaw movements. Extensive experiments with twenty participants validate the generalizability, effectiveness, robustness, and high accuracy of CanalScan.}
}


@article{DBLP:journals/tmc/WuJLLMMWL24,
	author = {Jinxiao Wu and
                  Xiangyang Ji and
                  Yongqiang Lyu and
                  Xuanshu Luo and
                  Yan Meng and
                  Eric Morales and
                  Dongsheng Wang and
                  Xiaomin Luo},
	title = {Touchscreens Can Reveal User Identity: Capacitive Plethysmogram-Based
                  Biometrics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {895--908},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3214846},
	doi = {10.1109/TMC.2022.3214846},
	timestamp = {Wed, 07 Feb 2024 17:24:23 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuJLLMMWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Biometrics are widely used for user identification/authentication, but the fact has rarely been noticed that general capacitive touchscreens can reveal user identities by touch signals. This paper proposes a new biometric method with inherent liveness detection for reliable user recognition based on the cardiac signal captured by the capacitive touchscreen, namely Capacitive Plethysmogram (CPG). And a systematic framework is designed for CPG collection, processing, and exploitation to identify users. Specifically, since the finger usually forms capacitors with multiple sensing electrodes during touching, we can extract several CPG signals simultaneously from the screen output. Then we propose a series of preprocessing algorithms to filter CPG for signal quality enhancement. Finally, to further leverage filtered CPG signals and extract efficient features for identifying users, we build an encoder based on 3D attention CNN and metric learning. Experimental results demonstrate that the proposed method can achieve an average accuracy of 96.73%, FAR of 3.03%, and FRR of 7.35% in the laboratory environment, which reveals the potential of CPG for user privacy protection and data security on various devices laced with capacitive touchscreens.}
}


@article{DBLP:journals/tmc/BaiTFJBX24,
	author = {Sen Bai and
                  Shoufeng Tong and
                  Xin Feng and
                  Zhengang Jiang and
                  Xin Bai and
                  Ranqi Xu},
	title = {Toward Dynamic Pricing for City-Wide Crowdsourced Instant Delivery
                  Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {909--924},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3228259},
	doi = {10.1109/TMC.2022.3228259},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BaiTFJBX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The booming crowdsourced delivery leverages networks of local, part time couriers to meet growing customer demands for increasingly speedy shipping. By conducting city-wide low-cost delivery networks, crowd-sourced enabled instant delivery platforms have fought to figure out a way to offer instant shipping at low price. Nevertheless, limited ubiquitous computing infrastructures impede the discovery of equilibrium between cost reduction and sufficient shipping capacity. In this article, we present our endeavor towards understanding and designing a dynamic pricing framework based on a large-scale food-delivery platform. To reduce delivery fees in capacity overload regions and resolve the decline of service quality caused by insufficient shipping capacity, we address the challenge of quantifying the inter-regional shipping capacity disparities by accordingly applying a variety of techniques, including cooperative game theory, probability analysis and address entity recognition. In particular, we provide an O(n^{2}) algorithm for the cooperative game based cost allocation, of which the exponential computational complexity O(n^{2}2^{n}) is generally considered a major challenge in real-world applications. City-wide experiments in Nanjing and Shenzhen illustrate that we achieve a 10% decrease in delivery fees under overload shipping capacity and a 3% increase in completion rates in customer locations with poor service quality.}
}


@article{DBLP:journals/tmc/WangQWSDZ24,
	author = {Guang Wang and
                  Zhou Qin and
                  Shuai Wang and
                  Huijun Sun and
                  Zheng Dong and
                  Desheng Zhang},
	title = {Towards Accessible Shared Autonomous Electric Mobility With Dynamic
                  Deadlines},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {925--940},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3213125},
	doi = {10.1109/TMC.2022.3213125},
	timestamp = {Mon, 12 Feb 2024 16:07:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangQWSDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Shared autonomous electric mobility has attracted significant interest in recent years due to its potential to save energy consumption, enhance mobility accessibility, reduce air pollution, mitigate traffic congestion, etc. Although providing convenient, low-cost, and environmentally-friendly mobility, there are still some roadblocks to achieve efficient shared autonomous electric mobility, e.g., how to enable the accessibility of shared autonomous electric vehicles in time. To overcome these roadblocks, in this article, we design Safari, an efficient Shared Autonomous electric vehicle Fleet mAnagement system with joint Repositioning and chargIng based on dynamic deadlines to improve both user experience and operating profits. Our Safari considers not only the highly dynamic user demand for vehicle repositioning (i.e., where to relocate) but also many practical factors like the time-varying charging pricing for charging scheduling (i.e., where to charge). To perform the two tasks efficiently, in Safari, we design a dynamic deadline-based deep reinforcement learning algorithm, which generates dynamic deadlines via usage prediction combined with an error compensation mechanism to adaptively learn the optimal decisions for satisfying highly dynamic and unbalanced user demand in real time. More importantly, we implement and evaluate the Safari system with 10-month real-world shared electric vehicle data, and the extensive experimental results show that our Safari achieves 100% of accessibility and effectively reduces 26.2% of charging costs and reduces 31.8% of vehicle movements for energy saving with a small runtime overhead at the same time. Furthermore, the results also show Safari has a great potential to achieve efficient and accessible shared autonomous electric mobility during its long-term expansion and evolution process.}
}


@article{DBLP:journals/tmc/HuLMSPLZZCM24,
	author = {Pengfei Hu and
                  Wenhao Li and
                  Yifan Ma and
                  Panneer Selvam Santhalingam and
                  Parth H. Pathak and
                  Hong Li and
                  Huanle Zhang and
                  Guoming Zhang and
                  Xiuzhen Cheng and
                  Prasant Mohapatra},
	title = {Towards Unconstrained Vocabulary Eavesdropping With mmWave Radar Using
                  {GAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {941--954},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3226690},
	doi = {10.1109/TMC.2022.3226690},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuLMSPLZZCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As acoustic communication systems become increasingly common in our daily life, eavesdropping brings severe security and privacy risks. Current methods of acoustic eavesdropping either provide low resolution due to the use of sub-6 GHz frequencies, work only for limited words based on classification approaches, or cannot work through-wall because of the use of optical sensors. In this article, we present milliEar, a mmWave acoustic eavesdropping system that leverages the high-resolution of mmWave FMCW ranging and generative machine learning models to not only extract vibrations but to reconstruct the audio. milliEar combines speaker vibration estimation with conditional generative adversarial networks to eavesdrop and recover high-quality audios (i.e., with no vocabulary constraints). We implement and evaluate milliEar using off-the-shelf mmWave radars deployed in different scenarios and settings. Evaluation results clearly show that milliEar can accurately reconstruct the audio even at different distances, angles, and through the wall with different insulator materials. In addition, our subjective and objective evaluations demonstrate that the reconstructed audio has a strong similarity with the original audio.}
}


@article{DBLP:journals/tmc/HuangTCJZ24,
	author = {Wenbin Huang and
                  Wenjuan Tang and
                  Hanyuan Chen and
                  Hongbo Jiang and
                  Yaoxue Zhang},
	title = {Unauthorized Microphone Access Restraint Based on User Behavior Perception
                  in Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {955--970},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3221463},
	doi = {10.1109/TMC.2022.3221463},
	timestamp = {Mon, 10 Feb 2025 14:42:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangTCJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microphone has been widely integrated into mobile devices to provide physical basis for human-device voice interaction. However, the microphone may be spitefully invoked by malicious mobile applications(apps) with arousing security and privacy concerns. In this work, to explore the issue of illegal microphone access, we develop spiteful apps through native and injection development to access the microphone viciously on a series of mobile devices. The results demonstrate that baleful apps could enable the microphone arbitrarily without any hint. To combat the unauthorized microphone access behavior, we design a microphone illegal access detection(MicDet) scheme by constructing a request-response time model using the Unix time stamps of voice icon touched and microphone invoked. Through conducting numerical analysis and hypothesis testing to effectively verify the request-response pattern of app's normal access, we detect illegal access by analyzing whether the touch operation matches the normal pattern. For friendly user experience, we design an intuitive floating window to alert users by displaying the name of the app that illegally accessed the microphone once the illegal behavior is detected. Finally, we apply our scheme to different mobile devices and test several apps, the experimental results show that the MicDet scheme achieves a high detection accuracy.}
}


@article{DBLP:journals/tmc/XuYCLZZL24,
	author = {Weitao Xu and
                  Huanqi Yang and
                  Jiongzhang Chen and
                  Chengwen Luo and
                  Jia Zhang and
                  Yuliang Zhao and
                  Wen Jung Li},
	title = {WashRing: An Energy-Efficient and Highly Accurate Handwashing Monitoring
                  System via Smart Ring},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {971--984},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3227299},
	doi = {10.1109/TMC.2022.3227299},
	timestamp = {Tue, 13 Aug 2024 07:51:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuYCLZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The outbreak of COVID-19 has greatly changed everyone's lifestyle all over the world. One of the best ways to prevent the spread of infections is by washing hands properly. Although a number of hand hygiene monitoring systems have been proposed, they either cannot achieve high accuracy in practice or work only in limited environments such as hospitals. Therefore, a ubiquitous, energy-efficient and highly accurate hand hygiene monitoring system is still lacking. In this paper, we present WashRing—the first smart ring-based handwashing monitoring system. In WashRing, we design a Partially Observable Markov Decision Process (POMDP) based adaptive sampling approach to achieve high energy efficiency. Then, we design an automatic feature extraction scheme based on wavelet scattering and a CNN-LSTM neural network to achieve fine-grained gesture recognition. Finally, we model the handwashing gesture classification as a few-shot learning problem to mitigate the burden of collecting extensive data from five fingers. We collect data from 25 subjects over 2 months and evaluate the system performance on both commercial OURA ring and customized ring. Evaluation results show that WashRing achieves 97.8% accuracy which is 10.2%–15.9% higher than state-of-the-arts. Our adaptive sampling approach reduces energy consumption by 64.2% compared to fixed duty cycle sampling strategies.}
}


@article{DBLP:journals/tmc/LinJXXWWZ24,
	author = {Chi Lin and
                  Chuanying Ji and
                  Jie Xiong and
                  Chaocan Xiang and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Wi-Rotate: An Instantaneous Angular Speed Measurement System Using
                  WiFi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {1},
	pages = {985--1000},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3223633},
	doi = {10.1109/TMC.2022.3223633},
	timestamp = {Sat, 13 Jan 2024 17:37:22 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinJXXWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose the design, implementation, and evaluation of an instantaneous angular speed (IAS) measurement system, namely Wi-Rotate, using commercial-off-the-shelf (COTS) WiFi hardware. Wi-Rotate exploits the Channel State Information (CSI) of WiFi signals to extract the physical characteristics of the rotation object to achieve accurate contact-free IAS measurements. Wi-Rotate contains three main components: Wi-Fresnel model, Wi-Phase model, and a combination model. Wi-Fresnel model explores the signal amplitude variation features when the rotating object cuts the Fresnel zone boundary to track target rotation. Wi-Phase model leverages signal phase variation and formalizes the problem of determining IAS as a linear programming problem. The combination model combines the IAS values obtained by Wi-Fresnel and Wi-Phase and utilizes a clustering method to further improve measurement accuracy. Comprehensive experiments are conducted to demonstrate the advantages of Wi-Rotate in terms of accuracy, sensing range, and system latency. Wi-Rotate is able to achieve real-time rotation measurements at an accuracy higher than 99% when the target is within 2 meters. Even when the target is 3 meters away, Wi-Rotate can still achieve an accuracy of 94%, demonstrating the long-range tracking capability which is critical for industrial applications.}
}


@article{DBLP:journals/tmc/LiuZOCW24,
	author = {Zhidan Liu and
                  Hongquan Zhang and
                  Guofeng Ouyang and
                  Junyang Chen and
                  Kaishun Wu},
	title = {Data-Driven Pick-Up Location Recommendation for Ride-Hailing Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1001--1015},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3208566},
	doi = {10.1109/TMC.2022.3208566},
	timestamp = {Thu, 08 Aug 2024 12:56:51 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZOCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ride-hailing service (RHS) has become an important transportation mode in our daily life. Although many works have been proposed to improve RHS from different aspects, only few works focus on the selections of pick-up locations, where rider and driver meet and start a trip. In this paper, we present MPLRec, a data-driven pick-up location recommendation system that exploits riders’ specific mobility demands, e.g., destination, and historical experiences to meet riders’ travel requirements. MPLRec generates potential pick-up locations over the road network and characterizes them with rich features that describe a location from the riders’ perspective. We also build spatio-temporal indexes to organize potential pick-up locations and historical data for facilitating online recommending. When processing an online recommendation request, MPLRec derives candidate pick-up locations and investigates them with materialized features, which are computed from historical order and trajectory data while considering rider's mobility demands. Based on these features, a novel scoring function is used to derive the best pick-up location for each request. Moreover, we implement an RHS simulator to evaluate MPLRec using large-scale practical ride-hailing datasets. Extensive experiments and simulations demonstrate the effectiveness and efficiency of MPLRec, which can complete each request within 0.5 s and largely reduce the ride-hailing costs when compared to baseline methods.}
}


@article{DBLP:journals/tmc/HuangKCDQCLH24,
	author = {Junqin Huang and
                  Linghe Kong and
                  Long Cheng and
                  Hong{-}Ning Dai and
                  Meikang Qiu and
                  Guihai Chen and
                  Xue (Steve) Liu and
                  Gang Huang},
	title = {BlockSense: Towards Trustworthy Mobile Crowdsensing via Proof-of-Data
                  Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1016--1033},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230758},
	doi = {10.1109/TMC.2022.3230758},
	timestamp = {Mon, 15 Apr 2024 21:32:44 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangKCDQCLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) can promote data acquisition and sharing among mobile devices. Traditional MCS platforms are based on a triangular structure consisting of three roles: data requester, worker (i.e., sensory data provider) and MCS platform. However, this centralized architecture suffers from poor reliability and difficulties in guaranteeing data quality and privacy, even provides unfair incentives for users. In this article, we propose a blockchain-based MCS platform, namely BlockSense, to replace the traditional triangular architecture of MCS models by a decentralized paradigm. To achieve the goal of trustworthiness of BlockSense, we present a novel consensus protocol, namely Proof-of-Data (PoD), which leverages miners to conduct useful data quality validation work instead of “useless” hash calculation. Meanwhile, in order to preserve the privacy of the sensory data, we design a homomorphic data perturbation scheme, through which miners can verify data quality without knowing the contents of the data. We have implemented a prototype of BlockSense and conducted case studies on campus, collecting over 7,000 data from workers’ mobile phones. Both simulations and real-world experiments show that BlockSense can not only improve system security, preserve data privacy and guarantee incentives fairness, but also achieve at least 5.6x faster than Ethereum smart contracts in verification efficiency.}
}


@article{DBLP:journals/tmc/BaiLCW24,
	author = {Zhuoyi Bai and
                  Yifan Lin and
                  Yang Cao and
                  Wei Wang},
	title = {Delay-Aware Cooperative Task Offloading for Multi-UAV Enabled Edge-Cloud
                  Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1034--1049},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232375},
	doi = {10.1109/TMC.2022.3232375},
	timestamp = {Mon, 29 Jul 2024 07:57:04 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaiLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) has received tremendous attention in the area of edge computing due to its flexible deployment and wide coverage accessibility. In weak infrastructure scenarios, multiple UAVs can form on-site edge computing clusters to handle the real-time tasks. Further, a multi-UAV enabled edge-cloud computing system is coined by cooperating the UAVs with remote cloud, which provides superior computing capability. However, the uneven distribution of tasks makes it difficult to meet the real-time requirements when load balancing is unavailable. To address above issue, a delay minimization problem for multi-UAV enabled edge-cloud cooperative offloading is investigated in this paper. The problem is formulated as a non-convex problem based on models that reflect characteristics of the system, such as ubiquitous network congestion, air-to-ground wireless channel and cooperative parallel computing. An efficient cooperative offloading algorithm is proposed to address the problem. Specifically, convex approximation is applied to make the original problem tractable, and Lyapunov optimization is utilized to make online task offloading decisions. Finally, the correctness of the models are verified through a practical UAV-edge computing platform. Simulations based on measurement results and real-world datasets indicate that, the proposed algorithm fully utilizes the available energy to significantly reduce the tasks’ completion delay.}
}


@article{DBLP:journals/tmc/OgawaHM24,
	author = {Yukio Ogawa and
                  Go Hasegawa and
                  Masayuki Murata},
	title = {Probabilistic Control of Dynamic Crowds Toward Uniform Spatial-Temporal
                  Coverage},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1050--1065},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3231530},
	doi = {10.1109/TMC.2022.3231530},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/OgawaHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular mobility and connectivity vary significantly over space and time when vehicular crowd sensing covers a city-wide area for a long time period, but it is important to achieve sufficiently uniform data coverage to satisfy the requirements of an environmental monitoring scenario. Our goal is thus to ensure uniform spatial-temporal coverage of sensed data over a city-wide area despite such vehicle dynamics. For a large area, trajectory-based approaches must deal with a great number and variety of participant mobility patterns. Hence, we propose a probabilistic control mechanism that adaptively adjusts the incentive to each participant, without using any prior information about participants. We provide a mathematical analysis that ensures stability of the number of participants with assigned tasks (called workers), and we evaluate the mechanism's robustness by using 24-hr vehicle trace data from a city-wide area. Our results demonstrate that, when the number of participants is up to 1500 times higher than the required number of workers, sensing actions result in a distribution with a mean of about 1 and an interquartile range of around 4 for a required sensing interval; moreover, the mean increases by 2% when 30% of communication messages are randomly lost.}
}


@article{DBLP:journals/tmc/LiLHXHX24,
	author = {Yafei Li and
                  Huiling Li and
                  Xin Huang and
                  Jianliang Xu and
                  Yu Han and
                  Mingliang Xu},
	title = {Utility-Aware Dynamic Ridesharing in Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1066--1079},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232215},
	doi = {10.1109/TMC.2022.3232215},
	timestamp = {Wed, 11 Sep 2024 20:37:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLHXHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With smartphones and geo-locating devices widely used around the world, ridesharing, as a main application field of spatial crowdsourcing, has been fast expanding its widespread adoption and potentially brings great benefits to human society and the environment. However, ridesharing has so far not been as popular as expected. A recent survey shows that notable obstacles come from concerns about social comfort and price fairness when riding with strangers. To defeat these obstacles, in this paper, we study a novel problem of utility-aware ride matching (URM) for dynamic ridesharing, where drivers and riders are matched in batches by considering their social comfort and price revenue. While the URM problem is of practical usefulness, we prove that this problem is Nondeterministic Polynomial Hard (NP-hard). To tackle the problem optimally and find exact answers, we present a novel bipartite matching algorithm by integrating an effective Driver-Rider Graph (DR-Graph) index. To balance the effectiveness and efficiency, we propose two efficient algorithms to solve the URM problem with only a small loss of utility. Leveraging a bounded Driver-Rider-group Graph (DRg-Graph) and several useful pruning bounds for matching utility and travel cost, we develop an \\epsilon\n-refining algorithm to find an \\epsilon\n-approximation matching utility of the optimal answer. Extensive experiments on real datasets showed that our proposed algorithms achieved effective matching results of social-pricing based utilities and the efficient performance of quick matching under various parameter settings.}
}


@article{DBLP:journals/tmc/MaXXLX24,
	author = {Zhenguo Ma and
                  Yang Xu and
                  Hongli Xu and
                  Jianchun Liu and
                  Yinxing Xue},
	title = {Like Attracts Like: Personalized Federated Learning in Decentralized
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1080--1096},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230712},
	doi = {10.1109/TMC.2022.3230712},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaXXLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Personalized Federated Learning (PFL) methods aim to produce personalized models for different users, so as to keep track of their individualized requirements in Edge Computing (EC). The centralized PFL methods may suffer from the communication bottleneck and single point of failure. As an alternative solution, the decentralized PFL (DPFL) methods are performed in a Peer-to-Peer (P2P) manner, and collaboratively train personalized models by model aggregation among congenial devices. However, these DPFL methods may incur high communication cost and low resource utilization induced by large-scale models. Herein, we take the communication constraint and heterogeneity into consideration and propose to realize communication-efficient DPFL with adaptive model pruning and neighbor selection. We theoretically analyze the convergence of the proposed DPFL method, and study the impacts of both model pruning and neighbor selection on training performance. Furthermore, we propose an efficient algorithm that combines model pruning and neighbor selection to achieve a trade-off between model quality and communication cost. Extensive simulation and testbed experiments on real-world datasets are conducted. The experimental results demonstrate that the proposed algorithm can improve the test accuracy by at most 13% and save the traffic consumption by 45.4% on average compared with the existing PFL methods.}
}


@article{DBLP:journals/tmc/YeZX24,
	author = {Songtao Ye and
                  Lingzi Zhao and
                  Wei Xie},
	title = {Crowd Bus Sensing: Resolving Conflicts Between the Ground Truth and
                  Map Apps},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1097--1111},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3231085},
	doi = {10.1109/TMC.2022.3231085},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YeZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, an increasing number of map apps have provided route planning services to their users. However, the quality of route planning services relies heavily on having correct data about transportation infrastructure. As many planned subway lines are being built across cities, there are conflicts between the actual conditions and the data provided by map apps for temporary bus stops, which may result in complaints against public transportation operators. However, it is difficult to tackle these complaints, as public transportation operators can obtain only inaccurate information about the locations of temporary bus stops. To resolve these conflicts, crowd bus sensing (CBS) is proposed in this paper. CBS is a new sensing paradigm that takes advantage of the extensive deployment of GPS trackers and prior knowledge about the transportation infrastructures covered by scheduled bus routes. Extensive experimental evaluations on real-world and synthetic datasets show that the proposed CBS system outperforms state-of-the-art methods.}
}


@article{DBLP:journals/tmc/ChenTC24,
	author = {Xianda Chen and
                  Tianxiang Tan and
                  Guohong Cao},
	title = {Macrotile: Toward QoE-Aware and Energy-Efficient 360-Degree Video
                  Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1112--1126},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3233022},
	doi = {10.1109/TMC.2022.3233022},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tile-based streaming techniques have been widely used to save bandwidth in 360{}^{\\circ } video streaming. However, it is a challenge to determine the right tile size which directly affects the bandwidth usage. Moreover, downloading and processing many small tiles consume a large amount of energy on mobile devices. To solve this problem, we propose to encode the video by taking into account the viewing popularity, where the popularly viewed areas are encoded as macrotiles. We propose techniques for identifying and building macrotiles, and adjusting their sizes to take into account practical issues such as head movement randomness. In some cases, the user's viewing area may not be covered by the constructed macrotiles, and then the conventional tiling scheme is used. To support macrotile based 360{}^{\\circ } video streaming, the client selects the right tiles (a macrotile or a set of conventional tiles) with the right quality level to maximize the QoE under bandwidth constraint. We formulate this problem as an optimization problem which is NP-hard, and then propose a heuristic algorithm to solve it. Through extensive evaluations based on real head movement traces, we demonstrate that the proposed algorithm can significantly improve QoE, save bandwidth usage, and reduce energy consumption.}
}


@article{DBLP:journals/tmc/ZhangJCHBZ24,
	author = {Jing Zhang and
                  Yue Jiang and
                  Jie Cui and
                  Debiao He and
                  Irina Bolodurina and
                  Hong Zhong},
	title = {{DBCPA:} Dual Blockchain-Assisted Conditional Privacy-Preserving Authentication
                  Framework and Protocol for Vehicular Ad Hoc Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1127--1141},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230853},
	doi = {10.1109/TMC.2022.3230853},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangJCHBZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular ad hoc networks (VANETs) connect all vehicles through wireless channels. They provide extensive real-time traffic information services that improve driving safety and traffic management efficiency. However, VANETs are vulnerable to security attacks because of the open wireless nature of their communication channels. Most security mechanisms for traditional VANETs are centralized and have certain limitations in satisfying security requirements, such as anti-single-point failure, distributed security authentication of messages, and privacy preservation in VANETs. To address these issues, herein, we propose a dual blockchain-assisted conditional privacy-preserving authentication framework and protocol for VANETs. The identity authentication and privacy preservation of vehicles in VANETs can be realized without relying on a centralized trusted third party. The proposed scheme also allows for the conditional tracking of illegal vehicles. The decentralized dynamic revocation of illegal vehicles can be realized through smart contracts, rendering the scheme efficient and scalable. We implement this scheme in an Ethereum test network to demonstrate its feasibility and conduct an in-depth security analysis and comprehensive performance evaluation of the proposed scheme. The results demonstrate that the proposed scheme is an effective solution for the development of a decentralized authentication system for VANETs.}
}


@article{DBLP:journals/tmc/FangFGHSJM24,
	author = {Zheng Fang and
                  Hao Fu and
                  Tianbo Gu and
                  Pengfei Hu and
                  Jinyue Song and
                  Trent Jaeger and
                  Prasant Mohapatra},
	title = {Towards System-Level Security Analysis of IoT Using Attack Graphs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1142--1155},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3231567},
	doi = {10.1109/TMC.2022.3231567},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FangFGHSJM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most IoT systems involve IoT devices, communication protocols, remote cloud, IoT applications, mobile apps, and the physical environment. However, existing IoT security analyses only focus on a subset of all the essential components, such as device firmware or communication protocols, and ignore IoT systems’ interactive nature, resulting in limited attack detection capabilities. In this work, we propose Iota, a logic programming-based framework to perform system-level security analysis for IoT systems. Iota generates attack graphs for IoT systems, showing all of the system resources that can be compromised and enumerating potential attack traces. In building Iota, we design novel techniques to scan IoT systems for individual vulnerabilities and further create generic exploit models for IoT vulnerabilities. We also identify and model physical dependencies between different devices as they are unique to IoT systems and are employed by adversaries to launch complicated attacks. In addition, we utilize NLP techniques to extract IoT app semantics based on app descriptions. Iota automatically translates vulnerabilities, exploits, and device dependencies to Prolog clauses and invokes MulVAL to construct attack graphs. To evaluate vulnerabilities’ system-wide impact, we propose three metrics based on the attack graph, which provide guidance on hardening IoT systems. Evaluation on 127 IoT CVEs (Common Vulnerabilities and Exposures) shows that Iota's exploit modeling module achieves over 80% accuracy in predicting vulnerabilities’ preconditions and effects. We apply Iota to 37 synthetic smart home IoT systems based on real-world IoT apps and devices. Experimental results show that our framework is effective and highly efficient. Among 27 shortest attack traces revealed by the attack graphs, 62.8% are not anticipated by the system administrator. It only takes 1.2 seconds to generate and analyze the attack graph for an IoT system consisting of 50 devices.}
}


@article{DBLP:journals/tmc/WangYCXS24,
	author = {Dazhuo Wang and
                  Jianfei Yang and
                  Wei Cui and
                  Lihua Xie and
                  Sumei Sun},
	title = {AirFi: Empowering WiFi-Based Passive Human Gesture Recognition to
                  Unseen Environment via Domain Generalization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1156--1168},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230665},
	doi = {10.1109/TMC.2022.3230665},
	timestamp = {Tue, 16 Jul 2024 11:14:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYCXS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based smart human sensing technology enabled by Channel State Information (CSI) has received great attention in recent years. However, CSI-based sensing systems suffer from performance degradation when deployed in different environments. Existing works solve this problem by domain adaptation using massive unlabeled high-quality data from the new environment, which is usually unavailable in practice. In this article, we propose a novel augmented environment-invariant robust WiFi gesture recognition system named AirFi that deals with the issue of environment dependency from a new perspective. The AirFi is a novel domain generalization framework that learns the critical part of CSI regardless of different environments and generalizes the model to unseen scenarios, which does not require collecting any data for adaptation to the new environment. AirFi extracts the common features from several training environment settings and minimizes the distribution differences among them. The feature is further augmented to be more robust to environments. Moreover, the system can be further improved by few-shot learning techniques. Compared to state-of-the-art methods, AirFi is able to work in different environment settings without acquiring any CSI data from the new environment. The experimental results demonstrate that our system remains robust in the new environment and outperforms the compared systems.}
}


@article{DBLP:journals/tmc/JinZDWZG24,
	author = {Huiying Jin and
                  Pengcheng Zhang and
                  Hai Dong and
                  Xinmiao Wei and
                  Yuelong Zhu and
                  Tao Gu},
	title = {Mobility-Aware and Privacy-Protecting QoS Optimization in Mobile Edge
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1169--1185},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230856},
	doi = {10.1109/TMC.2022.3230856},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JinZDWZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of 5G technologies, the demand of quality of service (QoS) from edge users, including high bandwidth and low latency, has increased dramatically. QoS within a mobile edge network is highly dependent on the allocation of edge users. However, the complexity of user movement greatly challenges edge user allocation, leading to privacy leakage. In addition, updating massive data constantly in a dynamic mobile edge network also crucial to ensure efficiency. To address these challenges, this paper proposes a dynamic QoS optimization strategy (MENIFLD_QoS) in mobile edge networks based on incremental learning and federated learning. MENIFLD_QoS optimizes service cache in edge regions and allocates edge servers to edge users according to the locations of edge servers accessed by edge users in mobile scenarios. While optimizing regional service quality, the system can effectively protect user privacy. In addition, for dynamic incremental data, MENIFLD_QoS trains updated data based on the strategy of incremental learning hence significantly improves optimization speed. Experimental results on an edge QoS dataset show that the proposed strategy achieves global optimization in both multi-variable and multi-peak user allocation scenarios and notably enhances the training efficiency of the regional invocation model.}
}


@article{DBLP:journals/tmc/LiuCWXGZLMZ24,
	author = {Xuewen Liu and
                  Gang Chuai and
                  Xin Wang and
                  Zhiwei Xu and
                  Weidong Gao and
                  Kaisa Zhang and
                  Qian Liu and
                  Saidiwaerdi Maimaiti and
                  Peiliang Zuo},
	title = {QoE-Driven Antenna Tuning in Cellular Networks With Cooperative Multi-Agent
                  Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1186--1199},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3230711},
	doi = {10.1109/TMC.2022.3230711},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCWXGZLMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Antenna tuning plays an essential role in ensuring high quality wireless communications. Targeting for higher Quality of Service (QoS), many existing network antenna tuning schemes are based on expert knowledge, rule-based policies or conventional optimization theory. However, maximizing the traffic-related QoS does not guarantee that all customers experience good services. In addition, existing schemes are often limited to some handcrafted rules or heuristics and lack of adaptability especially in a time-varying environment. Quality of Experience (QoE), a user-centric metric, can better measure users’ satisfaction for services in wireless networks. This article proposes the cooperative tuning of antennas based on QoE, a paradigm shift from network-centric QoS to user-centric QoE domain. In a normal cellular network, besides the need of improving the overall QoE, it requires handling faults from different cells. As Multi-agent Reinforcement Learning (MARL) has the capability of self-learning the dynamics of environment, we propose an antenna configuration algorithm based on multi-goal MARL. In our framework, agents from different cells not only need to cooperate with each other to achieve the global goal of increasing the overall QoE of the wireless network but also complete some personal goals by combating the faults encountered in their own cells. To accelerate the training efficiency, we introduce a novel two-stage curriculum learning. To reduce the collection time of each QoE sample, we develop an accurate and timely QoE/QoS mapping model with the cascading of a Random Forest Classifier (RFC) and a Deep Neural Network (DNN) (abbreviated as RFC-DNN), which can help us obtain QoE by collecting QoS measurements and perform QoE-based antenna configurations with smaller time granularity. Our proposed RFC-DNN model can reduce the time by 70% when predicting the QoE of a single sample. A huge amount of time will be saved in MARL when tens of thousands of transitions/samples need to be collected. The performance results show that our proposed antenna tuning schemes can not only address specific faults in each cell, but also significantly improve the global average QoE with a faster and more stable convergence speed.}
}


@article{DBLP:journals/tmc/FidaRAE24,
	author = {Mah{-}Rukh Fida and
                  Marie Roald and
                  Evrim Acar and
                  Ahmed Elmokashfi},
	title = {Modeling Variation in Mobile Download Speed in Presence of Missing
                  Samples},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1200--1214},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3231928},
	doi = {10.1109/TMC.2022.3231928},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FidaRAE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A stably fast mobile broadband connectivity is key to customer retention. Mobile networks, however, suffer unpredictability in performance. Analyzing variability in network speed is, therefore, challenging since it tends to exhibit patterns at several time scales. Additionally, frequently monitoring it over time, is costly. In this article, we analyze speed measurements from 78 stationary probes, spread across Norway. Monitoring was performed thrice per day across the year, to assess performance of the two largest network operators. Despite being unique, the dataset involves a non-trivial extent of missing data. This study investigates the effect of missing data on the extracted performance patterns. We capture patterns with tensor factorizations, that show that missing data at random has a minimal effect on the identified patterns, and that depending upon the determinism of an operator's performance, the acceptable size and structure of missing data varies. Our analysis shows that, for a probe, the difference in speed variation between real and imputed speed values can be around 7% for up to 40% missing data. We also identify that congestion, routine maintenance and sub-optimal network configuration cause high speed variability. These findings can help operators improving their offerings and deciding on optimal performance monitoring frequency.}
}


@article{DBLP:journals/tmc/KeHWKC24,
	author = {Jing{-}Wen Ke and
                  Ren{-}Hung Hwang and
                  Chih{-}Yu Wang and
                  Jian{-}Jhih Kuo and
                  Wei{-}Yu Chen},
	title = {Efficient {RRH} Activation Management for 5G {V2X}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1215--1229},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232547},
	doi = {10.1109/TMC.2022.3232547},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KeHWKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle-to-everything (V2X) communication is one of the key technologies of 5G New Radio to support emerging applications such as autonomous driving. Due to the high density of vehicles, Remote Radio Heads (RRHs) will be deployed as Road Side Units to support V2X. Nevertheless, activation of all RRHs during low-traffic off-peak hours may cause energy wasting. The proper activation of RRH and association between vehicles and RRHs while maintaining the required service quality are the keys to reducing energy consumption. In this work, we first formulate the problem as an Integer Linear Programming optimization problem and prove that the problem is NP-hard. Then, we propose two novel algorithms, referred to as “Least Delete (LD)” and ”Largest-First Rounding with Capacity Constraints (LFRCC).” The simulation results show that the proposed algorithms can achieve significantly better performance compared with existing solutions and are competitive with the optimal solution. Specifically, the LD and LFRCC algorithms can reduce the number of activated RRHs by 86\\% and 89\\% in low-density scenarios. In high-density scenarios, the LD algorithm can reduce the number of activated RRHs by 90\\%. In addition, the solution of LFRCC is larger than that of the optimal solution within 7\\% on average.}
}


@article{DBLP:journals/tmc/DaiHWXLL24,
	author = {Penglin Dai and
                  Biao Han and
                  Xiao Wu and
                  Huanlai Xing and
                  Bingyi Liu and
                  Kai Liu},
	title = {Distributed Convex Relaxation for Heterogeneous Task Replication in
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1230--1245},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232495},
	doi = {10.1109/TMC.2022.3232495},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiHWXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) is expected to support real-time services at wireless networks, where task replication is applied to guarantee job completion within a strict deadline through replicating multiple copies to different edge servers. Most of previous works focused on guaranteeing the reliability of individual task in MEC-based networks with the assumption of homogeneous task execution distribution. Further, these algorithms cannot suit dynamic network scales, due to overhigh communication or retraining overhead. Therefore, this article formulates the problem of heterogeneous task replication in a finer level by modeling outage probability of individual replication, where the decisions of all tasks are jointly optimized within the constraints of both mobile users and MEC servers for minimizing job outage probability. To adapt to varying network scales, we develop centralized and distributed algorithms, respectively. The centralized algorithm is developed based on Interior Point Method, which obtains the optimal solution of relaxed model and then approximates to the solution of original problem. Further, the distributed algorithm decomposes the HTR into multiple subproblems and parallelly compute each local solution based on Distributed ADMM. Finally, we build a simulation model and conduct comprehensive results, which demonstrates that the proposed algorithms can achieve high-accuracy solution with fast convergence.}
}


@article{DBLP:journals/tmc/FemeniasR24,
	author = {Guillem Femenias and
                  Felip Riera{-}Palou},
	title = {Mobile Edge Computing Aided Cell-Free Massive {MIMO} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1246--1261},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232510},
	doi = {10.1109/TMC.2022.3232510},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FemeniasR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A mobile edge computing (MEC) aided cell-free massive MIMO network is investigated in this paper that aims at optimizing the task offloading process from the wireless devices to the MEC server. Partial computation offloading strategies are designed that aim at optimizing either the aggregated latency or the energy consumption by considering the finite computational capability of the MEC server and assuming that the tasks are subject to a maximum latency constraint. The proposed optimization approach considers the joint optimization of both the per-task offloading ratio and the corresponding computational resources allocated to the tasks at the MEC server. This optimization is performed by taking into account the computing capabilities of both the wireless devices and the MEC server and both the UL and DL spectral efficiencies provided by the cell-free massive MIMO network. The proposed optimization problems are shown to be convex and, inspired by the well-known waterfilling algorithm, optimal solutions based on low-complexity iterative algorithms are devised. Extensive numerical results reveal the potential of the proposed MEC-enabled cell-free massive MIMO network.}
}


@article{DBLP:journals/tmc/RenCLYZCHL24,
	author = {Yanzhi Ren and
                  Chen Chen and
                  Hongbo Liu and
                  Jiadi Yu and
                  Zhourong Zheng and
                  Yingying Chen and
                  Pengcheng Huang and
                  Hongwei Li},
	title = {Secure Mobile Two-Factor Authentication Leveraging Active Sound Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1262--1277},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232172},
	doi = {10.1109/TMC.2022.3232172},
	timestamp = {Thu, 07 Mar 2024 15:07:46 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RenCLYZCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The two-factor authentication (2FA) has drawn increasingly attention as the mobile devices become more prevalent. For example, the user's possession of the enrolled phone could be used by the 2FA system as the second proof to protect his/her online accounts. Existing 2FA solutions mainly require some form of user-device interaction, which may severely affect user experience and creates extra burdens to users. In this article, we propose a secure 2FA system utilizing the proximity of a user's enrolled phone and the login device as the second proof without requiring the user's interactions. The basic idea of our 2FA system is to derive location signatures based on acoustic beep signals emitted alternately by both devices and sensing the echoes with microphones, and compare the extracted signatures for proximity detection. Moreover, to further enhance the security of our system, we also design a device authentication scheme which derives the acoustic fingerprint between the login device and enrolled phone to verify the identity of two devices. Given the received beep signal, our system designs a period selection scheme to identify two sound segments accurately: the chirp period is the sound segment propagating directly from the speaker to the microphone whereas the echo period is the sound segment reflected back by surrounding objects. To achieve an accurate proximity detection, we develop a new energy loss compensation extraction scheme by utilizing the extracted chirp periods to estimate the intrinsic differences of energy loss between microphones of the enrolled phone and the login device. Our proximity detection component then conducts the similarity comparison between the identified two echo periods after the energy loss compensation to effectively determine whether the enrolled phone and the login device are in proximity for 2FA. Moreover, to provide higher security, our device fingerprint-assisted proximity detection further utilizes the overall energy loss between the login device and enrolled phone as their hardware fingerprint to authenticate the identity of two devices. Our experimental results show that our system is accurate in providing 2FA and robust to both man-in-the-middle (MiM) and co-located attacks across different scenarios and device models.}
}


@article{DBLP:journals/tmc/ZhangZWL24,
	author = {Jixian Zhang and
                  Yi Zhang and
                  Hao Wu and
                  Weidong Li},
	title = {An Ordered Submodularity-Based Budget-Feasible Mechanism for Opportunistic
                  Mobile Crowdsensing Task Allocation and Pricing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1278--1294},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2022.3232513},
	doi = {10.1109/TMC.2022.3232513},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing services are divided into two categories: opportunistic and participatory. In opportunistic mobile crowdsensing services, users do not need to specify the crowdsensing tasks to be completed. Compared with participatory crowdsensing services, the application scope is wider and more user-friendly. In participatory crowdsensing, the service provider assumes that the user can successfully complete the data collection task. However, such an approach cannot work in an opportunistic crowdsensing service because in opportunistic crowdsensing, the user’s execution of the task is uncertain, which brings great challenges to the quality of the crowdsensing service. This article is based on the assumption of the user coverage probability model and transforms the opportunistic mobile crowdsensing value maximization problem into an ordered submodularity value function model with budget constraints. This model is also good at representing participatory crowdsourcing problems. To the best of our knowledge, this is the first study to apply the ordered submodularity feature to a mobile crowdsensing service. Furthermore, we combine the properties of ordered submodular and auction models and propose an ordered submodularity-proportional share mechanism (O-PSM) to solve the allocation and payment problems in opportunistic mobile crowdsensing services. Specifically, in the allocation stage, the winning users are selected based on the proportional share threshold, and in the payment stage, the payment price for the winning users is designed based on critical value theory. We prove that the mechanism satisfies the economic characteristics of individual rationality, truthfulness, and budget feasibility. In the experimental section, the mechanism design based on ordered submodularity is shown to enable the service provider to obtain a higher value and a lower payment.}
}


@article{DBLP:journals/tmc/XieZZTLN24,
	author = {Ning Xie and
                  Jiaheng Zhang and
                  Qihong Zhang and
                  Haijun Tan and
                  Alex X. Liu and
                  Dusit Niyato},
	title = {Hybrid Physical-Layer Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1295--1311},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235061},
	doi = {10.1109/TMC.2023.3235061},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XieZZTLN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Physical-Layer Authentication (PLA) attracts a lot of research interests because of its significant advantages over upper-layer authentication mechanisms: high security and low complexity. The PLA schemes can be categorized into passive and active schemes. In this paper, we extensively leverage the advantages of both the active and passive schemes as a reference scheme, named as the Direct Hybrid (DH) scheme. Although the DH scheme improves the authentication performance of the prior PLA schemes, it has limitations, e.g., high communication overhead. Then, we further propose two hybrid PLA schemes to overcome the limitations of the DH scheme. The first proposed scheme further uses the advantage of the Challenge-Response Authentication Mechanism (CRAM) scheme, named as the CR-based Hybrid (CRH) scheme. Although both DH and CRH schemes significantly improve the authentication performance of the prior PLA schemes, they do not address one significant limitation of the active scheme, i.e., to set the power allocation of a tag empirically. Thus, based on the CRH scheme, we further propose the Adaptive CR-Based Hybrid (ACRH) scheme to adaptively set the parameter instead of the empirical setting. Moreover, we provide the theoretical analysis of the proposed schemes over wireless fading channels and derive their closed-form expressions in terms of the Probability of Detection (PD), Probability of False Alarm (PFA), and optimal threshold, respectively. At last, we discuss the advantages and disadvantages of the proposed schemes and give some useful suggestions for seeking a better tradeoff. Our experimental results show that, in comparison with the active scheme, the DH scheme has better robustness, and the CRH scheme has better both robustness and compatibility but it sacrifices the security. The ARCH scheme achieves a better tradeoff than the remaining schemes.}
}


@article{DBLP:journals/tmc/WangYSZXQWX24,
	author = {Shibo Wang and
                  Shusen Yang and
                  Hairong Su and
                  Cong Zhao and
                  Chenren Xu and
                  Feng Qian and
                  Nanbin Wang and
                  Zongben Xu},
	title = {Robust Saliency-Driven Quality Adaptation for Mobile 360-Degree Video
                  Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1312--1329},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235103},
	doi = {10.1109/TMC.2023.3235103},
	timestamp = {Mon, 22 Jul 2024 08:25:39 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYSZXQWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile 360-degree video streaming has grown significantly in popularity but the quality of experience (QoE) suffers from insufficient and variable wireless network bandwidth. Recently, saliency-driven 360-degree streaming overcomes the buffer size limitation of head movement trajectory (HMT)-driven solutions and thus strikes a better balance between video quality and rebuffering. However, inaccurate network estimations and intrinsic saliency bias still challenge saliency-based streaming approaches, limiting further QoE improvement. To address these challenges, we design a robust saliency-driven quality adaptation algorithm for 360-degree video streaming, RoSal360. Specifically, we present a practical, tile-size-aware deep neural network (DNN) model with a decoupled self-attention architecture to accurately and efficiently predict the transmission time of video tiles. Moreover, we design a reinforcement learning (RL)-driven online correction algorithm to robustly compensate the improper quality allocations due to saliency bias. Through extensive prototype evaluations over real wireless network environments including commodity WiFi, 4 G/LTE, and 5 G links in the wild, RoSal360 significantly enhances the video quality and reduces the rebuffering ratio, thereby improving the viewer QoE, compared to the state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/ZhangNAWTA24,
	author = {Jiangjiang Zhang and
                  Zhenhu Ning and
                  Raja Hashim Ali and
                  Muhammad Waqas and
                  Shanshan Tu and
                  Iftekhar Ahmad},
	title = {A Many-Objective Ensemble Optimization Algorithm for the Edge Cloud
                  Resource Scheduling Problem},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1330--1346},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235064},
	doi = {10.1109/TMC.2023.3235064},
	timestamp = {Sun, 06 Oct 2024 21:41:32 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangNAWTA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An edge cloud architecture plays a key role in improving the user task computing service system by combining the powerful data processing capability of cloud centres with the low latency of edge computing. Existing methods for maximizing the efficiency of an edge cloud architecture take into account time and task parameters but ignore other factors such as load balancing, cost, and user satisfaction when scheduling resources. In this work, we propose a many-objective resource scheduling model for optimizing the performance of an edge cloud architecture, which takes into account the time spent on task, cost, load balance, user satisfaction, and trust measurement. The resource scheduling model converges to the optimal solution using a novel many-objective ensemble optimization algorithm based on a dynamic selection mechanism. The study also explores the support set convergence of eight evolutionary operators using the ensemble algorithm. The model solutions are dynamically updated with the help of the dynamic integration probability, and then a selection criteria is used to pick the best solutions from the pool of generated solutions. Two simulations on a benchmark dataset are used to verify the usefulness and performance of the designed algorithm. Our approach was able to locate more than half of the best solutions on the benchmark functions, and it also showed to be a better model solution than the some of the popular many-objective algorithms for dealing with the edge cloud resource scheduling problem, according to the results obtained from the simulations.}
}


@article{DBLP:journals/tmc/WangXWZWBL24,
	author = {Chuyu Wang and
                  Lei Xie and
                  Jiaying Wu and
                  Keyan Zhang and
                  Wei Wang and
                  Yanling Bu and
                  Sanglu Lu},
	title = {Spin-Antenna: Enhanced 3D Motion Tracking via Spinning Antenna Based
                  on {COTS} {RFID}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1347--1365},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3236360},
	doi = {10.1109/TMC.2023.3236360},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangXWZWBL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rising of demands for novel Human-Computer Interaction (HCI) approaches in the 3D space, a number of intelligent approaches have been proposed to achieve the HCI by tracking the translation and rotation of the target devices. In this article, we propose to realize a light-weight, battery-free, 3D motion tracking solution by leveraging a spinning linearly polarized antenna to track a passive RFID tag array. Instead of using the fixed antennas, which can only receive stable signal in some specific environments due to the unpredictable multipath effect, we propose to mitigate the multipath effect and the ambient interference by continuously spinning a linearly polarized antenna, and then extract the most distinctive features based on the optimal reading conditions of the spinning antenna. In particular, because the phase variation around the matching direction is more stable while the RSSI variation around the mismatching direction is more distinctive, we leverage such matching/mismatching property of the linearly polarized antenna to extract the most distinctive features for motion tracking. To depict the property, we build a theoretical model to explain the RSSI and the phase variation of the RFID tag along with the spinning of the antenna, and further extend the model from a single RFID tag to an RFID tag array. Based on the model, we can extract the distinctive RSSI features for the rotation tracking and the stable phase features for the translation tracking. Moreover, to tackle the low rate of feature extraction due to the spinning of antenna, we further propose to enhance the unstable phase features based on the overall trend of other tags with interpolation, such that the sampling rate can be efficiently improved. Finally, we propose a LSTM (Long Short Term Memory)-based network to track the 3D motion based on the signal features extracted based on the polarization model. The experimental results show that our system can achieve an average error of 10.45 cm in the translation tracking, and an average error of $6.02^\\circ$ in the rotation tracking in the 3D space.}
}


@article{DBLP:journals/tmc/ZhaiLWZL24,
	author = {Chao Zhai and
                  Yujun Li and
                  Xinhua Wang and
                  Lina Zheng and
                  Chunguo Li},
	title = {Wireless Powered Cooperative {NOMA} With Alamouti Coding and Selection
                  Relaying},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1366--1381},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235394},
	doi = {10.1109/TMC.2023.3235394},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaiLWZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nonorthogonal multiple access (NOMA) can facilitate simultaneous data transmissions towards multiple users by using the superposition coding and successive interference cancelation techniques, which can greatly improve the spectrum efficiency. Cooperative relaying and space-time coding can promisingly improve the communication robustness of poor-quality links by achieving the space-diversity gain. Energy harvesting (EH) can prolong the lifetime of energy-limited terminals and make them work continuously. In order to enhance the spectrum efficiency as well as the communication quality, we enable a cluster of EH relays to assist the data transmissions from a base station (BS) to two far-users by using the Alamouti coding based cooperative NOMA strategy. The relays are capable of harvesting wireless energy from a power beacon as well as BS by using time-switching or power-splitting method. According to the energy status and the data decoding status, one relay is selected in a distributed manner according to either Max-min, Max-sum, or Random criterion. We analyze the transmission success probability and the system throughput performance. Extensive simulations are performed to compare the performance of different EH-based space-time coded cooperative NOMA with various relay selection schemes as well as the counterpart orthogonal transmission schemes.}
}


@article{DBLP:journals/tmc/XiaoZLSKNH24,
	author = {Yong Xiao and
                  Xiaohan Zhang and
                  Yingyu Li and
                  Guangming Shi and
                  Marwan Krunz and
                  Diep N. Nguyen and
                  Dinh Thai Hoang},
	title = {Time-Sensitive Learning for Heterogeneous Federated Edge Intelligence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1382--1400},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3237374},
	doi = {10.1109/TMC.2023.3237374},
	timestamp = {Wed, 02 Oct 2024 21:58:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoZLSKNH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time machine learning (ML) has recently attracted significant interest due to its potential to support instantaneous learning, adaptation, and decision making in a wide range of application domains. In this paper, we investigate real-time ML in a federated edge intelligence (FEI) system. We propose a time-sensitive federated learning (TS-FL) framework to minimize the overall run-time for training a shared ML model with desirable accuracy. Training acceleration solutions for both TS-FL with synchronous coordination (TS-FL-SC) and asynchronous coordination (TS-FL-ASC) are developed. To address the straggler effect in TS-FL-SC, we develop an analytical solution to characterize the impact of selecting different subsets of edge servers on the model training time. A server dropping-based solution is proposed to allow slow-performance edge servers to be removed from participating in the model training if their impacts on the model accuracy are limited. A joint optimization algorithm is proposed to minimize the overall model training time by selecting participating edge servers, local epoch number, and data batch sizes. We also develop an analytical expression to characterize the impact of staleness effect of asynchronous coordination on the training time of TS-FL-ASC. We propose a load forwarding-based solution that allows slow edge servers to offload part of their training samples to trusted edge servers with fast processing capability. We develop a hardware prototype to evaluate the performance of our proposed solutions. Experimental results show that TS-FL-SC and TS-FL-ASC can provide up to 63% and 28% of reduction in the overall model training time, respectively, compared with traditional solutions.}
}


@article{DBLP:journals/tmc/SharmaKT24,
	author = {Himanshu Sharma and
                  Neeraj Kumar and
                  Rajkumar Tekchandani},
	title = {SecBoost: Secrecy-Aware Deep Reinforcement Learning Based Energy-Efficient
                  Scheme for 5G HetNets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1401--1415},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235429},
	doi = {10.1109/TMC.2023.3235429},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SharmaKT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a secrecy-aware energy-efficient scheme for a two-tier heterogeneous network (HetNet), consisting of a sub-6 GHz macrocell and multiple millimeter wave (mmWave) picocells. Each picocell is assumed to have several users and an eavesdropper (Eve) which intercepts the signal of the picocell users. In the proposed scheme, firstly, to maximize the secrecy energy-efficiency (SEE) of picocell users, a joint optimization problem of power control, channel allocation, and beamforming is formulated by considering the minimum secrecy rate and signal-to-interference-plus-noise ratio (SINR) constraints. Due to the non-convex nature of the aforementioned optimization problem in a highly dynamic HetNet environment, we transform it into a reinforcement learning (RL) problem using the Markov decision process (MDP). Then, a multi-agent reinforcement learning (MARL) technique is used to obtain the maximum long-term reward. Moreover, we propose a multi-agent cooperative deep reinforcement learning (DRL) scheme known as SecBoost to solve the MDP with large number of action and state spaces. It uses the dueling and double-Q architecture of dueling double deep Q-network (D3QN) to optimize power control, channel allocation, and beamforming vectors to maximize the SEE of picocells. Also, prioritized experience replay is used to increase the sampling efficiency of SecBoost. The SEE performance of SecBoost is compared with MARL, multi-agent deep Q-network (MA-DQN), state-of-the-art joint beamforming based secrecy energy efficiency maximization (JBF-SEEM) scheme, and one-time pad based encrypted data transmission (O-EDT). Simulation results demonstrated that the proposed SecBoost scheme achieves 14.7%, 8.33%, 30%, and 69% better average SEE in comparison to MARL, MA-DQN, JBF-SEEM, and O-EDT schemes, respectively, which reveals its effectiveness in improving SEE of picocells.}
}


@article{DBLP:journals/tmc/LiCZZSJZ24,
	author = {Xufei Li and
                  Yin Chen and
                  Jinxiao Zhu and
                  Shuiguang Zeng and
                  Yulong Shen and
                  Xiaohong Jiang and
                  Daqing Zhang},
	title = {Fractal Dimension of {DSSS} Frame Preamble: Radiometric Feature for
                  Wireless Device Identification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1416--1430},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235497},
	doi = {10.1109/TMC.2023.3235497},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCZZSJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper demonstrates that the fractal dimension of frame preamble serves as a new radiometric feature that can be used together with other known radiometric features to enhance the identification accuracy in wireless device identification. We first propose a fractal dimension estimation scheme for direct-sequence spread spectrum (DSSS) frame preamble, then provide theoretical analysis to reveal how the fractal dimension is primarily determined by the device hardware imperfections, and thus prove that the fractal dimension serves as an intrinsic radiometric feature. We further show simulation results to verify our theoretical modeling of the fractal dimension and also numerically evaluate the effects of device hardware imperfections and wireless channels on the fractal dimension. Finally, by jointly applying the fractal dimension and the five features reported in the literature, we conduct extensive experiments to demonstrate that the fractal dimension can lead to a further improvement of the state-of-the-art result in the radiometric feature-based device identification.}
}


@article{DBLP:journals/tmc/ZhangWWZ24,
	author = {Jiangtao Zhang and
                  Qi Wang and
                  Qingshan Wang and
                  Zhiwen Zheng},
	title = {Multimodal Fusion Framework Based on Statistical Attention and Contrastive
                  Attention for Sign Language Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1431--1443},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235935},
	doi = {10.1109/TMC.2023.3235935},
	timestamp = {Sat, 10 Feb 2024 18:05:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sign language recognition (SLR) enables hearing-impaired people to better communicate with able-bodied individuals. The diversity of multiple modalities can be utilized to improve SLR. However, existing multimodal fusion methods do not take into account multimodal interrelationships in-depth. This paper proposes SeeSign: a multimodal fusion framework based on statistical attention and contrastive attention for SLR. The designed two attention mechanisms are used to investigate intra-modal and inter-modal correlations of surface Electromyography (sEMG) and inertial measurement unit (IMU) signals, and fuse the two modalities. Statistical attention uses the Laplace operator and lower quantile to select and enhance active features within each modal feature clip. Contrastive attention calculates the information gain of active features in a couple of enhanced feature clips located at the same position in two modalities. The enhanced feature clips are then fused in their positions based on the gain. The fused multimodal features are fed into a Transformer-based network with connectionist temporal classification and cross-entropy losses for SLR. The experimental results show that SeeSign has accuracy of 93.17% for isolated words, and word error rates of 18.34% and 22.08% on one-handed and two-handed sign language datasets, respectively. Moreover, it outperforms state-of-the-art methods in terms of accuracy and robustness.}
}


@article{DBLP:journals/tmc/HeWWXLYM24,
	author = {Qiang He and
                  Yu Wang and
                  Xingwei Wang and
                  Weiqiang Xu and
                  Fuliang Li and
                  Kaiqi Yang and
                  Lianbo Ma},
	title = {Routing Optimization With Deep Reinforcement Learning in Knowledge
                  Defined Networking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1444--1455},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3235446},
	doi = {10.1109/TMC.2023.3235446},
	timestamp = {Mon, 24 Feb 2025 11:31:02 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeWWXLYM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional routing algorithms cannot dynamically change network environments due to the limited information for routing decisions. Meanwhile, they are prone to performance bottlenecks in the face of increasingly complex business requirements. Some approaches, such as deep reinforcement learning (DRL) have been proposed to address the routing problems. However, they hardly utilize the information about the network environment fully. The Knowledge Defined Networking (KDN) architecture inspires us to develop new learning mechanisms adapted to the dynamic characteristics of the network topology. In this paper, we propose an effective scheme to solve the routing optimization problem by adding a graph neural network (GNN) structure to DRL, called Message Passing Deep Reinforcement Learning (MPDRL). MPDRL uses the characteristics of GNN to interact with the network topology environment and extracts exploitable knowledge through the message passing process of information between links in the topology. The goal is to achieve the load balance of network traffic and improve network performance. We have conducted experiments on three Internet Service Provider (ISP) network topologies. The evaluation results show that MPDRL obtains better network performance than the baseline algorithms.}
}


@article{DBLP:journals/tmc/AmadeoCLMR24,
	author = {Marica Amadeo and
                  Claudia Campolo and
                  Gianmarco Lia and
                  Antonella Molinaro and
                  Giuseppe Ruggeri},
	title = {In-Network Placement of Reusable Computing Tasks in an SDN-Based Network
                  Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1456--1471},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3237765},
	doi = {10.1109/TMC.2023.3237765},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AmadeoCLMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing is aimed to support compute-intensive data-hungry interactive applications which can hardly run on resource-constrained consumer devices and may suffer from running in the cloud due to the long data transfer delay. The edge network nodes’ heterogeneous and limited (compared to the cloud) capabilities make the computing task placement a challenge. In this paper, we propose a novel in-network task placement strategy aimed at minimizing the edge network resources usage. The proposal specifically accounts for time-limited reusable computing tasks, i.e., tasks whose output can be cached to serve requests from different consumers for a certain time. Caching such results, during their time validity, achieves the twofold benefit of reducing the service provisioning time and improving the edge resource utilization, by avoiding redundant computations and data exchange. The devised strategy is implemented as a network application of a Software-defined Networking Controller in charge of overseeing the edge domain. We formulate the optimal task placement through an integer linear programming problem, and we define an efficient heuristic algorithm that well approximates the solution achieved through a standard optimal solver. Achieved results show that the proposal successfully meets the targeted objectives in a wide variety of simulated scenarios, by outperforming benchmark solutions.}
}


@article{DBLP:journals/tmc/PuSYCJZX24,
	author = {Lingjun Pu and
                  Jianxin Shi and
                  Xinjing Yuan and
                  Xu Chen and
                  Lei Jiao and
                  Tian Zhang and
                  Jingdong Xu},
	title = {: Erasure-Coded Multi-Source Streaming for {UHD} Videos Within Cloud
                  Native 5G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1472--1487},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3238356},
	doi = {10.1109/TMC.2023.3238356},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PuSYCJZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-High-Definition (UHD) videos have been getting increasing attention. However, existing video streaming solutions fail to deliver them due to the extremely high bandwidth requirement. The emerging cloud native 5G networks have opened up the possibility of enhancing UHD video quality by leveraging in-network video streaming. Unfortunately, the restricted storage and bandwidth of in-network servers could become the main bottleneck. To this end, we present {\\sf EMS}, a novel UHD video streaming framework, by integrating Erasure-coded storage with Multi-source Streaming. We respectively introduce a deadline-aware and a latency-sensitive metric to indicate the service quality of video servers and advocate a federated learning paradigm for the adaptive service quality update, including a reinforcement learning based multi-server selection (i.e., user local training) and a global service quality aggregation. To facilitate user local training without sacrificing streaming Quality-of-Experience (QoE), we cast the multi-server selection associated with the restriction on the average number of selected servers per video chunk into two kinds of Multi-Armed Bandit (MAB) models in terms of the proposed service quality metrics. We design lightweight Upper Confidence Bound (UCB) based algorithms with a theoretical performance guarantee. We implement a prototype of {\\sf EMS}, and extensive experiments confirm the superiority of the proposed algorithms.}
}


@article{DBLP:journals/tmc/YangZSYC24,
	author = {Shuai Yang and
                  Dongheng Zhang and
                  Ruiyuan Song and
                  Pengfei Yin and
                  Yan Chen},
	title = {Multiple WiFi Access Points Co-Localization Through Joint AoA Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1488--1502},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3239377},
	doi = {10.1109/TMC.2023.3239377},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangZSYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor localization is a fundamental task to many real-world applications, which however remains unresolved, especially with commodity WiFi Access Points (APs). In this paper, we tackle this problem and propose an accurate, robust, and real-time indoor localization system that can be directly deployed on commodity WiFi infrastructure. Specifically, the proposed system makes three key contributions: 1) we introduce a non-parametric metric to measure the accuracy of Angle of Arrival (AoA) estimation; 2) we are the first to explicitly consider the relationship among the AoAs of different APs and propose a multiple APs co-localization algorithm to exploit such a relationship to improve the localization performance; 3) we propose several strategies to reduce the computational complexity of our system to achieve real-time localization. Extensive experiments are conducted to evaluate the performance of the proposed system under various situations, which demonstrate that the proposed system can achieve a 4 degrees median error of AoA estimation and 30 cm localization median error, outperforming the state-of-the-art systems.}
}


@article{DBLP:journals/tmc/NingCNWGL24,
	author = {Zhaolong Ning and
                  Handi Chen and
                  Edith C. H. Ngai and
                  Xiaojie Wang and
                  Lei Guo and
                  Jiangchuan Liu},
	title = {Lightweight Imitation Learning for Real-Time Cooperative Service Migration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1503--1520},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3239845},
	doi = {10.1109/TMC.2023.3239845},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NingCNWGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the revolution of communication technology, the rapidly increasing number of mobile devices in edge networks generates various real-time service requests, requiring a considerable volume of heterogeneous resources all the time. However, edge devices with limited resources cannot afford substantial learning cost, while migrating services requires heterogeneous resources, especially for dynamic networks. To address these issues, we first establish a cooperative service migration framework and formulate a bi-objective optimization problem to optimize service performance and cost. By analyzing the optimal migration ratio of service cooperative migration, we propose an offline expert policy based on global states to provide optimal expert demonstrations. To realize real-time service migration based on observable states, we design a lightweight online agent policy to imitate expert demonstrations and leverage meta update to accelerate the model transfer. Experimental results show that our algorithm is exceptional in training cost and accuracy, and has significant superiors in multiple metrics such as the service latency and payment under different workloads, compared to other representative algorithms.}
}


@article{DBLP:journals/tmc/LinLLQLXL24,
	author = {Hao Lin and
                  Cai Liu and
                  Zhenhua Li and
                  Feng Qian and
                  Mingliang Li and
                  Ping Xiong and
                  Yunhao Liu},
	title = {Aging or Glitching? What Leads to Poor Android Responsiveness and
                  What Can We Do About It?},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1521--1533},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3237716},
	doi = {10.1109/TMC.2023.3237716},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinLLQLXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Almost all Android users have ever experienced poor responsiveness, including the common frame dropping events—slow rendering (SR) and frozen frames (FF), as well as the uncommon Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. This work takes two complementary approaches, controlled benchmarking and in-the-wild crowdsourcing, to comprehensively understand their prevalence, characteristics, and root causes, which turn out to be significantly different from common understandings and prior studies. We find that SR, FF, ANR, and SNR all occur prevalently on all the studied hardware models of Android phones, and better hardware does not seem to relieve ANR/SNR. Most surprisingly, they are oftentimes ascribed to defective software design that incurs substantial resource overuse—lightweight apps can experience severe SR/FF events due to redundant UI rendering, and the most ANR/SNR events stem from Android's aggressive implementation of write amplification mitigation. In fact, the former can be effectively overcome by simplifying the apps’ UI hierarchy, and we design a practical approach to address almost all (>99%) of the latter while only decreasing 3% of the data write speed with large-scale deployment. We have released our measurement code/data to the research community.}
}


@article{DBLP:journals/tmc/SuZLZ24,
	author = {Qian Su and
                  Qinghui Zhang and
                  Weidong Li and
                  Xuejie Zhang},
	title = {Primal-Dual-Based Computation Offloading Method for Energy-Aware Cloud-Edge
                  Collaboration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1534--1549},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3237938},
	doi = {10.1109/TMC.2023.3237938},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SuZLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of the Internet of Things (IoT), resource-constrained mobile edge computing (MEC) can no longer fully meet the needs of the rapidly growing number of mobile users; hence, cloud-edge collaborative computing has been developed. This paper focuses on the total energy consumption of the system and heterogeneity of scenarios, and a collaborative cloud-edge computation offloading approach with near real-time decision making is proposed. First, a general cloud-edge collaborative computation offloading model is abstracted from typical applications, and the energy consumption for edge and cloud offloading is calculated separately by considering both transmission and computational energy consumption. The problem is formulated as an integer linear program (ILP) with multidimensional resource constraints and is proven to be NP-hard. Then, a novel primal-dual computation offloading (PDCO) algorithm is designed to make near real-time offloading decisions one by one based on the sequential arrival of task requests. The approximation ratio of PDCO is derived through the weak duality property and the price-resource increment relationship. The experimental results show that under the guidance of total cost influenced by marginal prices, PDCO not only avoids blindly making offloading decisions but also effectively alleviates the shortage of resources on edge servers (ESs), approaching the optimal performance in terms of total energy consumption and resource utilization.}
}


@article{DBLP:journals/tmc/NguyenVVN24,
	author = {Hai N. Nguyen and
                  Marinos Vomvas and
                  Triet D. Vo{-}Huu and
                  Guevara Noubir},
	title = {{WRIST:} Wideband, Real-Time, Spectro-Temporal {RF} Identification
                  System Using Deep Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1550--1567},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3240971},
	doi = {10.1109/TMC.2023.3240971},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NguyenVVN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RF emissions’ detection, classification, and spectro-temporal localization are essential not only for understanding, managing, and protecting the radio frequency resources, but also for countering today's security threats such as jammers. Achieving this goal for wideband, real-time operation remains challenging. In this article, we present WRIST, a Wideband, Real-time, Spectro-Temporal RF Identification system. WRIST can detect, classify, and precisely locate RF emissions in time and frequency using RF samples of 100 MHz spectrum in real-time. The system leverages an one-stage object detection Deep Learning framework, and transfer learning to a multi-channel visual-based spectral representation. Towards developing WRIST, we devised an iterative training approach which leverages synthesized and augmented RF data to efficiently build a large dataset with high-quality labels. WRIST achieves over 99 \\% class detection accuracy, 94 \\% emission precision and recall, with less than 0.08 bandwidth and time offset ratios in a large anechoic chamber over-the-air environment. In the extremely congested in-the-wild environment, WRIST still achieves over 80 \\% precision and recall. WRIST currently supports five 2.4 GHz technologies (Bluetooth, Lightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are making our curated dataset available to the whole community. It comprises over 10 million labelled RF emissions from off-the-shelf wireless radios spanning the five classes of technologies.}
}


@article{DBLP:journals/tmc/XuXWTG24,
	author = {Yin Xu and
                  Mingjun Xiao and
                  Jie Wu and
                  Haisheng Tan and
                  Guoju Gao},
	title = {A Personalized Privacy Preserving Mechanism for Crowdsourced Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1568--1585},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3237636},
	doi = {10.1109/TMC.2023.3237636},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuXWTG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we focus on the privacy preserving mechanism design for crowdsourced Federated Learning (FL), where a requester can outsource its model training task to some workers via an FL platform. A potential way to preserve the privacy of workers’ local data is to leverage Differential Privacy (DP) mechanisms on local models. However, most of these studies cannot allow workers to dominate their own privacy protection levels by themselves. Thus, we propose a Personalized Privacy Preserving Mechanism, called P3M, to satisfy the heterogeneous privacy needs of workers, which consists of two parts. The first part includes a personalized privacy budget determination problem. We model it as a two-stage Stackelberg game, derive the personalized privacy budget for each worker and the optimal payment for the requester, and prove that they form a unique Stackelberg equilibrium. Second, we design a dynamic perturbation scheme to perturb model parameters. Through the theoretical analysis, we prove that P3M satisfies the desired DP property, and derive the bounds of the variance of average perturbed parameters and the convergence upper bound. This demonstrates that the global model accuracy can be controllable and P3M is endowed with the satisfactory convergence performance. In addition, we extend our problem to the scenario where the total privacy budget of all workers is limited, so as to prevent some workers from setting exorbitant privacy budgets. Under the privacy constraint, we re-determine the personalized privacy budget for each worker. Finally, exhaustive simulations of P3M are conducted based on real-world datasets, and the experimental results corroborate its effectiveness and practicability.}
}


@article{DBLP:journals/tmc/LouTJZL24,
	author = {Jiong Lou and
                  Zhiqing Tang and
                  Weijia Jia and
                  Wei Zhao and
                  Jie Li},
	title = {Startup-Aware Dependent Task Scheduling With Bandwidth Constraints
                  in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1586--1600},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3238868},
	doi = {10.1109/TMC.2023.3238868},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LouTJZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge computing, applications can be scheduled in the granularity of inter-dependent tasks to proximate edge servers to achieve high performance. Before execution, the edge server must initialize the corresponding runtime environment, named task startup. However, existing studies on dependent task scheduling severely ignore bandwidth constraints during task startups, which is impractical and incurs a long startup latency. To fill in this gap, we first model the task startup process with bandwidth constraints on edge servers. Then, we formulate the dependent task scheduling problem with startup latency in heterogeneous edge computing. To efficiently generate schedules and satisfy the real-time requirements in edge computing, a novel low-complexity list scheduling algorithm integrated with cloud clone, Startup-aware Dependent Task Scheduling (SDTS), is proposed. Constrained by bandwidth and computation resources, SDTS first coordinates task startup, dependent data transmission, and task execution to optimize each task's finish time. Then, a cloud clone for each task is deployed to utilize scalable resources and initialized runtime environments. Furthermore, task scheduling refinement is designed to release the bandwidth and computation resources consumed by redundant tasks and improve the schedule. Extensive simulations based on real-world datasets show that SDTS substantially reduces 30%-60% makespan compared with existing baselines.}
}


@article{DBLP:journals/tmc/JiangG24,
	author = {Maoran Jiang and
                  Wei Gong},
	title = {Bidirectional Bluetooth Backscatter With Edges},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1601--1612},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241202},
	doi = {10.1109/TMC.2023.3241202},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiangG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bidirectional links are widely adopted in many active radios to provide efficient data exchanges, e.g., WiFi and Bluetooth. However, they are underexplored for general-purpose backscatter communications. The downlinks of state-of-the-art Bluetooth backscatter systems are based on packet length modulation, which is inefficient and unreliable. In this paper, we propose BiBlue, a bidirectional Bluetooth backscatter system that uses an edge server to enable fast and reliable downlinks. Specifically, our edge server acts like a bridge that can translate commercial Bluetooth signals to Amplitude Shift Keying (ASK) signals. In addition, we design a reliable edge-to-tag link that adaptively decodes ASK signal under variance. Finally, we propose a low-power uplink design that enables connection-based bidirectional communication with commodity devices. We prototype BiBlue using FPGA and Software Defined Radio (SDR). Extensive experimental results demonstrate that BiBlue achieves more than 62x downlink throughput gains over FreeRider and meanwhile supports an 80 cm communication range of edge-to-tag with BER at around 1%.}
}


@article{DBLP:journals/tmc/FrancoGP24,
	author = {Mirko Franco and
                  Ombretta Gaggi and
                  Claudio Enrico Palazzi},
	title = {Can Messaging Applications Prevent Sexting Abuse? a Technology Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1613--1626},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3238189},
	doi = {10.1109/TMC.2023.3238189},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FrancoGP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The digital and mobile revolutions have changed the way people live their sexuality. Sexting, the practice of sending or receiving any sexually explicit content through mobile devices, has gained popularity, especially amongst teenagers and young adults, bringing several concerns, such as the uncontrolled spread of personal nude or semi-nude media without the owner's consent. Moreover, messaging applications generally used to communicate (and practice sexting) are not safe enough, e.g., they permit to send and forward any received content to anyone else. In this scenario, we believe that, beside education, technological solutions should be devised to avoid or limit sexting abuse. To this aim, we have developed SafeSext, a proof of concept messaging system, which also implements an image forwarding control feature. Through it, we have analyzed possible solutions, as well as their limits, in supporting a safer messaging environment where users retain some form of control over the forwarding of their self-generated sensitive contents.}
}


@article{DBLP:journals/tmc/ChenLTH24,
	author = {Jiale Chen and
                  Duc Van Le and
                  Rui Tan and
                  Daren Ho},
	title = {NNFacet: Splitting Neural Network for Concurrent Smart Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1627--1640},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3238342},
	doi = {10.1109/TMC.2023.3238342},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various deep neural networks (DNNs) including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have shown appealing performance in various classification tasks. However, due to their large sizes, a single DNN often cannot fit into the memory of resource-constrained smart IoT sensors. This paper presents a DNN splitting framework called NNFacet that aims to run a DNN-based classification task on a total of N\nconcurrent battery-based sensors observing the same physical process. We begin with determining the importance of all CNN filters or RNN units in learning each class. Then, an optimization problem divides the class set into N\nsubsets and assigns them to the sensors, where the important CNN filters or RNN units associated with a class subset form a small model that is deployed to a sensor. Lastly, a multilayer perceptron is trained and deployed to a cloud or edge server, which yields the final classification result based on the low-dimensional features extracted by the sensors using their small models for the same observation. We apply NNFacet to three case studies of voice sensing, vibration sensing, and visual sensing. Extensive evaluation shows that NNFacet outperforms four baseline approaches in terms of system lifetime, latency, and classification accuracy.}
}


@article{DBLP:journals/tmc/ShiHWZH24,
	author = {Siping Shi and
                  Chuang Hu and
                  Dan Wang and
                  Yifei Zhu and
                  Zhu Han},
	title = {Federated {HD} Map Updating Through Overlapping Coalition Formation
                  Game},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1641--1654},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241090},
	doi = {10.1109/TMC.2023.3241090},
	timestamp = {Fri, 14 Feb 2025 17:43:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiHWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High Definition (HD) maps have become core supporting components for autonomous driving. To date, their updates heavily depend on the vehicle fleets of the map vendors, which cannot scale and timely reflect the highly dynamic environment. To ensure the HD map quality, it is advocated social vehicles should be used. Nevertheless, there are privacy concerns and a lack of incentives for social vehicles to contribute data. In this paper, we leverage federated analytics (FA), a newly developed collaborative data analytics paradigm, where raw data are kept local and only the insights generated from local analytics are sent to a server for aggregation. We present a new Federated Analytics based HD map Updating model (FAUMap) to protect the privacy of social vehicles. To motivate social vehicles to contribute data and improve the HD map quality, we formulate an overlapping coalition formation game, OCFUMap, and develop an algorithm to find feasible coalitions. Simulations show that our approach can improve the quality of the updated HD map by 1.56 times. To study an end-to-end operation of the FAUMap model and OCFUMap game, we present a case of HD map updates of the Powell street in San Francisco using the autonomous driving simulator CarLA.}
}


@article{DBLP:journals/tmc/SunSLWC24,
	author = {Zemin Sun and
                  Geng Sun and
                  Yanheng Liu and
                  Jian Wang and
                  Dongpu Cao},
	title = {{BARGAIN-MATCH:} {A} Game Theoretical Approach for Resource Allocation
                  and Task Offloading in Vehicular Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1655--1673},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3239339},
	doi = {10.1109/TMC.2023.3239339},
	timestamp = {Wed, 19 Jun 2024 17:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunSLWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular edge computing (VEC) is emerging as a promising architecture of vehicular networks (VNs) by deploying the cloud computing resources at the edge of the VNs. However, efficient resource management and task offloading in the VEC network is challenging. In this work, we first present a hierarchical framework that coordinates the heterogeneity among tasks and servers to improve the resource utilization for servers and service satisfaction for vehicles. Moreover, we formulate a joint resource allocation and task offloading problem (JRATOP), aiming to jointly optimize the intra-VEC server resource allocation and inter-VEC server load-balanced offloading by stimulating the horizontal and vertical collaboration among vehicles, VEC servers, and cloud server. Since the formulated JRATOP is NP-hard, we propose a cooperative resource allocation and task offloading algorithm named BARGAIN-MATCH, which consists of a bargaining-based incentive approach for intra-server resource allocation and a matching method-based horizontal-vertical collaboration approach for inter-server task offloading. Besides, BARGAIN-MATCH is proved to be stable, weak Pareto optimal, and polynomial complex. Simulation results demonstrate that the proposed approach achieves superior system utility and efficiency compared to the other methods, especially when the system workload is heavy.}
}


@article{DBLP:journals/tmc/LiSLLCCW24,
	author = {Xinyi Li and
                  Fengyi Song and
                  Mina Luo and
                  Kang Li and
                  Liqiong Chang and
                  Xiaojiang Chen and
                  Zheng Wang},
	title = {: Towards Collaborative and Cross-Domain Wi-Fi Sensing: {A} Case Study
                  for Human Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1674--1688},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3242324},
	doi = {10.1109/TMC.2023.3242324},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiSLLCCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quality of a learning-based Wi-Fi sensing system is bounded by the quantity and quality of training data. However, obtaining sufficient and high-quality data across different domains is difficult due to extensive user involvement. We present CARING, a federated-learning-based framework to support collaborative and cross-domain Wi-Fi sensing. A key challenge of CARING is to allow the effective exchange and learning of knowledge across local models that are derived from heterogeneous data sources with uneven data distributions. We overcome this challenge by first extracting the activity-related representation to train local models. The shared global model aggregates received local model parameters and sends them back to individual devices for fine-tuning locally in the deployed environment. By leveraging the crowdsourced knowledge, CARING allows local models to quickly adapt to domain changes using just a few samples seen at test time. We demonstrate the benefit of CARING by applying it to activity recognition across three public datasets collected from 5 environments, 7 deployments, 31 users, and 29 activities. Experimental results show that CARING is highly effective and robust, improving the alternative approach for using single-sourced training data by up to 47%, giving an accuracy of over 80% (up to 100%) for various cross-domain scenarios.}
}


@article{DBLP:journals/tmc/LiLCSL24,
	author = {Songfan Li and
                  Shengyu Li and
                  Minghua Chen and
                  Chao Song and
                  Li Lu},
	title = {Frequency Scaling Meets Intermittency: Optimizing Task Rate for RFID-Scale
                  Computing Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1689--1700},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3239515},
	doi = {10.1109/TMC.2023.3239515},
	timestamp = {Mon, 25 Nov 2024 22:15:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLCSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RFID (Radio Frequency Identification) computing devices in practical applications often suffer from their poor computing performance in terms of low task throughput (also known as task rate) due to scarce harvested power. For optimizing the task throughput, the basic idea is to choose an optimal processor clock frequency when executing a specific task (e.g.\n, operate sensor) in order to maximize task execution rate. Existing methods are based on the common sense where the frequency and task throughput are directly proportional to each other, meaning that a higher frequency causes a higher task rate. In RFID-scale devices, however, we observe that the relationship between the frequency and task throughput overturns the common sense, in which if the device rises the frequency, the task throughput will increase first and then decrease due to intermittent task execution pattern on such devices. In this article, we present a systematic task throughput model to explain and formulate the non-monotonic relationship between the frequency and task throughput. Based on the throughput model, we further introduce dynamic optimal frequency scaling (DOFS) to calculate the optimal frequency for task execution and thus optimize the task throughput in the RFID-scale devices. The experimental results show that the task throughput can be improved by 45.8% on average compared to the existing best effort.}
}


@article{DBLP:journals/tmc/WangWZSSDCLW24,
	author = {Lei Wang and
                  Haoran Wan and
                  Ting Zhao and
                  Ke Sun and
                  Shuyu Shi and
                  Haipeng Dai and
                  Guihai Chen and
                  Haodong Liu and
                  Wei Wang},
	title = {{SCALAR:} Self-Calibrated Acoustic Ranging for Distributed Mobile
                  Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1701--1716},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241304},
	doi = {10.1109/TMC.2023.3241304},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangWZSSDCLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic ranging has been viewed as a promising Human-Computer Interaction (HCI) technology in many scenarios, such as Augmented Reality (AR)/Virtual Reality (VR) and smart appliances. Most ranging systems with distributed devices undergo an extra calibration process to remove the timing errors. However, the calibration process needs user intervention. Furthermore, it should assume that the clock drifts are linear and stable, which is disabled within tens of minutes. In this paper, we introduce a self-calibrated acoustic ranging system that achieves sub-millimeter accuracy on distributed asynchronous devices. Based on our theoretical timing model, we precisely cancel both the system delay and the nonlinear clock drift with carefully designed Orthogonal Frequency-Division Multiplexing (OFDM) ranging signals. Our synchronization scheme achieves a timing accuracy of 1.9 microseconds, which allows us to build large-scale virtual acoustic arrays. Based on such a calibration scheme, our localization system achieves a ranging error of \\text{0.39}\\;mm within three meters in real-world experiments.}
}


@article{DBLP:journals/tmc/ChenLZCMH24,
	author = {Xing Chen and
                  Ming Li and
                  Hao Zhong and
                  Xiaona Chen and
                  Yun Ma and
                  Ching{-}Hsien Hsu},
	title = {FUNOff: Offloading Applications at Function Granularity for Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1717--1734},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3240741},
	doi = {10.1109/TMC.2023.3240741},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLZCMH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) offers a promising technology that deploys computing resources closer to mobile devices for improving performance. Most of the existing studies support on-demand remote execution of the computing tasks in applications through program transformation, but they commonly assume that mobile devices merely resort to a single server for computation offloading, which cannot make full use of the scattered and changeable computing resources. Thus, for object-oriented applications, we propose a novel approach, called FUNOff, to support the dynamic offloading of applications in MEC at the function granularity. First, we extract a call tree via code analysis and locate the function invocations that are suitable for offloading. Next, we refactor the code of related object functions according to a specific program structure. Finally, we make offloading decisions referring to the context at runtime and send function invocations to multiple remote servers for execution. We evaluate the proposed FUNOff on two real-world applications. The results show that, compared with other approaches, FUNOff better supports the computation offloading of object-oriented applications in MEC, which reduces the response time by 10.7%-58.2%.}
}


@article{DBLP:journals/tmc/WuLRYQGZ24,
	author = {Fan Wu and
                  Feng Lyu and
                  Ju Ren and
                  Peng Yang and
                  Kai Qian and
                  Shijie Gao and
                  Yaoxue Zhang},
	title = {Characterizing Internet Card User Portraits for Efficient Churn Prediction
                  Model Design},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1735--1752},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241206},
	doi = {10.1109/TMC.2023.3241206},
	timestamp = {Mon, 25 Mar 2024 12:48:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuLRYQGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular Internet card (IC) as a new business model emerges, which penetrates rapidly and holds the potential to foster a great business market. However, with the explosive growth of IC users, the user churn problem becomes severe, affecting the IC business significantly, while there is lacking appropriate techniques in the literature to deal with the issue. In this article, we take the lead to study one large-scale data set from a provincial network operator of China, which contains about 4 million IC users and 22 million traditional card (TC) users. We first justify the IC user churn issue with data, and categorize the user churning reasons. Then, we shed light on understanding user portraits, which is the building block to enable efficient model design. Particularly, we conduct a systematical analytics on usage data by studying the difference of two types of users, examining the impact of user properties, and characterizing the user Internet using behaviors. Finally, by using the IC user portraits and usage patterns, we propose an IC user Churn Prediction model, named ICCP, which consists of a feature extraction component and a learning-based churn prediction architecture design. For feature extraction, both the static portrait features and temporal sequential features are captured. In the learning architecture, we devise the principal component analysis (PCA) block and the embedding/transformer layers to learn the respective information of two types of features, which are collectively fed into the classification multilayer perceptron layer (MPL) for churn prediction. A reference implementation of ICCP is conducted within the telecom system and extensive experiments corroborate the efficiency of ICCP.}
}


@article{DBLP:journals/tmc/TsigkariIS24,
	author = {Dimitra Tsigkari and
                  George Iosifidis and
                  Thrasyvoulos Spyropoulos},
	title = {Quid Pro Quo in Streaming Services: Algorithms for Cooperative Recommendations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1753--1768},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3240006},
	doi = {10.1109/TMC.2023.3240006},
	timestamp = {Wed, 24 Jan 2024 17:49:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TsigkariIS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendations are employed by Content Providers (CPs) of streaming services in order to boost user engagement and their revenues. Recent works suggest that nudging recommendations towards cached items can reduce operational costs in the caching networks, e.g., Content Delivery Networks (CDNs) or edge cache providers in future wireless networks. However, cache-friendly recommendations could deviate from users’ tastes, and potentially affect the CP's revenues. Motivated by real-world business models, this work identifies the misalignment of the financial goals of the CP and the caching network provider, and presents a network-economic framework for recommendations. We propose a cooperation mechanism leveraging the Nash bargaining solution that allows the two entities to jointly design the recommendation policy. We consider different problem instances that vary on the extent these entities are willing to share their cost and revenue models, and propose two cooperative policies, CCR and DCR, that allow them to make decisions in a centralized or distributed way. In both cases, our solution guarantees reaching a fair and Pareto optimal allocation of the cooperation gains. Moreover, we discuss the extension of our framework towards caching decisions. A wealth of numerical experiments in realistic scenarios show the policies lead to significant gains for both entities.}
}


@article{DBLP:journals/tmc/SunZMGXD24,
	author = {Wen Sun and
                  Yong Zhao and
                  Wenqiang Ma and
                  Bin Guo and
                  Lexi Xu and
                  Trung Q. Duong},
	title = {Accelerating Convergence of Federated Learning in {MEC} With Dynamic
                  Community},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1769--1784},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241770},
	doi = {10.1109/TMC.2023.3241770},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunZMGXD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) brings computational resources to the edge of network that triggers the paradigm shift of centralized machine learning towards federated learning. Federated learning enables edge nodes to collaboratively train a shared prediction model without sharing data. In MEC, heterogeneous edge nodes may join or leave the training phase during the federated learning process, resulting in slow convergence of dynamic communities and federated learning. In this paper, we propose a fine-grained training strategy for federated learning to accelerate its convergence rate in MEC with dynamic community. Based on multi-agent reinforcement learning, the proposed scheme enables each edge node to adaptively adjust its training strategy (aggregation timing and frequency) according to the network dynamics, while compromising with each other to improve the convergence of federated learning. To further adapt to the dynamic community in MEC, we propose a meta-learning-based scheme where new nodes can learn from other nodes and quickly perform scene migration to further accelerate the convergence of federated learning. Numerical results show that the proposed framework outperforms the benchmarks in terms of convergence speed, learning accuracy, and resource consumption.}
}


@article{DBLP:journals/tmc/ShiCZWWG24,
	author = {Jun{-}ling Shi and
                  Peiyu Cong and
                  Liang Zhao and
                  Xingwei Wang and
                  Shaohua Wan and
                  Mohsen Guizani},
	title = {A Two-Stage Strategy for UAV-Enabled Wireless Power Transfer in Unknown
                  Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1785--1802},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3240763},
	doi = {10.1109/TMC.2023.3240763},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiCZWWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the outstanding merits such as mobility, high maneuverability, and flexibility, Unmanned Aerial Vehicles (UAVs) are viable mobile power transmitters that can be rapidly deployed in geographically constrained regions. They are good candidates for supplying power to energy-limited Sensor Nodes (SNs) with Wireless Power Transfer (WPT) technology. In this paper, we investigate a UAV-enabled WPT system that transmits power to a set of SNs at unknown positions. A key challenge is how to efficiently gather the locations of SNs and design a power transfer scheme. We formulate a multi-objective optimization problem to jointly optimize these objectives: maximization of UAV's search efficiency, maximization of total harvested energy, minimization of UAV's flight energy consumption and maximization of UAV's energy utilization efficiency. To tackle these issues, we present a two-stage strategy that includes a UAV Motion Control (UMC) algorithm for obtaining the coordinates of SNs and a Dynamic Genetic Clustering (DGC) algorithm for power transfer via grouping SNs into clusters. First, the UMC algorithm enables the UAV to autonomously control its own motion and conduct target search missions. The objective is to make the energy-restricted UAV find as many SNs as feasible without any priori known location information. Second, the DGC algorithm is used to optimize the energy consumption of the UAV by combining a genetic clustering algorithm with a dynamic clustering strategy to maximize the amount of energy harvested by SNs and the energy utilization efficiency of the UAV. Finally, experimental results show that our proposed algorithms outperform their counterparts.}
}


@article{DBLP:journals/tmc/KalitaK24,
	author = {Alakesh Kalita and
                  Manas Khatua},
	title = {Time-Variant {RGB} Model for Minimal Cell Allocation and Scheduling
                  in 6TiSCH Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1803--1814},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241021},
	doi = {10.1109/TMC.2023.3241021},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KalitaK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IEEE has standardized the 802.15.4e Time Slotted Channel Hopping (TSCH) mode to provide stringent latency, higher reliability, and low duty-cycle in various Internet of Things (IoT) applications. TSCH eliminates interference and multi-path fading on channels, but its channel hopping feature severely affects the 6TiSCH (IPv6 over IEEE 802.15.4e TSCH mode) network formation. Further, 6TiSCH Minimal Configuration standard does not provide sufficient bandwidth (i.e., minimal cell) for quick transmission of control packets required by the new nodes (i.e., pledges) during their network association. Many works have been proposed on 6TiSCH network formation as it has high impact on network performance and lifetime. However, the existing works either did not use all the available physical channels while allocating minimal cell(s) or are not stable with topology changes. Therefore, this work proposes a Time-Variant RGB (TRGB) model for minimal cell allocation and scheduling, which results in faster association of pledges and maintains network stability. We evaluate the TRGB using Markov Chain model and also on a real 60-node testbed in FIT IoT-LAB. Testbed results show that TRGB achieves 51% and 23% improvement over the state-of-the-art scheme in terms of joining time and energy consumption, respectively, while maintaining stability of the network.}
}


@article{DBLP:journals/tmc/XiaoXLSNHNK24,
	author = {Yong Xiao and
                  Rong Xia and
                  Yingyu Li and
                  Guangming Shi and
                  Diep N. Nguyen and
                  Dinh Thai Hoang and
                  Dusit Niyato and
                  Marwan Krunz},
	title = {Distributed Traffic Synthesis and Classification in Edge Networks:
                  {A} Federated Self-Supervised Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1815--1829},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3240821},
	doi = {10.1109/TMC.2023.3240821},
	timestamp = {Wed, 02 Oct 2024 21:58:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoXLSNHNK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rising demand for wireless services and increased awareness of the need for data protection, existing network traffic analysis and management architectures are facing unprecedented challenges in classifying and synthesizing the increasingly diverse services and applications. This paper proposes FS-GAN, a federated self-supervised learning framework to support automatic traffic analysis and synthesis over a large number of heterogeneous datasets. FS-GAN is composed of multiple distributed Generative Adversarial Networks (GANs), with a set of generators, each being designed to generate synthesized data samples following the distribution of an individual service traffic, and each discriminator being trained to differentiate the synthesized data samples and the real data samples of a local dataset. A federated learning-based framework is adopted to coordinate local model training processes of different GANs across different datasets. FS-GAN can classify data of unknown types of service and create synthetic samples that capture the traffic distribution of the unknown types. We prove that FS-GAN can minimize the Jensen-Shannon Divergence (JSD) between the distribution of real data across all the datasets and that of the synthesized data samples. FS-GAN also maximizes the JSD among the distributions of data samples created by different generators, resulting in each generator producing synthetic data samples that follow the same distribution as one particular service type. Extensive simulation results show that the classification accuracy of FS-GAN achieves over 20% improvement in average compared to the state-of-the-art clustering-based traffic analysis algorithms. FS-GAN also has the capability to synthesize highly complex mixtures of traffic types without requiring any human-labeled data samples.}
}


@article{DBLP:journals/tmc/KrunzASA24,
	author = {Marwan Krunz and
                  Irmak Aykin and
                  Sopan Sarkar and
                  Berk Akgun},
	title = {Online Reinforcement Learning for Beam Tracking and Rate Adaptation
                  in Millimeter-Wave Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1830--1845},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3243910},
	doi = {10.1109/TMC.2023.3243910},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KrunzASA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose MAMBA, a restless multi-armed bandit framework for beam tracking in directional millimeter-wave (mmW) cellular systems. Instead of relying on explicit control messages, MAMBA utilizes the ACK/NACK packets transmitted by user equipments (UEs) to the base station (BS) as a part of the hybrid automatic repeat request (HARQ) procedure. These packets are used to measure the quality of the currently operating downlink beam, and select a new downlink beam along with an appropriate modulation and coding scheme (MCS) for future transmissions. At its core, MAMBA implements an online reinforcement learning technique called adaptive Thompson sampling (ATS), which determines a good beam and associated MCS to be used for the upcoming transmissions. To evaluate MAMBA's performance, we conduct extensive simulations and over-the-air (OTA) experiments over the 28 GHz band using phased-array antennas. We study fixed- as well as adaptive-rate variants of MAMBA, and contrast it with four other beam tracking strategies: a beam selection scheme similar to the one used in 5G NR (called ‘static oracle’), a theoretically optimal but practically infeasible beam tracking scheme (called ‘dynamic oracle’), an \\epsilon-greedy algorithm (Mohamed 2021), and the Unimodal Beam Alignment (UBA) algorithm (Hashemi et al. 2018). Our results show that MAMBA achieves 182% throughput gain over the ‘static oracle’ and is reasonably close to the throughput of the ‘dynamic oracle’. Compared to UBA, MAMBA achieves 25-35% gain in throughput, depending on UE mobility. Finally, when operated at a fixed MCS, MAMBA/ATS achieves 21% gain over the \\epsilon-greedy algorithm at the lowest applied MCS index, and 255% gain at the highest MCS index.}
}


@article{DBLP:journals/tmc/NiuZSDWGC24,
	author = {Xiaoguang Niu and
                  Kaiyi Zou and
                  Da Shen and
                  Steve Drew and
                  Shaowu Wu and
                  Guangyi Guo and
                  Ruizhi Chen},
	title = {UltraMotion: High-Precision Ultrasonic Arm Tracking for Real-World
                  Exercises},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1846--1862},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241077},
	doi = {10.1109/TMC.2023.3241077},
	timestamp = {Sat, 13 Apr 2024 22:38:44 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NiuZSDWGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Home exercise and self-served gyms allow a larger population to exercise regularly without the cost of hiring private coaches. In absence of professional guidance, however, exercisers can suffer from injuries to muscles and joints. High-precision, affordable arm tracking with commercial, off-the-shelf (COTS) wearable devices has become an urgent need to prevent workout injuries and improve exercise performance. Recent studies with inertial measurement units (IMUs) or audio signals are neither computationally feasible for real-time motion tracking with satisfactory accuracy using COTS devices nor practically usable due to the interference with noisy ambient environments. In this paper, we propose UltraMotion, a real-time, high-precision ultrasonic arm motion tracking system designed for practical use. UltraMotion performs point cloud queries based on hidden Markov models (HMMs), a novel ultrasonic acoustic ranging method, and an extended Kalman filter (EKF) to predict the locations of all three arm joints, making it the first system offering shoulder locations. Experimental results with only a smartphone and a smartwatch demonstrate the effectiveness of UltraMotion in tracking shoulder, elbow, and wrist locations with impressively small median errors of 6.4 cm, 7.1 cm, and 8.5 cm in real-world environments, outperforming all previous systems, making UltraMotion an ideal choice for daily exercise.}
}


@article{DBLP:journals/tmc/PanZ24,
	author = {Sheng Pan and
                  Xinming Zhang},
	title = {Cooperative Gigabit Content Distribution With Network Coding for mmWave
                  Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1863--1877},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3241074},
	doi = {10.1109/TMC.2023.3241074},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PanZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) directional communication has been deemed as a promising means to achieve the goal of multi-gigabit data rate with low latency in vehicular ad-hoc networks (VANETs). However, due to the frequent transmission errors caused by the lossy nature of mmWave wireless links and high dynamic of VANETs, the channel resources are not fully exploited. In order to enhance the resilience to transmission errors, we introduce the symbol level network coding (SLNC) to mmWave wireless communications. We propose a greedy network coding strategy based on graph-theoretic approach, named GTNC, so as to take full advantage of SLNC. Moreover, considering the directionality of mmWave and the effect of GTNC on degrading transmission redundancy, it has great potential to realize concurrent transmission in mmWave vehicular networks. Therefore, we propose a cooperative concurrent distribution scheme with GTNC, named CCDS-GTNC, to coordinate transmissions between vehicles, which carries out collaborative vehicle-to-vehicle (V2V) and vehicle-to-Infrastructure (V2I) communications and achieves concurrent transmissions of multiple transmitters simultaneously. The simulation results show that our proposed scheme can effectively improve the efficiency of content dissemination in view of reducing the transmission delay and transmission redundancy.}
}


@article{DBLP:journals/tmc/HeWHG24,
	author = {Yejun He and
                  Xinying Wu and
                  Zhou He and
                  Mohsen Guizani},
	title = {Energy Efficiency Maximization of Backscatter-Assisted Wireless-Powered
                  {MEC} With User Cooperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1878--1887},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3243161},
	doi = {10.1109/TMC.2023.3243161},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeWHG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integrated backscatter communication (BackCom) and active communication (AC) scheme can improve wireless-powered mobile edge computing (WPMEC) system performance in general single-user and multi-user scenarios. However, there is little research in the cooperation-assisted WPMEC scenario. In this article, we consider a cooperation-assisted WPMEC system consisting of a source node (SN), a helper and a hybrid access point (HAP) integrated with MEC servers. An innovative user cooperation (UC) scheme with the integrated BackCom and AC is proposed to enhance the system performance. As a relay, the helper can help the SN to transmit its computing tasks due to the poor communication link between the SN and the HAP. To be specific, we aim at maximizing the user energy efficiency (EE) by jointly optimizing the backscatter reflection coefficient for BackCom, the transmission power for AC, the system time and task optimization allocation while considering the minimum computation bits requirement, the channel capacity and energy constraints. Based on a fractional program, we first transform the EE maximization problem into an equivalent one (non-convex problem) and then transform this non-convex problem into a convex problem by exploiting variable substitution and convex optimization. In addition, semi-closed form expressions of the optimal solution are deduced. An energy efficiency maximization algorithm is proposed to solve this convex problem. Simulation results demonstrate that the proposed scheme significantly improves the user EE than the existing schemes.}
}


@article{DBLP:journals/tmc/TangHHSY24,
	author = {Yinxu Tang and
                  Jianfeng Hou and
                  Xi Huang and
                  Ziyu Shao and
                  Yang Yang},
	title = {Green Edge Intelligence Scheme for Mobile Keyboard Emoji Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1888--1901},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3243955},
	doi = {10.1109/TMC.2023.3243955},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangHHSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emoji prediction has been widely adopted in most mobile keyboards to improve the quality of user experience. Considering the resource constraints of smartphones, it is promising to deploy well-trained prediction models on edge servers, with which smartphones can carry out emoji prediction in an online fashion. However, a key issue in such a scenario lies in how the smartphone should select a subset of models to achieve high-accuracy and real-time emoji prediction with energy efficiency (a.k.a. the model selection problem). Moreover, part of the system dynamics such as the prediction accuracy and the inference latency of each model are usually unknown a priori in practice, further complicating the problem. In this paper, with an effective integration of history-aware online learning and online control, we propose the first green edge intelligence scheme to solve the model selection problem for mobile keyboard emoji prediction. Our theoretical analysis and simulation results verify the effectiveness of our proposed scheme in achieving a sub-linear round-averaged regret bound and energy efficiency with a high prediction accuracy and a low latency.}
}


@article{DBLP:journals/tmc/ZhangLLWCZW24,
	author = {Xiaohan Zhang and
                  Peng Liu and
                  Bohao Lu and
                  Yang Wang and
                  Xiaohan Chen and
                  Yanxin Zhang and
                  Zhi Wang},
	title = {MTSFBet: {A} Hand-Gesture-Recognition-Based Identity Authentication
                  Approach for Passive Keyless Entry Against Relay Attack},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1902--1913},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3243772},
	doi = {10.1109/TMC.2023.3243772},
	timestamp = {Fri, 22 Mar 2024 08:59:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLLWCZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Passive Keyless Entry and Start system (PKES) has become an essential element of vehicle systems since it allows owners to lock or unlock their properties without having to take out the keys. However, the system suffers from a potential and serious security problem because of a relay attack. This paper introduces dynamic biometrics to resolve such potential problems. Precisely, it opts for hand-gesture and proposes a Multi-Task Siamese Feature-pyramid network with Bidirectional Gated Recurrent Units (MTSFBet) against relay attack. MTSFBet is made up of a Dynamic Biometrics Extraction Module (DBEM) and Multi-Task Operation Module (MTOM). DBEM extracts trajectory and kinematic features from the gesture data of owners, whereas MTOM performs gesture classification and identity authentication using these features. Moreover, we designed a loss function for the identity authentication part. Eventually, based on the dataset collected by mobile devices, we constructed comparative experiments and ablation studies to demonstrate the effectiveness of the method. Our comprehensive model achieves an accuracy of 0.86 in gesture classification and an accuracy of 0.73 in identity authentication. The results showed that our MTSFBet significantly outperforms other comparison methods. The MTSFBet can also be used in any scenario associated with identity authentication based on hand gestures.}
}


@article{DBLP:journals/tmc/GaoCLZLLLHR24,
	author = {Ming Gao and
                  Yike Chen and
                  Yimin Li and
                  Lingfeng Zhang and
                  Jianwei Liu and
                  Li Lu and
                  Feng Lin and
                  Jinsong Han and
                  Kui Ren},
	title = {A Resilience Evaluation Framework on Ultrasonic Microphone Jammers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1914--1929},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3244581},
	doi = {10.1109/TMC.2023.3244581},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GaoCLZLLLHR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Covert eavesdropping via microphones has always been a major threat to user privacy. Benefiting from the acoustic non-linearity property, the ultrasonic microphone jammer (UMJ) is effective in resisting this long-standing attack. However, prior UMJ researches underestimate adversary's attacking capability in reality and miss critical metrics for a thorough evaluation. The strong assumptions of adversary unable to retrieve information under low word recognition rate, and adversary's weak denoising abilities in the threat model make these works overlook the vulnerability of existing UMJs. As a result, their UMJs’ resilience is overestimated. In this paper, we refine the adversary model and completely investigate potential eavesdropping threats. Correspondingly, we define a total of 12 metrics that are necessary for evaluating UMJs’ resilience. Using these metrics, we propose a comprehensive framework to quantify UMJs’ practical resilience. It fully covers three perspectives that prior works ignored to some degree, i.e., ambient information, semantic comprehension, and collaborative recognition. Guided by this framework, we can thoroughly and quantitatively evaluate the resilience of existing UMJs towards eavesdroppers. Our extensive assessment results reveal that most existing UMJs are vulnerable to sophisticated adverse approaches. We further outline the key factors influencing jammers’ performance and present constructive suggestions for UMJs’ future designs.}
}


@article{DBLP:journals/tmc/JiangZCL24,
	author = {Yi Jiang and
                  Hongzi Zhu and
                  Shan Chang and
                  Bo Li},
	title = {{MAUTH:} Continuous User Authentication Based on Subtle Intrinsic
                  Muscular Tremors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1930--1941},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3243687},
	doi = {10.1109/TMC.2023.3243687},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiangZCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuous authentication is viewed to be increasingly important for mobile devices, which store a wide range of private data and sensitive information of users. Traditional continuous authentication methods need user inputs (e.g. typing, sliding). In this work, we present MAUTH, a zero-effect continuous authentication scheme for mobile devices. With the built-in motion sensors on commercial off-the-shelf (COTS) devices, MAUTH can continuously extract, classify and verify the unique tremor features of users on how their body intrinsically shakes during the normal use of such devices. As a result, it is extremely difficult if not impossible to reproduce the same set of tremors as individuals differ in their muscle development. We implement MAUTH as a software on Android-based smartphones, which demonstrates that MAUTH is light-weight and unobtrusive to its users. We conduct extensive real-world experiments and trace-driven simulations in controlled and uncontrolled environments on 21 volunteers. The results show that MAUTH is difficult to counterfeit and achieves a low average false positive rate (FPR) of 6.73% under real-world spoofing attacks. Moreover, MAUTH is comfortable to use and can achieve a low average false negative rate (FNR) of 2.2% during uncontrolled and continuous usage of devices, leveraging isolation-forest-based classifiers trained with only 40 training samples.}
}


@article{DBLP:journals/tmc/ArumGM24,
	author = {Steve Chukwuebuka Arum and
                  David Grace and
                  Paul Daniel Mitchell},
	title = {Extending Coverage and Capacity From High Altitude Platforms With
                  a Two-Tier Cellular Architecture},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1942--1953},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3244426},
	doi = {10.1109/TMC.2023.3244426},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ArumGM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional coverage and capacity from a high altitude platform (HAP) over an extended coverage area suffer significantly from inter-cell interference (ICI), antenna beam broadening, and uneven cell loading, which results in poor edge performance. In this paper, we show how a single antenna array on a HAP can be used to mitigate against these and achieve ubiquitous coverage by forming two tiers of a homogeneous contiguous cellular structure. We propose separate algorithms that implement the two-tier architecture with many antenna beams, which are used to form cells, and associate users with an appropriate cell and tier. A user associates with the cell and tier, which offer the best carrier power-to-noise ratio (CNR) and carrier power-to-interference-plus-noise ratio (CINR) respectively. The performance of the architecture, which is evaluated using simulation, is compared with a typical one-tier architecture. The results show that the two-tier architecture achieves over 30% higher user throughput and enhances throughput fairness and edge-of-cell connectivity by centralising as many users as possible within cells compared to the typical one-tier architecture. These benefits are better exploited by ensuring spectrum orthogonality between the two tiers.}
}


@article{DBLP:journals/tmc/WangZ24,
	author = {Qinsi Wang and
                  Sihai Zhang},
	title = {{DGL:} Device Generic Latency Model for Neural Architecture Search
                  on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1954--1967},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3244170},
	doi = {10.1109/TMC.2023.3244170},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The low-cost Neural Architecture Search (NAS) for lightweight networks working on massive mobile devices is essential for fast-developing ICT technology. Current NAS work can not search on unseen devices without latency sampling, which is a big obstacle to the implementation of NAS on mobile devices. In this paper, we overcome this challenge by proposing the Device Generic Latency (DGL) model. By absorbing processor modeling technology, the proposed DGL formula maps the parameters in the interval theory to the seven static configuration parameters of the device. And to make the formula more practical, we refine it to low-cost form by decreasing the number of configuration parameters to four. Then based on this formula, the DGL model is proposed which introduces the network parameters predictor and accuracy predictor to work with the DGL formula to predict the network latency. We propose the DGL-based NAS framework to enable fast searches without latency sampling. Extensive experiments results validate that the DGL model can achieve more accurate latency predictions than existing NAS latency predictors on unseen mobile devices. When configured with current state-of-the-art predictors, DGL-based NAS can search for architectures with higher accuracy that meet the latency limit than other NAS implementations, while using less training time and prediction time. Our work shed light on how to adopt domain knowledge into NAS topic and play important role in low-cost NAS on mobile devices.}
}


@article{DBLP:journals/tmc/XieCLGBWL24,
	author = {Lei Xie and
                  Zihao Chu and
                  Yi Li and
                  Tao Gu and
                  Yanling Bu and
                  Chuyu Wang and
                  Sanglu Lu},
	title = {Industrial Vision: Rectifying Millimeter-Level Edge Deviation in Industrial
                  Internet of Things With Camera-Based Edge Device},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1968--1984},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3246176},
	doi = {10.1109/TMC.2023.3246176},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XieCLGBWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, to realize the intelligent manufacturing in Industrial Internet of Things (IIoT) scenarios, novel approaches in computer vision are in great demand to tackle the new challenges in IIoT environment. These approaches, which we call Industrial Vision, are expected to offer customized solutions for intelligent manufacturing in an accurate, time efficient and robust manner. In this paper, we propose a novel approach to industrial vision, called Edge-Eye, to rectify the edge deviation automatically for Irradiated Cross-linked Polyethylene Foam (IXPE) production with millimeter-level accuracy. We deploy a commercial camera with mobile edge node in front of the IXPE sheet to continuously detect and rectify the edge deviation. Particularly, to handle the complex production environment when extracting the edge of IXPE sheet, we deploy a pair of reference bars with high-contrast colors to efficiently differentiate the sheet edge from the background. Then, we propose a Bi-direction Edge Tracking method to perform the edge detection from both vertical and horizontal aspects. To realize the rectification using mobile edge nodes with limited computing resources, we reduce the cost of computation by extracting the Minimized Region of Interest, i.e., the edge area overlapped with the higher contrast reference bar on both sides. We further design a negative feedback control system with multi-stage feedback regulation mechanism, keeping the edge deviation within millimeter-level. The experimental results show that Edge-Eye achieves the average accuracy of 5 mm for the edge deviation rectification, with the average latency of 200 ms for edge deviation detection.}
}


@article{DBLP:journals/tmc/ChenTLLZYYLYY24,
	author = {Long Chen and
                  Feilong Tang and
                  Jiacheng Liu and
                  Xu Li and
                  Yanmin Zhu and
                  Jiadi Yu and
                  Laurence T. Yang and
                  Zhetao Li and
                  Bin Yao and
                  Yichuan Yu},
	title = {Time-Varying Resource Graph Based Processing on the Way for Space-Terrestrial
                  Integrated Vehicle Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {2},
	pages = {1985--2002},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3248376},
	doi = {10.1109/TMC.2023.3248376},
	timestamp = {Fri, 26 Jan 2024 07:56:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenTLLZYYLYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Desirable information processing in space-terrestrial integrated vehicle networks (STINs) handles data distributed in different satellites while transmitting, where efficient modeling time-varying resources is critical. Existing works are not applicable to STINs, however, because they lack the joint consideration of different movement patterns and fluctuating loads. In this paper, we propose the Time-Varying Resource Graph (TVRG) to model dynamic resources in STINs, by leveraging the advantages of software-defined networking in flexible resource management. First, we propose the STIN mobility model to uniformly model different movement patterns in STINs. Then, we propose a layered Resource Modeling and Abstraction (RMA) approach, where evolutions of node resources are modeled as Markov processes, by encoding predictable topologies and influences of fluctuating loads as states. Besides, we propose the low-complexity domain resource abstraction algorithm by defining two mobility-based and load-aware partial orders on resource abilities. Finally, we formulate the TVRG-based Processing on the Way (TPoW) problem for data flows with processing requirements and multiple sources. We propose a Multi-level Processing on the Way (MPoW) approach with a bounded approximation ratio, realizing adaptive matching of resources and demands of processing and transmission. To evaluate the RMA approach, we propose a TVRG-based Routing (TR) algorithm for time-sensitive and bandwidth-intensive data flows, with the multi-level on-demand scheduling ability. Comprehensive simulation results demonstrate that our RMA-TR and MPoW outperform most related schemes by decreasing nearly 40% bandwidth consumption with the shortest end-to-end delay.}
}


@article{DBLP:journals/tmc/JiangXXWLQQ24,
	author = {Zhida Jiang and
                  Yang Xu and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Jianchun Liu and
                  Chen Qian and
                  Chunming Qiao},
	title = {Computation and Communication Efficient Federated Learning With Adaptive
                  Model Pruning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2003--2021},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3247798},
	doi = {10.1109/TMC.2023.3247798},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiangXXWLQQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a promising distributed learning paradigm that enables a large number of mobile devices to cooperatively train a model without sharing their raw data. The iterative training process of FL incurs considerable computation and communication overhead. The workers participating in FL are usually heterogeneous and the workers with poor capabilities may become the bottleneck of model training. To address the challenges of resource overhead and system heterogeneity, this article proposes an efficient FL framework, called FedMP, that improves both computation and communication efficiency over heterogeneous workers through adaptive model pruning. We theoretically analyze the impact of pruning ratio on training performance, and employ a Multi-Armed Bandit based online learning algorithm to adaptively determine different pruning ratios for heterogeneous workers, even without any prior knowledge of their capabilities. As a result, each worker in FedMP can train and transmit the sub-model that fits its own capabilities, accelerating the training process without hurting model accuracy. To prevent the diverse structures of pruned models from affecting the training convergence, we further present a new parameter synchronization scheme, called Residual Recovery Synchronous Parallel (R2SP). Besides, our proposed framework can be extended to the peer-to-peer (P2P) setting. Extensive experiments on physical devices demonstrate that FedMP is effective for different heterogeneous scenarios and data distributions, and can provide up to 4.1× speedup compared to the existing FL methods.}
}


@article{DBLP:journals/tmc/NgLXNPSM24,
	author = {Wei Chong Ng and
                  Wei Yang Bryan Lim and
                  Zehui Xiong and
                  Dusit Niyato and
                  H. Vincent Poor and
                  Xuemin Sherman Shen and
                  Chunyan Miao},
	title = {Stochastic Resource Optimization for Wireless Powered Hybrid Coded
                  Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2022--2038},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3246994},
	doi = {10.1109/TMC.2023.3246994},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NgLXNPSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enable ubiquitous Artificial Intelligence (AI) in the next-generation wireless communications networks, computation-intensive tasks such as data processing and model training have to be performed by energy-constrained end users. In this paper, we present a hybrid coded edge computing network whereby users can choose to complete their computation task through: i) local computation with the wireless power transfer derived from base stations, ii) coded edge offloading, or iii) hybrid computation involving edge offloading and local computation. To minimize the overall network cost, we propose a stochastic resource optimization approach. Given the stochastic nature of wireless charging efficiency and edge servers computation capacities, which can only be observed ex-post, a computation strategy for each user is determined using the two-stage stochastic integer programming (SIP). To address the complexity of the SIP problem which scales with the size of the network, we introduce the efficient computation methods of Benders’ decomposition and sample average approximation. Besides, we present a special case of z-stage stochastic offloading optimization that is applicable when the corrective edge offloading action can be executed in multiple stages, e.g., for non-time-sensitive tasks that do not need to be completed by stage two. Finally, we provide extensive sensitivity analyses to evaluate the performance of the proposed cost minimization approach amid varying network parameters. We demonstrate that our approach outperforms deterministic optimization approaches for in-network cost minimization.}
}


@article{DBLP:journals/tmc/TangFLWCZ24,
	author = {Jianzhi Tang and
                  Luoyi Fu and
                  Fei Long and
                  Xinbing Wang and
                  Guihai Chen and
                  Chenghu Zhou},
	title = {Which Link Matters? Maintaining Connectivity of Uncertain Networks
                  Under Adversarial Attack},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2039--2053},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3248629},
	doi = {10.1109/TMC.2023.3248629},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangFLWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article studies connectivity maintenance in uncertain networks under adversarial attack, where a defender conceals crucial links to prevent the largest connected component from being decomposed by an attacker. In contrast with its static counterpart, connectivity maintenance in uncertain networks involves additional probing on links to determine their existence. Therefore, by modeling an uncertain network as a random graph with each link associated with an existence probability and a probing cost, our goal is to design a defensive strategy for link selection that maximizes the expected size of the largest remaining connected component with the minimum expected probing cost, and moreover, the strategy should be independent of the attacking patterns. To this end, we first unravel the computational complexity of the problem by proving its NP-hardness, and then propose optimal defensive strategies based on dynamic programming and multi-objective optimization. Due to the prohibitive computational cost of optimality, two approximate defensive strategies are further designed to pursue decent performance with quasilinear complexity, in which the first one is a heuristic approach that quantifies the link vulnerability through an analogy from the degree centrality of a vertex in static networks to the connectivity weight of a link in uncertain networks, and the second one is an adaptive greedy policy incorporating the minimax rule from game theory, which minimizes the possible loss suffered by the defender in a worst-case scenario and has a constant approximation ratio. Extensive experiments on both synthetic and real-world network datasets under diverse attacking patterns demonstrate the superiority of the proposed strategies over baselines.}
}


@article{DBLP:journals/tmc/KroepGVP24,
	author = {Kees Kroep and
                  Vineet Gokhale and
                  Joseph P. Verburg and
                  R. Venkatesha Prasad},
	title = {{ETVO:} Effectively Measuring Tactile Internet With Experimental Validation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2054--2065},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3246659},
	doi = {10.1109/TMC.2023.3246659},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KroepGVP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next frontier in communications is teleoperation – manipulation and control of remote environments with haptic feedback. Compared to conventional networked applications, teleoperation poses widely different requirements, ultra-low latency (ULL) is primary. Realizing ULL communication demands significant redesign of conventional networking techniques, and the network infrastructure envisioned for achieving this is termed as Tactile Internet (TI). The design of meaningful performance metrics is crucial for seamless TI communication. However, existing performance metrics fall severely short of comprehensively characterizing TI performance due to their inability to capture how well sensed signals are reproduced. We take Dynamic Time Warping(DTW) as the basis of our work and identify necessary changes for characterizing TI performance. Through substantial refinements to DTW, we design Effective Time- and Value-Offset (ETVO) – a new method for measuring the fine-grained performance of TI systems. Through an in-depth objective analysis, we demonstrate the improvements of ETVO over DTW. Through subjective experiments, we demonstrate that existing QoS and QoE methods fall short of estimating the TI session performance accurately. Using subjective experiments, we demonstrate the behavior of the proposed metrics, their ability to match theoretically derived performance, and finally, their ability to reflect user satisfaction in a practical setting.}
}


@article{DBLP:journals/tmc/HongD24,
	author = {Shu Hong and
                  Lingjie Duan},
	title = {Location Privacy Protection Game Against Adversary Through Multi-User
                  Cooperative Obfuscation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2066--2077},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3249465},
	doi = {10.1109/TMC.2023.3249465},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HongD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In location-based services(LBSs), it is promising for users to crowdsource and share their Point-of-Interest(PoI) information with each other in a common cache to reduce query frequency and preserve location privacy. Yet most studies on multi-user privacy preservation overlook the opportunity of leveraging their service flexibility. This paper is the first to study multiple users’ strategic cooperation against an adversary's optimal inference attack, by leveraging mutual service flexibility. We formulate the multi-user privacy cooperation against the adversary as a max-min adversarial game and solve it in a linear program. Unlike the vast literature, even if a user finds the cached information useful, we prove it beneficial to still query the platform to further confuse the adversary. As the linear program's computational complexity still increases superlinearly with the number of users’ possible locations, we propose a binary obfuscation scheme in two opposite spatial directions to achieve guaranteed performance with only constant complexity. Perhaps surprisingly, a user with a greater service flexibility should query with a less obfuscated location to add confusion. Finally, we provide guidance on the optimal query sequence among LBS users. Simulation results show that our crowdsourced privacy protection scheme greatly improves users’ privacy as compared with existing approaches.}
}


@article{DBLP:journals/tmc/RongWWZZ24,
	author = {Chenghao Rong and
                  Jessie Hui Wang and
                  Jilong Wang and
                  Yipeng Zhou and
                  Jun Zhang},
	title = {Live Migration of Video Analytics Applications in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2078--2092},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3246539},
	doi = {10.1109/TMC.2023.3246539},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RongWWZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to schedule resources efficiently or maintain applications’ continuity for mobile customers, edge platforms often need to adaptively migrate the applications on them. However, our measurement shows that existing migration solutions cannot solve the issue of migrating video analytics applications in edge computing because the memory states of video analytics applications have different characteristics from other applications. We conduct a breakdown analysis of the memory states of video analytics applications, and propose to treat three types of states separately with three different techniques, i.e., warm-up, sync, and replay, to minimize the negative influence of migrations on application performance. Based on this idea, we implement a prototype system in which two new components, i.e., state store and sidecar, are designed to achieve near-transparent live migration with minimal application code modifications. Evaluation experiments demonstrate that the time of application interruption caused by migrating a video analytics application with our solution is less than 405 ms, and our solution does not consume much resources.}
}


@article{DBLP:journals/tmc/WuSWLY24,
	author = {Liantao Wu and
                  Peng Sun and
                  Zhibo Wang and
                  Yanjun Li and
                  Yang Yang},
	title = {Computation Offloading in Multi-Cell Networks With Collaborative Edge-Cloud
                  Computing: {A} Game Theoretic Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2093--2106},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3246462},
	doi = {10.1109/TMC.2023.3246462},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuSWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread application of 5G and the Internet of things (IoT), edge computing and cloud computing have been collaboratively utilized for task offloading and processing. However, though the massive devices (e.g., smartphones) are organized into multi-cells, most of the existing works do not explore the computation offloading for edge-cloud computing under inter-cell interference. Thus, the offloading decisions may be inappropriate as the transmission rate is overestimated. To address this issue, we propose COMEC, a novel Computation Offloading scheme in Multi-cell networks with Edge-Cloud collaboration, which could minimize the total cost in terms of delay and energy consumption. Specifically, we first formulate COMEC as an optimization problem taking into account inter-cell interference. Then, considering the offloading decisions of all users are coupled, a non-cooperative game is formulated to minimize the total cost of each user in a distributed manner. We prove that this game is a general (ordinal) potential game and possesses a pure strategy Nash equilibrium (NE). Based on the finite improvement property of the potential game, we develop the corresponding computation offloading algorithm to achieve the NE. Finally, simulation results show that the proposed scheme can achieve superior performance in overall system cost compared with other baselines.}
}


@article{DBLP:journals/tmc/WeiLZCY24,
	author = {Zhiwei Wei and
                  Bing Li and
                  Rongqing Zhang and
                  Xiang Cheng and
                  Liuqing Yang},
	title = {Many-to-Many Task Offloading in Vehicular Fog Computing: {A} Multi-Agent
                  Deep Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2107--2122},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3250495},
	doi = {10.1109/TMC.2023.3250495},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WeiLZCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular fog computing (VFC) has emerged as a promising solution to mitigate vehicular network computation load. In the hierarchical VFC, vehicles are employed as mobile fog nodes at the edge to provide reliable and low-latency services. Particularly, since privately-owned vehicles are rational nodes, their intentions for both computation provision and service demand should be considered instead of overestimating their willingness. To remunerate the participation intentions of vehicles as well as improve vehicular fog resource utilization in the large-scale VFC, the trading-based mechanism is a potential solution. In this article, we propose a many-to-many task offloading framework based on the vehicular trading paradigm. This framework enables computational resource trading across different VFC subsystems and decides the multi-tier task offloading results based on the trading consensus. The trading process is viewed as a partially observable Markov decision process (POMDP) and a Multi-Agent Gated actor Attention Critic (MA-GAC) approach is designed to reach an effective and stable offload-and -serve cooperation among vehicles. Theoretical analyses and experiments verify the feasibility and efficiency of the proposed framework, and simulation results demonstrate that the coordinated MA-GAC approach not only benefits vehicles with higher long-term rewards but also optimizes the system social welfare in a distributed manner.}
}


@article{DBLP:journals/tmc/DaiHHWXY24,
	author = {Penglin Dai and
                  Yaorong Huang and
                  Kaiwen Hu and
                  Xiao Wu and
                  Huanlai Xing and
                  Zhaofei Yu},
	title = {Meta Reinforcement Learning for Multi-Task Offloading in Vehicular
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2123--2138},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3247579},
	doi = {10.1109/TMC.2023.3247579},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiHHWXY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing has been a promising solution to enable real-time service in vehicular networks. However, due to high dynamics of mobile environment and heterogeneous features of vehicular services, traditional expert-based or learning-based strategies has to update handcrafted parameters or retrain learning model, which leads to intolerant overhead. Therefore, this paper investigates the problem of multi-task offloading (MTO), where there exist multiple offloading scenarios with varying parameters, such as task topology, resource requirement and transmission/computation capability. The objective is to design a unified solution to minimize task execution time under different MTO scenarios. Accordingly, we develop a Seq2seq-based Meta Reinforcement Learning algorithm for MTO (SMRL-MTO). Specifically, a bidirectional gated recurrent units integrated with attention mechanism is designed to determine offloading action by encoding sequential offloading actions and showing different preferences to different parts of input sequence. Particularly, a meta reinforcement learning framework is designed based on model-agnostic meta learning, which trains a meta policy offline and fast adapts to new MTO scenario within a few training steps. Finally, we conduct performance evaluation based on task generator DAGGEN and realistic vehicular traces, which shows that the SMRL-MTO reduces task execution time by 11.36% on average compared with greedy algorithm.}
}


@article{DBLP:journals/tmc/LiuZXW24,
	author = {Linfeng Liu and
                  Houqian Zhang and
                  Jia Xu and
                  Ping Wang},
	title = {Providing Active Charging Services: An Assignment Strategy With Profit-Maximizing
                  Heat Maps for Idle Mobile Charging Stations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2139--2152},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3247441},
	doi = {10.1109/TMC.2023.3247441},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Internet of Electric Vehicles (IoEV), mobile charging stations (MCSs) have been deployed to complement fixed charging stations. Typically, MCSs are assigned to charge the electric vehicles with insufficient electricity which have made charging requests (termed IEVs). Moreover, there are some electric vehicles with insufficient electricity which have not made charging requests (termed quasi-IEVs). If idle MCSs are allowed to actively track quasi-IEVs according to their potential charging demand, then more IEVs could be promptly charged, and thus the charging profits of MCSs could be increased. However, due to the private ownership of electric vehicles, some private information cannot be provided in the potential charging demand of quasi-IEVs (e.g., the destinations and residual electricity), making the potential charging profits of idle MCSs hard to be evaluated, and thereby the proper assignments of idle MCSs are difficult to decide. To this end, we introduce the profit-maximizing heat maps to depict the potential charging demand of quasi-IEVs and evaluate the potential charging profits of idle MCSs. A profit-maximizing heat map remarks the positions around quasi-IEVs and displays them as continuous areas. Specifically, the different shades of colours are used to distinguish the quantities of potential charging profits of idle MCSs, and the sizes of coloured areas are used to indicate the possibility of quasi-IEVs passing through these positions. In this paper, we propose a Profit-Maximizing Assignment Strategy of Idle MCSs (PMASIM) to properly assign the idle MCSs to charge IEVs at selected charging positions, or track some quasi-IEVs according to the profit-maximizing heat maps. Extensive simulations and comparisons demonstrate the superior performance of PMASIM, i.e., with the profit-maximizing heat maps, the charging profits of MCSs are increased, and the proportion of charged IEVs is enhanced as well.}
}


@article{DBLP:journals/tmc/JiaoWHXW24,
	author = {Wenli Jiao and
                  Ju Wang and
                  Yelu He and
                  Xiangdong Xi and
                  Fuwei Wang},
	title = {SoilTAG: Fine-Grained Soil Moisture Sensing Through Chipless Tags},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2153--2170},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3253135},
	doi = {10.1109/TMC.2023.3253135},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiaoWHXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Soil moisture sensing plays an important role in agriculture, especially in greenhouses or vertical farms. However, existing soil moisture sensing systems are either expensive and require batteries or suffer from low accuracy, preventing their real-world applications. This paper introduces SoilTAG, a battery-free, chipless tag-based high accuracy soil moisture sensing system. The key insight is that the tag's resonator can convert changes in soil moisture levels into changes in the tag's frequency response. However, two challenges need to be addressed before applying the system to the real world. First, how to design a resonator whose frequency response is sensitive to even small moisture changes, which is the basis for high-precision moisture sensing. To solve the challenge, we design a special structure (i.e., a defected ground structure) as the tag's resonator and optimize its key parameters to increase the frequency response sensitivity for different soil moisture levels. Second, how to design a robust soil moisture feature that is independent of the tag's location changes, since the frequency response varies by both the tag location and soil moisture. To deal with this challenge, we introduce a relative frequency response feature whose amplitude ratio is only related to soil moisture levels and independent of the tag location changes. Extensive experiments show that SoilTAG achieves 90th percentile moisture sensing error of 2\\%, 3.64\\%, and 8\\% when the distance between transmitter and tag is 6 m, 10 m, and 13.9 m. Compared to current commodity sensors, SoilTAG saves the cost per sensor by more than 70%.}
}


@article{DBLP:journals/tmc/FanDXYYZ24,
	author = {Xiaoxuan Fan and
                  Xianjun Deng and
                  Yunzhi Xia and
                  Lingzhi Yi and
                  Laurence T. Yang and
                  Chenlu Zhu},
	title = {Tensor-Based Confident Information Coverage Reliability of Hybrid
                  Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2171--2185},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3250508},
	doi = {10.1109/TMC.2023.3250508},
	timestamp = {Wed, 20 Mar 2024 17:39:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanDXYYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread applications of the Hybrid Internet of Things (HIoT) have put forward higher requirements for network reliability. Coverage reliability is one of the important metrics of reliability, and reliable coverage ensures network data perception and transmission to improve the Quality of Service (QoS). In this article, we define Confident Information Coverage Reliability (CICR) based on the Confident Information Coverage Model (CIC), which comprehensively considers sensor multistate, sensor energy, coverage rate, and connectivity robustness to evaluate coverage reliability. Furthermore, a Tensor-based Confident Information Coverage Reliability Algorithm (T-CICR) is proposed based on tensor modeling to evaluate CICR. The algorithm uses a tensor-based Markov model to predict sensor multistate. Three tensors of coverage rate, sensor multistate, and sensor energy are constructed to provide unified representations. Simulation results show that our proposed algorithm can significantly improve coverage reliability in terms of duty cycle, coverage rate requirement, sensing range, Root Mean Square Error (RMSE) threshold, connectivity robustness requirement, and link reliability.}
}


@article{DBLP:journals/tmc/JiCZN24,
	author = {Jiequ Ji and
                  Lin Cai and
                  Kun Zhu and
                  Dusit Niyato},
	title = {Decoupled Association With Rate Splitting Multiple Access in UAV-Assisted
                  Cellular Networks Using Multi-Agent Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2186--2201},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3256404},
	doi = {10.1109/TMC.2023.3256404},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiCZN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In unmanned aerial vehicles (UAVs) assisted cellular networks, user association plays an important role in interference control and spectrum efficiency. In this paper, we study the performance of uplink-downlink decoupled (UDDe) user association in a multi-UAV assisted network in which each user can associate with different UAVs or the macro base station (MBS) for uplink (UL) and downlink (DL) transmissions. Since some popular data may be requested by multiple users, grouping these users and applying multicasting can significantly improve spectral efficiency. Unlike traditional linear precoding that treats interference entirely as noise, we propose a rate-splitting multiple access (RSMA) policy that employs rate splitting at the transmitter and successive interference cancellation (SIC) at the receiver. To be specific, the transmitted signal is split into a common part and a private part, and the interference is partially decoded and partially treated as noise. In this context, we formulate a joint optimization problem of UL-DL association and beamforming for maximizing the sum-rate of users in UL and that of multicast groups in DL under the constraints of UAV backhaul capacity and power budget. Since the formulated problem is non-convex with intricate states and an individual UAV may not know the rewards of other UAVs, we convert it into a robust partially observable Markov decision process (POMDP). Then we resort to multi-agent deep reinforcement learning (MADRL) that enables each UAV to learn and optimize its policy in a distributed manner. To achieve an optimal policy, we further propose an improved clip and count-based proximal policy optimization (PPO) algorithm to train actor and critic networks. Simulation results demonstrate the superiority of the proposed decoupled association strategy with RSMA and the MADRL learning algorithm.}
}


@article{DBLP:journals/tmc/TripathiPPGC24,
	author = {Sharda Tripathi and
                  Corrado Puligheddu and
                  Somreeta Pramanik and
                  Andres Garcia{-}Saavedra and
                  Carla{-}Fabiana Chiasserini},
	title = {Fair and Scalable Orchestration of Network and Compute Resources for
                  Virtual Edge Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2202--2218},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3254999},
	doi = {10.1109/TMC.2023.3254999},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TripathiPPGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The combination of service virtualization and edge computing allows for low latency services, while keeping data storage and processing local. However, given the limited resources available at the edge, a conflict in resource usage arises when both virtualized user applications and network functions need to be supported. Further, the concurrent resource request by user applications and network functions is often entangled, since the data generated by the former has to be transferred by the latter, and vice versa. In this paper, we first show through experimental tests the correlation between a video-based application and a vRAN. Then, owing to the complex involved dynamics, we develop a scalable reinforcement learning framework for resource orchestration at the edge, which leverages a Pareto analysis for provable fair and efficient decisions. We validate our framework, named VERA, through a real-time proof-of-concept implementation, which we also use to obtain datasets reporting real-world operational conditions and performance. Using such experimental datasets, we demonstrate that VERA meets the KPI targets for over 96% of the observation period and performs similarly when executed in our real-time implementation, with KPI differences below 12.4%. Further, its scaling cost is 54% lower than a centralized framework based on deep-Q networks.}
}


@article{DBLP:journals/tmc/LezcanoC24,
	author = {Daniel Lezcano and
                  Georges Da Costa},
	title = {{O/S} Level Interrupt Prediction for Performance and Energy Management
                  on Android},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2219--2230},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3253798},
	doi = {10.1109/TMC.2023.3253798},
	timestamp = {Fri, 08 Mar 2024 13:21:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LezcanoC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Billions smartphones and smart objects battery-powered use Android, i.e., on the Linux kernel. To save energy, the main kernel leverage is to put processors in a low power state as soon as they are idle. It predicts the next event to estimate the sleep duration and choose a sleep state accordingly. Several wake-up sources (interrupts, events...) impact this prediction which is usually done considering them as a single source. The resulting signal is nearly random and difficult to predict. Processors are recently supporting deeper idle states but the prediction paradigm was never challenged. We propose to predict the next event by splitting the wake-up source signal into simpler event patterns. We describe a fast and efficient algorithm and its kernel-level performance evaluation. We compare our approach with multiple reference sleep state selection algorithms on actual ARM and x86 boards using classical mobile workloads. Our proposal detects correctly (up to 20% improved correctness leading to 5% reduced energy consumption) the time of next interrupt, and thus the right sleep level for the processor. We show and discuss the energy impact of the tested prediction algorithm and we compare it with the different generations of sleep level managers in the Linux kernel.}
}


@article{DBLP:journals/tmc/ZabetianK24,
	author = {Negar Zabetian and
                  Babak Hossein Khalaj},
	title = {QoE-Aware Network Pricing, Power Allocation, and Admission Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2231--2240},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3253019},
	doi = {10.1109/TMC.2023.3253019},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZabetianK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In wireless communications, focusing on end-user satisfaction and maximizing network operators’ revenue are emerging business challenges. In this paper, we investigate the price-based power allocation problem where, first, base stations (BSs) set prices by maximizing users’ utility modeled by their mean opinion score (MOS). Then, each user's optimal power is set by maximizing operator revenue while ensuring the minimum data rate for each user. We propose a hybrid MOS-based pricing method to model users’ utility instead of the conventional achievable rate approach. Our hybrid approach applies a machine learning algorithm to model the MOS of the call service in terms of the received signal strength indicator (RSSI). In terms of MOS and outage probability, our proposed method outperforms the rate-based pricing method and the conventional objective MOS models. In addition, we consider a joint admission control and price-based power allocation problem. When a new user requests to connect, a central controller determines whether or not to accept the new connection based on the system's MOS and average outage probability. The results demonstrate a trade-off between the number of users admitted and their level of satisfaction, providing operators with crucial knowledge about how to use their network resources more effectively.}
}


@article{DBLP:journals/tmc/ZhuLZW24,
	author = {Botao Zhu and
                  Siyuan Lin and
                  Yifei Zhu and
                  Xudong Wang},
	title = {Collaborative Hyperspectral Image Processing Using Satellite Edge
                  Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2241--2253},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3253280},
	doi = {10.1109/TMC.2023.3253280},
	timestamp = {Fri, 14 Feb 2025 17:43:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancement of nanosatellite techniques has boosted the growth of satellite-originated data and applications. Satellite edge computing (SEC) is envisioned to provide in-orbit processing of the sensed data to save the scarce terrestrial-satellite communication resources and support mission-critical services. While most of the existing SEC studies mainly focus on general computing tasks, we present a two-tier collaborative processing framework for the important and unique hyperspectral image (HSI) processing task. Our framework carefully selects bands out of the collected HSIs and sends them back for further analysis. We first conduct a comprehensive data analysis to reveal the non-trivial relationship between the band selection and the eventual analytic performance. We then formulate the band selection problem in this collaborative setting as a utility maximization problem that jointly considers the analytic, energy, and communication factors. A novel multi-agent reinforcement learning approach, named MaHSI, is proposed to solve it in the dynamic SEC environment. Our multi-agent design judiciously embeds the complex correlations among bands as collaborations among agents and significantly reduces the exploration space. Extensive experiments on real-world HSI datasets prove that our approach not only outperforms the existing classical band selection algorithms in accuracy and inference speed but also brings the highest utility to the satellites.}
}


@article{DBLP:journals/tmc/ZhangXWWZLX24,
	author = {Jianhui Zhang and
                  Yanhong Xu and
                  Jiacheng Wang and
                  Hanxiang Wang and
                  Bei Zhao and
                  Liming Liu and
                  Feng Xia},
	title = {Efficient Throughput Maximization in Dynamic Rechargeable Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2254--2268},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3256007},
	doi = {10.1109/TMC.2023.3256007},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangXWWZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Dynamic Rechargeable Networks (DRNs), to maximize the throughput by efficient energy allocation, the existing studies usually consider the spatio-temporal dynamic factors of the harvested energy, and seldom the network dynamic factors simultaneously, such as the time variable network resources and wireless interference. To take the network dynamic factors together, this paper studies the quite challenging problem, the network throughput maximization in the DRNs. We introduce the Time-Expanded Graph (TEG) to describe the above dynamic factors in an obvious way and design the Single Pair Throughput maximization (SPT) algorithm based on TEG. In the case of multiple pairs of source-targets, this paper introduces the Garg and Könemann's framework and then designs the Multiple Pairs Throughput (MPT) algorithm to maximize the overall throughput of all pairs. To reduce the time complexity, this paper proposes a Distributed and Parallel Throughput algorithm (DPT). In real applications, the network dynamic factors may not be known in advance. This paper proposes an Online Time-Span Algorithm (OTA) with Markov approximation and Lyapunov optimization, and conducts the extensive numerical evaluation based on the simulated data and the data collected by our real system. The numerical simulation results demonstrate the throughput improvement of our algorithms.}
}


@article{DBLP:journals/tmc/WangSLHPHR24,
	author = {Zhibo Wang and
                  Yunan Sun and
                  Defang Liu and
                  Jiahui Hu and
                  Xiaoyi Pang and
                  Yuke Hu and
                  Kui Ren},
	title = {Location Privacy-Aware Task Offloading in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2269--2283},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3254553},
	doi = {10.1109/TMC.2023.3254553},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangSLHPHR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC), users can offload tasks to nearby MEC servers to reduce computation cost. Considering that the size of offloaded tasks could disclose user location information, several location privacy-preserving task offloading mechanisms have been proposed under the single-server scenario. However, to the best of our knowledge, none of them could provide a strict privacy protection guarantee or be applicable to the multi-server scenario where the user's location can be inferred more accurately if servers collude with each other. In this paper, we propose a novel location privacy-aware task offloading framework (LPA-Offload) for both single-server and multi-server scenarios, which provides strict and provable location privacy protection while achieving efficient task offloading. Specifically, we propose a location perturbation mechanism that allows each user to perturb its real location within a rational perturbation region and provides a differential privacy guarantee. To make a satisfactory offloading strategy, we propose a perturbation region determination mechanism and an offloading strategy generation mechanism that adaptively select a proper perturbation region according to the customized privacy factor, and then generate an optimal offloading strategy based on the perturbed location within the decided region. The determination of the perturbation region could achieve personalized privacy requirements while reducing computation cost. LPA-Offload is proved to satisfy (\\epsilon,\\delta)-differential privacy, and the experiments demonstrate the effectiveness of our framework.}
}


@article{DBLP:journals/tmc/ChiaraviglioLBEA24,
	author = {Luca Chiaraviglio and
                  Chiara Lodovisi and
                  Stefania Bartoletti and
                  Ahmed Elzanaty and
                  Mohamed{-}Slim Alouini},
	title = {Dominance of Smartphone Exposure in 5G Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2284--2302},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3252662},
	doi = {10.1109/TMC.2023.3252662},
	timestamp = {Sun, 06 Oct 2024 21:41:31 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChiaraviglioLBEA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of 5G networks is sometimes questioned due to the impact of ElectroMagnetic Field (EMF) generated by Radio Base Station (RBS) on users. The goal of this work is to analyze such issue from a novel perspective, by comparing RBS EMF against exposure generated by 5G smartphones in commercial deployments. The measurement of exposure from 5G is hampered by several implementation aspects, such as dual connectivity between 4G and 5G, spectrum fragmentation, and carrier aggregation. To face such issues, we deploy a novel framework, called 5G-EA, tailored to the assessment of smartphone and RBS exposure through an innovative measurement algorithm, able to remotely control a programmable spectrum analyzer. Results, obtained in both outdoor and indoor locations, reveal that smartphone exposure (upon generation of uplink traffic) dominates over the RBS one. Moreover, Line-of-Sight locations experience a reduction of around one order of magnitude on the overall exposure compared to Non-Line-of-Sight ones. In addition, 5G exposure always represents a small share (up to 38%) compared to the total one radiated by the smartphone.}
}


@article{DBLP:journals/tmc/CordeschiZTG24,
	author = {Nicola Cordeschi and
                  Weihua Zhuang and
                  Rahim Tafazolli and
                  Yue Gao},
	title = {Optimal Random Access Strategies for Trigger-Based Multiple-Packet
                  Reception Channels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2303--2320},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3258750},
	doi = {10.1109/TMC.2023.3258750},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CordeschiZTG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper focuses on trigger-based (TB) random access (RA) strategies for a multiple-packet reception channel with channel capability M (M-MPR channel), where up to M packets can be received simultaneously, while more than M concurrent packet transmissions result in collisions and are considered lost. We model the contention for the TB MPR framework and derive the optimal RA strategies that maximize two metrics: i) the normalized saturation throughput, and ii) the number of stations successfully occupying the MPR channel within each access round. We generalize the p-persistent carrier sense multiple access (CSMA) by enabling it to explore both the MPR dimension and the time dimension to adapt the access probabilities. We also propose suboptimal strategies to reduce the complexity, customized for the considered TB framework. Comprehensive performance evaluations and comparisons with respect to a wide range of system parameters and metrics are provided.}
}


@article{DBLP:journals/tmc/TangSZZSL24,
	author = {Chenjun Tang and
                  Wei Sun and
                  Xing Zhang and
                  Jin Zheng and
                  Jian Sun and
                  Chongpei Liu},
	title = {A Sequential-Multi-Decision Scheme for WiFi Localization Using Vision-Based
                  Refinement},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2321--2336},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3253893},
	doi = {10.1109/TMC.2023.3253893},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TangSZZSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, most mobile devices have WIFI and camera modules to locate their position. However, there are two main challenges in large, highly similar indoor environments (localization accuracy and localization time). Aiming to balance these problems, we propose a sequential-multi-decision integrated system that combines WIFI and vision to acquire users’ locations. This system has two phases: sequential fusion localization and adaptive multi-decision fusion localization. The former employs WIFI-based localization first, then image-based localization and fusion localization are used within the constraints of WIFI-based localization. In the WIFI-based localization phase, the gaussian process regression (GPR) model is used to construct a WIFI indoor map. Subsequently, we propose to apply the hybrid whale optimization algorithm (HWOA) to WIFI-based localization to improve its accuracy and stability. The latter uses an adaptive multi-decision fusion mechanism that integrates WIFI-based localization, image-based localization, and fusion localization to obtain the users’ location finally. The experiments show the effectiveness of HWOA applied to WIFI-based localization. We also experimentally evaluate the proposed fusion algorithm with other state-of-the-art fusion algorithms (e.g., accuracy and time) in a real environment (an area larger than 10,000m^{2}\n). The experimental results show that the proposed fusion system is competitive.}
}


@article{DBLP:journals/tmc/FelembanMPK24,
	author = {Noor Felemban and
                  Fidan Mehmeti and
                  Thomas F. La Porta and
                  Heesung Kwon},
	title = {{EDIR:} Efficient Distributed Image Retrieval of Novel Objects in
                  Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2337--2350},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3257268},
	doi = {10.1109/TMC.2023.3257268},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FelembanMPK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsourcing data collection from a network of mobile devices is useful in various applications. Mobile devices store a large amount of visual data that can aid in different application scenarios. Trained Convolutional Neural Networks (CNNs) can be deployed on mobile devices to be used in searching for objects of interest. Querying for novel objects, for which models have not been trained yet, presents some unique challenges. When novel objects are queried, new models must be trained and distributed to all edge devices. In this article, we propose an efficient method and a system, called EDIR, which enables answering these queries while taking into account the bandwidth limitations encountered in wireless networks, as well as the limited energy and computational power on mobile devices. Through extensive experimentation, we show that using distance-based classifiers, specifically those relying on the Cosine distance, leads to more efficient utilization of network resources by reducing the number of false positives. We perform analysis that enables the requester to tune the parameters of interest before issuing the query, and validate our theoretical results. EDIR reduces the amount of transferred data by more than 45% compared to other approaches while simultaneously achieving a good F1 score.}
}


@article{DBLP:journals/tmc/WangJZLLMD24,
	author = {Mengyuan Wang and
                  Hongbo Jiang and
                  Ping Zhao and
                  Jie Li and
                  Jiangchuan Liu and
                  Geyong Min and
                  Schahram Dustdar},
	title = {RoPriv: Road Network-Aware Privacy-Preserving Framework in Spatial
                  Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2351--2366},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3255232},
	doi = {10.1109/TMC.2023.3255232},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangJZLLMD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial Crowdsourcing (SC) has been an indispensable Location-based Service where the SC server assigns tasks to workers based on the locations of task requesters and workers, raising strong privacy concerns. Limited by the computational and time complexity, existing works prefer differential privacy-based methods to protect location privacy. However, most differential privacy-based works ignore the road network, perturbing locations on two-dimensional plane, resulting in more failures in tasks and moreover extensive privacy disclosure in practice. This paper aims to implement a multi-task assignment with both high utility and efficiency while protecting the location privacy of both task requesters and workers on road networks. Specifically, we design a Road Network-aware Exponential Mechanism and propose an Obfuscated Locations Selection algorithm to guarantee location privacy of all participants and extensive privacy. Then, we propose region distance. Based on this, we further formulate multi-task assignment as a Binary Linear Programming problem and a utility-aware optimization problem. We solve the first problem to obtain optimal efficiency and then propose a utility-aware optimization algorithm for the second problem to improve the utility. Our experiments demonstrate sufficient and stable privacy guarantee and the well-performance on both utility and efficiency of our framework.}
}


@article{DBLP:journals/tmc/VenierisALL24,
	author = {Stylianos I. Venieris and
                  M{\'{a}}rio Almeida and
                  Royson Lee and
                  Nicholas D. Lane},
	title = {{NAWQ-SR:} {A} Hybrid-Precision {NPU} Engine for Efficient On-Device
                  Super-Resolution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2367--2381},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3255822},
	doi = {10.1109/TMC.2023.3255822},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/VenierisALL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, image and video delivery systems have begun integrating deep learning super-resolution (SR) approaches, leveraging their unprecedented visual enhancement capabilities while reducing reliance on networking conditions. Nevertheless, deploying these solutions on mobile devices still remains an active challenge as SR models are excessively demanding with respect to workload and memory footprint. Despite recent progress on on-device SR frameworks, existing systems either penalize visual quality, lead to excessive energy consumption or make inefficient use of the available resources. This work presents NAWQ-SR, a novel framework for the efficient on-device execution of SR models. Through a novel hybrid-precision quantization technique and a runtime neural image codec, NAWQ-SR exploits the multi-precision capabilities of modern mobile NPUs in order to minimize latency, while meeting user-specified quality constraints. Moreover, NAWQ-SR selectively adapts the arithmetic precision at run time to equip the SR DNN's layers with wider representational power, improving visual quality beyond what was previously possible on NPUs. Altogether, NAWQ-SR achieves an average speedup of 7.9×, 3× and 1.91× over the state-of-the-art on-device SR systems that use heterogeneous processors (MobiSR), CPU (SplitSR) and NPU (XLSR), respectively.Furthermore, NAWQ-SR delivers an average of 3.2× speedup and 0.39 dB higher PSNR over status-quo INT8 NPU designs, but most importantly mitigates the negative effects of quantization on visual quality, setting a new state-of-the-art in the attainable quality of NPU-based SR.}
}


@article{DBLP:journals/tmc/GuoGWXZZCZ24,
	author = {Tao Guo and
                  Song Guo and
                  Feijie Wu and
                  Wenchao Xu and
                  Jiewei Zhang and
                  Qihua Zhou and
                  Quan Chen and
                  Weihua Zhuang},
	title = {Tree Learning: Towards Promoting Coordination in Scalable Multi-Client
                  Training Acceleration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2382--2394},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3259007},
	doi = {10.1109/TMC.2023.3259007},
	timestamp = {Mon, 21 Oct 2024 15:07:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoGWXZZCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Iteration based collaborative learning (CL) paradigms, such as federated learning (FL) and split learning (SL), faces challenges in training neural models over the rapidly growing yet resource-constrained edge devices. Such devices have difficulty in accommodating a full-size large model for FL or affording an excessive waiting time for the mandatory synchronization step in SL. To deal with such challenge, we propose a novel CL framework which adopts an tree-aggregation structure with an adaptive partition and ensemble strategy to achieve optimal synchronization and fast convergence at scale. To find the optimal split point for heterogeneous clients, we also design a novel partitioning algorithm by minimizing the idleness during communication and achieving the optimal synchronization between clients. In addition, a parallelism paradigm is proposed to unleash the potential of optimum synchronization between the clients and server to boost the distributed training process without losing model accuracy for edge devices. Furthermore, we theoretically prove that our framework can achieve better convergence rate than state-of-the-art CL paradigms. We conduct extensive experiments and show that our framework is 4.6× in training speed as compared with the traditional methods, without compromising training accuracy.}
}


@article{DBLP:journals/tmc/HeHLFGC24,
	author = {Shibo He and
                  Kang Hu and
                  Songyuan Li and
                  Lingkun Fu and
                  Chaojie Gu and
                  Jiming Chen},
	title = {A Robust RF-Based Wireless Charging System for Dockless Bike-Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2395--2406},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3255980},
	doi = {10.1109/TMC.2023.3255980},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeHLFGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past few years, dockless bike-sharing has become a popular means of public transportation and brought significant convenience to millions of citizens. As one of the key components of a shared bike, the smart locking/unlocking module has proposed a new challenge of how to provide robust power supplement for them. Current charging solutions for shared bikes are mainly based on mechanical power and solar power, and rarely take user experience and charging delay into consideration. In this article, we design a robust RF-based wireless charging system for dockless bike-sharing. Our system utilizes radio frequency (RF) power to provide stable charging service while preserving the quality of service. In our system, an RF wireless charging sensing node is integrated on the bike's basket, so that the mutual interference during charging process and space occupation can be reduced. In order to reduce charging delay, we first design an efficient charging direction scheduling algorithm for a single charger. Then, we extend the solution to multiple-charger scenarios via dynamic programming. Our system has been successfully implemented on a dockless bike-sharing system. The experimental results verify that our design can satisfy the charging demands of shared-bikes and achieve 85% of the optimal solution.}
}


@article{DBLP:journals/tmc/CorbettSJ24,
	author = {Matthew Corbett and
                  Jiacheng Shang and
                  Bo Ji},
	title = {GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze
                  Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2407--2421},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3255841},
	doi = {10.1109/TMC.2023.3255841},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CorbettSJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Augmented Reality (AR) devices become more prevalent and commercially viable, the need for quick, efficient, and secure schemes for pairing these devices has become more pressing. Current methods to securely exchange holograms require users to send this information through large data centers, creating security and privacy concerns. Existing techniques to pair these devices on a local network and share information fall short in terms of usability and scalability. These techniques either require hardware not available on AR devices, intricate physical gestures, removal of the device from the head, do not scale to multiple pairing partners, or rely on methods with low entropy to create encryption keys. To that end, we propose a novel pairing system, called GazePair, that improves on all existing local pairing techniques by creating an efficient, effective, and intuitive pairing protocol. GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate identical, independently generated symmetric encryption keys with 64 bits of entropy. GazePair also achieves improvements in pairing success rates and times over current methods. Additionally, we show that GazePair can extend to multiple users. Finally, we assert that GazePair can be used on any Mixed Reality (MR) device equipped with eye gaze tracking.}
}


@article{DBLP:journals/tmc/LiLXYL24,
	author = {Wenzhong Li and
                  Xiang Li and
                  Yeting Xu and
                  Yi Yang and
                  Sanglu Lu},
	title = {MetaABR: {A} Meta-Learning Approach on Adaptative Bitrate Selection
                  for Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2422--2437},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3260086},
	doi = {10.1109/TMC.2023.3260086},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLXYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video streaming is one of the most popular Internet applications that makes up a large amount of Internet traffic. A fundamental mechanism in video streaming is adaptive bitrate (ABR) selection which decides the proper compression level for each chunk of a video to optimize the users’ quality of experience (QoE). The existing ABR algorithms require significant tuning and do not generalize to diverse network conditions and personalized QoE objectives. In this article, we propose a novel framework for meta-learning based ABR design and discuss challenges of deploying learning based ABR mechanism in real-world video streaming systems. We utilize the proposed framework to design MetaABR, a novel adaptive bitrate selection algorithm based on meta-reinforcement learning to maximize users’ QoE. By jointly training multiple learning tasks with a shared meta-critic, it can provide transferrable meta-knowledge to supervise bitrate selection across tasks, and can be applied to efficiently learn a new task in unseen environment with only a few trials. We implement MetaABR on an emulation platform which connects to the Linux network protocol stack through virtual network interfaces. Extensive experiments based on real-world traces and wireless testbed show that MetaABR achieves the best comprehensive QoE compared with the state-of-the-art ABR algorithms in a variety of network environments.}
}


@article{DBLP:journals/tmc/GuoLK24,
	author = {Zhiwu Guo and
                  Ming Li and
                  Marwan Krunz},
	title = {Exploiting Successive Interference Cancellation for Spectrum Sharing
                  Over Unlicensed Bands},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2438--2455},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3264195},
	doi = {10.1109/TMC.2023.3264195},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoLK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Harmonious coexistence among different unlicensed wireless technologies has became increasingly important due to the spectrum shortage problem. As a representative case, we focus on addressing LTE-LAA and WiFi coexistence in unlicensed bands. Traditionally, collision avoidance-based medium access control (MAC) protocols adopted by LAA and WiFi have led to low channel utilization and fairness. In this paper, we explore physical-layer interference suppression techniques (e.g., successive interference cancellation, SIC) to enhance the spectrum utilization of coexisting LAA and WiFi networks. We propose a SIC-aware MAC protocol that embraces concurrent transmissions and optimizes the channel access strategy at the MAC layer, so as to mitigate the cross-technology interference and improve spectrum efficiency and fairness. We theoretically analyze the network throughput by extending Bianchi's Markov model, considering the impact of concurrent transmissions and SIC. We also extend the analysis to consider MIMO and MU-MIMO links with SIC. We validate our theoretical analysis and the effectiveness of the proposed MAC protocol via extensive simulations. We also implement a prototype LAA/WiFi SIC receiver on USRP devices to demonstrate the feasibility of cross-technology SIC and the proposed MAC protocol.}
}


@article{DBLP:journals/tmc/YinRZZH24,
	author = {Jiaming Yin and
                  Weixiong Rao and
                  Qinpei Zhao and
                  Chenxi Zhang and
                  Pan Hui},
	title = {Learn to Optimize the Constrained Shortest Path on Large Dynamic Graphs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2456--2469},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3258974},
	doi = {10.1109/TMC.2023.3258974},
	timestamp = {Wed, 07 Aug 2024 17:08:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YinRZZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The constrained shortest path (\\mathtt {CSP}) problem has wide applications in travel path planning, mobile video broadcasting and network routing. Existing works do not work well on large dynamic graphs and suffer from either ineffectiveness or low scalability issues. To overcome these issues, in this paper, we propose an efficient and effective solution framework, namely \\mathtt {CSP\\_{G}S}. The solution framework includes two key components: (1) the techniques to decompose a large \\mathtt {CSP} instance into multiple small sub-instances and (2) the developed learning model \\mathtt {CSP\\_{D}QN} to solve small \\mathtt {CSP} instances. The evaluation result on real road network graphs indicates that our approach \\mathtt {CSP\\_{G}S} performs well on large dynamic graphs by rather high quality and reasonable running time, and particularly adapt to significant graph changes even with broken edges. To the best of our knowledge, this is the first learning-based model to well solve the \\mathtt {CSP} problem on large dynamic graphs.}
}


@article{DBLP:journals/tmc/XuLLXXZRL24,
	author = {Zichuan Xu and
                  Dongrui Li and
                  Weifa Liang and
                  Wenzheng Xu and
                  Qiufen Xia and
                  Pan Zhou and
                  Omer F. Rana and
                  Hao Li},
	title = {Energy or Accuracy? Near-Optimal User Selection and Aggregator Placement
                  for Federated Learning in {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2470--2485},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3262829},
	doi = {10.1109/TMC.2023.3262829},
	timestamp = {Thu, 20 Jun 2024 15:06:43 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLLXXZRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To unveil the hidden value in the datasets of user equipments (UEs) while preserving user privacy, federated learning (FL) is emerging as a promising technique to train a machine learning model using the datasets of UEs locally without uploading the datasets to a central location. Customers require to train machine learning models based on different datasets of UEs, through issuing FL requests that are implemented by FL services in a mobile edge computing (MEC) network. A key challenge of enabling FL in MEC networks is how to minimize the energy consumption of implementing FL requests while guaranteeing the accuracy of machine learning models, given that the availabilities of UEs usually are uncertain. In this paper, we investigate the problem of energy minimization for FL in an MEC network with uncertain availabilities of UEs. We first consider the energy minimization problem for a single FL request in an MEC network. We then propose a novel optimization framework for the problem with a single FL request, which consists of (1) an online learning algorithm with a bounded regret for the UE selection, by considering various contexts (side information) that influence energy consumption; and (2) an approximation algorithm with an approximation ratio for the aggregator placement for a single FL request. We third deal with the problem with multiple FL requests, for which we devise an online learning algorithm with a bounded regret. We finally evaluate the performance of the proposed algorithms by extensive experiments. Experimental results show that the proposed algorithms outperform their counterparts by reducing at least 13% of the total energy consumption while achieving the same accuracy.}
}


@article{DBLP:journals/tmc/JiZZCQX24,
	author = {Xiaoyu Ji and
                  Juchuan Zhang and
                  Shan Zou and
                  Yi{-}Chao Chen and
                  Gang Qu and
                  Wenyuan Xu},
	title = {MagView++: Data Exfiltration via {CPU} Magnetic Signals Under Video
                  Decoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {3},
	pages = {2486--2503},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3262400},
	doi = {10.1109/TMC.2023.3262400},
	timestamp = {Thu, 29 Feb 2024 20:54:12 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiZZCQX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Air-gapped networks achieve security by using physical isolation to keep the computers and network from the Internet. However, magnetic covert channels based on CPU utilization have been proposed to help secret data to exfiltrate from the Faraday-cage and the air gap. Despite the success of such covert channels, they suffer from the high risk of being detected by the transmitter computer and the challenge of installing malware into such a computer. In this article, we propose MagView++, where sensitive information is embedded in other data such as video and can be transmitted over the internal network. When any computer uses the data such as playing the video, the sensitive information will leak through the magnetic signals. The “separation” of information embedding and leaking, combined with the fact that the data can be exfiltrated from any computer in a distributed manner, overcomes these limitations. We demonstrate that CPU utilization for video decoding can be effectively controlled by changing the video frame type, reducing the quantization parameter, and changing the timestamp of the frame, without video quality degradation. We prototype MagView++ and achieve 8.9 bps throughput with 0.0057 BER when using a smartphone as the receiver, and 59 bps throughput with 0.0025 BER when using a dedicated devices with high sampling rate as the receiver. Experiments under various environments are conducted to show the robustness of MagView++. Limitations and possible countermeasures are also discussed.}
}


@article{DBLP:journals/tmc/SongCYR24,
	author = {Zilin Song and
                  Kwan{-}Wu Chin and
                  Changlin Yang and
                  Montserrat Ros},
	title = {Methods to Assign UAVs for K-Coverage and Recharging in IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2504--2519},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3259461},
	doi = {10.1109/TMC.2023.3259461},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongCYR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article studies a coverage problem in Internet of things (IoT) networks using unmanned aerial vehicles (UAVs) supported by solar-powered charging platforms. The problem at hand is to determine an assignment of UAVs to either a charging station or a monitoring point over a planning horizon. A key constraint is K\n-coverage, where given a set of \\mathcal {M}\npoints, K\nof these points must be monitored by a UAV in each time slot. In this respect, the paper aims to design UAVs assignment solutions that yield the longest K\n-coverage lifetime. We formulate a novel mixed integer linear program (MILP) to jointly optimize UAVs assignments over a given planning horizon. The problem is challenging as the energy level of charging platforms and UAVs are coupled across time slots. Moreover, the formulated MILP requires non-causal energy arrivals information at charging platforms. To this end, we outline a model predictive control (MPC) and a Monte Carlo tree search (MCTS) based solution that use non-causal energy arrivals information. The simulation results show that MPC and MCTS achieve approximately 81.04% and 67.07% of the optimal results computed by MILP.}
}


@article{DBLP:journals/tmc/DaiXJL24,
	author = {Xingxia Dai and
                  Zhu Xiao and
                  Hongbo Jiang and
                  John C. S. Lui},
	title = {UAV-Assisted Task Offloading in Vehicular Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2520--2534},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3259394},
	doi = {10.1109/TMC.2023.3259394},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiXJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular edge computing (VEC) provides an effective task offloading paradigm by pushing cloud resources to the vehicular network edges, e.g., road side units (RSUs). However, overloaded RSUs are likely to occur especially in urban aggregation areas, possibly leading to greatly compromised offloading performance. Inspired by this, this article explores this situation by introducing an unmanned aerial vehicle (UAV) to address the VEC overload problem. Specifically, we formulate a novel online UAV-assisted vehicular task offloading problem to minimize vehicular task delay under the long-term UAV energy constraint. To solve the formulated problem, we first decouple the long-term energy constraint based on the Lyapunov optimization technique. In this way, the problem can be solved in a real-time manner without requiring future information. Then, we construct a Markov chain based on Markov approximation optimization to find out the close-to-optimal UAV-assisted offloading strategies. Furthermore, we derive a mathematical analysis to rigorously demonstrate the offloading performance of the proposed algorithm. Additionally, the simulation results show that the proposed method outperforms the baselines by significantly reducing the vehicular task delay constrained by the long-term UAV energy budget under various system parameters, such as the energy budget and computation workloads.}
}


@article{DBLP:journals/tmc/ZhaoDH24,
	author = {Haotian Zhao and
                  Julian Camilo Gomez Diaz and
                  Sebastian Hoyos},
	title = {Multi-Channel Nonlinearity Mitigation Using Machine Learning Algorithms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2535--2550},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3259880},
	doi = {10.1109/TMC.2023.3259880},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates multi-channel machine learning (ML) techniques in the presence of receiver nonlinearities and noise, and compares the results with the single-channel receiver architecture. It is known that the multi-channel architecture relaxes the sampling speed requirement of analog to digital conversion and provides significant robustness to clock jitter and front-end noise due to the bandwidth-splitting property inherent in these receivers. However, when a high-voltage swing signal is used in a wireline communication link, the received signal suffers from third-order harmonic distortions and inter-modulation products caused by the nonlinearity profile of the analog front-end (AFE). To this end, this paper proposes the channel decision passing (CDP) algorithm in combination with nonlinear feedback cancellation as a low-complexity candidate for nonlinearity mitigation and compares the performance of this solution with other well-known ML algorithms. Simulation results show significant improvement in a multi-channel receiver architecture equipped with nonlinear feedback cancellation and CDP in comparison with its single-channel counterpart under practical nonlinearity profiles and noise conditions.}
}


@article{DBLP:journals/tmc/YiLLKSCLMR24,
	author = {Xingrui Yi and
                  Jianqiang Li and
                  Yutong Liu and
                  Linghe Kong and
                  Ying Shao and
                  Guihai Chen and
                  Xue Liu and
                  Shahid Mumtaz and
                  Joel J. P. C. Rodrigues},
	title = {ArguteDUB: Deep Learning Based Distributed Uplink Beamforming in 6G-Based
                  IoV},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2551--2565},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3262320},
	doi = {10.1109/TMC.2023.3262320},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YiLLKSCLMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the last decade, MIMO spatial multiplexing and distributed beamforming play a significant role in improving data throughput through cooperative transmission. It has been widely used in wireless communication, especially in 6G. However, the distributed uplink beamforming is still an open problem in highly dynamic environments. However, the proposed 6G technology represents the further integration of deep learning and wireless communication. In this article, we propose Argute Distributed Uplink Beamforming (ArguteDUB), which uses a feedback algorithm with an offline-trained deep learning model to implement highly dynamic distributed uplink beamforming for the Internet of Vehicles (IoV) in 6G. Specifically, each vehicle enables the base station (BS)/access point (AP) to separate different channel state information (CSI) by inserting orthogonal sequences into the sending data. The BS adopts deep learning to filter the noise and predict the beamforming weight to achieve phase synchronization. Unlike traditional distributed uplink beamforming, ArguteDUB can be adapted to the highly dynamic time-varying channels. The simple network structure ensures the fast response of ArguteDUB. In addition, we make ArguteDUB Orthogonal Frequency Division Multiplexing (OFDM) compatible so that it can be easily deployed in 6G networks. Our evaluation shows that ArguteDUB has an signal-to-noise ratio (SNR) gain of about 5 dB to 5.3 dB over the single vehicle transmission mode.}
}


@article{DBLP:journals/tmc/QiWWSRHKQ24,
	author = {Saiyu Qi and
                  Wei Wei and
                  Jianfeng Wang and
                  Shifeng Sun and
                  Leszek Rutkowski and
                  Tingwen Huang and
                  Janusz Kacprzyk and
                  Yong Qi},
	title = {Secure Data Deduplication With Dynamic Access Control for Mobile Cloud
                  Storage},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2566--2582},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3263901},
	doi = {10.1109/TMC.2023.3263901},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiWWSRHKQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data deduplication is of vital importance for mobile cloud computing to cope with the explosive growth of outsourced mobile data. In order to ensure the privacy of sensitive mobile data against an untrusted cloud, Message-Locked Encryption (MLE) has been proposed to enable deduplication over ciphertext. However, MLE prohibits data access control since it uses deterministic content-derived encryption keys. Recently, a lightweight rekeying-aware encrypted deduplication system (REED) has been proposed to achieve dynamic access control for secure data deduplication. However, REED is vulnerable to key-retaining attack and stub-retaining attack, which leads to insecure access revocation, and thus cannot support secure dynamic access control. In response, we present AC-Dedup, an encrypted deduplication storage system that supports secure dynamic access control for mobile cloud storage. At the core of AC-Dedup are two novel encryption techniques named mixed message locked encryption and random stub re-encryption to resist the two types of attacks, respectively. To the best of our knowledge, AC-Dedup is the first practical system that achieves secure data deduplication and secure dynamic access control simultaneously. We conduct security analysis and experimental evaluation on mobile device and cloud platform with real-world IoT datasets. The results show that AC-Dedup enables secure and efficient dynamic access control while preserving deduplication effectiveness.}
}


@article{DBLP:journals/tmc/SharmaGMD24,
	author = {Nelson Sharma and
                  Aswini Ghosh and
                  Rajiv Misra and
                  Sajal K. Das},
	title = {Deep Meta Q-Learning Based Multi-Task Offloading in Edge-Cloud Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2583--2598},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3264901},
	doi = {10.1109/TMC.2023.3264901},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SharmaGMD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource-constrained edge devices can not efficiently handle the explosive growth of mobile data and the increasing computational demand of modern-day user applications. Task offloading allows the migration of complex tasks from user devices to the remote edge-cloud servers thereby reducing their computational burden and energy consumption while also improving the efficiency of task processing. However, obtaining the optimal offloading strategy in a multi-task offloading decision-making process is an NP-hard problem. Existing Deep learning techniques with slow learning rates and weak adaptability are not suitable for dynamic multi-user scenarios. In this article, we propose a novel deep meta-reinforcement learning-based approach to the multi-task offloading problem using a combination of first-order meta-learning and deep Q-learning methods. We establish the meta-generalization bounds for the proposed algorithm and demonstrate that it can reduce the time and energy consumption of IoT applications by up to 15%. Through rigorous simulations, we show that our method achieves near-optimal offloading solutions while also being able to adapt to dynamic edge-cloud environments.}
}


@article{DBLP:journals/tmc/WangCCHL24,
	author = {Lei Wang and
                  Shuhan Chen and
                  Feifei Chen and
                  Qiang He and
                  Jiyuan Liu},
	title = {B-Detection: Runtime Reliability Anomaly Detection for {MEC} Services
                  With Boosting {LSTM} Autoencoder},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2599--2613},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3262233},
	doi = {10.1109/TMC.2023.3262233},
	timestamp = {Mon, 08 Apr 2024 09:01:10 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By pushing computing resources from the cloud to the network edge close to mobile users, mobile edge computing (MEC) enables low latency for a wide variety of applications. Nevertheless, in dynamic MEC systems, MEC services are challenged by the risks of runtime reliability anomalies. Detecting runtime reliability anomalies for MEC services is challenging yet critical to ensuring the stability of MEC systems. The effectiveness of existing anomaly detection methods suffers from poor performance when handling MEC services’ large-volume, continuous, and volatile reliability streaming data. The key is to identify significant changes in the distribution of MEC services’ current reliability streaming data compared with their historical performance. Inspired by concept drift, this paper proposes B-Detection, a boosting Long Short-Term Memory (LSTM) Autoencoder for detecting MEC services’ runtime reliability anomalies based on distribution dissimilarity evaluation. B-Detection employs a deep learning method named LSTM Autoencoder to characterize the MEC services’ historical reliability data distribution. To cope with the challenge of modeling complex distribution characteristics of MEC services’ historical reliability streaming data and guarantee the real-time performance of B-Detection, we enhance LSTM Autoencoder with a weight-based reservoir sampling technique and an LSTM boosting algorithm. The reconstruction loss of the trained LSTM Autoencoder model is estimated for the up-to-date reliability streaming data, and the result is used to infer MEC services’ runtime reliability anomalies. The performance of B-Detection is verified through a series of experiments conducted on a real-world dataset.}
}


@article{DBLP:journals/tmc/AsheralievaNM24,
	author = {Alia Asheralieva and
                  Dusit Niyato and
                  Yoshikazu Miyanaga},
	title = {Efficient Dynamic Distributed Resource Slicing in 6G Multi-Access
                  Edge Computing Networks With Online {ADMM} and Message Passing Graph
                  Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2614--2638},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3262514},
	doi = {10.1109/TMC.2023.3262514},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AsheralievaNM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of resource slicing in the 6th generation multi-access edge computing (6G-MEC) network. The network includes many non-stationary space-air-ground-sea nodes with dynamic, unstable connections and resources, where any node can be in one of two hidden states: i) reliable – when the node generates/propagates no data errors; ii) unreliable – when the node can generate/propagate random errors. We show that solving this problem is challenging, since it represents a non-deterministic polynomial-time (NP) hard dynamic combinatorial optimization problem depending on the unknown distribution of hidden nodes’ states and time-varying parameters (connections and resources of nodes) which can only be observed locally. To tackle these challenges, we develop a new deep learning (DL) model based on the message passing graph neural network (MPNN) to estimate hidden nodes’ states in dynamic network environments. We then propose a novel algorithm based on the integration of MPNN-based DL and online alternating direction method of multipliers (ADMM) – extension of the well-known classical “static” ADMM to dynamic settings, where the slicing problem is solved distributedly, in real time, based on local information. We prove that our algorithm converges to a global optimum of our problem with a superior performance even in the highly-dynamic, unreliable scenarios.}
}


@article{DBLP:journals/tmc/ChuHBK24,
	author = {Zhe Chu and
                  Fei Hu and
                  Elizabeth S. Bentley and
                  Sunil Kumar},
	title = {Intelligent Routing in Directional Ad Hoc Networks Through Predictive
                  Directional Heat Map From Spatio-Temporal Deep Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2639--2656},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3264447},
	doi = {10.1109/TMC.2023.3264447},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChuHBK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By applying a simple shortest/minimum-cost routing algorithm, the mobile ad-hoc network (MANET) with heavy data transmissions may be easily congested if multiple routes meet at the same relay node. Therefore, those busy nodes should be avoided when a new path is established. The task of optimal path seeking becomes more challenging when a MANET is equipped with directional antennas that may cause directional interference with neighboring receivers. The motivation of our research is to build an intelligent proactive routing scheme for MANETs with directional antennas. Our directional routing protocol considers not only the global traffic distribution in different areas of the MANET, but also the properties of directional antennas. It uses a spatio-temporal deep learning algorithm to predict the next-time snapshot of a directional heat map (DHM), which shows the traffic density distribution in each network location as well as the coverage of each directional antenna. The DHM is then used to identify the optimal path that can avoid congested areas as well as the interference from all neighboring directional links. Furthermore, an optimization algorithm is designed to perform optimal path selection. It splits a single path into multiple paths converge later on into one path, if the path needs to go around a congested area. Therefore, our routing scheme achieves better quality-of-service (QoS) performance than existing routing schemes.}
}


@article{DBLP:journals/tmc/WangYB24,
	author = {Shuoyao Wang and
                  Junyan Yang and
                  Suzhi Bi},
	title = {Adaptive Video Streaming in Multi-Tier Computing Networks: Joint Edge
                  Transcoding and Client Enhancement},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2657--2670},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3263046},
	doi = {10.1109/TMC.2023.3263046},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancement of multimedia technology and wireless networks, there is a growing demand for high-quality video streaming. Delivering stable video streaming in extremely dynamic wireless networks, nevertheless, is still an open problem. Recent developments in client computing and mobile edge computing (MEC) technologies have both shown promise in enhancing the adaptive bitrate (ABR) streaming services. In this paper, we consider a video streaming system in multi-tier computing networks, enabled by joint edge-side video transcoding and client-side video enhancement. By “enhancement,” we mean that the client improves the video chunk quality via client-side image processing modules. In particular, we aim to design a joint bitrate adaptation, edge transcoding, and client image-processing algorithm, maximizing the quality of experience (QoE) of streaming services. The majority of the prior art has concentrated on super-resolution-enabled video streaming. Contrarily, we show that the video enhancement method outperforms the super-resolution approach in terms of signal-to-noise ratio and frames per second, implying a superior alternative for client-side processing in ABR streaming. We formulate the problem as an event-triggered Markov decision process (E-MDP), and propose a deep reinforcement learning (DRL)-based framework, named EDTEA. To deal with the delayed feedback induced by multi-tier computing, the entropy and the expected re-buffering terms are introduced to the objective and the reward, respectively. Extensive simulations based on real-world videos and bandwidth traces manifest that compared with state-of-the-art approaches, EDTEA provides 10.4\\%\\sim 78.4\\% extra QoE while reducing re-buffering time by 85.5\\%\\sim 91.7\\%.}
}


@article{DBLP:journals/tmc/GallettaTCFV24,
	author = {Antonino Galletta and
                  Javid Taheri and
                  Antonio Celesti and
                  Maria Fazio and
                  Massimo Villari},
	title = {Investigating the Applicability of Nested Secret Share for Drone Fleet
                  Photo Storage},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2671--2683},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3263115},
	doi = {10.1109/TMC.2023.3263115},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GallettaTCFV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Military drones can be used for surveillance or spying on enemies. They, however, can be either destroyed or captured, therefore photos contained inside them can be lost or revealed to the attacker. A possible solution to solve such a problem is to adopt Secret Share (SS) techniques to split photos into several sections/chunks and distribute them among a fleet of drones. The advantages of using such a technique are two folds. First, no single drone contains any photo in its entirety; thus even when a drone is captured, the attacker cannot discover any photos. Second, the storage requirements of drones can be simplified, and thus cheaper drones can be produced for such missions. In this scenario, a fleet of drones consists of t+r drones, where t (threshold) is the minimum number of drones required to reconstruct the photos, and r (redundancy) is the maximum number of lost drones the system can tolerate. The optimal configuration of t+r is a formidable task. This configuration is typically rigid and hard to modify in order to fit the requirements of specific missions. In this work, we addressed such an issue and proposed the adoption of a flexible Nested Secret Share (NSS) technique. In our experiments, we compared two of the major SS algorithms (Shamir's schema and the Redundant Residue Number System (RRNS)) with their Two-Level NSS (2NSS) variants to store/retrieve photos. Results showed that Redundant Residue Number System (RRNS) is more suitable for a drone fleet scenario.}
}


@article{DBLP:journals/tmc/JiangCXHLD24,
	author = {Hongbo Jiang and
                  Siyu Chen and
                  Zhu Xiao and
                  Jingyang Hu and
                  Jiangchuan Liu and
                  Schahram Dustdar},
	title = {Pa-Count: Passenger Counting in Vehicles Using Wi-Fi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2684--2697},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3263229},
	doi = {10.1109/TMC.2023.3263229},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangCXHLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Passenger counting is crucial for many applications such as vehicle scheduling and traffic capacity assessment. However, most of the existing solutions are either high-cost, privacy invasive or not suitable for passengers the vehicle scenarios. In this work, we propose the Pa-Count, an effective real-time Passenger Counting system deployed inside the vehicle via using Wi-Fi CSI (Channel State Information). Specifically, in Pa-Count, we design a set of combined filters to eliminate environmental interference and enhance CSI quality. In so doing, we can identify the fluctuation of weak CSI caused by passengers’ subtle movement, i.e., the fidgeting, and then obtain the distribution of fidgeting period and silent period. Following that, we describe the subtle movements of passengers via power law with exponential cutoff distribution and establish a counting model based on the queuing theory. A mathematical inference method with a priori probability is devised to calculate the number of real-time passengers through CSI. We evaluate the performance of the Pa-Count by conducting a set of experiments in real-world vehicle scenarios (including private car and subway). Experimental results show that Pa-Count can achieve robust performance with an average accuracy of over 92\\%\n.}
}


@article{DBLP:journals/tmc/ChenHQMPS24,
	author = {Rui Chen and
                  Chenpei Huang and
                  Xiaoqi Qin and
                  Nan Ma and
                  Miao Pan and
                  Xuemin Shen},
	title = {Energy Efficient and Differentially Private Federated Learning via
                  a Piggyback Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2698--2711},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3268323},
	doi = {10.1109/TMC.2023.3268323},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenHQMPS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This artilce aims to develop a differential private federated learning (FL) scheme with the least artificial noises added while minimizing the energy consumption of participating mobile devices. By observing that some communication efficient FL approaches and even the nature of wireless communications contribute to the differential privacy (DP) preservation of training data on mobile devices, in this paper, we propose to jointly leverage gradient compression techniques (i.e., gradient quantization and sparsification) and additive white Gaussian noises (AWGN) in wireless channels to develop a piggyback DP approach for FL over mobile devices. Even with the piggyback DP approach, information distortion caused by gradient compression and noise perturbation may slow down FL convergence, which in turn consumes more energy of mobile devices for local computing and model update communications. Thus, we theoretically analyze FL convergence and formulate an energy efficient FL optimization under piggyback DP, transmission power, and FL convergence constraints. Furthermore, we propose an efficient iterative algorithm where closed-form solutions for artificial DP noise and power control are derived. Extensive simulation and experimental results demonstrate the effectiveness of the proposed scheme in terms of energy efficiency and privacy preservation.}
}


@article{DBLP:journals/tmc/YangXQWSLG24,
	author = {Xiang Yang and
                  Zikang Xu and
                  Qi Qi and
                  Jingyu Wang and
                  Haifeng Sun and
                  Jianxin Liao and
                  Song Guo},
	title = {{PICO:} Pipeline Inference Framework for Versatile CNNs on Diverse
                  Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2712--2730},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265111},
	doi = {10.1109/TMC.2023.3265111},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangXQWSLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributing the inference of convolutional neural network (CNN) to multiple mobile devices has been studied in recent years to achieve real-time inference without losing accuracy. However, how to map CNN to devices remains a challenge. On the one hand, scheduling the workload of state-of-the-art CNNs with multiple devices is NP-Hard because the structures of CNNs are directed acyclic graphs (DAG) rather than simple chains. On the other hand, distributing the inference workload suffers from expensive communication and unbalanced computation due to the wireless environment and heterogeneous devices. This paper presents PICO, a pipeline cooperation framework to accelerate the inference of versatile CNNs on diverse mobile devices. At its core, PICO features: (1) a generic graph partition algorithm that considers the characteristics of any given CNN and orchestrates it into a list of model pieces with suitable granularity, and (2) a many-to-many mapping algorithm that produces the best pipeline configuration for heterogeneous devices. In our experiment with 2 \\sim 8 Raspberry-Pi devices, the throughput can be improved by 1.8 \\sim 6.8 \\times under different CPU frequencies.}
}


@article{DBLP:journals/tmc/WangZHRLZ24,
	author = {Wei Wang and
                  Yongmin Zhang and
                  Rui Huang and
                  Ju Ren and
                  Feng Lyu and
                  Yaoxue Zhang},
	title = {Efficient Resource Management and Expansion Scheme for Collaborative
                  Edge-Cloud Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2731--2747},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267497},
	doi = {10.1109/TMC.2023.3267497},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZHRLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating the advantages of both the edge and the cloud, the edge-cloud computing system emerges to provide high-quality computing services for mobile users. To improve system efficiency, we investigate a hybrid mode of resource collaboration and expansion for the edge-cloud computing system, in which edge servers not only can collaborate with the cloud by purchasing high-priority computation resources temporarily but also can expand their local computation resources permanently. In such a way, the edge server can maximize its long-term profit by making a trade-off between the purchasing cost and the expanding cost. By formulating the resource management problem as a long-term profit maximization one, we first analyze the relationships among the expected minimal purchasing cost, the computation delay, and the available computation resources. Then, we design an efficient resource reserving and expanding scheme to determine the optimal expected amounts of reserving resources and expansion resources. Next, we propose an efficient real-time resource purchasing scheme to obtain the optimal amount of real-time purchasing resources dynamically. Finally, simulation results show that the proposed efficient resource collaboration and expanding scheme can maximize the long-term profit while guaranteeing the computation delay.}
}


@article{DBLP:journals/tmc/TanZLXGL24,
	author = {Zhaowei Tan and
                  Jinghao Zhao and
                  Yuanjie Li and
                  Yifei Xu and
                  Yunqi Guo and
                  Songwu Lu},
	title = {{LDRP:} Device-Centric Latency Diagnostic and Reduction for Cellular
                  Networks Without Root},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2748--2764},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267805},
	doi = {10.1109/TMC.2023.3267805},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TanZLXGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We design and implement LDRP, a device-based, standard-compliant solution to latency diagnosis and reduction in mobile networks without root privilege. LDRP takes a data-driven approach and works with a variety of latency-sensitive applications. After identifying elements in LTE uplink latency, we design LDRP that can infer the critical parameter used in data transmission and infer them for diagnosis. In addition, LDRP designates small dummy messages, which precede uplink data transmissions, thus eliminating latency elements due to power-saving, scheduling, etc. It imposes proper timing control among dummy messages and data packets to handle various conflicts. We achieve the latency diagnosis and reduction without requiring root privilege and ensure the latency is no worse than the legacy LTE design. The design of LDRP is also applicable for 5G. The evaluation shows that, LDRP infers the latency with at most 4% error and reduces the median LTE uplink latency by a factor up to 7.4× (from 42 to 5 ms) for four apps over 4 mobile carriers.}
}


@article{DBLP:journals/tmc/FanGZWL24,
	author = {Ye Fan and
                  Jidong Ge and
                  Sheng Zhang and
                  Jie Wu and
                  Bin Luo},
	title = {Decentralized Scheduling for Concurrent Tasks in Mobile Edge Computing
                  via Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2765--2779},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3266226},
	doi = {10.1109/TMC.2023.3266226},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanGZWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) is a promising solution to enhance the computing capability of resource-limited networks. A fundamental problem in MEC is efficiently offloading tasks from user devices to edge servers. However, there still exists a gap to deploy in real-world environments: 1) traditional centralized approaches needs complete information of edge network, ignoring the communication costs generated by synchronization, 2) previous works do not consider concurrent computation on edge servers, which may cause dynamic changes in the environment, and 3) the scheduling algorithm should deliver individualized decisions for different users independently and with high efficiency To solve this mismatch, we studied a multi-user task offloading problem where user devices make offloading decisions independently. We consider the concurrent execution of tasks and formulate a non-divisible and delay-aware task offloading problem to jointly minimize the dropped task ratio and long-term latency. We propose a decentralized task scheduling algorithm based on DRL that makes offloading decisions without knowing the information of other user devices. We employ Double-DQN, Dueling-DQN, Prioritized Replay Memory, and Recurrent Neural Network (RNN) techniques to improve the algorithm's performance. The results of simulation experiments show that our method can significantly reduce the long-term latency and dropped task ratio compared to the baseline algorithms.}
}


@article{DBLP:journals/tmc/XieDZ24,
	author = {Zhilan Xie and
                  Shuping Dang and
                  Zhenrong Zhang},
	title = {On State Transition Probability and Performance of Direct Acyclic
                  Graph Based Ledgers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2780--2794},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265259},
	doi = {10.1109/TMC.2023.3265259},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Direct acyclic graph (DAG) based ledgers with multi-chain structures aim to solve the technical bottlenecks associated with classical blockchain technologies in the Internet of Things (IoT). The basic working principle of DAG-based ledgers is to validate new transactions by previous transactions in order to be added to the system. During the tip selection process in the unsteady regime, the state transition probability refers to the probability of a transaction changing from the initial state to an arbitrary state. The state transition probability plays an indispensable role in the performance and security analysis of the IoT relying on DAG-based ledgers. In this paper, we derive the exact expression and an approximate expression of the state transition probability, which both are in closed form. In addition, we propose and analyze three performance metrics, i.e., the expected cumulative weight, the expected number of steps, and the confirmation failure probability, which are derived from the state transition probability and greatly enrich the performance analysis and evaluation of the IoT. Markov chain Monte Carlo (MCMC) simulations are carried out to verify the derived analytical results and provide insight into the IoT using DAG-based ledgers.}
}


@article{DBLP:journals/tmc/WangWCM24,
	author = {Yiwen Wang and
                  Albert Kai{-}Sun Wong and
                  S.{-}H. Gary Chan and
                  Wai Ho Mow},
	title = {Leto: Crowdsourced Radio Map Construction With Learned Topology and
                  a Few Landmarks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2795--2812},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3266198},
	doi = {10.1109/TMC.2023.3266198},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing crowdsourced indoor positioning systems (CIPSs) usually require prior knowledge about the site and a tedious calibration process. Moreover, they may require a large number of landmarks while ignoring the topology information that may be contained in the crowdsourced data. In this paper, we present Leto, a system that uses learned topology information from combined user traces to construct a radio map. Leto relies on crowdsourced WiFi and accelerometer signals only without requiring any prior knowledge about the site. Our key idea is that learned topology information can reduce the required number of landmarks, while available landmarks can transform the topology into a map. We propose a novel framework that efficiently learns the map topology by a hybrid multidimensional scaling (HMDS) algorithm and accurately rectifies the map using only a few anchors by an adaptive force-directed (AFD) algorithm. We also provide a theoretical convergence analysis of the HMDS algorithm. Experimental results on real-world datasets show that Leto can capture useful topology information and achieve significant improvements in radio map construction compared to existing systems.}
}


@article{DBLP:journals/tmc/XuWXLWH24,
	author = {Yang Xu and
                  Lun Wang and
                  Hongli Xu and
                  Jianchun Liu and
                  Zhiyuan Wang and
                  Liusheng Huang},
	title = {Enhancing Federated Learning With Server-Side Unlabeled Data by Adaptive
                  Client and Data Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2813--2831},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265010},
	doi = {10.1109/TMC.2023.3265010},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWXLWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has been widely applied to collaboratively train deep learning (DL) models on massive end devices (i.e., clients). Due to the limited storage capacity and high labeling cost, the data on each client may be insufficient for model training. Conversely, in cloud datacenters, there exist large-scale unlabeled data, which are easy to collect from public access (e.g., social media). Herein, we propose the Ada-FedSemi system, which leverages both on-device labeled data and in-cloud unlabeled data to boost the performance of DL models. In each round, local models are aggregated to produce pseudo-labels for the unlabeled data, which are utilized to enhance the global model. Considering that the number of participating clients and the quality of pseudo-labels will have a significant impact on the training performance, we introduce a multi-armed bandit (MAB) based online algorithm to adaptively determine the participating fraction and confidence threshold. Besides, to alleviate the impact of stragglers, we assign local models of different depths for heterogeneous clients. Extensive experiments on benchmark models and datasets show that given the same resource budget, the model trained by Ada-FedSemi achieves 3%\\sim14.8% higher test accuracy than that of the baseline methods. When achieving the same test accuracy, Ada-FedSemi saves up to 48% training cost, compared with the baselines. Under the scenario with heterogeneous clients, the proposed HeteroAda-FedSemi can further speed up the training process by 1.3\\times \\sim 1.5\\times.}
}


@article{DBLP:journals/tmc/YingZLXWCFLWWC24,
	author = {Jie Ying and
                  Tiantian Zhu and
                  Qiang Liu and
                  Chunlin Xiong and
                  Zhengqiu Weng and
                  Tieming Chen and
                  Lei Fu and
                  Mingqi Lv and
                  Han Wu and
                  Ting Wang and
                  Yan Chen},
	title = {TrapCog: An Anti-Noise, Transferable, and Privacy-Preserving Real-Time
                  Mobile User Authentication System With High Accuracy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2832--2848},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265071},
	doi = {10.1109/TMC.2023.3265071},
	timestamp = {Wed, 07 Aug 2024 21:27:49 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YingZLXWCFLWWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The authentication technology of mobile device users has been studied for decades. To balance security, privacy, and usability, motion sensors-based user authentication methods are widely investigated in recent years. However, existing studies meet the problems such as scarcity of training samples, underutilization of data, poor de-noising ability, insufficient transferability, privacy leakage, and low accuracy. To overcome these difficulties, we propose a system, called TrapCog, with the following capabilities: 1) In the phase of data collection, TrapCog can eliminate man-made noise (mislabeling) through differential training based on down-sampling. 2) In the model training stage, the siamese neural network with Long Short-Term Memory (LSTM) as the sub-network is used to achieve sufficient coverage of sample patterns and the transferability of the model. 3) In the phase of real-world authentication, the privacy of the user is tremendously protected through end-side model deployment and local authentication. Experimental results on a dataset composed of 1,513 users with real-world noise show that TrapCog has high accuracy and strong transferability, which is much better than state-of-the-art studies.}
}


@article{DBLP:journals/tmc/LiHLJW24,
	author = {Weihe Li and
                  Jiawei Huang and
                  Jingling Liu and
                  Wanchun Jiang and
                  Jianxin Wang},
	title = {Learning Audio and Video Bitrate Selection Strategies via Explicit
                  Requirements},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2849--2863},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265380},
	doi = {10.1109/TMC.2023.3265380},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiHLJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile video streaming dominates today's network traffic, and adaptive bitrate (ABR) algorithms have been routinely adopted for transmitting media content across dynamic mobile networks. State-of-the-art ABR algorithms mainly alter video bitrate without considering audio bitrate as they consider the impact on the video negligible due to their small size. However, to bring users an immersive experience, recent content providers have applied high-quality audio with large sizes, like stereophonic sound. Therefore, improper audio bitrate selection will adversely affect video bitrate selection, leading to undesirable audio/video combinations (the highest video quality with the lowest audio quality, and vice versa) and frequent playback interruptions. To address these inefficiencies, we propose a Self-Play reinforcement learning-based Audio-aware ABR algorithm named SPA to learn strategies for audio and video bitrate selections. By learning from explicit goals, SPA can match the actual requirements and attain good performance. By conducting trace-driven and testbed-based experiments, we observe SPA's considerable superiority compared to existing approaches, including reducing the undesirable combinations by up to 34.17× and achieving zero stall time across 88.57% of traces. We also invite 35 volunteers to join a subjective test, and the result shows that 33/35 people consider SPA provides them with a satisfactory viewing experience.}
}


@article{DBLP:journals/tmc/ZhouTQSDL24,
	author = {Jianshan Zhou and
                  Daxin Tian and
                  Guixian Qu and
                  Zhengguo Sheng and
                  Xuting Duan and
                  Victor C. M. Leung},
	title = {Energy-Efficiency Optimization With Model Convexification for Wireless
                  Ad Hoc Networks With Multi-Packet Reception Capability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2864--2881},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265646},
	doi = {10.1109/TMC.2023.3265646},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouTQSDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy efficiency is a significant requirement of resource management and design optimization in information networks. In this article, we propose an iterative fractional programming framework embedded with a distributed primal-dual extra-gradient projection algorithm, which addresses a wide class of the energy-efficiency optimization problems in wireless ad hoc networks with full-duplex radios and multi-packet reception capability. Specifically, we propose a model convexification mechanism by joining an affine transformation and an exponential transformation into the nonlinear fractional programming, which enables us to deal with the challenge arising from the complexity and non-convex structure of the original problem. With the model convexification, we can map the non-convex power control space into a convex space and equivalently derive a sequence of convex subproblems, which relaxes the convexity assumption widely adopted in the existing literature. We further propose a distributed primal-dual algorithm based on extra-gradient projection to solve the convex subproblem at each iteration of the fractional programming. The convergence of the proposed iterative fractional programming and the distributed optimization method is theoretically proven. Numerical results also verify the proposed method and demonstrate its superior performance over other representative distributed and centralized schemes in terms of achieving global energy efficiency.}
}


@article{DBLP:journals/tmc/LacavaPSSBSZCM24,
	author = {Andrea Lacava and
                  Michele Polese and
                  Rajarajan Sivaraj and
                  Rahul Soundrarajan and
                  Bhawani Shanker Bhati and
                  Tarunjeet Singh and
                  Tommaso Zugno and
                  Francesca Cuomo and
                  Tommaso Melodia},
	title = {Programmable and Customized Intelligence for Traffic Steering in 5G
                  Networks Using Open {RAN} Architectures},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2882--2897},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3266642},
	doi = {10.1109/TMC.2023.3266642},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LacavaPSSBSZCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G and beyond mobile networks will support heterogeneous use cases at an unprecedented scale, thus demanding automated control and optimization of network functionalities customized to the needs of individual users. Such fine-grained control of the Radio Access Network (RAN) is not possible with the current cellular architecture. To fill this gap, the Open RAN paradigm and its specification introduce an “open” architecture with abstractions that enable closed-loop control and provide data-driven, and intelligent optimization of the RAN at the user-level. This is obtained through custom RAN control applications (i.e., xApps) deployed on near-real-time RAN Intelligent Controller (near-RT RIC) at the edge of the network. Despite these premises, as of today the research community lacks a sandbox to build data-driven xApps, and create large-scale datasets for effective Artificial Intelligence (AI) training. In this paper, we address this by introducing ns-O-RAN, a software framework that integrates a real-world, production-grade near-RT RIC with a 3GPP-based simulated environment on ns-3, enabling at the same time the development of xApps, automated large-scale data collection and testing of Deep Reinforcement Learning (DRL)-driven control policies for the optimization at the user-level. In addition, we propose the first user-specific O-RAN Traffic Steering (TS) intelligent handover framework. It uses Random Ensemble Mixture (REM), a Conservative Q\n-learning (CQL) algorithm, combined with a state-of-the-art Convolutional Neural Network (CNN) architecture, to optimally assign a serving base station to each user in the network. Our TS xApp, trained with more than 40 million data points collected by ns-O-RAN, runs on the near-RT RIC and controls the ns-O-RAN base stations. We evaluate the performance on a large-scale deployment with up to 126 users with 8 base stations, showing that the xApp-based handover improves throughput and spectral efficiency by an average of 50% over traditional handover heuristics, with less mobility overhead.}
}


@article{DBLP:journals/tmc/MohantiRECC24,
	author = {Subhramoy Mohanti and
                  Debashri Roy and
                  Mark Eisen and
                  Dave Cavalcanti and
                  Kaushik R. Chowdhury},
	title = {{L-NORM:} Learning and Network Orchestration at the Edge for Robot
                  Connectivity and Mobility in Factory Floor Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2898--2914},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3266643},
	doi = {10.1109/TMC.2023.3266643},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MohantiRECC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Robotic factory floors will revolutionize the future of manufacturing and the service industry by automating tasks. However, to fully supplement human effort, these robots will need low-latency, reliable connectivity throughout the work zone through links established by wireless access points (APs). This will allow the robot to assuredly respond to programming directives that rely on the real-time relaying of robot-generated sensor data to the Mobile Edge Computing (MEC) server. In this paper, we propose L-NORM, a multi-AP and multi-robot coordination framework, as a multi-tiered solution for such autonomous edge networks. First, multi-robot motion planning through reinforcement learning occurs at the MEC, using as input multi-modal robot sensor data. Second, multi-AP resource orchestration is performed using another reinforcement learning-based method that maps a subset of available APs to each robot toward meeting their sensor data delivery requirements. Furthermore, we suggest diversity combination of uplink channels with the 802.11ax scheduled access mode that will (i) support high reliability of multi-robot uplink sensor packets and (ii) enable multi-AP coordination, for optimized resource utilization. Through extensive simulation studies, we show that the probability of robot deviation to remain within 0.5 m from its optimal path, is 19% more in L-NORM compared to classical 802.11ax based edge network solution, considering \\sim\n1 MB of sensor data per robot.}
}


@article{DBLP:journals/tmc/PratapDM24,
	author = {Shashwat Pratap and
                  Prajnamaya Dass and
                  Sudip Misra},
	title = {CoTEV: Trustworthy and Cooperative Task Execution in Internet of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2915--2926},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267602},
	doi = {10.1109/TMC.2023.3267602},
	timestamp = {Mon, 01 Apr 2024 11:15:01 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PratapDM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the increasing number of service requests from the vehicles, the load at the road side units (RSUs) increases, which affects the delay-sensitive vehicle services. In Internet of Vehicles (IoV), the vehicles can communicate directly with other vehicles and take help from the vehicles to cooperatively accomplish a task. However, it is very challenging to cooperatively execute a task in an IoV environment with high traffic and dynamic vehicle movements. Furthermore, it is difficult for a task vehicle to choose trustworthy and cooperative vehicles. In this article, we propose algorithms for cooperative task execution by taking the help of trusted vehicles, when it is not possible to complete a deadline-specified task through the RSUs. We propose a hedonic coalition formation game-based approach to form distributed coalitions of cooperative vehicles. We consider the trust score of the vehicles along with their computational capabilities and journey routes. After each task execution, the service feedback is reflected in the trust score of each cooperative vehicle in the coalition. Our proposed algorithms allow the cooperative vehicles to autonomously choose the coalitions and select vehicle tasks to maximize their payoffs. To satisfy the task deadlines in multiple coalitions, we design the merging of vehicle coalitions. We consider the simulation of urban mobility (SUMO) tool to generate the mobility traces of the vehicles in a real road network of Berlin city, which considers the traffic junctions and vehicle density on the roads. Through extensive simulations, we show that the proposed algorithms significantly increase the service rate of delay-sensitive task requests by at least 30.5 \\% and the trust score by at least 20.61 \\%, compared to the benchmark schemes.}
}


@article{DBLP:journals/tmc/QiuZWL24,
	author = {Xiaohan Qiu and
                  Shan Zhang and
                  Zhiyuan Wang and
                  Hongbin Luo},
	title = {Integrated Host- and Content-Centric Routing for Efficient and Scalable
                  Networking of {UAV} Swarm},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2927--2942},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267451},
	doi = {10.1109/TMC.2023.3267451},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiuZWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient and scalable networking is a key enabler of the Unmanned Aerial Vehicle (UAV) swarms, wherein multiple UAVs cooperatively execute complicated tasks. Despite the intermittent connections due to the UAV mobility, stable paths may exist temporally in periods like formation keeping, which is rarely considered or utilized in existing UAV routing designs. In this article, we propose an integrated host- and content-centric routing (IHCR) mechanism to harness the advantages of both routing mechanisms. Specifically, the routing information of stable paths is reused in a host-centric manner to reduce the flooding for path exploring. In addition, the route failure detection and re-routing are content-centric to adjust to the topology dynamics. The challenges lie in the inherent contradiction between host-centric and content-centric routing mechanisms (e.g., naming spaces) and the tradeoff between path reusing and re-routing. To overcome these challenges, we appropriately incorporate node names into content names and then fully exploit reusable paths via time-based route failure detection and delayed forwarding. Packet-level simulation results show that IHCR increases the packet delivery ratio by 60.1%, and enlarges the achievable network scale by 4.2 times compared to state-of-the-art routing mechanisms.}
}


@article{DBLP:journals/tmc/ZhangSL24,
	author = {Yanbo Zhang and
                  Weiping Sun and
                  Mo Li},
	title = {WiRITE: General and Practical Wi-Fi Based Hand-Writing Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2943--2957},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3265988},
	doi = {10.1109/TMC.2023.3265988},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-free hand-writing systems identify the content that a user writes by hand movement in the air, thus providing an intuitive human computer interface. In this paper, we propose WiRITE, a Wi-Fi hand-writing recognition system built with commodity Wi-Fi APs. Unlike most existing machine learning based hand-writing recognition systems, which are often subject to severe limitations in generality, e.g., high training overhead when adapted across hand-writing alphabets, environments, and users, WiRITE is designed with unique consideration of its generality when applied to practice—being application-transferable, environment-agnostic, and user-independent. With little training overhead, WiRITE behaves inclusively to different users, environments, and applications, stemming from a comprehensive design of signal processing that is built into its core machine learning model. Extensive evaluation is conducted with five users for three applications, i.e., recognizing Digits, English letters, and Chinese characters, in realistic office environment. The experiment results demonstrate that WiRITE provides at least 0.9 accuracy in various combinations of users and applications with 0.93 accuracy in average.}
}


@article{DBLP:journals/tmc/ChuJYLL24,
	author = {Weibo Chu and
                  Xinming Jia and
                  Zhiwen Yu and
                  John C. S. Lui and
                  Yi Lin},
	title = {Joint Service Caching, Resource Allocation and Task Offloading for
                  MEC-Based Networks: {A} Multi-Layer Optimization Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2958--2975},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3268048},
	doi = {10.1109/TMC.2023.3268048},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChuJYLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To provide reliable and elastic Multi-access edge computing services, one feasible solution is to federate geographically proximate edge servers to form a logically centralized resource pool. Optimization of such systems, however, becomes challenging. In this paper, we study the problem of maximizing users’ QoE in a MEC-based network, through jointly optimizing service caching, resource allocation and task offloading decisions. We formulate a mixed-integer nonlinear programming (MINLP) problem for the task and establish its NP-hardness. To tackle it efficiently, we propose a novel two-stage algorithmic solution based on approximation and decomposition theory. The proposed algorithm achieves high system performance while at the same time, ensures all constraints from different layers are satisfied. Meanwhile, the structure of the algorithm also fits the multi-layer optimizing feature, making it suitable to be implemented at different layers. In addition, we propose a distributed and online version of our mechanism with very limited information exchange between MEC servers, and further demonstrate how the cost of service switches from real MEC systems can be incorporated into our framework. We evaluate our mechanisms through simulations with both synthetic and real-world traces, and results indicate they are effective as compared to representative baseline algorithms.}
}


@article{DBLP:journals/tmc/ZhanHWLM24,
	author = {Cheng Zhan and
                  Han Hu and
                  Jing Wang and
                  Zhi Liu and
                  Shiwen Mao},
	title = {Tradeoff Between Age of Information and Operation Time for {UAV} Sensing
                  Over Multi-Cell Cellular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2976--2991},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267656},
	doi = {10.1109/TMC.2023.3267656},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanHWLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) have a significant potential for sensing applications in further cellular networks due to their extensive coverage and flexible deployment. In this paper, we consider a multi-cell cellular network with a cellular-connected UAV, which senses data with onboard sensors and uploads sensory data to the ground base stations (BSs). To evaluate the freshness of sensory data, we employ the concept of age of information (AoI), which is defined as the time elapsed since the latest successful transmission of sensory data. A lower AoI implies fresher sensory data, which may lead to the increase of UAV operation time. To balance such tradeoff, we aim to minimize the weighted sum of operation time and total AoI for the UAV by jointly optimizing transmission scheduling, BS association, as well as UAV trajectory. The problem is formulated as a mixed-integer nonlinear programming (MINLP) problem, which is difficult to solve due to the time-varying propagation channels. To this end, we first characterize the average communication performance with statistic channel information, and then develop a search algorithm to obtain the optimal solution via employing the optimal structure as well as convex optimization techniques, while a low-complexity Double Graph based Algorithm (DGA) is developed to obtain a suboptimal solution. Then, by taking into account the site-specific performance and making fast decisions online, we propose a Deep reinforcement Learning Algorithm (DLA). Compared to DGA, DLA can adapt to the specific local environment and obtain a solution more rapidly once the training process is completed. Simulation results show that the proposed algorithms outperform the benchmarks about 30%, and achieve flexible tradeoff between operation time and AoI of UAV sensing, which is not available by considering just one objective.}
}


@article{DBLP:journals/tmc/WangLG24,
	author = {Jiaqin Wang and
                  Kai Liu and
                  Yi Gong},
	title = {Vehicle Position Prediction Using Particle Filtering Based on 3D {CNN-LSTM}
                  Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {2992--3004},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267853},
	doi = {10.1109/TMC.2023.3267853},
	timestamp = {Fri, 22 Mar 2024 08:59:08 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle position prediction (VPP) is of great significance for navigation planning and traffic safety of intelligent vehicles. In general, particle filtering (PF) uses global navigation satellite system (GNSS) to implement VPP. However, it does not consider geographic layer information (GLI) and its particle weight is not combined with the real-world geographic position information, which leads to insufficient prediction preparation. To resolve this problem, we propose a novel PF-based VPP method by using three-dimensional convolutional neural network and long short-term memory (3D CNN-LSTM) network model. First, for data preprocessing, we extract kinematic information features from GNSS, and evenly divide the area around each GNSS point into multiple grids and calculate the probability of grids center belonging to each GLI type. In addition, in order to better reflect the relationship between two consecutive positions due to the factors such as the conversion angle, we construct tilted cells to represent possible positions of each vehicle at any time. Second, a novel 3D CNN-LSTM model is designed to calculate the vehicle occurrence probability (VOP) in each tilted cell by processing the GLI and GNSS data, which can optimize the PF weight of each particle, and then improve PF to make more precise position prediction. Finally, the experimental results demonstrate that the proposed VPP method can improve the cell prediction accuracy, and then significantly improve the position prediction precision.}
}


@article{DBLP:journals/tmc/JeonS24,
	author = {Kang Eun Jeon and
                  James She},
	title = {Sensing-Aware Machine Learning Framework for Extended Lifetime of
                  IoT Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3005--3017},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267846},
	doi = {10.1109/TMC.2023.3267846},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JeonS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) beacon network is one of the essential infrastructures for many IoT and smart city applications that involve a plethora of sensing tasks. However, the BLE beacon network usually suffers from poor reliability and high maintenance costs due to the short-lived battery lifetime. Multiple works have attempted to extend the lifetime via energy harvesting hardware, adaptive advertising interval by user existence-aware operation, and energy-efficient routing schemes. However, few attempts were made to reduce the energy consumption related to sensing tasks. In light of this shortcoming, a sensor information-aware framework is proposed to adjust the sensing task interval adaptively based on the predicted portion of changes of the sensor measurements. Furthermore, to estimate the impact of varying sensing task intervals on the amount of sensed information, a model that correlates energy and amount of information is proposed. The sensor portion of changes is predicted with a novel neural network, coined oracle-interpreter network, that significantly reduces the energy consumption while upkeeping a good prediction accuracy by leveraging two independent neural networks tailored for feature extraction and prediction tasks. The effectiveness of the proposed framework is verified by comprehensive simulations based on real-life data. The results demonstrate that the proposed framework can effectively reduce the energy consumption involved in sensing tasks up to 30%, machine learning tasks up to 60%, and finally, extend the lifetime up to 75%.}
}


@article{DBLP:journals/tmc/BhawanaKRDKLK24,
	author = {Bhawana and
                  Sushil Kumar and
                  Rajkumar Singh Rathore and
                  Upasana Dohare and
                  Omprakash Kaiwartya and
                  Jaime Lloret and
                  Neeraj Kumar},
	title = {{BEET:} Blockchain Enabled Energy Trading for E-Mobility Oriented
                  Electric Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3018--3034},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3267565},
	doi = {10.1109/TMC.2023.3267565},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BhawanaKRDKLK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Renewable Energy Sources (RESs) are gaining considerable attention to reduce human dependence on fossil fuels and minimize harmful gases in our surroundings. Existing literature on energy trading focused on providing renewable energy to smart homes, smart buildings, and smart offices to fulfill their daily energy demands obtained from RESs. Besides, Electric Vehicles (EVs) use either power grid energy or a battery exchange mechanism to recharge their low EV batteries. The continuous use of power grids to recharge low EV batteries causes a significant load on power grids. Due to this, power grids are inadequate to fulfill the ever-increasing demands of EVs in the future. In this context, we propose a Blockchain Enabled Energy Trading (BEET) framework oriented EV charging. A system architecture of the BEET framework is presented to describe the functioning of each layer and its associated entities. We formulate an optimization problem that maximizes the revenue in the energy trading process using a knapsack optimization. Smart contracts are designed on the consortium blockchain network to sell and buy renewable energy to aggregators and from producers, respectively. Moreover, an EV charging mechanism is designed to intelligently allocate renewable energy to consumers at a low price. A comparative analysis is performed with state-of-the-art works in terms of charging price, revenue, throughput, and latency. The results indicate that the BEET framework outperforms compared to state-of-the-art works to address the renewable energy demand problem to realize E-mobility. It is clarified that the data considered in the experimental analysis were obtained from statistical simulations in realistic E-Mobility environment settings.}
}


@article{DBLP:journals/tmc/KazariAM24,
	author = {Kiarash Kazari and
                  Farid Ashtiani and
                  Mahtab Mirmohseni},
	title = {Cache Update and Delivery of Dynamic Contents: {A} Stochastic Game
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3035--3047},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273500},
	doi = {10.1109/TMC.2023.3273500},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KazariAM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a cache update policy for wireless networks considering dynamic popularity for file requests. Our network scenario is a wireless network comprised of some cache-equipped access points (APs) which are deployed densely in an area and have connections with core network servers. We model the dynamics of file requests as Markov-modulated Poisson processes. Considering the congestion-dependent delay of APs’ services and their overlapping coverage areas, we model the cache service for the cache-missed requests, as a stochastic game in which a Vickrey–Clarke–Groves (VCG) mechanism is exploited at each stage of the game as our proposed file routing policy, and the APs decide independently how to update their caches regarding the routed files. To this end, the APs’ utilities are defined based on long-term average file delivery delay including the queueing delay in APs. By finding the Nash equilibrium of the game, we propose policies for both delivery and cache update of dynamic contents. Finally, by comparing numerical results of our proposed scheme with some conventional caching schemes, we show significant improvements in network performance in terms of file delivery delay in different conditions.}
}


@article{DBLP:journals/tmc/LiLWL24,
	author = {Baogang Li and
                  Jia Liao and
                  Wenjing Wu and
                  Yonghui Li},
	title = {Intelligent Reflecting Surface Assisted Secure Computation of Wireless
                  Powered {MEC} System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3048--3059},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3269791},
	doi = {10.1109/TMC.2023.3269791},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of mobile edge computing (MEC) and wireless power transfer (WPT) can effectively improve the computing ability and energy sustainability of energy-constrained wireless devices in the Internet of Things (IoT) networks. Intelligent reflecting surface (IRS) has recently emerged as an effective technique to improve the performance of wireless systems by intelligently reconfiguring wireless environments. This paper studies the exploitation of IRS to improve the secure computation performance of WPT-MEC systems with a passive eavesdropper. A wireless access point (AP) first charge multiple users with the emitted energy signals, and then the users perform local computing and partial offloading to complete their computation tasks with the harvested energy in the presence of an eavesdropper, where the local computing can be executed during the whole process of WPT and offloading. Meanwhile, deploying IRS can improve the energy capture and secure offloading performance of the users. We maximize the secure computation task bits of users by jointly optimizing the AP energy transmit beamforming, the IRS phase shifts, the transmit power, users’ offloading time, and the local computation frequency of users, which are tangled with each other. An iterative optimal algorithm is developed to solve this non-convex problem by combining Taylor expansion method, semidefinite relaxation (SDR) algorithm, the Lagrange duality theory and Karush-Kuhn-Tucker (KKT) conditions. The numerical results show that the proposed scheme can effectively increase the secure computation task bits compared with other benchmark schemes, especially for the maximum transmit power of AP, the improvement is above 45\\%.}
}


@article{DBLP:journals/tmc/LiCSZWYG24,
	author = {Ye Li and
                  Liang Chen and
                  Li Su and
                  Kanglian Zhao and
                  Jue Wang and
                  Yongjie Yang and
                  Ning Ge},
	title = {PEPesc: {A} {TCP} Performance Enhancing Proxy for Non-Terrestrial
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3060--3076},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3269436},
	doi = {10.1109/TMC.2023.3269436},
	timestamp = {Wed, 07 Aug 2024 07:59:35 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiCSZWYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-terrestrial networks (NTNs) using flying objects such as satellites play key roles in the next-generation wireless system (6G). The NTN links with long propagation delay and random packet losses pose a great challenge to the performance of Transmission Control Protocol (TCP), which many Internet applications rely on. Performance enhancing proxy (PEP) is an easy-to-deploy approach for improving TCP's performance. In this paper, we design and implement a novel PEP called PEPesc which has two distinctive features. First, it features retransmission-free loss recovery, using an adaptive packet-level forward erasure correction method called streaming coding (SC). Second, as packet losses are recovered by SC, the congestion control problem is simplified to rate control and local acknowledgement between entities based on bandwidth estimation. Based on a queueing theoretic analysis of the design, we carefully devise a protocol and implement PEPesc as an open-source application. Extensive evaluations show that PEPesc can achieve much higher and smoother goodput than the canonical TCP variants and than other existing open-source PEPs in applications including iperf and HTTP-based adaptive streaming, and achieves similar performance in web browsing. Finally, we also present a deployment case over a real-world geostationary satellite link.}
}


@article{DBLP:journals/tmc/ZhangLZHTXLX24,
	author = {Xinyuan Zhang and
                  Jiang Liu and
                  Ran Zhang and
                  Yudong Huang and
                  Jincheng Tong and
                  Ning Xin and
                  Liang Liu and
                  Zehui Xiong},
	title = {Energy-Efficient Computation Peer Offloading in Satellite Edge Computing
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3077--3091},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3269801},
	doi = {10.1109/TMC.2023.3269801},
	timestamp = {Wed, 20 Mar 2024 14:27:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLZHTXLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, MEC has been integrated with satellite networks to process remote terrestrial computation tasks with superior coverage and delay. Since single satellite computation is hard to tackle spatially uneven computation workloads, computation peer offloading among multiple satellites is urgently needed to further improve service quality and resource utilization. However, considering limited resources, deficient energy, and costly overheads of communication and computation, how to enable efficient offloading cooperation in the time-varying satellite networks is a significant challenge. In this paper, we first design a satellite peer offloading scheme, where offloading is performed along multi-hop paths to explore collaborative computing capabilities. Second, we formulate the Multi-Hop Satellite Peer offloading (MHSPO) problem, aiming to jointly minimize the delay and energy consumption under system resources and backlog constraints. Then, to adapt to the network dynamics, the decision-making process with uncertain future workloads is optimized by leveraging the delayed online learning method under the Lyapunov framework. Finally, we develop a practical online distributed algorithm to solve the MHSPO problem, which is proven to achieve close-to-optimal performance. Extensive simulations show that multi-hop peer offloading among satellites improves edge computing performance efficiently.}
}


@article{DBLP:journals/tmc/HuangZWMCFD24,
	author = {Wenjie Huang and
                  Zhiwei Zhao and
                  Zi Wang and
                  Geyong Min and
                  Zheng Chang and
                  Luwei Fu and
                  Hancong Duan},
	title = {Adaptive Mobile Recharge Scheduling With Rapid Data Sharing in Wireless
                  Rechargeable Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3092--3105},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273213},
	doi = {10.1109/TMC.2023.3273213},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZWMCFD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent breakthrough in Wireless Power Transfer (WPT) provides a promising way to prolong network lifetime by employing a charging vehicle to replenish energy. Data transmissions from nodes typically happen in response to physical sensory events, leading to time-varying energy consumption. To improve charging efficiency, the existing schemes collect energy information by employing a data-gathering vehicle or data collection protocol. However, in duty cycle networks, these schemes either incur extra vehicles or high data collection delay. To solve this problem, we propose an mobile adaptive charging scheme with rapid data sharing (rShare), which establishes multi-layer collection trees and collects overall energy data to the vehicle. A spatial predicted active sending (SPAS) algorithm is proposed for distant nodes to actively estimate the future position and transmit their data to cover potential positions of the charging vehicle, which significantly reduces data collection delay. We also propose an estimated time of arrival (ETA)-aware scheme based on the TSP Nearest Neighbor algorithm that updates the charging path based on the collected data. Extensive simulation results demonstrate that our scheme outperforms the state-of-the-arts in terms of dead node avoidance with less communication overhead.}
}


@article{DBLP:journals/tmc/XiangCLZLZL24,
	author = {Chaocan Xiang and
                  Wenhui Cheng and
                  Chi Lin and
                  Xinglin Zhang and
                  Daibo Liu and
                  Xiao Zheng and
                  Zhenhua Li},
	title = {LSTAloc: {A} Driver-Oriented Incentive Mechanism for Mobility-on-Demand
                  Vehicular Crowdsensing Market},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3106--3122},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3271671},
	doi = {10.1109/TMC.2023.3271671},
	timestamp = {Tue, 02 Apr 2024 21:06:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiangCLZLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of Mobility-on-Demand (MOD) vehicles, a new market called MOD-Vehicular-Crowdsensing (MOVE-CS) was introduced for drivers to earn more by collecting road data. Unfortunately, MOVE-CS failed after two years of operation. To identify the root cause, we survey 581 drivers and reveal its simple incentive model based on blindly competitive rewards. This model brings most drivers few yields, resulting in their withdrawals. In contrast, a similar market termed MOD-Human-Crowdsensing (MOMAN-CS) remains successful thanks to a complex model based on exclusively customized rewards. Hence, we wonder whether MOVE-CS can be resurrected by learning from MOMAN-CS. Despite considerable similarity, we can hardly apply the incentive model of MOMAN-CS to MOVE-CS, since MOD drivers are also concerned with passenger missions that dominate their earnings. To this end, we analyze a large-scale dataset of 12,493 MOD vehicles, finding that drivers have explicit preference for short-term, immediate gains as well as implicit rationality in pursuit of long-term, stable profits. Therefore, we design a novel driver-oriented incentive mechanism for MOVE-CS, called LSTAloc, at the heart of which lies a spatial-temporal differentiation-aware task allocation scheme empowered by submodular optimization. Applied to the dataset, our design would essentially benefit both the drivers and platform to incentivize MOD vehicular crowdsensing efficiently, thus possessing the potential to resurrect MOVE-CS.}
}


@article{DBLP:journals/tmc/GautamVK24,
	author = {Sukriti Gautam and
                  Ridhima Verma and
                  Suman Kumar},
	title = {Dynamic Data Advertising and Packet Loss Analysis Using {BLE} Legacy
                  Advertising},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3123--3137},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3269104},
	doi = {10.1109/TMC.2023.3269104},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GautamVK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low energy (BLE) advertisements have great potential in Internet of Things applications like monitoring systems, which involve the real-time transfer of data collected from sensors interfaced to multiple power-constrained devices. Since sensor data conveys information about the present state of the system, packet loss is vital in such applications. The article proposes a customized BLE legacy advertising packet structure to advertise dynamic sensor data, and proposes two techniques: One-Set-Multiple-Events (OSME) and One-Set-X-Events (OSXE), to advertise the dynamic sensor data using the non-connectable and non-scannable undirected advertising mode of BLE. Using appropriate advertising parameters with OSXE, each advertising packet gets successfully advertised X times, whereas OSME neither guarantees successful advertisement of all the packets, nor does it allow control over the number of transmitted copies of each packet. Using OSXE, experimental analysis is carried out to study packet loss when single BLE node transmits alone, and when 25 nodes transmit simultaneously. Mathematical analysis has been carried out for packet collision for multiple nodes. Experimental results show that packet loss is also impacted by factors like advertising set duration, scan window, scanner's link layer activities like advertising report generation and channel switching. Using OSXE, packet loss close to 0% is obtained for X=3 when 25 nodes advertise simultaneously.}
}


@article{DBLP:journals/tmc/ChenZJQXLLL24,
	author = {Yu Chen and
                  Sheng Zhang and
                  Yibo Jin and
                  Zhuzhong Qian and
                  Mingjun Xiao and
                  Wenzhong Li and
                  Yu Liang and
                  Sanglu Lu},
	title = {Crowdsourcing Upon Learning: Energy-Aware Dispatch With Guarantee
                  for Video Analytics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3138--3155},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3269807},
	doi = {10.1109/TMC.2023.3269807},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenZJQXLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the last decade, the mobile crowdsourcing has become a paradigm to conduct the manual annotation and further analytics by recruited workers, with their rewards depending on the result quality. Existing dispatchers cannot precisely capture the resource-quality trade-off for video analytics, because the configurations supported by recruited workers are limited, and workers’ availability changes over time. To determine the most suitable configurations as well as workers for video analytics, we formulate a non-linear mixed program in long term, maximizing the crowdsourcing profit. Based on previous results under various configurations and workers, we design an algorithm via a series of subproblems to decide the configurations adaptively upon the prediction of workers’ feedbacks. Such prediction is based on volatile multi-armed bandit to capture workers’ availability and stochastic changes on resource uses. Furthermore, we extend the proposed algorithms to the multi-worker selection scenario where the platform needs to determine a candidate worker set instead of a single worker for video analytics. Via rigorous proof, the regret is ensured upon the Lyapunov optimization and the bandit, measuring the gap between the online decisions and the offline optimum. Extensive trace-driven experiments show that our proposed algorithm improves the profit by 37% compared with other algorithms.}
}


@article{DBLP:journals/tmc/HsiehLGG24,
	author = {Li{-}Tse Hsieh and
                  Hang Liu and
                  Yang Guo and
                  Robert Gazda},
	title = {Deep Reinforcement Learning-Based Task Assignment for Cooperative
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3156--3171},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3270242},
	doi = {10.1109/TMC.2023.3270242},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HsiehLGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) integrates computing resources in wireless access networks to process computational tasks in close proximity to mobile users with low latency. This paper investigates the task assignment problem for cooperative MEC networks in which a set of geographically distributed heterogeneous edge servers not only cooperate with remote cloud data centers but also help each other to jointly process user tasks. We introduce a novel stochastic MEC cooperation framework to model the edge-to-edge horizontal cooperation and the edge-to-cloud vertical cooperation. The task assignment optimization problem is formulated by taking into consideration dynamic network states, uncertain node computing capabilities and task arrivals, as well as the heterogeneity of the involved entities. We then develop and compare three task assignment algorithms, based on different deep reinforcement learning (DRL) approaches, value-based, policy-based, and hybrid approaches. In addition, to reduce the search space and computation complexity of the algorithms, we propose decomposition and function approximation techniques by leveraging the structure of the underlying problem. The evaluation results show that the proposed DRL-based task assignment schemes outperform the existing algorithms, and the hybrid actor-critic scheme performs the best under dynamic MEC network environments.}
}


@article{DBLP:journals/tmc/LimLLSKNKICGLH24,
	author = {Hyoyoung Lim and
                  Jinsung Lee and
                  Jongyun Lee and
                  Sandesh Dhawaskar Sathyanarayana and
                  Junseon Kim and
                  Anh Nguyen and
                  Kwang Taik Kim and
                  Youngbin Im and
                  Mung Chiang and
                  Dirk Grunwald and
                  Kyunghan Lee and
                  Sangtae Ha},
	title = {An Empirical Study of 5G: Effect of Edge on Transport Protocol and
                  Application Performance},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3172--3186},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3274708},
	doi = {10.1109/TMC.2023.3274708},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LimLLSKNKICGLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we conduct a measurement study on operational 5G networks deployed across different frequency bands (mmWave and sub-6GHz) and server locations (mobile edge and Internet cloud). Specifically, we assess 5G performance in both uplink and downlink across multiple operators’ networks. We then carry out extensive comparisons of transport-layer protocols using ten different algorithms in full-fledged 5G networks, including an edge computing environment. Finally, we evaluate representative mobile applications over the 5G network with and without edge servers. Our comprehensive measurements provide several insights that affect the experience of 5G users: (i) With a 5G edge server, existing TCP congestion control algorithms can achieve throughput up to 1.8Gbps with only a single flow. (ii) The maximum TCP receive buffer size, which is set by off-the-shelf 5G phones, can limit the throughput performance of 5G networks, which is not observed in 4G LTE-A networks. (iii) Despite significant latency gains in download-centric applications, the 5G edge service provides limited benefits to CPU-intensive tasks or those that use significant uplink bandwidth. To our knowledge, this is the first measurement-driven understanding of 5G edge computing “in the wild,” which can provide an answer to how edge computing would perform in real 5G networks.}
}


@article{DBLP:journals/tmc/XuWLHCG24,
	author = {Wenchao Xu and
                  Haozhao Wang and
                  Zhaoyi Lu and
                  Cunqing Hua and
                  Nan Cheng and
                  Song Guo},
	title = {Mobile Collaborative Learning Over Opportunistic Internet of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3187--3199},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273425},
	doi = {10.1109/TMC.2023.3273425},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWLHCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning models are widely applied for vehicular applications, which are essential to future intelligent transportation system (ITS). Traditional model training methods commonly employ a client-server architecture to perform local training and global iterative aggregations, which can consume significant bandwidth resources that are often absent in vehicular networks, especially in high vehicle density scenarios. Modern vehicle users naturally can collaboratively train machine learning models as they are the data owner and have strong local computing power from the onboard units (OBU). In this paper, we propose a novel collaborative learning scheme for mobile vehicles that can utilize the opportunistic vehicle-to-roadside (V2R) communication to exploit the common priors of vehicular data without interaction with a centralized coordinator. Specifically, vehicles perform local training during the driving journey, and simply upload its local model to roadside unit (RSU) encountered on the way. RSU's model will be updated accordingly and sent back to the vehicle via the V2R communication. We have theoretically shown that RSUs’ models can eventually converge without a backhaul connection. Extensive experiments upon various road configurations demonstrate that the proposed scheme can efficiently train models among vehicles without dedicated Internet access and scale well with both the road range and vehicle density.}
}


@article{DBLP:journals/tmc/ChenYHSCG24,
	author = {Yuhao Chen and
                  Qianqian Yang and
                  Shibo He and
                  Zhiguo Shi and
                  Jiming Chen and
                  Mohsen Guizani},
	title = {FTPipeHD: {A} Fault-Tolerant Pipeline-Parallel Distributed Training
                  Approach for Heterogeneous Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3200--3212},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3272567},
	doi = {10.1109/TMC.2023.3272567},
	timestamp = {Tue, 12 Nov 2024 11:36:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYHSCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing proliferation of Internet-of-Things (IoT) devices, there is a growing trend towards distributing the power of deep learning (DL) among edge devices rather than centralizing it at the cloud. To deploy deep and complex models at edge devices with limited resources, model partitioning of deep neural network (DNN) models has been widely studied. However, most of the existing literature only considers distributing the inference model while still training the model at the cloud. In this paper, we propose FTPipeHD, a novel DNN training approach that trains DNN models across distributed heterogeneous devices with the fault-tolerance mechanism. To accelerate the training with the time-varying computing power of each device, we optimize the partition points dynamically according to real-time computing capacities. We also propose a novel weight redistribution approach that replicates the weights to both the neighboring nodes and the central node periodically, which combats the failure of multiple devices during training while incurring limited communication costs. Our numerical results demonstrate that FTPipeHD is 6.8 times faster in training than the state-of-the-art method when the computing capacity of the best device is 10 times greater than the worst one. It is also shown that the proposed method is able to accelerate the training even with the existence of device failures.}
}


@article{DBLP:journals/tmc/ZhaoLXWW24,
	author = {Zhiyuan Zhao and
                  Fan Li and
                  Yadong Xie and
                  Yue Wu and
                  Yu Wang},
	title = {BSMonitor: Noise-Resistant Bowel Sound Monitoring via Earphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3213--3227},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3270926},
	doi = {10.1109/TMC.2023.3270926},
	timestamp = {Thu, 13 Feb 2025 08:09:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLXWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bowel sound (BS) is an important physiological signal of the human body, which is also an objective reflection of gastrointestinal motility. However, BS has characteristics of weak signal, strong noise, and randomicity, which bring great challenges to the daily detection of BS. In this paper, we propose BSMonitor, the first BS monitoring system with strong noise-resistant capability via earphones. BSMonitor uses one earphone attached to the abdomen to collect BS signals and the other earphone worn in the ear to collect external noises and internal noises. After eliminating the noises through the Kalman filter and band-pass filter, the signal containing BS is separated via the empirical mode decomposition. Then BSMonitor extracts MFCC features of BS signals and applies a carefully-designed LSTM network to perform highly-accurate BS detection. Finally, an alert mechanism calculates the frequency and duration of detected BS and compares with the normal values to alert users. Furthermore, to increase the amount and diversity of training data, we introduce a data augmentation method, which can further improve the accuracy and generalization of BSMonitor. Through extensive experiments with 18 volunteers, we find that BSMonitor not only achieves high accuracy of BS detection but also has strong generalization across different users and environments. Particularly, BSMonitor achieves accuracy up to 98.73% and 94.56% in the benchmark experiments and the cross experiments, respectively.}
}


@article{DBLP:journals/tmc/YangXSHMZ24,
	author = {Huaming Yang and
                  Zhongzhou Xia and
                  Jersy Shin and
                  Jingyu Hua and
                  Yunlong Mao and
                  Sheng Zhong},
	title = {A Comprehensive Study of Trajectory Forgery and Detection in Location-Based
                  Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3228--3242},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273411},
	doi = {10.1109/TMC.2023.3273411},
	timestamp = {Wed, 03 Apr 2024 17:41:22 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangXSHMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many mobile apps access users’ trajectories to provide critical services (e.g., trip tracking). Unfortunately, in such apps, malicious users may upload fake trajectories to cheat providers for illegal benefits. There are few works in the literature that delicately study trajectory forgery problems. In this paper, we first take the perspective of attackers and consider how they would fabricate vivid trajectories confronting a strict provider. In particular, we use the technique of adversarial examples in deep learning to propose a trajectory forgery method, which produces fake trajectories satisfying two conditions: (1) having the motion characteristics indistinguishable from those of real ones, and (2) matching reasonable walking, cycling, or driving routes when being projected to the map. Our experiments show that they can hardly be detected by mainstream trajectory service providers, even after being equipped with machine learning-based approaches. Therefore, we further present dedicated countermeasures by validating the reasonability of reported received signal strength indicator (RSSI) data of scanned WiFi APs in commercial areas and scanned Cellular APs in rural areas, respectively. They can deal well with the most challenging replay scenario, which can hardly be handled by existing radio-based location verification methods. We conduct extensive real-world experiments covering walking, cycling, and driving scenarios to demonstrate the high detection accuracy of both methods.}
}


@article{DBLP:journals/tmc/ZhaoYCXSZC24,
	author = {Haihong Zhao and
                  Bo Yang and
                  Jiaxu Cui and
                  Qianli Xing and
                  Jiaxing Shen and
                  Fujin Zhu and
                  Jiannong Cao},
	title = {Effective Fault Scenario Identification for Communication Networks
                  via Knowledge-Enhanced Graph Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3243--3258},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3271715},
	doi = {10.1109/TMC.2023.3271715},
	timestamp = {Fri, 07 Feb 2025 15:03:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoYCXSZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fault Scenario Identification (FSI) is a challenging task that aims to automatically identify the fault types in communication networks from massive alarms to guarantee effective fault recoveries. Existing methods are developed based on rules, which are not accurate enough due to the mismatching issue. In this paper, we propose an effective method named Knowledge-Enhanced Graph Neural Network (KE-GNN), the main idea of which is to integrate the advantages of both the rules and GNN. This work is the first work that employs GNN and rules to tackle the FSI task. Specifically, we encode knowledge using propositional logic and map them into a knowledge space. Then, we elaborately design a teacher-student scheme to minimize the distance between the knowledge embedding and the prediction of GNN, integrating knowledge and enhancing the GNN. To validate the performance of the proposed method, we collected and labeled three real-world 5G fault scenario datasets. Extensive evaluation conducted on these datasets indicates that our method achieves the best performance compared with other representative methods, improving the accuracy by up to 8.10%. Furthermore, the proposed method achieves the best performance against a small dataset setting and can be effectively applied to a new carrier site with a different topology structure.}
}


@article{DBLP:journals/tmc/LiangZHBWS24,
	author = {Junjie Liang and
                  Lei Zhang and
                  Chaolei Han and
                  Can Bu and
                  Hao Wu and
                  Aiguo Song},
	title = {A Collaborative Compression Scheme for Fast Activity Recognition on
                  Mobile Devices via Global Compression Ratio Decision},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3259--3273},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3271306},
	doi = {10.1109/TMC.2023.3271306},
	timestamp = {Mon, 24 Feb 2025 12:55:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiangZHBWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite strong representation ability, deep convolutional neural networks (CNNs) are largely hindered in practical human activity recognition (HAR) deployment due to high computational cost, which is often unaffordable on resource-limited wearable devices. In this article, to bridge the gap between on-device HAR and deep learning, we present a collaborative compression scheme to reduce the runtime of HAR with an acceptable performance degradation, which combines channel pruning and tensor decomposition to simultaneously handle sparsity and low-rankness when fully considering mutual interference in one network consisting of efficient 1-dimensional convolutional kernels. Our method includes two main stages. Concretely, given a target compression ratio, a global compression ratio decision optimization is first performed to automatically decide per-layer compression ratio by measuring compression sensitivity, without requiring labor-exhaustive human intervention. Then a multi-step collaborative compression is iteratively implemented to remove the least important compression unit based on an improved importance metric until the per-layer target compression ratio is attained. Extensive experiments on multiple HAR benchmarks show that our approach considerably outperforms previous compression strategies. For example, it can achieve around 50% FLOPs reduction with only an accuracy drop of 0.25% and 0.15% on UCI-HAR and PAMAP2, respectively. Actual implementation is evaluated on an embedded platform.}
}


@article{DBLP:journals/tmc/WangPCPLGH24,
	author = {Chenglong Wang and
                  Jun Peng and
                  Lin Cai and
                  Hui Peng and
                  Weirong Liu and
                  Xin Gu and
                  Zhiwu Huang},
	title = {AI-Enabled Spatial-Temporal Mobility Awareness Service Migration for
                  Connected Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3274--3290},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3271655},
	doi = {10.1109/TMC.2023.3271655},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangPCPLGH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the future 6G intelligent transportation system, the edge server will bring great convenience to the timely computing service for connected vehicles. To guarantee the quality of service, the time-critical services need to be migrated according to the future location of the vehicle. However, predicting vehicle mobility is challenging due to the time-varying of road traffic and the complex mobility patterns of vehicles. To address this issue, a spatial-temporal awareness proactive service migration strategy is proposed in this paper. First, a spatial-temporal neural network is designed to obtain accurate mobility by using gated recurrent units and graph convolutional layers extracting features from spatial road traffic and multi-time scales driving data. Then a proactive migration method is proposed to guarantee the reliability of services and reduce energy consumption. Considering the reliability of services and the real-time workload of servers, the migration problem is modeled as a multi-objective optimization problem, and the Lyapunov optimization method is utilized to obtain utility-optimal migration decisions. Extensive simulations based on real-world datasets are performed to validate the performance of the proposed method. The results show that the proposed method achieved 6% higher prediction accuracy, 10% lower dropping rate, and 10% lower energy consumption compared to state-of-the-art methods.}
}


@article{DBLP:journals/tmc/SunZSWLLL24,
	author = {Geng Sun and
                  Xiaoya Zheng and
                  Zemin Sun and
                  Qingqing Wu and
                  Jiahui Li and
                  Yanheng Liu and
                  Victor C. M. Leung},
	title = {UAV-Enabled Secure Communications via Collaborative Beamforming With
                  Imperfect Eavesdropper Information},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3291--3308},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273293},
	doi = {10.1109/TMC.2023.3273293},
	timestamp = {Wed, 19 Jun 2024 17:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunZSWLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are playing a pivotal role in wireless networks due to their high mobility and on-demand deployment advantages. However, the UAV-enabled communications are susceptible to be wiretapped by eavesdroppers due to the strong line-of-sight (LoS) dominated air-ground channel. In this paper, we consider a UAV-enabled secure communication scenario, in which a group of UAVs form a UAV-enabled virtual antenna array (UVAA) to transmit information towards the remote base stations (BSs) via collaborative beamforming (CB), while multiple known and unknown eavesdroppers aiming to wiretap the information. Specifically, a secure communication multi-objective optimization problem (SCMOP) is formulated to achieve the maximization of the worst-case secrecy rate, the minimization of the maximum sidelobe level (SLL) as well as the minimization of the flight energy consumption of UAVs. To solve the formulated SCMOP which is demonstrated to be non-convex and NP-hard, an improved multi-objective salp swarm algorithm (IMSSA) with several specific operating factors is proposed. Simulations results demonstrate that the proposed IMSSA can deal with the formulated SCMOP effectively and outperforms other benchmark strategies. Moreover, the multi-hop relay is introduced to verify the reasonability of the UVAA system, and two benchmark schemes of the formulated SCMOP are introduced to demonstrate the necessity of the formulated SCMOP. In addition, the performance of the UVAA system under certain unexpected circumstances is estimated. Finally, experimental implementation is conducted by using a Raspberry Pi and the results demonstrate the practicality of the proposed CB-based secure communication approach in real-world scenarios.}
}


@article{DBLP:journals/tmc/LyuPZGM24,
	author = {Chen Lyu and
                  Amit Pande and
                  Yuanyuan Zhang and
                  Dawu Gu and
                  Prasant Mohapatra},
	title = {Enabling Fast and Privacy-Preserving Broadcast Authentication With
                  Efficient Revocation for Inter-Vehicle Connections},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3309--3327},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3275218},
	doi = {10.1109/TMC.2023.3275218},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LyuPZGM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many vehicular applications, especially safety-related ones, rely on spatial-temporal messages periodically broadcast by vehicles. In the absence of a secure authentication scheme, invalid spatial-temporal messages may be sent out by malicious vehicles. Meanwhile, malicious applications may also collect a lot of personal information from spatial-temporal messages. Since inter-vehicle connections are often deployed in high-moving traffic, any authentication must be implemented in real-time. To meet all these properties, we propose a Fast and Anonymous Spatial-Temporal Trust (FastTrust) scheme for inter-vehicle connections. In contrast to most authentication protocols which rely on fixed infrastructures, FastTrust is mostly designed on hash chains and an entropy-based commitment, and is able to secure periodic spatial-temporal messages. FastTrust also protects vehicles’ privacy by deploying a pseudonym-varying scheduling mechanism to satisfy the anonymity and unlinkability requirements. Finally, in order to efficiently isolate malicious vehicles, a lightweight certificate management scheme is proposed for the limited bandwidth of vehicular networks. We provide analytical evaluations to show that our FastTrust achieves the security and privacy properties. Extensive validations are done to show that FastTrust can authenticate dozens of times faster than the existing signature algorithms, and isolate malicious vehicles at a low cost in terms of communication and computational resources.}
}


@article{DBLP:journals/tmc/WangYL24,
	author = {Xiong Wang and
                  Jiancheng Ye and
                  John C. S. Lui},
	title = {Online Learning Aided Decentralized Multi-User Task Offloading for
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3328--3342},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3275851},
	doi = {10.1109/TMC.2023.3275851},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing facilitates users to offload computation tasks to edge servers for meeting their stringent delay requirements. Previous works mainly explore task offloading when system-side information is given (e.g., server processing speed, cellular data rate), or centralized offloading under system uncertainty. But both generally fall short of handling task placement involving many coexisting users in an uncertain environment. In this paper, we develop a multi-user offloading framework considering unknown yet stochastic system-side information to enable a decentralized user-initiated service placement under overlapping server coverage. Specifically, we formulate the dynamic task placement as an online multi-user multi-armed bandit process, and propose a decentralized epoch based offloading (DEBO) to optimize user rewards which are subjected under network delay. We consider both cases without and with neighboring edge feedback once users’ tasks are processed, where the latter incorporates system-side information sharing among edge servers for an enhanced task placement. For both cases, we show that DEBO can gradually deduce the optimal user-server assignment during dynamic offloading, thereby achieving a close-to-optimal service performance and tight O(\\log _{2}\\!\\!T)\nregret. Moreover, we generalize DEBO to various common scenarios such as unknown reward gap, dynamic entering or leaving of clients, and fair reward distribution, while further exploring when users’ offloaded tasks require heterogeneous computing resources. Particularly, we accomplish a sub-linear regret for each of these instances. Real measurements based evaluations corroborate the superiority of our offloading schemes over state-of-the-art approaches in optimizing delay-sensitive rewards.}
}


@article{DBLP:journals/tmc/GuoWLNZ24,
	author = {Zhenzhen Guo and
                  Gaoli Wang and
                  Yingxin Li and
                  Jianqiang Ni and
                  Guoyan Zhang},
	title = {Attribute-Based Data Sharing Scheme Using Blockchain for 6G-Enabled
                  VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3343--3360},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3273222},
	doi = {10.1109/TMC.2023.3273222},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoWLNZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of 6 G communications technology will bring about a transition from the “Internet of Everything” to the “Intelligent Connection of Everything”. 6G-enabled vehicular ad hoc networks (VANETs) will enjoy lower latency, higher speed, and greater capacity network services. Nevertheless, achieving secure data sharing will be an even tougher challenge. Given this, we propose an attribute-based data sharing scheme with blockchain for 6G-enabled VANETs. First, we propose an efficient multi-tree-based user revocation mechanism. With the Chinese remainder theorem, our mechanism supports user batch revocation and batch joining. Second, we achieve distributed data storage by utilizing the blockchain and smart contracts. To solve the problem of insufficient storage capacity on the blockchain, we adopt a combination of on-chain and off-chain storage. Third, to reduce the computation burden on users, our proposal supports online/offline encryption and verifiable outsourced decryption. Meanwhile, our mechanism supports policy hiding, data revocation, and cross-domain data sharing. The proposed scheme is proven to satisfy the indistinguishability under chosen plaintext attack (IND-CPA) in the standard model. Theoretical analysis shows that our mechanism outperforms existing schemes in functionality and security. Simulation experiments show that our proposal is efficient and suitable for 6G-enabled VANETs.}
}


@article{DBLP:journals/tmc/LinTW24,
	author = {Kuang{-}Hsun Lin and
                  Cheng{-}Wei Tsai and
                  Hung{-}Yu Wei},
	title = {Integrated Power-Efficient Time-Frequency Operations With {DRX} and
                  {BWP} Switching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3361--3375},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3274776},
	doi = {10.1109/TMC.2023.3274776},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bandwidth part (BWP) and discontinuous reception (DRX) are critical designs to help devices save power. Unlike DRX, a time-domain power-saving mechanism, the BWP is a frequency-domain mechanism newly proposed in the 3GPP NR system. The interaction between the two mechanisms needs investigation for performance optimization. Therefore, this work proposed a joint design for the BWP switching algorithm and analytical models. The simulation results showed that when the DRX cycle is 320 ms, and the packet arrival rate is 0.01/ms, our method improves 31.4% of packet delay or 19.5% of power consumption compared to the baselines. We also showed the range of the traffic arrival rate that a UE with both mechanisms configured is sensitive to the BWP switch strategy.}
}


@article{DBLP:journals/tmc/JiRCZ24,
	author = {Jiequ Ji and
                  Xiangyu Ren and
                  Lin Cai and
                  Kun Zhu},
	title = {Downlink Scheduler for Delay Guaranteed Services Using Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3376--3390},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276697},
	doi = {10.1109/TMC.2023.3276697},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiRCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a novel scheduling scheme to guarantee per-packet delay in single-hop wireless networks for delay-critical applications. We consider several classes of packets with different delay requirements, where high-class packets yield high utility after successful transmission. Considering the correla-tionship of delays among competing packets, we apply a delay-laxity concept and introduce a new output gain function for scheduling decisions. Particularly, the selection of a packet takes into account not only its output gain but also the delay-laxity of other packets. In this context, we formulate a multi-objective optimization problem aiming to minimize the average queue length while maximizing the average output gain under the constraint of guaranteeing per-packet delay. However, due to the uncertainty in the environment (e.g., time-varying channel conditions and random packet arrivals), it is difficult and often impractical to solve this problem using traditional optimization techniques. We develop a deep reinforcement learning (DRL)-based framework to solve it. Specifically, we decompose the original optimization problem into a set of scalar optimization subproblems and model each of them as a partially observable Markov Decision Process (POMDP). We then resort to a Double Deep Q Network (DDQN)-based algorithm to learn an optimal scheduling policy for each subproblem, which can overcome the large-scale state space and reduce Q-value overestimation. Simulation results show that our proposed DDQN-based algorithm outperforms the conventional Q-learning algorithm in terms of reward and learning speed. In addition, our proposed scheduling scheme can achieve significant reductions in average delay and delay outage drop rate compared to other benchmark schemes.}
}


@article{DBLP:journals/tmc/LinAQWWWLZ24,
	author = {Chi Lin and
                  Asfandeyar Ahmad and
                  Rongsheng Qu and
                  Yi Wang and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Lin and
                  Qiang Zhang},
	title = {A Handwriting Recognition System With WiFi},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3391--3409},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3279608},
	doi = {10.1109/TMC.2023.3279608},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinAQWWWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Handwriting recognition systems are a convenient and alternative way of writing in the air with fingers rather than typing on keyboards. However, existing recognition systems are limited by their low accuracy and the requirement to wear dedicated devices. To address these issues, we propose WiWrite, an accurate contactless handwriting recognition system that allows users to write in the air without wearing any wearable devices. Specifically, we employ a novel CSI division scheme to process the noisy raw WiFi channel state information (CSI), which stabilizes the CSI phase and reduces noise in CSI amplitude. To automatically retain low noise data for identification in the LOS scenario, we propose a self-paced dense convolutional network (SPDCN), which is a self-paced loss function based on a modified convolutional neural network coupled with a dense convolutional network. Furthermore, to achieve accurate handwriting recognition in the NLOS scenario, we combine ADOA and PCA algorithms to remove location-induced interference and extract action features. Comprehensive experiments show the merits of WiWrite, revealing that the recognition accuracy for the same-size input and different-size input are 93.6% and 89.0%, respectively. Moreover, WiWrite can achieve accurate recognition regardless of environment and target diversity in LOS and NLOS scenarios.}
}


@article{DBLP:journals/tmc/WangXL24,
	author = {Xuchuang Wang and
                  Hong Xie and
                  John C. S. Lui},
	title = {Analyzing Queueing Problems via Bandits With Linear Reward {\&}
                  Nonlinear Workload Fairness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3410--3423},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276158},
	doi = {10.1109/TMC.2023.3276158},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Queueing models serve as important building blocks in many networking applications such as task scheduling in mobile edge computing nodes, traffic scheduling in networks, congestion control in Internet, etc. However, queueing theory often needs to make strong assumptions about the arrival process or service rewards at each queue. In addition, fairness in serving workload among all queues is of great importance in many applications. In this paper, we address how to optimize resource allocation among multiple queues with a fairness guarantee and without any a priori knowledge of these queues’ parameters. To characterize queues with unknown parameters and the fairness requirement, we formulate an online learning model with a varying and continuous action space, as well as a nonlinear utility objective. We design an online learning algorithm to tackle the problem. We prove that our algorithm has a regret upper bound of O(\\sqrt{T}\\log T) and our model has a regret lower bound of \\Omega (\\sqrt{T}), where T stands for the number of decision rounds. The asymptotic closeness of upper and lower bounds guarantees their near tightness and our algorithm's near optimality. We discuss our model's real-world applications in mobile edge computing, wireless networks, and crowdsourcing, and conduct simulations to validate our algorithm's effectiveness.}
}


@article{DBLP:journals/tmc/ZhangZKGBZ24,
	author = {Peiying Zhang and
                  Yi Zhang and
                  Neeraj Kumar and
                  Mohsen Guizani and
                  Ahmed Barnawi and
                  Wei Zhang},
	title = {Energy-Aware Positioning Service Provisioning for Cloud-Edge-Vehicle
                  Collaborative Network Based on {DRL} and Service Function Chain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3424--3435},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276314},
	doi = {10.1109/TMC.2023.3276314},
	timestamp = {Tue, 09 Apr 2024 17:45:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZKGBZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the collaborative intelligent transportation system, providing precise positioning services is costly. Reducing resource consumption and improving revenue are crucial to the development of positioning services. Therefore, a practical algorithm that combines cloud and edge network environments is necessary to improve the positioning services. Integrating network function virtualization and edge computing can provide users with more flexible and efficient services. Based on the above issues, we use the service function chain (SFC) to improve the positioning services provided in cloud-edge-vehicle collaborative networks (CEVCN). We propose a deep reinforcement learning-assisted SFC embedding algorithm and improve its performance through training. We construct a five-layer policy network to sense the environment of CEVCN and derive the optimal node selection strategy. Finally, we use the breadth-first search algorithm to solve the embedding scheme for virtual links. The simulation results show that our proposed algorithm has excellent performance. The long-term average revenue is improved by 21%, the long-term average revenue-cost ratio is improved by 13%, and the embedding rate is improved by 8%.}
}


@article{DBLP:journals/tmc/DuanW24,
	author = {Yubin Duan and
                  Jie Wu},
	title = {Optimizing Job Offloading Schedule for Collaborative {DNN} Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3436--3451},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276937},
	doi = {10.1109/TMC.2023.3276937},
	timestamp = {Fri, 22 Mar 2024 08:59:08 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuanW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Neural Networks (DNNs) have been widely deployed in mobile applications. DNN inference latency is a critical metric to measure the service quality of those applications. Collaborative inference is a promising approach for latency optimization, where partial inference workloads are offloaded from mobile devices to cloud servers. Model partition problems for collaborative inference have been well studied. However, little attention has been paid to optimizing offloading pipeline for multiple DNN inference jobs. In practice, mobile devices usually need to process multiple DNN inference jobs simultaneously. We propose to jointly optimize the DNN partitioning and pipeline scheduling for multiple inference jobs. We theoretically analyze the optimal scheduling conditions for homogeneous chain-structure DNNs. Based on the analysis, we proposed near-optimal partitioning and scheduling methods for chain-structure DNNs. We also extend those methods for general-structure DNNs. In addition, we extend our problem scenario to handle heterogeneous DNN inference jobs. A layer-level scheduling algorithm is proposed. Theoretical analyses show that our proposed method is optimal when computation graphs are tree-structure. Our joint optimization methods are evaluated in a real-world testbed. Experiment results show that our methods can significantly reduce the overall inference latency of multiple inference jobs compared to partition-only or schedule-only approaches.}
}


@article{DBLP:journals/tmc/YangCHKP24,
	author = {Qingyong Yang and
                  Shu{-}Chuan Chu and
                  Chia{-}Cheng Hu and
                  Lingping Kong and
                  Jeng{-}Shyang Pan},
	title = {A Task Offloading Method Based on User Satisfaction in {C-RAN} With
                  Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3452--3465},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3275580},
	doi = {10.1109/TMC.2023.3275580},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangCHKP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous development of the communication service industry, users pay more attention to the quality of network service. Previous studies on offloading problems, especially in the Cloud Radio Access Network (C-RAN) architecture with Mobile Edge Computing (MEC), are primarily focused on the economic perspective, with little consideration given to user-oriented satisfaction problems. To fill this gap, this article proposes a mathematical model for maximizing user satisfaction in the C-RAN architecture with multi-layer MEC. The problem is divided into two stages for solution. The first stage addresses the optimal connection problem between users and Remote Radio Heads (RRHs). The second stage then schedules user tasks reasonably based on the solution obtained in the first stage. The two-stage problems are all proved to be NP-Hard. Two efficient approximation algorithms, namely User-to-RRH Association Algorithm (URAA) and Maximum Satisfaction Algorithm (MSA), are proposed to solve the problems in different stages. This article proves and analyzes the theoretical performance of the two algorithms. Finally, the performance of the proposed algorithms is verified by simulation experiments. The experimental results demonstrate that the two proposed algorithms can achieve reasonable solutions to the problems, and the user satisfaction level can be maintained at a high level.}
}


@article{DBLP:journals/tmc/LotfiDNSK24,
	author = {Ismail Lotfi and
                  Hongyang Du and
                  Dusit Niyato and
                  Sumei Sun and
                  Dong In Kim},
	title = {On the Robustness of Channel Allocation in Joint Radar and Communication
                  Systems: An Auction Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {4},
	pages = {3466--3483},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276934},
	doi = {10.1109/TMC.2023.3276934},
	timestamp = {Mon, 01 Apr 2024 11:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LotfiDNSK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Joint radar and communication (JRC) is a promising technique for spectrum re-utilization, which enables radar sensing and data transmission to operate on the same frequencies and the same devices. However, due to the multi-objective property of JRC systems, channel allocation to JRC nodes should be carefully designed to maximize system performance. Additionally, because of the broadcast nature of wireless signals, a watchful adversary, i.e., a warden, can detect ongoing transmissions and attack the system. Thus, we develop a covert JRC system that minimizes the detection probability by wardens, in which friendly jammers are deployed to improve the covertness of the JRC nodes during radar sensing and data transmission operations. Furthermore, we propose a robust multi-item auction design for channel allocation for such a JRC system that considers the uncertainty in bids. The proposed auction mechanism achieves the properties of truthfulness, individual rationality, budget feasibility, and computational efficiency. The simulations clearly show the benefits of our design to support covert JRC systems and to provide incentive to the JRC nodes in obtaining spectrum, in which the auction-based channel allocation mechanism is robust against perturbations in the bids, which is highly effective for JRC nodes working in uncertain environments.}
}


@article{DBLP:journals/tmc/LinCYNWY24,
	author = {Kai Lin and
                  Honglong Chen and
                  Na Yan and
                  Zhichen Ni and
                  Zhibo Wang and
                  Jiguo Yu},
	title = {Double Polling-Based Tag Information Collection for Sensor-Augmented
                  {RFID} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3496--3509},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277925},
	doi = {10.1109/TMC.2023.3277925},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LinCYNWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The significance of RFID-based information collection is becoming increasingly visible as more and more sensor-augmented RFID systems are deployed. Tag information collection aims at efficiently and accurately collecting valuable information from target objects attached with RFID tags. Polling-based information collection can effectively avoid response collisions between RFID tags, and it is widely adopted to accurately inventory tags. However, in the traditional polling mode, a polling vector can only be used to query a tag at a time, which is inefficient. In this paper, we design a double polling mode to improve the utilization of polling vectors, which can simultaneously interrogate a pair of tags. Afterwards, several techniques are developed to reduce the polling vector length. First, the Basic Double Polling-based protocol (BDP) employs double indexes to collect information, which greatly reduces the number of polling vectors. Second, the Segmented Double Polling-based protocol (SDP) divides the double indexes into several segments to cut the polling vector length down. Third, the Partial Double Polling-based protocol (PDP) replaces the double index with the size of the empty segment between two adjacent non-zero indexes to further reduce the average polling vector length. Finally, the Differential Double Polling-based protocol (DDP) utilizes the size of the empty segment between two double indexes to improve the utilization of polling vectors. After that, extensive theoretical analyses and simulations are conducted, which demonstrate the feasibility and effectiveness of the proposed protocols.}
}


@article{DBLP:journals/tmc/HanXWZZX24,
	author = {Hao Han and
                  Kunming Xie and
                  Tongyu Wang and
                  Xiaojun Zhu and
                  Yanchao Zhao and
                  Fengyuan Xu},
	title = {RescQR: Enabling Reliable Data Recovery in Screen-Camera Communication
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3510--3522},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277212},
	doi = {10.1109/TMC.2023.3277212},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanXWZZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an increasing number of mobile devices equipped with screens and cameras, screen-camera communication (SCC) systems enable data exchange between devices conveniently and efficiently. By encoding data with spatial and temporal diversity on a screen, multiple users with a camera can receive data without setting up a wireless network. However, as the transmitter pushes the limits of increasing throughput with a high display rate, the receiver actually suffers from a low goodput caused by composite frames. Those frames cannot be decoded correctly with existing methods. To address this problem, we propose a reliable data recovery scheme named RescQR. In RescQR, a mixture separation scheme coupled with a dedicated frame border is proposed to separate composite frames. A Viterbi-based data recovery scheme is proposed to recover data from blurred regions in composite frames. Additionally, an auto-configuration method with the help of a front camera is proposed to adjust parameters automatically according to the estimated distance between the screen and the camera. Our prototype and experiments demonstrate that RescQR achieves a data goodput of 400+kbps even with standard QR codes, which significantly outperforms previous solutions.}
}


@article{DBLP:journals/tmc/YuanZHTSXL24,
	author = {Mu Yuan and
                  Lan Zhang and
                  Fengxiang He and
                  Xueting Tong and
                  Miao{-}Hui Song and
                  Zhengyuan Xu and
                  Xiang{-}Yang Li},
	title = {InFi: End-to-End Learning to Filter Input for Resource-Efficiency
                  in Mobile-Centric Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3523--3538},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3275981},
	doi = {10.1109/TMC.2023.3275981},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuanZHTSXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile-centric AI applications have high requirements for the resource-efficiency of model inference. Input filtering is a promising approach to eliminate redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formulate the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in feature embedding with robust discriminability. We design and implement InFi that supports different input modalities and mobile-centric deployments. Comprehensive evaluations confirm our theoretical results and show that InFi outperforms strong baselines in applicability, accuracy, and efficiency. InFi can achieve 8.5× throughput and save 95% bandwidth, while keeping over 90% accuracy, for a video analytics application on mobile platforms.}
}


@article{DBLP:journals/tmc/GaoZSZHLR24,
	author = {Ming Gao and
                  Lingfeng Zhang and
                  Leming Shen and
                  Xiang Zou and
                  Jinsong Han and
                  Feng Lin and
                  Kui Ren},
	title = {Exploring Practical Acoustic Transduction Attacks on Inertial Sensors
                  in {MDOF} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3539--3557},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277287},
	doi = {10.1109/TMC.2023.3277287},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoZSZHLR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cyber-physical systems, inertial sensors are the basis for identifying motion states and making actuation decisions. However, extensive studies have proved the vulnerability of those sensors under acoustic transduction attacks, which leverage malicious acoustics to trigger sensor measurement errors. Unfortunately, the threat from such attacks is not assessed properly because of the incomplete investigation on the attack's potential, especially towards multiple-degree-of-freedom systems, e.g., drones. To thoroughly explore the threat of acoustic transduction attacks, we revisit the attack model and design a new yet practical acoustic modulation-based attack, named KITE. Such an attack enables stable and controllable injections, even under frequency offset based distortions that limit the effect of prior attacking approaches. KITE exploits the potential threat of transduction attacks without the need of strengthening attackers’ abilities. Furthermore, we extend the attack surface to multiple-degree-of-freedom (MDOF) systems, which are more widely deployed but ignored by prior work. Our study also covers the scenario of attacking moving targets. By revealing the practical threat from acoustic transduction attacks, we appeal for both the attention to their harm and necessary countermeasures.}
}


@article{DBLP:journals/tmc/YangZLWYZ24,
	author = {Zhao Yang and
                  Shengbing Zhang and
                  Chuxi Li and
                  Miao Wang and
                  Jiaying Yang and
                  Meng Zhang},
	title = {Equalized Aggregation for Heterogeneous Federated Mobile Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3558--3575},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276900},
	doi = {10.1109/TMC.2023.3276900},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangZLWYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is widely used in mobile edge applications. However, the heterogeneity issues of mobile edge devices pose significant challenges to the generalization of the global model in FL. In this paper, we propose LegoFL to simultaneously solve multiple heterogeneity issues in response to mobile edge computing characteristics. LegoFL identifies two types of heterogeneous behaviors in FL, namely heterogeneous parameter training and communication behaviors, to address multiple heterogeneity issues. These two types of heterogeneous behaviors result in feature and feature representation range mismatches between local communication parameters. To reduce these mismatches and improve the generalization of the global model, LegoFL dynamically distinguishes the parameter feature representation of different nodes using the global model's common feature as guidance. Then, under the connection states and system communication constraints, LegoFL dynamically selects contribution parameters on each device that can guarantee the generalization and performance of the global model for communication. Finally, to avoid the overfitting problem of the global model, heterogeneous local models are aggregated at the central server with matched feature representations. Extensive experiments on various datasets show that LegoFL achieves competitive performance. The accuracy and communication efficiency are improved by up to 12.86\\% and 4.09× compared to state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/YaoLZ24,
	author = {Yao Yao and
                  Yan Li and
                  Ting Zhu},
	title = {Interference-Negligible Privacy-Preserved Shield for {RF} Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3576--3588},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3276930},
	doi = {10.1109/TMC.2023.3276930},
	timestamp = {Sat, 27 Jul 2024 13:40:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YaoLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Researchers have demonstrated the feasibility of detecting human motion behind the wall with radio frequency (RF) sensing techniques. With these techniques, an eavesdropper can monitor people's behavior from outside of the room without the need to access the room. This introduces a severe privacy-leakage issue. To address this issue, we propose Aegis, an interference-negligible RF sensing shield that i) incapacitates the RF sensing of eavesdroppers that work on any WiFi frequency bands and at any unknown locations outside of the protected area; ii) has minimum interference to the on-going WiFi communication; and iii) preserves authorized RF sensing inside the private region. Our extensive evaluation shows that when Aegis is activated, the accuracy of legitimate sensing system only decreases by 0.08, while the accuracy of the illegitimate sensing system is as low as 0.04. Moreover, the on-going data communication throughput is even increased by \\text{10}\\;\\text{MB/s} on \\text{2.4}\\;\\;\\text{GHz} WiFi band and \\text{5}\\;\\text{MB/s} on \\text{5}\\;\\;\\text{GHz} WiFi band.}
}


@article{DBLP:journals/tmc/KlusKTLGN24,
	author = {Lucie Klus and
                  Roman Klus and
                  Joaqu{\'{\i}}n Torres{-}Sospedra and
                  Elena Simona Lohan and
                  Carlos Granell and
                  Jari Nurmi},
	title = {EWOk: Towards Efficient Multidimensional Compression of Indoor Positioning
                  Datasets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3589--3604},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277333},
	doi = {10.1109/TMC.2023.3277333},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KlusKTLGN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor positioning performed directly at the end-user device ensures reliability in case the network connection fails but is limited by the size of the Received Signal Strength (RSS) radio map necessary to match the measured array to the device's location. Reducing the size of the RSS database enables faster processing, and saves storage space and radio resources necessary for the database transfer, thus cutting implementation and operation costs, and increasing the quality of service. In this work, we propose EWOk, an Element-Wise cOmpression using k-means, which reduces the size of the individual radio measurements within the fingerprinting radio map while sustaining or boosting the dataset's positioning capabilities. We show that the 7-bit representation of measurements is sufficient in positioning scenarios, and reducing the data size further using EWOk results in higher compression and faster data transfer and processing. To eliminate the inherent uncertainty of k-means we propose a data-dependent, non-random initiation scheme to ensure stability and limit variance. We further combine EWOk with principal component analysis to show its applicability in combination with other methods, and to demonstrate the efficiency of the resulting multidimensional compression. We evaluate EWOk on 25 RSS fingerprinting datasets and show that it positively impacts compression efficiency, and positioning performance.}
}


@article{DBLP:journals/tmc/ChenCCJ24,
	author = {Jinlin Chen and
                  Jiannong Cao and
                  Zhiqin Cheng and
                  Shan Jiang},
	title = {Towards Efficient Distributed Collision Avoidance for Heterogeneous
                  Mobile Robots},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3605--3619},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3279906},
	doi = {10.1109/TMC.2023.3279906},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenCCJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of distributed collision avoidance for mobile robotic systems, where a group of heterogeneous robots with different sizes and motion constraints avoid collisions with each other and static obstacles during the movements from their starting to goal locations. Existing methods mainly consider homogeneous robots and incur a high collision rate in environments with moving robots and static objects. Hence, we propose a distributed collision avoidance for heterogeneous mobile robots (Heter-CA), which allows each robot to independently avoid collisions considering the heterogeneity of robots and varying static obstacles. In Heter-CA, each robot predicts the trajectories of neighboring robots and estimates the varying size of static obstacles with the robots’ range-finder sensors before motion planning, which enables each robot to avoid obstacles safely. Besides, we prove that Heter-CA can guarantee collision-free movement between heterogeneous robots by satisfying sufficient conditions. We evaluate Heter-CA in numerous simulated and real-world scenarios in which groups of heterogeneous robots perform navigation tasks. The experimental results demonstrate that Heter-CA takes 10\\times\nless computation time and achieves 5% less collision rate than baseline algorithms.}
}


@article{DBLP:journals/tmc/YangXLLS24,
	author = {Yufei Yang and
                  Chenhao Xie and
                  Liansheng Liu and
                  Philip H. W. Leong and
                  Shuaiwen Leon Song},
	title = {Efficient Radius Search for Adaptive Foveal Sizing Mechanism in Collaborative
                  Foveated Rendering Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3620--3632},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277577},
	doi = {10.1109/TMC.2023.3277577},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangXLLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Foveated Rendering (CFR) is the latest collaborative rendering framework proposed to enable high frame rate VR applications on mobile devices. Compared with the strategies adopted in conventional collaborative rendering, the pixel-based Adaptive Foveal Sizing (AFS) mechanism in CFR offers a more flexible and intelligent workload trade-off by predicting the radius. However, the performance of the AFS mechanism in actual deployment depends on its adaptability to two factors, including the Sudden Environmental Variations (SEV) and the Random Discrete Latency (RDL). Guaranteeing the performance of the AFS mechanism by adapting to these two factors is of great significance to guaranteeing users’ immersive experience. This paper identifies the existence of the SEV and RDL phenomenon in the AFS mechanism for the first time, and contributes the first method that offers the effective and real-time AFS mechanism implementation for the practical deployment, namely the Efficient Radius Search (ERS). The ERS method efficiently searches the largest radius online that controls the rendering workload within the foveated layer just below the offline baked threshold, thereby achieving the immediate response to SEV and reducing the oscillating frame rendering latency led by RDL. Through the experiments on 3 open-source mobile VR applications and 4 mobile devices with representative SoCs, the resulting 2.44 × to 9.07 × higher frame rate precision compared with the state-of-the-art method demonstrate the superiority of the ERS method.}
}


@article{DBLP:journals/tmc/ZhangZYMAZ24,
	author = {Jiaxin Zhang and
                  Liang Zhao and
                  Keping Yu and
                  Geyong Min and
                  Ahmed Yassin Al{-}Dubai and
                  Albert Y. Zomaya},
	title = {A Novel Federated Learning Scheme for Generative Adversarial Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3633--3649},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3278668},
	doi = {10.1109/TMC.2023.3278668},
	timestamp = {Sat, 20 Apr 2024 10:27:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZYMAZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative adversarial networks (GANs) have been advancing and gaining tremendous interests from both academia and industry. With the development of wireless technologies, a huge amount of data generated at the network edge provides an unprecedented opportunity to develop GANs applications. However, due to the constraints such as bandwidth, privacy, and legal issues, it is inappropriate to collect and send all data to the cloud or servers for analysis, training, and mining. Thus, deploying and training GANs at the edge becomes a promising alternative solution. The instability of GANs introduced by non-independent and identical data (Non-IID) poses significant challenges to training GANs. To address these challenges, this paper presents a novel federated learning framework for GANs, namely, Collaborated gAme Parallel Learning (CAP). CAP supports parallel training of data and models for GANs, breaking the isolated training among generators that exists in the previous distributed algorithms, and achieving collaborative learning among cloud, edge servers, and devices. Then, to further enhance the ability of CAP-GAN for addressing Non-IID issues, we propose a Mix-Generator module (Mix-G) which divides a generator into the sharing layer and personalizing layer. The Mix-G module extracts the generic and personalization features and improves the performance of CAP-GAN on extremely personalizing datasets. Experimental results and analysis substantiate the usefulness and superiority of our proposed CAP-GAN scheme which can achieve better results in the Non-IID scenarios compared with the state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/MiaoLLLNLCD24,
	author = {Yinbin Miao and
                  Feng Li and
                  Xinghua Li and
                  Zhiquan Liu and
                  Jianting Ning and
                  Hongwei Li and
                  Kim{-}Kwang Raymond Choo and
                  Robert H. Deng},
	title = {Time-Controllable Keyword Search Scheme With Efficient Revocation
                  in Mobile E-Health Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3650--3665},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3277702},
	doi = {10.1109/TMC.2023.3277702},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MiaoLLLNLCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electronic health (e-health) systems may outsource data such as patient e-health records to mobile cloud servers for efficiency gains (e.g., minimizing local storage and computation costs). However, such a move may result in privacy implications in the presence of semi-honest cloud servers. Searchable Encryption (SE) can potentially facilitate privacy-preserving searches based on keywords for encrypted data stored in the mobile cloud, but most existing SE solutions do not support temporal access control (i.e., a mechanism that grants access permissions to users for specified time ranges). Hence, in this paper we design a time-controllable keyword search scheme by using an attribute-based comparable access control. This allows users to match indexes encrypted at specified time intervals. Then, we improve the basic framework to support efficient user revocation using secret sharing. We then formally prove the security of our proposed frameworks against chosen-keyword attack and key collusion attack, as well as achieving keyword secrecy. We also evaluate the performance of our proposed approach using a real-world dataset to demonstrate their practical utility.}
}


@article{DBLP:journals/tmc/IslamTM24,
	author = {Amirul Islam and
                  Nikolaos Thomos and
                  Leila Musavian},
	title = {Multi-Agent Deep Reinforcement Learning for Spectral Efficiency Optimization
                  in Vehicular Optical Camera Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3666--3679},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3278277},
	doi = {10.1109/TMC.2023.3278277},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/IslamTM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a vehicular optical camera communication system that can meet low bit error rate (BER) and ultra-low latency constraints. First, we formulate a sum spectral efficiency optimization problem that aims at finding the speed of vehicles and the modulation order that maximizes the sum spectral efficiency subject to reliability and latency constraints. This problem is mixed-integer programming with nonlinear constraints, and even for a small set of modulation orders, is NP-hard. To overcome the entailed high computational and time complexity which prevents its solution with traditional methods, we first model the optimization problem as a partially observable Markov decision process. We then solve it using an independent Q-learning framework, where each vehicle acts as an independent agent. Since the state-action space is large we then adopt deep reinforcement learning (DRL) to solve it efficiently. As the problem is constrained, we employ the Lagrange relaxation approach prior to solving it using the DRL framework. Simulation results demonstrate that the proposed DRL-based optimization scheme can effectively learn how to maximize the sum spectral efficiency while satisfying the BER and ultra-low latency constraints. The evaluation further shows that our scheme can achieve superior performance compared to radio frequency-based vehicular communication systems and other vehicular OCC variants of our scheme.}
}


@article{DBLP:journals/tmc/ShaoCWGLLG24,
	author = {Yulin Shao and
                  Yucheng Cai and
                  Taotao Wang and
                  Ziyang Guo and
                  Peng Liu and
                  Jianjun Luo and
                  Deniz G{\"{u}}nd{\"{u}}z},
	title = {Learning-Based Autonomous Channel Access in the Presence of Hidden
                  Terminals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3680--3695},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282790},
	doi = {10.1109/TMC.2023.3282790},
	timestamp = {Sat, 10 Aug 2024 12:22:17 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShaoCWGLLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of autonomous channel access (AutoCA), where a group of terminals tries to discover a communication strategy with an access point (AP) via a common wireless channel in a distributed fashion. Due to the irregular topology and the limited communication range of terminals, a practical challenge for AutoCA is the hidden terminal problem, which is notorious in wireless networks for deteriorating throughput and delay performances. To meet the challenge, this paper presents a new multi-agent deep reinforcement learning paradigm, dubbed MADRL-HT, tailored for AutoCA in the presence of hidden terminals. MADRL-HT exploits topological insights and transforms the observation space of each terminal into a scalable form independent of the number of terminals. To compensate for the partial observability, we put forth a look-back mechanism such that the terminals can infer behaviors of their hidden terminals from the carrier-sensed channel states as well as feedback from the AP. A window-based global reward function is proposed, whereby the terminals are instructed to maximize the system throughput while balancing the terminals’ transmission opportunities over the course of learning. Considering short-packet machine-type communications, extensive numerical experiments verified the superior performance of our solution benchmarked against the legacy carrier-sense multiple access with collision avoidance (CSMA/CA) protocol.}
}


@article{DBLP:journals/tmc/XuWWCCZG24,
	author = {Wenchao Xu and
                  Haodong Wan and
                  Haozhao Wang and
                  Nan Cheng and
                  Quan Chen and
                  Haibo Zhou and
                  Song Guo},
	title = {Fast Packet Loss Inferring via Personalized Simulation-Reality Distillation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3696--3706},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3281725},
	doi = {10.1109/TMC.2023.3281725},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWWCCZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet loss inferring can enable a transceiver to distinguish between channel impairment and collision for transmission failures, and thus can improve the network performance by exclusively performing rate adaptation or adjusting the medium access parameter. Machine learning methods from literature have shown great potential in producing models that can detect the loss causes over various network trace, however haven't considered accurate data-driven loss inferring on resource-constrained devices that cannot accommodate deep models. In this paper, we propose a novel packet loss inferring framework that can train lightweight models to distinguish between channel losses and collisions by learning the data trace from both simulation and real devices. Specifically, we first train a sophisticated teacher model based on extensive simulation datasets, whose knowledge is then transferred to a small student model that can be deployed on tiny device. The simulation-reality distillation is conducted via personalized trace from each client correspondingly, whose performance bound is analytically guaranteed. We have implemented our method on real testbed and show that the network access performance can be significantly improved, especially for sudden network variations.}
}


@article{DBLP:journals/tmc/GaoCD24,
	author = {Shuqin Gao and
                  Costas Courcoubetis and
                  Lingjie Duan},
	title = {Average-Case Analysis of Greedy Matching for Large-Scale {D2D} Resource
                  Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3707--3721},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3278681},
	doi = {10.1109/TMC.2023.3278681},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given the proximity of many wireless users and their diversity in consuming local resources (e.g., data-plans, computation and energy resources), device-to-device (D2D) resource sharing is a promising approach towards realizing a sharing economy. This paper adopts an easy-to-implement greedy matching algorithm with distributed fashion and only sub-linear O(\\log n)\nparallel complexity (in user number n\n) for large-scale D2D sharing. Practical cases indicate that the greedy matching's average performance is far better than the worst-case approximation ratio 50% as compared to the optimum. However, there is no rigorous average-case analysis in the literature to back up such encouraging findings and this paper is the first to present such analysis for multiple representative classes of graphs. For 1D linear networks, we prove that our greedy algorithm performs better than 86.5% of the optimum. For 2D grids, though dynamic programming cannot be directly applied, we still prove this average performance ratio to be above 76%. For the more challenging Erdos-Rényi random graphs, we equivalently reduce to the asymptotic analysis of random trees and successfully prove a ratio up to 79%. Finally, we conduct experiments using real data to simulate realistic D2D networks, and show that our analytical performance measure approximates well practical cases.}
}


@article{DBLP:journals/tmc/HuangCZWW24,
	author = {Yongzhi Huang and
                  Kaixin Chen and
                  Jiayi Zhao and
                  Lu Wang and
                  Kaishun Wu},
	title = {Beverage Deterioration Monitoring Based on Surface Tension Dynamics
                  and Absorption Spectrum Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3722--3740},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3279837},
	doi = {10.1109/TMC.2023.3279837},
	timestamp = {Mon, 15 Apr 2024 08:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangCZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Biochemical information sensing has always been one of the challenges in ubiquitous sensing research for mobile computing. Microorganisms will cause undetectable deterioration in drink production, such as wine and beverage, and microbial contamination is highly susceptible during storage like some liquors can be bottled for sometimes over ten years. Microbial culture methods are common for quality monitoring but unsuitable for real-time beverage quality monitoring. As far as we know, we are the first to use ubiquitous sensing for real-time microbial contamination detection. We designed a lightweight monitoring system called Microbe-Radar, which uses light signals to monitor real-time beverage quality. Microbe-Radar uses eight LEDs and a photodiode to detect fine-grained surface tension and absorption spectrum changes caused by microbial metabolites and growth during deterioration. Characteristic offset degree measurement and absorption spectrum dimension expansion are two critical technologies. Moreover, we implemented countermeasures against ambient light noise and sloshing interference. Microbe-Radar's surface tension and absorption spectrum measurement errors are only 0.89 mN/m and 2.4%, respectively, making identifying the contamination duration, microorganism content, and microorganism composition worthwhile. Experiments showed Microbe-Radar could determine potential issues with liquor quality when the liquid becomes health-threatening or even just contaminated, with an accuracy of 97.5%. Microbe-Radar can also be extended to beverage deterioration warning, with deterioration prediction accuracy of more than 90.6% for five beverages (milk, apple juice, etc.).}
}


@article{DBLP:journals/tmc/LiDOWYW24,
	author = {Xinglin Li and
                  Hanhui Deng and
                  Jinhui Ouyang and
                  Huayan Wan and
                  Weiren Yu and
                  Di Wu},
	title = {Act as What You Think: Towards Personalized {EEG} Interaction Through
                  Attentional and Embedded {LSTM} Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3741--3753},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283022},
	doi = {10.1109/TMC.2023.3283022},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiDOWYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The “mind-controlling” capability has always been in mankind's fantasy. With the recent advancements in electroencephalograph (EEG) techniques, brain-computer interface (BCI) researchers have explored some solutions to allow individuals to perform various tasks using their minds. However, the commercial off-the-shelf devices to run accurate EEG signal collection are usually expensive and the comparably cheaper devices can only present coarse results, which prevents the practical application of these devices in domestic services. To tackle this challenge, we propose and develop an end-to-end solution that enables fine brain-robot interaction (BRI) through embedded learning of coarse EEG signals from low-cost devices, namely PerBCI, so that people having difficulty moving, such as the elderly, can mind command and control a robot to perform some basic household tasks. Our contributions are three folds: 1) We present a stacked long short-term memory (BiLSTM) structure, along with specific pre-processing techniques to handle the time-dependency of EEG signals and their classification. 2) We propose a personalized design to adaptively capture multiple features and achieve accurate recognition of individual EEG signals by enhancing the signal interpretation of BiLSTM with an attention mechanism. 3) We develop a low-cost, real-time and end-to-end BRI system that can run our PerBCI models and algorithms in the embedded robot platform to perform more than one type of domestic task based on the users’ EEG signal inputs. Our real-world experiments with elderly participants of diverse backgrounds in a home setting and system comparison with other approaches show that the proposed end-to-end solution with low cost can achieve satisfactory run-time speed, accuracy and energy-efficiency.}
}


@article{DBLP:journals/tmc/ZhangQSSW24,
	author = {Songwei Zhang and
                  Tie Qiu and
                  Weisheng Si and
                  Quan Z. Sheng and
                  Dapeng Oliver Wu},
	title = {{TEAM:} {A} Layered-Cooperation Topology Evolution Algorithm for Multi-Sink
                  Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3754--3768},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282196},
	doi = {10.1109/TMC.2023.3282196},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangQSSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous sensor nodes deployed in the Internet of Things (IoT) can form a large heterogeneous network. The increased energy consumption of sensor nodes and the unbalanced communication load on multiple sink nodes reduce the energy efficiency of the network. Moreover, frequent network attacks also pose severe challenges to topology robustness. Optimizing the network topology to achieve the balance between energy efficiency and robustness is a complex problem. Multi-objective heuristic algorithms based on genetic evolution are commonly used to solve joint optimization problems. However, due to the lack of global search ability caused by the loss of genetic diversity, genetic operations are prone to premature convergence during multi-objective evolution. Therefore, this paper introduces multi-population cooperation into the multi-objective evolution process and proposes a novel layered-cooperation Topology Evolution Algorithm for Multi-sink IoT (TEAM). In TEAM, information entropy is used to measure the effectiveness of load balancing on multiple sink nodes. The crossover and mutation probabilities of different populations are dynamically adjusted to ensure genetic diversity. A layered-cooperation mechanism is designed to avoid premature convergence. Extensive experiments confirm that TEAM can effectively improve the energy efficiency and robustness of network topology while balancing the communication load on multi-sink nodes.}
}


@article{DBLP:journals/tmc/LiDWJL24,
	author = {Yang Li and
                  Chenglong Dou and
                  Yuan Wu and
                  Weijia Jia and
                  Rongxing Lu},
	title = {{NOMA} Assisted Two-Tier {VR} Content Transmission: {A} Tile-Based
                  Approach for QoE Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3769--3784},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3280739},
	doi = {10.1109/TMC.2023.3280739},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiDWJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual reality (VR) provides users with an immersive and interactive experience through head-mounted devices, which has attracted increasing attention in recent years. Specifically, tile-based VR content transmission provides a promising approach to alleviate the conflict between limited bandwidth and high-performance requirements (e.g., high-resolution and low-delay). However, the tiling pattern affects the encoding efficiency and visual distortion of the VR content. Accounting for this issue, in this paper, a quality of experience (QoE)-aware cost minimization problem is investigated for a tile-based VR content transmission scenario. In particular, an edge server (ES) co-located at a cellular base station (BS) separates its generated VR content into several tiles according to the tiling pattern selection, and a weighted-to-spherically-uniform quality model is used to evaluate the effect of different tiling patterns on QoE. Moreover, to improve the transmission performance between the edge server and VR users (VRUs), unmanned aerial vehicles (UAVs) are leveraged as relay points to provide line of sight channels. Then, we formulate an optimization problem to minimize the sum of weighted total energy consumption and VR content distortion (i.e., QoE-aware cost) by jointly optimizing the tiling pattern selections, the VRUs-UAV grouping, partial computing decisions, and resource allocation. The formulated problem is a mixed integer non-linear programming problem, which is challenging to solve. To address this difficulty, we equivalently decompose the formulated problem into three subproblems and propose corresponding algorithms to solve them, respectively. Numerical results demonstrate that our proposed solution can effectively reduce the QoE-aware cost for VR content transmission in comparison with other baseline algorithms.}
}


@article{DBLP:journals/tmc/WangLLGYLHZ24,
	author = {Tianben Wang and
                  Zhangben Li and
                  Xiantao Liu and
                  Tao Gu and
                  Honghao Yan and
                  Jing Lv and
                  Jin Hu and
                  Daqing Zhang},
	title = {MultiResp: Robust Respiration Monitoring for Multiple Users Using
                  Acoustic Signal},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3785--3801},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3279976},
	doi = {10.1109/TMC.2023.3279976},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLLGYLHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, we have seen efforts made to monitor respiration for multiple users. Existing approaches capture chest movement relying on signals directly reflected from chest or separate breath waves based on breath rate difference between subjects. However, several limitations exist: 1) they may fail when subjects face away from the transceiver or are blocked by obstacles or other subjects; 2) they may fail to separate subjects’ breath waves with the same or similar rates (i.e., breath rate difference < 1 bpm); 3) they assume a priori knowledge of number of subjects and cannot adapt to dynamic change of subject number during monitoring. To overcome these limitations, in this paper we propose MultiResp, a multi-user respiration monitoring system using acoustic signal. By fully leveraging the abundant acoustic signals reflected indirectly from subjects’ chest, MultiResp can robustly capture chest movement even when they face away from the transceiver or are blocked. By extracting fine-grained breath rate and phase difference between different subjects, MultiResp can separate the breath waves with the same or similar rates and adapt to dynamic change of subject number during monitoring. Extensive experiments show that MultiResp is able to accurately monitor the respiration of multiple users with a median error of 0.3 bpm in various indoor scenarios, however, it fails when the sound pressure is lower than 55 dB or body movement is happening.}
}


@article{DBLP:journals/tmc/ZhangLCLZCZ24,
	author = {Yufan Zhang and
                  Yan{-}Jun Li and
                  Bo Chen and
                  Ertao Li and
                  Kechen Zheng and
                  Kaikai Chi and
                  Yihua Zhu},
	title = {Design of an RFID-Based Self-Jamming Identification and Sensing Platform},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3802--3816},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3280942},
	doi = {10.1109/TMC.2023.3280942},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLCLZCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Commodity RFID tags backscatter stored electronic product code (EPC) to the reader, but do not have sensing capability. Existing works have made much effort on designing RFID-based sensing platform. But most of them either need intricate hardware design or rely on modification of the tag, which increases the cost or constrains the sensing capability. In this paper, we design a self-jamming identification and sensing platform (SJISP) consisting of SJISP nodes and a commodity RFID reader. A subtle design of the SJISP node is the adoption of a jammer radio module with the same frequency as the reader, controlled by the micro control unit (MCU) to decide whether to interfere with the query process of the RFID reader. The RFID tag is not readable if the jammer is turned on to generate interference signals. Otherwise, it is readable when the jammer is turned off. The sensing data is thus modulated by switching the jammer on and off for transmitting bit 0 and bit 1, respectively. The reader demodulates the data through the compatible EPC UHF Gen2 air interface protocol. To further save the energy of the SJISP node, we propose a prefix codebook based data delivery scheme, which leverages the difference of energy consumption (DEC) between transmitting bit 0 and bit 1. Our proposed scheme can save more than 50\\% of the energy than common communication without codebook. Experimental results based on our prototyped system show that the designed SJISP can achieve an average packet reception rate (PRR) of over 99\\% and is quite robust to environmental disturbance. Our designed platform provides a low-cost and compatible solution to extend the sensing capability of RFID system. A demo application with a temperature sensor and a light sensor embedded in two SJISP nodes respectively are developed to demonstrate how SJISP applies in real world scenario.}
}


@article{DBLP:journals/tmc/ZhangCHCZM24,
	author = {Xinran Zhang and
                  Zheng Chang and
                  Tao Hu and
                  Weilong Chen and
                  Xin Zhang and
                  Geyong Min},
	title = {Vehicle Selection and Resource Allocation for Federated Learning-Assisted
                  Vehicular Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3817--3829},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283295},
	doi = {10.1109/TMC.2023.3283295},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCHCZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To exploit the massive amounts of onboard data in vehicular networks while protecting data privacy and security, federated learning (FL) is regarded as a promising technology to support enormous vehicular applications. Despite that FL has great potential to improve the architecture of intelligent vehicular networks, the mobility of the vehicles and the dynamic nature of wireless channels make the integration of FL and vehicular networks more challenging. In this paper, we propose a vehicle mobility- and channel dynamic-aware FL (MADCA-FL) scheme to fit vehicular networks and enhance learning performances. This novel scheme enables the RSU to select appropriate vehicles and weightedly average the local models. Afterward, MADCA-FL formulates a problem to maximize the model accuracy while assuring the latency and energy restrictions, by jointly optimizing the computation and communication resources. With a mixed-integer non-linear programming structure, the problem is NP-hard. First, we utilize the successive convex approximation algorithm to handle the non-convexity, and then apply the Lagrange multiplier method and the block coordinate descent method to obtain the optimal solution. Extensive experiments are conducted to confirm the effectiveness of our proposed scheme.}
}


@article{DBLP:journals/tmc/SunBLCAL24,
	author = {Guolin Sun and
                  Gordon Owusu Boateng and
                  Liyuan Luo and
                  Huan Chen and
                  Daniel Ayepah{-}Mensah and
                  Guisong Liu},
	title = {Competitive Pricing for Resource Trading in Sliced Mobile Networks:
                  {A} Multi-Agent Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3830--3845},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3281203},
	doi = {10.1109/TMC.2023.3281203},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunBLCAL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of network slicing as a flagship technology in 5G networks has not only enhanced network expansion and flexibility in resource management for service continuity, but also provided an avenue for establishing a viable market for resource sharing. To optimize the network's resource usage, stakeholders are encouraged to take pragmatic steps toward dynamic resource sharing. This paper designs a techno-economic model for the strategic interactions among multiple competing mobile virtual network operators (MVNOs) and their users in a trading marketplace. We formulate the dynamic pricing problem as a two-stage Stackelberg game, where the MVNOs are leaders, and the users are followers. In the first stage, the MVNOs compete to set their differentiated unit prices using a negotiation mechanism while considering system-level network load. Then, the users decide their purchasing volumes to match the prices of the MVNOs. We transform the game-based optimization problem into a stochastic Markov decision process (MDP) problem and propose a multi-agent deep Q-network (MADQN) method that obtains an optimal solution for the formulated game. Simulation results and analysis reveal that the proposed algorithm achieves convergence under the competitive pricing scheme (CPS) and independent pricing scheme (IPS) while enhancing MVNOs and users’ utilities at acceptable levels.}
}


@article{DBLP:journals/tmc/LiuCYC24,
	author = {Kanghuai Liu and
                  Lin Chen and
                  Jihong Yu and
                  Haochen Cui},
	title = {On Batch Writing in {COTS} {RFID} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3846--3857},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283238},
	doi = {10.1109/TMC.2023.3283238},
	timestamp = {Fri, 02 Aug 2024 08:20:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the batch writing problem in RFID systems, where the reader seeks the most time-efficient way to write information into a given subset of tags. The problem is analogous to the multicast problem in classical networks, but one-to-many transmission is not supported in COTS RFID systems. Driven by the technical challenge, this paper addresses the problem of designing batch writing algorithms for COTS RFID systems. We make three contributions. First, we establish the minimal execution time for any batch writing algorithm, thus setting the theoretical performance limit. Second, we quantitatively compare and gauge the existing propositions applicable to our problem. Third, we develop a novel batch writing algorithm with minimal 25% performance gain over the best state-of-the-art solution. Our key technicalities are designing an encoding scheme allowing the reader to efficiently perform batch writing and optimizing the batch writing sequence to minimize the overall execution time. We also perform extensive experiments to demonstrate the effectiveness of our algorithm.}
}


@article{DBLP:journals/tmc/KarmakarKA24,
	author = {Raja Karmakar and
                  Georges Kaddoum and
                  Ouassima Akhrif},
	title = {A {PUF} and Fuzzy Extractor-Based UAV-Ground Station and {UAV-UAV}
                  Authentication Mechanism With Intelligent Adaptation of Secure Sessions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3858--3875},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3284216},
	doi = {10.1109/TMC.2023.3284216},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KarmakarKA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is crucial that communication between an unmanned aerial vehicle (UAV) and the ground station (GS) be secure, and both devices should mutually authenticate each other to ensure that an adversary cannot obtain communicated information. Moreover, dynamically adapting the session time of an authenticated session can decrease the idle time of a session and consequently reduce the window of opportunity for an adversary to interfere with the communication link. In light of these considerations, we design a physically unclonable function (PUF) and fuzzy extractor-based UAV-GS authentication mechanism called UAV Authentication with Adaptive Session (UAAS). In UAAS, both UAV-GS and UAV-UAV authentication are two-way. We use a Thompson Sampling (TS)-based approach to intelligently adapt the duration of a session. Both formal and informal security proofs are presented along with a computation and communication cost analysis to analyze the performance of UAAS. It is noted that UAAS is secure against various well-known attacks and has a lower communication cost than several baseline mechanisms. Due to the noise reduction that occurs in PUFs, the computational cost of UAAS is higher than that of baselines that ignore noise. Also, our simulation shows that UAAS significantly outperforms baselines when it comes to network performance.}
}


@article{DBLP:journals/tmc/WangWLLWZHGZ24,
	author = {Tianben Wang and
                  Zhisheng Wang and
                  Xiantao Liu and
                  Wenbo Liu and
                  Leye Wang and
                  Yuanqing Zheng and
                  Jin Hu and
                  Tao Gu and
                  Daqing Zhang},
	title = {OmniResMonitor: Omnimonitoring of Human Respiration using Acoustic
                  Multipath Reflection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3876--3889},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3281928},
	doi = {10.1109/TMC.2023.3281928},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangWLLWZHGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contactless respiration monitoring using wireless signals has drawn much attention in recent years. Many approaches have been proposed, however, they may not work when there is a lack of signals directly reflected from target's chest, e.g., a target faces away from the transceiver or a target is blocked by furniture. In this paper, we design and implement a novel omnimonitoring system for human respiration, OmniRespMonitor, using a pair of speaker and microphone. Different from Radio Frequency (RF) signal, acoustic signals cannot penetrate through walls and furniture. The multipath reflection in an indoor environment will result in highly abundant acoustic signals. In this case, even though there are lack of acoustic signals directly reflected by a target's chest, indirectly-reflected acoustic signals can still be received by the microphone. We can therefore monitor the target's respiration by extracting this subtle variation of indirectly reflected signals. To achieve this, we model chest movement using truncated System Frequency Response (SFR). We then develop a global search method based on the autocorrelation function to extract minute chest movement from SFR sequences. Finally, we dynamically synthesize the chest movement information to recover the breathing wave in real time. We conduct extensive experiments with both humans and animals (goat), the results show that OmniResMonitor is able to monitor single target's respiration within 5 meters in indoor environments in various challenging scenarios there are lack of directly-reflected acoustic signals.}
}


@article{DBLP:journals/tmc/ChiariottiDTLZZ24,
	author = {Federico Chiariotti and
                  Matteo Drago and
                  Paolo Testolina and
                  Mattia Lecci and
                  Andrea Zanella and
                  Michele Zorzi},
	title = {Temporal Characterization and Prediction of {VR} Traffic: {A} Network
                  Slicing Use Case},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3890--3908},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282689},
	doi = {10.1109/TMC.2023.3282689},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChiariottiDTLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past few years, the concept of Virtual Reality (VR) has attracted increasing interest thanks to its extensive industrial and commercial applications. Currently, the 3D models of the virtual scenes are generally stored in the VR visor itself, which operates as a standalone device. However, applications that entail multi-party interactions will likely require the scene to be processed by an external server and then streamed to the visors. However, the stringent Quality of Service (QoS) constraints imposed by the VR's interactive nature require Network Slicing (NS) solutions, for which profiling the traffic generated by the VR application is crucial. To this end, we collected more than 4 hours of traces in a real setup and analyzed their temporal correlation, focusing on the Constant Bit Rate (CBR) encoding mode, which should generate more predictable traffic streams. From the collected data, we then distilled two prediction models for future frame size, which can be instrumental in the design of dynamic resource allocation algorithms. Our results show that even the state-of-the-art H.264 CBR mode may have significant frame size fluctuations, impacting NS optimization. We then exploited the models to dynamically determine requirements in an NS scenario, providing the required QoS while minimizing resource usage.}
}


@article{DBLP:journals/tmc/LiuSMLY24,
	author = {Yu Liu and
                  Xiaojun Shang and
                  Yingling Mao and
                  Zhenhua Liu and
                  Yuanyuan Yang},
	title = {Availability Aware Online Virtual Network Function Backup in Edge
                  Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3909--3922},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282156},
	doi = {10.1109/TMC.2023.3282156},
	timestamp = {Sat, 27 Jul 2024 13:40:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuSMLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of edge computing and network function virtualization, it is promising to provide flexible and low-latency network services at the edge. However, due to the vulnerability of edge services and the volatility of edge computing system states, i.e., service request rates, failure rates, and resource prices, it is challenging to minimize the online service cost while providing the availability guarantee. This article considers the problem of online virtual network function backup under availability constraints (OVBAC) for cost minimization in edge environments. We formulate the problem based on the characteristics of the volatility system states derived from real-world data and show the hardness of the formulated problem. We use an online backup deployment scheme named Drift-Plus-Penalty (DPP) with provable near-optimal performance for the OVBAC problem. In particular, DPP needs to solve an integer programming problem at the beginning of each time slot. We propose a dynamic programming-based algorithm that can optimally solve the problem in pseudo-polynomial time. Extensive real-world data-driven simulations demonstrate that DPP significantly outperforms popular baselines used in practice.}
}


@article{DBLP:journals/tmc/ZhouTW24,
	author = {Xiaochen Zhou and
                  Yuchuan Tian and
                  Xudong Wang},
	title = {{MEC-DA:} Memory-Efficient Collaborative Domain Adaptation for Mobile
                  Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3923--3937},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282941},
	doi = {10.1109/TMC.2023.3282941},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is prevalent for a mobile edge device to conduct local inference using a compact machine learning model, which achieves lower latency and less compromise of data privacy as compared to cloud-based inference. To work in a new environment, the compact model needs to be adapted to the target data from the environment so as to maintain a high inference accuracy. However, directly applying domain adaptation to the compact model leads to a low inference accuracy. Hence, a scheme called memory-efficient collaborative domain adaptation (MEC-DA) is developed in this paper to boost the compact model's inference accuracy on the target data while preserving data privacy. It first deploys a large model to the mobile edge devices where domain adaptation is conducted to adapt the large model to the target data. This process requires training of the large model, which causes high memory consumption. A new method called lite residual hypothesis transfer (LRHT) is thus designed to achieve memory-efficient domain adaptation. The knowledge of the large model is then transferred to the compact model via knowledge distillation. To prevent the compact model from forgetting the knowledge of the source data, a collaborative knowledge distillation (Co-KD) method is developed to unify the source data on the server and the target data on an edge device to update the compact model. MEC-DA can protect data privacy and handle participant mobility properly via secure aggregation and user selection, respectively. Extensive experiments on several tasks of object recognition show that MEC-DA improves the inference accuracy by up to 12.5%, as compared to the state-of-the-art schemes.}
}


@article{DBLP:journals/tmc/LiaoMZZX24,
	author = {Yuying Liao and
                  Le Ma and
                  Bin Zhou and
                  Xuechen Zhao and
                  Feng Xie},
	title = {DraftFed: {A} Draft-Based Personalized Federated Learning Approach
                  for Heterogeneous Convolutional Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3938--3949},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283557},
	doi = {10.1109/TMC.2023.3283557},
	timestamp = {Fri, 16 Aug 2024 08:05:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiaoMZZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In conventional federated learning, each device is restricted to train a network model of a same structure. This greatly hinders the application of federated learning in edge devices and IoT scenarios where the data and devices are quite heterogeneous because of their different hardware equipment and communication networks. At the same time, most of the existing studies about federated learning of heterogeneous models are limited to horizontal heterogeneity which share a highly homogeneous vertical structure. Little work has been done on vertical heterogeneity such as models with different number of functional layers or different connection methods within them, not to mention the integrated heterogeneity scenarios. In DraftFed, a novel draft-based approach is proposed to implement personalized federated learning for integrated heterogeneous models. Unlike traditional federated learning in which the parameters/gradients are exchanged, DraftFed uses drafts as key knowledge to guide mutual learning of models, which makes it suitable for model structure personalization application scenarios.}
}


@article{DBLP:journals/tmc/OuyangYGWZ24,
	author = {Qiaolin Ouyang and
                  Neng Ye and
                  Jie Gao and
                  Aihua Wang and
                  Lian Zhao},
	title = {Joint In-Orbit Computation and Communication for Minimizing Download
                  Time From {LEO} Satellites},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3950--3963},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282243},
	doi = {10.1109/TMC.2023.3282243},
	timestamp = {Wed, 15 May 2024 16:13:33 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OuyangYGWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Downloading a large amount of data from a low Earth orbit satellite to a ground station can be challenging due to the limited contact window, dynamic channel quality, solar energy supply, and thermal management without an atmosphere. Considering such dynamics, this article proposes a joint design of in-orbit computation and communication for download time minimization. We combine the non-convex thermal constraints and energy constraints into unified energy budget constraints with upper bound approximation, and computational efficiency is achieved by decomposing the resulting large-scale problem into a non-convex communication sub-problem, a convex computation sub-problem solvable with interior point method and a master problem that optimizes the energy budget allocation between computation and communication. The communication sub-problem is solved with a generalized-benders-decomposition-based algorithm that decouples downlink scheduling and power allocation based on a closed-form solution of optimal dual variables in the power allocation primal problem. And the master problem is solved with ternary search by proving the minimal download time is quasi-convex with respect to the energy budget allocation between computation and communication. Simulation results demonstrate that the proposed solution effectively reduces the download time, especially under strict energy constraints and severe channel variations.}
}


@article{DBLP:journals/tmc/TianZLY24,
	author = {Fengsen Tian and
                  Xinglin Zhang and
                  Junbin Liang and
                  Zheng Yang},
	title = {Bidirectional Service Function Chain Embedding for Interactive Applications
                  in Mobile Edge networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3964--3980},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3282645},
	doi = {10.1109/TMC.2023.3282645},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TianZLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bidirectional service function chain (BSFC) consists of multiple virtual network functions (VNFs). Through VNF deployment and link mapping, BSFCs can be embedded into resource-constrained mobile edge networks to provide low-latency network function services to users participating in interactive applications such as multi-player online games. Data from these users are routed through BSFCs to the edge node where the application is located for interaction and then returned to the users through the BSFCs, thus enabling synchronization among multiple users. However, the edge nodes or links have limited computing or bandwidth resources to serve only a fraction of users simultaneously. Therefore, the embedding decisions among different users can affect each other. In this paper, we propose a novel BSFC embedding strategy for interactive applications with the goal of minimizing computing and bandwidth resources while satisfying users’ latency requirements. We first model the BSFC embedding problem as an integer nonlinear programming problem. Then, by closely examining the complexity of the problem, we propose a distributed algorithm based on game theory. We theoretically analyze the properties of the proposed algorithm and show that it can obtain a solution with a worst-case performance bound. Finally, extensive experiments show that the proposed algorithm outperforms several existing algorithms.}
}


@article{DBLP:journals/tmc/FaragSG24,
	author = {Hossam M. Farag and
                  Cedomir Stefanovic and
                  Mikael Gidlund},
	title = {Distributed Backlog-Aware Protocol for Heterogeneous {D2D} Communication-Assisted
                  Wireless Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3981--3992},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283064},
	doi = {10.1109/TMC.2023.3283064},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FaragSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information (AoI) and delay are crucial performance metrics for Industrial Internet of Things (IIoT) applications not only to perform seamless actuation and control actions but also to enable self-organized and re-configurable manufacturing systems. A challenging task in heterogeneous IIoT networks is to minimize the AoI while maintaining a predefined delay constraint. In this work, we consider a Device-to-Device (D2D)-based heterogeneous IIoT network that supports two types of traffic flows, namely AoI-sensitive flow and delay-sensitive flow. First, we introduce a distributed backlog-aware random access protocol that allows the AoI-sensitive nodes to opportunistically access the channel based on the queue occupancy of the delay-sensitive node. Then, we develop an analytical framework to evaluate the average delay and the average AoI, and formulate an optimization problem to minimize the AoI under a given delay constraint. Finally, we provide numerical results to demonstrate the impact of different network parameters on the performance in terms of the average delay and the average AoI. We also give numerical solutions of the optimal parameters that minimize the AoI subject to a defined delay constraint.}
}


@article{DBLP:journals/tmc/ChenWHCH24,
	author = {Wei{-}Yu Chen and
                  Chih{-}Yu Wang and
                  Ren{-}Hung Hwang and
                  Wen{-}Tsuen Chen and
                  Sin{-}Yu Huang},
	title = {Impact of Hardware Impairment on the Joint Reconfigurable Intelligent
                  Surface and Robust Transceiver Design in {MU-MIMO} System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {3993--4008},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3285239},
	doi = {10.1109/TMC.2023.3285239},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWHCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) is a revolutionary passive radio technique to facilitate capacity enhancement beyond the current massive multiple-input multiple-output (MIMO) transmission. However, the potential hardware impairment (HWI) of the RIS usually causes inevitable performance degradation and the amplification of imperfect CSI. These impacts still lack full investigation in the RIS-assisted wireless network. This paper developed a robust joint RIS and transceiver design algorithm to minimize the worst-case mean square error (MSE) of the received signal under the HWI effect and imperfect channel state information (CSI) in the RIS-assisted multi-user MIMO (MU-MIMO) wireless network. Specifically, since the proposed robust joint RIS and transceiver design problem yields non-convex characteristics under severe HWI, an iterative three-step convex algorithm is developed to approach the optimality by relaxation and convex transformation. Compared with the state-of-the-art baselines that ignore the HWI, the proposed robust algorithm inhibits the destruction of HWI while raising the worst-case MSE effectively in several numerical simulations. Moreover, due to the properties of the HWI, the performance loss is notable under the magnification of the number of reflected elements in the RIS-assisted MU-MIMO wireless network.}
}


@article{DBLP:journals/tmc/ShenTWB24,
	author = {Hang Shen and
                  Yibo Tian and
                  Tianjing Wang and
                  Guangwei Bai},
	title = {Slicing-Based Task Offloading in Space-Air-Ground Integrated Vehicular
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4009--4024},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3283852},
	doi = {10.1109/TMC.2023.3283852},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenTWB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A slicing-based collaborative task offloading framework for space-air-ground integrated vehicular networks is proposed in this study, which can provide differentiated quality-of-service (QoS) guarantees for task offloading for high-speed vehicles while maximizing the number of completed tasks. A service-oriented radio access network (RAN) slicing framework is presented that supports slicing window adaptation, spectrum and computing resource orchestration, and collaboration among heterogeneous base stations. Based on the queuing model, the collaborative decision-making of RAN slicing and task offloading is modeled as a problem of maximizing the number of long-term task completions, which consists of three subproblems-slicing window division, resource slicing, and task scheduling-which are solved by a multi-access edge computing (MEC)-enabled controller, forming a closed loop with the slicing window as the period. When a new slicing window arrives, the controller determines its duration according to task traffic fluctuations and allocates resources to RAN slices through an optimization method. A double deep Q-learning network (DDQN)-based algorithm is developed for scheduling workflow on small time scales within a slicing window. Simulation results demonstrate that the proposed scheme performs better than existing approaches in terms of adaptability, task completion rate, and control overhead.}
}


@article{DBLP:journals/tmc/CongZZM24,
	author = {Rong Cong and
                  Zhiwei Zhao and
                  Linyuanqi Zhang and
                  Geyong Min},
	title = {ParallEdge: Exploiting Computing-Mobility Parallelism for Efficient
                  5G/6G Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4025--4037},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3284217},
	doi = {10.1109/TMC.2023.3284217},
	timestamp = {Sat, 04 May 2024 10:56:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CongZZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the emergence of the 5G/6G communications, edge computing has attracted increasing research interests in recent years. To provide pervasive 5G/6G edge computing services, numerous edge servers are required for service coverage, and the deployment cost can be >\n10000 times larger than the deployment cost of the 4 G infrastructure. To address this fundamental limit, we propose ParallEdge, a deployment scheme that employs mobile edge servers for cost-effective service coverage. ParallEdge is designed based on the observation that the processing delay and server moving delay become comparable in many computing-intensive applications in the 5G/6G edge computing. Unlike the traditional “move-then-process” frameworks, ParallEdge follows a “move-while-processing” paradigm, which exploits the parallelism between task processing and server movement to enable resource sharing among more users. Moreover, with the joint optimization of path planning and task scheduling for multiple mobile servers, the deployment cost for service coverage can be further reduced. We analyze the approximation gap of the proposed algorithm, and conduct extensive simulation experiments based on the real-world application data, and the results show that ParallEdge can significantly reduce deployment cost and improve resource utilization compared to the state-of-the-art schemes.}
}


@article{DBLP:journals/tmc/PanahiP24,
	author = {Farzad H. Panahi and
                  Fereidoun H. Panahi},
	title = {Reliable and Energy-Efficient {UAV} Communications: {A} Cost-Aware
                  Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4038--4049},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3284531},
	doi = {10.1109/TMC.2023.3284531},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PanahiP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are expected to play an important role in future wireless networks, serving as communication relays, computing servers, and flying infrastructure for ground users when ground-based infrastructure is congested or inaccessible. However, typical UAVs are powered by on-board batteries, which results in limited battery lifetime and poses a major restriction for UAV applications in communications. To overcome this, we propose a consistent and cost-aware energy procurement framework for a UAV powered concurrently by laser beams, emitted from locally deployed laser beam directors, and local renewable energy (RE) sources. The UAV intends to lower its overall energy cost for a certain operation cycle by optimizing the quantities of energy obtained from its battery as well as laser beams at each time period. Given the optimization results, we also propose a cost-aware UAV placement strategy with the ultimate goal of ensuring quality communication-energy links for the UAV, ground devices (GDs) and users. In addition, we assess the amount of additional procured RE that can be transferred via wireless power transfer to charge a set of distributed GDs. The simulations provide interesting insights into the efficiency of the proposed framework.}
}


@article{DBLP:journals/tmc/ZhaoCXP24,
	author = {Zheyu Zhao and
                  Hao Cheng and
                  Xiaohua Xu and
                  Yi Pan},
	title = {Graph Partition and Multiple Choice-UCB Based Algorithms for Edge
                  Server Placement in {MEC} Environment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4050--4061},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3284994},
	doi = {10.1109/TMC.2023.3284994},
	timestamp = {Tue, 05 Nov 2024 13:40:44 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoCXP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of edge servers make a significant impact on the service quality of a Mobile Edge Computing (MEC) system. This service quality relies on solving two key sub-problems: 1) interference management between servers 2) the placement of MEC servers. To improve the Quality of Service (QoS), we propose a method based on Graph Partition (GP) and Upper Confidence Bound (UCB) for solving these two sub-problems. Regarding interference management, we use an undirected graph to represent the interference between MEC servers so that the overall graph can be divided into multiple subsets of non-interfering MEC servers. Regarding server placement, we propose a Multiple Choice-Upper Confidence Bound (MC-UCB) algorithm that place an collection of interference aware edge servers in each selection. To evaluate the performance, we define a user's QoS function based on transmission delay, throughput, and user density comprehensively and compared with Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) from previous work. The simulation results show that the performance of the proposed algorithms is improved by more than 4% compared with the GA algorithm and 6% compared with the PSO algorithm.}
}


@article{DBLP:journals/tmc/LiuMLY24,
	author = {Yu Liu and
                  Yingling Mao and
                  Zhenhua Liu and
                  Yuanyuan Yang},
	title = {Deep Learning-Assisted Online Task Offloading for Latency Minimization
                  in Heterogeneous Mobile Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4062--4075},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3285882},
	doi = {10.1109/TMC.2023.3285882},
	timestamp = {Mon, 22 Jul 2024 08:26:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuMLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of smart devices in recent years, many applications requiring high computing capability and low latency have emerged. Edge computing is one of the promising paradigms to support such applications. Due to the high volatility of edge environments, e.g., frequent movements of mobile devices, varying task sizes, and time-variant channel conditions, we have to make the offloading and resource management decisions on the fly. This paper formulates and studies the problem of online task offloading and resource management in heterogeneous mobile edge environments. The goal of the problem is to minimize the overall system latency. We prove that the problem is NP-hard. Moreover, traditional algorithms needing long decision-making times are insufficient to support applications with high volatility. This paper proposes a deep learning-assisted online algorithm that can make fast decisions. In particular, we design an offline solver for the proposed problem and use a deep neural network to emulate the solver. We conduct extensive simulations to evaluate the proposed approach. Results show that the proposed approach is around 50,000\\times and 500\\times faster than the commercial Gurobi solver for the optimal solution and the proposed offline approximation solver, respectively. Moreover, the overall latency under the proposed approach is near-optimal.}
}


@article{DBLP:journals/tmc/WuFWQXYW24,
	author = {Tao Wu and
                  Xiaochen Fan and
                  Hao Wei and
                  Yuben Qu and
                  Chaocan Xiang and
                  Panlong Yang and
                  Fan Wu},
	title = {Predictive Service Provisioning With Online Learning in Wireless Edge
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4076--4091},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3286847},
	doi = {10.1109/TMC.2023.3286847},
	timestamp = {Sat, 04 May 2024 10:56:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuFWQXYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) technology can be implemented at cellular base stations, enabling flexible and configurable provisions of services for mobile users to access. Nevertheless, the conventional solutions mainly focus on statical service provisioning, which ignores the dynamic nature of the arriving service requests. In this work, we first conduct comprehensive data-driven observations on over 4 million service requests throughout 9,800 base stations. Our key findings suggest that users’ demands intrinsically exhibit spatial and temporal patterns, which inevitably lead to performance degradation in statical service provisioning. Motivated by that, we design and implement MobiEdge, a predictive service provisioning system with online learning in wireless edge networks. We propose a graph embedding learning-based model for representation learning, thus to achieve accurate request prediction at different base stations. Then, based on the prediction of incoming service requests, we study the service provisioning reconfiguration problem, i.e., how to jointly optimize service placement and corresponding request scheduling across dual timescales, under constraints of network resources and the total budget. By leveraging the submodular technique, we transform the research issue into a submodular function maximization problem under the q-independence system constraint, where q is a positive constant related to the ratio of coefficients in constraint conditions. On this basis, we propose a 1/(1+q) approximation algorithm with rigorous theoretical analysis on the bounded maximum utility. Extensive trace-driven evaluations are conducted over networks of different scales, and MobiEdge shows remarkable performance enhancements by achieving the accuracy of up to 98% in service prediction and an average utility of 92.9% to the optimal solution in service provisioning.}
}


@article{DBLP:journals/tmc/XiangBCLWSW24,
	author = {Tianao Xiang and
                  Yuanguo Bi and
                  Xiangyi Chen and
                  Yuan Liu and
                  Boyang Wang and
                  Xuemin Shen and
                  Xingwei Wang},
	title = {Federated Learning With Dynamic Epoch Adjustment and Collaborative
                  Training in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4092--4106},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288392},
	doi = {10.1109/TMC.2023.3288392},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiangBCLWSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a distributed learning paradigm, federated learning (FL) can be applied in mobile edge computing (MEC) to support real-time artificial intelligence by leveraging edge computation resources while preserving data privacy in the end devices. However, the unpredictable wireless connections between end devices and edge servers in MEC (e.g., frequent handovers and unstable wireless channels) may result in the loss of important model parameters, which slows down the FL training process and degrades the quality of the global model. In this paper, we propose an adaptive collaborative federated learning (ACFL) scheme to accelerate the convergence and improve model reliability by mitigating communication-based parameter loss under a three-layer MEC architecture. First, a dynamic epoch adjustment method is proposed to reduce communication rounds by dynamically adjusting the training epochs in end devices. In addition, to accelerate the FL convergence, we present an edge server collaborative training scheme by leveraging a multi-layer computing architecture, where edge servers utilize their maintained data to collaboratively train models with end devices. Finally, extensive simulations are conducted and show that ACFL can efficiently improve model reliability and accelerate the convergence of the FL process in MEC.}
}


@article{DBLP:journals/tmc/XuXZWZZ24,
	author = {Yin Xu and
                  Mingjun Xiao and
                  Yu Zhu and
                  Jie Wu and
                  Sheng Zhang and
                  Jinrui Zhou},
	title = {AoI-Guaranteed Incentive Mechanism for Mobile Crowdsensing With Freshness
                  Concerns},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4107--4125},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3285779},
	doi = {10.1109/TMC.2023.3285779},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuXZWZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive spread of smart mobile devices, Mobile CrowdSensing (MCS) has been becoming a promising paradigm, by which a platform can coordinate a group of workers to complete large-scale data collection tasks using their mobile devices. In this paper, we investigate the incentive mechanism design in MCS systems, taking the freshness of collected data and social benefits into consideration. First, the Age of Information (AoI) metric is introduced to measure the freshness of data. Then, we model the incentive mechanism design with AoI guarantees as an incomplete information two-stage Stackelberg game with multiple constraints. Next, we consider the scenario that all participants share the public utility function parameters of the Stackelberg game. By deriving the optimal remuneration paid by the platform and the optimal data update frequency for each worker, and proving the existence of a unique Stackelberg equilibrium, we propose an AoI-guaranteed Incentive Mechanism (AIM) that enables the platform and all workers to maximize their utilities simultaneously. Furthermore, we extend AIM to a general scenario where each participant has no prior knowledge of the utility function parameters of the game. By resorting to the Deep Reinforcement Learning (DRL) technique and modeling the two-stage Stackelberg game as a Markov decision process, we propose a DRL-based Incentive Mechanism (DIM) with AoI guarantees, which makes each participant effectively seek its optimal strategy through trial and error. Meanwhile, the system can guarantee that the AoI values of all data uploaded to the platform are not larger than a given threshold. Finally, numerical experiments on real-world traces are conducted to validate the efficacy and efficiency of AIM and DIM.}
}


@article{DBLP:journals/tmc/HeWMLSLJWW24,
	author = {Yuan He and
                  Weiguo Wang and
                  Luca Mottola and
                  Shuai Li and
                  Yimiao Sun and
                  Jinming Li and
                  Hua Jing and
                  Ting Wang and
                  Yulei Wang},
	title = {Acoustic Localization System for Precise Drone Landing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4126--4144},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288299},
	doi = {10.1109/TMC.2023.3288299},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeWMLSLJWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present MicNest: an acoustic localization system enabling precise drone landing. In MicNest, multiple microphones are deployed on a landing platform in carefully devised configurations. The drone carries a speaker transmitting purposefully-designed acoustic pulses. The drone may be localized as long as the pulses are correctly detected. Doing so is challenging: i) because of limited transmission power, propagation attenuation, background noise, and propeller interference, the Signal-to-Noise Ratio (SNR) of received pulses is intrinsically low; ii) the pulses experience non-linear Doppler distortion due to the physical drone dynamics; iii) as location information is used during landing, the processing latency must be reduced to effectively feed the flight control loop. To tackle these issues, we design a novel pulse detector, Matched Filter Tree (MFT), whose idea is to convert pulse detection to a tree search problem. We further present three practical methods to accelerate tree search jointly. Our experiments show that MicNest can localize a drone 120 m away with 0.53% relative localization error at 20 Hz location update frequency. For navigating drone landing, MicNest can achieve a success rate of 94%. The average landing error (distance between landing point and target point) is only 4.3 cm.}
}


@article{DBLP:journals/tmc/ChuHNPDNS24,
	author = {Nam Hoai Chu and
                  Dinh Thai Hoang and
                  Diep N. Nguyen and
                  Khoa Tran Phan and
                  Eryk Dutkiewicz and
                  Dusit Niyato and
                  Tao Shu},
	title = {MetaSlicing: {A} Novel Resource Allocation Framework for Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4145--4162},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288085},
	doi = {10.1109/TMC.2023.3288085},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChuHNPDNS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Creating and maintaining the Metaverse requires enormous resources that have never been seen before, especially computing resources for intensive data processing to support the Extended Reality, enormous storage resources, and massive networking resources for maintaining ultra high-speed and low-latency connections. Therefore, this work aims to propose a novel framework, namely MetaSlicing, that can provide a highly effective and comprehensive solution in managing and allocating different types of resources for Metaverse applications. In particular, by observing that Metaverse applications may have common functions, we first propose grouping applications into clusters, called MetaInstances. In a MetaInstance, common functions can be shared among applications. As such, the same resources can be used by multiple applications simultaneously, thereby enhancing resource utilization dramatically. To address the real-time characteristic and resource demand's dynamic and uncertainty in the Metaverse, we develop an effective framework based on the semi-Markov decision process and propose an intelligent admission control algorithm that can maximize resource utilization and enhance the Quality-of-Service for end-users. Extensive simulation results show that our proposed solution outperforms the Greedy-based policies by up to 80% and 47% in terms of long-term revenue for Metaverse providers and request acceptance probability, respectively.}
}


@article{DBLP:journals/tmc/WangWZZX24,
	author = {Yingqi Wang and
                  Zhong{-}qin Wang and
                  Jian Andrew Zhang and
                  Haimin Zhang and
                  Min Xu},
	title = {Vital Sign Monitoring in Dynamic Environment via mmWave Radar and
                  Camera Fusion},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4163--4180},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288850},
	doi = {10.1109/TMC.2023.3288850},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWZZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contact-free vital sign monitoring, which uses wireless signals for recognizing human vital signs (i.e, breath and heartbeat), is an attractive solution to health and security. However, the subject’s body movement and the change in actual environments can result in inaccurate frequency estimation of heartbeat and respiratory. In this paper, we propose a robust mmWave radar and camera fusion system for monitoring vital signs, which can perform consistently well in dynamic scenarios, e.g., when some people move around the subject to be tracked, or a subject waves his/her arms and marches on the spot. Three major processing modules are developed in the system, to enable robust sensing. First, we utilize a camera to assist a mmWave radar to accurately localize the subjects of interest. Second, we exploit the calculated subject position to form transmitting and receiving beamformers, which can improve the reflected power from the targets and weaken the impact of dynamic interference. Third, we propose a weighted multi-channel Variational Mode Decomposition (WMC-VMD) algorithm to separate the weak vital sign signals from the dynamic ones due to subject’s body movement. Experimental results show that, the 90th percentile errors in respiration rate (RR) and heartbeat rate (HR) are less than 0.5 RPM (respirations per minute) and 6 BPM (beats per minute), respectively.}
}


@article{DBLP:journals/tmc/LiuPJWWPW24,
	author = {Qin Liu and
                  Yu Peng and
                  Hongbo Jiang and
                  Jie Wu and
                  Tian Wang and
                  Tao Peng and
                  Guojun Wang},
	title = {Authorized Keyword Search on Mobile Devices in Secure Data Outsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4181--4195},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288160},
	doi = {10.1109/TMC.2023.3288160},
	timestamp = {Sat, 04 May 2024 10:56:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuPJWWPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing awareness of secure data outsourcing, dynamic searchable symmetric encryption (DSSE) that enables searches and updates over encrypted data has begun to receive growing attention. Despite promising, existing DSSE schemes with forward and backward privacy are still hard to achieve authorized keyword searches on mobile devices while supporting secure and flexible updates. In this article, we propose a DSSE scheme, named \\mathsf{FLY_{++}} based on a flexible index structure \\mathsf{Hybrid} that incorporates the merits of inverted indexes and forward indexes while compacting the index size. Specifically, \\mathsf{FLY_{++}} encrypts the newly added data with a fresh key and disperses previous keys into \\mathsf{Hybrid} for forward privacy, while applying symmetric puncturable encryption (SPE) and a dual-key mechanism to realize backward privacy further. Compared with the state-of-the-art work, \\mathsf{FLY_{++}} has the following advantages: (1) Authorized search. It dispenses with caching or re-encrypting search results, enabling a mobile device to search only designated keywords over the data outsourced before authorization. (2) Flexibility. It not only allows for sublinear search time, but also simultaneously supports fine-grained and coarse-grained updates of outsourced data. The detailed security analysis and extensive experiments conducted on a real dataset demonstrate the security and practicality of \\mathsf{FLY_{++}}, respectively.}
}


@article{DBLP:journals/tmc/JiaZZ24,
	author = {Wenzhen Jia and
                  Shengjie Zhao and
                  Kai Zhao},
	title = {Human Mobility Prediction Based on Trend Iteration of Spectral Clustering},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4196--4211},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288132},
	doi = {10.1109/TMC.2023.3288132},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiaZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human mobility prediction is crucial for epidemic control, urban planning, and traffic forecasting systems. We observe urban traffic flow prediction has a hierarchical structure, in which human mobility prediction should consider not only the spatial and the temporal relationships, but also the high-level mobility trend between individuals and regions. In this paper, we propose a human mobility clustering algorithm based on trend iteration of spectral clustering (TISC) to incorporate the high-level human mobility trend between individuals and regions. We integrate our TISC clustering algorithm with two existing urban traffic flow predictive models: namely, deep spatio-temporal residual network (ST-ResNet) and deep spatio-temporal 3D network (ST-3DNet). By adapting our TISC clustering algorithm, the prediction accuracy of both algorithms has been improved significantly (30.96\\%\nfor ST-ResNet and 24.66\\%\nfor ST-3DNet). We also compare the TISC-based predictive framework with 26 state-of-the-art human mobility prediction algorithms. We observe that our TISC algorithm considerably outperforms all 26 methods, reducing the predictive error from 6.93% to 69.55\\%\n.}
}


@article{DBLP:journals/tmc/ChengLNLXZ24,
	author = {Yanyu Cheng and
                  Jianyuan Lu and
                  Dusit Niyato and
                  Biao Lyu and
                  Minrui Xu and
                  Shunmin Zhu},
	title = {Performance Analysis and Power Allocation for Covert Mobile Edge Computing
                  With RIS-Aided {NOMA}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4212--4227},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3302413},
	doi = {10.1109/TMC.2023.3302413},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChengLNLXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) is a key enabling technology for the sixth-generation (6G) wireless networks. In this paper, we apply covert communications to MEC to prevent information leakage, where two candidate technologies of 6G, reconfigurable intelligent surface (RIS) and non-orthogonal multiple access (NOMA), are adopted. Specifically, a legitimate transmitter sends messages to a pair of legitimate receivers, while a warden aims to detect whether the legitimate transmission exists. We can hide the existence of the stronger-signal receiver's transmission from the warden by exploiting the nature of NOMA, and we use a jammer to further hide this existence. We first analyze the performance for the case of fixed power allocation between the legitimate transmitters and the jammer. The closed-form expressions for the minimum detection error probability and ergodic public/covert rates are derived. Then, we design a reinforcement learning (RL)-based power-allocation optimization algorithm that maximizes the sum rate while ensuring covertness, by optimizing the power allocation between the transmitters and the jammer. Simulation results validate the correctness of our analysis and demonstrate the covertness of the proposed scheme. Furthermore, the performance of the RL-based algorithm is significantly better than that of the baseline scheme, which reflects the effectiveness of our proposed algorithm.}
}


@article{DBLP:journals/tmc/XuLZZLXH24,
	author = {Weiye Xu and
                  Jianwei Liu and
                  Shimin Zhang and
                  Yuanqing Zheng and
                  Feng Lin and
                  Fu Xiao and
                  Jinsong Han},
	title = {Anti-Spoofing Facial Authentication Based on {COTS} {RFID}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4228--4245},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3289708},
	doi = {10.1109/TMC.2023.3289708},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLZZLXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current facial authentication (FA) systems are mostly based on the images of human faces, thus suffering from privacy leakage and spoofing attacks. Mainstream systems utilize facial geometry features for spoofing mitigation, but they are still vulnerable to feature manipulation, e.g., 3D-printed human faces. In this article, we propose a novel privacy-preserving anti-spoofing FA system, named RFace, which extracts both the 3D geometry and inner biomaterial features of faces using a COTS RFID tag array. These features are difficult to obtain and forge, hence are resistant to spoofing attacks. Unlike images, RF signals are not perceptible to human eyes, so RFace protects user's privacy. We build a theoretical model to rigorously prove the feasibility of feature acquisition and the correlation between facial features and RF signals. To enhance the security of RFace, we specify the tag reading order for each authentication to defend against the signal replay attack. For practicality, we design an effective algorithm to mitigate the impact of unstable distance and angle deflection from the face to the array. Extensive experiments with 30 participants and three types of spoofing attacks show that RFace achieves an average authentication success rate of over 95.7\\% and an EER of 4.4\\%. More importantly, no replay attack or spoofing attack succeeds in deceiving RFace in the experiments.}
}


@article{DBLP:journals/tmc/FengCXWZG24,
	author = {Yunyun Feng and
                  Si Chen and
                  Wei Xi and
                  Shuai Wang and
                  Jia Zhao and
                  Wei Gong},
	title = {Heartbeating With {LTE} Networks for Ambient Backscatter},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4246--4258},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3290298},
	doi = {10.1109/TMC.2023.3290298},
	timestamp = {Mon, 10 Jun 2024 20:41:09 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FengCXWZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Different from intermittent ISM signals like Bluetooth and WiFi, LTE signals are continuous in time and more pervasive in space, which makes them suitable carriers for ambient backscatter systems. However, due to the continuous LTE traffic and complex frame structures, existing ambient backscatter systems, such as HitchHike and LScatter, cannot reliably backscatter LTE signals in a standard-compatible way. We observe that the primary cause of their failures is that tags cannot accurately synchronize with LTE excitations. To address this issue, we propose SyncLTE, an LTE backscatter system that achieves high-accuracy synchronization and standard-compatible backscatter communication. The key novelty is a new tag design that uses the periodicity of LTE signals for synchronization and provides a customized single-symbol modulation scheme for LTE carriers. Our design is prototyped using FPGAs, SDR LTE eNBs and UEs. Comprehensive experiments have been done in various scenarios, including LoS, NLoS, indoor and outdoor. Results show that SyncLTE is 22.4x and 7.4x better than LScatter and Multiscatter in terms of the 80th percentiles of synchronization errors. Also, SyncLTE can deliver throughputs of up to 200 bps using BPSK and 400 bps using QPSK while other systems suffer from failures.}
}


@article{DBLP:journals/tmc/ZhaoZWHAMZ24,
	author = {Liang Zhao and
                  Enchao Zhang and
                  Shaohua Wan and
                  Ammar Hawbani and
                  Ahmed Yassin Al{-}Dubai and
                  Geyong Min and
                  Albert Y. Zomaya},
	title = {{MESON:} {A} Mobility-Aware Dependent Task Offloading Scheme for Urban
                  Vehicular Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4259--4272},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3289611},
	doi = {10.1109/TMC.2023.3289611},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZWHAMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular Edge Computing (VEC) is the transportation version of Mobile Edge Computing (MEC) in road scenarios. One key technology of VEC is task offloading, which allows vehicles to send their computation tasks to the surrounding Roadside Units (RSUs) or other vehicles for execution, thereby reducing computation delay and energy consumption. However, the existing task offloading schemes still have various gaps and face challenges that should be addressed because vehicles with time-varying trajectories need to process massive data with high complexity and diversity. In this paper, a VEC-based computation offloading model is developed with consideration of data dependency of tasks. The minimization of the average response time and average energy consumption of the system is defined as a combinatorial optimization problem. To solve this problem, we propose a Mobility-aware dependent task offloading (MESON) Scheme for urban VEC and develop a DRL-based algorithm to train the offloading strategy. To improve the training efficiency, a vehicle mobility detection algorithm is further designed to detect the communication time between vehicles and RSUs. In this way, MESON can avoid unreasonable decisions by lowering the size of the action space. Moreover, to improve the system stability and the offloading successful rate, we design a task priority determination scheme to prioritize the tasks in the waiting queue. The experimental results show that MESON is superior compared to other task offloading schemes in terms of the average response time, average system energy consumption, and offloading successful rate.}
}


@article{DBLP:journals/tmc/PerezValeroBSOGC24,
	author = {Jes{\'{u}}s P{\'{e}}rez{-}Valero and
                  Albert Banchs and
                  Pablo Serrano and
                  Jorge Ort{\'{\i}}n and
                  Jaime Garc{\'{\i}}a{-}Reinoso and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {Energy-Aware Adaptive Scaling of Server Farms for {NFV} With Reliability
                  Requirements},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4273--4284},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288604},
	doi = {10.1109/TMC.2023.3288604},
	timestamp = {Sat, 27 Apr 2024 21:29:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PerezValeroBSOGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Auto-scaling techniques aim to keep the right number of active servers for the current load: if this number is too small we risk service disruption, but if it is too large we waste resources. Despite the interest in the efficient operation of this type of systems, no prior work has addressed auto-scaling techniques for Network Function Virtualization (NFV) with stringent reliability requirements such as those envisioned in 5G (5 or 6 nines). To achieve such levels of reliability, we need to account for both the activation delay until servers become available (i.e., the wake-up or activation time) and the fallible nature of servers (which may fail with some probability). In this article, we build on control theory to design an auto-scaling technique for a server farm for NFV that guarantees certain reliability while minimizing the number of active resources. We show that the use of well-established tools from control theory results in convergence times much shorter than those obtained with state-of-the-art reinforcement learning techniques. This shows that, despite the current trend to apply machine learning to all sorts of networking problems, there may be some cases where other techniques (such as control theory) can be more suitable.}
}


@article{DBLP:journals/tmc/LiuZQWCLYLH24,
	author = {Yu Liu and
                  Zirui Zhuang and
                  Qi Qi and
                  Jingyu Wang and
                  Dezhi Chen and
                  Lu Lu and
                  Hongwei Yang and
                  Jianxin Liao and
                  Zhu Han},
	title = {Slice Sandwich: Jagged Slicing Multi-Tier Dynamic Resources for Diversified
                  {V2X} Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4285--4302},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288637},
	doi = {10.1109/TMC.2023.3288637},
	timestamp = {Thu, 01 Aug 2024 08:00:49 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZQWCLYLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancement of intelligent transportation systems, a series of diversified V2X applications come into being, which have different key performance indicators (KPIs) and transmission features. Moreover, multi-tier computing as a new system-level architecture distributes computing and communication capabilities anywhere between the cloud and the end-user. Unfortunately, the existing network paradigm for V2X services adopts a one-shot allocation of resources ignoring the inherent differences of V2X service. To cope with these problems, three types of refined network slices for V2X services are first proposed to simultaneously support heterogeneous service characteristics without excessively splitting resources. Considering the spatiotemporal correlation between service traffic and physical resources, a jagged slicing in multi-tier dynamic resources, which forms a “slice sandwich” brightly, is realized by a dual timescale intelligent resource management scheme. The inter-slice resource configuration is based on neural bandits with upper confidence bounds at each large-time period, while the exclusive resources are managed elastically by deep Q-learning in terms of the real-time changing network state in the small slot. We developed a simulation environment by Simulation of Urban Mobility (SUMO) including real-world road conditions and traffic models. The experiment results demonstrate that the proposed scheme can effectively guarantee KPIs of V2X services and improve the system revenue compared with benchmark algorithms.}
}


@article{DBLP:journals/tmc/XuWLXZH24,
	author = {Xiaobin Xu and
                  Qi Wang and
                  Shuopeng Li and
                  Haitao Xu and
                  Hui Zhao and
                  Zhu Han},
	title = {An Adaptive Dual-Mode Task-Oriented Resource Management Strategy for
                  {GEO} Relay Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4303--4317},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3289030},
	doi = {10.1109/TMC.2023.3289030},
	timestamp = {Sat, 04 May 2024 10:56:40 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuWLXZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fierce global competition on satellite networks, the building of satellite constellations grows explosively. Sharply increasing on-orbit data will face the challenge of satellite-ground data transmission. GEO satellites become the top choice for satellite data relay due to their stable satellite-ground link. Most existing spectrum resource management for GEO relays is equipment-oriented and benefit priority, which may lead to a waste of spectrum resources. In this paper, we propose a real-time task-oriented resource allocation strategy for GEO relay systems. We model the spectrum allocation problem as a distributed non-cooperative Stackelberg game process. We prove that when both sides of the game pursue the maximization of personal revenue, the system will enter a Nash equilibrium state, whereas spectrum resources are not fully used. Based on the maximization of individual utilities (U-prior) and spectrum utilization (S-prior) methods, we design an adaptive dual-mode pricing mode to maximize the spectrum resources within a certain loss of revenue. The simulation results show that the S-prior and U-prior have better performance than the baseline method and existing optimization methods. Our proposed dual-mode strategy is making more throughputs and has less delay with little loss of utility values than that of individual utility maximization.}
}


@article{DBLP:journals/tmc/WeiCZBGH24,
	author = {Lu Wei and
                  Jie Cui and
                  Hong Zhong and
                  Irina Bolodurina and
                  Chengjie Gu and
                  Debiao He},
	title = {A Decentralized Authenticated Key Agreement Scheme Based on Smart
                  Contract for Securing Vehicular Ad-Hoc Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4318--4333},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3288930},
	doi = {10.1109/TMC.2023.3288930},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiCZBGH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the communication channels in vehicular ad-hoc networks (VANETs) are wireless and open, malicious adversaries can monitor or fabricate messages transmitted across them. To secure vehicular communications, an authenticated key agreement (AKA) scheme needs to be designed for VNAETs. Traditional VANETs AKA schemes require the trusted authority (TA) to authenticate the legality of message and corresponding sender. However, the TA in these schemes is vulnerable to suffer from single-point-of-failure issues. Some blockchain-based VANETs AKA schemes have been proposed recently to address the deficiency. However, these schemes rely on the consortium or private blockchain in which TAs are still required for key generation, resulting that the practicality is limited. To solve the issue, we design a smart contract-based VANETs AKA scheme, where the AKA algorithm of our proposed scheme is implemented on smart contract deployed on a public blockchain system and the TA that is responsible for key generation will not be required. The security proof and analysis show that our proposed scheme satisfies the session-key semantic security and essential security and privacy requirements, respectively. The performance analysis demonstrates that our proposed scheme outperforms existing blockchain-based VANETs AKA schemes.}
}


@article{DBLP:journals/tmc/YanCZDSSS24,
	author = {Xiaoyong Yan and
                  Jiannong Cao and
                  Jian Zhou and
                  Chuntao Ding and
                  Han Sun and
                  Lijuan Sun and
                  Aiguo Song},
	title = {{DCP-AHS:} {A} High-Performance Distributed Cooperative Positioning
                  Model for Concave Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4334--4347},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3291130},
	doi = {10.1109/TMC.2023.3291130},
	timestamp = {Sat, 27 Apr 2024 21:29:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanCZDSSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node positioning is an essential function of wireless networks and serves as the foundation for many applications. In the existing works, the cooperative positioning approaches have been extensively studied and are shown to be effective for scenarios with energy and cost constraints. However, these approaches may not perform well in concave networks with holes or obstacles. To address this issue, this paper proposes a distributed cooperative positioning model with adaptive hop-range selection (DCP-AHS for short) for concave networks. DCP-AHS first uses a low-complexity and fast convergent distance estimation method based on the local neighbor nodes. It then uses an adaptive hop-range selection method based on the residual analysis between pairs of anchors. Within the hop range, an unknown node uses multi-lateration with the optimal weight function to determine its estimated position. Finally, a weighted Bounding-Box method with the virtual anchor is employed to avoid significant position estimation errors caused by the collinearity issues. Simulation results demonstrated that the proposed DCP-AHS significantly outperformed the existing algorithms regarding efficiency, accuracy, and stability in various concave networks. Specifically, our proposed model achieved a median improvement of 16.62% to 81.65% in positioning accuracy compared to the comparison algorithms.}
}


@article{DBLP:journals/tmc/NguyenHNXND24,
	author = {Cong Thanh Nguyen and
                  Dinh Thai Hoang and
                  Diep N. Nguyen and
                  Yong Xiao and
                  Dusit Niyato and
                  Eryk Dutkiewicz},
	title = {MetaShard: {A} Novel Sharding Blockchain Platform for Metaverse Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4348--4361},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3290955},
	doi = {10.1109/TMC.2023.3290955},
	timestamp = {Wed, 02 Oct 2024 21:58:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NguyenHNXND24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to its security, transparency, and flexibility in verifying virtual assets, blockchain has been identified as one of the key technologies for Metaverse. Unfortunately, blockchain-based Metaverse faces serious challenges such as massive resource demands, scalability, and security/privacy concerns. To address these issues, this paper proposes a novel sharding-based blockchain framework, namely MetaShard, for Metaverse applications. Particularly, we first develop an effective consensus mechanism, namely Proof-of-Engagement, that can incentivize MUs’ data and computing resource contribution. Moreover, to improve the scalability of MetaShard, we propose an innovative sharding management scheme to maximize the network’s throughput while protecting the shards from 51% attacks. Since the optimization problem is NP-complete, we develop a hybrid approach that decomposes the problem (using the binary search method) into sub-problems that can be solved effectively by the Lagrangian method. As a result, the proposed approach can obtain solutions in polynomial time, thereby enabling flexible shard reconfiguration and reducing the risk of corruption from the adversary. Extensive numerical experiments show that, compared to the state-of-the-art commercial solvers, our proposed approach can achieve up to 66.6% higher throughput in less than 1/30 running time. Moreover, the proposed approach can achieve global optimal solutions in most experiments.}
}


@article{DBLP:journals/tmc/ChenWH24,
	author = {Weiwei Chen and
                  Shuai Wang and
                  Tian He},
	title = {Magnifier: Leveraging the Fine-Grained Hardware Information and Their
                  Temporal Patterns for Concurrent LoRa Decoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4362--4375},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3292279},
	doi = {10.1109/TMC.2023.3292279},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa Wide Area Network (LoRaWAN) is famous for its low power consumption and wide coverage area. A critical issue that constrains the scalability of LoRaWAN is how to support concurrent packet reception efficiently. To this end, our work first carefully investigates all the hardware imperfections between a LoRa transceiver pair and studies their impacts on the received signal's temporal patterns with fine granularity. Such imperfections and the associated temporal patterns are intrinsic and unique for any transceiver pair in a wireless system. Therefore, they provide a brand-new and highly influential perspective for identifying different packets. Motivated by this, we propose Magnifier, which integrates the two factors mentioned above for LoRa concurrent decoding. Though some prior work leverage partial hardware imperfections, they fail to systematically study and exploit the temporal patterns caused by the imperfections for concurrent packet reception. We also evaluate the performance of Magnifier with 40 transmitters (commodity LoRa chips) and one receiver (USRP B210). Both critical complexity analysis and extensive experiments show that with the same hardware settings and the same scale of complexity, Magnifier can support 3× to 8× throughput of other existing schemes.}
}


@article{DBLP:journals/tmc/HouWZLLHL24,
	author = {Wenjing Hou and
                  Hong Wen and
                  Ning Zhang and
                  Wenxin Lei and
                  Haojie Lin and
                  Zhu Han and
                  Qiang Liu},
	title = {Adaptive Training and Aggregation for Federated Learning in Multi-Tier
                  Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4376--4388},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3289940},
	doi = {10.1109/TMC.2023.3289940},
	timestamp = {Wed, 17 Apr 2024 17:17:09 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HouWZLLHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-tier computing (MC) utilizes computing resources from the cloud, fog, edge, and end layers to promote intelligent Internet of Things (IoT) applications. Federated learning (FL) in MC offers a prospective distributed and privacy-preserving framework to deploy deep learning applications in different layers. Due to time-varying network topologies, wireless channel states, and computational workloads, MC faces a dynamic and uncertain environment, which poses additional challenges to FL task processing. In this paper, we propose a novel adaptive training and aggregation federated learning (ATAFL) framework. Specifically, local model training can be performed at end devices, edge nodes, and fog nodes. The global aggregator can choose from the edge, fog, and cloud layers. A joint optimization problem of training, aggregation node selection, and resource allocation is further formulated to minimize system latency and energy consumption. Moreover, digital twin and deep reinforcement learning (DRL) techniques are integrated into the MC network to design optimal node selection and resource allocation strategies based on captured state information of the MC system. We implement a prototype, and experimental results show that our proposed DRL-based algorithm reduces system latency and energy consumption compared with the benchmark algorithms.}
}


@article{DBLP:journals/tmc/TianWXPWLH24,
	author = {Bingxin Tian and
                  Li Wang and
                  Lianming Xu and
                  Wen Pan and
                  Huaqing Wu and
                  Liang Li and
                  Zhu Han},
	title = {UAV-Assisted Wireless Cooperative Communication and Coded Caching:
                  {A} Multiagent Two-Timescale {DRL} Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4389--4404},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298641},
	doi = {10.1109/TMC.2023.3298641},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TianWXPWLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In emergency scenarios, strong mobility and serious interference cause unstable transmission of on-site information such as close-up photos and high resolution videos, which requires a robust temporary communication network. In this paper, we focus on a UAV-assisted wireless cooperative communication and coded caching network, where emergency command vehicles and a UAV serve as content providers (CPs) to cache and transmit coded fragments or complete files for rescuers regarded as content requesters (CRs). The delivery success probability and content hit ratio are theoretically derived by incorporating the physical connectivity and social relationship between CPs and CRs. Aiming at maximizing the overall content hit ratio, we propose a multiagent two-timescale deep reinforcement learning (MA2T-DRL) algorithm to jointly optimize the transmission power and caching strategies for CPs. Specifically, we develop a two tier deep-Q networks (DQNs) framework integrating a slow-timescale DQN (ST-DQN) and a fast-timescale DQN (FT-DQN) for caching decision-making and power decision-making respectively, and then the QMIX framework is leveraged to aggregate all the outputs from local ST-DQNs. Considering the cooperative characteristics of coded caching, we further propose a novel clustering method for CPs such that CPs in the same cluster have the same willingness to serve CRs, and each cluster is regarded as the agent for training which further reduces the aggregation scale of the mixing network. Simulation results show that the proposed MA2T-DRL algorithm is efficient in model training, and presents the advantages in performance and complexity compared with the single-agent centralized training and the multiagent independent distributed training.}
}


@article{DBLP:journals/tmc/RenGCQ24,
	author = {Yinlin Ren and
                  Shaoyong Guo and
                  Bin Cao and
                  Xuesong Qiu},
	title = {End-to-End Network {SLA} Quality Assurance for {C-RAN:} {A} Closed-Loop
                  Management Method Based on Digital Twin Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4405--4422},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3291012},
	doi = {10.1109/TMC.2023.3291012},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenGCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enable intelligent and low-cost End-to-End (E2E) network service deployment and Service Level Agreement (SLA) quality management in the two-level Cloud Radio Access Network (C-RAN), this paper studies a DTN-based SLA quality closed-loop management scheme, which mainly includes acquisition module, base module, deployment module, and monitoring module. The deployment module is responsible for constructing the service deployment optimization model with the goal of minimizing the average E2E delay of packets, and quickly obtain deployment decisions through a Weighted GraphSAGE (WGraphSAGE)-assisted Double Deep Q-network (DDQN)-based two-stage service deployment (WDTSD) algorithm. The monitoring module uses the state monitoring model based on Bayesian Convolutional Neural Network (BCNN) to complete the abnormal detection of physical devices. The modular closed-loop interaction provides a virtual environment for network service deployment, verification, monitoring, and policy revision, achieving SLA quality assurance. Extensive results validate the effectiveness of the WDTSD algorithm, state monitoring model, and DTN. WDTSD outperforms existing solutions in terms of memory overhead, computing speed, E2E delay, and service access ratio. The state monitoring model has better performance in indicators such as accuracy. The results under different data acquisition periods show that the service deployment effect is better when the DTN is closer to the physical network.}
}


@article{DBLP:journals/tmc/GaydamakaSMATK24,
	author = {Anna Gaydamaka and
                  Andrey Samuylov and
                  Dmitri Moltchanov and
                  Mateen Ashraf and
                  Bo Tan and
                  Yevgeni Koucheryavy},
	title = {Dynamic Topology Organization and Maintenance Algorithms for Autonomous
                  {UAV} Swarms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4423--4439},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3293034},
	doi = {10.1109/TMC.2023.3293034},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaydamakaSMATK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The swarms of unmanned aerial vehicles (UAV) are nowadays finding numerous applications in different fields. While performing their missions, UAVs have to rely on external positioning information to maintain connectivity and communications between units in a swarm. However, some of the critical applications such as rescue missions are performed in locations, where this information is partially or fully not available, e.g., deep woods, mountains, indoors. In this paper, we propose a method for dynamic topology organization and maintenance in UAV swarms. In addition to the baseline functionality, we also design advanced features required for dynamic swarms merging and disjoining, making it suitable for practical applications. Specifically, the proposal is based on the virtual coordinates system allowing for the utilization of conventional geographical routing algorithms. We test the proposed algorithm in different swarm conditions to illustrate that: (i) it is insensitive to distance estimates up to at least 30% allowing for simple estimation techniques, (ii) the accuracy of the topology inference is at least 90% even under impairments caused by mobility and temporal loss of connectivity, and (iii) the impact of the developed merging algorithm for swarms lasts for multiple tens of time steps that correspond to just few seconds in practice. The set of developed algorithms can be utilized to ensure always connected topology in conditions where positioning information is partially or fully unavailable.}
}


@article{DBLP:journals/tmc/ChenXYWW24,
	author = {Xiao Chen and
                  Guoliang Xue and
                  Ruozhou Yu and
                  Haiqin Wu and
                  Dawei Wang},
	title = {A Vehicular Trust Blockchain Framework With Scalable Byzantine Consensus},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4440--4452},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294968},
	doi = {10.1109/TMC.2023.3294968},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXYWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The maturing blockchain technology has gradually promoted decentralized data storage from cryptocurrencies to other applications, such as trust management, resulting in new challenges based on specific scenarios. Taking the mobile trust blockchain within a vehicular network as an example, many users require the system to process massive traffic information for accurate trust assessment, preserve data reliably, and respond quickly. While existing vehicular blockchain systems ensure immutability, transparency, and traceability, they are limited in terms of scalability, performance, and security. To address these issues, this paper proposes a novel decentralized vehicle trust management solution and a well-matched blockchain framework that provides both security and performance. The paper primarily addresses two issues: i) To provide accurate trust evaluation, the trust model adopts a decentralized and peer-review-based trust computation method secured by trusted execution environments (TEEs). ii) To ensure reliable trust management, a multi-shard blockchain framework is developed with a novel hierarchical Byzantine consensus protocol, improving efficiency and security while providing high scalability and performance. The proposed scheme combines the decentralized trust model with a multi-shard blockchain, preserving trust information through a hierarchical consensus protocol. Finally, real-world experiments are conducted by developing a testbed deployed on both local and cloud servers for performance measurements.}
}


@article{DBLP:journals/tmc/XuGLDGZXZ24,
	author = {Chenhao Xu and
                  Jiaqi Ge and
                  Yong Li and
                  Yao Deng and
                  Longxiang Gao and
                  Mengshi Zhang and
                  Yong Xiang and
                  James Xi Zheng},
	title = {{SCEI:} {A} Smart-Contract Driven Edge Intelligence Framework for
                  IoT Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4453--4466},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3290925},
	doi = {10.1109/TMC.2023.3290925},
	timestamp = {Sat, 04 May 2024 10:56:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuGLDGZXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables collaborative training of a shared model on edge devices while maintaining data privacy. FL is effective when dealing with independent and identically distributed (IID) datasets, but struggles with non-iid datasets. Various personalized approaches have been proposed, but such approaches fail to handle underlying shifts in data distribution, such as data distribution skew commonly observed in real-world scenarios (e.g., driver behavior in smart transportation systems changing across time and location). Additionally, trust concerns among unacquainted devices and security concerns with the centralized aggregator pose additional challenges. To address these challenges, this paper presents a dynamically optimized personal deep learning scheme based on blockchain and federated learning. Specifically, the innovative smart contract implemented in the blockchain allows distributed edge devices to reach a consensus on the optimal weights of personalized models. Experimental evaluations using multiple models and real-world datasets demonstrate that the proposed scheme achieves higher accuracy and faster convergence compared to traditional federated and personalized learning approaches.}
}


@article{DBLP:journals/tmc/LiLDQEZ24,
	author = {Yantao Li and
                  Li Liu and
                  Shaojiang Deng and
                  Huafeng Qin and
                  Mounim A. El{-}Yacoubi and
                  Gang Zhou},
	title = {Memory-Augmented Autoencoder Based Continuous Authentication on Smartphones
                  With Conditional Transformer GANs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4467--4482},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3290834},
	doi = {10.1109/TMC.2023.3290834},
	timestamp = {Fri, 26 Apr 2024 07:58:46 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLDQEZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the last years, sensor-based continuous authentication on mobile devices has achieved great success on personal information protection. These proposed mechanisms, however, require both legal and illegal users’ data for authentication model training, which takes time and is impractical. In this paper, we present MAuGANs, a lightweight and practical Memory-Augmented Autoencoder-based continuous Authentication system on smartphones with conditional transformer Generative Adversarial Networks (GANs), where the conditional transformer GANs (CTGANs) are used for data augmentation and the memory-augmented autoencoder (MAu) is utilized to identify users. Specifically, MAuGANs exploits the smartphone built-in accelerometer and gyroscope sensors to implicitly collect users’ behavioral patterns. With the normalized legitimate user's sensor data, MAuGANs uses a CTGAN composed of a conditional transformer-based generator and a conditional transformer-based discriminator to create additional training data for the MAu. Then, the MAu is trained on the augmented legitimate user's data. The trained MAu reconstructs the current user data and then calculates the reconstruction error between the reconstructed data and current user data. To carry out user authentication, MAuGANs compares the reconstruction error with a predefined authentication threshold. We evaluate the performance of MAuGANs on our dataset, where our extensive experiments demonstrate that MAuGANs reaches the best authentication performance, when comparing with the representative state-of-the-art methods, by 0.33% EER and 99.65% accuracy on 10 unseen users.}
}


@article{DBLP:journals/tmc/DaiZWLC24,
	author = {Haipeng Dai and
                  Yikang Zhang and
                  Xiaoyu Wang and
                  Alex X. Liu and
                  Guihai Chen},
	title = {Omnidirectional Chargability With Directional Antennas},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4483--4500},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294391},
	doi = {10.1109/TMC.2023.3294391},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiZWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Power Transfer (WPT) has received more and more attention for its convenience and reliability. In this paper, we first propose the notion of omnidirectional charging. First, we consider the problem of detecting whether the target area achieves omnidirectional charging given a deterministic deployment of chargers. We use piecewise constant approximation and area discretization techniques to partition the target area and approximate charging power as constants. Next, we propose the Minimum Coverage Set extraction technique to design a fast detection algorithm. Second, we design a charger deployment scheme that satisfies omnidirectional charging. By placing the chargers at the triangle lattice points, we estimate the length of triangle lattice side length that satisfies omnidirectional charging, and derive the error bound with the optimal length. Third, we determine the probability that the target area achieves omnidirectional charging given a random deployment of chargers. We devise both analytical and numerical solutions for the problem with good accuracy. Finally, we conduct simulation and field experiments, and the results show that the running speed of our omnidirectional charging detection algorithm is at least 1\\times faster than comparison algorithms, and the consistency degree of our theoretical results and field experimental results is larger than 93.6 \\%.}
}


@article{DBLP:journals/tmc/PegoraroLMBRW24,
	author = {Jacopo Pegoraro and
                  Jesus Omar Lacruz and
                  Francesca Meneghello and
                  Enver Bashirov and
                  Michele Rossi and
                  Joerg Widmer},
	title = {{RAPID:} Retrofitting {IEEE} 802.11ay Access Points for Indoor Human
                  Detection and Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4501--4519},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3291882},
	doi = {10.1109/TMC.2023.3291882},
	timestamp = {Thu, 07 Nov 2024 13:38:57 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PegoraroLMBRW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work we present RAPID, the first joint communication and radar system based on next-generation IEEE 802.11ay WiFi networks operating in the 60 GHz band. Unlike existing approaches for human sensing at millimeter-wave frequencies, which rely on special-purpose radars, RAPID achieves radar-level sensing accuracy with IEEE 802.11ay access points, thus avoiding the burden of installing ad-hoc sensors. RAPID enables contactless human sensing applications, such as people tracking, Human Activity Recognition (HAR), and person identification without requiring modifications to the standard packet structure. Specifically, we leverage IEEE 802.11ay beam training to accurately localize and track multiple individuals within the same environment. Then, we propose a new way of using beam tracking to extract micro-Doppler signatures from the time-varying Channel Impulse Response (CIR) estimated from reflected packets. Such signatures are fed to a deep learning classifier to perform HAR and person identification. RAPID is implemented on a cutting-edge IEEE 802.11ay-compatible FPGA platform with phased antenna arrays, and evaluated on a large dataset of CIR measurements. It is robust across different environments and subjects, and outperforms state-of-the-art sub-6 GHz WiFi sensing techniques. Using two access points, RAPID reliably tracks multiple subjects, reaching HAR and person identification accuracies of 94% and 90%, respectively.}
}


@article{DBLP:journals/tmc/LiuCWLC24,
	author = {Shijia Liu and
                  Zhenghua Chen and
                  Min Wu and
                  Chang Liu and
                  Liangyin Chen},
	title = {WiSR: Wireless Domain Generalization Based on Style Randomization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4520--4532},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3292229},
	doi = {10.1109/TMC.2023.3292229},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current wireless cross-domain solutions are limited to cross-one-factor tasks, requiring target domain data participation for training or position-independent feature extraction using multiple transceivers. Therefore, this paper aims to demonstrate cross-domain wireless sensing in a more challenging domain generalization (DG) setting without multiple transceivers or target domain data. Specifically, we propose a style-randomized cross-domain wireless sensing model called WiSR, which extracts domain-invariant features from multiple source domains. It quantifies Channel State Information (CSI) differences in the subcarrier dimensions as subcarrier-domain styles and instructs the feature extractor to gradually bias the gesture signals by randomizing the subcarrier-domain styles at the feature level. Meanwhile, a domain classifier that shares the same feature extractor is instructed to gradually bias the domain signals by randomizing the gesture features. Then, an adversarial training framework enables the domain classifier to reduce the influence of domain signals on the feature extractor. Extensive experiments have been performed on three gesture datasets with varying amounts of subcarriers from devices with different NICs, including cross-one-factor (such as room, user, location, and orientation) and cross-multi-factor sensing tasks. The results demonstrate that our method considerably increases performance on wireless DG tasks.}
}


@article{DBLP:journals/tmc/ZhouS24,
	author = {Bo Zhou and
                  Walid Saad},
	title = {Age of Information in Ultra-Dense IoT Systems: Performance and Mean-Field
                  Game Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4533--4547},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3292515},
	doi = {10.1109/TMC.2023.3292515},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a dense Internet of Things (IoT) monitoring system is considered in which a large number of IoT devices contend for channel access so as to transmit timely status updates to the corresponding receivers using a carrier sense multiple access (CSMA) scheme. Under two packet management schemes with and without preemption in service, the closed-form expressions of the average age of information (AoI) and the average peak AoI of each device is characterized. It is shown that the scheme with preemption in service always leads to a smaller average AoI and a smaller average peak AoI, compared to the scheme without preemption in service. Then, a distributed noncooperative medium access control game is formulated in which each device optimizes its waiting rate so as to minimize its average AoI or average peak AoI under an average energy cost constraint on channel sensing and packet transmitting. To overcome the challenges of solving this game for an ultra-dense IoT, a mean-field game (MFG) approach is proposed to study the asymptotic performance of each device for the system in the large population regime. The accuracy of the MFG is analyzed, and the existence, uniqueness, and convergence of the mean-field equilibrium (MFE) are investigated. Simulation results show that the proposed MFG is accurate even for a small number of devices; and the proposed CSMA-type scheme under the MFG analysis outperforms three baseline schemes with fixed and dynamic waiting rates. Moreover, it is observed that the average AoI and the average peak AoI under the MFE do not necessarily decrease with the arrival rate.}
}


@article{DBLP:journals/tmc/YinWXXL24,
	author = {Yafeng Yin and
                  Zheng Wang and
                  Kang Xia and
                  Lei Xie and
                  Sanglu Lu},
	title = {Acoustic-Based Lip Reading for Mobile Devices: Dataset, Benchmark
                  and a Self Distillation-Based Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4548--4565},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294416},
	doi = {10.1109/TMC.2023.3294416},
	timestamp = {Wed, 17 Jul 2024 17:18:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YinWXXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Speech is a natural communication way between people and a good way for human-computer interaction. However, speech with audible voices often faces the following problems, e.g., being affected by surrounding noises, breaking the quiet environment, leaking privacy, etc. Therefore, silent speech was proposed, especially lip reading, which aims to recognize speech content based on lip movements. In this paper, we utilize inaudible acoustic signals generated from mobile device to sense and recognize lip movements for lip reading. Considering the lack of public dataset in acoustic-based lip reading, we propose and release a large-scale lip-reading dataset {\\sf LIPCMD} with 30000 acoustic-based recordings. To advance the further research in lip reading, we provide benchmark evaluation on {\\sf LIPCMD}, while using traditional machine learning solutions and recent deep learning approaches. To recognize weak acoustic signals as words for lip reading, we propose a self distillation based approach LipReader, which distills the probability distribution and attention map in convolutional neural network itself for better classification. Finally, we implement LipReader on smartphone and evaluate it on {\\sf LIPCMD} dataset as well as under complex scenarios. Experimental results show that LipReader can achieve a good recognition accuracy for lip reading, i.e., 91.58%, while outperforming baseline solutions and existing work.}
}


@article{DBLP:journals/tmc/XieHHGX24,
	author = {Xin Xie and
                  Cunqing Hua and
                  Jianan Hong and
                  Pengwenlong Gu and
                  Wenchao Xu},
	title = {AirCon: Over-the-Air Consensus for Wireless Blockchain Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4566--4582},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3292898},
	doi = {10.1109/TMC.2023.3292898},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieHHGX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain has been deemed as a promising solution for providing security and privacy protection in the next-generation wireless networks. Large-scale concurrent access for massive wireless devices to accomplish the consensus procedure may consume prohibitive communication and computing resources, and thus may limit the application of blockchain in wireless conditions. As most existing consensus protocols are designed for wired networks, directly apply them for wireless users equipment (UEs) may exhaust their scarce spectrum and computing resources. In this paper, we propose AirCon, a byzantine fault-tolerant (BFT) consensus protocol for wireless UEs via the over-the-air computation. The novelty of AirCon is to take advantage of the intrinsic characteristic of the wireless channel and automatically achieve the consensus in the physical layer while receiving from the UEs, which greatly reduces the communication and computational cost that would be caused by traditional consensus protocols. We implement the AirCon protocol integrated into an LTE system and provide solutions to the critical issues for over-the-air consensus implementation. Experimental results are provided to show the feasibility of the proposed protocol, and simulation results to show the performance of the AirCon protocol under different wireless conditions.}
}


@article{DBLP:journals/tmc/WuZQLL24,
	author = {Guangyu Wu and
                  Fuhui Zhou and
                  Yuben Qu and
                  Puhan Luo and
                  Xiang{-}Yang Li},
	title = {QoS-Ensured Model Optimization for AIoT: {A} Multi-Scale Reinforcement
                  Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4583--4600},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294512},
	doi = {10.1109/TMC.2023.3294512},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZQLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optimizing deep neural network (DNN) models to meet Quality of Service (QoS) requirements in terms of accuracy and computation is of crucial importance for realizing efficient on-device inference in resource-constrained Artificial Intelligence of Things (AIoT). However, most existing works can hardly satisfy the aforementioned QoS requirements since the intrinsic multi-scale characteristic of DNN structures has been seldom considered. In this paper, we formulate a QoS-ensured DNN model structure optimization problem as a novel multi-scale Markov decision process (MSMDP), which can collaboratively decide the DNN structures from different scales. To efficiently solve the above problem, we propose a multi-scale reinforcement learning (MSRL) algorithm, which jointly optimizes block and channel number by interactive multi-scale decision, while ensuring QoS by QoS-based decision evaluation and policy update. Extensive experiments are conducted in both the actual AIoT scenarios and public datasets for different tasks by using different AIoT devices. The results confirm that our proposed MSRL outperforms the baseline schemes in terms of QoS satisfaction, convergence performance, and complexity. Specifically, our algorithm respectively reduces 98.6% computation and 95.7% model size at most while ensuring the QoS compared with the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/ZhaoQXSCG24,
	author = {Yao Zhao and
                  Youyang Qu and
                  Yong Xiang and
                  Chaochen Shi and
                  Feifei Chen and
                  Longxiang Gao},
	title = {Long-Term Over One-Off: Heterogeneity-Oriented Dynamic Verification
                  Assignment for Edge Data Integrity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4601--4616},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294180},
	doi = {10.1109/TMC.2023.3294180},
	timestamp = {Sat, 27 Apr 2024 21:29:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoQXSCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge Intelligence (EI), a burgeoning research area, motivates App vendors to cache data replicas on geographically distributed edge servers to deliver better services. On the downside, this benefit also incurs more data integrity audit overhead on App vendors, which calls for more efficient Edge Data Integrity (EDI) verification approaches. However, existing EDI solutions totally rely on an implicit resource homogeneity assumption-edge servers have identical resource availability throughout EDI inspection execution in each round-but it rarely holds in reality. The edge servers with insufficient computation and/or communication capacity greatly limit overall EDI verification efficiency from a round perspective. Thus, in this work, we release the identified impractical assumption and accordingly study the EDI Dynamic Verification Assignment (DVA) problem for the first time. The problem aims to maximize the number of data replicas being verified in the long term under the constraints of verification delay in resource-limited environments. In this way, App vendors merely need to check the integrity of selected data replicas in each round for efficiency improvement. Specifically, we first formalize the DVA problem as a delay-constrained long-term stochastic optimization problem and further prove its \\mathcal {NP}-hardness. To resolve the problem efficiently, we decompose it to an easy-to-handle form and then develop a polynomial-time Priority-based approach named DVA-P with a theoretical analysis of its time complexity and performance bound. Finally, experimental evaluations validate that DVA-P can be seamlessly incorporated into existing EDI solutions to enhance overall verification efficiency while guaranteeing verification performance.}
}


@article{DBLP:journals/tmc/CaiLTM24,
	author = {Yang Cai and
                  Jaime Llorca and
                  Antonia M. Tulino and
                  Andreas F. Molisch},
	title = {Joint Compute-Caching-Communication Control for Online Data-Intensive
                  Service Delivery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4617--4633},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297598},
	doi = {10.1109/TMC.2023.3297598},
	timestamp = {Fri, 19 Apr 2024 13:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiLTM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data-intensive augmented information (AgI) services (e.g., metaverse applications such as virtual/augmented reality), designed to deliver highly interactive experiences resulting from the real-time combination of live data-streams and pre-stored digital content, are accelerating the need for distributed compute platforms with unprecedented storage, computation, and communication requirements. To this end, the integrated evolution of next-generation networks (5G/6G) and distributed cloud technologies (mobile/edge/cloud computing) have emerged as a promising paradigm to address the interaction- and resource-intensive nature of data-intensive AgI services. In this paper, we focus on the design of control policies for the joint orchestration of compute, caching, and communication (3C) resources in next-generation 3C networks for the delivery of data-intensive AgI services. We design the first throughput-optimal control policy that coordinates joint decisions around (i) routing paths and processing locations for live data streams, with (ii) cache selection and distribution paths for associated data objects. We then extend the proposed solution to include a max-throughput data placement policy and two efficient replacement policies. Numerical results demonstrate the superior performance obtained via the novel multi-pipeline flow control and 3C resource orchestration mechanisms of the proposed policy, compared with state-of-the-art algorithms that lack full 3C integrated control.}
}


@article{DBLP:journals/tmc/SakaiSKW24,
	author = {Kazuya Sakai and
                  Min{-}Te Sun and
                  Wei{-}Shinn Ku and
                  Jie Wu},
	title = {Barrier Penetration Routing Against Wireless Spy Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4634--4647},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294415},
	doi = {10.1109/TMC.2023.3294415},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SakaiSKW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a potential communication problem in national security, where wireless spy sensors with eavesdropping capability are strategically deployed around an area of interest. For counterintelligence, achieving secure communication by penetrating such a spy barrier is of great importance. In this paper, we first formulate the problem of barrier penetration routing against spy barriers consisting of strategically deployed wireless sensors. We point out that existing multi-path avoidance routing protocols cannot efficiently counteract collusion attacks, where connected adversaries collaborate with each other to compromise data packets. We propose a barrier penetration routing (BPR) protocol to securely penetrate the barrier of adversaries. In the protocol, a set of physically distanced paths are identified based on distance vectors as well as network-wide flooding. Then, each data packet encoded by xor coding is routed via a different path. Unlike existing avoidance routing, the proposed scheme does not rely on the assumption that the adversary's locations are known. The simulation results demonstrate that the proposed BPR outperforms the baseline protocol as well as existing routing protocols in terms of secure delivery rate.}
}


@article{DBLP:journals/tmc/ZhangYRZHLCLZL24,
	author = {Jinrui Zhang and
                  Huan Yang and
                  Ju Ren and
                  Deyu Zhang and
                  Bangwen He and
                  Youngki Lee and
                  Ting Cao and
                  Yuanchun Li and
                  Yaoxue Zhang and
                  Yunxin Liu},
	title = {HiMoDepth: Efficient Training-Free High-Resolution On-Device Depth
                  Perception},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4648--4664},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294188},
	doi = {10.1109/TMC.2023.3294188},
	timestamp = {Thu, 14 Nov 2024 17:45:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangYRZHLCLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-resolution depth estimation, with a minimum resolution of 1280\\times 960, is essential for achieving more immersive experiences in on-device 3D vision applications. However, implementing high-resolution solutions on resource-limited mobile devices presents significant challenges, such as the need for additional expensive depth sensors, computation-intensive machine learning models requiring large-scale datasets, or the need for device motion while the target object remains stationary. In this study, we propose HiMoDepth, an efficient training-free high-resolution depth estimation system that utilizes widely-available on-device dual cameras. HiMoDepth consists of two modules: 1) homogenizing the on-device heterogeneous cameras by iteratively cropping the Field-of-Views to make the focal length of the cameras equal and filtering out the out-of-sync frames based on time stamps, and 2) designing a hierarchical mobile GPU-friendly stereo matching method that effectively reduces the latency of stereo matching with high-resolution depth maps by using efficient data layout, reducing the number of memory accesses, and searching the corresponding pixel over a coarse-to-fine hierarchy. We implement HiMoDepth on multiple commodity mobile devices and conduct comprehensive evaluations. Experimental results show that HiMoDepth significantly outperforms the baselines in both accuracy and running speed on mobile devices that support high-resolution depth maps.}
}


@article{DBLP:journals/tmc/MishraG24,
	author = {Rahul Mishra and
                  Hari Prabhat Gupta},
	title = {Designing and Training of Lightweight Neural Networks on Edge Devices
                  Using Early Halting in Knowledge Distillation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4665--4677},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297026},
	doi = {10.1109/TMC.2023.3297026},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MishraG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automated feature extraction capability and significant performance of Deep Neural Networks (DNN) make them suitable for Internet of Things (IoT) applications. However, deploying DNN on edge devices becomes prohibitive due to the colossal computation, energy, and storage requirements. This paper presents a novel approach, EarlyLight, for designing and training lightweight DNN using large-size DNN. The approach considers the available storage, processing speed, and maximum allowable processing time to execute the task on edge devices. We present a knowledge distillation based training procedure to train the lightweight DNN to achieve adequate accuracy. During the training of lightweight DNN, we introduce a novel early halting technique, which preserves network resources; thus, speedups the training procedure. Finally, we present the empirically and real-world evaluations to verify the effectiveness of the proposed approach under different constraints using various edge devices.}
}


@article{DBLP:journals/tmc/LiuZX24,
	author = {Linfeng Liu and
                  Yaoze Zhou and
                  Jia Xu},
	title = {A Cloud-Edge-End Collaboration Framework for Cruising Route Recommendation
                  of Vacant Taxis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4678--4693},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294898},
	doi = {10.1109/TMC.2023.3294898},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Taxis can provide convenient and flexible transportation services for citizens. The proper cruising routes should be recommended to vacant taxis, so as to help them to pick up passengers as early as possible, and thus increase their business profits. To this end, we propose a Cloud-edge-end Collaboration Framework for the Cruising Route Recommendation of vacant taxis (CCF-CRR). In CCF-CRR, each vacant taxi trains a local model based on its historical cruising route segments, and the local model parameters of the vacant taxis in the same region are periodically uploaded to an edge server for parameter aggregation. Then, the aggregated model parameters are released by the edge server to vacant taxis for their use. In addition, the future waiting time of passengers is predicted by the edge servers in different regions and is uploaded to the cloud server, and then the cloud server can measure the potential taxi demand in regions and dispatch vacant taxis among regions to achieve the taxi demand-supply equilibrium. Extensive simulations and comparisons demonstrate the superior performance of our proposed CCF-CRR, i.e., with the cloud-edge-end collaboration framework, the business profits of taxis can be significantly increased, and the pick-up distance of taxis can be largely shortened.}
}


@article{DBLP:journals/tmc/ZhouZLJL24,
	author = {Siwang Zhou and
                  Xiao Zhang and
                  Yonghe Liu and
                  Hongbo Jiang and
                  Keqin Li},
	title = {Decentralized and Compressed Data Storage for Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4694--4708},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3294969},
	doi = {10.1109/TMC.2023.3294969},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZLJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sensing data acquired with crowdsensing are generally stored at central cloud servers, since massive data are involved and sensing devices do not have enough space to store them. Although each sensing device only has limited storage capacity, the total size of storage across thousands of devices can be considerable. In view of this, this article addresses decentralized storage problem in mobile crowdsensing system, providing an alternative to cloud-based data storage. By investigating a virtual sensor model, the movement of a participant in the target sensing area is formulated as a random sampling over the data field related to this area. With a particular encoding algorithm, the data field is compressed into only one measurement along with a random sampling process. Each participant stores its own measurements as if various compressed snapshots of the data field are separately stored by different participants. We further investigate a recovery algorithm, reconstructing the original data field by carefully decoding enough measurements. Extensive experiments validate the proposed storage scheme under various crowdsensing scenarios, and our scheme achieves excellent performance in terms of recruitment overhead, decoding time, and decoding accuracy.}
}


@article{DBLP:journals/tmc/FuXLYDQ24,
	author = {Yaru Fu and
                  Xinyu Xu and
                  Hanlin Liu and
                  Quan Yu and
                  Hong{-}Ning Dai and
                  Tony Q. S. Quek},
	title = {Joint Assortment and Cache Planning for Practical User Choice Model
                  in Wireless Content Caching Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4709--4722},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297987},
	doi = {10.1109/TMC.2023.3297987},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FuXLYDQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In wireless content caching networks (WCCNs), a user's content consumption crucially depends on the assortment offered. Here, the assortment refers to the recommendation list. An appropriate user choice model is essential for greater revenue. Therefore, in this paper, we propose a practical multinomial logit choice model to capture users’ content requests. Based on this model, we first derive the individual demand distribution per user and then investigate the effect of the interplay between the assortment decision and cache planning on WCCNs’ achievable revenue. A revenue maximization problem is formulated while incorporating the influences of the screen size constraints of users and the cache capacity budget of the base station (BS). The formulated optimization problem is a non-convex integer programming problem. For ease of analysis, we decompose it into two folds, i.e., the personalized assortment decision problem and the cache planning problem. By using structure-oriented geometric properties, we design an iterative algorithm with examinable quadratic time complexity to solve the non-convex assortment problem in an optimal manner. The cache planning problem is proved to be a 0-1 Knapsack problem and thus can be addressed by a dynamic programming approach with pseudo-polynomial time complexity. Afterwards, an alternating optimization method is used to optimize the two types of variables until convergence. It is shown by simulations that the proposed scheme outperforms various existing benchmark schemes.}
}


@article{DBLP:journals/tmc/ZhouYLWCW24,
	author = {Zhengyang Zhou and
                  Kuo Yang and
                  Yuxuan Liang and
                  Binwu Wang and
                  Hongyang Chen and
                  Yang Wang},
	title = {Predicting Collective Human Mobility via Countering Spatiotemporal
                  Heterogeneity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4723--4738},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3296501},
	doi = {10.1109/TMC.2023.3296501},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouYLWCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human mobility forecasting is the key to energizing considerable mobile computing services. However, we find that the collective mobility suffers the spatiotemporal heterogeneity issue and therefore leads to inferior performances of conventional homogeneous aggregations. Given two fundamental factors, i.e., data and objectives in machine learning, we propose to counter such heterogeneity by improving data utilization and optimization objectives. 1) From data utilization perspective, we discover that such heterogeneity is inherently induced by mobility-related context factors and thus these factors can be exploited to learn heterogeneous mobility patterns. 2) From the optimization perspective, the dependencies among output elements, which give another prior to learning, can extract heterogeneous correlations within output sequences. Specifically, we propose a novel Context-Directional SpatioTemporal Graph Network (CD-STGNet), which tackles the above-mentioned heterogeneity, for achieving accurate mobility predictions. First, we improve data utilization by inputting the encoded context-wise interactions to a direction field learner, which realizes directional spatial aggregations. Second, regarding series learning and optimization objectives, a context-trend highway is designed to enable context-aware temporal learning while two regularization objectives are proposed to keep the correlations among predicted elements consistent with the ground-truth. Experiments demonstrate that CD-STGNet surpasses competitive baselines by 13% to 22% and boosts the interpretability of context-directional learning.}
}


@article{DBLP:journals/tmc/HuangZ24,
	author = {Xiufeng Huang and
                  Sheng Zhou},
	title = {QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent
                  Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4739--4751},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3296726},
	doi = {10.1109/TMC.2023.3296726},
	timestamp = {Tue, 16 Apr 2024 15:32:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages. The key insight is spending the precious communication resources on important messages. The message importance depends not only on the messages themselves, but also on the needs of agents who receive them. Accordingly, we propose a query-message-based architecture, called QMNet. Agents generate queries and messages with the environment observation. Sharing queries can help calculate message importance. Exchanging messages can help agents cooperate better. Besides, we exploit the message importance to deal with random access collisions in decentralized systems. Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted. Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fraction of agents can send messages due to limited wireless resources. Results show that QMNet can extract valuable information to guarantee the system performance even when only 30% of agents can share messages. By exploiting message prediction, the system can further save 40% of wireless resources. The importance-aware decentralized multi-access mechanism can effectively avoid collisions, achieving almost the same performance as centralized scheduling.}
}


@article{DBLP:journals/tmc/TuBAW24,
	author = {Shanshan Tu and
                  Akhtar Badshah and
                  Hisham Alasmary and
                  Muhammad Waqas},
	title = {{EAKE-WC:} Efficient and Anonymous Authenticated Key Exchange Scheme
                  for Wearable Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4752--4763},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297854},
	doi = {10.1109/TMC.2023.3297854},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TuBAW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wearable computing has shown tremendous potential to revolutionize and uplift the standard of our lives. However, researchers and field experts have often noted several privacy and security vulnerabilities in the field of wearable computing. In order to tackle these problems, various schemes have been proposed in the literature to improve the efficiency of authentication and key establishment procedure. However, the existing schemes have relatively high computation and communication overheads and are not resilient to various potential security attacks, which reduces their significance for applicability in constrained wearable devices. In this work, we propose an efficient and anonymous authenticated key exchange scheme for wearable computing (EAKE-WC), which performs mutual authentication between the user and the wearable device, and between the cloud server and the user. It also establishes secret session keys for each session to secure communication among the communicating entities. Additionally, the proposed EAKE-WC scheme is designed using authenticated encryption with associated data (AEAD) primitives like ASCON, bitwise XOR, and hash functions. Our results from the security analysis depict compliance of the proposed EAKE-WC with wearable computing's security criteria. In addition, we also demonstrate through a comprehensive comparative analysis that the proposed scheme, EAKE-WC, outperforms the existing benchmark schemes in various key performance areas, including lower communication and computational overheads, enhanced security, and added functionality.}
}


@article{DBLP:journals/tmc/ZhouKQ24,
	author = {Xiaobo Zhou and
                  Zhihui Ke and
                  Tie Qiu},
	title = {Recommendation-Driven Multi-Cell Cooperative Caching: {A} Multi-Agent
                  Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4764--4776},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297213},
	doi = {10.1109/TMC.2023.3297213},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouKQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 5G small cell networks, edge caching is a key technique to alleviate the backhaul burden by caching user desired contents at network edges such as small base stations (SBSs). However, due to storage space limitation and diverse user preference patterns, a single SBS is unable to cache all the user desired contents and thus leading to low caching efficiency. In this paper, we propose a recommendation-driven multi-cell cooperative caching strategy to improve the caching efficiency. The idea is to aggregate the storage spaces of multiple SBSs into a large shared resource pool, and guide users to access cached contents by content recommendation. First, we formulate the joint cooperative caching and recommendation problem as a multi-agent multi-armed bandit (MAMAB) problem with the aim of minimizing the average download latency. Then, we propose a multi-agent reinforcement learning (MARL)-based algorithm, MARL-JCR, to solve the problem in a fully distributed manner with limited information exchange among the agents. We also develop a modified combinatorial upper confidence bound algorithm to reduce each agent's decision space to reduce computational complexity. The experiment results evaluated on the MovieLens dataset show MARL-JCR decreases the average download latency by up to 60% as compared with the state-of-the-art solutions.}
}


@article{DBLP:journals/tmc/LiuLNZXX24,
	author = {Libin Liu and
                  Jingzong Li and
                  Zhixiong Niu and
                  Wei Zhang and
                  Chun Jason Xue and
                  Hong Xu},
	title = {Efficient Time-Series Data Delivery in IoT With Xender},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4777--4792},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3296608},
	doi = {10.1109/TMC.2023.3296608},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLNZXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large amounts of time-series data need to be continually delivered from IoT devices to the cloud for real-time data analytics. The data delivery process is intrinsically slow and costly. Therefore, lots of work proposes various data reduction methods to accelerate it. Yet, they are either designed for the simple linear time-series data or computation-intensive, which is not suitable for the IoT devices with limited resources. In this paper, we propose Xender, a system to accelerate time-series data delivery. Xender consists of two key components: data sampler and data generator. Data sampler works on IoT devices to sample time-series data with low resource footprint, and data generator works on the cloud to efficiently generate data that significantly resembles the original. Besides, Xender can adapt to the dynamic characteristics of the time-series data with the content-aware mechanism, as well as the dynamic computation resources by supporting multiple data generation quality levels and using the anytime generation mechanism. We implement Xender and evaluate it with testbed experiments using six real-world datasets. The results show that it can significantly reduce data delivery time by 45.79% on average compared against existing schemes, and adapt to computation resources with up to 1014.40Mbps data generation throughput.}
}


@article{DBLP:journals/tmc/ZhangCCWJ24,
	author = {Yong Zhang and
                  Andong Cheng and
                  Bin Chen and
                  Yujie Wang and
                  Lu Jia},
	title = {A Location-Independent Human Activity Recognition Method Based on
                  {CSI:} System, Architecture, Implementation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4793--4805},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3296987},
	doi = {10.1109/TMC.2023.3296987},
	timestamp = {Wed, 07 Aug 2024 17:08:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCCWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the application of human activity recognition (HAR) based on channel state information (CSI), due to the high dynamic characteristics of wireless channel to different environments, the features of human activity samples in different locations are different. In addition, the existing CSI-based HAR approaches limit the extraction of activity features to the euclidean space and ignores the rich relational information between samples, categories and locations, which result in insufficient generalization performance for location-independent HAR. To address this challenge, this paper proposes a CSI-based location-independent HAR system CSI-MTGN. The system represents the classification task under each training sample collection location (TSCL) as a task, which is composed of three interactive parts: sample hidden representation, activity features extraction based on hierarchical graph neural network (HGNN) and information exchange based on multi-task learning. The proposed system improves the sample hidden representation, which is benefit for activity feature extraction and classification. The HGNN is designed to express various relationship information between samples, categories and locations in the form of graph structure, and the classification task under each TSCL is constructed through data augmentation, so as to improve the knowledge understanding and inference capabilities of the recognition model. The multi-task learning is used to achieve implicit data augmentation by sharing parameters among tasks through soft parameter sharing, and improves the generalization performance of the system. To validate the performance of the proposed system, experiments were conducted in a hall and a conference room, where samples of 10 categories of activities under 7 TSCLs were used for training the system, and the HAR accuracy rates at any locations were 94.1% and 93.3%, respectively.}
}


@article{DBLP:journals/tmc/LiangWW24,
	author = {Yongheng Liang and
                  Hejun Wu and
                  Haitao Wang},
	title = {Asynchronous Multi-Agent Reinforcement Learning for Collaborative
                  Partial Charging in Wireless Rechargeable Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4806--4818},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3299238},
	doi = {10.1109/TMC.2023.3299238},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiangWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online Scheduling for Partial charging with Multi-Mobile Chargers (OSPM) is critical for Wireless Rechargeable Sensor Networks (WRSNs) performing high-power monitoring tasks with a large number of simultaneous charging requests. However, existing studies for online scheduling assume full charging of sensors, leading to delays and inefficient resource utilization. Partially charging the sensors can improve scheduling efficiency and flexibility, but these studies focus on off-line scheduling, hindering dynamic decision-making. Multi-Agent Reinforcement Learning (MARL) is advantageous in online collaboration. Nevertheless, existing MARL methods assume synchronized actions, while Mobile Chargers (MCs) performing charging tasks asynchronously due to the difference in movement and charging times. On the other hand, hybrid actions are required to capture the simultaneous decision-making of MCs, involving sensor selection (discrete action) and energy allocation (continuous parameter). This introduces a circular dependency between a discrete action and its corresponding continuous parameter due to their interdependence. To deal with the above problems and address OSPM, we propose Asynchronous and Scalable Multi-agent Hybrid Proximal Policy Optimization (ASM-HPPO). The evaluation results not only indicate that our ASM-HPPO has advantages in terms of various performance metrics over existing schemes, but also demonstrate that our methods achieve higher stability and scalability.}
}


@article{DBLP:journals/tmc/MaoZYLLDLZ24,
	author = {Yuzhu Mao and
                  Zihao Zhao and
                  Meilin Yang and
                  Le Liang and
                  Yang Liu and
                  Wenbo Ding and
                  Tian Lan and
                  Xiao{-}Ping Zhang},
	title = {{SAFARI:} Sparsity-Enabled Federated Learning With Limited and Unreliable
                  Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4819--4831},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3296624},
	doi = {10.1109/TMC.2023.3296624},
	timestamp = {Thu, 05 Dec 2024 20:56:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaoZYLLDLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables edge devices to collaboratively learn a model in a distributed fashion. Many existing researches have focused on improving communication efficiency of high-dimensional models and addressing bias caused by local updates. However, most FL algorithms are either based on reliable communications or assuming fixed and known unreliability characteristics. In practice, networks could suffer from dynamic channel conditions and non-deterministic disruptions, with time-varying and unknown characteristics. To this end, in this paper we propose a sparsity-enabled FL framework with both improved communication efficiency and bias reduction, termed as SAFARI. It makes use of similarity among client models to rectify and compensate for bias that results from unreliable communications. More precisely, sparse learning is implemented on local clients to mitigate communication overhead, while to cope with unreliable communications, a similarity-based compensation method is proposed to provide surrogates for missing model updates. With respect to sparse models, we analyze SAFARI under bounded dissimilarity. It is demonstrated that SAFARI under unreliable communications is guaranteed to converge at the same rate as the standard FedAvg with perfect communications. Implementations and evaluations on the CIFAR-10 dataset validate the effectiveness of SAFARI by showing that it can achieve the same convergence speed and accuracy as FedAvg with perfect communications, with up to 60% of the model weights being pruned and a high percentage of client updates missing in each round of model updates.}
}


@article{DBLP:journals/tmc/KarmakarKA24a,
	author = {Raja Karmakar and
                  Georges Kaddoum and
                  Ouassima Akhrif},
	title = {A Novel Federated Learning-Based Smart Power and 3D Trajectory Control
                  for Fairness Optimization in Secure UAV-Assisted {MEC} Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4832--4848},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298935},
	doi = {10.1109/TMC.2023.3298935},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KarmakarKA24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs)-aided mobile-edge computing (MEC) systems face several challenges that hinder their practical implementation. First, the broadcast nature of wireless communications can cause security issues. Second, UAVs have constrained onboard power. Finally, the UAV should be able to serve a maximum number of ground users (GUs). It is also crucial to maintain fairness such that all GUs get equal opportunities to securely offload tasks to UAVs. We seek to address the aforementioned challenges by designing an intelligent mechanism, FairLearn, which maximizes the fairness in secure MEC services by controlling the UAV 3D trajectory, transmission power, and scheduling time for task offloading by mobile GUs. To this end, we formulate a maximization problem and solve it using a deep neural network (DNN)-based model, where the UAVs collaboratively learn the model by utilizing a federated learning (FL) approach. Each UAV uses a reinforcement learning (RL)-based approach to individually generate the training dataset, making the training data span different network scenarios. Our model is based on UAV pairs, where one UAV executes the GUs’ offloaded tasks, while the other is a jammer that suppresses eavesdroppers. The simulation evaluation of FairLearn shows that it significantly improves the performance of UAV-enabled MEC systems.}
}


@article{DBLP:journals/tmc/SiYXWLZTC24,
	author = {Junjun Si and
                  Jin Yang and
                  Yang Xiang and
                  Hanqiu Wang and
                  Li Li and
                  Rongqing Zhang and
                  Bo Tu and
                  Xiangqun Chen},
	title = {TrajBERT: BERT-Based Trajectory Recovery With Spatial-Temporal Refinement
                  for Implicit Sparse Trajectories},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4849--4860},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3297115},
	doi = {10.1109/TMC.2023.3297115},
	timestamp = {Wed, 22 May 2024 14:53:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SiYXWLZTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of human mobility data analysis, a multitude of constraints result in the publication of sparse, non-uniform implicit trajectories without explicit location information, such as coordinates. Researchers have dedicated substantial efforts towards trajectory recovery, aiming to densify trajectories and gain a more comprehensive understanding of human mobility. However, existing trajectory recovery methods focus on explicit trajectories, and require extensive historical data to capture users’ mobility patterns. Nevertheless, implicit trajectories are usually more sparse than explicit trajectories. Addressing these challenges, we propose TrajBERT, an innovative BERT-based trajectory recovery method with spatial-temporal refinement. TrajBERT employs the Transformer encoder to learn mobility patterns bi-directionally and enhances the predictions by cross-stage temporal refinement. Subsequently, we design an output layer with global spatial refinement with a novel spatial-temporal aware loss function. To evaluate the performance of TrajBERT, we conduct a series of experiments on real-world datasets. Remarkably, TrajBERT yields at least 8.2% performance improvement compared to the state-of-the-art trajectory recovery approachs. Furthermore, TrajBERT successfully mitigates the cold start problem commonly experienced with new users lacking historical trajectories. It also shows superior robustness when faced with extremely sparse trajectories, thus demonstrating its potential as a practical tool in the field of human mobility analysis.}
}


@article{DBLP:journals/tmc/ChiQXZ24,
	author = {Jiancheng Chi and
                  Tie Qiu and
                  Fu Xiao and
                  Xiaobo Zhou},
	title = {{ATOM:} Adaptive Task Offloading With Two-Stage Hybrid Matching in
                  MEC-Enabled Industrial IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4861--4877},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3302834},
	doi = {10.1109/TMC.2023.3302834},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChiQXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Industrial Internet of Things (IIoT) integrates diverse wireless and heterogeneous devices to enable time-sensitive applications. Multi-access edge computing (MEC) offers computing services for nearby tasks to meet their time requirements. However, offloading a large number of tasks to servers with minimal time is a challenging issue. Existing approaches typically allocate tasks into equal-length timeslots for offloading based on optimization or heuristic methods, overlooking the time-varying nature of task arrival density. This neglect significantly increases task execution time. To address this problem, we propose an Adaptive Task Offloading scheme with two-stage hybrid Matching (ATOM). In ATOM, a global buffer with an adjustable threshold is employed to store task information, enabling it to adapt to the time-varying arrival density and execute different offloading stages accordingly. In the online matching stage, if the threshold is not reached, tasks in the buffer are promptly offloaded to the most suitable server. In the offline matching stage, when the threshold is exceeded, all tasks in the buffer are optimally matched with servers and offloaded in batches. Experimental results demonstrate that ATOM outperforms state-of-the-art schemes in terms of average execution time and timeout rate, achieving reductions of 23.3% and 10.4%, respectively.}
}


@article{DBLP:journals/tmc/BadiaZZ24,
	author = {Leonardo Badia and
                  Andrea Zanella and
                  Michele Zorzi},
	title = {A Game of Ages for Slotted {ALOHA} With Capture},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4878--4889},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298716},
	doi = {10.1109/TMC.2023.3298716},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BadiaZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within a recent line of research, age of information is supported as an alternate network performance metric with respect to throughput or delay, to evaluate the performance of medium access techniques, especially for remote sensing applications. Analytical investigations based on game theory have shown how selfish players can behave efficiently in random access systems if they are driven by AoI-based objectives. We extend this kind of reasoning to the case of a slotted ALOHA system with capture. We present a fully analytical derivation of the general framework and its main results. We provide a quantitative characterization for the strength of capture in relation to the efficiency of the resulting Nash equilibrium, which provides extremely useful insights for a distributed system management. We apply our analysis to some scenarios of interest, in particular the case of exponentially distributed powers, for which we obtain a closed-form relationship. We highlight the impact of the system parameters, specifically the cost coefficient and the capture threshold, towards achieving an efficient allocation that represents an equilibrium for the network management. It is ultimately shown that, when capture is strong, as quantified through precise conditions (the system is driven towards a Nash equilibrium achieving near-optimal performance).}
}


@article{DBLP:journals/tmc/LiSDW24,
	author = {Jiahui Li and
                  Geng Sun and
                  Lingjie Duan and
                  Qingqing Wu},
	title = {Multi-Objective Optimization for {UAV} Swarm-Assisted IoT With Virtual
                  Antenna Arrays},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4890--4907},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298888},
	doi = {10.1109/TMC.2023.3298888},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiSDW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) network is a promising technology for assisting Internet-of-Things (IoT), where a UAV can use its limited service coverage to harvest and disseminate data from IoT devices with low transmission abilities. The existing UAV-assisted data harvesting and dissemination schemes largely require UAVs to frequently fly between the IoTs and access points, resulting in extra energy and time costs. To reduce both energy and time costs, a key way is to enhance the transmission performance of IoT and UAVs. In this work, we introduce collaborative beamforming into IoTs and UAVs simultaneously to achieve energy and time-efficient data harvesting and dissemination from multiple IoT clusters to remote base stations (BSs). Except for reducing these costs, another non-ignorable threat lies in the existence of the potential eavesdroppers, whereas the handling of eavesdroppers often increases the energy and time costs, resulting in a conflict with the minimization of the costs. Moreover, the importance of these goals may vary relatively in different applications. Thus, we formulate a multi-objective optimization problem (MOP) to simultaneously minimize the mission completion time, signal strength towards the eavesdropper, and total energy cost of the UAVs. We prove that the formulated MOP is an NP-hard, mixed-variable optimization, and large-scale optimization problem. Thus, we propose a swarm intelligence-based algorithm to find a set of candidate solutions with different trade-offs which can meet various requirements in a low computational complexity. We also show that swarm intelligence methods need to enhance solution initialization, solution update, and algorithm parameter update phases when dealing with mixed-variable optimization and large-scale problems. Simulation results demonstrate the proposed algorithm outperforms state-of-the-art swarm intelligence algorithms and also show that the proposed method can reduce time and energy costs significantly compared with the benchmark strategies based on multi-hop and long-range flight.}
}


@article{DBLP:journals/tmc/LeeY24,
	author = {Byung Moo Lee and
                  Hong Yang},
	title = {A Sectorized {RS} Reuse Massive {MIMO} for Massive IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4908--4917},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298577},
	doi = {10.1109/TMC.2023.3298577},
	timestamp = {Wed, 26 Feb 2025 16:30:10 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LeeY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider a sectorized Massive multiple-input multiple-output (MIMO) system to simultaneously support a large amount of Internet of things (IoT) devices. To implement low complexity and low latency IoT networks, it has been shown that the reuse of orthogonal uplink reference signal (RS) is quite effective. Sectorization can reduce the heavy RS reuse in the situation of massive IoT connectivity, and consequently reduce interference from RS contamination. However, inter-sector interference (ISI) can be another source that reduces performance. We investigate the characteristics of sectorized Massive MIMO with RS reuse, and compare the performance with the case of unsectorized Massive MIMO. We present theoretical closed-form expressions of spectral efficiency (SE) applying sectorized antenna systems in the situation of massive IoT connectivity. We show that, in the massive IoT connectivity situation, sectorized Massive MIMO can show better total SE performance than unsectorized Massive MIMO, while outage probability can be seriously increased due to nonuniform antenna patterns and ISI. We also show that the power control mechanism which has been applied to unsectorized RS reuse Massive MIMO also can be successfully applied to the sectorized RS reuse Massive MIMO.}
}


@article{DBLP:journals/tmc/XiaoSJMLI24,
	author = {Zhu Xiao and
                  Jinmei Shu and
                  Hongbo Jiang and
                  Geyong Min and
                  Jinwen Liang and
                  Arun Iyengar},
	title = {Toward Collaborative Occlusion-Free Perception in Connected Autonomous
                  Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4918--4929},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3298643},
	doi = {10.1109/TMC.2023.3298643},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoSJMLI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In connected autonomous vehicles (CAVs), the driving safety can be greatly deteriorated, in the presence of occlusions which are adverse to CAVs’ perception of region-of-interest (RoI). Collaborative perception on the basis the information sharing of occlusions among CAVs, in a real-time and accurate manner, provides a means of the occlusion-free RoI perception for safe driving. In this paper, we propose a novel framework of Collaborative Occlusion-free Perception (COFP) in CAVs, to regain the real-time and accurate occlusion awareness. The innovative COFP targets two goals: well-balanced computation resource allocation, as well as fast and high-quality RoI information fusion. Specifically, the resource allocation problem, with the objective of minimizing CAVs’ completion delay, is formulated as a multi-player continuous potential game and solved by a better response dynamics (BRD) algorithm. The RoI information fusion, with the objective of maximizing the overall object depiction quality, is formulated as a combinatorial optimization problem, and solved by a modified discrete salp swarm (MDSSA) algorithm. Experimental results show that the proposed COFP with 5 GHz computing power can achieve full occlusion awareness for CAVs with 69.61% completion time reduction and 19.03% fusion quality improvement, compared to the existing methods.}
}


@article{DBLP:journals/tmc/BartoliniCMP24,
	author = {Novella Bartolini and
                  Andrea Coletta and
                  Gaia Maselli and
                  Matteo Prata},
	title = {TaMaRA: {A} Task Management and Routing Algorithm for FANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4930--4942},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3300746},
	doi = {10.1109/TMC.2023.3300746},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BartoliniCMP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flying ad-hoc networks (FANETs) are a powerful tool for inspecting safety-critical scenarios, including post-disaster areas or military fields, where they ensure prompt area monitoring and fast detection of events of interest. However, wide area deployment of FANETs requires fast and reliable communications among devices and their base station to ensure prompt intervention upon detection of anomalies. Existing long-range communication technologies are inadequate to meet the data rate requirements and delay constraints of safety-critical applications. Previous solutions to enable ad-hoc communications in mobile networks also fall short of exploiting the controllable mobility of FANETs. To face this challenge, we formulate the connected deployment problem, where we require the FANET to dynamically create connected coverage formations to ensure multi-hop low-latency communications while performing the monitoring task. We show that addressing the above problem under the joint requirement of maximizing event coverage is NP-hard. We propose a joint Task Management and Routing Algorithm called TaMaRA, a polynomial-time solution based on a two-phase approximation of the problem. By means of extensive simulations and real field experiments we show that our approach outperforms existing solutions in terms of monitoring accuracy and system responsiveness.}
}


@article{DBLP:journals/tmc/HedhlyAAS24,
	author = {Wafa Hedhly and
                  Osama Amin and
                  Mohamed{-}Slim Alouini and
                  Basem Shihada},
	title = {Intelligent Reflecting Surfaces Assisted Hyperloop Wireless Communication
                  Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4943--4955},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3299792},
	doi = {10.1109/TMC.2023.3299792},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HedhlyAAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hyperloop or evacuated-tube transportation is a groundbreaking technology that can reach aircraft-like speeds. Its uncommon configuration of a steel-made tube isolates the moving pod from the outside wireless world. In this work, we propose an inner tube network architecture that can provide the moving pod with a seamless and reliable connection. The proposed network consists of successive access points (APs) and intelligent reflecting surfaces (IRS) strategically positioned along the tube and connected to a control station (CS) through wired links to improve the wireless cell coverage. The subsequent entities of the proposed design are intelligently placed along the movement path, steering the transmitted beam towards the receiver, while a soft handover is achieved between consecutive cells. First, we optimize each IRS's positioning and phase shifts to maximize cell coverage thanks to the IRS scanning abilities while keeping a minimum quality of service. Afterward, we exploit the centralized operation at the CS and design a soft handover scheme for the inner-tube wireless network. The numerical results show that the proposed approach provides good cell coverage and spectral efficiency with different IRS scanning ranges.}
}


@article{DBLP:journals/tmc/HassanPTSHH24,
	author = {Sheikh Salman Hassan and
                  Yu Min Park and
                  Yan Kyaw Tun and
                  Walid Saad and
                  Zhu Han and
                  Choong Seon Hong},
	title = {Satellite-Based {ITS} Data Offloading {\&} Computation in 6G Networks:
                  {A} Cooperative Multi-Agent Proximal Policy Optimization {DRL} With
                  Attention Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4956--4974},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3300314},
	doi = {10.1109/TMC.2023.3300314},
	timestamp = {Mon, 15 Apr 2024 08:25:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HassanPTSHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of intelligent transportation systems (ITS) has led to increasing demand for diverse network applications. However, conventional terrestrial access networks (TANs) are inadequate in accommodating various applications for remote ITS nodes, i.e., airplanes and ships. In contrast, satellite access networks (SANs) offer supplementary support for TANs, in terms of coverage flexibility and availability. In this study, we propose a novel approach to ITS data offloading and computation services based on SANs. We use low-Earth orbit (LEO) and cube satellites (CubeSats) as independent mobile edge computing (MEC) servers that schedule the processing of data generated by ITS nodes. To optimize offloading task selection, computing, and bandwidth resource allocation for different satellite servers, we formulate a joint delay and rental price minimization problem that is mixed-integer non-linear programming (MINLP) and NP-hard. We propose a cooperative multi-agent proximal policy optimization (Co-MAPPO) deep reinforcement learning (DRL) approach with an attention mechanism to deal with intelligent offloading decisions. We also decompose the remaining subproblem into three independent subproblems for resource allocation and use convex optimization techniques to obtain their optimal closed-form analytical solutions. We conduct extensive simulations and compare our proposed approach to baselines, resulting in performance improvements of 9.9%, 5.2%, and 4.2%, respectively.}
}


@article{DBLP:journals/tmc/CaoZSYWMTT24,
	author = {Zijian Cao and
                  Dong Zhao and
                  Hanxing Song and
                  Haitao Yuan and
                  Qiyue Wang and
                  Huadong Ma and
                  Jianjun Tong and
                  Chang Tan},
	title = {F{\textdollar}{\^{}}\{3\}{\textdollar}3VeTrac: Enabling Fine-Grained,
                  Fully-Road-Covered, and Fully-Individual- Penetrative Vehicle Trajectory
                  Recovery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4975--4991},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301871},
	doi = {10.1109/TMC.2023.3301871},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoZSYWMTT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Obtaining urban-scale vehicle trajectories is essential to understand urban mobility and benefits various downstream applications. The mobility knowledge obtained from existing vehicle trajectory sensing techniques is typically incomplete. To fill the gap, we propose F^{3}VeTrac, an efficient deep-learning-based vehicle trajectory recovery system that utilizes complementary characteristics of the Camera Surveillance System and the Vehicle Tracking System to obtain fine-grained, fully-road-covered, and fully-individual-penetrative (F^{3}) trajectories. F^{3}VeTrac utilizes five well-designed modules to model the co-occurrence relationships hidden in both coarse-grained and fine-grained trajectories from the two complementary sensing systems and fuse them to recover the coarse-grained trajectories. We implement and evaluate F^{3}VeTrac with two real-world datasets from over 100 million regular vehicle trajectories and 16 million commercial vehicle trajectories in two cities of China, together with an on-field case study based on 251 regular vehicle trajectories collected by 17 volunteers, demonstrating its great advantages over six state-of-the-art alternative schemes. Moreover, we present a downstream application of F^{3}VeTrac for traffic condition estimation, which obtains obvious performance gains.}
}


@article{DBLP:journals/tmc/ShangYYL24,
	author = {Fei Shang and
                  Panlong Yang and
                  Yubo Yan and
                  Xiang{-}Yang Li},
	title = {Contactless and Fine-Grained Liquid Identification Utilizing Sub-6
                  GHz Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {4992--5008},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3300356},
	doi = {10.1109/TMC.2023.3300356},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShangYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The existing RF-based liquid identification systems usually rely on prior knowledge, such as pre-build database or the material and width information of the vessel. Furthermore, existing methods may not work in scenarios where the height of liquid is smaller than that of antenna. In this paper, we proposes LiqRay^+, a contactless system which can identify liquids in a fine-grained level without prior knowledge. To remove the effect of vessel, we build a dual-antenna model and craft a relative frequency response factor, exploring diversity of the permittivity in frequency domain. To eliminate the effect of different height, we devise the electric field distribution model at the receiving antenna, solving the unknown heights via spatio-differential model. Among eight different solvents, LiqRay^+ can identify alcohol solutions with a concentration difference of 1% with 92.9% accuracy. Even if the liquid height is about 4 cm, which is fairly lower than that of most antennas’ heights, the accuracy is more than 85%.}
}


@article{DBLP:journals/tmc/ZhouQTSDGL24,
	author = {Jianshan Zhou and
                  Guixian Qu and
                  Daxin Tian and
                  Zhengguo Sheng and
                  Xuting Duan and
                  Yong Liang Guan and
                  Victor C. M. Leung},
	title = {Joint Energy-Efficiency Communication Optimization and Perimeter Traffic
                  Flow Control for Multi-Region {LTE-V2V} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5009--5026},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3300311},
	doi = {10.1109/TMC.2023.3300311},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouQTSDGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy-efficiency (EE) optimization of long-term evolution (LTE) networks dedicated to vehicle-to-vehicle communications (LTE-V2V) is critical for connected vehicles. In this paper, we integrate perimeter control methodologies from transportation science into EE optimization to make vehicular communications adaptive to temporal-spatial dynamics of macroscopic traffic flows in multiple urban regions. Specifically, we develop a hierarchical framework of joint LTE-V2V EE optimization and perimeter traffic flow control. Its goal is to minimize the total traffic network delay, defined as the integral of the vehicle accumulations in the urban regions over a prediction horizon time, meanwhile maximizing the energy efficiency of the LTE-V2V communications in the same regions. We propose a model predictive perimeter controller at a low level, using a macroscopic fundamental diagram (MFD) to capture the relationship between the traffic density and the outflow of each urban region. We also propose a high-level EE optimization model and an iterative algorithm, considering the multi-region coordinated traffic dynamics, to jointly optimize vehicular transmission power and beacon frequency. Simulation results validate our proposed models and show that our method outperforms the latest solutions by improving at least 9.57% EE of the multiple regions. Our method can also provide 27.69% improvement in resource utilization fairness, indicating a fairer EE performance distribution among these regions.}
}


@article{DBLP:journals/tmc/ZhangLSX24,
	author = {Letian Zhang and
                  Zhuo Lu and
                  Linqi Song and
                  Jie Xu},
	title = {CrossVision: Real-Time On-Camera Video Analysis via Common RoI Load
                  Balancing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5027--5039},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301391},
	doi = {10.1109/TMC.2023.3301391},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLSX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart cameras with on-device deep learning inference capabilities are enabling distributed video analytics at the data source without sending raw video data over the often unreliable and congested wireless network. However, how to unleash the full potential of the computing power of the camera network requires careful coordination among the distributed cameras, catering to the uneven workload distribution and the heterogeneous computing capabilities. This paper presents CrossVision, a distributed framework for real-time video analytics, that retains all video data on cameras while achieving low inference delay and high inference accuracy. The key idea behind CrossVision is that there is a significant information redundancy in the video content captured by cameras with overlapped Field-of-Views (FoVs), which can be exploited to reduce inference workload as well as improve inference accuracy between correlated cameras. CrossVision consists of three main components to realize its function: a Region-of-Interest (RoI) Matcher that discovers video content correlation based on a segmented FoV transformation scheme; a Workload Balancer that implements a randomized workload balancing strategy based on a bulk-queuing analysis, taking into account the cameras’ predicted future workload arrivals; an Accuracy Guard that ensures that the inference accuracy is not sacrificed as redundant information is discarded. We evaluate CrossVision in a hardware-augmented simulator and on real-world cross-camera datasets, and the results show that CrossVision is able to significantly reduce inference delay while improving the inference accuracy compared to a variety of baseline approaches.}
}


@article{DBLP:journals/tmc/ZouLHW24,
	author = {Xiang Zou and
                  Jianwei Liu and
                  Jinsong Han and
                  Zhi Wang},
	title = {Exploring Polarization in Hybrid Modulation for LED-Camera Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5040--5053},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3300315},
	doi = {10.1109/TMC.2023.3300315},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZouLHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of LED infrastructure and the camera on smartphone, LED-Camera visible light communication (VLC) has become a realistic and promising technology. However, the existing LED-Camera VLC has limited throughput due to the sampling manner of camera. In this paper, by introducing a polarization dimension, we propose a hybrid modulation scheme with LED and polarization signals to boost throughput. Nevertheless, directly mixing LED and polarized signals may suffer from channel conflict. We exploit well-designed packet structure and Symmetric Return-to-Zero Inverted (SRZI) coding to overcome the conflict. In addition, in the demodulation of hybrid signal, we alleviate the noise of polarization on the LED signals by the polarization background subtraction. We further propose a pixel-free approach to correct the perspective distortion caused by the shift of view angle by adding polarizers around the liquid crystal array. We build a prototype of this hybrid modulation scheme using off-the-shelf optical components. We enhance the basic version (Zou et al. 2023) of preliminary work by analyzing the performance with FSK modulation. Extensive experimental results demonstrate that the hybrid modulation scheme can achieve reliable communication, achieving 13.4 kbps throughput, which is 400 \\% of the existing state-of-the-art LED-Camera VLC.}
}


@article{DBLP:journals/tmc/ChenTZK24,
	author = {Hong Chen and
                  Terence D. Todd and
                  Dongmei Zhao and
                  George Karakostas},
	title = {Wireless and Service Allocation for Mobile Computation Offloading
                  With Task Deadlines},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5054--5068},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301577},
	doi = {10.1109/TMC.2023.3301577},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenTZK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile computation offloading (MCO), mobile devices (MDs) can choose to either execute tasks locally or have them executed on a remote edge server (ES). This paper addresses the problem of assigning the wireless communication bandwidth and the ES capacity used for the task execution, so that task completion time constraints are satisfied. The objective is to minimize the average power consumption of the mobile devices, subject to a cost budget constraint for obtaining the communication and computation resources. The paper includes contributions for both soft and hard task completion deadline constraints. The problems are first formulated as mixed integer nonlinear programs (MINLPs). Approximate solutions are then obtained by decomposing the problems into a collection of convex subproblems that can be efficiently solved. Results are presented that demonstrate the quality of the proposed solutions, which can achieve near optimum performance over a wide range of system parameters.}
}


@article{DBLP:journals/tmc/ZhangCCMXDLW24,
	author = {Qiyang Zhang and
                  Xiangying Che and
                  Yijie Chen and
                  Xiao Ma and
                  Mengwei Xu and
                  Schahram Dustdar and
                  Xuanzhe Liu and
                  Shangguang Wang},
	title = {A Comprehensive Deep Learning Library Benchmark and Optimal Library
                  Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5069--5082},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301973},
	doi = {10.1109/TMC.2023.3301973},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCCMXDLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying deep learning (DL) on mobile devices has been a notable trend in recent years. To support fast inference of on-device DL, DL libraries play a critical role as algorithms and hardware do. Unfortunately, no prior work ever dives deep into the ecosystem of modern DL libraries and provides quantitative results on their performance. In this paper, we first build a comprehensive benchmark that includes 6 representative DL libraries and 15 diversified DL models. Then we perform extensive experiments on 10 mobile devices, and the results reveal the current landscape of mobile DL libraries. For example, we find that the best-performing DL library is severely fragmented across different models and hardware, and the gap between DL libraries can be rather huge. In fact, the impacts of DL libraries can overwhelm the optimizations from algorithms or hardware, e.g., model quantization and GPU/DSP-based heterogeneous computing. Motivated by the fragmented performance of DL libraries across models and hardware, we propose an effective DL Library selection framework to obtain the optimal library on a new dataset that has been created. We evaluate the DL Library selection algorithm, and the results show that the framework at it can improve the prediction accuracy by about 10% than benchmark approaches on average.}
}


@article{DBLP:journals/tmc/LiuCWWXC24,
	author = {Shijia Liu and
                  Zhenghua Chen and
                  Min Wu and
                  Hao Wang and
                  Bin Xing and
                  Liangyin Chen},
	title = {Generalizing Wireless Cross-Multiple-Factor Gesture Recognition to
                  Unseen Domains},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5083--5096},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301501},
	doi = {10.1109/TMC.2023.3301501},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCWWXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain wireless sensing has always been challenging due to the sensitivity of wireless signals to various environmental factors, which we refer to as subdomains. However, current efforts are limited to cross-one-subdomain tasks requiring target domain data for model training or multiple receivers for data collection. Taking common gesture recognition as an application example, we attempt to demonstrate the feasibility of cross-multiple-subdomain wireless sensing in the more challenging domain generalization (DG) setting. It is possible to extract domain-invariant features from one or several source domain(s), thereby avoiding the need for multiple receivers or target domain data. We also propose an intelligent wireless data augmentation technique based on subdomain-guided perturbations, named WiSGP. Specifically, the independent domain model generates perturbations in the direction of the largest subdomain variations. Then, these subdomain-guided perturbations augment the gesture model's input to enable better domain-invariant feature extraction, even when various subdomains interact. Similarly, gesture-guided perturbations augment the domain model's input, resulting in more accurate subdomain-guided perturbations and minimal gesture label changes. Extensive experiments have been conducted on three datasets collected from various NICs. In terms of room, location, orientation, and user subdomains, WiSGP exhibits excellent accuracy, generalizability, and portability for both cross-one-subdomain and cross-multiple-subdomain tasks.}
}


@article{DBLP:journals/tmc/ShokrnezhadTD24,
	author = {Masoud Shokrnezhad and
                  Tarik Taleb and
                  Patrizio Dazzi},
	title = {Double Deep Q-Learning-Based Path Selection and Service Placement
                  for Latency-Sensitive Beyond 5G Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5097--5110},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301506},
	doi = {10.1109/TMC.2023.3301506},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShokrnezhadTD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B&B-CCRA and WF-CCRA, based on the Branch & Bound and Water-Filling algorithms to solve it when the system is fully known. Then, for partially known systems, a Double Deep Q-Learning (DDQL) architecture is designed. Numerical simulations show that B&B-CCRA optimally solves the problem, whereas WF-CCRA delivers near-optimal solutions in a substantially shorter time. Furthermore, it is demonstrated that DDQL-CCRA obtains near-optimal solutions in the absence of request-specific information.}
}


@article{DBLP:journals/tmc/AoXLTW24,
	author = {Eerdemotai Ao and
                  Shiqing Xin and
                  Feng Li and
                  Changhe Tu and
                  Wenping Wang},
	title = {Efficient Capacity Constrained Assignment for Dynamic Network Coverage},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5111--5129},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3305714},
	doi = {10.1109/TMC.2023.3305714},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AoXLTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fast development of the 5G wireless communications, the Internet of Things (IoT) becomes a hot research topic. Unmanned aerial vehicles (UAVs), due to the high mobility and low labor cost, have a big potential to be applied in the future IoT communication networks, e.g., data collection in remote areas. In this paper, we take a UAV as a monitor and an IoT device as an agent, and study how to utilize UAVs to establish network coverage and enhance the overall performance. Given ground agents and aerial monitors, each agent needs to be supervised by one monitor that can at most take charge of certain amount, while such assignment should guarantee the required service quality. This is much different from the conventional assumption that each monitor owns exactly a fixed number of agents without considering sensing quality under limited transmit power. Suppose that a monitor supervises an agent with a cost as the negative value of transmission rate in Rician fading, we then maximize the sum of transmission induced by every monitor-agent connection constrained with workload capacity for each monitor. To achieve the above goals, we first present a fast algorithm to report the least-cost assignment plan. Then, we seek for the minimum number of monitors to maintain the required service quality. Last, we discuss the assignment problem in the scenario of dynamic agents and dynamic monitors. We also give a set of strategies on how to initialize assignment, optimize monitor locations and manage power consumption. Extensive experimental results on both simulated datasets and real-life traffic data demonstrate our effectiveness and high performance.}
}


@article{DBLP:journals/tmc/MehmetiPK24,
	author = {Fidan Mehmeti and
                  Thomas F. La Porta and
                  Wolfgang Kellerer},
	title = {Efficient Resource Allocation With Provisioning Constrained Rate Variability
                  in Cellular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5130--5149},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3303097},
	doi = {10.1109/TMC.2023.3303097},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MehmetiPK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While LTE networks are known to provide relatively high data rates, reaching values as high as tens of Mbps, these rates exhibit considerable variability over time. The rate variability hurts especially the performance of applications and services that require stable data rates, such as real-time video streaming, online gaming, virtual reality, augmented reality, etc. 5G emerged as a solution to this as well as to many other problems. However, it has been shown that strict constant data rates come at the cost of underutilized network resources, resulting in inefficient operation of cellular networks. Therefore, a tradeoff between the data rate stability, important to cellular users, and the efficient utilization of resources, important to network operators, needs to be taken into account. To that end, in this paper, we consider the problem of allocating all the network resources to cellular users in such a way that it provides as high a data rate as possible to all users while limiting the rate variation within tight bounds. We do this for different scenarios in terms of the user activity, user type, and the nature of the policy. First, we consider the case of static allocation policy, irrespective of channel conditions, for users that are always active. Then, for these same users, we look at the case when resources are allocated dynamically over time. Second, we consider static and dynamic policies for users that are only intermittently active. Third, we consider the case with users having different Service Level Agreements (SLAs) with the cellular operator. Furthermore, we run extensive simulations with input parameters from real traces. Results show that allocating the resources dynamically improves performance in terms of data rates over static allocation mechanisms by an additional 10%, and that allowing a slightly higher outage in not complying with the guaranteed data rate further increases the user's throughput by at least 20%.}
}


@article{DBLP:journals/tmc/HeHLTWY24,
	author = {Yu He and
                  Guangjie Han and
                  Aohan Li and
                  Tarik Taleb and
                  Chenyang Wang and
                  Hao Yu},
	title = {A Federated Deep Reinforcement Learning-Based Trust Model in Underwater
                  Acoustic Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5150--5161},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3301825},
	doi = {10.1109/TMC.2023.3301825},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeHLTWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater acoustic sensor networks (UASNs) have been widely deployed in many areas, such as marine ranching, naval applications, and marine disaster warning systems. The security of UASNs, particularly insider threats, is of growing concern. Internal attacks carried out via compromised normal nodes are more damaging and stealthy than external attacks, such as signal stealing, data decryption, and identity forgery. As a security mechanism for internal threat detection based on interaction data, trust models have proven to enhance the security of UASNs. However, traditional trust models lack sufficient scalability when faced with movable underwater devices, heterogeneous network environments, and variable attack patterns. Therefore, in this paper, a novel trust model based on federated deep reinforcement learning is proposed for UASNs. First, the evidence acquisition mechanism, including communication, energy, and data evidence, is improved based on existing ones to better accommodate the topological dynamics of UASNs. Second, acquired trust evidence is fed into the corresponding deep reinforcement learning-based local trust model to accomplish trust prediction and model training. Finally, a federated learning-based update method periodically aggregates and updates the parameters of the local models. The experimental results prove that the proposed scheme exhibits satisfactory performance in terms of improving trust prediction accuracy and energy efficiency.}
}


@article{DBLP:journals/tmc/YangZYZZHZS24,
	author = {Dong Yang and
                  Weiting Zhang and
                  Qiang Ye and
                  Chuan Zhang and
                  Ning Zhang and
                  Chuan Huang and
                  Hongke Zhang and
                  Xuemin Shen},
	title = {DetFed: Dynamic Resource Scheduling for Deterministic Federated Learning
                  Over Time-Sensitive Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5162--5178},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3303017},
	doi = {10.1109/TMC.2023.3303017},
	timestamp = {Mon, 22 Jul 2024 08:23:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangZYZZHZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a three-layer (i.e., device, field, and factory layers) deterministic federated learning (FL) framework, named DetFed, which accelerates collaborative learning process for ultra-reliable and low-latency industrial Internet of Things (IoT) via integrating 6G-oriented Time-sensitive Networks (TSN). Utilizing dispersive local data, industrial IoT devices distributively train a deep neural network (DNN) model, and the updated model parameters are aggregated at their associated field servers every round or at a centralized factory server every a few rounds. Aiming at optimizing the learning accuracy of FL without affecting the co-transmission of burst traffic (e.g., safety-critical traffic), an integrated TSN is considered to establish connections among the three layers, where a cyclic queuing and forwarding mechanism is deployed in each switch to support deterministic model parameter transmission with microsecond-level delay and near-zero packet loss requirements. To improve the FL performance, we formulate a multi-objective stochastic optimization problem to simultaneously maximize the scheduling success ratio and learning accuracy while satisfying the deterministic requirements of delay, jitter, and packet loss. Since the objective function is implicit and the available time slots of the considered TSN in each FL round are temporally correlated, the problem is difficult to solve in real time. Therefore, we transform the problem into a Markov decision process formulation and propose a dynamic resource scheduling algorithm, based on deep reinforcement learning, to make optimal resource scheduling decisions while adapting to device heterogeneity and network dynamics. Experimental results based on real-world dataset demonstrate that the proposed DetFed significantly accelerates FL convergence and improves learning accuracy as compared to state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/GuoGWTX24,
	author = {Tao Guo and
                  Song Guo and
                  Junxiao Wang and
                  Xueyang Tang and
                  Wenchao Xu},
	title = {PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead
                  of Models - Federated Learning in Age of Foundation Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5179--5194},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3302410},
	doi = {10.1109/TMC.2023.3302410},
	timestamp = {Mon, 21 Oct 2024 15:07:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoGWTX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quick global aggregation of effective distributed parameters is crucial to federated learning (FL), which requires adequate bandwidth for parameters communication and sufficient user data for local training. Otherwise, FL may cost excessive training time for convergence and produce inaccurate models. In this paper, we propose a brand-new FL framework, PromptFL, that replaces the federated model training with the federated prompt training, i.e., let federated participants train prompts instead of a shared model, to simultaneously achieve the efficient global aggregation and local training on insufficient data by exploiting the power of foundation models (FM) in a distributed way. PromptFL ships an off-the-shelf FM, i.e., CLIP, to distributed clients who would cooperatively train shared soft prompts based on very few local data. Since PromptFL only needs to update the prompts instead of the whole model, both the local training and the global aggregation can be significantly accelerated. And FM trained over large scale data can provide strong adaptation capability to distributed users tasks with the trained soft prompts. We empirically analyze the PromptFL via extensive experiments, and show its superiority in terms of system feasibility, user privacy, and performance.}
}


@article{DBLP:journals/tmc/CaiLZXTL24,
	author = {Qixuan Cai and
                  Xiulong Liu and
                  Kaixuan Zhang and
                  Xin Xie and
                  Xinyu Tong and
                  Keqiu Li},
	title = {{ACF:} An Adaptive Compression Framework for Multimodal Network in
                  Embedded Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5195--5211},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3303350},
	doi = {10.1109/TMC.2023.3303350},
	timestamp = {Sat, 04 May 2024 10:56:41 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiLZXTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ubiquitous Internet-of-Things (IoT) devices generate vast amounts of multimodal data, and the deep multimodal fusion network (DMFN) is a promising technology for processing multimodal data. Deploying DMFNs locally on embedded IoT devices is a profitable way to provide privacy-preserving and robust sensing services. However, the current compression methods suffer from the following limitations: First, they are designed based on unimodal networks or specific model structures. Hence, it is hard to extend these methods to diverse DMFNs; Second, existing works never relate their efforts to disparate computational demands of multimodal data and modalities. Easy samples and redundant modalities consume the same computational resources as powerful modalities and complex samples. We propose an Adaptive Compression Framework (ACF) for DMFNs to address those challenges. It enables input-dependent runtime compression locally on resource-constrained embedded devices. Specifically, we propose an offline model transformation module to upgrade the static network with two kinds of dynamic components to support online structural adjustment. Then we design a lightweight policy network to generate multi-granularity and data-dependent compression strategies for different model parts. Finally, we evaluate ACF on four DMFNs across three embedded platforms. Compared with the best results of the existing schemes, ACF obtains up to 2.61\\times\nlatency reduction and 2.30\\times\nenergy consumption reduction, with up to 3.57% accuracy improvement.}
}


@article{DBLP:journals/tmc/YouZCSF24,
	author = {Lizhao You and
                  Xinbo Zhao and
                  Rui Cao and
                  Yulin Shao and
                  Liqun Fu},
	title = {Broadband Digital Over-the-Air Computation for Wireless Federated
                  Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5212--5228},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3304652},
	doi = {10.1109/TMC.2023.3304652},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YouZCSF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article presents the first orthogonal frequency-division multiplexing(OFDM)-based digital over-the-air computation (AirComp) system for wireless federated edge learning, where multiple edge devices transmit model data simultaneously using non-orthogonal OFDM subcarriers, and the edge server aggregates data directly from the superimposed signal. Existing analog AirComp systems often assume perfect phase alignment via channel precoding and utilize uncoded analog transmission for model aggregation. In contrast, our digital AirComp system leverages digital modulation and channel codes to overcome phase asynchrony, thereby achieving accurate model aggregation for phase-asynchronous multi-user OFDM systems. To realize a digital AirComp system, we develop a medium access control (MAC) protocol that allows simultaneous transmissions from different users using non-orthogonal OFDM subcarriers, and put forth joint channel decoding and aggregation decoders tailored for convolutional and LDPC codes. To verify the proposed system design, we build a digital AirComp prototype on the USRP software-defined radio platform, and demonstrate a real-time LDPC-coded AirComp system with up to four users. Trace-driven simulation results on test accuracy versus SNR show that: 1) analog AirComp is sensitive to phase asynchrony in practical multi-user OFDM systems, and the test accuracy performance fails to improve even at high SNRs; 2) our digital AirComp system outperforms two analog AirComp systems at all SNRs, and approaches the optimal performance when SNR \\geq\n6 dB for two-user LDPC-coded AirComp, demonstrating the advantage of digital AirComp in phase-asynchronous multi-user OFDM systems.}
}


@article{DBLP:journals/tmc/GeZQWW24,
	author = {Shuxin Ge and
                  Xiaobo Zhou and
                  Tie Qiu and
                  Guobin Wu and
                  Xiaotong Wang},
	title = {Towards Supply-Demand Equilibrium With Ridesharing: An Elastic Order
                  Dispatching Algorithm in MoD System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5229--5244},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3303090},
	doi = {10.1109/TMC.2023.3303090},
	timestamp = {Mon, 10 Feb 2025 14:42:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GeZQWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobility on demand (MoD) systems utilize ridesharing, i.e., multiple orders with high associating utility share a single vehicle, to reduce carbon footprint and alleviate traffic pressure. Existing methods mainly promote ridesharing by flocking multiple orders to the minimum required vehicles. However, supply-demand variations may aggregate undersupply in the long run and affect the order completion rate. Meanwhile, it is difficult to accurately estimate associating utility among ridesharing orders with lane-level features, such as traffic flow. To fill this gap, we propose ERShare, an elastic order dispatching algorithm to maximize the order completion rate in the MoD system. First, the ridesharing order dispatching problem is formulated as an offline optimization problem, and then it is proved that the order completion rate is maximized when the MoD system achieves long-term supply-demand equilibrium. Next, a dummy order/vehicle generation method is proposed to generate dummies as a spinner to achieve supply-demand equilibrium elastically. Also, a lane-level ridesharing rule is designed to accurately estimate the associating utility based on an order association graph. Subsequently, a dummy-based order dispatching algorithm is proposed to find the optimal dispatching decisions. Finally, the simulations on real-world data validate the superiority of ERShare over state-of-the-art solutions regarding order completion rate.}
}


@article{DBLP:journals/tmc/ChenLAWS24,
	author = {Yali Chen and
                  Min Liu and
                  Bo Ai and
                  Yuwei Wang and
                  Sheng Sun},
	title = {Adaptive Bitrate Video Caching in UAV-Assisted {MEC} Networks Based
                  on Distributionally Robust Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5245--5259},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3304624},
	doi = {10.1109/TMC.2023.3304624},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLAWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To alleviate the pressure on the ground base station (BS) from intensive video requests, unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has become a promising and flexible solution. The UAV carries a MEC server to provide caching and transcoding services for adaptive bitrate video streaming, which can reduce duplicate transmissions of the BS and the content acquisition latency of users, while improving the flexibility of video delivery. However, considering the uncertainty of user requests and content popularity distribution, improving the robustness of video caching is a challenge to promote practical applications. Thus, by integrating caching and transcoding on the UAV, as well as backhaul retrieving, we study the bitrate-aware video caching and processing with uncertain popularity distribution. Then, the problem of joint cache placement and video delivery scheduling under the worst-case distribution is formulated to minimize the total expected system latency with energy consumption constrained. Specifically, we use \\zeta-structure probability metrics to characterize the uncertainty and construct confidence sets of arrival distribution. Furthermore, a distributionally robust latency optimization algorithm based on convex optimization theory is designed to obtain a robust solution. Finally, we conduct extensive simulations using real-world datasets to evaluate the effectiveness and robustness of the proposed scheme.}
}


@article{DBLP:journals/tmc/ZhaiWYLZC24,
	author = {Zhiwei Zhai and
                  Qiong Wu and
                  Shuai Yu and
                  Rui Li and
                  Fei Zhang and
                  Xu Chen},
	title = {FedLEO: An Offloading-Assisted Decentralized Federated Learning Framework
                  for Low Earth Orbit Satellite Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5260--5279},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3304988},
	doi = {10.1109/TMC.2023.3304988},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaiWYLZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth orbit (LEO) satellites enable complex Earth observation tasks (e.g., remote sensing and cooperative monitoring) by leveraging large-scale satellite-generated Earth imageries and state-of-the-art machine learning (ML) techniques. However, due to restricted downlink bandwidth and spotty connectivity, it is infeasible for the satellites to transmit all the imageries to ground stations for ML model training. To address this issue, we use federated learning (FL) to mitigate the significant overhead of raw data transmission only by enabling model parameter exchange. Traditional FL requires a central server for model parameter aggregation, which is impractical for distributed LEO satellite constellation due to the difficulty of identifying a suitable central satellite. To tackle such challenge, we take the unique topological characteristics of the LEO satellite constellation to design a decentralized FL framework that enables efficient model aggregation in LEO satellite networks without a central server. The framework can avoid the reliability and communication bandwidth problems of the central server in centralized FL. To mitigate the straggler effect and address the statistical heterogeneity, we then propose a novel offloading framework for decentralized FL in LEO satellite networks to aid the collaboration among multiple satellites for resource sharing. Based on it, we derive a satellite-centric threshold-based offloading strategy and a system-wide greedy-based iterative offloading decision making algorithm, in order to achieve delay and accuracy optimization under the computation and communication power constraints. Theoretical analysis demonstrates that the proposed framework contributes to the high training performance of the global model. Extensive experiments based on realistic datasets show that the proposed framework can reduce the system delay by up to 41% on average and improve the global model accuracy by up to 9.39% compared with benchmark policies.}
}


@article{DBLP:journals/tmc/XiaYLXM24,
	author = {Shichao Xia and
                  Zhixiu Yao and
                  Yun Li and
                  Zhitong Xing and
                  Shiwen Mao},
	title = {Distributed Computing and Networking Coordination for Task Offloading
                  Under Uncertainties},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5280--5294},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3305013},
	doi = {10.1109/TMC.2023.3305013},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaYLXM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The multi-access edge computing (MEC) and ultra-dense network (UDN) are regarded as essential and complementary technologies in the age of Internet of Things (IoT). Deploying MEC servers at the macro-cell and small-cell stations can significantly improve user experience as well as increase network capacity. Nevertheless, there still remain many obstacles in practical MEC-enabled UDNs. Among them, a unique challenge is how to coordinate computing and networking to fit the diverse offloading demands of IoT applications in dynamic network environments. To this end, this paper first investigates a distributed delay-constrained computation offloading methodology based on computing and networking coordination in the UDN. An extended game-theoretic approach based on the Lyapunov optimization theory is designed to achieve adaptive task offloading and computing power management in time-varying environments. Furthermore, considering the uncertainty in users’ mobility and limited edge resources, distributed two-stage and multi-stage stochastic programming algorithms under various uncertainties are proposed. The proposed algorithms take posterior recourse actions to compensate for inaccurate predicted network information. Extensive simulations validate the effectiveness and rationality of the proposed algorithms and their superior performance over several benchmark schemes.}
}


@article{DBLP:journals/tmc/XuZWDXL24,
	author = {Jia Xu and
                  Kaijun Zhou and
                  Sixu Wu and
                  Haipeng Dai and
                  Lijie Xu and
                  Linfeng Liu},
	title = {Robust Fault-Tolerant Placement of Wireless Chargers for Directional
                  Charging},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5295--5309},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3304658},
	doi = {10.1109/TMC.2023.3304658},
	timestamp = {Thu, 11 Apr 2024 16:57:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuZWDXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Power Transmission (WPT) has been widely used to replenish energy for wireless rechargeable sensor networks. This paper concerns the fundamental issue of robust fault-tolerant placement of wireless chargers for directional charging. Following the general directional charging model, we formulate the Charger Placement for Robust Coverage (CPRC) problem, which has continuous and infinite constraints, for resisting the wireless charger failure. We transform the problem to the equivalent integer program problem without performance loss by area partition and dominating strategy extraction. We show that the greedy algorithm achieves the logarithmic approximation ratio. We further formulate the Charger Placement for Robust Utility (CPRU) problem for resisting the sensor node failure. This problem also has continuous and infinite constraints. We transform the problem to the combinational optimization problem with finite strategy space through the techniques of charging power approximation, area discretization and dominating strategy extraction. We present the algorithm, which utilizes the combination of binary search and greedy algorithm, to solve the CPRU problem. We conduct both simulations and field experiments to validate our theoretical results. The simulation results show that the proposed algorithms for CPRC and CPRU can outperform comparison algorithms by at least 17.48% and 21.15%, respectively.}
}


@article{DBLP:journals/tmc/RenLCLYCYL24,
	author = {Yanzhi Ren and
                  Siyi Li and
                  Chen Chen and
                  Hongbo Liu and
                  Jiadi Yu and
                  Yingying Chen and
                  Haomiao Yang and
                  Hongwei Li},
	title = {Robust Indoor Location Identification for Smartphones Using Echoes
                  From Dominant Reflectors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5310--5326},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3307695},
	doi = {10.1109/TMC.2023.3307695},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenLCLYCYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The indoor location awareness has drawn increasing attention as the mobile apps are used extensively in our daily lives. Existing indoor localization solutions either require a pre-installed infrastructure or can only achieve room-level accuracy, which could not provide a function-location service for mobile devices. In this work, we propose a new active sensing system that enables smartphones to identify some pre-defined indoor locations robustly without requiring any additional sensors or pre-installed infrastructure. The main idea behind our system is to utilize the acoustic signatures, which are derived from the mobile device by emitting a beep signal and selecting its echoes created by dominant reflectors, as the robust fingerprint for location identification. Given the microphone samplings, our system designs a correlation based technique to accurately detect the beginning points of echoes from the received beep signal. To achieve a robust location identification, we develop a new echo selection scheme to select echoes created by dominant reflectors by exploiting the relationships between propagation delays of different orders of echoes. To deal with the variable number of selected echoes, our location identification component then derives histograms from selected echoes and uses the one-against-all SVM classifiers to determine the current location. Our experimental results show that our proposed system is accurate and robust for location identification under various real-world scenarios.}
}


@article{DBLP:journals/tmc/XuMXCLX24,
	author = {Yang Xu and
                  Zhenguo Ma and
                  Hongli Xu and
                  Suo Chen and
                  Jianchun Liu and
                  Yinxing Xue},
	title = {FedLC: Accelerating Asynchronous Federated Learning in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5327--5343},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3307610},
	doi = {10.1109/TMC.2023.3307610},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuMXCLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has been widely adopted to process the enormous data in the application scenarios like Edge Computing (EC). However, the commonly-used synchronous mechanism in FL may incur unacceptable waiting time for heterogeneous devices, leading to a great strain on the devices’ constrained resources. In addition, the alternative asynchronous FL is known to suffer from the model staleness, which will lead to performance degradation of the trained model, especially on non-i.i.d. data. In this paper, we design a novel asynchronous FL mechanism, named FedLC, to handle the non-i.i.d. issue in EC by enabling the local collaboration among edge devices. Specifically, apart from uploading the local model directly to the server, each device will transmit its gradient to the other devices with different data distributions for local collaboration, which can improve the model generality. We theoretically analyze the convergence rate of FedLC and obtain the quantitative relationship between convergence bound and local collaboration. We design an efficient algorithm utilizing demand-list to determine the set of devices receiving gradients from each device. To handle the model staleness, we further assign different learning rates for various devices according to their participation frequency. The extensive experimental results demonstrate the effectiveness of our proposed mechanism.}
}


@article{DBLP:journals/tmc/HuWSXLCA24,
	author = {Guojie Hu and
                  Qingqing Wu and
                  Jiangbo Si and
                  Kui Xu and
                  Zan Li and
                  Yunlong Cai and
                  Naofal Al{-}Dhahir},
	title = {STAR-RIS-Assisted Information Surveillance Over Suspicious Multihop
                  Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5344--5365},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3307392},
	doi = {10.1109/TMC.2023.3307392},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuWSXLCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless information surveillance has received widespread attention due to the urgency of monitoring growing suspicious communications. This paper considers a challenging surveillance scenario, where the monitor (E) intends to eavesdrop the suspicious multihop communications from a long distance to ensure concealment, leading to the eavesdropping condition undesirable. To tackle this challenging, we propose a novel simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-assisted surveillance strategy, where the STAR-RIS, acts as a “bridge”, is deliberately deployed between the suspicious system and E, to adaptively transmit and reflect the suspicious signal and E's jamming signal, and then facilitate E's eavesdropping. Specifically, we consider the adaptive rate transmission and the delay-limited transmission for the suspicious system, and accordingly maximize E's instantaneous and average eavesdropping rate, by jointly optimizing the passive transmission- and reflection-coefficient matrices at the STAR-RIS, the jamming set and jamming power allocations of E (across all hops). The optimization problems in both transmission modes include numerous integer and continuous variables and thus are highly non-convex. Nevertheless, we show by detailed analysis that the original problem in each mode can be solved by only considering two possible cases, where E and the STAR-RIS intend to enhance and reduce the suspicious transmission rate, respectively. More importantly, in each case, many of necessary prerequisites for achieving the optimal solution are first determined analytically. Armed with these, the optimization problem then can be solved by leveraging the successive convex approximation technique and the simple search. As demonstrated by simulation results, since our proposed strategy is adaptive in term of varying the suspicious transmission rate, it will achieve significant eavesdropping performance gain as compared to other competitive benchmarks.}
}


@article{DBLP:journals/tmc/FengCWWZH24,
	author = {Zhiying Feng and
                  Xu Chen and
                  Qiong Wu and
                  Wen Wu and
                  Xiaoxi Zhang and
                  Qianyi Huang},
	title = {FedDD: Toward Communication-Efficient Federated Learning With Differential
                  Parameter Dropout},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5366--5384},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3311188},
	doi = {10.1109/TMC.2023.3311188},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FengCWWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated Learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model parameter uploading ratios tailored to different clients’ heterogeneous conditions and also select the proper set of important model parameters for uploading subject to clients’ dropout rate constraints. Specifically, the dropout rate allocation is formulated as a convex optimization problem, taking system heterogeneity, data heterogeneity, and model heterogeneity among clients into consideration. The uploaded parameter selection strategy prioritizes on eliciting important parameters for uploading to speedup convergence. Furthermore, we theoretically analyze the convergence of the proposed FedDD scheme. Extensive performance evaluations demonstrate that the proposed FedDD scheme can achieve outstanding performances in both communication efficiency and model convergence, and also possesses a strong generalization capability to data of rare classes.}
}


@article{DBLP:journals/tmc/LiLMHXX24,
	author = {Yafei Li and
                  Huiling Li and
                  Baolong Mei and
                  Xin Huang and
                  Jianliang Xu and
                  Mingliang Xu},
	title = {Fairness-Guaranteed Task Assignment for Crowdsourced Mobility Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5385--5400},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310591},
	doi = {10.1109/TMC.2023.3310591},
	timestamp = {Wed, 11 Sep 2024 20:37:00 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLMHXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a new computing paradigm, crowdsourced mobility service is booming with the rapid development of sharing economy. In the typical crowdsourced mobility service, a large number of part-time workers perform the spatial tasks offered by the platform and share the benefits in proportion, thereby, the strategy of task assignment directly affects the level of revenue and fairness among workers. In order to balance the revenue and fairness of workers, in this paper we study a novel type of fairness-aware spatial crowdsourcing problem, namely Fairness-Guaranteed Task Assignment (FGTA), which aims to maximize the total revenue of workers at a certain level of fairness guarantee and that is proved to be NP-hard. To solve this problem, we propose an efficient game-theory based approach for task assignment, which makes use of the best-response framework to iteratively select the best strategy for each worker until a Nash equilibrium is reached. Inspired by the observation that tasks with similar spatial and temporal features can be assigned together to a worker, we propose a spatial-temporal grouping based optimization to further improve the efficiency of task assignment. Furthermore, to improve the quality of Nash equilibrium, we present an effective large neighborhood search based optimization that trains a DQN decision model as destroy operator to accelerate the convergence of optimal task assignment. Finally, extensive experiments conducted on two real-world datasets demonstrate that our proposed approaches achieve better effectiveness and efficiency than the state-of-the-arts.}
}


@article{DBLP:journals/tmc/UddinXCLYG24,
	author = {Md. Palash Uddin and
                  Yong Xiang and
                  Borui Cai and
                  Xuequan Lu and
                  John Yearwood and
                  Longxiang Gao},
	title = {{ARFL:} Adaptive and Robust Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5401--5417},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310248},
	doi = {10.1109/TMC.2023.3310248},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/UddinXCLYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a machine learning technique that enables multiple local clients holding individual datasets to collaboratively train a model, without exchanging the clients’ datasets. Conventional FL approaches often assign a fixed workload (local epoch) and step size (learning rate) to the clients during the client-side local model training and utilize all collaborating trained models’ parameters evenly during the server-side global model aggregation. Consequently, they frequently experience problems with data heterogeneity and high communication costs. In this paper, we propose a novel FL approach to mitigate the above problems. On the client side, we propose an adaptive model update approach that optimally allocates a needful number of local epochs and dynamically adjusts the learning rate to train the local model and regularizes the conventional objective function by adding a proximal term to it. On the server side, we propose a robust model aggregation strategy that potentially supplants the local outlier updates (models’ weights) prior to the aggregation. We provide the theoretical convergence results and perform extensive experiments on different data setups over the MNIST, CIFAR-10, and Shakespeare datasets, which manifest that our FL scheme surpasses the baselines in terms of communication speedup, test-set performance, and global convergence.}
}


@article{DBLP:journals/tmc/ZhaoQCXG24,
	author = {Yao Zhao and
                  Youyang Qu and
                  Feifei Chen and
                  Yong Xiang and
                  Longxiang Gao},
	title = {Data Integrity Verification in Mobile Edge Computing With Multi-Vendor
                  and Multi-Server},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5418--5432},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310532},
	doi = {10.1109/TMC.2023.3310532},
	timestamp = {Sat, 27 Apr 2024 21:29:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoQCXG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Mobile Edge Computing (MEC) paradigm reforms the way of data caching by motivating App vendors to store latency-sensitive data on distributed edge servers. In volatile MEC environments, ensuring Edge Data Integrity (EDI) is a major concern for App vendors. Existing EDI solutions only consider the scenario with a single App vendor and multiple edge servers, neglecting more complex multi-vendor and multi-server cases. If multiple App vendors check their data replicas cached on the same edge server simultaneously, integrity verification efficiency will drop exponentially. To mitigate this challenge, we make the first attempt to develop a Smart Inspection Algorithm (SIA) to pre-select unreliable data replicas for different App vendors in each verification round by jointly considering cache services’ QoS (Quality-of-Service) and data replicas’ unverified time. By implementing this approach, edge servers can merely verify the selected data replicas, greatly reducing computation and communication overheads in EDI verification. Theoretically, SIA can achieve \\mathcal {O}(n) expected time complexity. Supported by SIA, we expand the EDI problem in multi-vendor and multi-server MEC environments (referred to as the MVMS-EDI problem) and propose a smart contract-based approach entitled MVMS-SC to tackle the problem efficiently and impartially. We provide a rigorous theoretical analysis of the correctness, security, and efficiency of MVMS-SC. Both large-scale and small-scale experiments with real-world datasets are correspondingly performed on a single machine and a real platform to validate the superiority of MVMS-SC in terms of computation and communication efficiencies.}
}


@article{DBLP:journals/tmc/CaiJLYWFTKSS24,
	author = {Zekun Cai and
                  Renhe Jiang and
                  Xinlei Lian and
                  Chuang Yang and
                  Zhaonan Wang and
                  Zipei Fan and
                  Kota Tsubouchi and
                  Hill Hiroki Kobayashi and
                  Xuan Song and
                  Ryosuke Shibasaki},
	title = {Forecasting Citywide Crowd Transition Process via Convolutional Recurrent
                  Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5433--5445},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310789},
	doi = {10.1109/TMC.2023.3310789},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiJLYWFTKSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Perceiving and modeling urban crowd movements are of great importance to smart city-related fields. Governments and public service operators can benefit from such efforts as they can be applied to crowd management, resource scheduling, and early emergency warning. However, most prior research on urban crowd modeling has failed to describe the dynamics and continuity of human mobility, leading to inconsistent and irrelevant results when they tackle multiple homogeneous forecasting tasks as they can only be modeled independently. To overcome this drawback, we propose to model human mobility from a new perspective, which uses the citywide crowd transition process constituted by a series of transition matrices from low order to high order, to help us understand how the crowd dynamics evolve step-by-step. We further propose a Deep Transition Process Network to process and predict such new high-dimensional data, where novel grid embedding with Graph Convolutional Network, parameter-shared Convolutional LSTM, and High-Dimensional Attention mechanism are designed to learn the complicated dependencies in terms of spatial, temporal, and ordinal features. We conduct experiments on two datasets generated by a large amount of GPS data collected from a real-world smartphone application. The experiment results demonstrate the superior performance of our proposed methodology over existing approaches.}
}


@article{DBLP:journals/tmc/XuJXWQQ24,
	author = {Yang Xu and
                  Zhida Jiang and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Chen Qian and
                  Chunming Qiao},
	title = {Federated Learning With Client Selection and Gradient Compression
                  in Heterogeneous Edge Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5446--5461},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3309497},
	doi = {10.1109/TMC.2023.3309497},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuJXWQQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has recently gained tremendous attention in edge computing and Internet of Things, due to its capability of enabling distributed clients to cooperatively train models while keeping raw data locally. However, the existing works usually suffer from limited communication resources, dynamic network conditions and heterogeneous client properties, which hinder efficient FL. To simultaneously tackle the above challenges, we propose a heterogeneity-aware FL framework, called FedCG, with adaptive client selection and gradient compression. Specifically, FedCG introduces diversity to client selection and aims to select a representative client subset considering statistical heterogeneity. These selected clients are assigned different compression ratios based on heterogeneous and time-varying capabilities. After local training, they upload sparse model updates matching their capabilities for global aggregation, which can effectively reduce the communication cost and mitigate the straggler effect. More importantly, instead of naively combining client selection and gradient compression, we highlight that their decisions are tightly coupled and indicate the necessity of joint optimization. We theoretically analyze the impact of both client selection and gradient compression on convergence performance. Guided by the convergence rate, we develop an iteration-based algorithm to jointly optimize client selection and compression ratio decision using submodular maximization and linear programming. On this basis, we propose the quantized extension of FedCG, termed Q-FedCG, which further adjusts quantization levels based on gradient innovation. Extensive experiments on both real-world prototypes and simulations show that FedCG and its extension can provide up to 6.4× speedup.}
}


@article{DBLP:journals/tmc/ShenCWLXLAS24,
	author = {Jinglong Shen and
                  Nan Cheng and
                  Xiucheng Wang and
                  Feng Lyu and
                  Wenchao Xu and
                  Zhi Liu and
                  Khalid Aldubaikhy and
                  Xuemin Shen},
	title = {RingSFL: An Adaptive Split Federated Learning Towards Taming Client
                  Heterogeneity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5462--5478},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3309633},
	doi = {10.1109/TMC.2023.3309633},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenCWLXLAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has gained increasing attention due to its ability to collaboratively train while protecting client data privacy. However, vanilla FL cannot adapt to client heterogeneity, leading to a degradation in training efficiency due to stragglers, and is still vulnerable to privacy leakage. To address these issues, this article proposes RingSFL, a novel distributed learning scheme that integrates FL with a model split mechanism to adapt to client heterogeneity while maintaining data privacy. In RingSFL, all clients form a ring topology. For each client, instead of training the model locally, the model is split and trained among all clients along the ring through a pre-defined direction. By properly setting the propagation lengths of heterogeneous clients, the straggler effect is mitigated, and the training efficiency of the system is significantly enhanced. Additionally, since the local models are blended, it is less likely for an eavesdropper to obtain the complete model and recover the raw data, thus improving data privacy. The experimental results on both simulation and prototype systems show that RingSFL can achieve better convergence performance than benchmark methods on independently identically distributed (IID) and non-IID datasets, while effectively preventing eavesdroppers from recovering training data.}
}


@article{DBLP:journals/tmc/LuoKLHW24,
	author = {Fei Luo and
                  Salabat Khan and
                  Anna Li and
                  Yandao Huang and
                  Kaishun Wu},
	title = {EdgeActNet: Edge Intelligence-Enabled Human Activity Recognition Using
                  Radar Point Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5479--5493},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3309938},
	doi = {10.1109/TMC.2023.3309938},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoKLHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition (HAR) has become a research hotspot because of its wide range of application prospects. It has higher requirements for real-time and power-efficient processing. However, a large amount of data transfer between sensors and servers, and computation-intensive recognition models hinder the implementation of real-time HAR systems. Recently, edge computing has been proposed to address this challenge by moving computational and data storage resources to the sensors, rather than depending on a centralized server/cloud. In this paper, we investigated binary neural networks for edge intelligence-enabled HAR using radar point cloud. Point cloud can provide 3-dimensional spatial information, which is helpful to improve recognition accuracy. Time-series point cloud also brings challenges, such as larger data volume, 4-dimensional data processing, and more intensive computation. To tackle these challenges, we adopt the 2-dimensional histograms for point cloud multi-view processing and propose the EdgeActNet, a binary neural network for point cloud-based human activity classification on edge devices. In the evaluation, the EdgeActNet achieved the best results with average accuracies of 97.63% on the MMActivity dataset and 95.03% on the point cloud samples of the DGUHA dataset respectively; and saved 16.9\\times memory consumption and 11.5\\times inference time compared to its full-precision version. Our work also is the first to apply 2D histogram-based multi-view representation and BNNs for time-series point cloud classification.}
}


@article{DBLP:journals/tmc/LiLCC24,
	author = {Jingyi Li and
                  Guocheng Liao and
                  Lin Chen and
                  Xu Chen},
	title = {Roulette: {A} Semantic Privacy-Preserving Device-Edge Collaborative
                  Inference Framework for Deep Learning Classification Tasks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5494--5510},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312304},
	doi = {10.1109/TMC.2023.3312304},
	timestamp = {Mon, 22 Jul 2024 08:26:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5 G/6 G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Moreover, we provide a differential privacy guarantee and analyze the hardness of ground truth inference attacks. To validate the proposed Roulette, we conduct extensive performance evaluations using realistic datasets, which demonstrate that Roulette can effectively defend against various attacks and meanwhile achieve good model accuracy. In a situation where the non-i.i.d. is very severe, Roulette improves the inference accuracy by 21% averaged over benchmarks, while making the accuracy of discrimination attacks almost equivalent to random guessing.}
}


@article{DBLP:journals/tmc/OstrikovaBMGGIKS24,
	author = {Darya Y. Ostrikova and
                  Vitalii Beschastnyi and
                  Dmitri Moltchanov and
                  Elizaveta Golos and
                  Yuliya Gaidamaka and
                  Ilya Ivanov and
                  Yevgeni Koucheryavy and
                  Konstantin E. Samouylov},
	title = {Battery Lifetime and Power Consumption in 5G Systems With Intra- and
                  Inter-RAT Dual-Connectivity},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5511--5526},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3311348},
	doi = {10.1109/TMC.2023.3311348},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OstrikovaBMGGIKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The 5G millimeter wave (mmWave) New Radio (NR) systems are prone to blockage and micromobility effects. To improve service reliability, 3GPP proposed a dual-connectivity allowing UE to maintain links to two base stations (BS). However, this functionality is power-hungry resulting in a trade-off between performance and power efficiency. In this paper, we compare user equipment (UE) power efficiency, consumption, and battery lifetime for different intra- and inter-radio access technologies single- and dual-connectivity under different UE usage scenarios, micromobility, and blockage impairments. We evaluate five schemes: (i) BSs in FR1 and FR2 bands (NR-DC FR1/FR2), (ii) BSs in FR2 band (NR-DC FR2/FR2), and (iii) LTE BS and NR BS in FR2 band (EN-DC LTE/FR2), (iv) NR FR1, and (iv) LTE. Our results show that the LTE-only scheme provides three times longer battery lifetime as compared to its nearest rival – NR FR1 single-connectivity option. For dual-connectivity options, the best lifetime is observed for EN-DC FR2/LTE, and the worst – for NR-DC FR2/FR1. The difference between NR-DC FR2/FR1 and NR-DC FR2/FR2 schemes is negligible. Densification negatively affects all the dual-connectivity schemes as it forces UEs to switch between master and backup technologies more often. We recommend EN-DC FR2/LTE for low-traffic outage-sensitive applications, while NR-DC FR2/FR2 – for heavy-traffic outage non-sensitive applications.}
}


@article{DBLP:journals/tmc/LiuHWXQW24,
	author = {Bingyi Liu and
                  Weizhen Han and
                  Enshu Wang and
                  Shengwu Xiong and
                  Chunming Qiao and
                  Jianping Wang},
	title = {An Efficient Message Dissemination Scheme for Cooperative Drivings
                  via Cooperative Hierarchical Attention Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5527--5542},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312220},
	doi = {10.1109/TMC.2023.3312220},
	timestamp = {Wed, 08 Jan 2025 08:46:44 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHWXQW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A group of connected and autonomous vehicles with common interests can drive in a cooperative manner, namely cooperative driving. In such a networked control system, an efficient message dissemination scheme is critical for cooperative drivings to periodically broadcast their kinetic status, i.e., beacon. However, most existing researches are designed for a simple or specific scenario, e.g., ignoring the impacts of the complex communication environment and emerging hybrid traffic scenarios. Worse still, the inevitable message transmission interference and the limited interaction among vehicles in harsh communication environments seriously hinder cooperation among cooperative drivings and deteriorate the beaconing performance. In this paper, we formulate the decision-making process of cooperative drivings as a Markov game. Furthermore, we propose a cooperative hierarchical attention reinforcement learning (CHA) framework to solve this Markov game. Specifically, the hierarchical structure of CHA leads cooperative drivings to be foresighted. Besides, we integrate each hierarchical level of CHA separately with graph attention networks to incorporate agents’ mutual influences in the decision-making process. Moreover, each hierarchical level learns a cooperative reward function to motivate each agent to cooperate with others under harsh communication conditions. Finally, we set up a simulator and conduct extensive experiments to validate the effectiveness of CHA.}
}


@article{DBLP:journals/tmc/WeiMTSCS24,
	author = {Jianghong Wei and
                  Meixia Miao and
                  Guohua Tian and
                  Jun Shen and
                  Xiaofeng Chen and
                  Willy Susilo},
	title = {Optimal Verifiable Data Streaming Under Concurrent Queries},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5543--5557},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3309270},
	doi = {10.1109/TMC.2023.3309270},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiMTSCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of both hardware and software has promoted the popularization of various real-time applications like health monitoring and intrusion detection that are widely deployed in outsourcing scenarios, e.g., mobile edge computing and cloud computing. In these applications, end devices continuously generate unbounded sequences of data items at a fast rate, i.e., the so-called streaming data. Nevertheless, storing and processing massive amounts of streaming data poses a challenge for resources-restricted end devices. Although outsourcing data items to edge servers or cloud servers is an attractive solution to the above problem, it also brings a new challenge, i.e., how to guarantee the integrity of outsourced data, since streaming data applications are usually sensitive of both location and the corresponding context, and servers are not completely trusted. To this end, the primitive of verifiable data streaming (VDS) protocol was introduced to maintain outsourced streaming data, while preserving its integrity. However, existing VDS constructions mainly use the structure of Merkle hash tree, and inherently have logarithmic costs. Consequently, they are infeasible for real-time applications that are delay sensitive and generate unpredictable size of streaming data. In this paper, we optimize previous VDS protocols from the aspects of communication overhead and computation cost. Specifically, we adopt a technical route different from Merkle hash tree, i.e, combining the digital signature with the cryptographic accumulator. In our construction, we employ Boneh-Lynn-Shacham (BLS) signature to guarantee the integrity of the context and position of each outsourced data item, and adopt an RSA accumulator to invalidate the old signature after the corresponding data item was updated. This immediately yields an optimal VDS construction that has constant costs even under concurrent queries, which is more desirable for those resource-limited mobile devices. In addition, the aggregability of BLS signature makes our VDS construction capable of data auditing, which enables the user to remotely verify the integrity of outsourced streaming data. We provide a formal security proof of the proposed VDS construction under well-studied complexity assumptions in the random oracle model. As a proof-of-concept, we also implement our proposal, and conduct extensive experiments to demonstrate its practicability.}
}


@article{DBLP:journals/tmc/SunYWWGZ24,
	author = {Zhuo Sun and
                  Zhiwen Yu and
                  Qi Wang and
                  Zhu Wang and
                  Bin Guo and
                  Hualei Zhang},
	title = {CovertEye: Gait-Based Human Identification Under Weakly Constrained
                  Trajectory},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5558--5570},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310508},
	doi = {10.1109/TMC.2023.3310508},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunYWWGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a non-intrusive sensing approach, the gait-based human identification technique attracts extensive attention. For the gait-based human identification technique, the unique gait feature is captured and extracted. Owing to the strong environment robustness and good privacy protection, the radar, especially the single-input multiple-output (SIMO) Doppler radar, is proposed as a promising way to capture the gait feature. However, the existing SIMO Doppler radar-based methods require the person to walk along a straight-line trajectory, which hinders their practical application. In this paper, we propose a gait-based human identification system for the weakly constrained trajectory, called CovertEye. In CovertEye, the person can be identified, when he/she walks along variable directions. To this end, we propose a trajectory segmentation algorithm to divide the trajectory into many straight-line trajectory segments. Based on the trajectory segments, we design the gait-based human identification method. In particular, we propose a normalization method to eliminate the differences in the direction of movement and the length among trajectory segments. The normalized signal spectrogram is exploited for the deep learning based feature extraction and human identification. We develop a prototype of the CovertEye system. The extensive experimental results demonstrate that our proposed system can achieve the identification accuracy of 82.4%.}
}


@article{DBLP:journals/tmc/HanYDL24,
	author = {Feiyu Han and
                  Panlong Yang and
                  Haohua Du and
                  Xiang{-}Yang Li},
	title = {Accuth{\textdollar}{\^{}}+{\textdollar}+: Accelerometer-Based Anti-Spoofing
                  Voice Authentication on Wrist-Worn Wearables},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5571--5588},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3314837},
	doi = {10.1109/TMC.2023.3314837},
	timestamp = {Wed, 12 Jun 2024 11:25:58 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanYDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing voice-based user authentication systems mainly rely on microphones to capture the unique vocal characteristics of an individual, which are vulnerable to various acoustic attacks and may suffer high-security risks. In this work, we present Accuth^++, a novel authentication system on the wrist-worn device that takes advantage of a low-cost accelerometer to verify the user's identity and resist spoofing acoustic attacks. Accuth^++ captures unique sound vibrations during the human pronunciation process and extracts multi-level features to verify the user's identity. Specifically, we analyze and model the differences between the physical sound field of human beings and loudspeakers, and extract a novel sound-field-level liveness feature to defend against spoofing attacks. Accuth^++ is an effective complement to existing wearable authentication approaches as it only leverages a ubiquitous, low-cost, and small-size accelerometer. In real-world experiments. Accuth^++ achieves over 92.85% averaged identification accuracy among 15 human participants and an averaged equal error rate (EER) of 1.91% for spoofing attack detection.}
}


@article{DBLP:journals/tmc/ZhaoLYZXXC24,
	author = {Ziming Zhao and
                  Zhaoxuan Li and
                  Jiongchi Yu and
                  Fan Zhang and
                  Xiaofei Xie and
                  Haitao Xu and
                  Binbin Chen},
	title = {{CMD:} Co-Analyzed IoT Malware Detection and Forensics via Network
                  and Hardware Domains},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5589--5603},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3311012},
	doi = {10.1109/TMC.2023.3311012},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLYZXXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread use of Internet of Things (IoT) devices, malware detection has become a hot spot for both academic and industrial communities. Existing approaches can be roughly categorized into network-side and host-side. However, existing network-side methods are difficult to capture contextual semantics from cross-source traffic, and previous host-side methods could be adversary-perceived and expose risks for tampering. More importantly, a single perspective cannot comprehensively track the multi-stage lifecycle of IoT malware. In this paper, we present CMD, a co-analyzed IoT malware detection and forensics system by combining hardware and network domains. For the network part, CMD proposes a tailored capsule neural network to capture the contextual semantics from cross-source traffic. For the hardware part, CMD designs an entire file operation recovery process in a side-channel manner by leveraging the Serial Peripheral Interface (SPI) signals from on-chip traces. These traffic provenance and operating logs information could benefit the anti-virus countermeasures for security practitioners. By practical evaluation, we demonstrate that CMD realizes outstanding detection effects (e.g., \\sim\n99.88% F1-score) compared with seven state-of-the-art methods, and recovers 96.88%\\sim\n99.75% operation commands even if against adaptive adversaries (that could kill processes or tamper with operation log files). A by-product benefit of such an external monitor is CMD introduces zero latency on the IoT device, and incurs negligible IoT CPU utilization. Also, since SPI focuses on file operations, the proposed hardware trace forensics does not have the data explosion problem like previous work, e.g., recovered logs of CMD only take up limited extra space overhead (e.g., \\sim\n0.2 MB per malware). Furthermore, we provide the model interpretability for the capsule network and develop a case study (Hajime) of the operation logs recovery.}
}


@article{DBLP:journals/tmc/WuZCST24,
	author = {Yuan Wu and
                  Jian Zhang and
                  Yanjiao Chen and
                  Wuxuan Shi and
                  Huiri Tan},
	title = {MC-Tracking: Towards Ubiquitous Menstrual Cycle Tracking Using the
                  Smartphone},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5604--5615},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310941},
	doi = {10.1109/TMC.2023.3310941},
	timestamp = {Sat, 27 Apr 2024 21:29:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZCST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tracking the menstrual cycle (MC) is essential for women to manage their health and schedule, especially for those with irregular MC. Existing MC tracking methods either rely on length of previous cycles (e.g., calendar noting) or require additional devices to collect more information (e.g., basal temperature), which are not able to realize both accuracy and convenience. Inspired by the medical studies that gait patterns will be affected by MC, we design a smartphone-based application named MC-Tracking, which monitors MC based on the Inertial Measurement Unit (IMU) signals. By identifying the walking activity based on the acceleration and angular velocity signals, we train an attention-based prediction model that can be generalized to new users with meta learning. 40 volunteers participate in an extensive experiment for more than 3 months, in which more than 2.4 TB of time-series data is collected to evaluate the performance of MC-tracking. It is verified that MC-tracing can predict the onset of MC seven days in advance with an average error of 0.56 days. We also demonstrate that the prediction accuracy is robust to the age, emotion, biological clock and smartphone brand.}
}


@article{DBLP:journals/tmc/JiaoYCLZC24,
	author = {Luofang Jiao and
                  Kai Yu and
                  Jiacheng Chen and
                  Tingting Liu and
                  Haibo Zhou and
                  Lin Cai},
	title = {Performance Analysis of Uplink/Downlink Decoupled Access in Cellular-V2X
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5616--5630},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310659},
	doi = {10.1109/TMC.2023.3310659},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiaoYCLZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper first develops an analytical framework to investigate the performance of uplink (UL)/downlink (DL) decoupled access in cellular vehicle-to-everything (C-V2X) networks, in which a vehicle's UL/DL can be connected to different macro/small base stations (MBSs/SBSs), separately. Using the stochastic geometry analytical tool, the UL/DL decoupled access C-V2X is modeled as a Cox process, and we obtain the following theoretical results, i.e., 1) the probability of different UL/DL joint association cases i.e., both the UL and DL are associated with the different MBSs or SBSs, or they are associated with different types of BSs; 2) the distance distribution of a vehicle to its serving BSs in each case; 3) the spectral efficiency of UL/DL in each case; and 4) the UL/DL coverage probability of MBS/SBS. The analyses reveal the insights and performance gain of UL/DL decoupled access. Through extensive simulations, the accuracy of the proposed analytical framework is validated. Both the analytical and simulation results show that UL/DL decoupled access can improve spectral efficiency. The theoretical results can be directly used for estimating the statistical performance of a UL/DL decoupled access C-V2X network.}
}


@article{DBLP:journals/tmc/ZhangWW24,
	author = {Jiangtao Zhang and
                  Qingshan Wang and
                  Qi Wang},
	title = {{HDTSLR:} {A} Framework Based on Hierarchical Dynamic Positional Encoding
                  for Sign Language Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5631--5643},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310712},
	doi = {10.1109/TMC.2023.3310712},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sign language is the basic way for people with hearing impairment to communicate, and sign language recognition (SLR) could effectively help in this regard. Mainstream Transformer-based SLR requires positional encoding to sense the positional information of the data. However, existing PE methods globally encode the sign data result in weaken or even ignoring the sequence variation within the gestures. This article proposes HDTSLR: A Transformer-based SLR framework built on hierarchical dynamic positional encoding (HDPE) enhances individual gesture sequence features while preserving the sign overall temporal features. HDPE designs semantic positional encoding utilizing predefined scale functions with trainable biases to emphasize sign semantic relationships. The t-distribution is used by the designed lexical positional encoding to explore the unique variation of gestures. Before the HDPE operation, the sign language data is split into equal-length feature clips while feature extraction and chunking are performed by the autoencoder. The feature clips with significant changes in gesture chunk are further selected and aggregated with the remaining ones by deforming Gram matrix. In addition, HDTSLR is evaluated on the one-handed and two-handed datasets, achieving word error rates of 16.59% and 21.67%, respectively. Comparison experiments show that it outperforms known SLR methods in both accuracy and robustness.}
}


@article{DBLP:journals/tmc/XieW24,
	author = {Xin Xie and
                  Heng Wang},
	title = {Minimizing Age of Usage Information for Capturing Freshness and Usability
                  of Correlated Data in Edge Computing Enabled IoT Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5644--5659},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312130},
	doi = {10.1109/TMC.2023.3312130},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information (AoI) is a popular metric of data freshness, however, it neglects the usability of data. In light of this, we introduce a new metric, Age of Usage Information (AoUI), which can jointly capture the freshness and usability of correlated data in the Internet of Things in a fine-grained manner. Based on the proposed metric, we investigate the optimization problem of minimizing average AoUI, where the correlated nodes transmit data to the destination via noisy channels. To seek the optimal data scheduling policy, we first develop a virtual queue based (VQ) policy under the assumption that the priori knowledge of the channel state is known. Then, considering the case where the channel state is unknown, we utilize the model-free characteristic of double deep Q-network (DDQN) to design an improved exploration based DDQN (IE-DDQN) policy which does not require a priori knowledge of the channel state. Furthermore, we investigate the development of joint data scheduling and usage policy and introduce decoupled action branches to improve the structure of the neural network of DDQN proposing a decoupling action based DDQN (DA-DDQN) policy. Simulation results show that the proposed VQ, IE-DDQN, and DA-DDQN policies all exhibit superior performance compared to baseline algorithms such as the classical DDQN method and the greedy policy of scheduling the node with the largest product of AoUI and usable factor, which may be due to the consideration of the stability of the virtual queue, the improvement of the exploration process, and the reconstruction of the neural network structure, respectively.}
}


@article{DBLP:journals/tmc/FidaADOEM24,
	author = {Mah{-}Rukh Fida and
                  Azza H. Ahmed and
                  Thomas Dreibholz and
                  Andr{\'{e}}s F. Ocampo and
                  Ahmed Elmokashfi and
                  Foivos Michelinakis},
	title = {Bottleneck Identification in Cloudified Mobile Networks Based on Distributed
                  Telemetry},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5660--5676},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312051},
	doi = {10.1109/TMC.2023.3312051},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FidaADOEM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloudified mobile networks are expected to deliver a multitude of services with reduced capital and operating expenses. A characteristic example is 5G networks serving several slices in parallel. Such mobile networks, therefore, need to ensure that the SLAs of customised end-to-end sliced services are met. This requires monitoring the resource usage and characteristics of data flows at the virtualised network core, as well as tracking the performance of the radio interfaces and UEs. A centralised monitoring architecture can not scale to support millions of UEs though. This paper, proposes a 2-stage distributed telemetry framework in which UEs act as early warning sensors. After UEs flag an anomaly, a ML model is activated, at network controller, to attribute the cause of the anomaly. The framework achieves 85% F1-score in detecting anomalies caused by different bottlenecks, and an overall 89% F1-score in attributing these bottlenecks. This accuracy of our distributed framework is similar to that of a centralised monitoring system, but with no overhead of transmitting UE-based telemetry data to the centralised controller. The study also finds that passive in-band network telemetry has the potential to replace active monitoring and can further reduce the overhead of a network monitoring system.}
}


@article{DBLP:journals/tmc/XiaWXLX24,
	author = {Qiufen Xia and
                  Guijie Wang and
                  Zichuan Xu and
                  Weifa Liang and
                  Zhou Xu},
	title = {Efficient Algorithms for Service Chaining in NFV-Enabled Satellite
                  Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5677--5694},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312352},
	doi = {10.1109/TMC.2023.3312352},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaWXLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite-terrestrial networks are emerging as the next-generation networking paradigm for Beyond-5 G (B5G) and 6 G networks. Meanwhile, Mobile Edge Computing (MEC) is envisioned as the key technology to provide network services within the proximity of users, by deploying computing resource in ground locations that are close to users. With the fast deployment of Low-Earth-Oribt (LEO) satellites, a new paradigm of MEC is emerging by enabling LEO satellites serving as edge servers in lower orbits that are close to ground users. In this way, the ground users can be further served by LEO satellites in lower orbits instead of conventional high-orbit satellites. Also, since LEO satellites provide shorter paths from users to services, the performance is enhanced compared with ground MEC networks. In this paper, we aim to enable low-latency network services in a Satellite Edge Computing (SEC) network that integrates the MEC and satellite-terrestrial networks. In particular, we consider that each network service is composed of a sequence of Virtualized Network Functions (VNFs), where the traffic of user requests has to be processed by the VNFs in a service chain in the specified order before reaching its destination. To this end, we first formulate a delay-aware service chaining problem in an SEC network to minimize the average delay of implementing a user request, by jointly placing VNFs to LEO satellites in the SEC network and routing the traffic of each user request from its source to destination. We then devise an approximation algorithm with an approximation ratio for the problem in an SEC network with a single user request, by devising a novel concept of chaining orbit and auxiliary graph construction technique. We also design an online algorithm for the online delay-aware service chaining problem in an SEC network, if user requests arrive into the system without the knowledge of their arrivals and the network delays are uncertain. We finally evaluate the performance of the proposed algorithms using real satellite network topologies, and results show that the proposed algorithms achieve 28.5% lower delay than their counterparts.}
}


@article{DBLP:journals/tmc/WangSL24,
	author = {Shangshang Wang and
                  Ziyu Shao and
                  John C. S. Lui},
	title = {Next-Word Prediction: {A} Perspective of Energy-Aware Distributed
                  Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5695--5708},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310536},
	doi = {10.1109/TMC.2023.3310536},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pursuit of high-quality artificial intelligence generated contents (AIGC) with fast response has prompted the evolution of natural language processing (NLP) services, notably those enabled at the edge (i.e., edge NLP). For concreteness, we study distributed inference for next-word prediction which is a prevalent edge NLP service for mobile keyboards on user devices. Accordingly, we optimize coupled metrics, i.e., maximize prediction click-through rate (CTR) for improved quality-of-service (QoS), minimize user impatience for enhanced quality-of-experience (QoE), and keep energy consumption within budget for sustainability. Moreover, we consider the real-world setting where there is no prior knowledge of heterogeneous NLP models’ prediction accuracy. Via an integration of online learning and online control, we propose a novel distributed inference algorithm for online next-word prediction with user impatience (DONUT) to estimate models’ prediction accuracy and balance the trade-offs among coupled metrics. Our theoretical analysis reveals that DONUT achieves sub-linear regret (loss of CTR), ensures bounded user impatience, and maintains within-budget energy consumption. Through numerical simulations, we not only establish DONUT's superior performance over other baseline methods, but also demonstrate its adaptability to various settings.}
}


@article{DBLP:journals/tmc/YuZWTWG24,
	author = {Zheng Yu and
                  Junyu Zhang and
                  Zheng Wen and
                  Andrea Tacchetti and
                  Mengdi Wang and
                  Ian Gemp},
	title = {Teamwork Reinforcement Learning With Concave Utilities},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5709--5721},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315120},
	doi = {10.1109/TMC.2023.3315120},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuZWTWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Complex reinforcement learning (RL) tasks often require a divide-and-conquer approach, where a large task is divided into pieces and solved by individual agents. In this paper, we study a teamwork RL setting where individual agents make decisions on disjoint subsets (blocks) of the state space and have private interests (reward functions), while the entire team aims to maximize a general long-term team utility function and may be subject to constraints. This team utility, which is not necessarily a cumulative sum of rewards, is modeled as a nonlinear function of the team's joint state-action occupancy distribution. By leveraging the inherent duality of policy optimization, we propose a min-max multi-block policy optimization framework to decompose the overall problem into individual local tasks. This enables a federated teamwork mechanism where a team lead coordinates individual agents via reward shaping, and each agent solves its local task defined only on its local state subset. We analyze the convergence of this teamwork policy optimization mechanism and establish an O(1/T)\nconvergence rate to the team's joint optimum. This mechanism allows team members to jointly find the global socially optimal policy while keeping their local privacy.}
}


@article{DBLP:journals/tmc/GouZLYSC24,
	author = {Yu Gou and
                  Tong Zhang and
                  Jun Liu and
                  Tingting Yang and
                  Shanshan Song and
                  Jun{-}Hong Cui},
	title = {Achieving Fair-Effective Communications and Robustness in Underwater
                  Acoustic Sensor Networks: {A} Semi-Cooperative Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5722--5739},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310476},
	doi = {10.1109/TMC.2023.3310476},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GouZLYSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the fair-effective communication and robustness in imperfect and energy-constrained underwater acoustic sensor networks (IC-UASNs). Specifically, we investigate the impact of unexpected node malfunctions on the network performance under the time-varying acoustic channels. Each node is expected to satisfy Quality of Service (QoS) requirements. However, achieving individual QoS requirements may interfere with other concurrent communications. Underwater nodes rely excessively on the rationality of other underwater nodes when guided by fully cooperative approaches, making it difficult to seek a trade-off between individual QoS and global fair-effective communications under imperfect conditions. Therefore, this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that achieves fair-effective communication and robustness in IC-UASNs. The approach is distributed multi-agent reinforcement learning (MARL)-based, and the objectives are twofold. On the one hand, each intelligent node individually decides the transmission power to simultaneously optimize individual and global performance. On the other hand, advanced training algorithms are developed to provide imperfect environments for training robust models that can adapt to the time-varying acoustic channels and handle unexpected node failures in the network. Numerical results are presented to validate our proposed approach.}
}


@article{DBLP:journals/tmc/GuoNWD24,
	author = {Jianxiong Guo and
                  Qiufen Ni and
                  Weili Wu and
                  Ding{-}Zhu Du},
	title = {Multi-Task Diffusion Incentive Design for Mobile Crowdsourcing in
                  Social Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5740--5754},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3310383},
	doi = {10.1109/TMC.2023.3310383},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoNWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsourcing (MCS) is a novel distributed computing paradigm that recruits skilled workers to perform location-dependent tasks. A number of mature incentive mechanisms have been proposed to address the worker recruitment problem in MCS systems. However, most of them assume that there is a large enough worker pool and a sufficient number of users can be selected. This may be impossible in large-scale crowdsourcing environments. To address this challenge, we consider the MCS system defined on a location-aware social network provided by a social platform. In this system, we can recruit a small number of seed workers from the existing worker pool to spread the information of multiple tasks in the social network, thus attracting more users to perform tasks. In this article, we propose a Multi-Task Diffusion Maximization (MT-DM) problem that aims to maximize the total utility of performing multiple crowdsourcing tasks under the budget. To accommodate multiple tasks diffusion over a social network, we create a multi-task diffusion model, and based on this model, we design an auction-based incentive mechanism, MT-DM-L. To deal with the high complexity of computing the multi-task diffusion, we adopt Multi-Task Reverse Reachable (MT-RR) sets to approximate the utility of information diffusion efficiently. Through both complete theoretical analysis and extensive simulations by using real-world datasets, we validate that our estimation for the spread of multi-task diffusion is accurate and the proposed mechanism achieves individual rationality, truthfulness, computational efficiency, and (1-1/\\sqrt{e}-\\varepsilon) approximation with at least 1-\\delta probability.}
}


@article{DBLP:journals/tmc/YangZSSCZ24,
	author = {Xiansheng Yang and
                  Yuan Zhuang and
                  Min Shi and
                  Xiao Sun and
                  Xiaoxiang Cao and
                  Bingpeng Zhou},
	title = {RatioVLP: Ambient Light Noise Evaluation and Suppression in the Visible
                  Light Positioning System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5755--5769},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312550},
	doi = {10.1109/TMC.2023.3312550},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangZSSCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Visible Light Positioning (VLP), a promising indoor positioning technique, has gained wide popularity worldwide because of its ubiquitous infrastructure, low power consumption, and high positioning precision. However, VLP systems based on photodiodes (PDs) often suffer from varying ambient light with time and space, which seriously degrades their positioning precision and robustness. In this article, we carefully evaluate the influence of the ambient light on the VLP system, which includes the reduction of positioning accuracy by varying ambient light with time and the inaccurate parameter calibration by unevenly distributed ambient light. Then, we figure out that the influence of ambient light on the Received Signals Strength (RSS) values is determined by the ambient light intensity and PD, which is independent of external factors, including distance, frequency, LED, etc. Next, we propose a new positioning framework, RatioVLP, where a ratio model that is more robust to varying ambient light with time is used. However, the ratio model is severely dependent on the Lambert parameters that are vulnerable to ambient light, which reduces the framework's precision when the calibration area is unevenly covered by ambient light. Thus, we design new parameters that are less sensitive to ambient light, called R parameter, to connect the RSS ratio and its corresponding distance ratio, which can strengthen the ratio model's robustness and effectively reduce the influence of ambient light on the parameter calibration process. Experimental results show that the positioning precision of the proposed method is improved by more than 50 % when compared to the conventional Lambert model in scenes influenced by ambient light.}
}


@article{DBLP:journals/tmc/GuoTZL24,
	author = {Delin Guo and
                  Lan Tang and
                  Xinggan Zhang and
                  Ying{-}Chang Liang},
	title = {Joint Optimization of Trajectory and Jamming Power for Multiple UAV-Aided
                  Proactive Eavesdropping},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5770--5785},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3311484},
	doi = {10.1109/TMC.2023.3311484},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuoTZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies a novel wireless information surveillance scenario, where the legitimate party aims to eavesdrop on multiple suspicious communication links with the help of multiple unmanned aerial vehicles (UAVs). Each suspicious link is comprised of a UAV (transmitter) and its fixed destination. To improve the eavesdropping ability, cooperative legitimate UAVs emit jamming signals to reduce the capacities of suspicious channels and plan the flight trajectory to enhance the capacity of the eavesdropping channels. Considering the system dynamics, it is natural to model this sequential decision-making problem as a Markov Decision Process (MDP), which might be solved by reinforcement learning (RL). However, it is difficult to design a policy in RL that determines jamming powers satisfying the considered eavesdropping constraints. Therefore, we decompose the optimization process into two phases, 1) obtaining the non-learning-based optimal solver for jamming power allocation under each state, and 2) optimizing the policy of moving action by RL. We will show this decoupled optimization process also holds the optimality. Considering the flying safety, we will determine the individual moving policy for each legitimate UAV rather than a centralized policy that controls all UAVs. Finally, extensive simulations are conducted to demonstrate the effectiveness of the proposed solution.}
}


@article{DBLP:journals/tmc/BonatiPDBM24,
	author = {Leonardo Bonati and
                  Michele Polese and
                  Salvatore D'Oro and
                  Stefano Basagni and
                  Tommaso Melodia},
	title = {NeutRAN: An Open {RAN} Neutral Host Architecture for Zero-Touch {RAN}
                  and Spectrum Sharing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5786--5798},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3311728},
	doi = {10.1109/TMC.2023.3311728},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BonatiPDBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Obtaining access to exclusive spectrum, cell sites, Radio Access Network (RAN) equipment, and edge infrastructure imposes major capital expenses to mobile network operators. A neutral host infrastructure, by which a third-party company provides RAN services to mobile operators through network virtualization and slicing techniques, is seen as a promising solution to decrease these costs. Currently, however, neutral host providers lack automated and virtualized pipelines for onboarding new tenants and to provide elastic and on-demand allocation of resources matching operators’ requirements. To address this gap, this paper presents NeutRAN, a zero-touch framework based on the O-RAN architecture to support applications on neutral hosts and automatic operator onboarding. NeutRAN builds upon two key components: (i) an optimization engine to guarantee coverage and to meet quality of service requirements while accounting for the limited amount of shared spectrum and RAN nodes, and (ii) a fully virtualized and automated infrastructure that converts the output of the optimization engine into deployable micro-services to be executed at RAN nodes and cell sites. NeutRAN was prototyped on an OpenShift cluster and on a programmable testbed with 4 base stations and 10 users from 3 different tenants. We evaluate its benefits, comparing it to a traditional license-based RAN where each tenant has dedicated physical and spectrum resources. We show that NeutRAN can deploy a fully operational neutral host-based cellular network in around 10 seconds. Experimental results show that it increases the cumulative network throughput by 2.18× and the per-user average throughput by 1.73× in networks with shared spectrum blocks of 30 MHz. NeutRAN provides a 1.77× cumulative throughput gain even when it can only operate on a shared spectrum block of 10 MHz (one third of the spectrum used in license-based RANs).}
}


@article{DBLP:journals/tmc/LvWTWLX24,
	author = {Gerui Lv and
                  Qinghua Wu and
                  Qingyue Tan and
                  Weiran Wang and
                  Zhenyu Li and
                  Gaogang Xie},
	title = {Accurate Throughput Prediction for Improving QoE in Mobile Adaptive
                  Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5799--5817},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3313592},
	doi = {10.1109/TMC.2023.3313592},
	timestamp = {Mon, 29 Jul 2024 21:17:51 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LvWTWLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video streaming is the most important mobile application today. To improve users’ quality of experience (QoE), the client player runs adaptive bitrate (ABR) algorithms that dynamically select the bitrate for video chunks based on throughput or delivery time predictions. This paper aims to design an accurate predictor for mobile adaptive streaming by investigating all its components, including input features, output target, and mapping function. We construct the first theoretical framework that reveals potential factors affecting chunk throughput and delivery time. To verify this framework, we provide formulation analysis and measurement observations based on 2500+ video sessions collected in real-world mobile networks. We find that previous works have failed to achieve accurate prediction due to overlooking the impact of the transport mechanism and application behavior on throughput. Furthermore, we show that throughput is a better target for data-driven predictors than delivery time, due to the long-tailed distribution of delivery time. Based on the above, we propose Lumos, a decision-tree-based throughput predictor that can be integrated into various ABR algorithms. Extensive experiments in real-world mobile Internet show that Lumos achieves high prediction accuracy and improves the QoE of MPC by 6.3%, and MPC+Lumos outperforms Pensieve by 19.2%.}
}


@article{DBLP:journals/tmc/NingYWSGJ24,
	author = {Zhaolong Ning and
                  Yuxuan Yang and
                  Xiaojie Wang and
                  Qingyang Song and
                  Lei Guo and
                  Abbas Jamalipour},
	title = {Multi-Agent Deep Reinforcement Learning Based {UAV} Trajectory Optimization
                  for Differentiated Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5818--5834},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3312276},
	doi = {10.1109/TMC.2023.3312276},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NingYWSGJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by the increasing computational demand of real-time mobile applications, Unmanned Aerial Vehicle (UAV) assisted Multi-access Edge Computing (MEC) has been envisioned as a promising paradigm for pushing computational resources to network edges and constructing high-throughput line-of-sight links for ground users. Most exsiting studies consider simplified scenarios, such as a single UAV, Service Provider (SP) or service type, and centralized UAV trajectory control. In order to be more in line with real-world cases, we intend to achieve distributed trajectory control of multiple UAVs in UAV-assisted MEC networks with multiple SPs providing differentiated services. Our objective is to minimize the short-term computational costs of ground users and the long-term computational cost of UAVs, simultaneously based on incomplete information. We first solve the formulated problem by reaching the Nash Equilibrium (NE) of the game among SPs based on complete information. We further formulate a Markov game model and propose a Deep Reinforcement Learning (DRL)-based UAV trajectory optimization algorithm, where only local observations of each UAV are required for each SP's flying action execution. Theoretical analysis and performance evaluation demonstrate the convergence, efficiency, scalability, and robustness of our algorithm compared with other representative algorithms.}
}


@article{DBLP:journals/tmc/NarottamaJS24,
	author = {Bhaskara Narottama and
                  Triwidyastuti Jamaluddin and
                  Soo Young Shin},
	title = {Quantum Neural Network With Parallel Training for Wireless Resource
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5835--5847},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3321467},
	doi = {10.1109/TMC.2023.3321467},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NarottamaJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A quantum neural network with parallel training (called PS-QNN) is presented in this study to optimize wireless resource allocation. Instead of sending the whole dataset, each edge only requires to send the statistical parameters of the dataset; hence reducing the dimension of the training data. As a particular case, the proposed PS-QNN is utilized to optimize transmit precoding and power allocation in non-orthogonal multiple access with multiple-input and multiple-output antennas (MIMO-NOMA). Compared to the conventional training method, analysis shows that the proposed parallel training yields a lower complexity, while achieving a comparable sum rate compared to conventional method.}
}


@article{DBLP:journals/tmc/ZhaoCM24,
	author = {Linlin Zhao and
                  Xuefen Chi and
                  Shaodan Ma},
	title = {Delay-QoS-Aware Local-Information-Driven Multiple Access for {MTC}
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5848--5862},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3314645},
	doi = {10.1109/TMC.2023.3314645},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Carrier sense multiple access (CSMA) provides a feasible way to support machine type communications (MTC). However, in CSMA how to guarantee the diverse delay QoS efficiently is unsolved because the relationship between the back-off factor and delay QoS is unexploited. In this paper, we propose a new CSMA-type scheme named QoS-aware Local-Information-driven Multiple Access (LIMA) to handle this problem. The LIMA is formulated as a throughput maximization problem subject to the statistical delay QoS including the delay bound and delay bound violation probability. The diverse statistical delay QoS constraints are decomposed constraint into a series of the parameter adjusting period (PAP)-progressive QoS constraints in the time scale. Based on the effective capacity, the relationship between the back-off factor (defined as the average of back-off duration) and statistical delay QoS is studied. Then, each PAP-progressive QoS constraint is explicitly expressed in a simple form. Finally, a distributed algorithm is developed to find the optimal back-off factors based on local information. The global optimality of the proposed solution is theoretically proved. The salient feature of LIMA is that it can satisfy the diverse delay QoS requirements in a distributed way without signaling overhead for traffic information of other devises and scheduling information from the coordinator/access point (AP).}
}


@article{DBLP:journals/tmc/HuangHZYZZ24,
	author = {Xiaowen Huang and
                  Tao Huang and
                  Wenjie Zhang and
                  Chai Kiat Yeo and
                  Shuguang Zhao and
                  Guanglin Zhang},
	title = {Pricing Optimization in {MEC} Systems: Maximizing Resource Utilization
                  Through Joint Server Configuration and Dynamic Operation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5863--5879},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315334},
	doi = {10.1109/TMC.2023.3315334},
	timestamp = {Fri, 07 Feb 2025 15:03:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangHZYZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The resource allocation problem in Multi-access Edge Computing (MEC) has been widely studied to maximize its operation efficiency under limited resource constraint. However, the existing literatures overlooked the setup cost and the associated dynamic operations. In this work, we consider server configuration and overload in the multi-server scenario where servers are switched on/off depending on the network environment. A novel pricing mechanism maximizing the utility of base station (BS) monitoring multiple servers is proposed, which jointly optimizes the setup cost and server load. We aim to maximize the BS utility under one-day task requests, and divide the time into off-peak and peak periods based on task requests. In the off-peak period, we flexibly switch on/off servers for BS to reduce setup costs. In the peak period, to avoid overloading, we introduce crowdsourcing where servers as agents purchase idle resources from private users (PUs) for mobile users (MUs) and minimize MUs’ cost by a contract-based knapsack algorithm. Lastly, a pricing mechanism is proposed to solve the BS utility maximization problem with an exploratory Upper Confidence Bound (UCB)-based algorithm adjusting server prices dynamically. Simulation results show that the proposed algorithm is superior to others in minimizing MUs cost and maximizing BS utility.}
}


@article{DBLP:journals/tmc/ZhuLTN24,
	author = {Guogang Zhu and
                  Xuefeng Liu and
                  Shaojie Tang and
                  Jianwei Niu},
	title = {Aligning Before Aggregating: Enabling Communication Efficient Cross-Domain
                  Federated Learning via Consistent Feature Extraction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5880--5896},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316645},
	doi = {10.1109/TMC.2023.3316645},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLTN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain federated learning (FL), where data on local clients come from different domains, is a common case of FL. In such a cross-domain case, features extracted from the raw data of different clients deviate from each other in the feature space, leading to a so-called feature shift. This phenomenon can reduce feature discrimination and degrade the performance of the learned model. However, most existing FL methods are not specifically designed for the cross-domain setting. In this article, we propose a novel cross-domain FL method named AlignFed. In AlignFed, each client model consists of a personalized feature extractor and a shared lightweight classifier. The feature extractor maps the features to a consistent space by aligning them to identical global target points. Inspired by recent studies in contrastive learning, AlignFed regards points that are uniformly distributed on the hypersphere as global target points. It then pushes features toward global target points of their corresponding classes and away from those of other classes to improve feature discrimination. The shared classifier aggregates knowledge across clients over the consistent feature space, which can mitigate performance degradation caused by feature shift while reducing communication cost. We conduct convergence analysis and perform extensive experiments to evaluate AlignFed.}
}


@article{DBLP:journals/tmc/DeebakH24,
	author = {Bakkiam David Deebak and
                  Seong Oun Hwang},
	title = {Healthcare Applications Using Blockchain With a Cloud-Assisted Decentralized
                  Privacy-Preserving Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5897--5916},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315510},
	doi = {10.1109/TMC.2023.3315510},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DeebakH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent times, cloud-enabled healthcare services have gained much attention in fulfilling the analysis of privacy risks associated with effective decision management systems including trustworthiness and secure data sharing. This system has revolutionized the medical architecture to evolve unprecedented opportunities in using the technologies of next-generation networks, including services, control, and signaling, to effectively improve content delivery to individuals and organizations. However, it is a highly complex matter to distribute confidential data over a public network because data privacy and device security are at risk. As a result, a cloud-based healthcare application is introduced that integrates the computation capabilities of ubiquitous computing to provide extensive communications over dedicated Internet access in order to store health records. To manage access control mechanisms and process sensitive data without extensive computation, the existing cloud-centric systems access remote servers via dedicated networks. Based on a centralized architecture, a dedicated network uses different application domains to deliver information to the healthcare industry. Unfortunately, computation complexity greatly deteriorates the performance of peer-to-peer (P2P) communications. Thus, this paper presents a cloud-assisted decentralized privacy preserving framework (CA-DPPF) using blockchain and key agreement (KA) mechanisms to achieve secure data storage and privacy. The detailed security analysis proves that the proposed scheme fulfills the desired security properties of healthcare supply chain management (H-SCM) such as conditional traceability, data immutability, and data integrity. Overall, exploratory analysis shows that CA-DPPF guarantees better transaction efficiencies such as less latency and more throughput in order to improve the service utilization factor.}
}


@article{DBLP:journals/tmc/XuHLHAM24,
	author = {Guanyu Xu and
                  Zhiwei Hao and
                  Yong Luo and
                  Han Hu and
                  Jianping An and
                  Shiwen Mao},
	title = {DeViT: Decomposing Vision Transformers for Collaborative Inference
                  in Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5917--5932},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315138},
	doi = {10.1109/TMC.2023.3315138},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuHLHAM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the great success of vision transformer (ViT), which has achieved state-of-the-art performance on multiple computer vision benchmarks. However, ViT models suffer from vast amounts of parameters and high computation cost, leading to difficult deployment on resource-constrained edge devices. Existing solutions mostly compress ViT models to a compact model but still cannot achieve real-time inference. To tackle this issue, we propose to explore the divisibility of transformer structure, and decompose the large ViT into multiple small models for collaborative inference at edge devices. Our objective is to achieve fast and energy-efficient collaborative inference while maintaining comparable accuracy compared with large ViTs. To this end, we first propose a collaborative inference framework termed DeViT to facilitate edge deployment by decomposing large ViTs. Subsequently, we design a decomposition-and-ensemble algorithm based on knowledge distillation, termed DEKD, to fuse multiple small decomposed models while dramatically reducing communication overheads, and handle heterogeneous models by developing a feature matching module to promote the imitations of decomposed models from the large ViT. Extensive experiments for three representative ViT backbones on four widely-used datasets demonstrate our method achieves efficient collaborative inference for ViTs and outperforms existing lightweight ViTs, striking a good trade-off between efficiency and accuracy. For example, our DeViTs improves end-to-end latency by 2.89× with only 1.65% accuracy sacrifice using CIFAR-100 compared to the large ViT, ViT-L/16, on the GPU server. DeDeiTs surpasses the recent efficient ViT, MobileViT-S, by 3.54% in accuracy on ImageNet-1 K, while running 1.72× faster and requiring 55.28% lower energy consumption on the edge device.}
}


@article{DBLP:journals/tmc/QiuLLC24,
	author = {Yu Qiu and
                  Junbin Liang and
                  Victor C. M. Leung and
                  Min Chen},
	title = {Online Security-Aware and Reliability-Guaranteed {AI} Service Chains
                  Provisioning in Edge Intelligence Cloud},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5933--5948},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3314580},
	doi = {10.1109/TMC.2023.3314580},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QiuLLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of edge intelligence cloud (EIC), mobile users are not satisfied with a single artificial intelligence inference service, but require multiple inference services with chain dependencies to process data. Each AI service chain (AISC) is provided as a series of interconnected virtual network functions (VNFs) on-demand deployed on edge servers. However, AISCs experience unpredictable failures and potential attacks in EIC, which may violate different inference requirements of mobile users for reliability, security, and accuracy. How to optimally deploy VNFs and BVNFs on trusted edge servers, and select secure links to form satisfactory AISCs, meanwhile throughput of receiving requests is maximized while deployment cost of computing resources used to create VNFs and BVNFs with different model sizes is minimized in real-time, is a challenging problem. In this paper, the problem is first formulated as an integer linear programming and proved to be NP-hard. Then, we consider the problem under two online backup scenarios: one is an on-site scenario where AISC requests from the mobile devices arrive one by one, and link securities between VNFs and corresponding BVNFs are ignored because they are always on the same edge server; another is an off-site scenario where a set of AISC requests are given, and VNFs and BVNFs are deployed on different servers. Finally, two online algorithms with provable competitive ratios are proposed to solve the above two problems in polynomial time. Theoretical analyses and experiments based on real network topologies demonstrate that our algorithms are promising compared to baseline algorithms.}
}


@article{DBLP:journals/tmc/MhaisenIL24,
	author = {Naram Mhaisen and
                  George Iosifidis and
                  Douglas J. Leith},
	title = {Online Caching With no Regret: Optimistic Learning via Recommendations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5949--5965},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3317943},
	doi = {10.1109/TMC.2023.3317943},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MhaisenIL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The design of effective online caching policies is an increasingly important problem for content distribution networks, online social networks and edge computing services, among other areas. This paper proposes a new algorithmic toolbox for tackling this problem through the lens of optimistic online learning. We build upon the Follow-the-Regularized-Leader (FTRL) framework, which is developed further here to include predictions for the file requests, and we design online caching algorithms for bipartite networks with pre-reserved or dynamic storage subject to time-average budget constraints. The predictions are provided by a content recommendation system that influences the users viewing activity and hence can naturally reduce the caching network's uncertainty about future requests. We also extend the framework to learn and utilize the best request predictor in cases where many are available. We prove that the proposed optimistic learning caching policies can achieve sub-zero performance loss (regret) for perfect predictions, and maintain the sub-linear regret bound O(\\sqrt{T}), which is the best achievable bound for policies that do not use predictions, even for arbitrary-bad predictions. The performance of the proposed algorithms is evaluated with detailed trace-driven numerical tests.}
}


@article{DBLP:journals/tmc/XuZLWC24,
	author = {Anran Xu and
                  Zhenzhe Zheng and
                  Qinya Li and
                  Fan Wu and
                  Guihai Chen},
	title = {{VAP:} Online Data Valuation and Pricing for Machine Learning Models
                  in Mobile Health},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5966--5983},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316145},
	doi = {10.1109/TMC.2023.3316145},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuZLWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile health (mHealth) applications, benefiting from mobile computing, have generated numerous mHealth data. However, they are dispersed across isolated devices, which hinders discovering insights underlying the aggregated data. Considering the online characteristics of mHealth, in this work, we present the first online data VAluation and Pricing mechanism, namely VAP, to incentive users to contribute mHealth data for machine learning (ML) tasks in mHealth systems. Under the Bayesian framework, we propose a new metric based on the concept of entropy to calculate data valuation during model training in an online manner. In proportion to the data valuation, we then determine payments as compensations for users to contribute their data. We formulate this pricing problem as a contextual multi-armed bandit with the goal of profit maximization and propose a new algorithm based on the characteristics of pricing. Furthermore, to tackle the budget constraint, we incorporate a two-stage multi-armed bandit with a knapsack method. We also extend VAP to advanced ML models by computing the entropy on the prediction space. Finally, we have evaluated VAP on two real-world mHealth data sets. Evaluation results show that VAP outperforms the state-of-the-art data valuation and pricing mechanisms in terms of computational complexity and extracted profit.}
}


@article{DBLP:journals/tmc/YangWWXXZ24,
	author = {Dingcheng Yang and
                  Jun Wang and
                  Fahui Wu and
                  Lin Xiao and
                  Yu Xu and
                  Tiankui Zhang},
	title = {Energy Efficient Transmission Strategy for Mobile Edge Computing Network
                  in UAV-Based Patrol Inspection System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5984--5998},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315477},
	doi = {10.1109/TMC.2023.3315477},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangWWXXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider an unmanned aerial vehicle (UAV)-based patrol inspection scenario, where the cellular-connected UAV traverses multiple pre-determined waypoints for data collection, and then offloads computation task to the ground base stations (GBSs). This paper aims to minimize the sum of the total energy consumption by jointly optimizing the task completion time, communication scheduling, computation resource allocation, and UAV's trajectory. First, we decompose the original problem into two trackable subproblems: 1) design the optimal traverse order among cruise points; and 2) determine the optimal transmission strategy between two consecutive cruise points. Then, by involving the communication rate performance and the topology construct among the GBSs and the cruise points, a novel weighted factor of the edge is proposed to design the traverse order, which can be compatible with the light and heavy task offloading scenarios. The successive convex approximation (SCA) technique and block coordinate descent (BCD) framework are adopted to optimize the UAV's trajectory and the wireless resource allocation. The numerical results finally indicate that our proposed transmission strategy solution decreases the total energy consumption in various scenarios and outperforms other benchmark schemes.}
}


@article{DBLP:journals/tmc/ZhangLLLZG24,
	author = {Chuan Zhang and
                  Xingqi Luo and
                  Jinwen Liang and
                  Ximeng Liu and
                  Liehuang Zhu and
                  Song Guo},
	title = {{POTA:} Privacy-Preserving Online Multi-Task Assignment With Path
                  Planning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {5999--6011},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315324},
	doi = {10.1109/TMC.2023.3315324},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLLLZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving online multi-task assignment is a crucial aspect of spatial crowdsensing on untrusted platforms, where multiple real-time tasks are allocated to appropriate workers in a privacy-preserving manner. While existing schemes ensure the privacy of tasks and users, they seldom focus on minimizing the total moving distances for crowdsensing workers when assigning multiple tasks in real time, which adversely impacts the efficiency of online multi-task assignments. To address this issue, we propose POTA, the first privacy-preserving online multi-task assignment scheme with path planning that minimizes the total moving distances for crowdsensing workers without additional noise. POTA cryptographically implements the extended minimum-cost flow model, which models the encrypted data of workers and tasks in a graph and later produces optimized routing. With such a secure path-planning component, POTA reduces the total moving distances by 25.19\\%-52.78\\% in the tested dataset compared with the state-of-the-art schemes with obfuscated path planning. Security analysis proves that POTA guarantees the confidentiality of sensitive data, a stronger security property than introducing obfuscation to sensitive data. Experimental evaluations on real-world datasets demonstrate the feasibility of POTA in terms of running time and its ability to achieve minimized total moving distances.}
}


@article{DBLP:journals/tmc/LiuYXWHX24,
	author = {Jianchun Liu and
                  Jiaming Yan and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Jinyang Huang and
                  Yang Xu},
	title = {Finch: Enhancing Federated Learning With Hierarchical Neural Architecture
                  Search},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6012--6026},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315451},
	doi = {10.1109/TMC.2023.3315451},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuYXWHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has been widely adopted to train machine learning models over massive data in edge computing. Most works of FL employ pre-defined model architectures on all participating clients for model training. However, these pre-defined architectures may not be the optimal choice for the FL setting since manually designing a high-performance neural architecture is complicated and burdensome with intense human expertise and effort, which easily makes the model training fall into the local suboptimal solution. To this end, Neural Architecture Search (NAS) has been applied to FL to address this critical issue. Unfortunately, the search space of existing federated NAS approaches is extraordinarily large, resulting in unacceptable completion time on the resource-constrained edge clients, especially under the non-independent and identically distributed (non-IID) setting. In order to remedy this, we propose a novel framework, called Finch, which adopts hierarchical neural architecture search to enhance federated learning. In Finch, we first divide the clients into several clusters according to the data distribution. Then, some subnets are sampled from a pre-trained supernet and allocated to the specific client clusters for searching the optimal model architecture in parallel, so as to significantly accelerate the process of model searching and training. The extensive experimental results demonstrate the high effectiveness of our proposed framework. Specifically, Finch can reduce the completion time by about 30.6%, and achieve an average accuracy improvement of around 9.8% compared with the baselines.}
}


@article{DBLP:journals/tmc/KurtAYMSE24,
	author = {Ahmet Kurt and
                  Kemal Akkaya and
                  Sabri Yilmaz and
                  Suat Mercan and
                  Omer Shlomovits and
                  Enes Erdin},
	title = {LNGate{\textdollar}{\^{}}\{2\}{\textdollar}2: Secure Bidirectional
                  IoT Micro-Payments Using Bitcoin's Lightning Network and Threshold
                  Cryptography},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6027--6044},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3317704},
	doi = {10.1109/TMC.2023.3317704},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KurtAYMSE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bitcoin has emerged as a revolutionary payment system with its decentralized ledger concept; however it has significant problems such as high transaction fees and low throughput. Lightning Network (LN), which was introduced much later, solves most of these problems with an innovative concept called off-chain payments. With this advancement, Bitcoin has become an attractive venue to perform micro-payments which can also be adopted in many IoT applications (e.g., toll payments). Nevertheless, it is not feasible to host LN and Bitcoin on IoT devices due to the storage, memory, and processing restrictions. Therefore, in this paper, we propose a secure and efficient protocol that enables an IoT device to use LN's functions through an untrusted gateway node. Through this gateway which hosts the LN and Bitcoin nodes, the IoT device can open & close LN channels and send & receive LN payments. This delegation approach is powered by a threshold cryptography based scheme that requires the IoT device and the LN gateway to jointly perform all LN operations. Specifically, we propose thresholdizing LN's Bitcoin public and private keys as well as its public and private keys for the new channel states (i.e., commitment points). We prove with a game theoretical security analysis that the IoT device is secure against collusion attacks. We implemented the proposed protocol by changing LN's source code and thoroughly evaluated its performance using several Raspberry Pis. Our evaluation results show that the protocol; is fast, does not bring extra cost overhead, can be run on low data rate wireless networks, is scalable and has negligible energy consumption overhead. To the best of our knowledge, this is the first work that implemented threshold cryptography in LN.}
}


@article{DBLP:journals/tmc/XieLW24,
	author = {Yadong Xie and
                  Fan Li and
                  Yu Wang},
	title = {FingerSlid: Towards Finger-Sliding Continuous Authentication on Smart
                  Devices Via Vibration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6045--6059},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315291},
	doi = {10.1109/TMC.2023.3315291},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, mobile smart devices are widely used in daily life. It is increasingly important to prevent malicious users from accessing private data, thus a secure and convenient authentication method is urgently needed. Compared with common one-off authentication (e.g., password, face recognition, and fingerprint), continuous authentication can provide constant privacy protection. However, most studies are based on behavioral features and vulnerable to spoofing attacks. To solve this problem, we study the unique influence of sliding fingers on active vibration signals, and further propose an authentication system, FingerSlid, which uses vibration motors and accelerometers in mobile devices to sense biometric features of sliding fingers to achieve behavior-independent continuous authentication. First, we design two kinds of active vibration signals and propose a novel signal generation mechanism to improve the anti-attack ability of FingerSlid. Then, we extract different biometric features from the received two kinds of signals, and eliminate the influence of behavioral features in biometric features using a carefully designed Triplet network. Last, user authentication is performed by using the generated behavior-independent biometric features. FingerSlid is evaluated through a large number of experiments under different scenarios, and it achieves an average accuracy of 95.4% and can resist 99.5% of attacks.}
}


@article{DBLP:journals/tmc/WuGHLXZ24,
	author = {Leijie Wu and
                  Song Guo and
                  Zicong Hong and
                  Yi Liu and
                  Wenchao Xu and
                  Yufeng Zhan},
	title = {Long-Term Adaptive {VCG} Auction Mechanism for Sustainable Federated
                  Learning With Periodical Client Shifting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6060--6073},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3317063},
	doi = {10.1109/TMC.2023.3317063},
	timestamp = {Thu, 18 Jul 2024 08:29:29 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuGHLXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) system needs to incentivize clients since they may be reluctant to participate in the resource consuming process. Existing incentive mechanisms fail to construct a sustainable environment for the long-term development of FL system: 1) They seldom focus on system economic properties (e.g., social welfare, individual rationality, and incentive compatibility) to guarantee client attraction. 2) Current online auction modeling methods divide the whole continual process into multiple independent rounds and solve them one-by-one, which breaks the correlation between each round. Besides, the inherent characteristics of FL system (model-agnostic and privacy-sensitive) also prevent it from the optimal strategy by precise mathematical analysis. 3) Current system modelings ignore the practical problem of periodical client shifting, which cannot adaptively update its strategy to handle system dynamics. To overcome the above challenges, this paper proposes a long-term adaptive Vickrey–Clarke–Groves (VCG) auction mechanism for FL system, which incorporate a multi-branch deep reinforcement learning (DRL) algorithm. First, VCG auction is the only one that can simultaneously guarantee all crucial economic properties. Second, we extend the economic properties to long-term forms and apply the experience-driven DRL algorithm to directly obtain long-term optimal strategy, without any prior system knowledge. Third, we reconstruct a multi-branch DRL network to accommodate periodical client shifting by adaptive decision head switching for different time periods. Finally, we theoretically prove he extended economic properties (i.e., IC) and conduct extensive experiments on several real-world datasets. Compared with state-of-the-art approaches, the long-term social welfare of FL system increases by 36% with a 37% reduction in payment. Besides, the multi-branch network can adaptively handle periodical client shifting on the timeline.}
}


@article{DBLP:journals/tmc/ErnestM24,
	author = {Tan Zheng Hui Ernest and
                  A. S. Madhukumar},
	title = {Computation Offloading in MEC-Enabled IoV Networks: Average Energy
                  Efficiency Analysis and Learning-Based Maximization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6074--6087},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315275},
	doi = {10.1109/TMC.2023.3315275},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ErnestM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the energy efficiency of computation offloading strategies in multi-access edge computing-enabled (MEC-enabled) Internet-of-Vehicles (IoV) networks. First, the energy efficiency of computation offloading strategies in the MEC-enabled IoV network are derived in closed-form. Thereafter, a multi-agent deep reinforcement learning based (MADRL-based) energy efficiency maximization algorithm is proposed to enable computation offloading strategies to attain maximum energy efficiency in the MEC-enabled IoV network. It is shown through extensive analysis that the maximum attained energy efficiency hinges on the choice of task size and transmission timeout threshold, with a computation offloading strategy that jointly considers transmission and computation latencies outperforming existing strategies. It is also shown that the proposed MADRL-based energy efficiency maximization algorithm achieves near-optimal energy efficiency in the MEC-enabled IoV network, making it a promising solution towards achieving energy efficient MEC-enabled IoV networks.}
}


@article{DBLP:journals/tmc/HanYZYSWG24,
	author = {Lei Han and
                  Zhiwen Yu and
                  Xuan Zhang and
                  Zhiyong Yu and
                  Weihua Shan and
                  Liang Wang and
                  Bin Guo},
	title = {Co-Optimization of Cell Selection and Data Offloading in Sparse Mobile
                  Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6088--6103},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315232},
	doi = {10.1109/TMC.2023.3315232},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HanYZYSWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell selection and data offloading are the keys to obtaining MCS services with low sensing cost and low data processing delay. Due to the spatiotemporal correlation between data and the local-area coverage of edge servers, cell selection and data offloading will affect each other and require co-optimization. To achieve the co-optimization, we design the method OptInter based on the hierarchical reinforcement learning. OptInter can realize the interactive training between cell selection model and data offloading model. Finally, we evaluate our proposed method based on four datasets, each of which composited by real-world (e.g., NO_{2}\nconcentration, AQI value, Didi order, and Didi trajectory) data and simulated data. Compared with the four baseline methods (e.g., OptMOEA/D, OptStageCD, OptStageDC, and OptWeight), the comprehensive performance of our proposed method can be improved by 11.83%, 20.48%, 10.14%, and 42.27% on average, respectively.}
}


@article{DBLP:journals/tmc/ZhangYZ24,
	author = {Zhaofeng Zhang and
                  Sheng Yue and
                  Junshan Zhang},
	title = {Towards Resource-Efficient Edge {AI:} From Federated Learning to Semi-Supervised
                  Model Personalization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6104--6115},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316189},
	doi = {10.1109/TMC.2023.3316189},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A central question in edge intelligence is “how can an edge device learn its local model with limited data and constrained computing capacity?” In this study, we explore the approach where a global model initialization is first obtained by running federated learning (FL) across multiple edge devices, based on which a semi-supervised algorithm is devised for a single edge device to carry out quick adaptation with its local data. Specifically, to account for device heterogeneity and resource constraints, a global model is first trained via FL, where each device conducts multiple local updates only for its customized subnet. A subset of devices can be selected to upload updates for aggregation during each training round. Further, device scheduling is optimized to minimize the training loss of FL, subject to resource constraints, based on the carefully crafted reward function defined as the one-round progress of FL each device can provide. We examine the convergence behavior of FL for the general non-convex case. For semi-supervised model personalization, we use the FL-based model initialization as a teacher network to impute soft labels on unlabeled data, thereby addressing the insufficiency of labeled data. Experiments are conducted to evaluate the performance of the proposed algorithms.}
}


@article{DBLP:journals/tmc/RenGW24,
	author = {Jie Ren and
                  Ling Gao and
                  Zheng Wang},
	title = {JavaScript Performance Tuning as a Crowdsourced Service},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6116--6132},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316167},
	doi = {10.1109/TMC.2023.3316167},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {JavaScript (JS) is one of the most used programming languages for mobile applications. As JS is increasingly used in computation-intensive and latency-sensitive components, JS application performance can significantly impact user experience. While compilers play a crucial role in optimizing JS performance on mobile systems, their optimizations must be simple due to the computation and battery usage limitations of the underlying hardware platforms. We present JSTuner, a machine-learning system to leverage compiler-based autotuning techniques to optimize JS performance by finding a good compiler optimization sequence. JSTuner is designed to reduce the cost of autotuning by using prior knowledge of JS programs collected through a crowdsourcing framework to bootstrap the search process. It allows the user to seamlessly utilize the computation resources of a cloud server to perform the heavy-lifting autotuning process for repeatedly running JS components. This enables aggressive search-based optimizations that are too expensive to run on the user's device. We evaluate JSTuner by applying it to 60 JS benchmarks across three distinct mobile devices and comparing it against four search-based techniques. Experimental results show that JSTuner consistently outperforms prior techniques and improves JS performance by 1.62x on average (up to 3.33x) over the default compiler setting used by the Chrome V8 JS engine.}
}


@article{DBLP:journals/tmc/LiuXGCXYGYWL24,
	author = {Chunchi Liu and
                  Minghui Xu and
                  Hechuan Guo and
                  Xiuzhen Cheng and
                  Yinhao Xiao and
                  Dongxiao Yu and
                  Bei Gong and
                  Arkady Yerukhimovich and
                  Shengling Wang and
                  Weifeng Lyu},
	title = {{TBAC:} {A} Tokoin-Based Accountable Access Control Scheme for the
                  Internet of Things},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6133--6148},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316622},
	doi = {10.1109/TMC.2023.3316622},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuXGCXYGYWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Overprivilege Attack, a widely reported phenomenon in IoT that accesses unauthorized or excessive resources, is notoriously hard to prevent, trace and mitigate. In this paper, we propose TBAC, a Tokoin-Based Access Control model enabled by blockchain and Trusted Execution Environment (TEE) technologies, to offer fine-grained access control and strong auditability for IoT. TBAC materializes the virtual access power into a definite-amount, secure and accountable cryptographic coin, termed “tokoin” (token+coin), and manages it using atomic and accountable state-transition functions in a blockchain. A tokoin carries a fine-grained policy defined by the resource owner to specify the requirements to be satisfied before an access is granted, and the behavioral constraints that describe the correct procedure to follow during access. The strong-auditability is achieved with blockchain and a TEE-enabled trusted access control object (TACO) to ensure that all access activities are securely monitored and auditable. We prototype TBAC by implementing all its functions with well-studied cryptographic primitives over different blockchain platforms, building a TACO on top of the ARM Cortex-M33 TEE microcontroller, and constructing a user-friendly APP for regular users. A case study is finally presented to demonstrate how TBAC is employed to enable autonomous and secure in-home cargo delivery.}
}


@article{DBLP:journals/tmc/AbderrahimAS24,
	author = {Wiem Abderrahim and
                  Osama Amin and
                  Basem Shihada},
	title = {Data Center-Enabled High Altitude Platforms: {A} Green Computing Alternative},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6149--6162},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3316204},
	doi = {10.1109/TMC.2023.3316204},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AbderrahimAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information technology organizations and companies are seeking greener alternatives to traditional terrestrial data centers to mitigate global warming and reduce carbon emissions. Currently, terrestrial data centers consume a significant amount of energy, estimated at about 1.5% of worldwide electricity use. Furthermore, the increasing demand for data-intensive applications is expected to raise energy consumption, making it crucial to consider sustainable computing paradigms. In this study, we propose a data center-enabled High Altitude Platform (HAP) system, where a flying data center supports the operation of terrestrial data centers. We conduct a detailed analytical study to assess the energy benefits and communication requirements of this approach. Our findings demonstrate that a data center-enabled HAP is more energy-efficient than a traditional terrestrial data center, owing to the naturally low temperature in the stratosphere and the ability to harvest solar energy. Adopting a data center-HAP can save up to 14% of energy requirements while overcoming the offloading outage problem and the associated delay resulting from server distribution. Our study highlights the potential of a data center-enabled HAP system as a sustainable computing solution to meet the growing energy demands and reduce carbon footprint.}
}


@article{DBLP:journals/tmc/BoutibaBK24,
	author = {Karim Boutiba and
                  Miloud Bagaa and
                  Adlen Ksentini},
	title = {Multi-Agent Deep Reinforcement Learning to Enable Dynamic {TDD} in
                  a Multi-Cell Environment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6163--6177},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3315961},
	doi = {10.1109/TMC.2023.3315961},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BoutibaBK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic Time Division Duplex (D-TDD) is a promising solution to address newly emerging 5G and 6G services characterized by asymmetric and dynamic uplink (UL) and downlink (DL) traffic demands. However, there are two major issues: (i) determining the TDD scheme (i.e., the number of slots devoted to UL and DL) to meet the dynamic traffic demands of the Users Equipment (UE); (ii) cross-link interference between cells that use different TDD schemes. The 3GPP standard neither specifies algorithms or solutions to derive the TDD configuration nor solves the cross-link interference. To fill this gap, we model the dynamic TDD problem in 5G NR as a linear programming problem. Then, we design Multi-Agent Deep Reinforcement Learning based 5G RAN TDD Pattern (MADRP), a fully decentralized solution based on the Multi-Agent Deep Reinforcement Learning (MADRL) approach. Based on the simulation results, the algorithm effectively prevents buffer overflows, avoids cross-link interference, and adapts to changes in the traffic pattern, ensuring its versatility. We compared our solution with the optimal solution and different static TDD configurations. We found that MADRP outperforms the static TDD configurations. We finally discuss the algorithm's limitations in terms of the number of cells, traffic variance, and cross-link interference probability.}
}


@article{DBLP:journals/tmc/KarmakarKA24b,
	author = {Raja Karmakar and
                  Georges Kaddoum and
                  Ouassima Akhrif},
	title = {A Blockchain-Based Distributed and Intelligent Clustering-Enabled
                  Authentication Protocol for {UAV} Swarms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6178--6195},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3319544},
	doi = {10.1109/TMC.2023.3319544},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KarmakarKA24b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are operated remotely without the presence of a unified system of identity authentication, and wireless communications in untrusted environments can cause the loss of valuable data carried by UAVs. Traditional UAV authentication mechanisms are centralized approaches, which suffer from a single point of failure problem and may incur high complexity computations. Therefore, it is crucial to establish a distributed authentication mechanism between the ground station controller (GSC) and a UAV. Moreover, in case of UAV swarms, the high mobility of the UAVs affects the stability of UAV communications, which leads to the degradation of the UAV authentication performance. Addressing these challenges, we design a blockchain-based distributed authentication mechanism, known as SwarmAuth, for UAV swarms, where the GSC and UAVs follow a mutual authentication approach using physical unclonable functions (PUFs), and the K-means clustering-based intelligent approach is used to dynamically create location-based clusters. The blockchain helps store UAVs’ authentication information in an immutable storage and the associated smart contracts provide a convenient access control model. The security analysis of SwarmAuth is carried out through both formal and informal proofs considering general attacks. Experimental evaluation shows that SwarmAuth can assure trustworthy communications and improve the network performance.}
}


@article{DBLP:journals/tmc/LuongLFDNK24,
	author = {Nguyen Cong Luong and
                  Thuan Van Le and
                  Shaohan Feng and
                  Hongyang Du and
                  Dusit Niyato and
                  Dong In Kim},
	title = {Edge Computing for Metaverse: Incentive Mechanism versus Semantic
                  Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6196--6211},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3317092},
	doi = {10.1109/TMC.2023.3317092},
	timestamp = {Mon, 07 Oct 2024 21:54:12 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuongLFDNK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate incentive mechanism designs for edge computing trading between virtual service providers (VSPs) and an edge computing provider (ECP). The VSPs deploy unmanned aerial vehicles (UAVs) to collect sensing data from physical objects for updating their digital twins (DTs). In the case with a single computing unit, we design a deep learning (DL)-based auction constructed from the Myerson theorem to maximize the ECP's revenue and guarantee incentive compatibility (IC) and individual rationality (IR). In the case of multiple computing units, a DL-based auction based on an augmented Lagrangian method is proposed that maximizes the ECP's revenue and guarantees IC, IR, and budget (BG) constraints. A semantic communication (SemCom) technique is employed to reduce the collected data and offloading cost for the VSPs. To train the deep learning algorithms, we use valuations of the computing resources to the VSPs, which particularly are a function of the age of DT, semantic symbol size, and communication time of the UAVs. We provide numerical results showing that the proposed auctions outperform the classical auctions in terms of ECP's revenue, IR, IC, BG, and their ability of preventing the false bid submissions. Also, SemCom reduces the offloading cost for the VSPs.}
}


@article{DBLP:journals/tmc/ShiGWZH24,
	author = {Siping Shi and
                  Yingya Guo and
                  Dan Wang and
                  Yifei Zhu and
                  Zhu Han},
	title = {Distributionally Robust Federated Learning for Network Traffic Classification
                  With Noisy Labels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6212--6226},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3319657},
	doi = {10.1109/TMC.2023.3319657},
	timestamp = {Fri, 14 Feb 2025 17:43:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiGWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic classifiers of mobile devices are widely learned with federated learning(FL) for privacy preservation. Noisy labels commonly occur in each device and deteriorate the accuracy of the learned network traffic classifier. Existing noise elimination approaches attempt to solve this by detecting and removing noisy labeled data before training. However, they may lead to poor performance of the learned classifier, as the remaining traffic data in each device is few after noise removal. Motivated by the observation that the data feature of the noisy labeled traffic data is clean and the underlying true distribution of the noisy labeled data is statistically close to the clean traffic data, we propose to utilize the noisy labeled data by normalizing it to be close to the clean traffic data distribution. Specifically, we first formulate a distributionally robust federated network traffic classifier learning problem (DR-NTC) to jointly take the normalized traffic data and clean data into training. Then we specify the normalization function under Wasserstein distance to transform the noisy labeled traffic data into a certified robust region around the clean data distribution, and we reformulate the DR-NTC problem into an equivalent DR-NTC-W problem. Finally, we design a robust federated network traffic classifier learning algorithm, RFNTC, to solve the DR-NTC-W problem. Theoretical analysis shows the robustness guarantee of RFNTC. We evaluate the algorithm by training classifiers on a real-world dataset. Our experimental results show that RFNTC significantly improves the accuracy of the learned classifier by up to 1.05 times.}
}


@article{DBLP:journals/tmc/JiZZL24,
	author = {Guoliang Ji and
                  Baoxian Zhang and
                  Guo Zhang and
                  Cheng Li},
	title = {Online Incentive Mechanisms for Socially-Aware and Socially-Unaware
                  Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6227--6242},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3321701},
	doi = {10.1109/TMC.2023.3321701},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) has been a promising paradigm for gathering sensing data from surrounding environment by leveraging smart devices carried by mobile users and also their subjective initiatives. In this sensing paradigm, mobile users can make full use of such sensors-rich smart devices for task executions. Recently, social mobile crowdsensing (SMCS) has received a lot of attention and much work has been carried out. Many incentive mechanisms exploit the social relations among users/workers for improving the system performance. However, most existing work in this area focused on offline and socially-aware scenarios. In this paper, we study both online socially-aware and socially-unaware scenarios for maximizing the platform utility. We formulate the problem of worker selection for maximizing the platform utility and prove this problem is NP-hard. For the socially-aware scenario, we propose an incentive mechanism (called SA-WGRA), which adopts sociality and capability based clustering algorithm for Worker Group formation and uses Reverse Auction for worker selection. For the socially-unaware scenario, we propose an incentive mechanism (called SUA-CGRA), which adopts Coalitional Game combined with Reversed Auction for worker selection. We prove that both mechanisms achieve computational efficiency, individual rationality, and platform rationality. Moreover, for SUA-CGRA, we prove that its formed coalitions satisfy coalition rationality, and further each of its formed coalitions is convex and hence the Shapley value is in the core solutions for profit distribution in each formed coalition. Simulations results show that both SA-WGRA and SUA-CGRA can effectively improve the platform utility.}
}


@article{DBLP:journals/tmc/YangAZY24,
	author = {Xueyuan Yang and
                  Zhenlin An and
                  Xiaopeng Zhao and
                  Lei Yang},
	title = {Transfer Beamforming via Beamforming for Transfer},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6243--6257},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3318741},
	doi = {10.1109/TMC.2023.3318741},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangAZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although billions of battery-free backscatter devices (e.g., RFID tags) are intensively deployed nowadays, they are still unsatisfying in the two major performance limitations (i.e., short reading range and high miss reading rate) resulting from the current harvesting inefficiency. The classic beamforming technique is regarded as the most promising solution to address the issue. However, applying it to backscatter systems meets the deadlock start problem, i.e., without enough power, the backscatter cannot wake up to provide channel parameters; but, without channel parameters, the system cannot form beams to provide power. In this work, we propose a new paradigm called transfer beamforming ({\\sf TBF}), namely, the beamforming strategies can be transferred from reference tags with known positions to power up other unknown neighbor tags of interest. In short, transfer beamforming (is accomplished) via (launching) beamforming (to reference tags first) for (the purpose of) transfer. To do so, we adopt the semi-active tags as the reference tags, which can be powered up with a normal reader in a wide range. Then the beamforming is initiated and transferred to power up the low-sensitive but cost-effective passive tags surrounded by reference tags. A prototype evaluation of {\\sf TBF} with 8 transmitting antennas presents a 99.9% inventory coverage rate in a crowded warehouse with 2,160 RFID tags. Our comprehensive evaluation reveals that {\\sf TBF} can improve the power transmission by 6.9 dB and boost the inventory speed by 2× compared with state-of-art methods.}
}


@article{DBLP:journals/tmc/DingZZWTTM24,
	author = {Lige Ding and
                  Dong Zhao and
                  Boqing Zhu and
                  Zhaofeng Wang and
                  Chang Tan and
                  Jianjun Tong and
                  Huadong Ma},
	title = {SpeedAdv: Enabling Green Light Optimized Speed Advisory for Diverse
                  Traffic Lights},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6258--6271},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3319697},
	doi = {10.1109/TMC.2023.3319697},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DingZZWTTM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Green Light Optimized Speed Advisory (GLOSA) systems have emerged to allow drivers to pass traffic lights during a green interval. However, various adaptive and intelligent traffic light control approaches have been adopted in many cities, resulting in the development of current GLOSA technologies lagging behind that of traffic light technologies. When taking diverse dynamic traffic lights into account, it is difficult to model the interactions between vehicles and traffic lights, which is further exacerbated by the hybrid control strategies of traffic lights. To this end, we design a new GLOSA system SpeedAdv to provide optimal speed advisory for addressing diverse traffic lights. We formulate the problem as a Multi-Agent Markov Decision Process (MAMDP) with an implicit common goal and propose a heterogeneous-agent collaborative framework based on reinforcement learning. Three main modules are used in the system: i) a spatio-temporal relation reasoning module based on the phase-aware attention mechanism pays more attention to the traffic rules and traffic flow diversion of adjacent intersections to predict traffic conditions for a few seconds later; ii) a behavior approximating module based on imitation learning is introduced to approximate the phases of diverse traffic lights; iii) a speed advisory module provides the optimal speed advisory based on policy gradient reinforcement learning relying on the above two modules and other information collected by vehicles. We implement and evaluate SpeedAdv with a real-world trajectory dataset, together with a field test based on a prototype system, demonstrating that SpeedAdv improves the overall performance by at least 24.1% in terms of travel time, energy consumption, safety, and comfort compared to the state-of-the-art GreenDrive method.}
}


@article{DBLP:journals/tmc/GomezVegaLGWC24,
	author = {Carlos A. G{\'{o}}mez{-}Vega and
                  Zhenyu Liu and
                  Carlos A. Guti{\'{e}}rrez and
                  Moe Z. Win and
                  Andrea Conti},
	title = {Efficient Deployment Strategies for Network Localization With Assisting
                  Nodes},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6272--6287},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333309},
	doi = {10.1109/TMC.2023.3333309},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GomezVegaLGWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location awareness is crucial for a variety of emerging applications. The accuracy of localization depends heavily on the spatial topology of the network, especially in complex and infrastructure-limited wireless environments. In these environments, assisting nodes can be deployed to achieve desirable localization performance. This paper presents efficient strategies for deploying assisting nodes to improve the localization accuracy of a target agent. Specifically, it provides a methodology to determine a finite set of candidate positions for the assisting nodes. Based on this methodology, we present a convex relaxation method to select near-optimal positions for the assisting nodes and establish a theoretical limit on the localization accuracy provided by assisting nodes. We also propose an approximate dynamic programming algorithm to deploy assisting nodes with amenable complexity. A case study validates the proposed strategies and shows the benefits of deploying assisting nodes for accurate localization.}
}


@article{DBLP:journals/tmc/DinaniHNMR24,
	author = {Mina Aghaei Dinani and
                  Adrian Holzer and
                  Hung Nguyen and
                  Marco Ajmone Marsan and
                  Gianluca Rizzo},
	title = {A Gossip Learning Approach to Urban Trajectory Nowcasting for Anticipatory
                  {RAN} Management},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6288--6303},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320551},
	doi = {10.1109/TMC.2023.3320551},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DinaniHNMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In future radio access networks, machine learning (ML) based strategies for short-term forecasting of vehicular trajectories will be key for anticipatory resource allocation and management at the mobile edge. However, training ML models in a centralized fashion, over data collected from a massive heterogeneous and dynamic set of devices, poses significant scalability, reliability, and efficiency challenges, which are still open to date. In this article, we look at the specific issue of scalable and resource-efficient training of ML models in a vehicular environment. To address such a challenge, we propose a new Gossip Learning scheme, i.e., a fully distributed, collaborative training approach based on direct, opportunistic model exchanges via wireless device-to-device (D2D) communications with no centralized support. Our approach is based on constantly improving each node's own model instance through knowledge transfer among nodes, and on different strategies for estimating the potential contribution of neighboring nodes to the training process at a node. Extensive numerical assessments on a variety of measurement-based dynamic urban scenarios suggest that our schemes are able to converge rapidly and provide sufficiently accurate forecasts of vehicle position for time horizons which are typical of future 5 G/6 G dynamic resource allocation algorithms.}
}


@article{DBLP:journals/tmc/ChenYKZT24,
	author = {Yunzhong Chen and
                  Jiadi Yu and
                  Linghe Kong and
                  Yanmin Zhu and
                  Feilong Tang},
	title = {Sensing Human Gait for Environment-Independent User Authentication
                  Using Commodity {RFID} Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6304--6317},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3318753},
	doi = {10.1109/TMC.2023.3318753},
	timestamp = {Mon, 15 Apr 2024 08:25:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYKZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gait-based user authentication schemes have been widely explored because of their ability of non-invasive sensing and avoid replay attacks. However, existing gait-based user authentication methods are environment-dependent. In this paper, we present an environment-independent gait-based user authentication system, RFPass, which can identify different individuals leveraging RFID signals. Specifically, we find that Doppler shift of RF signals can describe environment-independent gait features for different individuals. In RFPass, when a user walks through the RFPass system, RF signals are first collected by a deployed RFID tag array. Then, RFPass removes environmental interference from the collected RF signals through a proposed Multipath Direction of arrival (DoA) Signal Select (MDSS) algorithm, and further constructs the environment-independent gait profile. Afterwards, environment-independent gait features are extracted from the constructed gait profile by a proposed CNN-RNN model. Based on the extracted gait features, a hierarchical classifier is trained for user authentication and spoofer detection. Extensive experiments in different real environments demonstrate that RFPass can achieve environment-independent gait-based user authentication.}
}


@article{DBLP:journals/tmc/GhoseGLLXL24,
	author = {Nirnimesh Ghose and
                  Kaustubh Gupta and
                  Loukas Lazos and
                  Ming Li and
                  Ziqi Xu and
                  Jincheng Li},
	title = {{ZITA:} Zero-Interaction Two-Factor Authentication Using Contact Traces
                  and In-Band Proximity Verification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6318--6333},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3321514},
	doi = {10.1109/TMC.2023.3321514},
	timestamp = {Mon, 30 Dec 2024 13:47:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GhoseGLLXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Two-factor authentication provides an additional layer of protection to commonly-occurring password breaches. However, existing TFA methods, often involve special hardware interfaces, or require human effort which is prone to errors and acts as an adoption detractor for older adults and novice technology users. To address these limitations, we propose a zero-interaction, two-factor authentication (ZITA) protocol. In ZITA, the first factor is implemented using the conventional username and password methods. The second factor is completed without any human effort provided that the user is not accessing the service from an unregistered public device and a designated secondary device is physically co-present. To automate the second factor, ZITA exploits the long-term contact between the login device and the secondary device such as a smartphone. Moreover, to thwart man-in-the-middle and co-located attacks, ZITA incorporates a proximity verification test that relies on the randomness of ambient RF signals. Compared with other zero-effort TFA protocols, ZITA remains secure against advanced threats and does not require out-of-band sensors such as microphones, speakers, or photoplethysmography (PPG) sensors.}
}


@article{DBLP:journals/tmc/YaoYLQKL24,
	author = {Xin{-}Wei Yao and
                  Xiao{-}Tian Yang and
                  Qiang Li and
                  Chu{-}Feng Qi and
                  Xiangjie Kong and
                  Xiang{-}Yang Li},
	title = {{UMIM:} Utility-Maximization Incentive Mechanism for Mobile Crowd
                  Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6334--6346},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320106},
	doi = {10.1109/TMC.2023.3320106},
	timestamp = {Mon, 24 Jun 2024 20:34:52 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YaoYLQKL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowd Sensing (MCS) represents a novel paradigm which utilizes intelligent devices carried by mobile users to collect and transmit data. Appropriate incentives are essential to recruit enough participants for sensing tasks. Existing works have designed some incentive mechanisms for MCS, which are not suitable for scenarios when the participants increase significantly as a result of the booming cost. To solve the above cost problem, a Utility-Maximization Incentive Mechanism (UMIM) is proposed in this paper by leveraging the influence propagation on the social network. Participants in the same social network can benefit from the data shared by others, which shows the utility of sensing data and can be regarded as a non-monetary incentive and make the participants stay positive under relative low payoff. Therefore, by improving the utility of sensing data, the incentive cost can be effectively reduced. To maximize the data utility, we further design a tree-based structure to improve the priority experience replay mechanism of Proximal Policy Optimization (PPO) in UMIM. This improvement makes the high priority experience to be sampled more quickly and efficiently, as a result, the network can learn more effectively. Numerical results show that UMIM can further improve the data utility and have better convergence.}
}


@article{DBLP:journals/tmc/HuNRG24,
	author = {Zheyuan Hu and
                  Jianwei Niu and
                  Tao Ren and
                  Mohsen Guizani},
	title = {Achieving Fast Environment Adaptation of DRL-Based Computation Offloading
                  in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6347--6362},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320253},
	doi = {10.1109/TMC.2023.3320253},
	timestamp = {Mon, 03 Mar 2025 09:23:10 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuNRG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the key issues in mobile edge computing (MEC) is computation offloading, most policies of which are developed based on mathematical programming (MP). Due to the high computational complexity of iterative programming in MP-based policies, recent years have seen a popular trend to develop offloading policies based on deep reinforcement learning (DRL). However, on account of the poor generalization ability of DRL models in MEC environments with different network sizes and settings, it is difficult to directly apply DRL-based offloading policies in unseen MEC environments. Motivated by this, we propose a DRL-based environment-adaptive offloading framework (DEAT), including a size-adaptive scheme (SIED) and setting-adaptive component (SEAL). SIED leverages the idea of ‘time division multiplexing’ to adapt to varying MEC network sizes and order-unaware feature extraction to mitigate impacts of different size-changing orders. SEAL adopts system dynamics embedding and offloading policy embedding, which guide the finding of the closest pre-training MEC environment and offloading policy, respectively, to achieve fast setting-adaptation with only few exploring interactions in unseen MEC environments. Extensive experiments are conducted via both simulation and testbed to demonstrate the adaptation performance advantages of DEAT in unseen MEC environments compared to the state-of-the-art offloading approaches.}
}


@article{DBLP:journals/tmc/AminiAWB24,
	author = {Mohammad Reza Amini and
                  Ala'a Al{-}Habashna and
                  Gabriel A. Wainer and
                  Gary Boudreau},
	title = {DQ-Based Random Access {NOMA} for Massive Critical IoT Scenarios in
                  5G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6363--6376},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3319545},
	doi = {10.1109/TMC.2023.3319545},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AminiAWB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet-of-Things (IoT) networks provide massive connectivity for many application scenarios. Recently, much work has been dedicated to develop spectrum access strategies for IoT networks with a massive number of nodes and sporadic data traffic behavior. The case becomes more challenging in critical applications when Ultra-Reliable Low-Latency (URLL) transmissions are required. Such networks entail spectrum-efficient transmission schemes in which Non-Orthogonal Multiple-Access (NOMA) is considered a key enabler. We proposed a Distributed Queuing (DQ) approach in NOMA for critical massive IoT (mIoT) applications. More specifically, we introduce a frame structure to support DQ-based NOMA so that dynamic NOMA clustering (at the nodes) and dynamic Successive Interference Cancellation (SIC) ordering at the Base Station (BS) are supported. We also use adaptive power back-off strategy to reduce power collisions by utilizing both nodes’ and clusters’ activation index. We investigate network performance metrics, such as reliability, delay violation probability, and effective sum rate. These metrics are derived analytically, and the effect of different network parameters such as blocklength, active node arrival rate, and the number of contention subslots on the network metrics are investigated and compared with the S-ALOHA-TD benchmark.}
}


@article{DBLP:journals/tmc/WangWWZMY24,
	author = {Xueyi Wang and
                  Dongkuo Wu and
                  Xingwei Wang and
                  Rongfei Zeng and
                  Lianbo Ma and
                  Ruiyun Yu},
	title = {Truthful Auction-Based Resource Allocation Mechanisms With Flexible
                  Task Offloading in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6377--6391},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320104},
	doi = {10.1109/TMC.2023.3320104},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWWZMY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computation (MEC) has recently emerged as a promising computing paradigm for supporting latency-sensitive mobile applications. Due to the limited resources of the edge servers (ESs), efficient resource allocation mechanisms are key to realize the MEC paradigm. In such a resource allocation process, it is a significant challenge to guarantee truthfulness while enabling flexible task offloading and satisfying the locality constraint. To address such a challenge, we propose a truthful auction-based resource allocation mechanism with flexible task offloading (TARFO) in an MEC system. Specifically, we first design the minimum delay task graph partitioning algorithm, aiming at calculating the minimum completion time and the task offloading solutions under different resource profiles. Based on this algorithm, for each smart mobile device (SMD), we further determine the set of feasible non-dominated resource profiles and the corresponding task offloading solutions. We next propose an efficient primal-dual approximation winning bid selection algorithm to determine the set of the winning bids and a critical value based pricing algorithm to calculate the payments of the winning bids. Strict theoretical analysis demonstrates TARFO can ensure truthfulness, individual rationality, computational efficiency and a smaller approximation ratio. Simulation results verify the effectiveness and efficiency of TARFO.}
}


@article{DBLP:journals/tmc/BaghersalimiTAA24,
	author = {Saleh Baghersalimi and
                  Tom{\'{a}}s Teijeiro and
                  Amir Aminifar and
                  David Atienza},
	title = {Decentralized Federated Learning for Epileptic Seizures Detection
                  in Low-Power Wearable Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6392--6407},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320862},
	doi = {10.1109/TMC.2023.3320862},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BaghersalimiTAA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In healthcare, data privacy of patients regulations prohibits data from being moved outside the hospital, preventing international medical datasets from being centralized for AI training. Federated learning (FL) is a data privacy-focused method that trains a global model by aggregating local models from hospitals. Existing FL techniques adopt a central server-based network topology, where the server assembles the local models trained in each hospital to create a global model. However, the server could be a point of failure, and models trained in FL usually have worse performance than those trained in the centralized learning manner when the patient's data are not independent and identically distributed (Non-IID) in the hospitals. This paper presents a decentralized FL framework, including training with adaptive ensemble learning and a deployment phase using knowledge distillation. The adaptive ensemble learning step in the training phase leads to the acquisition of a specific model for each hospital that is the optimal combination of local models and models from other available hospitals. This step solves the non-IID challenges in each hospital. The deployment phase adjusts the model's complexity to meet the resource constraints of wearable systems. We evaluated the performance of our approach on edge computing platforms using EPILEPSIAE and TUSZ databases, which are public epilepsy datasets.}
}


@article{DBLP:journals/tmc/WangJPL24,
	author = {Mengyuan Wang and
                  Hongbo Jiang and
                  Peng Peng and
                  Youhuan Li},
	title = {Accurately Estimating Frequencies of Relations With Relation Privacy
                  Preserving in Decentralized Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6408--6422},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320669},
	doi = {10.1109/TMC.2023.3320669},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangJPL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Abundant valuable knowledge can be obtained by learning frequencies of relations in a decentralized network, which benefits various further complex tasks, such as range query and commodity recommendation. Nonetheless, counting frequencies in original networks can reveal sensitive data and pose a risk to individual privacy, specifically relation privacy. Current privacy notions do not fully preserve both the privacy of relations’ values and existence. In this paper, we introduce an enhanced privacy notion, relation local differential privacy (relation LDP), which provides comprehensive preservation in relation privacy. However, a significant amount of noise in perturbed networks often leads to severe errors in the accuracy of relation frequencies. To obtain accurate frequencies, we propose a framework called frequency estimation based on combination (Fest-C), which decomposes relation frequencies into two independent parts, the total frequency of relations and relative proportions. Binning relations into hyper-relations, Fest-C reduces errors of total frequency and relative proportions, respectively, and then estimates frequencies by logically multiplying them. Finally, we rigorously prove that Fest-C satisfies relation LDP. Our experiments on various datasets confirm the high accuracy of Fest-C in estimating relation frequencies with a much lower time overhead compared to competitors.}
}


@article{DBLP:journals/tmc/KongZQYS24,
	author = {Xiangjie Kong and
                  Wenyi Zhang and
                  Youyang Qu and
                  Xin{-}Wei Yao and
                  Guojiang Shen},
	title = {FedAWR: An Interactive Federated Active Learning Framework for Air
                  Writing Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6423--6436},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3320147},
	doi = {10.1109/TMC.2023.3320147},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KongZQYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of technology such as virtual reality and augmented reality, coupled with the reduced direct contact due to the COVID-19 pandemic, has led to the emergence of a more advanced mode of interaction: air handwriting. This new form of human-computer interaction allows users to input text by writing in the air freely. However, deploying and applying existing air handwriting recognition systems in real-world scenarios still presents challenges, particularly in real-time performance, privacy protection, and label scarcity. To address these challenges, we propose a federated active learning framework called FedAWR for air handwriting recognition tasks. FedAWR utilizes distributed learning to train a shared global model in the cloud from multiple user devices at the network's edge, while keeping the user's handwritten data local to ensure privacy. In addition, FedAWR employs an interactive active learning strategy to collect user-provided annotations for iterative training during the online federated learning process, bootstrapping personalized models for each client. To further enhance interactivity and real-time performance, we designed a lightweight recognition model, which is integrated into FedAWR. Finally, extensive experiments were conducted on real-world air handwritten datasets to validate the superiority of FedAWR.}
}


@article{DBLP:journals/tmc/XieBGWN24,
	author = {Xin Xie and
                  Tong Bai and
                  Weiwei Guo and
                  Zhipeng Wang and
                  Arumugam Nallanathan},
	title = {Cooperative Computing for Mobile Crowdsensing: Design and Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6437--6454},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323350},
	doi = {10.1109/TMC.2023.3323350},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieBGWN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing number of mobile devices, mobile crowdsensing (MCS) has garnered significant attention in research. However, computing infrastructures such as edge/cloud nodes, which are necessary for processing sensor data, are not always readily available. To address this issue, we propose a cooperative computing framework that enables the offloading of sensor data to nearby mobile devices with unused computational resources (known as helpers) for processing. Our approach considers a scenario with multiple sources and multiple helpers, where computational tasks can be partially offloaded to several helpers. We jointly optimize task offloading strategy, communication resources, and computational resources to minimize the weighted sum energy consumption of mobile devices. We model the optimization problem as a mixed-integer nonlinear programming (MINLP), with the source-helper assignment solved using a distributed algorithm based on matching theory, and the joint task partition and resource allocation problem solved using an alternating optimization (AO) method. Simulation results demonstrate the efficacy of our cooperative computing framework and scheduling scheme, which offer significant advantages over local computing in terms of reducing the weighted sum energy consumption and improving the task completion ratio.}
}


@article{DBLP:journals/tmc/QinLLZW24,
	author = {Langtian Qin and
                  Hancheng Lu and
                  Yao Lu and
                  Chenwu Zhang and
                  Feng Wu},
	title = {Joint Optimization of Base Station Clustering and Service Caching
                  in User-Centric {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6455--6469},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323161},
	doi = {10.1109/TMC.2023.3323161},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/QinLLZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge service caching can effectively reduce the delay or bandwidth overhead for acquiring and initializing applications. To address single-base station (BS) transmission limitation and serious edge effect in traditional cellular-based edge service caching networks, in this paper, we proposed a novel user-centric edge service caching framework where each user is jointly provided with edge caching and wireless transmission services by a specific BS cluster instead of a single BS. To minimize the long-term average delay under the constraint of the caching cost, a mixed integer non-linear programming (MINLP) problem is formulated by jointly optimizing the BS clustering and service caching decisions. To tackle the problem, we propose JO-CDSD, an efficiently joint optimization algorithm based on Lyapunov optimization and generalized benders decomposition (GBD). In particular, the long-term optimization problem can be transformed into a primal problem and a master problem in each time slot that is much simpler to solve. The near-optimal clustering and caching strategy can be obtained through solving the primal and master problem alternately. Extensive simulations show that the proposed joint optimization algorithm outperforms other algorithms and can effectively reduce the long-term delay and caching cost.}
}


@article{DBLP:journals/tmc/PengGSGX24,
	author = {Cheng Peng and
                  Linqing Gui and
                  Biyun Sheng and
                  Zhengxin Guo and
                  Fu Xiao},
	title = {RoSeFi: {A} Robust Sedentary Behavior Monitoring System With Commodity
                  WiFi Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {5},
	pages = {6470--6489},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3321306},
	doi = {10.1109/TMC.2023.3321306},
	timestamp = {Mon, 15 Apr 2024 08:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengGSGX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sedentary behaviors are shown to be hazardous to human health. Detecting sedentary behaviors in a ubiquitous way can be realized by the promising WiFi sensing technique. The accurate detection of sedentary behaviors is determined by the accurate recognition of sit-stand postural transition (SPT). However, according to our findings, SPT recognition errors are inevitable even with advanced machine-learning methods, because different SPTs may result in a similar change in WiFi channel state information (CSI). To effectively reduce SPT recognition errors, in this paper we propose RoSeFi, a robust sedentary behavior monitoring system. We first classify the errors in SPT recognition results into two categories: the errors violating SPT's consistency and the errors violating SPTs’ symmetry. To correct the above errors, we reveal two inherent features in the CSI data of SPTs, i.e., contextual association and waveform mirror symmetry. Then a novel metric named WMSF is defined to quantify the degree of waveform mirror symmetry between two SPTs’ CSI data. Integrating the above features, the problem of recognition error correction can be modeled as a constrained nonlinear optimization problem (CNOP). To solve the problem, we design a unified error detection/correction scheme, named UEDC, which converts the CNOP into a sequence decoding problem in Hidden Markov Model (HMM). A tailored Viterbi algorithm combined with WMSF is proposed to detect and correct the errors simultaneously. The experimental results show that RoseFi reduces 60-82% SPT recognition errors, gains 15-20% relative improvement in the accuracy of SPT recognition, and eventually reduces the sedentary time estimation errors by 10%-20%, compared with typical existing systems. In addition, our error correction method can be adapted to most existing machine learning based human action recognition methods, effectively improving their performance.}
}


@article{DBLP:journals/tmc/WangCCVL24,
	author = {Guangchen Wang and
                  Peng Cheng and
                  Zhuo Chen and
                  Branka Vucetic and
                  Yonghui Li},
	title = {Inverse Reinforcement Learning With Graph Neural Networks for Full-Dimensional
                  Task Offloading in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6490--6507},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3324332},
	doi = {10.1109/TMC.2023.3324332},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCCVL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-increasing number of ubiquitous Internet of Things (IoT) applications entails a high demand for scarce communication and network resources. To meet this stringent requirement, mobile edge computing (MEC) is envisioned as a transformative technique to significantly streamline the existing network operations. Recently, device-to-device (D2D) communication has been proposed as a promising technology in 5G and beyond networks with a significantly increased transmission efficiency, especially suitable for small-packet task exchanges. In this paper, we incorporate D2D communication into the multi-layer computing network and propose a full-dimensional task offloading scheme by jointly optimizing task offloading decisions and computation/communication resource allocation. We formulate it as mixed-integer nonlinear programming (MINLP) problem, where the optimal branch-and-bound (B \\& amp; B) algorithm with the full strong branching (FSB) variable selection policy features an extremely high complexity. To address this challenge, we propose inverse reinforcement learning with graph neural networks (GIRL) to generate a new variable selection policy that closely matches the FSB variable selection. Without sacrificing the global optimality, the GIRL can directly infer the variable selection with a much lower complexity, significantly accelerating the original B \\& amp; B algorithm. Simulation results show that the GIRL achieves a lower complexity without sacrificing the global optimality. Furthermore, our proposed full-dimensional task offloading scheme achieves better performance than the existing schemes in terms of average delay for all mobile devices (MDs).}
}


@article{DBLP:journals/tmc/HuQTYLS24,
	author = {Shihong Hu and
                  Zhihao Qu and
                  Bin Tang and
                  Baoliu Ye and
                  Guanghui Li and
                  Weisong Shi},
	title = {Joint Service Request Scheduling and Container Retention in Serverless
                  Edge Computing for Vehicle-Infrastructure Collaboration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6508--6521},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323524},
	doi = {10.1109/TMC.2023.3323524},
	timestamp = {Sun, 04 Aug 2024 19:47:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuQTYLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Lightweight and layered structure containers in serverless edge computing (SEC) provide flexible service configurations and computing for vehicles with diverse service requests in the Vehicle-Infrastructure Collaboration (VIC) environment. Despite progress in service request scheduling for the VIC system, the effect of layer sharing between different service images on request scheduling has not been fully explored. Additionally, the cold-start latency of service containers in SEC can significantly degrade the responsiveness of vehicle services, and container retention is proposed to minimize its impact and improve overall system performance. However, the existing research neglects the complex coupling relationship between request scheduling and container retention decisions, while focusing on the single decision optimization problem. Consequently, minimizing system costs by single decision optimization may not achieve the effect of joint decision optimization. To bridge this gap, we study the joint service request scheduling and container retention problem based on layer sharing and container caching. First, we model the joint decision problem with specific constraints and aim to minimize the long-term system cost while considering vehicle mobility. Second, an online co-decision scheme called Onco is proposed to solve the problem, which incorporates request scheduling and container retention for multiple vehicle services. Finally, both synthetic and real trace-driven simulation experiments have been conducted to evaluate the performance of Onco. The experimental results show that Onco outperforms state-of-the-art baselines in terms of system cost reduction and response time improvement.}
}


@article{DBLP:journals/tmc/WangLXJWWPW24,
	author = {Shulan Wang and
                  Qin Liu and
                  Yang Xu and
                  Hongbo Jiang and
                  Jie Wu and
                  Tian Wang and
                  Tao Peng and
                  Guojun Wang},
	title = {Protecting Inference Privacy With Accuracy Improvement in Mobile-Cloud
                  Deep Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6522--6537},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323450},
	doi = {10.1109/TMC.2023.3323450},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangLXJWWPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide spread of data-driven deep learning applications, a growing number of users outsource compute-intensive inference processes to the cloud. To protect inference privacy, Liu et al. (INFOCOM 2022) proposed two steganography-based solutions, named \\mathsf{GHOST} and \\mathsf{GHOST}^+, relying on the mobile-cloud collaborative framework, where the mobile device hides sensitive images into public cover images before feature extraction, while launching adversarial attacks on the cloud-side deep neural network (DNN) to obtain desired results. Although both solutions demonstrate significant advantages in private deep learning, they suffer from limited practicality; since the inference accuracy decreases sharply as the hiding ratio increases. To address this, we propose two improved solutions, \\mathsf{IGHO} and \\mathsf{IGHO}^+, which ensure high inference accuracy even when abundant sensitive images need to be hidden. Specifically, \\mathsf{IGHO} as the improved version of \\mathsf{GHOST} proposes two feature fusion methods, feature synthesis and pixel synthesis, to preprocess cover images, making the poisoned DNN learn hidden sensitive features better, while \\mathsf{IGHO}^+ as the improved version of \\mathsf{GHOST}^+ designs a novel feature mining generative adversarial network (FMGAN) to craft adversarial perturbations highly robust against variable sensitive types. Experimental results show that the proposed solutions highly improve the practicality of \\mathsf{GHOST} and \\mathsf{GHOST}^+.}
}


@article{DBLP:journals/tmc/PanCZLL24,
	author = {Qiying Pan and
                  Hangrui Cao and
                  Yifei Zhu and
                  Jiangchuan Liu and
                  Bo Li},
	title = {Contextual Client Selection for Efficient Federated Learning Over
                  Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6538--6548},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323645},
	doi = {10.1109/TMC.2023.3323645},
	timestamp = {Fri, 14 Feb 2025 17:43:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PanCZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a prominent distributed learning paradigm, enabling collaborative training of neural network models across local devices with raw data stay local. However, FL systems often encounter significant challenges due to data heterogeneity. Specifically, the non-IID dataset in FL systems substantially slows down the convergence speed during training and adversely impacts the accuracy of the final model. In our article, we introduce a novel client selection framework that judiciously leverages correlations across local datasets to accelerate training. Our framework first employs a lightweight locality-sensitive hashing algorithm to extract client features while respecting data privacy and incurring minimal overhead. We then design a novel Neural Contextual Combinatorial Bandit (NCCB) algorithm to establish relationships between client features and rewards, enabling intelligent selection of client combinations. We theoretically prove that our proposed NCCB has a bounded regret. Extensive experiments on real-world datasets further demonstrate that our framework surpasses state-of-the-art solutions, resulting in a 50% reduction in training time and a 17% increase in final model accuracy, closing to the performance in the ideal IID case.}
}


@article{DBLP:journals/tmc/YangGCC24,
	author = {Lei Yang and
                  Yingqi Gan and
                  Jinru Chen and
                  Jiannong Cao},
	title = {AutoSF: Adaptive Distributed Model Training in Dynamic Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6549--6562},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323456},
	doi = {10.1109/TMC.2023.3323456},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YangGCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed learning on edges aims at training the AI model collaboratively in a network of edge devices via frequent model aggregations. Achieving the desired training performance requires the aggregation structure and frequency to fit well with the dynamic edge environment. Existing works often consider the optimization of either aggregation structure or frequency, assuming that the edge environment is stable and deterministic. In this paper, we propose a novel approach, AutoSF, to automatically optimize the aggregation structure and frequency jointly in dynamic edge computing so as to minimize the global loss function. The main idea of AutoSF is that when the edge environment changes, the automated machine learning approach is triggered to find out the near-optimal aggregation structure and frequency that adapt to time-varying edge resources. When the environment keeps unchanged, a heuristic approach is used to tune the aggregation structure and frequency to further tame the heterogeneity caused by data distributions. We validate the effectiveness of AutoSF via numerical experiments with real datasets on our self-developed edge computing testbed. Evaluation results demonstrate that AutoSF outperforms the benchmark approaches by up to 16.3× speedups in convergence speed and 31.0\\% increases in training accuracy.}
}


@article{DBLP:journals/tmc/RenLMLWZLD24,
	author = {Zhe Ren and
                  Xinghua Li and
                  Yinbin Miao and
                  Zhuowen Li and
                  Zihao Wang and
                  Mengyao Zhu and
                  Ximeng Liu and
                  Robert H. Deng},
	title = {Intelligent Adaptive Gossip-Based Broadcast Protocol for {UAV-MEC}
                  Using Multi-Agent Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6563--6578},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323296},
	doi = {10.1109/TMC.2023.3323296},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RenLMLWZLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (UAV-MEC) has been proposed to offer computing resources for smart devices and user equipment. UAV cluster aided MEC rather than one UAV-aided MEC as edge pool is the newest edge computing architecture. Unfortunately, the data packet exchange during edge computing within the UAV cluster hasn't received enough attention. UAVs need to collaborate for the wide implementation of MEC, relying on the gossip-based broadcast protocol. However, gossip has the problem of long propagation delay, where the forwarding probability and neighbors are two factors that are difficult to balance. The existing works improve gossip from only one factor, which cannot select suitable forwarding probability and avoid redundant messages. Besides, these schemes do not consider the historical packet reception of new neighbors when UAVs fly around, which decreases forwarding efficiency. To solve these problems, we first propose a data structure called Bitgraph that can record the historical packet reception of UAVs. Then, we formulate gossip broadcasting as a partially observable Markov decision process. Based on Bitgraph, we design the reward function. Finally, we design a multi-agent reinforcement learning algorithm, Branching Deep Graph Network (BDGN), which simultaneously makes decisions on forwarding probability and neighbors. Extensive experiments illustrate that our proposal gets more than 29% advantage in terms of the propagation delay and 20% advantage in terms of the redundant messages compared to the existing works.}
}


@article{DBLP:journals/tmc/ZhangLCZWBH24,
	author = {Jing Zhang and
                  Xinzhong Liu and
                  Jie Cui and
                  Hong Zhong and
                  Lu Wei and
                  Irina Bolodurina and
                  Debiao He},
	title = {{CBDDS:} Secure and Revocable Cache-Based Distributed Data Sharing
                  for Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6579--6591},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323305},
	doi = {10.1109/TMC.2023.3323305},
	timestamp = {Sat, 11 Jan 2025 00:33:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLCZWBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In vehicular networks, caching content on an edge server (ES) is a popular method for quickly responding to massive vehicle service requests, reducing communication delays, and enhancing driver and passenger service experiences. However, after integrating ESs with vehicular networks to provide vehicles access to the cached content in these ESs, significant challenges regarding protecting the privacy of vehicle data and communication security arise. In this study, to address security and privacy-preserving issues, we propose a secure and revocable cache-based distributed data sharing scheme for vehicular networks wherein a token authentication mechanism and multi-authority ciphertext-policy attribute-based encryption are integrated. In this scheme, both authentication and authorization capabilities are delegated to an ES while restricting access to service content to only legal vehicles, achieving proper access control between vehicles and ESs, and effectively preserving the privacy of vehicle data. Moreover, we attributed the revocations of ESs to the associated attribute authorities, eliminating the need for a system-wide update of keying materials. Through rigorous security proofs and detailed security analyses, we demonstrate that the scheme meets the security requirements of vehicular networks and can resist more security attacks. The proposed scheme achieves better balance between computational and communication costs than related schemes.}
}


@article{DBLP:journals/tmc/ChrysologouDCSK24,
	author = {Athanasios P. Chrysologou and
                  Panagiotis D. Diamantoulakis and
                  Nestor D. Chatzidiamantis and
                  Harilaos G. Sandalidis and
                  George K. Karagiannidis},
	title = {Next Generation Distributed Radio Access Networks With {FSO} Fronthauling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6592--6605},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3324710},
	doi = {10.1109/TMC.2023.3324710},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChrysologouDCSK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we address a novel framework for next-generation distributed radio access. In contrast with existing studies, where all remote radio heads (RRHs) in a distributed network are directly connected to a central unit (CU), an alternative architecture for advanced flexibility is proposed. In our setup, only one of the RRHs, namely the primary RRH, communicates directly with the CU, while the connectivity between the rest RRHs, namely secondary RRHs, and the CU is achieved through the primary RRH via free-space optical links. Assuming that users exploit non-orthogonal multiple access (NOMA) for their transmissions, we introduce two successive interference cancellation (SIC) cooperation schemes, depending on the one-directional or bidirectional communication between the RRHs, as well as, a four-step centralized algorithm for efficient user-RRH association and decoding order operations is proposed. The feasibility of the suggested schemes is adequately demonstrated by deriving analytical expressions for users’ outage probabilities and providing valuable insights into the high signal-to-noise ratio regime. Furthermore, the performance of the proposed system under various weather conditions is investigated via simulation and analytical results. The comparison with a benchmark scheme, where all RRHs are directly connected to the CU and cooperate with each other via ideal links, is provided and it is revealed that although the performance of the proposed system model is weather dependent, in most of the practical cases it achieves similar performance with the ideal benchmark.}
}


@article{DBLP:journals/tmc/HuJLXZML24,
	author = {Jingyang Hu and
                  Hongbo Jiang and
                  Daibo Liu and
                  Zhu Xiao and
                  Qibo Zhang and
                  Geyong Min and
                  Jiangchuan Liu},
	title = {Real-Time Contactless Eye Blink Detection Using {UWB} Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6606--6619},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323280},
	doi = {10.1109/TMC.2023.3323280},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuJLXZML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blink detection is essential for various human-computer interaction scenarios, such as virtual reality and driving state detection. It has gained significant attention from industry and academia alike in recent years. Existing non-contact detection systems (cameras, acoustics, etc.) have made significant progress, but various issues have prevented their widespread adoption, including privacy concerns, line-of-sight requirements, and cost issues. Therefore, there is a critical need for a simple and robust system that can detect eye blinks using common commercial equipment. In this paper, we propose BlinkRadar, which uses a low-cost customized impulse-radio ultra-wideband (IR-UWB) radar for non-contact and fine-grained blink detection. BlinkRadar can reliably detect driver blinks in driving conditions, making it possible to infer drowsy driving. To effectively extract the eye blink signal, we analyzed real experimental data to study the characteristics of the eye blink pattern and successfully used the multi-sequence variational mode decomposition (MS-VMD) algorithm to separate the blink signal from the noise signal. We conducted extensive experiments in two different environments (a quiet room and moving vehicles) and found that BlinkRadar had an average blink detection accuracy of over 96.2%. Our results demonstrate the feasibility of using UWB radar for non-contact eye blink detection.}
}


@article{DBLP:journals/tmc/WangW24,
	author = {Zihuan Wang and
                  Vincent W. S. Wong},
	title = {Bayesian Meta-Learning for Adaptive Traffic Prediction in Wireless
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6620--6633},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325301},
	doi = {10.1109/TMC.2023.3325301},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless traffic prediction is indispensable for network planning and resource management. Due to different population distributions and user behavior, there exist strong spatial-temporal variations in wireless traffic across different regions. Most of the conventional traffic prediction approaches can only tackle a particular spatial-temporal pattern and cannot capture such variations in wireless traffic. This motivates us to develop an adaptive approach which can tackle spatial-temporal variations and predict wireless traffic in different regions. In this paper, we formulate an adaptive traffic prediction problem from a probabilistic inference perspective and develop a variational spatial-temporal Bayesian meta-learning (VST-BML) algorithm. We model the traffic prediction in different regions as different prediction tasks. The proposed VST-BML algorithm can learn the common spatial-temporal features shared by all prediction tasks, and adaptively infer the task-specific parameters to tackle spatial-temporal variations. We evaluate the performance of our proposed VST-BML algorithm using a real-world traffic dataset. Experimental results show that the proposed algorithm can quickly adapt to different prediction tasks by using only a small number of data samples and provide accurate traffic prediction in different regions. When compared with five baseline methods, the proposed algorithm can reduce the root-mean-square error (RMSE) and mean absolute error (MAE) by 53.0% and 48.4%, respectively.}
}


@article{DBLP:journals/tmc/LuoSYTL24,
	author = {Wenjie Luo and
                  Qun Song and
                  Zhenyu Yan and
                  Rui Tan and
                  Guosheng Lin},
	title = {Indoor Smartphone {SLAM} With Acoustic Echoes},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6634--6649},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3323393},
	doi = {10.1109/TMC.2023.3323393},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoSYTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor self-localization has become a highly desirable system function for smartphones. The existing systems based on imaging, radio frequency, and geomagnetic sensing may have sub-optimal performance when their limiting factors prevail. In this paper, we present a new indoor simultaneous localization and mapping (SLAM) system that is based on the smartphone's built-in audio hardware and inertial measurement unit (IMU). Our system uses a smartphone's loudspeaker to emit near-inaudible chirps and then the microphone to record the acoustic echoes from the indoor environment. The echoes contain the smartphone's location information with sub-meter granularity. To enable SLAM, we apply contrastive learning to train an echoic location feature (ELF) extractor, such that the loop closures on the smartphone's trajectory can be accurately detected from the associated ELF trace. The detection results effectively regulate the IMU-based trajectory reconstruction. The reconstructed trajectories are used for trajectory map superimposition and room geometry reconstruction. Extensive experiments show that our SLAM achieves median localization errors of \\text{0.1}\\,\\text{m}, \\text{0.53}\\,\\text{m}, and \\text{0.4}\\,\\text{m} in a living room, an office, and a shopping mall, and outperforms both the Wi-Fi and geomagnetic SLAM systems. The room geometry reconstruction achieves up to 4× lower errors compared with the latest echo-based approaches.}
}


@article{DBLP:journals/tmc/DengZZWZM24,
	author = {Kaikai Deng and
                  Dong Zhao and
                  Zihan Zhang and
                  Shuyue Wang and
                  Wenxin Zheng and
                  Huadong Ma},
	title = {Midas++: Generating Training Data of mmWave Radars From Videos for
                  Privacy-Preserving Human Sensing With Mobility},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6650--6666},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325399},
	doi = {10.1109/TMC.2023.3325399},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DengZZWZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter wave radar is gaining traction recently for enabling privacy-preserving human sensing. However, the lack of large-scale, dynamic radar datasets impedes progress in developing robust and generalized deep learning models for mobile sensing applications. To address this problem, we resort to designing a software pipeline that leverages wealthy dynamic videos to generate synthetic radar data, but it faces two key challenges including i) incorrect camera and human positions leading to erroneous superposition of signal intensity and ii) the signal reflection of the background and humans in mobile scenes. To this end, we design Midas++ to utilize rich videos to generate realistic radar data via two components: (i) a human mesh fitting and calibration component calculates the camera ego-motion parameters to calibrate the extracted human positions; (ii) a reflection and noise signal estimation component combines several key modules, depth prediction, reflection model, and spatiotemporal noise estimation, to output coarse radar data, followed by a U-Net model to generate realistic radar data. We implement and evaluate Midas++ with video data from public data sources and real-world radar data, demonstrating that Midas++ outperforms other state-of-the-art approaches for both activity recognition and object detection tasks.}
}


@article{DBLP:journals/tmc/HeTB24,
	author = {TianZhang He and
                  Adel Nadjaran Toosi and
                  Rajkumar Buyya},
	title = {Efficient Large-Scale Multiple Migration Planning and Scheduling in
                  SDN-Enabled Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6667--6680},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3326610},
	doi = {10.1109/TMC.2023.3326610},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeTB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Services provided by mobile edge clouds offer low-latency responses for large-scale and real-time applications. Dynamic service management algorithms generate live service migration requests to support user mobility and ensure service latency in mobile edge clouds. To handle these migration requests, multiple migration planning and scheduling algorithms are necessary to calculate the migration order and optimize the performance and overhead of multiple migrations. However, current planning and scheduling algorithms in cloud data centers are not suitable for dynamic and large-scale scenarios in edge computing, as the network topology expands and the number of migration requests increases. Edge computing requires near real-time scheduling to handle user mobility-induced live migrations. To address this issue, this paper presents an efficient multiple migration planning and scheduling framework for edge computing. The framework includes a lifecycle management framework and innovative iterative Maximal Independent Set-based scheduling algorithms based on the resource dependency graph of multiple migrations. Our solution is shown to efficiently schedule live migrations at scale using real-world taxi traces and telecom base station coordinates. It can achieve significant processing speedups over existing migration planning algorithms in clouds, up to 3,000 times, while ensuring multiple and individual migration performance for time-critical services.}
}


@article{DBLP:journals/tmc/GaoLC24,
	author = {Zhenguo Gao and
                  Chang Liu and
                  Yan Chen},
	title = {Scheduling of ERD-Assisted Charging of a {WRSN} Using a Directional
                  Mobile Charger},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6681--6696},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3324454},
	doi = {10.1109/TMC.2023.3324454},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For the capability of concentrating radiation energy along a direction, using Directional Mobile Chargers (DMCs) for charging the nodes in a Wireless Rechargeable Sensor Network (WRSN) via wireless power transfer has become a research hotspot. However, existing research neglect cooperative Energy ReDistribution (ERD) among nodes, handering energy charging efficiency. This motivated us to focus on the scenario of charging a WRSN using a DMC and address the underlying ERD-Assisted Directional Charging Schedule (ERADCS) problem. This problem involves determining a charging schedule with minimal energy loss and minimum time span. We first proved the NP-hardness of ERADCS and then proposed a Directional Charging Schedule algorithm Based on Greedy Strategy (DCSBGS) to solve it. In DCSBGS, to tackle the infinite charging directions, we created cMFRDS algorithm to determine a minimum-size representative direction set functionally equivalent to the original infinite directions, and proved its optimality. Subsequently, we assumed Virtual Mobile Chargers (VMCs) fixed to the representative directions, transformed ERADCS into a charging schedule problem using the VMCs, solving it using a two-step framework. We also established key properties of DCSBGS and its sub-algorithms. Furthermore, we extended DCSBGS for ERADCS involving a DMC with multiple charging beams. Our simulation results validate the superiority of DCSBGS over other typical algorithms.}
}


@article{DBLP:journals/tmc/HaoHJM24,
	author = {Lifei Hao and
                  Baoqi Huang and
                  Bing Jia and
                  Guoqiang Mao},
	title = {On the Fine-Grained Crowd Analysis via Passive WiFi Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6697--6711},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3324334},
	doi = {10.1109/TMC.2023.3324334},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HaoHJM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regarding the passive WiFi sensing based crowd analysis, this paper first theoretically investigates its limitations, and then proposes a deep learning based scheme targeted for returning fine-grained crowd states in large surveillance areas. To this end, three key challenges are coped with: to relieve the influences of the randomness and sparsity induced by passive WiFi sensing, an attention-based deep convolutional autoencoder model is designed to recover accurate crowd density maps in a way similar to image reconstruction; to combat the anonymity caused by MAC randomization, following the identification of local high-density crowds (LHDCs) with the density clustering algorithm, i.e., DM-DBSCAN, a bidirectional convolutional LSTM based model is employed to infer LHDC speeds; to overcome the absence of passive WiFi sensing datasets for model training, three semi-synthetic datasets are produced by emulating passive WiFi sensing with practical pedestrian tracking datasets. Extensive experiments confirm that, the proposed scheme significantly outperforms existing WiFi-based methods in terms of crowd density estimation and provides superior crowd speed estimation. More importantly, the scheme can also produce consistent crowd states on a real-world dataset, revealing that it has the ability to support accurate, visualized and real-time crowd monitoring in large surveillance areas.}
}


@article{DBLP:journals/tmc/DuWLL24,
	author = {Yao Du and
                  Zehua Wang and
                  Cyril Leung and
                  Victor C. M. Leung},
	title = {Accelerating and Securing Blockchain-Enabled Distributed Machine Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6712--6730},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325334},
	doi = {10.1109/TMC.2023.3325334},
	timestamp = {Sun, 04 Aug 2024 19:47:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Things (IoT) employing centralized machine learning, security is a major concern due to the heterogeneity of end devices. Malicious devices could launch poisoning attacks to degrade machine learning models. Distributed machine learning (DML) with blockchain provides a potential solution. Once local weights are recorded on the blockchain, model aggregation with defensive schemes can be executed on smartphones to prevent attacks. However, blockchain with the proof-of-work (PoW) consensus mechanism wastes computing resources and adds latency to DML. Computing resources can be utilized more efficiently with proof-of-useful-work (uPoW), which secures transactions by solving relevant real-world problems. We propose a novel uPoW method to minimize per-round latency of DML. The uPoW mining process schedules DML instances among multi-access edge computing (MEC) servers by solving a multi-way number partitioning problem. Moreover, poisoning attacks on heterogeneous training data pose significant challenges to blockchain-based DML. To address this problem, we propose a novel aggregation protocol, named {Corrected\\ Krum}, to counter such attacks and improve the convergence speed of DML. By leveraging the mean-field approximation method, training errors are corrected to reduce the negative impact of poisoning attacks. Simulation results show that our proposed blockchain approach can significantly speed up DML compared with benchmarks.}
}


@article{DBLP:journals/tmc/ZhouZT24,
	author = {Tailin Zhou and
                  Jun Zhang and
                  Danny H. K. Tsang},
	title = {FedFA: Federated Learning With Feature Anchors to Align Features and
                  Classifiers for Heterogeneous Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6731--6742},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325366},
	doi = {10.1109/TMC.2023.3325366},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation due to heterogeneous data at clients. Common solutions involve designing an auxiliary loss to regularize weight divergence or feature inconsistency during local training. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between feature inconsistency and classifier divergence across clients. This vicious cycle causes client models to be updated in inconsistent feature spaces with more diverged classifiers. To break the vicious cycle, we propose a novel framework named Federated learning with Feature Anchors (FedFA). FedFA utilizes feature anchors to align features and calibrate classifiers across clients simultaneously. This enables client models to be updated in a shared feature space with consistent classifiers during local training. Theoretically, we analyze the non-convex convergence rate of FedFA. We also demonstrate that the integration of feature alignment and classifier calibration in FedFA brings a virtuous cycle between feature and classifier updates, which breaks the vicious cycle existing in current approaches. Extensive experiments show that FedFA significantly outperforms existing approaches on various classification datasets under label distribution skew and feature distribution skew.}
}


@article{DBLP:journals/tmc/WuZZZWC24,
	author = {Duo Wu and
                  Dayou Zhang and
                  Miao Zhang and
                  Ruoyu Zhang and
                  Fangxin Wang and
                  Shuguang Cui},
	title = {{ILCAS:} Imitation Learning-Based Configuration- Adaptive Streaming
                  for Live Video Analytics With Cross-Camera Collaboration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6743--6757},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3327097},
	doi = {10.1109/TMC.2023.3327097},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZZZWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high-accuracy and resource-intensive deep neural networks (DNNs) have been widely adopted by live video analytics (VA), where camera videos are streamed over the network to resource-rich edge/cloud servers for DNN inference. Common video encoding configurations (e.g., resolution and frame rate) have been identified with significant impacts on striking the balance between bandwidth consumption and inference accuracy and therefore their adaption scheme has been a focus of optimization. However, previous profiling-based solutions suffer from high profiling cost, while existing deep reinforcement learning (DRL) based solutions may achieve poor performance due to the usage of fixed reward function for training the agent, which fails to craft the application goals in various scenarios. In this paper, we propose ILCAS, the first imitation learning (IL) based configuration-adaptive VA streaming system. Unlike DRL-based solutions, ILCAS trains the agent with demonstrations collected from the expert which is designed as an offline optimal policy that solves the configuration adaption problem through dynamic programming. To tackle the challenge of video content dynamics, ILCAS derives motion feature maps based on motion vectors which allow ILCAS to visually “perceive” video content changes. Moreover, ILCAS incorporates a cross-camera collaboration scheme to exploit the spatio-temporal correlations of cameras for more proper configuration selection. Extensive experiments confirm the superiority of ILCAS compared with state-of-the-art solutions, with 2–20.9% improvement of mean accuracy and 19.9–85.3% reduction of chunk upload lag.}
}


@article{DBLP:journals/tmc/ZabetianAK24,
	author = {Negar Zabetian and
                  Golara Ahmadi Azar and
                  Babak Hossein Khalaj},
	title = {Hybrid Non-Intrusive QoE Assessment of VoIP Calls Based on an Ensemble
                  Learning Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6758--6769},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325290},
	doi = {10.1109/TMC.2023.3325290},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZabetianAK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the Mean Opinion Score (MOS) is the most well-known way to quantify Quality of Experience (QoE), it only provides average insight. In this paper, we will demonstrate that instead of only relying on the MOS value, predicting users’ perceived quality through probabilistic metrics provides service providers with more accurate insight to improve their network decisions, relying on more accurate measures of how many of them are satisfied or not with the provided service. Furthermore, a hybrid non-intrusive ensemble learner based on the selection of multiple base learners is designed to estimate the QoE of the VoIP signal. The performance of our proposed model is compared to that of individual learners, demonstrating that the proposed scheme outperforms earlier schemes. In addition, in contrast with earlier schemes that relied on their own laboratory-generated dataset, another key advantage of our approach is that it extracts a wide variety of different system parameters, such as noise type and echo delay, and signal parameters, such as fundamental frequency, only from the degraded signal. Finally, we show how the more accurate predicted QoE values can be used by service providers to properly modify network parameters to get closer to the required QoE levels.}
}


@article{DBLP:journals/tmc/WangHLHW24,
	author = {Li Wang and
                  Luyang Hou and
                  Sixuan Liu and
                  Zhu Han and
                  Jie Wu},
	title = {Reinforcement Contract Design for Vehicular-Edge Computing Scheduling
                  and Energy Trading via Deep Q-Network With Hybrid Action Space},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6770--6784},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3329643},
	doi = {10.1109/TMC.2023.3329643},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangHLHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancements in information and communication technology have led to the emergence of innovative edge computing models that incorporate the computing power of vehicles into the energy sector. Electric vehicles (EVs), functioning as edge computing nodes, offer flexible computing offloading services for charging stations (CS). However, coordinating EV computing and charging should consider the interdependence with CS's specific computing requirements due to information asymmetry. Additionally, it is crucial to consider EV's charging demands and their social distance to computing tasks. In this context, it is natural to view EVs and CSs as self-interested prosumers who prioritize their individual utilities. To address the integration of strategic EV-CS interactions and uncertainties into the joint computing scheduling and energy trading, this paper proposes a parameterized deep Q-network-based reinforcement contract design framework, which employs a hybrid action space to design contracts that facilitate CSs in pairing computing tasks and charging resources with EVs. The objective is to incentivize EV participation and maximize long-term social welfare by incorporating incentive compatibility, individual rationality constraints, and capacity constraints into the contract design. Experimental results demonstrate that the proposed framework surpasses parameterized deep deterministic policy gradient-based and greedy-based contract designs, and achieves near-optimal solutions by solving deterministic optimizations.}
}


@article{DBLP:journals/tmc/WangHJWMZDZ24,
	author = {Guang Wang and
                  Sihong He and
                  Lin Jiang and
                  Shuai Wang and
                  Fei Miao and
                  Fan Zhang and
                  Zheng Dong and
                  Desheng Zhang},
	title = {FairMove: {A} Data-Driven Vehicle Displacement System for Jointly
                  Optimizing Profit Efficiency and Fairness of Electric For-Hire Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6785--6802},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3326676},
	doi = {10.1109/TMC.2023.3326676},
	timestamp = {Mon, 26 Aug 2024 16:11:59 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangHJWMZDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the worldwide mobility electrification initiative to reduce air pollution and energy security, more and more for-hire vehicles are being replaced with electric ones. A key difference between gas for-hire vehicles and electric for-hire vehicles (EFHV) is their energy replenishment mechanisms, i.e., refueling or charging, which is reflected in two aspects: (i) much longer charging processes versus much shorter refueling processes and (ii) time-varying electricity prices versus time-invariant gasoline prices during a day. The complicated charging issues (e.g., long charging time and dynamic charging pricing) potentially reduce the daily operation time and profits of EFHVs, and also cause overcrowded charging stations during some off-peak charging pricing periods. Motivated by a set of findings obtained from a data-driven investigation and field studies, in this paper, we design a fairness-aware vehicle displacement system called FairMove to jointly optimize the overall profit efficiency and profit fairness of EFHV drivers by considering both the passenger travel demand and vehicle charging demand. We first formulate the EFHV displacement problem as a Markov decision problem, and then we present a fairness-aware multi-agent actor-critic approach to tackle this problem. More importantly, we implement and evaluate FairMove with real-world streaming data from the Chinese city Shenzhen, including GPS data and transaction data from over 20,100 EFHVs, coupled with the data of 123 charging stations, which constitute, to our knowledge, the largest EFHV network in the world. Extensive experimental results show that our fairness-aware FairMove effectively improves the profit efficiency and profit fairness of the EFHV fleet by 26.9% and 54.8%, respectively. It also improves the charging station utilization fairness by 38.4%.}
}


@article{DBLP:journals/tmc/WuCWP24,
	author = {Chao{-}Lun Wu and
                  Te{-}Chuan Chiu and
                  Chih{-}Yu Wang and
                  Ai{-}Chun Pang},
	title = {Mobility-Aware Deep Reinforcement Learning With Seq2seq Mobility Prediction
                  for Offloading and Allocation in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6803--6819},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328996},
	doi = {10.1109/TMC.2023.3328996},
	timestamp = {Fri, 17 May 2024 21:40:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuCWP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile/multi-access edge computing (MEC) is developed to support the upcoming AI-aware mobile services, which require low latency and intensive computation resources at the edge of the network. One of the most challenging issues in MEC is service provision with mobility consideration. It has been known that the offloading decision and resource allocation need to be jointly handled to optimize the service provision efficiency within the latency constraints, which is challenging when users are in mobility. In this paper, we propose Mobility-Aware Deep Reinforcement Learning (M-DRL) framework for mobile service provision in the MEC system. M-DRL is composed of two parts: glimpse, a seq2seq model customized for mobility prediction to predict a sequence of locations just like a “glimpse” of the future, and a DRL specialized in supporting offloading decisions and resource allocation in MEC. By integrating the proposed DRL and glimpse mobility prediction model, the proposed M-DRL framework is optimized to handle the MEC service provision with average 70% performance improvements.}
}


@article{DBLP:journals/tmc/MorinMICAVP24,
	author = {Diego Gonz{\'{a}}lez Mor{\'{\i}}n and
                  Daniele Medda and
                  Athanasios C. Iossifides and
                  Periklis Chatzimisios and
                  Ana Garc{\'{\i}}a Armada and
                  {\'{A}}lvaro Villegas and
                  Pablo P{\'{e}}rez},
	title = {An eXtended Reality Offloading {IP} Traffic Dataset and Models},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6820--6834},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3326893},
	doi = {10.1109/TMC.2023.3326893},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MorinMICAVP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, advances in immersive multimedia technologies, such as extended reality (XR) technologies, have led to more realistic and user-friendly devices. However, these devices are often bulky and uncomfortable, still requiring tether connectivity for demanding applications. The deployment of the fifth generation of telecommunications technologies (5G) has set the basis for XR offloading solutions with the goal of enabling lighter and fully wearable XR devices. In this paper, we present a traffic dataset for two demanding XR offloading scenarios that substantially extend those available in the current state of the art, captured using a fully developed end-to-end XR offloading solution. We also propose a set of accurate traffic models for the proposed scenarios based on the captured data, accompanied by a simple and consistent method to generate synthetic data from the fitted models. Finally, using an open-source 5G radio access network (RAN) emulator, we validate the models both at the application and resource allocation layers. Overall, this work aims to provide a valuable contribution to the field with data and tools for designing, testing, improving, and extending XR offloading solutions in academia and industry.}
}


@article{DBLP:journals/tmc/HuJLXZLD24,
	author = {Jingyang Hu and
                  Hongbo Jiang and
                  Daibo Liu and
                  Zhu Xiao and
                  Qibo Zhang and
                  Jiangchuan Liu and
                  Schahram Dustdar},
	title = {Combining {IMU} With Acoustics for Head Motion Tracking Leveraging
                  Wireless Earphone},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6835--6847},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3325826},
	doi = {10.1109/TMC.2023.3325826},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuJLXZLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Head motion tracking is a promising research field with vast applications in ubiquitous human-computer interaction (HCI) scenarios. Unfortunately, solutions based on vision and wireless sensing have shortcomings in user privacy and tracking range, respectively. To address these issues, we propose IA-Track, a novel head motion tracking system that combines inertial measurement units (IMU) and acoustic sensing. Our wireless earphone-based method balances flexibility, computational complexity, and tracking accuracy, requiring only an earphone with an IMU and a smartphone. However, we still face two challenges. First, wireless earphones have limited hardware resources, making acoustic Doppler effect-based method unsuitable for acoustic tracking. Second, traditional Kalman filter-based trajectory restoration methods may introduce significant cumulative errors. To tackle these challenges, we rely on IMU sensor data to recover the trajectory and use smartphones to emit ”inaudible” acoustic signals that the earphone receives to adjust the IMU drift track. We conducted extensive experiments involving 50 volunteers in various potential IA-Track usage scenarios, demonstrating that our well-designed system achieves satisfactory head motion tracking performance.}
}


@article{DBLP:journals/tmc/XuZYWPNQ24,
	author = {Chao Xu and
                  Xinyan Zhang and
                  Howard H. Yang and
                  Xijun Wang and
                  Nikolaos Pappas and
                  Dusit Niyato and
                  Tony Q. S. Quek},
	title = {Optimal Status Updates for Minimizing Age of Correlated Information
                  in IoT Networks With Energy Harvesting Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6848--6864},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3329170},
	doi = {10.1109/TMC.2023.3329170},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuZYWPNQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-time applications of the Internet of Things (IoT) need to deal with correlated information generated by multiple sensors. The design of efficient status update strategies that minimize the Age of Correlated Information (AoCI) is a key factor. In this paper, we consider an IoT network consisting of sensors equipped with the energy harvesting (EH) capability. We optimize the average AoCI at the data fusion center (DFC) by appropriately managing the energy harvested by sensors, whose true battery states are unobservable during the decision-making process. Particularly, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC. In order to address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors’ true battery states, and large-scale discrete action space, we devise a deep reinforcement learning (DRL)-based dynamic status update algorithm. The algorithm leverages the advantages of the soft actor-critic and long short-term memory techniques. Meanwhile, it incorporates our proposed action decomposition and mapping mechanism. Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs.}
}


@article{DBLP:journals/tmc/DiaoWZXCPMXZ24,
	author = {Zulong Diao and
                  Xin Wang and
                  Dafang Zhang and
                  Gaogang Xie and
                  Jianguo Chen and
                  Changhua Pei and
                  Xuying Meng and
                  Kun Xie and
                  Guangxing Zhang},
	title = {{DMSTG:} Dynamic Multiview Spatio-Temporal Networks for Traffic Forecasting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6865--6880},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328038},
	doi = {10.1109/TMC.2023.3328038},
	timestamp = {Wed, 18 Sep 2024 14:53:45 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DiaoWZXCPMXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic sensor networks are widely applied in smart cities to monitor traffic in real-time. Exploiting such data to forecast future traffic conditions has the potential to enhance the decision-making capabilities of intelligent transportation systems, which attracts widespread attention from both industries and academia. Among them, network-wide prediction based on graph convolutional neural networks(GCN) has become mainstream. It models the spatial dependencies of sensors in a graph with a pre-defined Laplacian matrix. However, understanding spatio-temporal traffic patterns is quite challenging as there is a huge difference in terms of traffic patterns during different periods or in different regions. In addition, the actual data collected can be polluted due to unavoidable data loss from severe communication conditions or sensor failures. Considering these issues, we propose a novel dynamic multiview spatial-temporal prediction framework which takes into consideration various factors, including local/global, short/long term spatio-temporal dependencies and their dynamic changes. We creatively design two different modules to comprehensively perceive the changes in traffic patterns. We first propose a dynamic learning module based on our theoretical derivation to estimate the Laplacian matrix of the graph for GCN timely. We also design a self-attention based module to dynamically assign a weight to each part in traffic data. The spatio-temporal features from multiple views are deeply fused by a feature fusion module. The forecasting performance is evaluated with 5 real-time traffic datasets. Experiment results demonstrate that our framework can consistently outperform the state-of-the-art baselines and be more robust under noisy environments.}
}


@article{DBLP:journals/tmc/TuliCJ24,
	author = {Shreshth Tuli and
                  Giuliano Casale and
                  Nicholas R. Jennings},
	title = {PreGAN+: Semi-Supervised Fault Prediction and Preemptive Migration
                  in Dynamic Mobile Edge Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6881--6895},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3330679},
	doi = {10.1109/TMC.2023.3330679},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TuliCJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Typical mobile edge computing infrastructures have to contend with unreliable computing devices at their end-points. The limited resource capacities of mobile edge devices gives rise to frequent contentions, node overloads or failures. This is exacerbated by the strict deadlines of modern applications. To avoid failures, fault-tolerant approaches utilize preemptive migration to transfer active tasks across nodes and prevent nodes running at capacity. However, prior work struggles to dynamically adapt in settings with highly volatile workloads or even accurately detect and diagnose anomalies for optimal remediation. To meet the strict service level objectives of contemporary workloads, there is a need for dynamic fault-tolerant methods that can quickly adapt to changes in edge environments while having parsimonious remediation in the form of preemptive migration to avoid stressing the system network. This work proposes PreGAN, featuring a Generative Adversarial Network (GAN) based approach to predict contentions, pinpoint specific resource types with high chance of overload, and generate migration decisions to proactively avoid system downtime. PreGAN leverages coupled-simulations to train the GAN model at run-time and a few-shot fault classifier to update decisions of an underpinning scheduler. We also extend it to PreGAN+ that also periodically tunes the decision model using semi-supervised training and a Transformer based neural network for low tuning time, albeit with higher memory overheads. Experiments on a Raspberry-Pi based edge environment demonstrate that both models outperform state-of-the-art baselines in fault detection and diagnosis scores by up to 12.5% and 31.2% respectively. This also translates in improvements in Quality of Service against baseline approaches.}
}


@article{DBLP:journals/tmc/KimL24,
	author = {Kyoung Min Kim and
                  Tae{-}Jin Lee},
	title = {Random Access and Uplink Shared Channel Resource Allocation With {NOMA}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6896--6907},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328028},
	doi = {10.1109/TMC.2023.3328028},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KimL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a cellular network, User Equipments (UEs) have to conduct the Random Access (RA) procedure with the Base Station (BS) before transmitting data. In the conventional four-step RA procedure, UEs perform RA competition by exchanging four messages with the BS by utilizing the limited resources. To support the massive Machine Type Communications (mMTC) in a 5G network, various solutions have been studied to reduce collision probability when many UEs perform RA competition simultaneously. By applying Non-Orthogonal Multiple Access (NOMA) to the RA procedure, the BS can decode multiple signals with different transmission power levels from multiple UEs by successive interference cancellation (SIC) technique and the RA delay can be reduced by decreasing collisions. To support latency-sensitive applications in a 5G network, the 3rd Generation Partnership Project (3GPP) adopts a two-step RA procedure which is a shortened version compared to a four-step RA procedure. However, it is difficult to apply NOMA to the two-step RA procedure since UEs perform the RA procedure before the BS allocates dedicated uplink resources to UEs. As researches have not been conducted on how many resources should be allocated by the BS in the NOMA-based two-step RA procedure, a new approach and resource allocation algorithm are required to utilize NOMA in the two-step RA procedure efficiently. In this paper, we propose a novel NOMA-based two-step RA procedure to increase the RA efficiency and reduce the RA delay. In the proposed two-step RA procedure, UEs determine the transmission power level based on the probability informed by the BS. The BS also allocates the optimal resource by estimating the number of UEs participating in RA. Through the proposed two-step RA with NOMA and optimal resource allocation algorithm, the RA efficiency can be reached up to 52%, compared to 37% of the conventional scheme, and the performance is evaluated by a Markov chain and simulations to show the improvement of the RA.}
}


@article{DBLP:journals/tmc/ZhouGLQ24,
	author = {Xiaobo Zhou and
                  Shuxin Ge and
                  Pengbo Liu and
                  Tie Qiu},
	title = {DAG-Based Dependent Tasks Offloading in MEC-Enabled IoT With Soft
                  Cooperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6908--6920},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328333},
	doi = {10.1109/TMC.2023.3328333},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouGLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC)-enabled Internet of Things (IoT) has become a powerful solution to run computation-intensive applications on end devices. These applications are composed of multiple dependent tasks, which can be abstracted as directed acyclic graphs (DAGs). Moreover, applications can share partial intermediate data with each other based on dynamic network conditions to boost its performance, so-called soft cooperation. However, it is quite challenging to make optimal offloading decisions with external dependency between tasks of different DAGs introduced by soft cooperation, as well as the subsequent huge continuous solution space caused. In this paper, we propose a DAG-based dependent tasks offloading method with soft cooperation in MEC-enabled IoT. First, we formulate the problem as a Markov decision process (MDP), aiming to minimize the application latency and energy consumption, and to maximize the cooperation gain simultaneously. Then, we propose a branch soft actor-critic (BSAC) algorithm to make optimal decisions under dynamic network conditions, including the offloaded tasks, the CPU frequency of end devices, and the sharing ratio of intermediate data. Specifically, BSAC uses multiple branch networks to reduce the solution space. Finally, a series of simulations are conducted to establish the superiority of the BASC algorithm over state-of-the-art solutions.}
}


@article{DBLP:journals/tmc/LuanWJYYW24,
	author = {Dongming Luan and
                  En Wang and
                  Nan Jiang and
                  Bo Yang and
                  Yongjian Yang and
                  Jie Wu},
	title = {A Data-Driven Crowdsensing Framework for Parking Violation Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6921--6935},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3331429},
	doi = {10.1109/TMC.2023.3331429},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuanWJYYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parking violation is a common urban problem in major cities all over the world. Traditional approaches for detecting parking violations mainly rely on fixed deployed sensors and enforcement agencies, which suffer from high deployment costs and limited coverage. With the rapid development of mobile networks, Mobile CrowdSensing (MCS) has been an effective sensing paradigm. The crowdsensing data can help predict the future parking violation distribution, and the prediction results can provide guidance for user scheduling, i.e., sending the mobile users to patrol areas where many parking violation events may occur. Inspired by this idea, we propose a comprehensive data-driven crowdsensing framework, which incorporates the nested design of a generative model for spatial-temporal data and a user scheduling model. The generative model extracts parking violation hotspots via a data completion module and violation prediction module. Since crowdsensing data is usually temporally sparse and unevenly distributed, a data completion module is proposed to infer the missing statistics in unsensed areas. The violation prediction module then predicts the parking violation distribution. Given the predicted results, the deep reinforcement learning-based user scheduling model coordinates users to visit hotspots for violation detection. Iteratively, the newly collected data can be used to predict the future violation distribution. Finally, we conduct extensive simulations based on two real-world datasets from two large urban cities. The simulation verifies the prediction accuracy and scheduling effectiveness of the proposed framework compared with the baselines.}
}


@article{DBLP:journals/tmc/BakopoulouYZPM24,
	author = {Evita Bakopoulou and
                  Mengwei Yang and
                  Jiang Zhang and
                  Konstantinos Psounis and
                  Athina Markopoulou},
	title = {Location Leakage in Federated Signal Maps},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6936--6953},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332034},
	doi = {10.1109/TMC.2023.3332034},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BakopoulouYZPM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of predicting cellular network performance (signal maps) from measurements collected by several mobile devices. We formulate the problem within the online federated learning framework: (1) federated learning (FL) enables users to collaboratively train a model, while keeping their training data on their devices; (2) measurements are collected as users move around over time and are used for local training in an online fashion. We consider an honest-but-curious server, who observes the updates from target users participating in FL and infers their location using a deep leakage from gradients (DLG) type of attack, originally developed to reconstruct training data of DNN image classifiers. We make the key observation that a DLG attack, applied to our setting, infers the average location of a batch of local data, and can thus be used to reconstruct the target users’ trajectory at a coarse granularity. We build on this observation to protect location privacy, in our setting, by revisiting and designing mechanisms within the federated learning framework including: tuning the FL parameters for averaging, curating local batches so as to mislead the DLG attacker, and aggregating across multiple users with different trajectories. We evaluate the performance of our algorithms through both analysis and simulation based on real-world mobile datasets, and we show that they achieve a good privacy-utility tradeoff.}
}


@article{DBLP:journals/tmc/DuWXB24,
	author = {Junhui Du and
                  Huaming Wu and
                  Minxian Xu and
                  Rajkumar Buyya},
	title = {Computation Energy Efficiency Maximization for NOMA-Based and Wireless-Powered
                  Mobile Edge Computing With Backscatter Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6954--6970},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328612},
	doi = {10.1109/TMC.2023.3328612},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuWXB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Things (IoT) environment, a wide variety of mobile devices (MDs) have become part of it, leading to a dramatic increase in the amount of task data. However, due to the limited battery capacity and computing resources of MDs, a lot of effort is required to be taken on how to process more data with less energy. In this paper, we take into account the low utilization of spectrum resources and the short battery life of the equipment, and a backscatter communication-mobile edge computing (BC-MEC) network system based on Non-orthogonal multiple access (NOMA) communication mode is proposed. In order to maximize the computation energy efficiency (CEE) of the system, we jointly optimize the backscatter coefficient of each MD, the backscatter communication duration, the direct offloading duration, the MEC server processing time, the local processing time, the direct offloading power of each MD, the calculation frequency of the MEC server, and the local calculation frequency of each MD. We then formulate it as a joint fractional optimization problem, which is a non-convex optimization problem that is difficult to solve by heuristic algorithms with high computational complexity. To this end, we transform such a problem into a convex problem and apply the Lagrangian dual method to solve it efficiently. Furthermore, in order to meet different user requirements, two effective iterative Dinkelbach algorithms based on Backscatter Coefficient Updates (DBCU) are proposed to solve this problem. Extensive simulation results demonstrate the superiority of our proposed approach, which improves the system CEE by at least 10% compared to state-of-the-art methods.}
}


@article{DBLP:journals/tmc/YanG24,
	author = {Litao Yan and
                  Xiaohu Ge},
	title = {Entropy-Based Energy Dissipation Analysis of Mobile Communication
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6971--6982},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328621},
	doi = {10.1109/TMC.2023.3328621},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the most prominent physical aspects of mobile communication systems is that they are inherently non-equilibrium systems. Traditional researches on the energetic costs of communication systems pay little attention to the relationship between the energy and the information transmitted and processed by the system. On the other hand, recent breakthroughs in nonequilibrium thermodynamics have led to a deeper understanding of the thermodynamics of information. To investigate the energetic costs of a mobile communication system at a fundamental level, in this paper, an entropy-based energy dissipation model based on nonequilibrium thermodynamics is first proposed for mobile communication systems. The energy dissipation model relates the energy and information through the common concept “entropy” from thermodynamics and information theory. Moreover, the theoretical minimal energy dissipation limits are derived for typical modulations in mobile communication systems. Simulation results show that the practical energy dissipation of information processing and information transmission is three and seven orders of magnitude away from the theoretical minimal energy dissipation limits in mobile communication systems, respectively. The energy dissipation model and results derived in the paper provide guidelines on how to design future mobile communication systems to minimize the thermodynamic costs.}
}


@article{DBLP:journals/tmc/MishraK24,
	author = {Soumya Nandan Mishra and
                  Manas Khatua},
	title = {Reliable and Delay Efficient Multi-Path {RPL} for Mission Critical
                  IoT Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6983--6996},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328346},
	doi = {10.1109/TMC.2023.3328346},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MishraK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Routing Protocol for Low Power and Lossy Networks (RPL) is the de-facto routing standard for Internet of Things (IoT) applications. It has been designed to work with resource-constrained devices in constrained environment. In RPL, objective functions (OFs) play the role of selecting optimal paths. However, the existing OFs are not suitable for Mission Critical IoT (MC-IoT) applications. MC-IoT applications expect that the data should reach the destination within a strict deadline. So, reliability and delay are the two significant requirements in such type of applications. An existing work RMP-RPL achieves the hard reliability requirement for MC-IoT applications. However, it did not consider the delay requirement. Also, it was not tested for different traffic rates. Therefore, this paper proposes a reliable and delay-efficient multi-path RPL (RDMP-RPL) that optimizes both reliability and delay. It considers buffer and channel loss probabilities while calculating reliability. For delay calculation, it considers queueing and link delays. In addition, the protocol presents a method to select backup parents to meet end-to-end delay requirements. The results demonstrate that RDMP-RPL maintains a packet delivery ratio (PDR) of at least 99% and at most one second end-to-end delay (EED) considering a maximum of 5 hops for a packet to reach the destination from any of the source nodes for grid topology. But, for random topology, the number of hops can exceed 5. RDMP-RPL cannot achieve the strict requirements in situations where traffic rate, mobility and number of nodes cross a specific threshold. However, it outperforms other benchmark protocols by a significant margin.}
}


@article{DBLP:journals/tmc/GuCHWZD24,
	author = {Yangyang Gu and
                  Jing Chen and
                  Kun He and
                  Cong Wu and
                  Ziming Zhao and
                  Ruiying Du},
	title = {WiFiLeaks: Exposing Stationary Human Presence Through a Wall With
                  Commodity Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {6997--7011},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328349},
	doi = {10.1109/TMC.2023.3328349},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GuCHWZD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi devices are ubiquitous and may leak user and household privacy. In this paper, we report an attack, namely WiFiLeaks, which uses a commodity mobile device to passively detect stationary human presence through a wall by analyzing the channel state information of wireless signals transmitted by indoor WiFi devices. In our adversarial scenario, attackers cannot control the WiFi transmitter or use advanced radio devices. The main challenge of this attack is how to extract robust features from non-customized signals for stationary human presence. To address this challenge, we first combine methods based on outliers and wavelet denoising to enhance the low-frequency information related to human presence. Then we propose a novel feature extraction method based on the correlation among subcarriers since stationary human presence can enhance their correlations. We evaluate WiFiLeaks using nine different WiFi transmitter and one commodity smartphone in four different settings. The evaluations show WiFiLeaks can still achieve accuracy rates of 83.33% and 100% for human presence and absence at 20 meters between the monitor device and the transmitter in through-the-wall scenarios.}
}


@article{DBLP:journals/tmc/TuliJ24,
	author = {Shikhar Tuli and
                  Niraj K. Jha},
	title = {EdgeTran: Device-Aware Co-Search of Transformers for Efficient Inference
                  on Mobile Edge Platforms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7012--7029},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328287},
	doi = {10.1109/TMC.2023.3328287},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TuliJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-search technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage block-level grow-and-prune post-processing step that further improves accuracy in a hardware-aware manner. The obtained transformer model is 2.8× smaller and has a 0.8% higher GLUE score than the baseline (BERT-Base). Inference with it on the selected edge device enables 15.0% lower latency, 10.0× lower energy, and 10.8× lower peak power draw compared to an off-the-shelf GPU.}
}


@article{DBLP:journals/tmc/XuSWTH24,
	author = {Dianlei Xu and
                  Xiang Su and
                  Huandong Wang and
                  Sasu Tarkoma and
                  Pan Hui},
	title = {Towards Risk-Averse Edge Computing With Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7030--7047},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3329444},
	doi = {10.1109/TMC.2023.3329444},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuSWTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, artificial intelligence paves the way for the development of smart services for people anytime and anywhere, which poses great challenges on accessing computing resources. Multi-access edge computing complements existing cloud computing infrastructure at the edge of the network, where mobile users can offload computationally intensive tasks of smart applications to edge servers that are in proximity to the users themselves. Existing offloading schemes mainly focus on selecting edge servers for each offloading task with the goal of optimizing the overall average latency. However, the solutions with the optimal overall average latency may be not the most suitable for all offloading tasks. There is still a possibility that offloading leads to an extreme case of ultra-high latency, which is not acceptable for latency-sensitive applications. To address this problem, we therefore introduce modern portfolio theory (MPT) to jointly consider the overall average latency and potential risks in optimal edge server selection. The task offloading problem is regarded as an investment portfolio with the objective of maximizing the ‘return’ while minimizing the risk. Combining MPT with deep reinforcement learning (DRL), we design two proximal policy optimization (PPO)-based task offloading algorithms to jointly optimize these two objectives. The algorithm computes a portfolio for each mobile user that enables the diversification of edge server selection, thereby minimizing the risk and the average latency. Extensive simulation results based on three real-world trace datasets show that our algorithms significantly outperform the state-of-the-art solutions and can reduce the overall average latency and the risk by 59% and 85% at most, respectively.}
}


@article{DBLP:journals/tmc/HanKCNMCB24,
	author = {Dong{-}Jun Han and
                  Do{-}Yeon Kim and
                  Minseok Choi and
                  David Nickel and
                  Jaekyun Moon and
                  Mung Chiang and
                  Christopher G. Brinton},
	title = {Federated Split Learning With Joint Personalization-Generalization
                  for Inference-Stage Optimization in Wireless Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7048--7065},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3331690},
	doi = {10.1109/TMC.2023.3331690},
	timestamp = {Fri, 07 Feb 2025 08:28:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanKCNMCB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for intelligent services at the network edge has introduced several research challenges. One is the need for a machine learning architecture that achieves personalization (to individual clients) and generalization (to unseen data) properties concurrently across different applications. Another is the need for an inference strategy that can satisfy network resource and latency constraints during testing-time. Existing techniques in federated learning have encountered a steep trade-off between personalization and generalization, and have not explicitly considered the resource requirements during the inference-stage. In this paper, we propose SplitGP, a joint edge-AI training and inference strategy that simultaneously captures generalization/personalization for efficient inference across resource-constrained clients. The training process of SplitGP is based on federated split learning, with the key idea of optimizing the client-side model to have personalization capability tailored to its main task, while training the server-side model to have generalization capability for handling out-of-distribution tasks. During testing-time, each client selectively offloads inference tasks to the server based on the uncertainty threshold tunable based on network resource availability. Through formal convergence analysis and inference time analysis, we provide guidelines on the selection of key meta-parameters in SplitGP. Experimental results confirm the advantage of SplitGP over existing baselines.}
}


@article{DBLP:journals/tmc/ChenQCTDZ24,
	author = {Wuhui Chen and
                  Xiaoyu Qiu and
                  Zhongteng Cai and
                  Bingxin Tang and
                  Linlin Du and
                  Zibin Zheng},
	title = {Graph Neural Network-Enhanced Reinforcement Learning for Payment Channel
                  Rebalancing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7066--7083},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328473},
	doi = {10.1109/TMC.2023.3328473},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenQCTDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Building on top of blockchain, payment channel networks-backed (PCNs) cryptocurrencies emerge as a promising solution for a mobile payment system with fewer intermediaries, more security, higher speed, and lower cost. A key problem for PCN is payment channel rebalancing, that is, finding a set of circular transactions that restore a PCN with skewed channel balances back into an equilibrium state. However, existing practice on payment channel rebalancing either has a hard limit on the problem size or tends to fall into local optimum. To address these challenges, we propose DRL-PCR, a Deep Reinforcement Learning-based Payment Channel Rebalancing algorithm. On one hand, DRL-PCR leverages the strong approximation ability of deep neural networks to handle large problem spaces. On the other hand, DRL-PCR decomposes the rebalancing problem into a sequence of decision-making problems and progressively builds the final solution. By aiming to find a globally optimized solution and solving the long-term optimization model of DRL, DRL-PCR is superior to greedy-based algorithms and can mitigate the risk of getting trapped in a local optimum. In particular, payment channel rebalancing typically involves dealing with graph-structured data, where the major obstacle lies in understanding the sophisticated circular dependencies between payment channels and routing paths. DRL-PCR achieves this by encoding the input data with a novel graph neural network-based model and capturing the circular dependencies through a customized message passing process. In addition, considering the distributed nature of PCN, DRL-PCR uses a leadership election protocol to elect leaders for decision-making. Evaluations on the historical data of two real-world PCNs demonstrate that DRL-PCR can restore the PCN to a more balanced state and improve the transaction throughput and success ratios by up to 2.1x and 1.6x, respectively.}
}


@article{DBLP:journals/tmc/LiWLZXML24,
	author = {Yuanzhe Li and
                  Shangguang Wang and
                  Yuanchun Li and
                  Ao Zhou and
                  Mengwei Xu and
                  Xiao Ma and
                  Yunxin Liu},
	title = {Seamless Cross-Edge Service Migration for Real-Time Rendering Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7084--7098},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3331773},
	doi = {10.1109/TMC.2023.3331773},
	timestamp = {Tue, 14 May 2024 19:49:46 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWLZXML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Seamless cross-edge migration for real-time rendering applications is challenging. The strong interactive nature of real-time rendering applications demands a downtime lower than \\text{15}\\;\\text{ms} to achieve an imperceptible migration. Existing methods based on virtual machine migration and container migration suffer from unpleasant downtime brought by dirty page retransmission-induced repeated memory data copy and the shared storage failure-induced extensive disk data copy. In this article, we propose Cloud-assisted Service Migration (CSM) which leverages cloud-edge collaboration to achieve seamless service migration for real-time rendering applications. CSM improves service migration user experience in three folds: First, it introduces a dual rendering mechanism to bypass the peer-to-peer data copy and compresses the freezing stage. Second, a user equipment-centric session switch mechanism is proposed to save time by well coordinating application session switches and 5G user plane session switches. Third, a smooth switching mechanism is leveraged to prevent unpleasant frame flickers during session switching. We implement CSM in edge-rendering multiplayer games and deploy it on a 5G test bed with a full-stack user plane protocol stack. The evaluation results show that CSM can reduce downtime to < \\text{14}\\;\\text{ms} and the service migration process is user imperceptible.}
}


@article{DBLP:journals/tmc/CaiCCDVL24,
	author = {Yue Cai and
                  Peng Cheng and
                  Zhuo Chen and
                  Ming Ding and
                  Branka Vucetic and
                  Yonghui Li},
	title = {Deep Reinforcement Learning for Online Resource Allocation in Network
                  Slicing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7099--7116},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3328950},
	doi = {10.1109/TMC.2023.3328950},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaiCCDVL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a key enabler of 5G and beyond networks to satisfy the diverse quality of service (QoS) requirements of different services simultaneously. In network slicing, radio access network (RAN) slicing is essential to establish a functional network slice by connecting mobile devices and mapping virtualized resource units to different slices. This requires a highly efficient resource allocation scheme to maximize resource utilization efficiency and meet the diverse QoS requirements. In this paper, we propose a dynamic RAN slicing model that incorporates multiple distributions to accommodate different user request types and diverse priorities among traffic types in the same slice, where the total available resources are dynamically changing over time. We formulate resource allocation as a time-sequential dynamic optimization problem that takes into account system stability, resource limitation, different timescales, long-term system performance, and user priority. We propose a deep reinforcement learning-based (DRL-based) approach referred to as prediction-aided weighted DRL (PW-DRL) to online infer the power allocation and user acceptance decisions that can maximize a predefined reward function. Additionally, a prediction network is formulated to capture the correlation between current and future states. Simulation results validate that our proposed PW-DRL significantly outperforms state-of-the-art approaches by achieving the highest long-term reward and fastest convergence.}
}


@article{DBLP:journals/tmc/LiuZHWG24,
	author = {Yibing Liu and
                  Xiongtao Zhang and
                  Lijun Huo and
                  Jun Wu and
                  Mohsen Guizani},
	title = {Swarm Learning and Knowledge Distillation Empowered Self-Driving Detection
                  Against Threat Behavior for Intelligent IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7117--7134},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3330514},
	doi = {10.1109/TMC.2023.3330514},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZHWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The combination of mobile communication and the Internet of Things (IoT) has made physical devices more intelligent, bringing great convenience to our lives. However, the deep integration of personal information and the Internet increases the risk of data leakage and is easily exploited maliciously. In addition, due to limited system resources, smart devices with lightweight design are required. Therefore, it is necessary to realize low-energy and effective abnormal behavior detection of IoT devices, but the existing detection methods have disadvantages such as leakage of user privacy, low accuracy, and difficulty in dynamically improving the effect. To address these issues, this paper proposes a dynamic interactive minor anomaly detection scheme called ADONIS based on Swarm Learning (SL). The scheme combines the concept of swarm defense and utilizes SL to achieve local data fusion, which improves the detection effect and protects user privacy. Moreover, the decentralized structure of SL can cope with the impact of single node damage to enhance the robustness of IoT services. Furthermore, we propose training and detection decoupling framework to achieve high accuracy, low energy consumption, and low latency. It improves the performance by fitting the training model with full data, and simplifies the complexity of the detection model using knowledge distillation. We also design a self-enhancing dynamic strategy based on the decoupling framework to maintain powerful detection capability through human-computer interaction (HCI) and continuous learning. The framework relies on traffic data to keep the model sensitive to new behavior through iterative training without disturbing the user. Finally, simulation experiments show that our proposed scheme can achieve 82.2% accuracy, reduce the average detection time to 8.22 ms, and simplify the model complexity by 15.9%. Compared with existing methods, ADONIS can provide lighter, safer and more accurate anomaly detection.}
}


@article{DBLP:journals/tmc/MaoYWH24,
	author = {Yulian Mao and
                  Qingqing Ye and
                  Qi Wang and
                  Haibo Hu},
	title = {Utility-Aware Time Series Data Release With Anomalies Under {TLDP}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7135--7147},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332963},
	doi = {10.1109/TMC.2023.3332963},
	timestamp = {Fri, 09 Aug 2024 16:05:02 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaoYWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the prevalence of mobile computing, mobile devices have been generating numerous sensor data, a.k.a., time series. Since these time series may include sensitive information, users are posed with severe privacy risks. To protect individuals’ privacy, local differential privacy (LDP) is proposed. However, the added noise satisfying LDP typically degrades the utility of released data, especially for anomaly detection such as healthcare monitoring and hazard alarming. In this paper, we study privacy-preserving time series release with anomalies. Recently, local differential privacy in the temporal setting (TLDP) is proposed to perturb the temporal order rather than the values. While it improves the utility for releasing value-critical data, it still suffers from low utility for anomaly detection, because of the inevitable missing and delayed values incurred by TLDP perturbation. We propose to improve its utility from two aspects. To reduce the missing values, we utilize selective substitution according to items’ anomaly scores. To decrease the delayed values, we define metric-based (\\alpha, \\delta )-TLDP and propose a mechanism that can prioritize anomaly release at a close timestamp while still guaranteeing the same TLDP privacy. Through theoretical and empirical evaluation, we show superior performance gain over existing TLDP-based mechanisms on both synthetic and real-world datasets.}
}


@article{DBLP:journals/tmc/GongXLXC24,
	author = {Biyao Gong and
                  Tianzhang Xing and
                  Zhidan Liu and
                  Wei Xi and
                  Xiaojiang Chen},
	title = {Towards Hierarchical Clustered Federated Learning With Model Stability
                  on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7148--7164},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332637},
	doi = {10.1109/TMC.2023.3332637},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GongXLXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustered federated learning (CFL) has proved to be an effective way to alleviate the non-IID (not independently and identically distributed) data challenge, which severely restricts the wider application of federated learning. However, existing approaches either lack adaptability, i.e., they require an additional number of clusters as a guide when clustering, or lack effectiveness in terms of communication. In this paper, we explore the differences in the ability of different layers in a model to represent non-IID data, and propose a hierarchical CFL approach, named HiCFL, which considers both adaptivity and communication efficiency. The improvement of communication efficiency is due to our proposed novel concept of model stability, which characterizes the variation of model weights during training. Based on model stability, HiCFL can find the proper time to bi-partition the clusters of mobile devices in a hierarchical manner more quickly. We conduct extensive experiments based on popular datasets with various non-IID data settings. The results show that HiCFL achieves excellent performance effectiveness and efficiency. Compared to state-of-the-art approaches, HiCFL can improve the model accuracy by 2.0\\%\\! \\sim \\!9.0\\%, while reducing the communication overheads by 27.3\\%\\! \\sim\\! 80.6\\%.}
}


@article{DBLP:journals/tmc/CuiYWFH24,
	author = {Zhenhua Cui and
                  Tao Yang and
                  Xiaofeng Wu and
                  Hui Feng and
                  Bo Hu},
	title = {The Data Value Based Asynchronous Federated Learning for {UAV} Swarm
                  Under Unstable Communication Scenarios},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7165--7179},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3331906},
	doi = {10.1109/TMC.2023.3331906},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CuiYWFH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has provided a new approach to coordinating a group of clients to train a machine learning model collaboratively, which can be easily embedded into Unmanned Aerial Vehicle (UAV) swarms. Compared with the terrestrial wireless networks, the UAV swarm faces more precarious communication conditions, rendering synchronous aggregation no longer tenable. Additionally, the data collected from UAVs tend to be heterogeneous due to different deployment regions or requirements. To overcome these restrictions, this article has proposed a novel two-stage Asynchronous Federated Learning scheme for the UAV swarm. Initially, the convergence property of both convex and non-convex models trained by the proposed scheme is analyzed. In the pre-training stage, we modeled the learning process as a cooperative game with demonstrated monotonicity and submodularity. Furthermore, the Shapley Value is imported to quantify data values of UAVs, and the upper bound of its estimation error rate is derived. In the training stage, a new concept named Network Age of Updates (AoU) is proposed to address the fairness issue, quantifying the model's generalization capability with data value consideration, and a sequential UAV selection scheduling is performed through the AoU minimization by Whittle Index method. Finally, the system performance is validated through both theoretical analysis and simulations.}
}


@article{DBLP:journals/tmc/ZhangYZWLGY24,
	author = {Yao Zhang and
                  Zhiwen Yu and
                  Jun Zhang and
                  Liang Wang and
                  Tom H. Luan and
                  Bin Guo and
                  Chau Yuen},
	title = {Learning Decentralized Traffic Signal Controllers With Multi-Agent
                  Graph Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7180--7195},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332081},
	doi = {10.1109/TMC.2023.3332081},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangYZWLGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers optimal traffic signal control in smart cities, which has been taken as a complex networked system control problem. Given the interacting dynamics among traffic lights and road networks, attaining controller adaptivity and scalability stands out as a primary challenge. Capturing the spatial-temporal correlation among traffic lights under the framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution. Nevertheless, existing MARL algorithms ignore effective information aggregation which is fundamental for improving the learning capacity of decentralized agents. In this paper, we design a new decentralized control architecture with improved environmental observability to capture the spatial-temporal correlation. Specifically, we first develop a topology-aware information aggregation strategy to extract correlation-related information from unstructured data gathered in the road network. Particularly, we transfer the road network topology into a graph shift operator by forming a diffusion process on the topology, which subsequently facilitates the construction of graph signals. A diffusion convolution module is developed, forming a new MARL algorithm, which endows agents with the capabilities of graph learning. Extensive experiments based on both synthetic and real-world datasets verify that our proposal outperforms existing decentralized algorithms.}
}


@article{DBLP:journals/tmc/PangHZZLC24,
	author = {Jinlong Pang and
                  Ziyi Han and
                  Ruiting Zhou and
                  Renli Zhang and
                  John C. S. Lui and
                  Hao Chen},
	title = {Eris: An Online Auction for Scheduling Unbiased Distributed Learning
                  Over Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7196--7209},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333368},
	doi = {10.1109/TMC.2023.3333368},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PangHZZLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of edge intelligence has made smart IoT services (e.g., video/audio surveillance, autonomous driving and smart city) a reality. To ensure the quality of service, edge service providers train unbiased models of distributed machine learning jobs over the local datasets collected by edge networks, and usually adopt the parameter server (PS) architecture. However, the training of unbiased distributed learning (UDL) depends on geo-distributed data and edge resources, bringing a new challenge for service providers: how to effectively schedule and price UDL jobs such that the long-term system utility (i.e., social welfare) can be maximized. In this paper, we propose an online auction-based scheduling algorithm Eris, which determines the data workload, the number and the placement of concurrent workers and PSs for each arriving UDL job, and dynamically prices limited edge resources based on current resource consumption. Eris applies a primal-dual framework which calls an efficient dual subroutine to schedule UDL jobs, achieving a good competitive ratio and pseudo-polynomial time complexity. To evaluate the effectiveness of Eris, we implement both a testbed and a large-scaled simulator. The results demonstrate that Eris outperforms and achieves up to 44% more social welfare compared to state-of-the-art algorithms in today's cloud system.}
}


@article{DBLP:journals/tmc/ZhouZSWLHYL24,
	author = {Man Zhou and
                  Yuting Zhou and
                  Shuao Su and
                  Qian Wang and
                  Qi Li and
                  Shengshan Hu and
                  Chunwu Yu and
                  Zhengxiong Li},
	title = {FingerPattern: Securing Pattern Lock via Fingerprint-Dependent Friction
                  Sound},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7210--7224},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3338148},
	doi = {10.1109/TMC.2023.3338148},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZSWLHYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pattern lock is widely used for user authentication in mobile devices due to its simplicity and ease of remembering. However, it is vulnerable to various attacks, e.g., shoulder surfing attacks. In this paper, we propose FingerPattern, a novel enhanced pattern lock authentication system by using friction sound as a second authentication factor. When the user inputs the pattern by swiping his/her fingertip on the screen, the friction sound is generated based on the user's fingerprint and is unique. Thus, FingerPattern can identify and rule out the illegality by utilizing fingerprint-dependent friction sound even if the adversary has inferred the pattern. By this method, FingerPattern secures pattern lock without the need for a change in user unlocking habits. Extensive experiments demonstrate that FingerPattern can identify legitimate users with 97.5% TAR in one attempt and defend against various attacks (e.g., only 16.8% FAR in five attempts even if the adversary can clearly spy on the user's unlocking process). FingerPattern can be incorporated into the existing pattern lock of mobile devices, which is cost-free. Furthermore, a user experience study shows that FingerPattern is well-received by users.}
}


@article{DBLP:journals/tmc/FangXZCLBW24,
	author = {Hui Fang and
                  Zhu Xiao and
                  Pengfei Zheng and
                  Hongyang Chen and
                  Zhao Li and
                  Jiajun Bu and
                  Haishuai Wang},
	title = {Learning Co-occurrence Patterns for Next Destination Recommendation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7225--7237},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333944},
	doi = {10.1109/TMC.2023.3333944},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FangXZCLBW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next destination recommendation is a crucial research area for understanding human travel behavior. However, existing studies often overlook the problem of underfitting, which arises due to the limited regularity in users’ travel patterns. To tackle this issue, we leverage diverse co-occurrence patterns (CoPs) to discover potential user preferences. These patterns capture intersections with similar spatial and temporal characteristics in users’ travels. However, traditional graph neural network (GNN)-based approaches struggle to effectively handle complex spatial-temporal CoPs. To overcome these challenges, we propose a novel framework called DHIN (Dynamic Heterogeneous Information Network). First, to address the problem of underfitting, DHIN constructs intricate CoPs by leveraging abundant features and connection relationships. Additionally, to solve the needs of cold-start users, DHIN generates potential connections by capturing dynamic urban hotspots based on global users’ travel trajectories. Moreover, to model dynamic heterogeneous information, DHIN utilizes a hierarchical attention mechanism and integrates a dynamic encoder. The mechanism integrates multi-level attention to learn informative embeddings from heterogeneous attributes and structures, while the dynamic encoder processes dynamic temporal information for updating node representations. Finally, extensive experiments conducted on real-world trajectory data demonstrate the effectiveness of the proposed DHIN model.}
}


@article{DBLP:journals/tmc/LiHJPZTYL24,
	author = {Anran Li and
                  Jiahui Huang and
                  Ju Jia and
                  Hongyi Peng and
                  Lan Zhang and
                  Luu Anh Tuan and
                  Han Yu and
                  Xiang{-}Yang Li},
	title = {Efficient and Privacy-Preserving Feature Importance-Based Vertical
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7238--7255},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333879},
	doi = {10.1109/TMC.2023.3333879},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiHJPZTYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vertical Federated Learning (VFL) enables multiple data owners, each holding a different subset of features about a largely overlapping set of data samples, to collaboratively train a global model. The quality of data owners’ local features affects the performance of the VFL model, which makes feature selection vitally important. However, existing feature selection methods for VFL either assume the availability of prior knowledge on the number of noisy features or prior knowledge on the post-training threshold of useful features to be selected, making them unsuitable for practical applications. To bridge this gap, we propose the Federated Stochastic Dual-Gate based Feature Selection (FedSDG-FS) approach. It consists of a Gaussian stochastic dual-gate to efficiently approximate the probability of a feature being selected. FedSDG-FS further designs a local embedding perturbation approach to achieve differential privacy for local training data. To reduce overhead, we propose a feature importance initialization method based on Gini impurity, which can accomplish its goals with only two parameter transmissions between the server and the clients. The enhanced version, FedSDG-FS++, protects the privacy for both the clients’ training data and the server's labels through Partially Homomorphic Encryption (PHE) without relying on a trusted third-party. Theoretically, we analyze the convergence rate, privacy guarantees and security analysis of our methods. Extensive experiments on both synthetic and real-world datasets show that FedSDG-FS and FedSDG-FS++ significantly outperform existing approaches in terms of achieving more accurate selection of high-quality features as well as improving VFL performance in a privacy-preserving manner.}
}


@article{DBLP:journals/tmc/HeHFL24,
	author = {Yan He and
                  Qiuye He and
                  Song Fang and
                  Yao Liu},
	title = {Precise Wireless Camera Localization Leveraging Traffic-Aided Spatial
                  Analysis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7256--7269},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333272},
	doi = {10.1109/TMC.2023.3333272},
	timestamp = {Thu, 28 Nov 2024 21:30:08 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeHFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless cameras nowadays commonly employ motion sensors to identify that something is occurring in their fields of vision before starting to record and notifying the property owner of the activity. In this paper, we discover that the motion sensing action can disclose the location of the camera through a novel wireless camera localization technique we call MotionCompass. By creating motion stimuli and sniffing wireless traffic for a response to that stimuli, a user can obtain the motion trajectories within the motion detection zone and then use them to calculate the camera's location. We also extend the camera localization algorithm to pinpoint cameras in always-active mode. We develop an Android app to implement MotionCompass. Our extensive experiments using the developed app and 18 popular wireless cameras demonstrate that for cameras with one motion sensor, MotionCompass can attain a mean localization error of around 5 cm with less than 140 seconds. We also discuss defenses against MotionCompass. Our localization technique builds upon existing work that detects the existence of hidden cameras, to pinpoint their exact location.}
}


@article{DBLP:journals/tmc/LeHLTNDPV24,
	author = {Anh{-}Tu Le and
                  Tran Dinh Hieu and
                  Chi{-}Bao Le and
                  Phu Tran Tin and
                  Tan N. Nguyen and
                  Zhiguo Ding and
                  H. Vincent Poor and
                  Miroslav Vozn{\'{a}}k},
	title = {Power Beacon and NOMA-Assisted Cooperative IoT Networks With Co-Channel
                  Interference: Performance Analysis and Deep Learning Evaluation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7270--7283},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333764},
	doi = {10.1109/TMC.2023.3333764},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LeHLTNDPV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study investigates a two-way relaying non-orthogonal multiple access (TWR-NOMA) enabled Internet-of-Things (IoT) network, in which two NOMA users communicate via an IoT access point (IAP) relay using a decode-and-forward (DF) protocol. A power beacon (PB) is used to power the IAP to address the IAP's limited lifetime due to energy constraints. Since co-channel interference (CCI) is inevitable in IoT systems, this effect is also studied in the proposed system to improve practicality. Based on the proposed system model, the closed-form equations for the exact and asymptotic outage probability (OP) and ergodic data (ED) of the NOMA users’ signals are first derived to describe the performance of TWR-NOMA systems. The system's diversity order and throughput are then evaluated according to the derived results. To further improve the system's performance, a low-complexity strategy 2D golden section search (GSS) is performed, subject to power allocation (PA) and time-switching (TS) factors, to optimize the outage performance. Finally, a deep learning design with minimal computing complexity and precision OP prediction is established for a real-time IoT network configuration. The numerical results are discussed and analyzed in terms of the effects of the CCI, the TS ratio, the PA factor, the fading parameter on the OP, system throughput, and ED.}
}


@article{DBLP:journals/tmc/CaoLCLZYW24,
	author = {Yetong Cao and
                  Fan Li and
                  Huijie Chen and
                  Xiaochen Liu and
                  Shengchun Zhai and
                  Song Yang and
                  Yu Wang},
	title = {Live Speech Recognition via Earphone Motion Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7284--7300},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333214},
	doi = {10.1109/TMC.2023.3333214},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLCLZYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent literature advances motion sensors mounted on smartphones and AR/VR headsets to speech eavesdropping due to their sensitivity to subtle vibrations. The popularity of motion sensors in earphones has fueled a rise in their sampling rate, which enables various enhanced features. This paper investigates a new threat of eavesdropping via motion sensors of earphones by developing EarSpy, which builds on our observation that the earphone's accelerometer can capture bone conduction vibrations (BCVs) and ear canal dynamic motions (ECDMs) associated with speaking; they enable EarSpy to derive unique information about the wearer's speech. Leveraging a study on the motion sensor measurements captured from earphones, EarSpy gains abilities to disentangle the wearer's live speech from interference caused by body motions and vibrations generated when the earphone's speaker plays audio. To enable user-independent attacks, EarSpy involves novel efforts, including a trajectory instability reduction method to calibrate the waveform of ECDMs and a data augmentation method to enrich the diversity of BCVs. Moreover, EarSpy explores effective representations from BCVs and ECDMs, and develops a neural network model with character-level and word-level speech recognition models to realize speech recognition. Extensive experiments involving 14 participants demonstrate that EarSpy reaches a promising recognition for the wearer's speech.}
}


@article{DBLP:journals/tmc/WangCZLH24,
	author = {Ziyi Wang and
                  Yihong Chen and
                  Hao Zheng and
                  Meng Liu and
                  Ping Huang},
	title = {Body {RFID} Skeleton-Based Human Activity Recognition Using Graph
                  Convolution Neural Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7301--7317},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333043},
	doi = {10.1109/TMC.2023.3333043},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangCZLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition (HAR) has become a hotspot. However, the existing HAR has some shortcomings, such as few recognized human activities, no identification, privacy leakage, and battery maintenance. Therefore, we design body RFID skeleton which fully senses the key features of human activity, and further propose human activity recognitions by taking advantage of RFID (privacy protection, identification, and battery-less maintenance). First, we devise RFID skeleton activity graph as human activity model. Second, we propose baseline RFID skeleton activity graph convolution network (FSCN) by using the graph convolution network (GCN), and FSCN classifies the RFID skeleton activity graph for human activity recognition. Third, to solve the tag response signal feature data over-smoothing and the different information aggregation degree in FSCN, we propose improved FSCN with the residual network (R-FSCN). Finally, we design the parallel RFID skeleton activity graph convolution network (PR-FSCN) for optimizing R-FSCN. Massive experiments show that PR-FSCN has comprehensive superiority to the existing HARs. As far as we know, this study is the first work that appropriately designs body RFID skeleton for HAR, and that successfully introduces GCN to investigate the new bound-RFID HAR with high recognition performance.}
}


@article{DBLP:journals/tmc/LiuMLYY24,
	author = {Yu Liu and
                  Yingling Mao and
                  Zhenhua Liu and
                  Fan Ye and
                  Yuanyuan Yang},
	title = {Joint Task Offloading and Resource Allocation in Heterogeneous Edge
                  Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7318--7334},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335198},
	doi = {10.1109/TMC.2023.3335198},
	timestamp = {Mon, 22 Jul 2024 08:26:53 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuMLYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing has emerged as a prevalent computing paradigm to support applications that demand low latency and high computational capacity. Hardware reconfigurable accelerators exhibit high energy efficiency and low latency compared to general-purpose servers, making them ideal for integration into mobile edge computing systems. This article investigates the problem of joint task offloading, access point selection, and resource allocation in heterogeneous edge environments for latency minimization. Given the heterogeneity of edge computing devices and the interdependence of the decisions required for offloading, access point selection, and resource allocation, it is challenging to optimize over them simultaneously. We decomposed the proposed problem into two disjoint subproblems and developed algorithms for each of them. The first subproblem is to jointly determine access point selection and communication resource allocation decisions, for which we have proposed an algorithm with a provable approximation ratio of 2.62/(1-8\\lambda ), where \\lambda is a tunable parameter balancing the approximation ratio and time complexity. Additionally, we offer a faster variant of the algorithm with an approximation ratio of (\\sqrt{3}+1)^{2}. The second subproblem is to determine offloading and computing resource allocation decisions jointly and is NP-hard, where we developed algorithms based on relaxation and rounding. We conducted comprehensive numerical simulations to evaluate the proposed algorithms, and the results demonstrated that our algorithms outperformed existing baselines and achieved near-optimal performance across various settings.}
}


@article{DBLP:journals/tmc/LiGLWCZYJ24,
	author = {Jing Li and
                  Song Guo and
                  Weifa Liang and
                  Jianping Wang and
                  Quan Chen and
                  Yue Zeng and
                  Baoliu Ye and
                  Xiaohua Jia},
	title = {Digital Twin-Enabled Service Provisioning in Edge Computing via Continual
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7335--7350},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332668},
	doi = {10.1109/TMC.2023.3332668},
	timestamp = {Sun, 04 Aug 2024 19:47:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiGLWCZYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Propelled by recent advances in Mobile Edge Computing (MEC) and the Internet of Things (IoT), the digital twin technique has been envisioned as a de-facto driving force to bridge the virtual and physical worlds through creating digital portrayals of physical objects. In virtue of the flourishing of edge intelligence and abundant IoT data, data-driven modelling facilitates the implementation and maintenance of digital twins, where simulations of physical objects are usually performed based on Deep Neural Networks (DNNs). A significant advantage of adopting digital twins is to enable decisive prediction on the behaviours of objects in near future without waiting for that really happen. To provide accurate predictions, it is vital to keep each digital twin synchronized with its physical object in real-time. However, it is challenging to maintain the real-time synchronization between a digital twin and its physical object due to the dynamics of physical objects and sensing data drift over time, i.e., the live data from a physical object diverge from the model training data of its digital twin. To address this critical issue, continual learning is a promising solution to retrain models of digital twins incrementally. In this article, we investigate digital twin synchronization issues via continual learning in an MEC environment, with the aim to maximize the total utility gain, i.e., the enhanced model accuracy. We study two novel optimization problems: the static digital twin synchronization problem per time slot and the dynamic digital twin synchronization problem for a finite time horizon. We first formulate an Integer Linear Program (ILP) solution for the static digital twin synchronization problem when the problem size is small; otherwise, we develop a randomized approximation algorithm at the expense of bounded resource violations for it. We also devise a deterministic approximation algorithm with guaranteed performance for a special case of the static digital twin synchronization problem. We third consider the dynamic digital twin synchronization problem by proposing an efficient online algorithm for it. Finally, we evaluate the performance of the proposed algorithms for continuous digital twin synchronization through simulations. Simulation results show that the proposed algorithms are promising, outperforming counterpart benchmarks by no less than 13.2%.}
}


@article{DBLP:journals/tmc/VermaCR24,
	author = {Janardan Kumar Verma and
                  Jitender Kumar Chhabra and
                  Virender Ranga},
	title = {Track Consensus-Based Labeled Multi-Target Tracking in Mobile Distributed
                  Sensor Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7351--7362},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3333916},
	doi = {10.1109/TMC.2023.3333916},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/VermaCR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an efficient algorithm for tracking multiple targets using a network of static and mobile sensors (robots). Multi-target tracking has a broad array of applications, including crowd monitoring, vehicle tracking, warehouse automation, and pedestrian safety, among others. The problem of distributed labeled multi-target tracking comprises constraints on sensing range, communication, label consistency, and motion. Hence, our algorithm strives to minimize label mismatching, communication, movement of robots, and tracking error, which are serious concerns in the existing solutions. The problem is decomposed into two sub-problems: distributed estimation and adaptive movement control. We present a novel track consensus algorithm for estimating the number and tracks of targets, complemented by an efficient label consensus method. This algorithm can effectively identify similar tracks and fuse them in cluttered scenarios. Various movement control strategies are proposed to minimize the moving distance of the robots while keeping the maximum number of targets in the sensing range. The maximum target sensing problem is NP-hard; therefore, we propose and compare heuristic, approximation, and randomized algorithms. We have also verified our proposed solution through extensive simulations and compared the distributed estimation and movement control algorithms with other prominent solutions. We also analyze estimation accuracy using the Optimal Sub-Pattern Assignment (OSPA) metric, asymptotic performance, and communication cost and confirm the real-time computation of our proposed algorithms.}
}


@article{DBLP:journals/tmc/ZhaoMSLLDZ24,
	author = {Zihao Zhao and
                  Yuzhu Mao and
                  Zhenpeng Shi and
                  Yang Liu and
                  Tian Lan and
                  Wenbo Ding and
                  Xiao{-}Ping Zhang},
	title = {{AQUILA:} Communication Efficient Federated Learning With Adaptive
                  Quantization in Device Selection Strategy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7363--7376},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3332901},
	doi = {10.1109/TMC.2023.3332901},
	timestamp = {Thu, 05 Dec 2024 20:56:40 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoMSLLDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices’ data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization in device selection strategy), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the exact global model stored by devices enables a more precise device selection criterion, reduces model deviation, and limits the need for hyperparameter adjustments. Furthermore, AQUILA presents an innovative quantization criterion, optimized to improve communication efficiency while assuring model convergence. Our experiments demonstrate that AQUILA significantly decreases communication costs compared to existing methods, while maintaining comparable model performance across diverse non-homogeneous FL settings, such as Non-IID data and heterogeneous model architectures.}
}


@article{DBLP:journals/tmc/ZhouLW24,
	author = {Longyu Zhou and
                  Supeng Leng and
                  Qing Wang},
	title = {A Federated Digital Twin Framework for UAVs-Based Mobile Scenarios},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7377--7393},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335386},
	doi = {10.1109/TMC.2023.3335386},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of communication networks and Artificial Intelligence (AI) technologies, Digital Twin (DT) now emerges to support various applications such as engineering, monitoring, controlling, healthcare and the optimization of cyber-physical systems. There is an increasing demand to create DTs that can represent physical entities for improving operational efficiency. A conventional DT consists of monitoring, imitation, and feedback control. However, conventional DTs cannot ensure efficient real-time imitation due to the high dynamics of physical systems such as UAV-based target tracking scenario. To address this issue, we propose a federated DT framework to support the imitation of mobile systems. It can guarantee real-time and accurate imitations under the prerequisite of comprehensive information acquired by a cooperative collection algorithm with the aid of UAVs. The framework can rapidly aggregate local DT models using an attention-based mechanism to improve mobile imitation accuracy. Additionally, we propose a multimodal-based DT inspection algorithm that can correct the postures of UAVs affected by winds for reliable imitations. We implement the framework in Gazebo. Our system simulations demonstrate the efficiency of the proposed federated DT framework. Our solution can reduce the imitation latency by an average of 68.4%, meanwhile, can improve the imitation accuracy by 16.4% on average when compared to traditional centralized and distributed imitation schemes.}
}


@article{DBLP:journals/tmc/AtikCGB24,
	author = {Syeda Tanjila Atik and
                  Akshar Shravan Chavan and
                  Daniel Grosu and
                  Marco Brocanelli},
	title = {A Maintenance-Aware Approach for Sustainable Autonomous Mobile Robot
                  Fleet Management},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7394--7407},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3334589},
	doi = {10.1109/TMC.2023.3334589},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AtikCGB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous mobile robots (AMRs) are capable of carrying out operations continuously for 24/7, which enables them to optimize tasks, increase throughput, and meet demanding operational requirements. To ensure seamless and uninterrupted operations, an effective coordination of task allocation and charging schedules is crucial while considering the preservation of battery sustainability. Moreover, regular preventive maintenance plays an important role in enhancing the robustness of AMRs against hardware failures and abnormalities during task execution. However, existing works do not consider the influence of properly scheduling AMR maintenance on both task downtime and battery lifespan. In this paper, we propose {\\sf MTC}, a maintenance-aware task and charging scheduler designed for fleets of AMR operating continuously in highly automated environments. {\\sf MTC} leverages Linear Programming (LP) to first help decide the best time to schedule maintenance for a given set of AMRs. Subsequently, the Kuhn-Munkres algorithm, a variant of the Hungarian algorithm, is used to finalize task assignments and carry out the charge scheduling to minimize the combined cost of task downtime and battery degradation. Experimental results demonstrate the effectiveness of {\\sf MTC}, reducing the combined total cost up to 3.45 times and providing up to 68% improvement in battery capacity degradation compared to the baselines.}
}


@article{DBLP:journals/tmc/LiuRWXLMX24,
	author = {Tang Liu and
                  Meixuan Ren and
                  Die Wu and
                  Jing Xue and
                  Jingwen Li and
                  Sun Mao and
                  Wenzheng Xu},
	title = {Utilizing the Neglected Back Lobe for Directional Charging Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7408--7421},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3334518},
	doi = {10.1109/TMC.2023.3334518},
	timestamp = {Fri, 31 May 2024 21:08:04 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuRWXLMX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefitting from the breakthrough of wireless power transfer technology, the lifetime of Wireless Sensor Networks (WSNs) can be significantly prolonged by scheduling a mobile charger (MC) to charge sensors. Compared with omnidirectional charging, the MC equipped with directional antenna can concentrate energy in the intended direction, making charging more efficient. However, all prior arts ignore the considerable energy leakage behind the directional antenna (i.e., back lobe), resulting in energy wasted in vain. To address this issue, we study a fundamental problem of how to utilize the neglected back lobe and schedule the directional MC efficiently. Towards this end, we first build and verify a directional charging model considering both main and back lobes. Then, we focus on jointly optimizing the number of dead sensors and energy usage effectiveness. We achieve these by introducing a scheduling scheme that utilizes both main and back lobes to charge multiple sensors simultaneously. Finally, extensive simulations and field experiments demonstrate that our scheme reduces the number of dead sensors by 49.5% and increases the energy usage effectiveness by 10.2% on average as compared with existing algorithms.}
}


@article{DBLP:journals/tmc/BasnayakeMCJ24,
	author = {Vishaka Basnayake and
                  Hakim Mabed and
                  Philippe Canalda and
                  Dushantha Nalin K. Jayakody},
	title = {Continuous and Responsive {D2D} Victim Localization for Post-Disaster
                  Emergencies},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7422--7437},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3336353},
	doi = {10.1109/TMC.2023.3336353},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/BasnayakeMCJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the most challenging tasks in a disaster scenario is the detection and localization of victims with high accuracy and minimum delay, especially in out-of-coverage areas. In the event of a disaster that disrupts the cellular network infrastructure, emergency calls can be relayed to the core network via multi-hop D2D communications. In this paper, a localization system is proposed that uses radio measurements obtained through such D2D multi-hop assisted emergency calls to localize in-coverage and out-of-coverage devices. To address the uncertainty and gradual reception of data in real-time in this scenario, a dynamic constraint satisfaction-based Multi Victim Localization Algorithm (MVLA) is proposed. This algorithm locates multi-hop devices in a progressive propagation manner to provide fast and accurate updates on victim locations. Additionally, three modes of {MVLA}, namely {MVLA}_{recent}, {MVLA}_{seq}, and {MVLA}_{all} are proposed. Simulation results demonstrate that {MVLA}_{all} has a lower localization error compared to {MVLA}_{recent} and {MVLA}_{seq}. Moreover, {MVLA}_{all}, is compared with an existing particle filtering-based localization algorithm called RSSI Monte-Carlo Boxed Localization (RSSI-MCL) under an increasing number of emergency user devices and functional gNodeBs. Results show that {MVLA}_{all} significantly outperforms the RSSI-MCL method in terms of localization accuracy and computational delay.}
}


@article{DBLP:journals/tmc/TutuncuogluD24a,
	author = {Feridun T{\"{u}}t{\"{u}}nc{\"{u}}oglu and
                  Gy{\"{o}}rgy D{\'{a}}n},
	title = {Joint Resource Management and Pricing for Task Offloading in Serverless
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7438--7452},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3334914},
	doi = {10.1109/TMC.2023.3334914},
	timestamp = {Sat, 08 Jun 2024 13:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TutuncuogluD24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of resource allocation, pricing and application caching for latency sensitive task offloading in serverless edge computing. We model the interaction between a profit-maximizing operator and cost-minimizing Wireless Devices (WDs) as a Stackelberg game where the operator is the leader and decides the price, resource allocation and set of applications to cache, while the WDs are the followers and decide whether to offload their tasks. We first show that the game has a Subgame Perfect Equilibrium (SPE), but computing it, is NP-hard. Importantly, we show that an SPE, which maximizes the operator's revenue, results in minimal energy consumption among the WDs. For computing an approximate SPE, we propose a linear time approximation algorithm with bounded approximation ratio for resource allocation and pricing, and we propose an efficient heuristic based on the utility density of individual applications for the joint optimization of caching, resource allocation and pricing. Our results show that the proposed algorithm outperforms state-of-the-art methods by up to an order of magnitude both in terms of revenue and total energy savings and has small computational overhead. An interesting feature of our results is that the utility of the operator is maximized by a solution that maximizes the WDs’ energy savings through computation offloading, which makes it a promising candidate for energy efficient edge cloud deployments.}
}


@article{DBLP:journals/tmc/LiaoXXWQQ24,
	author = {Yunming Liao and
                  Yang Xu and
                  Hongli Xu and
                  Lun Wang and
                  Chen Qian and
                  Chunming Qiao},
	title = {Decentralized Federated Learning With Adaptive Configuration for Heterogeneous
                  Participants},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7453--7469},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335403},
	doi = {10.1109/TMC.2023.3335403},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiaoXXWQQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data generated at the network edge can be processed locally by leveraging the paradigm of edge computing (EC). Aided by EC, decentralized federated learning (DFL), which overcomes the single-point-of-failure problem in the parameter server based federated learning, is becoming a practical and popular approach for machine learning over distributed data. However, DFL faces two critical challenges, i.e., system heterogeneity and statistical heterogeneity introduced by edge devices. To ensure fast convergence with the existence of slow edge devices, we present an efficient DFL method, termed FedHP, which integrates adaptive control of both local updating frequency and network topology to better support the heterogeneous participants. We establish a theoretical relationship between local updating frequency and network topology regarding model training performance and obtain a convergence upper bound. Upon the convergence bound, we propose an optimization algorithm that adaptively determines local updating frequencies and constructs the network topology, so as to speed up convergence and improve the model accuracy. We evaluate the performance of FedHP through extensive simulation and testbed experiments. Evaluation results show that the proposed FedHP can reduce the completion time by about 51% and improve model accuracy by at least 5% in heterogeneous scenarios, compared with the baselines.}
}


@article{DBLP:journals/tmc/XueQXLL24,
	author = {Jiahao Xue and
                  Zhe Qu and
                  Jie Xu and
                  Yao Liu and
                  Zhuo Lu},
	title = {Bandwidth Allocation for Federated Learning With Wireless Providers
                  and Cost Constraints},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7470--7482},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335258},
	doi = {10.1109/TMC.2023.3335258},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XueQXLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) trains a global learning model by using a central server to collaborate with multiple decentralized clients. In a wireless network, the data transmission latency between a client and the FL server is substantially affected by signal quality dynamics and bandwidth allocation. FL clients require synchronized communication at each round to update their models simultaneously, which makes bandwidth allocation methods for conventional wireless tasks infeasible to use. Existing bandwidth allocation studies for FL mainly focused on allocating bandwidth of one bandwidth provider without cost. In this paper, we consider a more practical and challenging problem: how to assign the bandwidth to clients under multiple wireless providers to minimize the FL round length (i.e., the latency that FL finishes one round of model training and updating) with bandwidth capability and cost constraints? We propose a model that maps the problem into a new variant of the knapsack problem, called multi-dimensional max-min multiple knapsacks (MDM^{\\,3}KP). Based on MDM^{\\,3}KP, we create an iterative solution to find the client assignment and bandwidth allocation that minimizes the FL round length. Comprehensive simulation results show that the solution reduces the FL round length by up to 70.8% compared with other benchmarks.}
}


@article{DBLP:journals/tmc/ZhaoSCHLW24,
	author = {Guangrong Zhao and
                  Yiran Shen and
                  Ning Chen and
                  Pengfei Hu and
                  Lei Liu and
                  Hongkai Wen},
	title = {EV-Tach: {A} Handheld Rotational Speed Estimation System With Event
                  Camera},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7483--7498},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335221},
	doi = {10.1109/TMC.2023.3335221},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoSCHLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rotational speed is one of the important metrics to be measured for calibrating electric motors in manufacturing, monitoring engines during car repairs, detecting faults in electrical appliance and more. However, existing measurement techniques either require prohibitive hardware (e.g., high-speed camera) or are inconvenient to use in real-world application scenarios. In this article, we propose, EV-Tach, a novel handheld rotational speed estimation system that utilizes emerging imaging sensors known as event cameras or dynamic vision sensors (DVS). The pixels of DVS work independently and trigger an event as soon as a per-pixel intensity change is detected, without global synchronization like conventional RGB cameras. Thus, its unique design features high temporal resolution and generates sparse events, which benefits the high-speed rotation estimation. To achieve accurate and efficient rotational speed estimation, a series of signal processing algorithms are specifically designed for the event streams generated by event cameras on an embedded platform. First, a new cluster-centroids initialization module is proposed to initialize the centroids of the clusters to address the issue that common clustering approaches are easy to fall into a local optimal solution without proper initial centroids. Second, an outlier removal module is designed to suppress the background noise caused by subtle hand movements and host devices vibrations. Third, a coarse-to-fine alignment strategy is proposed with an event stream alignment method to obtain angle of rotation and achieve accurate estimation for rotational speed in a large range. With these bespoke components, EV-Tach is able to extract the rotational speed accurately from the event stream produced by an event camera recording rotary targets. According to our extensive evaluations under controlled and practical experiment settings, the Relative Mean Absolute Error (RMAE) of EV-Tach is as low as 0.3\\%_{0}, which is comparable to the state-of-the-art laser tachometer under fixed measurement mode. Moreover, EV-Tach is robust to subtle movement of user's hand and dazzling light outdoor, therefore, can be used as a handheld device under challenging lighting condition, where the laser tachometer fails to produce reasonable results. To speed up the processing of EV-Tach and reduce its resource consumption on embedded devices, event stream is significantly downsampled by merging neighboring events while preserving its formation in spatial-temporal domain. At last, we implement EV-Tach on Raspberry Pi and the evaluation results show that the downsampling process preserves the high measurement accuracy while saving the computation speed and energy consumption by approximately 8 times and 30 times in average.}
}


@article{DBLP:journals/tmc/LiXYMZC24,
	author = {Danyang Li and
                  Jingao Xu and
                  Zheng Yang and
                  Qiang Ma and
                  Li Zhang and
                  Pengpeng Chen},
	title = {LeoVR: Motion-Inspired Visual-LiDAR Fusion for Environment Depth Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7499--7516},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3334271},
	doi = {10.1109/TMC.2023.3334271},
	timestamp = {Fri, 17 May 2024 21:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiXYMZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Environment depth estimation by fusing camera and radar enables a broad spectrum of applications such as autonomous driving, environmental perception, context-aware localization and navigation. Various pioneering approaches have been proposed to achieve accurate and dense depth estimation by integrating vision and LiDAR through deep learning. However, due to the challenges of sparse sampling of in-vehicle LiDARs, high ground-truth annotation overhead, and severe dynamics in real environments, existing solutions have not yet achieved widespread deployment on commercial autonomous vehicles. In this paper, we propose LeoVR, a motion-inspired self-supervised visual-LiDAR fusion approach that enables accurate environment depth estimation. Leveraging the vehicle motion information, LeoVR employs two effective system frameworks to (i) optimize the depth estimation results, and (ii) provide supervision signals for DNN training. We fully implemented LeoVR on both a robotic testbed and a commercial vehicle and conducted extensive experiments over an 8-month period. The results demonstrate that LeoVR achieves remarkable performance with an average depth estimation error of 0.17 m, outperforming existing state-of-the-art solutions by > 45.9%. Besides, even cold-start in real environments by self-supervised training, LeoVR still achieves an average error of 0.2 m, outperforming the related works by > 47.8% and comparable to supervised training methods.}
}


@article{DBLP:journals/tmc/DaiZWGQLXZDC24,
	author = {Haipeng Dai and
                  Yikang Zhang and
                  Weijun Wang and
                  Rong Gu and
                  Yuben Qu and
                  Chi Lin and
                  Lijie Xu and
                  Jiaqi Zheng and
                  Wanchun Dou and
                  Guihai Chen},
	title = {Placing Wireless Chargers With Multiple Antennas},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7517--7536},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3338563},
	doi = {10.1109/TMC.2023.3338563},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DaiZWGQLXZDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Charger placement is an important problem in improving the quality of service in wireless rechargeable sensor networks. This paper studies the problem of Wireless ChArger PlacemeNt with Multiple (Directional) Antennas (WANDA). The problem is described as follows: given a set of wireless chargers equipped with multiple directional antennas and a set of wireless rechargeable sensors, determine the chargers’ positions and orientations to maximize the overall charging utility. According to the relative positional relationship between the antennas, the problem is classified into Relative Orientation Fixed (WANDA-ROF) and Relative Orientation Unfixed (WANDA-ROU) situations. To address WANDA, we present a piecewise constant function to approximate the nonlinearity of charging power and propose an area discretization technique to reduce the infinite solution space to a limited one without performance loss. Then, we prove the monotonic submodularity of WANDA, and present a \\frac{1}{2}-\\epsilon approximation algorithm for the ROF situation and a \\frac{1}{6}-\\epsilon approximation algorithm for the ROU situation, all run in polynomial time. Finally, we conduct extensive simulation and experiments to show that our algorithms outperform comparison algorithms by at least 16% for ROF situation and 12% for ROU situation.}
}


@article{DBLP:journals/tmc/SunWMZWH24,
	author = {Yimiao Sun and
                  Weiguo Wang and
                  Luca Mottola and
                  Jia Zhang and
                  Ruijin Wang and
                  Yuan He},
	title = {Indoor Drone Localization and Tracking Based on Acoustic Inertial
                  Measurement},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {6},
	pages = {7537--7551},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335860},
	doi = {10.1109/TMC.2023.3335860},
	timestamp = {Thu, 08 Aug 2024 08:11:04 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunWMZWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present Acoustic Inertial Measurement ({\\sf AIM}\n), a one-of-a-kind technique for indoor drone localization and tracking. Indoor drone localization and tracking are arguably a crucial, yet unsolved challenge: in GPS-denied environments, existing approaches enjoy limited applicability, especially in Non-Line of Sight (NLoS), require extensive environment instrumentation, or demand considerable hardware/software changes on drones. In contrast, {\\sf AIM}\nexploits the acoustic characteristics of the drones to estimate their location and derive their motion, even in NLoS settings. We tame location estimation errors using a dedicated Kalman filter and the Interquartile Range rule (IQR) and demonstrate that {\\sf AIM}\ncan support indoor spaces with arbitrary ranges and layouts. We implement {\\sf AIM}\nusing an off-the-shelf microphone array and evaluate its performance with a commercial drone under varied settings. Results indicate that the mean localization error of {\\sf AIM}\nis 46\\mathrm{\\%}\nlower than that of commercial UWB-based systems in a complex 10\\,\nm\\,\n×\\,\n10\\,\nm indoor scenario, where state-of-the-art infrared systems would not even work because of NLoS situations. When distributed microphone arrays are deployed, the mean error can be reduced to less than 0.5\\mathrm{m}\nin a 20\\mathrm{m}\nrange, and even support spaces with arbitrary ranges and layouts.}
}


@article{DBLP:journals/tmc/ChenGWXLCGZ24,
	author = {Quan Chen and
                  Song Guo and
                  Kaijia Wang and
                  Wenchao Xu and
                  Jing Li and
                  Zhipeng Cai and
                  Hong Gao and
                  Albert Y. Zomaya},
	title = {Towards Real-Time Inference Offloading With Distributed Edge Computing:
                  The Framework and Algorithms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7552--7571},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335051},
	doi = {10.1109/TMC.2023.3335051},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenGWXLCGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By combining edge computing and parallel computing, distributed edge computing has emerged as a new paradigm to exploit the booming IoT devices at the edge. To accelerate computation at the edge, i.e., the inference tasks for DNN-driven applications, the parallelism of both computation and communication needs to be considered for distributed edge computing, and thus, the problem of Minimum Latency joint Communication and Computation Scheduling (MLCCS) is proposed. However, existing works have rigid assumptions that the communication time of each device is fixed and the workload can be split arbitrarily small. Aiming at making the work more practical and general, the MLCCS problem without the above assumptions is studied in this paper. First, the MLCCS problem under a general model is formulated and proved to be NP-hard. Second, a pyramid-based computing model is proposed to consider the parallelism of communication and computation jointly, which has an approximation ratio of 1+\\delta, where \\delta is related to devices’ communication rates. An interesting property under such a computing model is identified and proved, i.e., the optimal latency can be obtained under arbitrary scheduling order when all the devices share the same communication rate. When the workload cannot be split arbitrarily, an approximation algorithm with a ratio of at most 2\\cdot (1+\\delta) is proposed. Additionally, for handling the dynamically changing network scenarios, several algorithms are also proposed accordingly. Finally, the theoretical analysis and simulation results verify that the proposed algorithm has high performance in terms of latency. Two testbed experiments are also conducted, which show that the proposed method outperforms the existing methods, reducing the latency by up to 29.2% for inference tasks at the edge.}
}


@article{DBLP:journals/tmc/SunMZ24,
	author = {Yuchang Sun and
                  Yuyi Mao and
                  Jun Zhang},
	title = {MimiC: Combating Client Dropouts in Federated Learning by Mimicking
                  Central Updates},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7572--7584},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3338021},
	doi = {10.1109/TMC.2023.3338021},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a promising framework for privacy-preserving collaborative learning, where model training tasks are distributed to clients and only the model updates need to be collected at a server. However, when being deployed at mobile edge networks, clients may have unpredictable availability and drop out of the training process, which hinders the convergence of FL. This paper tackles such a critical challenge. Specifically, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg may oscillate around a stationary point of the global loss function in the worst case, which is caused by the divergence between the aggregated and desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modifies each received model update based on the previous ones. The proposed modification of the received model updates mimics the imaginary central update irrespective of dropout clients. The theoretical analysis of MimiC shows that divergence between the aggregated and central update diminishes with proper learning rates, leading to its convergence. Simulation results further demonstrate that MimiC maintains stable convergence performance and learns better models than the baseline methods.}
}


@article{DBLP:journals/tmc/WangZYYW24,
	author = {En Wang and
                  Mijia Zhang and
                  Bo Yang and
                  Yongjian Yang and
                  Jie Wu},
	title = {Large-Scale Spatiotemporal Fracture Data Completion in Sparse CrowdSensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7585--7601},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339089},
	doi = {10.1109/TMC.2023.3339089},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZYYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile CrowdSensing (MCS) is a widely adopted approach that involves engaging mobile users to collaboratively perform diverse sensing tasks. In Sparse CrowdSensing, the completion of data from partially-sensed sources plays a pivotal role in urban sensing applications. This process is essential as it enables efficient data representation, enhances urban analysis capabilities, and ultimately facilitates informed city planning decisions. By leveraging the power of mobile users, Sparse CrowdSensing contributes to the comprehensive understanding of urban environments, enabling effective utilization of data for optimizing urban infrastructure and fostering sustainable urban development. To achieve accurate completion results, previous methods usually utilize the universal similarity and conventional tendency while incorporating only a single dataset to infer the full map. However, in real-world scenarios, there may exist many kinds of data (inter-data), that could help to complement each other. Moreover, for each kind of data (intra-data), there usually exists a few but important spatiotemporal fracture data which caused by the special events (e.g. data loss, equipment failure, etc.), which may behave in a different way as the statistical patterns. Thus, it is an essential task to consider spatiotemporal fracture data caused by the special cases in spatiotemporal data inference, especially using both intra- and inter-data, because of the following challenges: 1) the sparsity of the sensed data, 2) the complex spatiotemporal relations and 3) the uncertain scale of a spatiotemporal fracture. To this end, focusing on the large-scale spatiotemporal fracture, we propose a data completion method that exploits both intra- and inter-data correlations for enhancing performance. Specifically, for the purpose of generating spatiotemporal fracture data, there is Stacked Generative Matrix Completion ($\\mathcal {SGMC}$) by combining previous Stacked Deep Matrix Factorization (SDMF) and Generative Adversarial Networks (GANs), which improves a lot. Along this line, we extract the features of spatiotemporal data and further efficiently complete and predict the unsensed data. Finally, we conduct both qualitative and quantitative experiments on two different datasets, and the results demonstrate that the performance of our method outperforms the state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/WuLLZ24,
	author = {Xianjing Wu and
                  Zhidan Liu and
                  Zhenjiang Li and
                  Shengjie Zhao},
	title = {Greta: Towards a General Roadside Unit Deployment Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7602--7617},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3335853},
	doi = {10.1109/TMC.2023.3335853},
	timestamp = {Thu, 04 Jul 2024 22:02:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an essential component, roadside units (RSUs) play an indispensable role in realizing Vehicle-to-Everything (V2X) by seamlessly connecting various intelligent devices and vehicles. To facilitate the construction of V2X, much research has been done in designing effective RSU deployment strategies. However, most of these efforts are largely limited by design utility and deployment scalability. To address the limitations of previous works, this paper proposes a general RSU deployment framework, Greta, which can evaluate candidate deployment sites from different perspectives with rich input data, and satisfy different requirements on optimization metrics. To this end, we model the general RSU deployment problem as a customized reinforcement learning (RL) problem that intelligently explores the deployment environment to find a good deployment strategy. Specifically, we design an effective data profiling network to extract features from multi-modality input data. These extracted features are gradually weighted, fused, and encoded as part of the state representation of the RL model. We further design new reward functions considering various deployment metrics and propose an action space pruning scheme to speed up model training. We implement a prototype system of Greta and extensively evaluate its performance using real-world data. The results show Greta achieves remarkable performance gains compared to recent RSU deployment methods.}
}


@article{DBLP:journals/tmc/ShenZMWCC24,
	author = {Guanxiong Shen and
                  Junqing Zhang and
                  Alan Marshall and
                  Roger F. Woods and
                  Joseph R. Cavallaro and
                  Liquan Chen},
	title = {Towards Receiver-Agnostic and Collaborative Radio Frequency Fingerprint
                  Identification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7618--7634},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3340039},
	doi = {10.1109/TMC.2023.3340039},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShenZMWCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency fingerprint identification (RFFI) is an emerging device authentication technique, which exploits the hardware characteristics of the RF front-end as device identifiers. The receiver hardware impairments interfere with the feature extraction of transmitter impairments, but their effect and mitigation have not been comprehensively studied. In this paper, we propose a receiver-agnostic RFFI system by employing adversarial training to learn the receiver-independent features. Moreover, when there are multiple receivers, collaborative inference are designed to enhance classification accuracy. Finally, we show how it is possible to leverage fine-tuning for further improvement with fewer collected signals. To validate the approach, we have conducted extensive experimental evaluation by applying the approach to a LoRaWAN case study involving ten LoRa devices and 20 software-defined radio (SDR) receivers. The results show that receiver-agnostic training enables the trained neural network to become robust to changes in receiver characteristics. The collaborative inference improves classification accuracy by up to 20% beyond a single-receiver RFFI system and fine-tuning can bring a 40% improvement for underperforming receivers. The system is further evaluated on a more practical testbed. By making additional use of online augmentation and multi-packet inference, the identification accuracy is improved from 50% to 90% at 10 dB.}
}


@article{DBLP:journals/tmc/PengLTZHJ24,
	author = {Junkun Peng and
                  Qing Li and
                  Xun Tang and
                  Dan Zhao and
                  Chuang Hu and
                  Yong Jiang},
	title = {A Cooperative Caching System in Heterogeneous Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7635--7649},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3336955},
	doi = {10.1109/TMC.2023.3336955},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PengLTZHJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the rapid growth of video content and the increasing demand for high Quality of Experience (QoE) have significantly strained the backbone network. Edge caching is a promising approach to alleviate the strain by caching content closer to users. However, it confronts challenges stemming from the low capability of individual edge nodes and the high density of their distribution, resulting in low hit ratios and unbalanced workloads. In this paper, we conduct in-depth analyses of these challenges and formulate a typical cooperative edge caching problem. Based on the insights, we introduce MagNet, a cooperative edge caching system featuring two key mechanisms: Automatic Content Congregating (ACC) and Mutual Assistance Group (MAG). ACC improves hit ratios by intelligently guiding requests to their optimal edges, thereby facilitating content aggregation. Complementing this, Quick Cache is implemented to accelerate this congregation process by prefetching content and optimizing cache space, effectively boosting hit ratios. MAG, on the other hand, achieves workload balance by dynamically forming groups to augment edge capabilities and redistribute requests on overloaded edges. To elucidate the design principles of MagNet, we conduct detailed component-level comparisons and quantitative analyses. To validate the overall performance, we compare it with various caching solutions using real-world datasets, demonstrating significant performance improvements.}
}


@article{DBLP:journals/tmc/XiangZZ24,
	author = {Liyao Xiang and
                  Shuang Zhang and
                  Quanshi Zhang},
	title = {Learning to Prevent Input Leakages in the Mobile Cloud Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7650--7663},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3340338},
	doi = {10.1109/TMC.2023.3340338},
	timestamp = {Sat, 20 Jul 2024 15:06:27 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiangZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Powered by machine learning services in the cloud, numerous learning-driven mobile applications are gaining popularity in the market. As deep learning tasks are mostly computation-intensive, it has become a trend to process raw data on devices and send the deep neural network (DNN) features to the cloud, where the features are further processed to return final results. However, there is always an unexpected leakage with the release of features, by which an adversary could infer much information on the original data. We propose a privacy-preserving framework on top of the mobile cloud infrastructure from the perspective of DNN structures. Our framework aims to learn a policy to modify the base DNNs to prevent information leakage while maintaining high inference accuracy. The policy can also be readily transferred to large-size DNNs and large-scale datasets to speed up learning. Extensive evaluations on a variety of DNNs have shown that our framework successfully finds privacy-preserving DNN structures to defend privacy attacks.}
}


@article{DBLP:journals/tmc/LiuZDJZC24,
	author = {Weijie Liu and
                  Xiaoxi Zhang and
                  Jingpu Duan and
                  Carlee Joe{-}Wong and
                  Zhi Zhou and
                  Xu Chen},
	title = {{DYNAMITE:} Dynamic Interplay of Mini-Batch Size and Aggregation Frequency
                  for Federated Learning With Static and Streaming Datasets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7664--7679},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3337016},
	doi = {10.1109/TMC.2023.3337016},
	timestamp = {Fri, 24 Jan 2025 08:36:54 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZDJZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a distributed learning paradigm that can coordinate heterogeneous edge devices to perform model training without sharing private data. While prior works have focused on analyzing FL convergence with respect to hyperparameters like batch size and aggregation frequency, the joint effects of adjusting these parameters on model performance, training time, and resource consumption have been overlooked, especially when facing dynamic data streams and network characteristics. This article introduces novel analytical models and optimization algorithms that leverage the interplay between batch size and aggregation frequency to navigate the trade-offs among convergence, cost, and completion time for dynamic FL training. We establish a new convergence bound for training error considering heterogeneous datasets across devices and derive closed-form solutions for co-optimized batch size and aggregation frequency that are consistent across all devices. Additionally, we design an efficient algorithm for assigning different batch configurations across devices, improving model accuracy and addressing the heterogeneity of both data and system characteristics. Further, we propose an adaptive control algorithm that dynamically estimates network states, efficiently samples appropriate data batches, and effectively adjusts batch sizes and aggregation frequency on the fly. Extensive experiments demonstrate the superiority of our offline optimal solutions and online adaptive algorithm.}
}


@article{DBLP:journals/tmc/ZhangZSWW24,
	author = {Dayou Zhang and
                  Hao Zhu and
                  Kai Shen and
                  Dan Wang and
                  Fangxin Wang},
	title = {{DSJA:} Distributed Server-Driven Joint Route Scheduling and Streaming
                  Adaptation for Multi-Party Realtime Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7680--7694},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339671},
	doi = {10.1109/TMC.2023.3339671},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZSWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread availability of convenient wireless network connection and video capture have fueled the development of multi-party realtime video streaming (MRVS) services, such as Zoom or Microsoft Teams. These services have transformed the generation and distribution of realtime streaming content and offer a new way of online communication, striving to provide high Quality-of-Experience (QoE) for individuals. However, delivering high QoE in MRVS is more challenging than in traditional video scenarios due to the stringent delay requirements and complex multi-party interactive architectures. In this paper, we propose DSJA, a distributed server-driven multi-party realtime video streaming framework that conquers the challenges. We first design an appropriate QoE model for MRVS services to capture the interplay among perceptual quality, variations, bitrate mismatch, loss damage, and streaming delay. We then model the QoE maximization problem in MRVS as a route scheduling and streaming adaptation problem. Afterward, we design DSJA which seamlessly integrates multiple selective forwarding units (SFU) architecture and server-driven approaches based on a two-step solution of route scheduling and streaming adaptation. DSJA first determines the most suitable SFU and streaming routes for each video session based on SFUs’ job queuing delay and path latency. Then, the server conducts joint loss and bitrate adaptation decisions to optimize the streaming configuration of all clients, considering network conditions and QoE preferences. Our evaluations show that our framework outperforms state-of-the-art solutions by 23.1\\% \\! \\sim \\! 41.7\\% from the perspective of QoE, and reduces the backbone network transmission by 14.0\\% \\! \\sim \\! 36.6\\%.}
}


@article{DBLP:journals/tmc/XiaoCPJX24,
	author = {Tingting Xiao and
                  Chen Chen and
                  Qingqi Pei and
                  Zhiyuan Jiang and
                  Shugong Xu},
	title = {{SFO:} An Adaptive Task Scheduling Based on Incentive Fleet Formation
                  and Metrizable Resource Orchestration for Autonomous Vehicle Platooning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7695--7713},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3337246},
	doi = {10.1109/TMC.2023.3337246},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoCPJX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous vehicle platooning has tremendous potential to relieve the burden of Vehicular Edge Computing (VEC) by sharing resources with nearby vehicles. Therefore, fleet formation and resource orchestration within vehicle platoons have recently ignited significant research interest. However, most fleet formation works focus on the intra-platoon configuration and information exchange, but few consider trajectory matching and joining willingness. Likewise, in multi-platoon scenarios, static resource orchestration for a single platoon no longer meets the demand from dynamic resource scheduling. To tackle these problems, we proposed the SFO scheme, an adaptive task Scheduling based on incentive fleet Formation and metrizable resource Orchestration. First, we design a fleet Formation algorithm based on Trajectory matching and Joining willingness (FTJ) to ensure the stable underlying architecture. Second, we use the Weighted Sum of Energy Consumption (WSEC) as the performance metric for resource orchestration and formulate the time-average WSEC minimization problem. Third, an Adaptive task Scheduling under Partitionable Applications and variable Resources (ASPAR) is proposed for an asymptotic optimal solution in reaction to the changeable backlog of the timeout queue. Finally, our numerical results demonstrate that our approach is superior to other latest and classic works in energy consumption and execution latency.}
}


@article{DBLP:journals/tmc/AbuD24,
	author = {Avi Abu and
                  Roee Diamant},
	title = {A {SLAM} Approach to Combine Optical and Sonar Information From an
                  {AUV}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7714--7724},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3336697},
	doi = {10.1109/TMC.2023.3336697},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AbuD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article proposes a simultaneous localization and mapping (SLAM) solution that combines inputs from sonar and optical images. Our solution for this navigation problem is solved by matching objects detected by the AUV's camera with objects identified within the SAS image. In particular, upon detecting an object using the AUV's camera, e.g., an underwater structure or even a rock, we rank the similarity of this object to all objects found within the SAS image. Taking a SLAM approach, the decision for the best match, and thus the position of the AUV within the SAS image, is made based not only on the current detected object, but also on similarity ranking previously detected objects while accounting for the AUV's self-measured heading. The solution is handled by a tracking mechanism that considers the possible positions of the objects within the SAS image as problem states, and modeling the state relations using a hidden Markov chain. Experimental results on real sonar and optical images implemented within a simulated environment show high accuracy in matching the location of an AUV within an SAS image in multiple scenarios.}
}


@article{DBLP:journals/tmc/MiaoSLLCD24,
	author = {Yinbin Miao and
                  Lin Song and
                  Xinghua Li and
                  Hongwei Li and
                  Kim{-}Kwang Raymond Choo and
                  Robert H. Deng},
	title = {Privacy-Preserving Arbitrary Geometric Range Query in Mobile Internet
                  of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7725--7738},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3336621},
	doi = {10.1109/TMC.2023.3336621},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MiaoSLLCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The mobile Internet of Vehicles (IoVs) has great potential for intelligent transportation, and creates spatial data query demands to realize the value of data. Outsourcing spatial data to a cloud server eliminates the need for local computation and storage, but it leads to data security and privacy threats caused by untrusted third-parties. Existing privacy-preserving spatial range query colorredsolutions based on Homomorphic Encryption (HE) have been developed to increase security. However, in the single server model, the private key is held by the query user, which incurs high computation and communication burdens on query users due to multiple rounds of interactions. Moreover, exposing data access patterns to semi-honest servers is highly vulnerable to frequency and statistical attacks. To solve these issues, in this paper we propose a secure spatial location query within arbitrary geometric range while protecting access pattern. Specifically, we apply Paillier algorithm and polynomial fitting technique to achieve secure arbitrary geometric range query, design secure and efficient search protocol to hide data access patterns and alleviate query users from high computation and communication burdens under dual-server model. Formal security analysis shows that our scheme is secure under semi-honest model, and extensive experiments demonstrate that our work can reduce users’ communication costs by more than 90% compared to previous schemes under single server model, which is practice in real-world scenarios.}
}


@article{DBLP:journals/tmc/LiWLFYW24,
	author = {Zhetao Li and
                  Jianhui Wang and
                  Saiqin Long and
                  Jianming Fu and
                  Min Yang and
                  Jian Weng},
	title = {A Trust Evaluation Joint Active Detection Method in Video Sharing
                  {D2D} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7739--7752},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339652},
	doi = {10.1109/TMC.2023.3339652},
	timestamp = {Thu, 13 Jun 2024 10:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiWLFYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The potentially malicious devices in D2D networks may spread forged videos to compromise system reliability. The prevailing passive trust model solutions have limitations, such as insufficient and inaccurate trust evidence, as well as weaker resistance to collusion attacks. This paper presents the Trust Evaluation Joint Active Detection (TEAD) method, which employs content correctness as trust evidence and allows devices to verify the authenticity of received videos via the base station. TEAD incorporates an active detection method to proactively determine the trustworthiness of devices. This promotes interaction among devices, resulting in an increase in trust evidence and an improvement in the accuracy of evaluation. Moreover, TEAD introduces a trust calculation method with a penalty mechanism to strengthen the system's resilience against malicious attacks. Empirical results show that TEAD outperforms state-of-the-art methods by achieving a more accurate trust evaluation and faster trust convergence with low extra energy consumption.}
}


@article{DBLP:journals/tmc/HuangLMZLSLG24,
	author = {Jinyang Huang and
                  Bin Liu and
                  Chenglin Miao and
                  Xiang Zhang and
                  Jianchun Liu and
                  Lu Su and
                  Zhi Liu and
                  Yu Gu},
	title = {PhyFinAtt: An Undetectable Attack Framework Against {PHY} Layer Fingerprint-Based
                  WiFi Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7753--7770},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3338954},
	doi = {10.1109/TMC.2023.3338954},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLMZLSLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi connection has been suffering from MAC forgery attacks due to the loose authentication mechanism between access points (APs) and clients. To address this problem, the physical (PHY) layer information-based fingerprint has been adopted for safe WiFi authentication. Since such a fingerprint is constant and unique for each specific network interface card (NIC), it can effectively prevent MAC forgery attacks. However, the PHY layer information-based fingerprint is still vulnerable to malicious attacks as it is extracted from Channel State Information (CSI), and its stability can be affected by the wireless environment. In this paper, we propose a novel undetectable attack framework, called PhyFinAtt, base on which the attacker can undermine the stability of the PHY layer-based authentication fingerprints through human movement and further attack the WiFi authentication protocols. Specifically, we first demonstrate that human movement at a designated location can affect the PHY fingerprint. We then illustrate the impact of human movement on the PHY fingerprint and the relationship between the movement and the channel quality to ensure that the PHY fingerprint is destroyed by the movement in an undetected way without affecting normal communication. Extensive experiments in real-world scenarios show that our proposed attack can effectively disrupt the stability of the PHY fingerprints and significantly degrade the performance of the authentication protocols based on such fingerprints. To the best of our knowledge, this is the first study on effective attacks against the PHY information-based WiFi authentication protocols. Furthermore, we also present a practical defense mechanism without involving any additional equipment to mitigate attacks similar to PhyFinAtt.}
}


@article{DBLP:journals/tmc/YuWQWZH24,
	author = {Lebin Yu and
                  Qiexiang Wang and
                  Yunbo Qiu and
                  Jian Wang and
                  Xudong Zhang and
                  Zhu Han},
	title = {Effective Multi-Agent Communication Under Limited Bandwidth},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7771--7784},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339213},
	doi = {10.1109/TMC.2023.3339213},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuWQWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the fast development of multi-agent reinforcement learning, communication among agents has become a new research hotspot for its significant role in promoting the cooperation of automated devices. However, in real-world scenarios, agents such as unmanned vehicles and robots are likely to suffer from communication resource constraints, making designing efficient communication protocols essential. In this paper, we propose to quantize messages and reduce discrete entropy to achieve effective multi-agent communication under bandwidth limits. Achieving this goal requires solving two challenges: The first one is that the gradients of discrete entropy remain zero except for several discontinuous points wherein the gradients are undefined, making it hard to reduce discrete entropy via gradient-based training. To overcome it, we design Surrogate Entropy Minimization (SEM) scheme and confirm its effectiveness theoretically. The second challenge is maximizing cooperation performance under a given bandwidth limit. We model it as a constrained optimization problem and design Soft Barrier Method (SBM). Our proposed scheme is evaluated alongside four other methods in six environment settings and five different bandwidth limits, and demonstrates outstanding performance. Specifically, it manages to reduce bandwidth consumption by up to 90% with little or no loss of cooperation performance.}
}


@article{DBLP:journals/tmc/PulighedduACR24,
	author = {Corrado Puligheddu and
                  Jonathan D. Ashdown and
                  Carla{-}Fabiana Chiasserini and
                  Francesco Restuccia},
	title = {{SEM-O-RAN:} Semantic {O-RAN} Slicing for Mobile Edge Offloading of
                  Computer Vision Tasks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7785--7800},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339056},
	doi = {10.1109/TMC.2023.3339056},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PulighedduACR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of mobile networks (NextG) will require careful resource management to support edge offloading of resource-intensive deep learning (DL) tasks. Current slicing frameworks treat all DL tasks equally without adjusting to their high-level objectives, resulting in sub-optimal performance. To overcome this, we propose SEM-O-RAN, a semantic and flexible slicing framework for computer vision task offloading in NextG Open RANs. Our framework accounts for the semantic nature of object classes as well as the level of data quality to optimally tailor data compression and minimize the usage of networking and computing resources. In fact, we show that different object classes tolerate different levels of image compression while preserving detection accuracy. To address the above issues, we first present the mathematical formulation of the Semantic Flexible Edge Slicing Problem (SF-ESP), which turns out to be NP-hard. We thus define a greedy algorithm to solve it efficiently, which is also able to always select the resource allocation that yields the best resource utilization, whenever multiple allocations satisfy the DL task requirements. We evaluate SEM-O-RAN's performance through extensive numerical analysis and real-world experiments on the Colosseum testbed, considering state-of-the-art computer-vision tasks and DL models. The obtained results demonstrate that SEM-O-RAN allocates up to 169% more tasks and obtains 52% higher revenues than the state of the art.}
}


@article{DBLP:journals/tmc/HabibaMH24,
	author = {Ummy Habiba and
                  Setareh Maghsudi and
                  Ekram Hossain},
	title = {A Repeated Auction Model for Load-Aware Dynamic Resource Allocation
                  in Multi-Access Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7801--7817},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3338602},
	doi = {10.1109/TMC.2023.3338602},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HabibaMH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC) is one of the enabling technologies for high-performance computing at the edge of the 6 G networks, supporting high data rates and ultra-low service latency. Although MEC is a remedy to meet the growing demand for computation-intensive applications, the scarcity of resources at the MEC servers degrades its performance. Hence, effective resource management is essential; nevertheless, state-of-the-art research lacks efficient economic models to support the exponential growth of the MEC-enabled applications market. We focus on designing a MEC offloading service market based on a repeated auction model with multiple resource sellers (e.g., network operators and service providers) that compete to sell their computing resources to the offloading users. We design a computationally-efficient modified Generalized Second Price (GSP)-based algorithm that decides on pricing and resource allocation by considering the dynamic offloading requests arrival and the servers’ computational workloads. Besides, we propose adaptive best-response bidding strategies for the resource sellers, satisfying the symmetric Nash equilibrium (SNE) and individual rationality properties. Finally, via intensive numerical results1, we show the effectiveness of our proposed resource allocation mechanism.}
}


@article{DBLP:journals/tmc/RawlinsS24,
	author = {Charles C. Rawlins and
                  Jagannathan Sarangapani},
	title = {Predicting IoT Distributed Ledger Fraud Transactions With a Lightweight
                  {GAN} Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7818--7829},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339384},
	doi = {10.1109/TMC.2023.3339384},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/RawlinsS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decision-making and consensus in traditional blockchain protocols is formulated as a repeated Bernoulli trial that solves a computationally-intense lottery puzzle, called Proof-of-Work (PoW) in Bitcoin. This approach has shown robustness through practice, but does not scale with increasing network size and generation of new transactions. Resource constrained Internet of Things (IoT) networks are incompatible with full computation of schemes like Bitcoin's PoW. Our effort proposes a first step towards an alternative consensus using machine learning-based decision-making with prediction of fraud transactions to alleviate need for intense computation. To improve base approval probabilities for fraud detection in an ideal security setting, Vector GAN (VecGAN) is proposed to augment blockchain data in classifier training, which combines error-driven learning with Bayesian estimation to alleviate calculations. This two-step approach with augmentation and classification on new transactions is proposed as a novel approach to blockchain decision-making. Experimental prediction accuracy using VecGAN improved up to 3% on simplistic classifiers compared to other state-of-the-art augmentation techniques. Resource consumption in a realistic blockchain setting was reduced while improving block throughput by 50% compared to PoW. Future work will explore Sybil-spam defensive measures for realistic protocol implementation with this approach.}
}


@article{DBLP:journals/tmc/LiangSZPZM24,
	author = {Yumeng Liang and
                  Pu Shi and
                  Zixin Zheng and
                  Lingyu Pu and
                  Anfu Zhou and
                  Huadong Ma},
	title = {Mmtaster: {A} Mobile System for Fine-Grained and Robust Alcohol Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7830--7847},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3339141},
	doi = {10.1109/TMC.2023.3339141},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiangSZPZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensing offers a promising approach to identify the content of liquids without opening the container or directly touching the liquid. Although existing methods aim to achieve fine-grained identification, i.e., distinguishing a 1% v/v difference in alcohol content, they still have limitations in detecting highly deceptive counterfeit liquors that have much smaller content differences, sometimes as low as 0.2% v/v alcohol content. In this paper, we propose mmTaster, a mobile system that combines the mmWave radar with a smartphone to perform fine-grained and robust alcohol sensing. To achieve the desired fine granularity, we introduce a novel feature extraction model that exploits the unique reflection responses across multiple mmWave frequencies, which provide discriminative information about liquid content. Furthermore, we observe the serious interference of target displacement on identification performance, which hinders the various applications in mobile scenarios. To enhance the robustness, mmTaster incorporates a customized translation-invariant neural network, ConvNet, to remove the location interference and extract stable liquid-dependent features regardless of target displacement. Extensive experimental results demonstrate that mmTaster can accurately distinguish the alcohol differences as low as 0.2% v/v with an accuracy of over 90.8% even in scenarios involving diverse displacements and rotations.}
}


@article{DBLP:journals/tmc/WangMZCZZZ24,
	author = {Pei Wang and
                  Xujun Ma and
                  Rong Zheng and
                  Luan Chen and
                  Xiaolin Zhang and
                  Djamal Zeghlache and
                  Daqing Zhang},
	title = {SlpRoF: Improving the Temporal Coverage and Robustness of RF-Based
                  Vital Sign Monitoring During Sleep},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7848--7864},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3340925},
	doi = {10.1109/TMC.2023.3340925},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangMZCZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing RF-based vital sign monitoring systems either assume that a human subject is stationary or discard measurements when motion is detected in order to output reliable respiration rates and heart rates. Such an assumption greatly limits the usability of these systems in practice. Even during sleep, one can undergo various body states including turns and involuntary twitches in light sleep, motionlessness during deep sleep, or abnormal limb movements due to sleep disorders such as restless legs syndrome. In this work, we develop SlpRoF, a low-cost contact-free system using a commercial-off-the-shelf UWB radar that achieves high temporal coverage and high accuracy in vital sign monitoring during sleep. By classifying body states into the motionless state, limb movement state, and torso movement state, and extracting vital signs during the first two states, it directly increases effective reporting periods over nights. By analyzing high-order harmonics and leveraging spatial diversity in captured signals from multiple on-body areas, it improves the accuracy of heart rate estimations and thus indirectly increases temporal coverage through reliable assessments. Experiment results show that SlpRoF is able to achieve an average median absolute error (MAE) of 0.44 beats per minute (bpm) in respiration rates, 1.55 s in respiration intervals, and 0.9 bpm for heart rates, respectively.}
}


@article{DBLP:journals/tmc/NdikumanaNC24,
	author = {Anselme Ndikumana and
                  Kim Khoa Nguyen and
                  Mohamed Cheriet},
	title = {Age of Processing-Aware Offloading Decision for Autonomous Vehicles
                  in 5G Open {RAN} Environment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7865--7877},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3341082},
	doi = {10.1109/TMC.2023.3341082},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NdikumanaNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In state-of-the-art autonomous vehicles, data from the vehicle's sensors is often processed using fast and expensive onboard hardware. Such an onboard processing scheme quickly drains the vehicle's battery and consumes computing resources. Recent research proposed to offload parts of processing tasks onto cloud. However, offloading tasks to the cloud is challenging because of the low latency needed for reliable and safe autonomous driving decisions. To address this issue, we propose an Age of Processing (AoP)-aware offloading mechanism for autonomous vehicles. First, we develop a collaboration space of edge clouds to process data closely as possible to the vehicles. Second, we reveal a new communication planning model that allows the vehicle to find suitable open radio units available in route to offload tasks to edge clouds and reduce variation in offloading delay. Third, we formulate an optimization problem that minimizes AoP, i.e., elapsed time from generating tasks and getting computation results. Our AoP-based approach allows a status update to be available to the vehicle after computation. To solve the formulated non-convex problem, we apply dual decomposition and design an AoP-aware algorithm to compute the solution in near real-time. The results demonstrate that our approach meets computation deadlines while minimizing AoP.}
}


@article{DBLP:journals/tmc/ZhangWWCG24,
	author = {Yin Zhang and
                  Ranran Wang and
                  Yiran Wang and
                  Min Chen and
                  Mohsen Guizani},
	title = {Diversity-Driven Proactive Caching for Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7878--7894},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3340733},
	doi = {10.1109/TMC.2023.3340733},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Content caching in mobile networks is a highly promising technology for reducing traffic load latency and energy consumption levels. Its fundamental goal is to satisfy the supply-and-demand relationships between content providers and content-requesting users. However, previous research primarily focused on the optimization goals of mobile network operators, and although these caching strategies yield improved latency and energy consumption levels, they fall short of satisfying the diverse content demands of users in real-world scenarios. Therefore, this paper proposes a diversity-driven proactive caching strategy that considers multiple stakeholders’ requirements, in which the cache hit rate, cache gain for network operators, and content diversity are jointly optimized. Specifically, a novel improved Latent Dirichlet Allocation (LDA) is designed for radio access network caching, which enables diverse topic associations. For user device caching at the network edge, Gradient-Guided Contrastive Learning (GGCL) is proposed to optimize the multiple objectives of cache systems with limited labeled data resources. Finally, extensive experiments conducted on the MovieLens dataset demonstrate that the proposed method significantly outperforms other methods in various aspects, including content diversity, the cache hit rate, and some network performance metrics, such as the traffic load and cache gain.}
}


@article{DBLP:journals/tmc/XiaoZZLG24,
	author = {Yalong Xiao and
                  Junfeng Zhu and
                  Shigeng Zhang and
                  Xuan Liu and
                  Song Guo},
	title = {Fall-Attention: An Attention-Based Fall Detection Method for Adjoint
                  Activities},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7895--7909},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3344125},
	doi = {10.1109/TMC.2023.3344125},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoZZLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based wireless sensing has gained popularity for enabling smart indoor services, one of which is fall detection which plays a vital role in mitigating health risks for elders. Previous approaches have treated daily activities as independent events and built models to distinguish falls from others. However, human activities are usually adjoint in practice, e.g., the elder may suddenly fall when she walks. This adjoining introduces shared features between different activities, thereby affecting the classification performance. To address this problem, we propose Fall-attention, an attention-based fall detection method that can focus on the features related to fall events and suppress interference of irrelevant activities to improve performance. Its basic idea is to produce a task-oriented feature representation of fall events inside the signal using attention-based sentence embedding techniques and Recurrent Neural Network (RNN). We incorporate multi-task learning into Fall-attention by adopting multiple independent classification modules. This enables the model to explore different regions of the signal, capturing the composition of adjoint activities. A series of signal preprocessing and data enhancement techniques are also adopted to promote model training. Experimental results of the dataset containing adjoint activities demonstrate the superiority of Fall-attention over previous methods, which achieves an average accuracy of 95%.}
}


@article{DBLP:journals/tmc/NgLXNSM24,
	author = {Wei Chong Ng and
                  Wei Yang Bryan Lim and
                  Zehui Xiong and
                  Dusit Niyato and
                  Xuemin Sherman Shen and
                  Chunyan Miao},
	title = {Distributionally Robust Cost Minimized Edge Semantic Intelligence
                  in the Sustainable Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7910--7926},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343715},
	doi = {10.1109/TMC.2023.3343715},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/NgLXNSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the recent development of the Metaverse, people are more connected with each other. Avatars are used to represent the people, to communicate with one another, and they can build the community virtually. In these processes, a massive amount of data is exchanged between the physical and the virtual world. However, the existing communication technologies are insufficient to support the Metaverse, and the energy consumption of the Metaverse is huge. Therefore, semantic communication is one of the emerging communication paradigms to reduce the size of the data transmitted and reduce energy consumption while maintaining its meaning. Virtual service providers (VSPs) who provide services in the Metaverse can purchase semantic data from the nearby edge sensing units by using two subscription plans: reservation and on-demand. However, in practice, the demand of the VSPs is uncertain due to the variability of the Metaverse. To minimize the cost of the network and prevent over- and under-subscription of the resources, we propose a two-phase stochastic semantic resource allocation (SSRA) scheme. In phase one, a double dutch auction performs a one-to-one matching between VSPs and edge sensing units. The matching is dynamic and depends on the quality of experience (QoE) from the Metaverse users and the semantic data transmission cost from the edge sensing units. The matching changes whenever QoE and the semantic data transmission cost vary. In phase two, we consider the demand uncertainty and matching result from the phase one to formulate a distributed robust optimization (DRO) problem to minimize the operation cost of the VSPs. Using a real-world dataset, simulation results demonstrate that our proposed scheme is fully dynamic and minimizes the operation cost/energy consumption of VSPs in the presence of stochastic uncertainties.}
}


@article{DBLP:journals/tmc/AmiriRH24,
	author = {Mahdi Amiri and
                  Mohammad Hossein Rohban and
                  Shaahin Hessabi},
	title = {A Robust Heterogeneous Offloading Setup Using Adversarial Training},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7927--7938},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3346877},
	doi = {10.1109/TMC.2023.3346877},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AmiriRH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Neural Networks (DNNs) are very resource-demanding at inference time. Hence, one needs to be able to offload the model execution on the cloud as a solution. The problem is that we should use the same model on both resource-constrained devices and cloud sides. On the other hand, adversarial robustness is one of the main issues in many real-world applications, such as autonomous driving, where one desires model stability under imperceptible but adversarial input perturbations. However, adversarial training (AT) requires access to the actual model architecture and weights during the training. In our setup, two different deep models (suitable for each side) are broken into several blocks. Then, we select a combination of blocks to perform the computation according to the constraints in the inference time, and each block is executed on its respective side. Moreover, we propose a novel modified AT method that can virtually train all the mentioned blocks collectively. Rigorous evaluations of our method on CIFAR-10 and CIFAR-100 show that the proposed AT is effective in making the models robust under various offloading scenarios. Furthermore, we show that the more blocks of the large network are present in the selected model, the higher the final accuracy. To the best of our knowledge, our method is the first one, in which a heterogeneous offloading scheme under adversarial robustness is investigated.}
}


@article{DBLP:journals/tmc/ZhangLLJZZQ24,
	author = {Maojun Zhang and
                  Yang Li and
                  Dongzhu Liu and
                  Richeng Jin and
                  Guangxu Zhu and
                  Caijun Zhong and
                  Tony Q. S. Quek},
	title = {Joint Compression and Deadline Optimization for Wireless Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7939--7951},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3344108},
	doi = {10.1109/TMC.2023.3344108},
	timestamp = {Tue, 13 Aug 2024 07:52:14 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLLJZZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated edge learning (FEEL) is a popular distributed learning framework for privacy-preserving at the edge, in which densely distributed edge devices periodically exchange model-updates with the server to complete the global model training. Due to limited bandwidth and uncertain wireless environment, FEEL may impose heavy burden to the current communication system. In addition, under the common FEEL framework, the server needs to wait for the slowest device to complete the update uploading before starting the aggregation process, leading to the straggler issue that causes prolonged communication time. In this paper, we propose to accelerate FEEL from two aspects: i.e., 1) performing data compression on the edge devices and 2) setting a deadline on the edge server to exclude the straggler devices. However, undesired gradient compression errors and transmission outage are introduced by the aforementioned operations respectively, affecting the convergence of FEEL as well. In view of these practical issues, we formulate a training time minimization problem, with the compression ratio and deadline to be optimized. To this end, an asymptotically unbiased aggregation scheme is first proposed to ensure zero optimality gap after convergence, and the impact of compression error and transmission outage on the overall training time are quantified through convergence analysis. Then, the formulated problem is solved in an alternating manner, based on which, the novel joint compression and deadline optimization (JCDO) algorithm is derived. Numerical experiments for different use cases in FEEL including image classification and autonomous driving show that the proposed method is nearly 30X faster than the vanilla FedSGD algorithm, and outperforms the state-of-the-art schemes.}
}


@article{DBLP:journals/tmc/DOroBPM24,
	author = {Salvatore D'Oro and
                  Leonardo Bonati and
                  Michele Polese and
                  Tommaso Melodia},
	title = {OrchestRAN: Orchestrating Network Intelligence in the Open {RAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7952--7968},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342711},
	doi = {10.1109/TMC.2023.3342711},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DOroBPM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of cellular networks will be characterized by softwarized, open, and disaggregated architectures exposing analytics and control knobs to enable network intelligence via innovative data-driven algorithms. How to practically realize this vision, however, is largely an open problem. Specifically, for a given intent, it is still unclear how to select which data-driven models should be deployed and where, which parameters to control, and how to feed them appropriate inputs. In this article, we take a decisive step forward by presenting OrchestRAN, a network intelligence orchestration framework for next generation systems that embraces and builds upon the Open Radio Access Network (RAN) paradigm to provide a practical solution to these challenges. OrchestRAN has been designed to execute in the non-Real-time (RT) RAN Intelligent Controller (RIC) as an rApp and allows Network Operators (NOs) to specify high-level control/inference objectives (i.e., adapt scheduling, and forecast capacity in near-RT, e.g., for a set of base stations in Downtown New York). OrchestRAN automatically computes the optimal set of data-driven algorithms and their execution location (e.g., in the cloud, or at the edge) to achieve intents specified by the NOs while meeting the desired timing requirements and avoiding conflicts between different data-driven algorithms controlling the same parameters set. We show that the intelligence orchestration problem in Open RAN is NP-hard. To support real-world applications, we also propose three complexity reduction techniques to obtain low-complexity solutions that, when combined, can compute a solution in 0.1 s for large network instances. We prototype OrchestRAN and test it at scale on Colosseum, the world's largest wireless network emulator with hardware in the loop. Our experimental results on a network with 7 base stations and 42 users demonstrate that OrchestRAN is able to instantiate data-driven services on demand with minimal control overhead and latency.}
}


@article{DBLP:journals/tmc/ZhaoSLWZWK24,
	author = {Wei Zhao and
                  Ke Shi and
                  Zhi Liu and
                  Xuangou Wu and
                  Xiao Zheng and
                  Linna Wei and
                  Nei Kato},
	title = {{DRL} Connects Lyapunov in Delay and Stability Optimization for Offloading
                  Proactive Sensing Tasks of RSUs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7969--7982},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342102},
	doi = {10.1109/TMC.2023.3342102},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoSLWZWK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of Roadside Units (RSUs) is vital for the development of autonomous driving technologies. Challenges arise from sinking computing capabilities into RSUs and vehicles in the paradigm of Vehicle Edge Computing (VEC), particularly due to heterogeneous computation and communication capacities of network nodes and multiple sources of computing tasks (node-mounted and offloading tasks). These challenges complicate network stability from the perspective of a long-term optimization evolving over time, considering unpredictable task distribution and environmental states. To tackle these challenges, we approach the problem of partial task offloading to minimize task delay while meeting the demand of system stability over time as a dynamic long-term optimization. Utilizing Lyapunov stochastic optimization tools, we successfully decouple the long-term delay minimization and stability constraint, transforming it into a per-slot scheduling problem. Since the per-slot scheduling problem with complicated Lyapunov drift functions can not be solved by numerical optimization at each time step, our solution leverages a proposed deep reinforcement learning algorithm, leading to extensive simulations that demonstrate the superior effectiveness and efficiency of our proposal compared to existing schemes.}
}


@article{DBLP:journals/tmc/TuYHGHGW24,
	author = {Chunyu Tu and
                  Zhiyong Yu and
                  Lei Han and
                  Xianwei Guo and
                  Fangwan Huang and
                  Wenzhong Guo and
                  Leye Wang},
	title = {Adaptive Budgeting for Collaborative Multi-Task Data Collection in
                  Online Sparse Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7983--7998},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342206},
	doi = {10.1109/TMC.2023.3342206},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TuYHGHGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse crowdsensing collects data from a subset of the sensing area and infers data for unsensed areas, reducing data collection costs. Previous works have primarily focused on independently collecting and inferring single types of data. However, real-world scenarios often involve multiple types of data that can complement each other by providing missing spatiotemporal distribution information. In this paper, we fully consider both intra-data correlations among data of the same type and inter-data correlations among data of different types, enabling collaborative execution of various tasks. In addition, we enhance the adaptability in practical application scenarios by utilizing real-time collected sparse data to guide task execution. For this purpose, we propose a multi-task adaptive budgeting framework for online sparse crowdsensing, called MTAB-SC. This framework consists of three parts: training data updating, data inference, and data collection. First, we propose a multi-task data updating method to keep models up-to-date. Second, we design a data inference network for multi-task data joint inference. Finally, to allocate suitable budgets for each task and facilitate collaborative data collection across multiple tasks, we propose an Adaptive Budgeting for Collaborative Data Collection model (AB-CoDC). The effectiveness of our proposals is demonstrated through extensive experiments on two real-world datasets.}
}


@article{DBLP:journals/tmc/HuangLYMSCY24,
	author = {Yaodong Huang and
                  Zelin Lin and
                  Tingting Yao and
                  Changkang Mo and
                  Xiaojun Shang and
                  Laizhong Cui and
                  Yuanyuan Yang},
	title = {Mobility-Aware Seamless Virtual Function Migration in Deviceless Edge
                  Computing Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {7999--8014},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343969},
	doi = {10.1109/TMC.2023.3343969},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLYMSCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless Computing and Function-as-a-Service (FaaS) offer convenient and transparent services to developers and users. The deployment and resource allocation of services are managed by the cloud service providers. Meanwhile, the development of smart mobile devices and network technology enables the collection and transmission of a huge amount of data, which shifts tasks to the network edge for mobile users. In this paper, we propose a deviceless edge computing system targeting the mobility of end users using the data migration of virtual functions. We focus on the adjustment of migration among virtual functions to provide uninterrupted services to mobile users. We introduce the deviceless edge computing model and propose a seamless data migration scheme of virtual functions with limited involvement of function developers. We formulate the migration decision problem into integer linear programming and use receding horizon control (RHC) for online solutions. We implement the migration system to support delay-sensitive scenarios over real edge devices and develop a streaming game as the virtual function to test the performance. Extensive experiments in real scenarios exhibit the system has the ability to support high-mobility and delay-sensitive application scenarios. Extensive simulation results show the applicability of the proposed system over large-scale networks.}
}


@article{DBLP:journals/tmc/TaskouRH24,
	author = {Shiva Kazemi Taskou and
                  Mehdi Rasti and
                  Ekram Hossain},
	title = {End-to-End Resource Slicing for Coexistence of eMBB and {URLLC} Services
                  in 5G-Advanced/6G Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {7},
	pages = {8015--8032},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3341810},
	doi = {10.1109/TMC.2023.3341810},
	timestamp = {Tue, 18 Jun 2024 09:25:13 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TaskouRH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the problem of end-to-end (E2E) network slicing, i.e., joint slicing of the radio access network (RAN) and core network (CN), for the coexistence of enhanced mobile broadband (eMBB) and ultra-reliable and low latency communication (URLLC) services in future generation cellular (e.g., 5G-Advanced/6G) networks. The E2E resource slicing problem is defined as a mixed-integer non-linear programming problem to minimize the E2E energy consumption and the cost of utilized resources. To overcome the difficulty of solving this problem, we decompose it into two sub-problems, namely, RAN resource allocation (RRA) and CN resource allocation (CRA) problems. In both RRA and CRA problems, the existence of binary variables makes them intractable. To tackle this difficulty, we relax the binary variables by introducing penalty functions. Then, we make the RRA and CRA problems convex by employing the majorization-minimization approximation method. Via simulation results, we compare our proposed joint RAN and CN resource allocation algorithm (JRCRA) with the disjoint solution where RAN and CN resources are allocated to users separately. The joint allocation of resources in the RAN and CN has the advantage that the E2E tolerable latency of users can be flexibly divided between RAN and CN. In contrast, if resources in RAN and CN are allocated separately, a predefined part of the E2E tolerable latency should be considered as the tolerable latency in RAN and CN. The simulation results illustrate that our proposed JRCRA algorithm obtains a 34% improvement in energy consumption and a 24% improvement in cost compared to the disjoint one. Moreover, via simulation results, we illustrate that in comparison with existing algorithms, our proposed JRCRA obtains a higher performance. Besides, simulation results confirm that JRCRA reaches a close performance to the optimal solution.}
}


@article{DBLP:journals/tmc/MittalTH24,
	author = {Vandana Mittal and
                  Hina Tabassum and
                  Ekram Hossain},
	title = {Deployment Cost-Aware {UAV} and {BS} Collaboration in Cell-Free Integrated
                  Aerial-Terrestrial Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8033--8049},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3341809},
	doi = {10.1109/TMC.2023.3341809},
	timestamp = {Tue, 06 Aug 2024 17:20:55 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MittalTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To enable massive connectivity and connecting the unconnected, aerial communications are becoming critical to complement with the terrestrial infrastructure. Integrated aerial-terrestrial network (IATN) offers both line-of-sight (LoS) and non-LoS (NLoS) connectivity and deployment flexibility. This paper presents a framework to optimize the deployment of aerial network and cooperation among aerial-terrestrial network such that the network deployment cost efficiency (i.e. the ratio of network sum-rate and deployment-plus-energy-cost) is maximized. The cooperation among unmanned aerial vehicles (UAVs) and terrestrial base-station (BSs) is supported with clustered cell-free massive MIMO (C-CF-M-MIMO). Specifically, we first formulate a Deployment Cost Efficiency (DCE) maximization problem subject to power budget, zero intra-cell pilot contamination, and UAV location constraints. We then propose a grid-based joint UAV density and location optimization, a pilot-contamination aware user clustering, and a distributed coalition game approach for clustering in C-CF-M-MIMO-enabled IATN. Complexity and convergence of the proposed algorithm are presented. Our numerical results show the efficacy of the proposed algorithm compared to conventional benchmarks. The proposed C-CF-M-MIMO-enabled IATN also outperforms terrestrial-only and aerial-only networks enabled with typical cell-free configurations, namely, (i) traditional CF-MIMO, and (ii) user-centric CF-MIMO.}
}


@article{DBLP:journals/tmc/WangLZCWZPQDY24,
	author = {Shaowei Wang and
                  Yuntong Li and
                  Yusen Zhong and
                  Kongyang Chen and
                  Xianmin Wang and
                  Zhili Zhou and
                  Fei Peng and
                  Yuqiu Qian and
                  Jiachun Du and
                  Wei Yang},
	title = {Locally Private Set-Valued Data Analyses: Distribution and Heavy Hitters
                  Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8050--8065},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342056},
	doi = {10.1109/TMC.2023.3342056},
	timestamp = {Mon, 11 Nov 2024 08:03:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLZCWZPQDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many mobile applications, user-generated data are presented as set-valued data. To tackle potential privacy threats in analyzing these valuable data, local differential privacy has been attracting substantial attention. However, existing approaches only provide sub-optimal utility and are expensive in computation and communication for set-valued data distribution estimation and heavy-hitter identification. In this paper, we propose a utility-optimal and efficient set-valued data publication method (i.e., Wheel mechanism). On the user side, the computational complexity is only\nO(min{mlogm,m\ne\nϵ\n})\nand communication costs are\nO(ϵ+logm)\nbits, where\nm\nis the number of items,\nd\nis the domain size and\nϵ\nis the privacy budget, while existing approaches usually depend on\nO(d)\nor\nO(logd)\n(\nd≫m\n). Our theoretical analyses reveal the estimation errors have been reduced from the previously known\nO(\nm\n2\nd\nn\nϵ\n2\n)\nto the optimal rate\nO(\nmd\nn\nϵ\n2\n)\n. Additionally, for heavy-hitter identification, we present a variant of the Wheel mechanism as an efficient frequency oracle, entailing only\nO(\nn\n−\n−\n√\n)\ncomputational complexity. This heavy-hitter protocol achieves an identification bar of\nO\n~\n(\n1\nϵ\nm\nn\nlogd\n−\n−\n−\n−\n−\n−\n√\n)\n, reducing by a factor of\nm\n−\n−\n√\nrelative to existing protocols. Extensive experiments demonstrate our methods are 3-100x faster than existing approaches and have optimized statistical efficiency.}
}


@article{DBLP:journals/tmc/WeiQLGC24,
	author = {Kaimin Wei and
                  Guozi Qi and
                  Zhetao Li and
                  Song Guo and
                  Jinpeng Chen},
	title = {Group Task Recommendation in Mobile Crowdsensing: An Attention-Based
                  Neural Collaborative Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8066--8076},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3345865},
	doi = {10.1109/TMC.2023.3345865},
	timestamp = {Fri, 14 Feb 2025 15:01:46 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WeiQLGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative tasks often require the cooperation of multiple individuals to be completed in mobile crowdsensing (MCS). However, previous task recommendations predominantly focused on individuals rather than groups, making them less effective for collaborative tasks. It is crucial to study the collaborative task recommendation problem in MCS. In this work, we propose an Attention-based Neural Collaborative approach (ANC) for group task recommendation. In particular, a grouping method is designed based on participant abilities to form groups that meet the needs of collaborative tasks. Meanwhile, a dual-attention mechanism is constructed to aggregate member preferences and enhance the representation of tasks and groups. The neural network-based collaborative filter mechanism is employed to generate top-K recommendation lists. Experimental results, based on two real-world datasets, demonstrate that ANC outperforms others, validating its effectiveness and feasibility.}
}


@article{DBLP:journals/tmc/LiSHLWLWTYZCWY24,
	author = {Yang Li and
                  Fanglei Sun and
                  Jingchen Hu and
                  Chang Liu and
                  Fan Wu and
                  Kai Li and
                  Ying Wen and
                  Zheng Tian and
                  Yaodong Yang and
                  Jiangcheng Zhu and
                  Zhifeng Chen and
                  Jun Wang and
                  Yang Yang},
	title = {Self-Supervised {MAFENN} for Classifying Low-Labeled Distorted Images
                  Over Mobile Fading Channels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8077--8091},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343939},
	doi = {10.1109/TMC.2023.3343939},
	timestamp = {Mon, 30 Sep 2024 07:53:47 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiSHLWLWTYZCWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Image distortion during wireless transmission presents a significant challenge for real-world artificial intelligence (AI) applications. Recent methods have attempted to address this issue by integrating neural networks into the wireless transmission system. However, these approaches often require a large volume of labeled training data, which can be expensive and time-consuming to collect. To address this issue, we propose a novel approach, Self-Supervised Multi-Agent Feedback Enabled Neural Networks (S2MAFENN). S2MAFENN is designed to improve the efficiency of labeled data in wireless image transmission. It incorporates a Feedbacker agent that emulates the error correction mechanisms observed in primate brains and employs self-supervised contrastive learning to extract representations from unlabeled distorted images independently. From a theoretical perspective, we model the training process of S2MAFENN as a three-player Stackelberg game and provide evidence that S2MAFENN can achieve exponential convergence rates. We then empirically validate our approach by assessing the representations learned through S2MAFENN. We use varied labeled CIFAR10 and CIFAR100 data to simulate real image transmissions over the Rayleigh fading and 5G channels. Our results show that S2MAFENN matches or even surpasses the performance of state-of-the-art self-supervised training methods, even when only 50% of labels are used. Moreover, S2MAFENN yields average accuracy gains of 5.11%, 5.8%, and 4.58% with only 0.1, 0.2, and 0.5 of the labels transmitted over the 5G channel, respectively. For the downstream task of semantic segmentation over the 5G channel, S2MAFENN exhibits significant advancements on the ADE20K dataset. It achieves enhancements of approximately 7% and 8.7% in Mean IoU and DICE metrics, respectively, surpassing the performance of current state-of-the-art methods.}
}


@article{DBLP:journals/tmc/Fan24,
	author = {Wenhao Fan},
	title = {Blockchain-Secured Task Offloading and Resource Allocation for Cloud-Edge-End
                  Cooperative Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8092--8110},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342817},
	doi = {10.1109/TMC.2023.3342817},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/Fan24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enhanced by blockchain and cloud-edge-end cooperation, an edge computing network is capable to provide IoT (Internet of Things) devices higher task processing performance and better security and privacy guarantee. However, the joint resource management for both the task offloading and the blockchain services was less fully studied by existing works. To this end, in this paper, we focus on the task processing delay and energy consumption optimization problem in a multi-device and multi-base-station cloud-edge-end cooperative network. The task offloading, transmit power allocation, transmission rate allocation, and computing resource allocation are jointly optimized to minimize the long-term average total task processing delay of the tasks of all the devices while keeping the stability of the energy consumption of the devices and guaranteeing that the block mining speed matches the task offloading processes. We transform the optimization problem based on the Lyapunov optimization theory, and then design a hybrid deep reinforcement learning (DRL)-based algorithm. We decompose the problem into multiple sub-problems, and then embed multiple fast numerical methods into the twin delayed deep deterministic policy gradient (TD3) architecture as optimization subroutines to improve the learning performance of the DRL model. We also design a distributed deployment scheme for the algorithm and analyze the algorithm complexity. We demonstrate the superior performance of our algorithm in comparison with 5 reference schemes via extensive experiments in 7 scenarios.}
}


@article{DBLP:journals/tmc/ZhaoZJCXC24,
	author = {Kongyange Zhao and
                  Zhi Zhou and
                  Lei Jiao and
                  Shen Cai and
                  Fei Xu and
                  Xu Chen},
	title = {Taming Serverless Cold Start of Cloud Model Inference With Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8111--8128},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348165},
	doi = {10.1109/TMC.2023.3348165},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoZJCXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless computing is envisioned as the de-facto standard for next-generation cloud computing. However, the cold start dilemma has impeded its adoption by delay-sensitive and burst applications. In this paper, we propose to tame serverless cold start in a cloud inference system with edge computing. Specifically, the proposed solution smooths the serverless cloud workload with user-owned edge computing, reducing the number of cold starts. Leveraging the configurability of requests and serverless functions, the proposed solution further reduces the transmission latency and serverless cost by adapting request configuration (e.g., image resolution) and function configuration (e.g., memory). To alleviate the potential inference accuracy degradation incurred by configuration adaption, we aim to strike a nice balance between inference latency, cost, and accuracy. However, achieving this goal is non-trivial since the underlying optimization is non-convex and involves future uncertain information. To simultaneously address dual challenges, the presented cold-start-aware online algorithms apply the regularization technique to decompose the problem into separate convex subproblems. Then, it applies lazy switching to smooth the number of provisioned functions and thus reduces the cold start. Through rigorous theoretical analysis, realistic prototype evaluations on AWS Lambda, and trace-driven simulations, we comprehensively validate the theoretical and empirical performance of our proposed solution.}
}


@article{DBLP:journals/tmc/ChenLWHZ24,
	author = {Ying Chen and
                  Kaixin Li and
                  Yuan Wu and
                  Jiwei Huang and
                  Lian Zhao},
	title = {Energy Efficient Task Offloading and Resource Allocation in Air-Ground
                  Integrated {MEC} Systems: {A} Distributed Online Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8129--8142},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3346431},
	doi = {10.1109/TMC.2023.3346431},
	timestamp = {Thu, 12 Sep 2024 20:54:23 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLWHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many remote areas lacking ground communication infrastructure support, such as wilderness, desert, ocean, etc., an integrated edge computing network in the air with edge computing nodes is an effective solution. It can provide over-the-air computing services for ground devices (GDs) with limited computing resources and battery life. In this paper, we study task offloading and resource allocation in the aerial-based mobile edge computing (MEC) system supported by a high altitude platform (HAP) and unmanned aerial vehicles (UAVs), with the goal of minimizing the GD's energy consumption. Considering that the task arrival of GDs and wireless communication quality are both stochastic and dynamic, we apply stochastic optimization techniques to transform this task offloading and resource allocation problem into two subproblems, i.e., 1) a subproblem for local computation resource allocation and 2) a subproblem for offloading resource allocation. For the first subproblem, we use convex optimization methods to address it. For the second subproblem, we use game theory to formulate the competition of offloading resources among GDs and propose the Distributed Game-theoretical Multi-server Selection (DGMS) algorithm and the Transmission Power Allocation (TPA) algorithm. Finally, we propose a Distributed Online Task Offloading and Resource Allocation (DOTORA) algorithm and give the theoretical performance analysis of the algorithm. We perform extensive experiments, including the comparison experiments with the UAV-Only and HAP-Only framework, and the comparison experiments with other algorithms under our HAP-UAV framework. The experimental results validate our proposed framework and the DOTORA algorithm.}
}


@article{DBLP:journals/tmc/XueYZZZS24,
	author = {Jianzhe Xue and
                  Kai Yu and
                  Tianqi Zhang and
                  Haibo Zhou and
                  Lian Zhao and
                  Xuemin Shen},
	title = {Cooperative Deep Reinforcement Learning Enabled Power Allocation for
                  Packet Duplication {URLLC} in Multi-Connectivity Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8143--8157},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3347580},
	doi = {10.1109/TMC.2023.3347580},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XueYZZZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra reliable low latency communication (URLLC) in vehicular networks is crucial for safety-related vehicular applications. Mini-slot with a short packet that carries only a few symbols is used to reduce the transmission time interval and enable quick scheduling for URLLC that requires extremely low latency. However, a single air interface transmission of URLLC packets may fail due to the high mobility of vehicles. Leveraging multi-connectivity technologies, the real-time reliability of URLLC can be greatly enhanced without relying on packet retransmission. In this paper, we propose a multi-connectivity URLLC downlink transmission scheme for vehicular networks, where the URLLC packet is duplicated and transmitted over multiple independent wireless links to improve packet reliability. Specifically, we design a multi-agent cooperative deep reinforcement learning algorithm, called transformer associated proximal policy optimization (TAPPO), to achieve real-time robust power allocation for multi-connectivity URLLC with imperfect channel state information (CSI). The transformer neural network architecture is employed to share the information among multiple links serving the same URLLC user and choose appropriate transmit powers, enabling cooperation to ensure reliability while minimizing inter-cell interference and energy consumption. Extensive simulation results validate the effectiveness of multi-connectivity packet duplication for URLLC and proposed TAPPO for power allocation.}
}


@article{DBLP:journals/tmc/ZhaoLZLWHG24,
	author = {Liang Zhao and
                  Tianyu Li and
                  Enchao Zhang and
                  Yun Lin and
                  Shaohua Wan and
                  Ammar Hawbani and
                  Mohsen Guizani},
	title = {Adaptive Swarm Intelligent Offloading Based on Digital Twin-assisted
                  Prediction in {VEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8158--8174},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3344645},
	doi = {10.1109/TMC.2023.3344645},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLZLWHG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular Edge Computing (VEC) is the transportation version of Mobile Edge Computing (MEC). In VEC, task offloading enables vehicles to offload computing tasks to nearby Roadside Units (RSUs), thereby reducing the computation cost. Recent trends in task offloading cause a proliferation of studies in academia. However, the existing offloading schemes still face many challenges, such as high-dynamic network topology, massive and complex data, dynamic scenes with high-speed vehicles and low-latency requirements. Digital Twin (DT)-based VEC is emerging as a promising solution. It monitors the state of the VEC network in real time through mappings and interactions between the physical and virtual entities. Consequently, the task offloading scheme can make more reasonable offloading decisions at the physical layer and further improve the efficiency of VEC. Above all, we propose a VEC computing offloading scheme, namely, Adaptive Swarm Intelligent Offloading Scheme Based on Digital-Twin-Assisted PRediction In VEC (STRIVE). The VEC network architecture is established to combines DT with an improved Generative Adversarial Network (GAN). The powerful prediction ability of GAN is used to assist in constructing DT in the pre-processing phase, reducing the size of the decision space. To adapt to the dynamic nature of VEC, we establish an adaptive model to adjust the real-time parameter under various scenarios. Then, we deploy an improveD genetIc simulatEd annealing-baSEd particLe swarm optimization (DIESEL) algorithm to task offloading decision-making, which can provide reliable computing services for vehicles at a lower cost. The simulation results demonstrate that the proposed scheme can effectively reduce computing delay and energy consumption compared with its counterparts.}
}


@article{DBLP:journals/tmc/WangGDWHZH24,
	author = {Shuai Wang and
                  Baoshen Guo and
                  Yi Ding and
                  Guang Wang and
                  Suining He and
                  Desheng Zhang and
                  Tian He},
	title = {Time-Constrained Actor-Critic Reinforcement Learning for Concurrent
                  Order Dispatch in On-Demand Delivery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8175--8192},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3342815},
	doi = {10.1109/TMC.2023.3342815},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangGDWHZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On-demand delivery has experienced rapid growth in recent years, revolutionizing people's lifestyles with its timeliness and convenience. The order dispatch process in on-demand delivery is concurrent, wherein couriers continuously accept new orders and deliver them to customers within strict time constraints and dynamic demand and supply. Most of the existing order dispatch mechanisms are designed for independent dispatch or concurrent dispatch without strict deadlines, rendering them unsuitable for real-time concurrent dispatch in on-demand delivery. To address the challenge, we propose a Time-Constrained Actor-Critic Reinforcement learning based concurrent dispatch system called TCAC-Dispatch to reduce the overdue rate and enhance the long-term revenue. Specifically, we first design a deep matching network (DMN) with a variable action space, which integrates both states embedding (including route behaviors encoding) and actions’ embedding into a long-term value for dispatching decisions. Additionally, we design a time-constrained action pruning module to ensure compliance with time constraints. Then we utilize the Actor-Critic framework to tackle the concurrent dispatch considering strict time constraints and stochastic demand-supply. To further optimize the efficiency and delivery resource utilization, we propose an extension of TCAC (i.e., TCAC+), which consists of (i) a learning-based order service time prediction module to determine whether to relax the deadline of some orders; and (ii) a multi-critic framework to optimize concurrent order dispatch with both tight deadlines and relaxed deadlines using dynamic weighting mechanism. We evaluate the TCAC-Dispatch with one-month data involved with 36.48 million orders and 42,000 couriers collected from Eleme, one of the largest on-demand delivery companies in China. Experiments are conducted on a data-driven emulator deployed on the development environment of Eleme and the results demonstrate that our method outperforms state-of-the-art baselines with various metrics in both tight deadline and mixed deadline scenarios.}
}


@article{DBLP:journals/tmc/YeQZS24,
	author = {Xuehan Ye and
                  Kaige Qu and
                  Weihua Zhuang and
                  Xuemin Shen},
	title = {Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous
                  Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8193--8207},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343709},
	doi = {10.1109/TMC.2023.3343709},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YeQZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To maintain high perception performance among connected and autonomous vehicles (CAVs), in this paper, we propose an accuracy-aware and resource-efficient raw-level cooperative sensing and computing scheme among CAVs and road-side infrastructure. The scheme enables fined-grained partial raw sensing data selection, transmission, fusion, and processing in per-object granularity, by exploiting the parallelism among object classification subtasks associated with each object. A supervised learning model is trained to capture the relationship between the object classification accuracy and the data quality of selected object sensing data, facilitating accuracy-aware sensing data selection. We formulate an optimization problem for joint sensing data selection, subtask placement and resource allocation among multiple object classification subtasks, to minimize the total resource cost while satisfying the delay and accuracy requirements. A genetic algorithm based iterative solution is proposed for the optimization problem. Simulation results demonstrate the accuracy awareness and resource efficiency achieved by the proposed cooperative sensing and computing scheme, in comparison with benchmark solutions.}
}


@article{DBLP:journals/tmc/ShiZWCCLL24,
	author = {Xiaohang Shi and
                  Sheng Zhang and
                  Jie Wu and
                  Ning Chen and
                  Ke Cheng and
                  Yu Liang and
                  Sanglu Lu},
	title = {AdaPyramid: Adaptive Pyramid for Accelerating High-Resolution Object
                  Detection on Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8208--8224},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343448},
	doi = {10.1109/TMC.2023.3343448},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShiZWCCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep convolutional neural network (NN)-based object detectors are not appropriate for straightforward inference on high-resolution videos at edge devices, as maintaining high accuracy often brings about prohibitively long latency. Although existing solutions have attempted to reduce on-device inference latency by selecting a cheaper configuration (e.g., choosing a more lightweight NN or scaling a frame to a smaller size before inference) or eliminating a background containing no object, they often ignore various high-resolution features and fail to optimize for those videos. We thus present AdaPyramid, a framework to reduce as much on-device inference latency as possible, especially for high-resolution videos, while achieving the accuracy demand approximately. We observe that the cheapest configuration to achieve the accuracy demand varies significantly across both different frames and different regions in a frame. The underlying reason is that object features (e.g., the location, size and category of objects) are more uneven in high-resolution videos, both temporally and spatially. Moreover, we observe that the object size presents a prominent hierarchical distribution in high-resolution frames. AdaPyramid thus partitions each frame hierarchically just like a pyramid and chooses a content-aware configuration for each region, which is adapted online based on the feedback. We evaluate the performance of AdaPyramid on a public dataset and our collected real-world videos. The obtained results show that under comparable accuracy to the state-of-the-art solutions, AdaPyramid can decrease inference latency by 40% on average, with up to 2.5× speed-up.}
}


@article{DBLP:journals/tmc/FanLYLL24,
	author = {Wenhao Fan and
                  Xun Liu and
                  Hao Yuan and
                  Nan Li and
                  Yuan'an Liu},
	title = {Time-Slotted Task Offloading and Resource Allocation for Cloud-Edge-End
                  Cooperative Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8225--8241},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3349551},
	doi = {10.1109/TMC.2024.3349551},
	timestamp = {Sun, 04 Aug 2024 19:47:57 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FanLYLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In time-slotted edge computing systems, task scheduling is conducted at the end of each time slot to make task offloading decisions and resource allocation for all the tasks pending for scheduling during the time slot. However, the existing works omitted the task scheduling delay, which is a period that a task has to wait from the task generation time point to the end of the current time slot. Such simplification is impractical in real scenarios because the task scheduling delay is a non-negligible part of the task processing delay, which was understood by existing works as the sum of only the task transmission and computing delays. In this paper, a novel time-slotted task offloading and resource allocation scheme for cloud-edge-end cooperative computing networks is proposed to realize the total task processing delay minimization for all the devices under the energy consumption constraint of each device. Our scheme makes task-offloading decision for each device from local processing, offloading to its affiliated base station (BS), to another BS, and to the cloud server. Besides, transmit power allocation, transmission rate allocation, and computing resource allocation are also jointly optimized in our optimization problem. We consider the impact of the task scheduling delay and design a two-stage distributed algorithm to decrease the negative impact by dividing the algorithm into a device-side part and a network-side part. The advantages of our scheme are validated by extensive simulations, where 4 reference schemes are compared in 8 different scenarios.}
}


@article{DBLP:journals/tmc/HuGG24,
	author = {Rui Hu and
                  Yuanxiong Guo and
                  Yanmin Gong},
	title = {Federated Learning With Sparsified Model Perturbation: Improving Accuracy
                  Under Client-Level Differential Privacy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8242--8255},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3343288},
	doi = {10.1109/TMC.2023.3343288},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) that enables edge devices to collaboratively learn a shared model while keeping their training data locally has received great attention recently and can protect privacy in comparison with the traditional centralized learning paradigm. However, sensitive information about the training data can still be inferred from model parameters shared in FL. Differential privacy (DP) is the state-of-the-art technique to defend against those attacks. The key challenge to achieving DP in FL lies in the adverse impact of DP noise on model accuracy, particularly for deep learning models with large numbers of parameters. This paper develops a novel differentially-private FL scheme named Fed-SMP that provides a client-level DP guarantee while maintaining high model accuracy. To mitigate the impact of privacy protection on model accuracy, Fed-SMP leverages a new technique called Sparsified Model Perturbation (SMP) where local models are sparsified first before being perturbed by Gaussian noise. We provide a tight end-to-end privacy analysis for Fed-SMP using Rényi DP and prove the convergence of Fed-SMP with both unbiased and biased sparsifications. Extensive experiments on real-world datasets are conducted to demonstrate the effectiveness of Fed-SMP in improving model accuracy with the same DP guarantee and saving communication cost simultaneously.}
}


@article{DBLP:journals/tmc/YuZMDCRZ24,
	author = {Le Yu and
                  Shufan Zhang and
                  Yan Meng and
                  Suguo Du and
                  Yuling Chen and
                  Yanli Ren and
                  Haojin Zhu},
	title = {Privacy-Preserving Location-Based Advertising via Longitudinal Geo-Indistinguishability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8256--8273},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348136},
	doi = {10.1109/TMC.2023.3348136},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YuZMDCRZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As location data have been increasingly adopted in location-based advertising (LBA), revealing locations to untrusted service providers has raised severe privacy concerns. Recent studies propose obfuscation mechanisms built upon geo-indistinguishability (geo-IND) to provide formal privacy guarantee. Unfortunately, due to the high degree of spatiotemporal regularity in human mobility pattern, the privacy cost will be unacceptably high in this situation, leading to accurate inference of user real locations. In this study, we identify this privacy risk in LBA scenarios under long-term and multi-platform assumption. We demonstrate an attacker can infer 75\\%\\sim 90\\%\nof top-1 locations within a range of only 200 meters. To address it, we propose PrivLocAd, a novel system which can provide longitudinal privacy guarantee. The novelty of PrivLocAd stems from a novel surrogate-based obfuscation, which generates multiple surrogate locations to improve the privacy-utility trade-off. In addition, two novel obfuscation mechanisms, the two-stage Gaussian and multi-level surrogate generation mechanism in charge of surrogate generation can achieve the longitudinal privacy guarantee in intra- and inter-platform condition respectively. Our experimental results demonstrate PrivLocAd is able to defend against the attack, which reduces the inference rate to less than 1% of user top-1 locations in the 200 m range.}
}


@article{DBLP:journals/tmc/XuQLCZLGZ24,
	author = {Ziheng Xu and
                  Wei Quan and
                  Mingyuan Liu and
                  Nan Cheng and
                  Xue Zhang and
                  Yan Luo and
                  Deyun Gao and
                  Hongke Zhang},
	title = {{DOFMS:} DRL-Based Out-of-Order Friendly Multipath Scheduling in Mobile
                  Heterogeneous Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8274--8288},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3346480},
	doi = {10.1109/TMC.2023.3346480},
	timestamp = {Wed, 13 Nov 2024 12:48:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuQLCZLGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath transmission brings strong bandwidth aggregation capability for services in wireless networks. Nonetheless, the heterogeneous nature of paths and the motion of terminals results in varying transmission delays, leading to out-of-order (OFO) delivery and transmission quality decrease. Traditional algorithms, due to their solid operating logic, fail to strike a balance between high bandwidth and low OFO extent. Recent studies have focused on utilizing learning algorithms to find a multi-metric joint optimal transmission strategy. In light of this, this paper proposes a framework called DRL-based OFO-Friendly Multipath Scheduling (DOFMS) to ensure high bandwidth and low OFO extent transmission in mobile heterogeneous networks. In particular, the framework introduces a novel OFO evaluation index to assess the degree of OFO more accurately. To achieve elastic scheduling, the framework employs the Double Deep Q Network (DDQN) to dynamically regulate the scheduling ratio. Recognizing the dynamic and unpredictable nature of path delays, an asynchronous module is introduced to enhance learning accuracy. Experimental results demonstrate that the DOFMS reduces the OFO rate by 25% compared to traditional bandwidth aggregation algorithms, while maintaining low bandwidth and packet loss rates. Furthermore, compared to conventional OFO avoidance algorithms, the DOFMS improves bandwidth by 4% and reduces fluctuation by 90%.}
}


@article{DBLP:journals/tmc/YaoHSXZW24,
	author = {Junmei Yao and
                  Haolang Huang and
                  Jiongkun Su and
                  Ruitao Xie and
                  Xiaolong Zheng and
                  Kaishun Wu},
	title = {Enabling Cross-Technology Coexistence for ZigBee Devices Through Payload
                  Encoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8289--8306},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3345830},
	doi = {10.1109/TMC.2023.3345830},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YaoHSXZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of Internet of Things, the number of heterogeneous wireless devices working in the same frequency band increases dramatically, leading to severe cross-technology interference. To enable coexistence, researchers have proposed a large number of mechanisms to manage interference. However, existing mechanisms have severe modifications in either the physical or MAC (medium access control) layers, making them very different from the standard. In this paper, we design and implement SledZig to boost cross-technology coexistence for low-power devices through both enabling more transmission opportunities and avoiding interference. SledZig is fully compatible with the standard in both physical and MAC layers. It decreases the WiFi signal power on the channel of low-power devices while keeps the WiFi transmission power unchanged, through making constellation points on the overlapped subcarriers have the lowest power, which can be achieved by just encoding the WiFi payload. We implement SledZig on hardware testbed and evaluate its performance under different settings. Experiment results show that SledZig can effectively increase ZigBee transmissions and improve its performance over a WiFi channel under various WiFi data traffic, with as low as 6.94\\%\nWiFi throughput loss.}
}


@article{DBLP:journals/tmc/HeWQYLLS24,
	author = {Bo He and
                  Jingyu Wang and
                  Qi Qi and
                  Qiang Ye and
                  Qihao Li and
                  Jianxin Liao and
                  Xuemin Shen},
	title = {ShuttleBus: Dense Packet Assembling With {QUIC} Stream Multiplexing
                  for Massive IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8307--8322},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3345898},
	doi = {10.1109/TMC.2023.3345898},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeWQYLLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate dense short packet forwarding for clustering-based massive Internet-of-Things (mIoT). The objective is to support the data forwarding with minimal communication overhead while satisfying the differentiated latency constraints from the transport layer perspective. To this end, we propose a dense packet assembling scheme, named ShuttleBus, for forwarding devices in mIoT to achieve effective data merging. The assembling scheme is designed based on the stream multiplexing mechanism of the Quick UDP Internet Connection (QUIC) protocol. With ShuttleBus, the payload data sent from IoT devices are extracted as independent frames belonging to different data streams. The ShuttleBus can bundle data frames from multiple streams into a single packet while ensuring data integrity of these streams. Furthermore, we develop a resilient packing mechanism in packet assembling to merge data received from IoT devices within a cluster. In addition, a latency-oriented scheduling mechanism for backlogged QUIC data is established to guarantee satisfactory delivery of diverse transmission tasks. To accommodate the dynamic network environment, we tailor a learning-based algorithm to determine the optimal packet assembling time adaptively. We evaluate the performance of ShuttleBus under various network load conditions. Both analytical and experimental results demonstrate that the proposed scheme significantly reduces communication overhead and enhances data delivery performance under stringent latency constraints.}
}


@article{DBLP:journals/tmc/HouGLYZD24,
	author = {Xindi Hou and
                  Shuai Gao and
                  Ningchun Liu and
                  Fangtao Yao and
                  Hongke Zhang and
                  Sajal K. Das},
	title = {L3Geocast: Enabling P4-Based Customizable Network-Layer Geocast at
                  the Network Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8323--8340},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3345933},
	doi = {10.1109/TMC.2023.3345933},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HouGLYZD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geocast is a one-to-many communication paradigm that enables the transmission of data packets to a designated area rather than an IP address. The most common geocast solutions rely on the application-layer Geolocation-to-IP database. But these IP-based approaches cannot cope with the challenges of flexibility and mobility in a granularity-customizable geocast scenario. While some non-IP network-layer (L3) attempts have resulted in low addressing accuracy and poor routing scalability. Besides, the clean-slate design is incompatible with the existing network. To address these issues, this article proposes an innovative network-layer geographic addressing scheme that leverages P4-based Software Defined Networks (SDN) to enable flexible geocast with high accuracy. Based on the aggregation relationship of the geographic area, a network-layer routing strategy is designed to enhance routing scalability. Compatibility is improved by deploying the network-layer designs only at the network edge where granularity-customizable geocast is implemented, without requiring changes to the current IP infrastructure. Then, the network-layer functions are integrated with an application-layer mapping service to support intercommunication between different network edges. Furthermore, a prototype system is built to implement and evaluate the proposed L3Geocast, which outperforms the existing approaches in terms of communication latency and mapping overhead.}
}


@article{DBLP:journals/tmc/HuangDYLWW24,
	author = {Haojun Huang and
                  Fengxiang Ding and
                  Hao Yin and
                  Gaoyang Liu and
                  Chen Wang and
                  Dapeng Oliver Wu},
	title = {EgoMUIL: Enhancing Spatio-Temporal User Identity Linkage in Location-Based
                  Social Networks With Ego-Mo Hypergraph},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8341--8354},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3345312},
	doi = {10.1109/TMC.2023.3345312},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangDYLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Users tend to own multiple accounts on different location-based social network (LBSN) platforms, and they typically engage with diverse social circles on each platform within the same locations. Consequently, linking these accounts across separate networks becomes essential, playing a critical role in information fusion. Previous works accomplishing user identity linkage (UIL) utilize individual mobility records, which are significantly affected by the issue of data scarcity. In this paper, we propose EgoMUIL, a heterogeneous graph embedding approach specifically devised for information propagation, aiming to alleviate the scarcity problem to some extent. Considering that follow relations of respective networks also hold great significance for the UIL task, we are inspired to enrich individual limited mobility records through follow relations. Our preliminary research reveals that direct common follow relations are quite insufficient. Since the followers with the same spatio-temporal mode tend to have social connections, we first mine closely-related users for each user through topology and locality similarity, generating respective cross-domain ego-networks. Subsequently, we construct a heterogeneous ego-mo hypergraph consisting of mobility and ego-networks. We propose a novel graph convolutional network (GCN)-based approach to learn user representations, which enables the aggregation of information from surrounding nodes, incorporating topological similarities, stay locality similarities, and co-occurrence frequencies. The resulting embeddings provide comprehensive representations of users and locations, capturing their characteristics and relationships across platforms, which further facilitates the UIL task. Our experimental results on real-world check-in datasets from Foursquare and Twitter demonstrate that EgoMUIL outperforms the state-of-the-art methods on the UIL task. Notably, EgoMUIL exhibits superior performance in scenarios involving limited check-in records and follow relations.}
}


@article{DBLP:journals/tmc/ChenXWQLZG24,
	author = {Duyu Chen and
                  Wenchao Xu and
                  Haozhao Wang and
                  Yining Qi and
                  Ruixuan Li and
                  Pan Zhou and
                  Song Guo},
	title = {Caching User-Generated Content in Distributed Autonomous Networks
                  via Contextual Bandit},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8355--8369},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3349849},
	doi = {10.1109/TMC.2024.3349849},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXWQLZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The escalating proliferation of user generated contents such as videos and images are dominating the network traffic. The optimal strategy for mitigating backbone congestion and minimizing user request latency lies in prudent caching at edge stations within distributed autonomous networks, obviating the necessity to transmit data to the cloud. However, accurately caching content based on distributed autonomous networks requires elaborative collaboration between edge servers, which remains a great challenge, especially when content is highly dynamic and the storage resources of edge stations are limited. To tackle this challenge, this paper proposes a contextual bandit-based online caching algorithm for evaluating the optimal content hit rate reward, which can adapt to the constantly changing stream of emerging content. We build the content space, BS space, and a fine-grained space searching method to cache contents and corresponding edge stations. Furthermore, to perform collaborative caching and sharing between edges, we propose a federated autonomous multi-layer caching framework, whereby each server can locally learn the model for accurate caching and a synchronous mechanism is set up for global updating, further improving the hit rates. Finally, we perform theoretical proofs and simulations, demonstrating that our regret is sublinear and our caching algorithm outperforms several state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/PanAZY24,
	author = {Qingrui Pan and
                  Zhenlin An and
                  Xiaopeng Zhao and
                  Lei Yang},
	title = {The Power of Precision: High-Resolution Backscatter Frequency Drift
                  in {RFID} Identification},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8370--8385},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3347681},
	doi = {10.1109/TMC.2023.3347681},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/PanAZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Physical-layer identification uses manufacturing variations to create unique identifiers for each device. A decade ago, this concept was applied to RFID tags using backscatter frequency drift (BFD), a specific kind of ‘fingerprint’ determined by the difference between the actual backscatter signal received and the expected backscatter link frequency (BLF). However, BFD has been undervalued due to its low performance in tag identification, achieving less than 30% accuracy. In this study, we reevaluate BFD, focusing on the issue of frequency resolution as the cause of its poor performance. The problem doesn't lie in the BFD's uniqueness, but in the inferior way we measure the frequency of a backscatter signal, which is limited by the current air interface protocol. This situation is akin to trying to identify human fingerprints using low-quality imaging. We propose a practical solution to improve the frequency resolution from kilohertz to sub-hertz, without requiring hardware or protocol changes. Our findings show that this high-resolution BFD approach significantly enhances the distinguishability to 99.4% and the identification accuracy to 94% when tested on a dataset of 7,135 RFID tags across nine models.}
}


@article{DBLP:journals/tmc/JeonS24a,
	author = {Kang Eun Jeon and
                  James She},
	title = {Extending Beacon Lifetime by Predicting User Occupancy Using Deep
                  Neural Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8386--8397},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350045},
	doi = {10.1109/TMC.2024.3350045},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JeonS24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) beacon network is one of the essential infrastructures for many IoT and smart city applications. However, the BLE beacon network usually suffers from poor reliability and high maintenance costs due to the short-lived battery lifetime. A few recent works tackled the challenge by adjusting the operating configuration subject to the nearby user occupancy in a reactive manner. However, previous works failed to adapt to dynamically changing user occupancy behaviors due to the lack of a prediction mechanism. Such shortcomings lead to a shorter lifetime and longer packet arrival delays. To overcome this limitation, a novel neural network architecture that makes accurate and timely user occupancy prediction is introduced. The proposed network learns partial correlation of the time-series data and attention score for robust prediction. The predictions are then utilized to adaptively change the operation configurations to maximize the lifetime and minimize the packet arrival delay. To the best of our knowledge, this is the first work to leverage time-series prediction to optimize the performance of a BLE beacon. The effectiveness of the proposed learning methods is verified by comprehensive simulations with real-life data. The results demonstrate that the proposed method can extend a beacon lifetime by 50% more than the existing reactive approach. Moreover, the packet arrival delays are also reduced by up to 40%.}
}


@article{DBLP:journals/tmc/OhKLLJKS24,
	author = {Sangeun Oh and
                  Ahyeon Kim and
                  Sunjae Lee and
                  Kilho Lee and
                  Dae R. Jeong and
                  Steven Y. Ko and
                  Insik Shin},
	title = {Supporting Flexible and Transparent User Interface Distribution Across
                  Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8398--8417},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3349561},
	doi = {10.1109/TMC.2024.3349561},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/OhKLLJKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing trend of multi-device ownerships creates opportunities to use applications across devices. However, the current methods of app development/usage remain in the single-device paradigm, which is far below user expectations. For example, it is currently impossible for users to dynamically partition an existing app across different devices to utilize multiple surfaces. We introduce {\\sf FLUID}\n, a novel multi-device platform that supports simultaneous operation of multiple devices. {\\sf FLUID}\naims to i) distribute the user interfaces (UIs) of a single app across multiple devices, ii) support unmodified legacy apps without extra engineering, and iii) support numerous apps with customized UIs. Previous approaches, like screen mirroring and app migration, do not satisfy those goals altogether. However, {\\sf FLUID}\nis designed to satisfy the goals. It can efficiently deploy UI objects to different devices by identifying only UI states necessary for accurate rendering. And {\\sf FLUID}\ncan execute the distributed UI objects by supporting cross-device method invocations transparently and synchronizing the replicated UIs across devices. Furthermore, {\\sf FLUID}\nautomatically handles unexpected events that may degrade its usability by efficiently maintaining the distributed UIs up to date. Our evaluation using 20 legacy apps shows that {\\sf FLUID}\ncan transparently support numerous apps and is fast enough for interactive use.}
}


@article{DBLP:journals/tmc/HaoD24,
	author = {Shugang Hao and
                  Lingjie Duan},
	title = {To Save Mobile Crowdsourcing From Cheap-Talk: {A} Game Theoretic Learning
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8418--8430},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348801},
	doi = {10.1109/TMC.2023.3348801},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HaoD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today mobile crowdsourcing platforms invite users to provide anonymous reviews about service experiences, yet many reviews are found biased to be extremely positive or negative. The existing methods find it difficult to learn from biased reviews to infer the actual service state, as the state can also be extreme and the platform cannot verify the truthfulness of reviews immediately. Further, reviewers can hide their (positive or negative) bias types and proactively adjust their anonymous reviews against the platform's inference. To our best knowledge, we are the first to study how to save mobile crowdsourcing from cheap-talk and strategically learn from biased users’ reviews. We formulate the problem as a dynamic Bayesian game, including users’ service-type messaging and the platform's follow-up rating/inference. Our closed-form PBE shows that an extremely-biased user may still honestly message to convince the platform of listening to his review. Such Bayesian game-theoretic learning obviously outperforms the latest common schemes especially when there are multiple diversely-biased users to compete. For the challenging single-user case, we further propose a time-evolving mechanism with the platform's commitment inferences to ensure the biased user's truthful messaging all the time, whose performance improves with more time periods to learn from more historical data.}
}


@article{DBLP:journals/tmc/TianLL24,
	author = {Jinmei Tian and
                  Yang Lu and
                  Jiguo Li},
	title = {Lightweight Searchable and Equality-Testable Certificateless Authenticated
                  Encryption for Encrypted Cloud Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8431--8446},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348849},
	doi = {10.1109/TMC.2023.3348849},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TianLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Public key encryption with equality test (PKE-ET) is a novel cryptosystem to deal with the problem of multi-public-key encrypted data computing. It can be used to verify if different ciphertexts are encryptions of same plaintext under different public keys without decryption. As an extension of PKE-ET, certificateless encryption with equality test (CLE-ET) has the merits of no key escrow and no certificate. However, the existing CLE-ET schemes are vulnerable to the message recovery (MR) attack and suffer from low efficiency due to using the computationally expensive bilinear pairing. In this work, an elliptic-curve-based certificateless authenticated encryption with keyword search and equality test (CLAE-KS&ET) scheme is developed. The scheme not only provides resistance to the MR attack, but also satisfies the lightweight requirement of the resources-restricted environments. Moreover, it supports a ciphertext retrieval function resisting keyword guessing attacks. This function enables a user to seek out the desired ciphertexts on the cloud server first before making ciphertext equality test with others. Based on the computational Diffie-Hellman (CDH) and decisional Diffie-Hellman (DDH) problems, we formally prove its security. Compared with the existing CLE-ET schemes, it significantly improves computational efficiency and is more suited to the user terminals with limited resources in cloud.}
}


@article{DBLP:journals/tmc/XieZLHSC24,
	author = {Xuecheng Xie and
                  Dongheng Zhang and
                  Yadong Li and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {Robust WiFi Respiration Sensing in the Presence of Interfering Individual},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8447--8462},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348879},
	doi = {10.1109/TMC.2023.3348879},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieZLHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based respiration sensing technology has gained increasing attention due to its contactless sensing capabilities and utilization of existing WiFi devices. However, existing studies are limited to certain scenarios without addressing the motion interference from other individuals. In this paper, we tackle the challenge of robust respiration sensing in the presence of other individuals. Specifically, through an in-depth examination of the correlation between respiratory signals and spatial beam patterns, we develop a respiratory-energy based approach to evaluate the diverse impact of dynamic interference on respiratory signals. When significant interference is detected, we employ a convex-optimization-based beam control strategy, which exploits the inherent characteristics of human respiration, to adaptively adjust the spatial beam pattern. This approach enables a robust and precise gain adjustment between the target and interfering individual, effectively mitigating the impact of interference. Experimental results demonstrate that our approach can reduce the mean absolute error (MAE) of respiration detection by up to 32% compared to state-of-the-art methods, significantly enhancing the accuracy and robustness of WiFi-based respiration sensing.}
}


@article{DBLP:journals/tmc/ShengHCXGG24,
	author = {Biyun Sheng and
                  Rui Han and
                  Hui Cai and
                  Fu Xiao and
                  Linqing Gui and
                  Zhengxin Guo},
	title = {CDFi: Cross-Domain Action Recognition Using WiFi Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8463--8477},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348939},
	doi = {10.1109/TMC.2023.3348939},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShengHCXGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contactless WiFi based human action recognition exhibits remarkable prospects in the fields such as human-computer interaction and smart home. However, domain dependency restricts its generalization into the real-world deployment. Since it is expensive to label enough new data for retaining a model, it is beneficial to explore few-shot learning for cross-domain sensing with limited target labels. Nevertheless, there are two challenges to be addressed. The first challenge is how to select a suitable dataset from a series of available source domains to prevent negative transfer. The second is to mine action-related characteristics by the feature learning model for the following effective knowledge transfer. In order to tackle the above challenges, we present a cross-domain sensing framework named CDFi, which consists of Nearest Neighbor based Domain Selector (NNDS) and Fine-to-Coarse-Grained Transformer Network (FCGTN). NNDS is proposed to evaluate the source-target domain similarities by measurements among local and global feature distributions. Besides, FCGTN embeds convolution map based hierarchical transformer structures and the modified linear layer into an end-to-end deep network, which can quickly adapt to the unseen domain by few samples. Comprehensive experiments show that CDFi can effectively realize cross-domain action recognition, and achieve about 4% and 10% improvement on cross-user and cross-scene cases, respectively, compared to the state-of-the-art.}
}


@article{DBLP:journals/tmc/ZhuLCMWZ24,
	author = {Jianhang Zhu and
                  Rongpeng Li and
                  Xianfu Chen and
                  Shiwen Mao and
                  Jianjun Wu and
                  Zhifeng Zhao},
	title = {Semantics-Enhanced Temporal Graph Networks for Content Popularity
                  Prediction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8478--8492},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3349315},
	doi = {10.1109/TMC.2023.3349315},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLCMWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surging demand for high-definition video streaming services and large neural network models implies a tremendous explosion of Internet traffic. To mitigate the traffic pressure, architectures with in-network storage have been proposed to cache popular contents at devices in closer proximity to users. Correspondingly, in order to maximize caching utilization, it becomes essential to devise an effective popularity prediction method. In that regard, predicting popularity with dynamic graph neural network (DGNN) models achieves remarkable performance. However, DGNN models still suffer from tackling sparse datasets where most users are inactive. Therefore, we propose a reformative temporal graph network, named semantics-enhanced temporal graph network (STGN), which attaches extra semantic information into the user-content bipartite graph and could better leverage implicit relationships behind the superficial topology structure. On top of that, we customize its temporal and structural learning modules to further boost the prediction performance. Specifically, in order to efficiently aggregate the diversified semantics that a content might possess, we design a user-specific attention (UsAttn) mechanism for the temporal learning. Unlike the attention mechanism that only analyzes the influence of genres on content, UsAttn also considers the attraction of semantic information to a specific user. Meanwhile, as for the structural learning, we introduce the concept of positional encoding into our attention-based graph learning and novelly adopt a semantic positional encoding (SPE) function, which effectively boost the performance of lightweight algorithms. Finally, extensive simulations verify the superiority of our models and demonstrate their effectiveness in content caching.}
}


@article{DBLP:journals/tmc/SunXFLWFC24,
	author = {Xue Sun and
                  Jie Xiong and
                  Chao Feng and
                  Haoyu Li and
                  Yuli Wu and
                  Dingyi Fang and
                  Xiaojiang Chen},
	title = {EarSSR: Silent Speech Recognition via Earphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8493--8507},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3356719},
	doi = {10.1109/TMC.2024.3356719},
	timestamp = {Sun, 19 Jan 2025 14:43:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunXFLWFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the most natural and convenient way to communicate with people, speech is always preferred in Human-Computer Interactions. However, voice-based interaction still has several limitations. It raises privacy concerns in some circumstances and the accuracy severely degrades in noisy environments. To address these limitations, silent speech recognition (SSR) has been proposed, which leverages the inaudible information (e.g., lip movements and throat vibration) to recognize the speech. In this paper, we present EarSSR, an earphone-based silent speech recognition system to enable interaction without a need of vocalization. The key insight is that when people are speaking, their ear canals exhibit unique deformation patterns and the corresponding deformation patterns are related to words/letters even without any vocalization. We utilize the built-in microphone and speaker of an earphone to capture the ear canal deformation. Ultrasound signals are emitted and the reflected signals are analyzed to extract the signal features corresponding to speech-induced ear canal deformation for silent speech recognition. We design a two-channel hierarchical convolutional neural network to achieve fine-grained letter/word recognition. Our extensive experiments show that EarSSR can achieve an accuracy of 82% for single alphabetic letter recognition and an accuracy of 93% for word recognition.}
}


@article{DBLP:journals/tmc/LiuGZWHZ24,
	author = {Yi Liu and
                  Song Guo and
                  Yufeng Zhan and
                  Leijie Wu and
                  Zicong Hong and
                  Qihua Zhou},
	title = {Chiron: {A} Robustness-Aware Incentive Scheme for Edge Learning via
                  Hierarchical Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8508--8524},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350654},
	doi = {10.1109/TMC.2024.3350654},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuGZWHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past few years, edge learning has achieved significant success in mobile edge networks. Few works have designed incentive mechanism that motivates edge nodes to participate in edge learning. However, most existing works only consider myopic optimization and assume that all edge nodes are honest, which lacks long-term sustainability and the final performance assurance. In this paper, we propose Chiron, an incentive-driven Byzantine-resistant long-term mechanism based on hierarchical reinforcement learning (HRL). First, our optimization goal includes both learning-algorithm performance criteria (i.e., global accuracy) and systematical criteria (i.e., resource consumption), which aim to improve the edge learning performance under a given resource budget. Second, we propose a three-layer HRL architecture to handle long-term optimization, short-term optimization, and byzantine resistance, respectively. Finally, we conduct experiments on various edge learning tasks to demonstrate the superiority of the proposed approach. Specifically, our system can successfully exclude malicious nodes and lazy nodes out of the edge learning participation and achieves 14.96% higher accuracy and 12.66% higher total utility than the state-of-the-art methods under the same budget limit.}
}


@article{DBLP:journals/tmc/DingZWM24,
	author = {Lige Ding and
                  Dong Zhao and
                  Zhaofeng Wang and
                  Huadong Ma},
	title = {LAMD{\textdollar}{\^{}}\{2\}{\textdollar}2: Enabling Economical and
                  Green Travel for Diversified Mobility on Demand Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8525--8540},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3353621},
	doi = {10.1109/TMC.2024.3353621},
	timestamp = {Sun, 08 Sep 2024 16:07:48 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DingZWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The diversified mobility on demand (MoD) systems integrate both traditional fuel vehicles and green transportation tools (e.g. shared bicycles and shared e-bikes), which can not only reduce the fleet size of traditional fuel vehicles but also address the demand for short-distance travel and alleviate environmental pollution. However, despite having a variety of travel tools, the existing MoD systems neglect the guidance on passengers according to their preferences and travel characteristics and thus lead to the failure of effective cooperation among multiple travel modes and additional waste of resources. This inspired us to design a novel order allocation mechanism for diversified MoD systems. Specifically, we construct a heterogeneous order graph based on the order sets, transform the minimum fleet problem into the minimum trajectory coverage problem on the heterogeneous order graph and propose a learning-based order allocation method LAMD^{2} containing three modules. i) The online breadth-first order search framework fully considers the characteristics of different travel modes and the interaction of multiple vehicles, and then leverages the competitive mechanism to well handle the heterogeneity of travel modes and improve the overall efficiency. ii) The multi-semantic travel mode selection module analyzes users’ preferences for diversified travel modes based on multi-semantic historical travel data and then determines the service mode based on the similarity of order spatiotemporal characteristics. iii) The Reinforcement Learning (RL)-based order evaluation module evaluates the long-term benefits of expanding existing For-Hire Vehicle (FHV) trajectories with different orders and updates the behavioral strategies through interactive feedback with the environment. We implement and evaluate the proposed method with a real-world trajectory dataset, demonstrating that LAMD^{2} outperforms all the baselines and reduces the fleet size and energy consumption by the average of 2.93% and 8.01%, respectively, compared to the real-world systems.}
}


@article{DBLP:journals/tmc/XieLWL24,
	author = {Pengjin Xie and
                  Lingkun Li and
                  Jiliang Wang and
                  Yunhao Liu},
	title = {Passive Visible Light Tag System for Localization and Posture Estimation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8541--8556},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2023.3348802},
	doi = {10.1109/TMC.2023.3348802},
	timestamp = {Fri, 02 Aug 2024 21:40:16 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XieLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the development of the Internet of Things, location service plays a more important role in mobile computing. To provide location service for the already deployed devices and objects, we present LiTag, a visible light-based localization and posture estimation solution with commercial off-the-shelf (COTS) cameras. The core of LiTag is based on the design of a chip-less and battery-less optical tag which can show different color patterns from different observation directions. After capturing a photo containing the tag, LiTag can calculate the tag position and posture by combining the color pattern and the geometric relation in camera imaging. To solve the localization ambiguity, we propose an ambiguity-avoidance method based on a projection relationship. LiTag can work with a single camera without calibration, which significantly reduces the calibration overhead and deployment costs. We implement LiTag and evaluate its performance extensively. Results show that LiTag can provide the tag position with a median error of 1 \\rm{cm} in the 2D plane, a median error of 5 cm in the 3D space, and posture estimation with a median error of 0.8^{\\circ }. We believe that LiTag has high potential to provide a low-cost and easy-to-use solution for ubiquitous localization and posture estimation with widely deployed cameras.}
}


@article{DBLP:journals/tmc/ZengLZLW24,
	author = {Qiuyang Zeng and
                  Fan Li and
                  Zhiyuan Zhao and
                  Youqi Li and
                  Yu Wang},
	title = {AcouWrite: Acoustic-Based Handwriting Recognition on Smartphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {8},
	pages = {8557--8568},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3351484},
	doi = {10.1109/TMC.2024.3351484},
	timestamp = {Thu, 13 Feb 2025 08:09:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZengLZLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Off-screen handwriting recognition enriches the handwriting interaction paradigm for mobile devices. However, the existing approaches are only applicable to the specific environment and equipment conditions. In this article, we propose AcouWrite, a general, scalable and real-time handwriting recognition system based on active acoustic sensing. In detail, AcouWrite relies on active acoustic sensing using only a pair of microphones and speakers on the smartphone to capture real-time handwriting input. Particularly, we extract the short-time dCIR (st-dCIR) to monitor the changes in the acoustic transmission channel resulting from finger movement. Technically, we use a CNN-GRU classifier to complete the recognition task in AcouWrite. Moreover, we use data augmentation and spelling error correction methods to improve AcouWrite's robustness. To improve the generalization of our AcouWrite for new characters, we incorporate the transfer learning module into our AcouWrite. In various real-world environments, experiments demonstrate that AcouWrite achieves a mean recognition accuracy of 97.62%, a word accuracy (WA) of 96.4% and a character error rate (CER) of 1.5% for 100 common words, and an average response time of 94 milliseconds.}
}


@article{DBLP:journals/tmc/FanLYLH24,
	author = {Wenzhe Fan and
                  Xingkang Li and
                  Guang Yang and
                  Chunguo Li and
                  Yongming Huang},
	title = {Low-Complexity Mobile User Tracking in Quantized mmWave {MIMO} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8569--8581},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3351634},
	doi = {10.1109/TMC.2024.3351634},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FanLYLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of large-scale arrays in millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems has enabled highly accurate localization by reaping the benefits of high angular resolution. However, the use of massive antennas in mmWave systems results in expensive hardware costs and computational burdens. This paper considers mobile user localization in quantized mmWave MIMO systems, where each base station (BS) antenna is equipped with low-resolution analog-to-digital converters. The proposed approach integrates the beamspace model with off-grid information to capture channel sparsity in the angular domain. The temporal correlation of angle-of-arrival (AoA) is characterized by a Markov process for moving users. To estimate channel gains and time-varying AoAs, while keeping the computational complexity low, we further develop generalized approximate message passing and AoA tracking methods. In dense multipath environments, determining line-of-sight (LoS) paths for precise localization poses a challenge. To address this issue, we propose a fast direct localization based on LoS identification that can also be applied when the LoS paths of some BSs are obstructed. In the final stage, the moving user locations are recovered via triangulation. Simulation results validate the effectiveness of the proposed algorithms and showcase the feasibility of implementing quantized mmWave systems for localization purposes.}
}


@article{DBLP:journals/tmc/SunHSWLLNL24,
	author = {Geng Sun and
                  Long He and
                  Zemin Sun and
                  Qingqing Wu and
                  Shuang Liang and
                  Jiahui Li and
                  Dusit Niyato and
                  Victor C. M. Leung},
	title = {Joint Task Offloading and Resource Allocation in Aerial-Terrestrial
                  {UAV} Networks With Edge and Fog Computing for Post-Disaster Rescue},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8582--8600},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350886},
	doi = {10.1109/TMC.2024.3350886},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunHSWLLNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are playing an increasingly important role in assisting fast-response post-disaster rescue due to their fast deployment, flexible mobility, and low cost. However, UAVs face the challenges of limited battery capacity and computing resources, which could shorten the expected flight endurance of UAVs and increase the rescue response delay during performing mission-critical tasks. To address these challenges, we first present a three-layer post-disaster rescue computing architecture by leveraging the aerial-terrestrial edge capabilities of mobile edge computing (MEC) and vehicle fog computing (VFC), which consists of a vehicle fog layer, a UAV client layer, and a UAV edge layer. Moreover, we formulate a joint task offloading and resource allocation optimization problem (JTRAOP) with the aim of maximizing the time-average system utility. Since the formulated JTRAOP is proved to be NP-hard, we propose an MEC-VFC-aided task offloading and resource allocation (MVTORA) approach, which consists of a game theoretic algorithm for task offloading decision, a convex optimization-based algorithm for MEC resource allocation, and an evolutionary computation-based hybrid algorithm for VFC resource allocation. Simulation results validate that the proposed approach can achieve superior system performance compared to alternative approaches, especially under heavy system workloads.}
}


@article{DBLP:journals/tmc/ZhangSWLLNL24,
	author = {Chuang Zhang and
                  Geng Sun and
                  Qingqing Wu and
                  Jiahui Li and
                  Shuang Liang and
                  Dusit Niyato and
                  Victor C. M. Leung},
	title = {{UAV} Swarm-Enabled Collaborative Secure Relay Communications With
                  Time-Domain Colluding Eavesdropper},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8601--8619},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350885},
	doi = {10.1109/TMC.2024.3350885},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangSWLLNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) as aerial relays are practically appealing for assisting the Internet of Things (IoT) network. In this article, we aim to utilize a UAV swarm to assist the secure communication between the micro base station (MBS) equipped with the planar antenna array (PAA) and the IoT terminal devices by collaborative beamforming (CB), so as to counteract the effects of the eavesdropper colluding in the time domain. Specifically, we formulate a UAV swarm-enabled secure relay multi-objective optimization problem (US2RMOP) for simultaneously maximizing the achievable sum rate of the associated IoT terminal devices, minimizing the achievable sum rate of the eavesdropper and minimizing the energy consumption of UAV swarm, by jointly optimizing the excitation current weights of both MBS and UAV swarm, the selection of the UAV receiver, the position of UAVs and user association order of IoT terminal devices. Furthermore, the formulated US2RMOP is proved to be a non-convex, NP-hard and large-scale optimization problem. Therefore, we propose an improved multi-objective grasshopper algorithm (IMOGOA) with some specific designs to address the problem. Simulation results exhibit the effectiveness of the proposed UAV swarm-enabled collaborative secure relay strategy and demonstrate the superiority of IMOGOA.}
}


@article{DBLP:journals/tmc/MollahosseiniAA24,
	author = {Poorya Mollahosseini and
                  Sepehr Asvadi and
                  Farid Ashtiani},
	title = {Effect of Variable Backoff Algorithms on Age of Information in Slotted
                  {ALOHA} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8620--8633},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3351054},
	doi = {10.1109/TMC.2024.3351054},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MollahosseiniAA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we analyze the effect of implementing variable backoff algorithms on the average peak age of information (PAoI) and the average age of information (AoI) in slotted ALOHA networks. To this end, we first analytically derive the average PAoI in exponential backoff slotted ALOHA when the number of backoff stages is finite. We also show that the reduction in average PAoI achieved by the exponential backoff algorithm can significantly increase the average AoI, which is not desirable. Then, we propose an age-based backoff algorithm and present the intuition behind it. In contrast to exponential backoff, our proposed age-based backoff algorithm reduces the average PAoI significantly, while changing the average AoI limitedly. The numerical results confirm our analytical results as well as the superiority of the proposed age-based backoff algorithm over exponential backoff.}
}


@article{DBLP:journals/tmc/LiXLXL24,
	author = {Jiao Li and
                  Jiakai Xu and
                  Yang Liu and
                  Weitao Xu and
                  Zhenjiang Li},
	title = {Enhancing the Applicability of Sign Language Translation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8634--8648},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350111},
	doi = {10.1109/TMC.2024.3350111},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiXLXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses a significant problem in American Sign Language (ASL) translation systems that has been overlooked. Current designs collect excessive sensing data for each word and treat every sentence as new, requiring the collection of sensing data from scratch. This approach is time-consuming, taking hours to half a day to complete the data collection process for each user. As a result, it creates an unnecessary burden on end-users and hinders the widespread adoption of ASL systems. In this study, we identify the root cause of this issue and propose GASLA–a wearable sensor-based solution that automatically generates sentence-level sensing data from word-level data. An acceleration approach is further proposed to optimize the data generation speed. Moreover, due to the gap between the generated sentence data and directly collected sentence data, a template strategy is proposed to make the generated sentences more similar to the collected sentence. The generated data can be used to train ASL systems effectively while reducing overhead costs significantly. GASLA offers several benefits over current approaches: it reduces initial setup time and future new-sentence addition overhead; it requires only two samples per sentence compared to around ten samples in current systems; and it improves overall performance significantly.}
}


@article{DBLP:journals/tmc/HaoXZYM24,
	author = {Hao Hao and
                  Changqiao Xu and
                  Wei Zhang and
                  Shujie Yang and
                  Gabriel{-}Miro Muntean},
	title = {Joint Task Offloading, Resource Allocation, and Trajectory Design
                  for Multi-UAV Cooperative Edge Computing With Task Priority},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8649--8663},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350078},
	doi = {10.1109/TMC.2024.3350078},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HaoXZYM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has emerged as a solution to address the demands of computation-intensive network services by providing computational capabilities at the network edge, thus reducing service delays. Due to the flexible deployment, wide coverage and reliable wireless communication, unmanned aerial vehicles (UAVs) have been employed to assist MEC. This paper investigates the task offloading problem in a UAV-assisted MEC system with collaboration of multiple UAVs, highlighting task priorities and binary offloading mode. We defined the system gain based on energy consumption and task delay. The joint optimization of UAVs’ trajectory design, binary offloading decision, computation resources allocation, and communication resources management is formulated as a mixed integer programming problem with the goal of maximizing the long-term average system gain. Considering the discrete-continuous hybrid action space of this problem, we propose a novel deep reinforcement learning (DRL) algorithm based on the latent space to solve it. The evaluation results demonstrate that our proposed algorithm outperforms three state-of-the-art alternative solutions in terms of task delay and system gain.}
}


@article{DBLP:journals/tmc/ZhangJYZL24,
	author = {Xiuling Zhang and
                  Riheng Jia and
                  Quanjun Yin and
                  Zhonglong Zheng and
                  Minglu Li},
	title = {Intelligent Trajectory Design and Charging Scheduling in Wireless
                  Rechargeable Sensor Networks With Obstacles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8664--8679},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3350075},
	doi = {10.1109/TMC.2024.3350075},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangJYZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless rechargeable sensor networks (WRSNs) are promising in maintaining sustainable large-area monitoring tasks. Mobile chargers (MCs) are commonly used in WRSNs to replenish energy to nodes due to its flexibility and easy maintenance. Most existing works on WRSNs focus on designing offline or model-based online charging methods, which need the exact system information to conduct the optimization. However, in practical WRSNs, the exact system information such as the nodes’ locations and energy consumption rates may not be easily accessible to the optimizer due to their unpredictability and high dynamics. Thus, in this work, we jointly optimize the MC’s trajectory design and charging scheduling in a general and practical WRSN with inaccessibility to the exact system information, such that the charging utility of the MC is maximized. To address this problem, we introduce the model-free reinforcement learning (RL) technique, which enables the MC to learn to jointly optimize its moving trajectory and charging scheduling by interacting with the environment and tracking feedback signals from nodes and obstacles in real time. Specifically, we develop a soft actor-critic based mobile security policy intervened algorithm (SAC-MSPI) based on a novel safe RL framework, which maximizes the MC’s charging utility while maintaining the safe movement (not hitting obstacles) for the MC during the entire charging period. Extensive evaluation results show that the proposed SAC-MSPI algorithm outperforms existing main RL solutions and traditional algorithms with respect to the charging utility maximization as well as the collision avoidance.}
}


@article{DBLP:journals/tmc/SongSYPGT24,
	author = {Zihang Song and
                  Yiyuan She and
                  Jian Yang and
                  Jinbo Peng and
                  Yue Gao and
                  Rahim Tafazolli},
	title = {Nonuniform Sampling Pattern Design for Compressed Spectrum Sensing
                  in Mobile Cognitive Radio Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8680--8693},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3353591},
	doi = {10.1109/TMC.2024.3353591},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SongSYPGT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compressed spectrum sensing (CSS) plays a pivotal role in dynamic spectrum access within mobile cognitive radio networks by offering reduced power consumption and lower hardware costs. The multicoset sampler, a well-known implementation for periodic nonuniform sampling, has been widely studied and is considered a promising architecture for realizing CSS. This article focuses on the design of the multicoset sampling pattern, aiming at enhancing the isometry property of the sensing matrix. Unlike previous studies which assume a noise-free setup, our work considers the problem in a real-world environment with noise. First, we propose a deterministic algorithm for sampling pattern generation, particularly for specific hardware setup parameters. This algorithm offers strict mutual-coherence control in the multicoset sensing matrix. To address more general hardware configurations, we propose two optimization algorithms. One of them searches for nearly optimal sampling patterns through a random search strategy, while the other employs a greedy pursuit strategy to find a local optimizer. Furthermore, we propose an algorithm to iteratively optimize the sampling pattern between consecutive spectrum sensing windows by minimizing a restricted version of mutual coherence. The excellent performance of our proposed algorithms has been demonstrated through numerical experiments and has been verified on a self-developed hardware platform.}
}


@article{DBLP:journals/tmc/YangACWHZC24,
	author = {Yanni Yang and
                  Zhenlin An and
                  Jiannong Cao and
                  Yanwen Wang and
                  Pengfei Hu and
                  Guoming Zhang and
                  Xiuzhen Cheng},
	title = {Jump Out of Resonance: {A} Practical {NFC} Tag Fingerprinting Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8694--8709},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3354813},
	doi = {10.1109/TMC.2024.3354813},
	timestamp = {Mon, 27 Jan 2025 07:22:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangACWHZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {NFC tag authentication is crucial for preventing tag misuse. Existing NFC fingerprinting methods use physical-layer signals, which incorporate tag hardware imperfections, for authentication purposes. However, these methods suffer from limitations such as low scalability for a large number of tags or incompatibility with various NFC protocols, hindering practical application. To address these issues, we propose a new NFC fingerprinting scheme called NFChain^+. Instead of sticking to the NFC resonant frequency, NFChain^+ excavates the tag hardware uniqueness from the protocol-agnostic tag response signal using an agile and compatible frequency band of NFC to extract the tag fingerprint from a chain of tag responses over multiple frequencies. This significantly improves fingerprint scalability. However, extracting the desired fingerprint presents two challenges: fingerprint inconsistency under different configurations, and fingerprint variations due to the signal noise in generic readers. To overcome these challenges, we design an effective signal elimination method to remove the effect of device configurations and employ contrastive learning to reduce fingerprint variations for accurate tag authentication. We further cultivate a data augmentation strategy to save the cost of manually collecting fingerprint measurements for training the authentication model. Extensive experiments show that we can achieve as low as 3.4% FRR and 4.1% FAR for over 600 NFC tags.}
}


@article{DBLP:journals/tmc/PettoraliRVDA24,
	author = {Marco Pettorali and
                  Francesca Righetti and
                  Carlo Vallati and
                  Sajal K. Das and
                  Giuseppe Anastasi},
	title = {Mobility Management in TSCH-Based Industrial Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8710--8728},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3354798},
	doi = {10.1109/TMC.2024.3354798},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PettoraliRVDA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Sensor and Actuator Networks (WSANs) are an effective technology for improving the efficiency and productivity in many industrial domains, and are also the building blocks for the Industrial Internet of Things (IIoT). To support this trend, the IEEE has defined the 802.5.4 Time-Slotted Channel Hopping (TSCH) protocol. Unfortunately, TSCH does not provide any mechanism to manage node mobility, while many current industrial applications involve Mobile Nodes (MNs), e.g., mobile robots or wearable devices carried by workers. In this article, we present a framework to efficiently manage mobility in TSCH networks, by proposing an enhanced version of the Synchronized Single-hop Multiple Gateway (SHMG) architecture. We first define a flexible scheduling algorithm, called Shared Downstream-Dedicated Upstream (SD-DU), that can be configured to adapt to different types of traffic in industrial applications. Then, we develop a mathematical framework to formalize the problem of Border Routers (BRs) placement to guarantee the complete coverage of the deployment area, in the presence of obstacles and unreliable communication. A methodology for network sizing is also proposed to calculate the maximum number of MNs that can be supported by the network without violating the application requirements. Finally, we evaluate the performance of the proposed solutions, both analytically and through simulations. Our results show that the proposed enhancements allow a very effective management of node mobility, by providing mobility transparency without a significant impact on performance.}
}


@article{DBLP:journals/tmc/GeXWNFL24,
	author = {Yiyang Ge and
                  Ke Xiong and
                  Qiong Wang and
                  Qiang Ni and
                  Pingyi Fan and
                  Khaled Ben Letaief},
	title = {AoI-Minimal Power Adjustment in RF-EH-Powered Industrial IoT Networks:
                  {A} Soft Actor-Critic-Based Method},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8729--8741},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3356229},
	doi = {10.1109/TMC.2024.3356229},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GeXWNFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article investigates the radio-frequency-energy-harvesting-powered (RF-EH-powered) wireless Industrial Internet of Things (IIoT) networks, where multiple sensor nodes (SNs) are first powered by a wireless power station (WPS), and then collect status updates from the industrial environment and finally transmit the collected data to the monitor with their harvested energy. To enhance the timeliness of data, age of information (AoI) is used as a metric to optimize the system. Particularly, an expected sum AoI (ESA) minimization problem is formulated by optimizing the power adjustment policy for the SNs under multiple practical constraints, including the EH, the minimal signal-to-noise-plus-interference ratio (SINR) and the battery capacity constraints. To solve the non-convex problem with no explicit AoI expression, we transform it into a Markov decision problem (MDP) with continuous state space and action space. Then, inspired by the Soft Actor-Critic (SAC) framework in deep reinforcement learning, a SAC-based age-aware power adjustment (SAPA) method is proposed by modeling the power adjustment as a stochastic strategy. Furthermore, to reduce the communication overhead of SAPA, a multi-agent version of SAPA, i.e., MSAPA, is proposed, with which each SN is able to adjust its transmit power based on its local observations. The communication overhead of SAPA and MSAPA is also analyzed theoretically. Simulation results show that the proposed SAPA and MSAPA converge well with different numbers of SNs. It is also shown that the ESA achieved by the proposed SAPA and MSAPA is lower than that achieved by the baseline methods.}
}


@article{DBLP:journals/tmc/GaoLZY24,
	author = {Ronghao Gao and
                  Yue Li and
                  Qinyu Zhang and
                  Zhihua Yang},
	title = {Semantic-Aware Bundle Delivery in Space Disruption-Tolerant Networks
                  via Cross-Layer Design on {BP} and {LTP}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8742--8756},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3351428},
	doi = {10.1109/TMC.2024.3351428},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In large-span space communication, the current bundle delivery mechanism using the Disruption-Tolerant Networks (DTN) technique confronts huge challenges such as high packet loss rate and huge latency. These challenges incur obviously low goodput when delivering scientific and engineering data for the target missions, such as instructions, text, and images. However, the context correlations in these data are not yet excavated to resist the above challenges by current works. To address this issue, therefore, we propose a semantic-aware bundle delivery mechanism for context-dependent data via a cross-layer design on Bundle Protocol (BP) and Licklider Transmission Protocol (LTP), which has the excellent error-tolerance capability by the well-designed semantic-oriented Automatic Repeat reQuest (ARQ) scheme. In particular, the jointed cross-layer design consists of a Semantic Blocking (SB) and Semantic Coding (SC)-based Bundle Updating (BU) mechanism and a dynamic Red/Green-part Allocation method based on Semantic Importance (RGA-SI) for bundles and segments in the two layers. Simulation results show that the proposed mechanism can reduce data latency and improve goodput from about 50% to 70% compared with the current bundle delivery mechanism in DTN with optimal segment size, especially under bad channel conditions.}
}


@article{DBLP:journals/tmc/ZhangHMCG24,
	author = {Yongchao Zhang and
                  Jia Hu and
                  Geyong Min and
                  Xin Chen and
                  Nektarios Georgalas},
	title = {Joint Charging Scheduling and Computation Offloading in EV-Assisted
                  Edge Computing: {A} Safe {DRL} Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8757--8772},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3355868},
	doi = {10.1109/TMC.2024.3355868},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangHMCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electric Vehicle-assisted Multi-access Edge Computing (EV-MEC) is a promising paradigm where EVs share their computation resources at the network edge to perform intensive computing tasks while charging. In EV-MEC, a fundamental problem is to jointly decide the charging power of EVs and computation task allocation to EVs, for meeting both the diverse charging demands of EVs and stringent performance requirements of heterogeneous tasks. To address this challenge, we propose a new joint charging scheduling and computation offloading scheme (OCEAN) for EV-MEC. Specifically, we formulate a cooperative two-timescale optimization problem to minimize the charging load and its variance subject to the performance requirements of computation tasks. We then decompose this sophisticated optimization problem into two sub-problems: charging scheduling and computation offloading. For the former, we develop a novel safe deep reinforcement learning (DRL) algorithm, and theoretically prove the feasibility of learned charging scheduling policy. For the latter, we reformulate it as an integer non-linear programming problem to derive the optimal offloading decisions. Extensive experimental results demonstrate that OCEAN can achieve similar performances as the optimal strategy and realize up to 24% improvement in charging load variance over three state-of-the-art algorithms while satisfying the charging demands of all EVs.}
}


@article{DBLP:journals/tmc/YangFSLLI24,
	author = {Yang Yang and
                  Lei Feng and
                  Yao Sun and
                  Yangyang Li and
                  Wenjing Li and
                  Muhammad Ali Imran},
	title = {Multi-Cluster Cooperative Offloading for {VR} Task: {A} {MARL} Approach
                  With Graph Embedding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8773--8788},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3354056},
	doi = {10.1109/TMC.2024.3354056},
	timestamp = {Sun, 19 Jan 2025 14:43:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangFSLLI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual reality (VR) technology has recently achieved notable success and been widely expected to interplay with more mobile multimedia services. To further enhance real-time immersive experience for VR applications, exploiting cooperative offloading among capable terminal devices should be emerged as an effective means. However, faced with diverse and surging mobile VR user requests, terminal-assisted offloading needs to support comprehensive cached content, ultra-low latency delivery, and continuous energy provisioning, to guarantee stringent quality of service requirements, which poses a critical challenge for resource-constrained terminals. Hence, this paper proposes a Cooperative Offloading framework for Terminal Clusters (named CO-TC), in which VR terminal clusters form several cooperation groups for sharing cached field of view (FoV) tiles and available computing resources to cooperatively perform FoV rendering and content delivery. To maximize energy efficiency in CO-TC, an optimization problem is formulated to jointly decide the task offloading and computing resource utilization. An intelligent offloading scheme is designed based on multi-agent reinforcement learning (MARL) specially using agent relation feature graph embeddings. Moreover, we theoretically prove the permutation invariance and convergence of the proposed algorithm and derive the optimal observation range of the agent to balance the performance gain and interaction overhead in the distributed MARL frame. Finally, simulation results show that the proposed offloading scheme outperforms other baselines in terms of VR service performance, including latency, energy consumption, and energy efficiency.}
}


@article{DBLP:journals/tmc/XiaoYLZCF24,
	author = {Ke Xiao and
                  Song Yang and
                  Fan Li and
                  Liehuang Zhu and
                  Xu Chen and
                  Xiaoming Fu},
	title = {Making Serverless Not So Cold in Edge Clouds: {A} Cost-Effective Online
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8789--8802},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3355118},
	doi = {10.1109/TMC.2024.3355118},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoYLZCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applying the serverless paradigm to edge computing improves edge resource utilization while bringing the benefits of flexible scaling and pay-as-you-go to latency-sensitive applications. This extends the boundaries of serverless computing and improves the quality of service for Function-as-a-Service users. However, as an emerging cloud computing paradigm, serverless edge computing faces pressing challenges, with one of the biggest obstacles being delay caused by excessively long container cold starts. Cold start delay is defined as the time between when a serverless function is triggered and when it begins to execute, and its existence seriously impacts resource utilization and Quality of Service (QoS). In this article, we study how to minimize the total system cost by caching function containers and selecting routes for neighboring functions via edge or public clouds. We prove that the proposed problem is NP-hard even in the special case where the user request contains only one function, and that the unpredictability of user requests and the impact between adjacent time decisions require that the problem to be solved in an online fashion. We then design the Online Lazy Caching algorithm, an online algorithm with a worst-case competitive ratio using a randomized dependent rounding algorithm to solve the problem. Extensive simulation results show that the proposed online algorithm can achieve close-to-optimal performance in terms of both total cost and cold start cost compared to other existing algorithms, with average improvements of 31.6\\%\nand 51.7\\%\n.}
}


@article{DBLP:journals/tmc/ChoiOKHK24,
	author = {Myeongwon Choi and
                  Sangeun Oh and
                  Insu Kim and
                  Jeongwoo Heo and
                  Hyosu Kim},
	title = {Extracting Payment Tokens Out of Sounds Produced by Magnetic Field
                  Fluctuations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8803--8821},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359266},
	doi = {10.1109/TMC.2024.3359266},
	timestamp = {Sun, 19 Jan 2025 14:43:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChoiOKHK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Samsung Pay, a widely-used mobile payment service, enables users to pay using just their smartphone thanks to Magnetic Secure Transmission (MST). This technology facilitates communication between smartphones and magnetic card terminals by transmitting payment tokens through magnetic waves. Intriguingly, such magnetic waves inherently produce a distinct sound pattern (called MST sound) containing payment information, which opens up new opportunities for both potential attackers and payment users. That is, MST sound can serve either as a new side channel for attackers to eavesdrop on MST transactions or as an easily accessible communication channel that enhances the payment experience for users. Inspired by these possibilities, we aim to deeply explore the potential of MST sound across these two dimensions, presenting two frameworks with different objectives: MagSnoop and M2APay. The first is the inference framework, which accurately, robustly, and efficiently infers payment tokens by listening to MST sounds. The second is the payment framework, which helps users establish a secure communication channel between MST-supported smartphones and microphone-equipped smartphones by shielding the vulnerability inherent in MST sound. Our experiments with prototypes of these frameworks achieved high accuracy in token inference and data transmission. Furthermore, both MagSnoop and M2APay are capable of accurately decoding tokens in diverse payment environments, including noisy environments and real-world scenarios.}
}


@article{DBLP:journals/tmc/WangSYCC24,
	author = {Chih{-}Hang Wang and
                  Yishuo Shi and
                  De{-}Nian Yang and
                  Wei{-}Yu Chen and
                  Wen{-}Tsuen Chen},
	title = {Joint IoT Device Selection and Health-Aware Beamforming Design for
                  {MIMO-WPT}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8822--8838},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366535},
	doi = {10.1109/TMC.2024.3366535},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangSYCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless power transfer (WPT) has emerged to enhance the robustness of the energy harvesting Internet of Things (EH-IoT), whereas beamforming has been leveraged to significantly boost the efficiency of far-field WPT. Nevertheless, potential negative impacts due to high electromagnetic fields (EMF) exposure for radiation-susceptible users have not been thoroughly considered in the design of WPT for EH-IoT with IoT application-level requirements (e.g., coverage). In this article, we explore the health-aware beamforming and IoT selection problem under the EH and human safety constraints. First, we formulate a new optimization problem Health-Aware Beamforming and IoT Selection (HABIS) and prove the NP-hardness. Second, we design an approximation algorithm, named Minimum Radiation Exposure and Maximum IoT Coverage (MREMIC), to exploit the EH-health dependency (EHHD) graph for properly addressing the trade-off between EH efficiency and potential EMF radiation exposure to human bodies. We also discover the optimal health-aware beamforming to minimize the total radiation energy absorption of humans. Simulation results show that MREMIC can effectively charge IoT devices and significantly outperforms existing EH approaches by more than 200% regarding human safety.}
}


@article{DBLP:journals/tmc/YangLZ24,
	author = {Zhenyu Yang and
                  Yantao Li and
                  Gang Zhou},
	title = {Unsupervised Sensor-Based Continuous Authentication With Low-Rank
                  Transformer Using Learning-to-Rank Algorithms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8839--8854},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3353209},
	doi = {10.1109/TMC.2024.3353209},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the Internet of Things (IoTs) and mobile communications, mobile devices have become indispensable in our daily lives. Given the substantial amount of private information stored on these devices, the security of mobile devices has emerged as a significant concern for users. Different from conventional methods such as PINs, fingerprints, and face IDs, which authenticate users only during the initial login stage, continuous authentication ensures consistent verification while mobile devices are in use. Current continuous authentication methods require extensive data from a series of users for effective training. Nevertheless, it is challenging to collect sufficient amount of data within a limited time. In this paper, we propose CALL, an unsupervised sensor-based Continuous Authentication system with a Low-rank transformer using Learning-to-rank algorithms. The lightweight CALL is capable of providing both spatial and temporal features for end-to-end authentication. Specifically, CALL utilizes time series data from a legitimate user, collected by the accelerometer, gyroscope, and magnetometer sensors on smartphones, to train a pure one-dimensional autoencoder for spatial features and a shuffle low-rank Transformer (SLRT) for temporal features in the training phase. In the authentication phase, the trained pure one-dimensional autoencoder captures spatial features by reconstructing input data to obtain the reconstruction error, and SLRT captures temporal features by predicting a ranking vector that reveals the order of the shuffled feature sequence. The predicted ranking vector is then used to recover the shuffled sequence and the similarity between the frequency spectrum sequences of the recovered sequence and the original time series data is calculated. The reconstruction error and similarity are compared against pre-defined thresholds, and CALL authenticates a user as legitimate only if both values fall below their respective thresholds. Finally, we evaluate the performance of CALL on UCI_HAR, WISDM_HARB, and our dataset, and the extensive experiments illustrate that CALL reaches the best performance with 96.43%, 95.24% and 96.92% accuracy, and 4.28%, 4.76% and 3.86% EERs on the three datasets, outperforming state-of-the-art continuous authentication methods.}
}


@article{DBLP:journals/tmc/OliveiraFCD24,
	author = {Ant{\'{o}}nio J. Oliveira and
                  Bruno M. Ferreira and
                  Nuno Alexandre Cruz and
                  Roee Diamant},
	title = {Probabilistic Positioning of a Mooring Cable in Sonar Images for In-Situ
                  Calibration of Marine Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8855--8868},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3354126},
	doi = {10.1109/TMC.2024.3354126},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/OliveiraFCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The calibration of sensors stationed along a cable in marine observatories is a time-consuming and expensive operation that involves taking the mooring out of the water periodically. In this paper, we present a method that allows an underwater vehicle to approach a mooring, in order to take reference measurements along the cable for in-situ sensor calibration. We use the vehicle's Mechanically Scanned Imaging Sonar (MSIS) to identify the cable's reflection within the sonar image. After pre-processing the image to remove noise, enhance contour lines, and perform smoothing, we employ three detection steps: 1) selection of regions of interest that fit the cable's reflection pattern, 2) template matching, and 3) a track-before-detect scheme that utilized the vehicle's motion. The later involves building a lattice of template matching responses for a sequence of sonar images, and using the Viterbi algorithm to find the most probable sequence of cable locations that fits the maximum speed assumed for the surveying vessel. Performance is explored in pool and sea trials, and involves an MSIS onboard an underwater vehicle scanning its surrounding to identify a steel-core cable. The results show a sub-meter accuracy in the multi-reverberant pool environment and in the sea trial. For reproducibility, we share our implementation code.}
}


@article{DBLP:journals/tmc/ChoCB24,
	author = {In{-}Sop Cho and
                  Chao Chen and
                  Seungjun Baek},
	title = {Optimal Scheduling for Uncoded and Coded Multicast in Millimeter Wave
                  Networks Leveraging Directionality and Reflections},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8869--8885},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3355526},
	doi = {10.1109/TMC.2024.3355526},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChoCB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the minimum-delay multicast scheduling problem for millimeter wave (mmWave) networks. Salient characteristics of mmWave links, directionality and reflections, are considered under sectored antenna model. We first consider the model where the signal is received at a single Direction-of-Arrival (DoA) with the highest SNR at each node. We identify the property such that the optimal policy can be recursively partitioned into smaller sizes and propose an iterative method based on graphs which finds the optimal schedule in polynomial time. Next, we extend our model where a node leverages signals received at multiple DoAs through reflections. We introduce the concept of receiving direction diversity (RDD) which states that the availability of multiple receiving directions enables opportunistic reduction of multicast delay. We prove NP-hardness of the problem, and propose approximations with performance bounds and heuristics of reduced complexity. Next, we consider multicast scheduling with rateless codes (RCs) which reduces delay by flexible packet reception. For both cases of coded multicast with and without RDD, we formulate linear programming problems and propose greedy algorithms with nearly optimal performance and reduced complexity. By simulation we show the outperformance of our method over conventional ones, and numerically characterize the gain of RDD and RCs.}
}


@article{DBLP:journals/tmc/LiTYLGWLCR24,
	author = {Weimin Li and
                  Weihong Tian and
                  Zhengmao Yan and
                  Zitong Li and
                  Jie Gao and
                  Fan Wu and
                  Jianxun Liu and
                  Wenxiong Chen and
                  Ju Ren},
	title = {CoralDB: {A} Collaborative Database for Data Sharing Based on Permissioned
                  Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8886--8901},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357499},
	doi = {10.1109/TMC.2024.3357499},
	timestamp = {Tue, 11 Feb 2025 07:46:00 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiTYLGWLCR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Systems that integrate distributed databases and existing blockchain platforms have recently emerged, which conveniently leverage their respective strengths to build efficient, secure, and usable data sharing and collaboration environments for different organizations. However, the performance of such systems can be limited by the native blockchain platforms due to the high latency of transactions. In this article, we present CoralDB, a bottom-up fully redesigned hybrid system of blockchain and database, aimed at enabling untrusted organizations to collaborate and share data efficiently and securely at the database level. The storage layer of CoralDB ensures data security and system throughput through key modules such as customized block structure, consensus mechanism, and transaction pool. On top of the storage layer, a database layer is introduced, which extends the blockchain of the storage layer by incorporating connection pools, collaborative tables, and query interfaces, to enhance the usability and efficiency of data collaboration and sharing. Extensive experimental results demonstrate that CoralDB provides security assurances at the level of blockchain and enables efficient decentralized data collaboration and sharing.}
}


@article{DBLP:journals/tmc/DuLNKXHM24,
	author = {Hongyang Du and
                  Zonghang Li and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Huawei Huang and
                  Shiwen Mao},
	title = {Diffusion-Based Reinforcement Learning for Edge-Enabled AI-Generated
                  Content Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8902--8918},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3356178},
	doi = {10.1109/TMC.2024.3356178},
	timestamp = {Sun, 19 Jan 2025 14:43:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuLNKXHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Metaverse emerges as the next-generation Internet paradigm, the ability to efficiently generate content is paramount. AI-Generated Content (AIGC) emerges as a key solution, yet the resource-intensive nature of large Generative AI (GAI) models presents challenges. To address this issue, we introduce an AIGC-as-a-Service (AaaS) architecture, which deploys AIGC models in wireless edge networks to ensure broad AIGC services accessibility for Metaverse users. Nonetheless, an important aspect of providing personalized user experiences requires carefully selecting AIGC Service Providers (ASPs) capable of effectively executing user tasks, which is complicated by environmental uncertainty and variability. Addressing this gap in current research, we introduce the AI-Generated Optimal Decision (AGOD) algorithm, a diffusion model-based approach for generating the optimal ASP selection decisions. Integrating AGOD with Deep Reinforcement Learning (DRL), we develop the Deep Diffusion Soft Actor-Critic (D2SAC) algorithm, enhancing the efficiency and effectiveness of ASP selection. Our comprehensive experiments demonstrate that D2SAC outperforms seven leading DRL algorithms. Furthermore, the proposed AGOD algorithm has the potential for extension to various optimization problems in wireless networks, positioning it as a promising approach for future research on AIGC-driven services.}
}


@article{DBLP:journals/tmc/YeF24a,
	author = {Xiaowen Ye and
                  Liqun Fu},
	title = {Joint Codebook Selection and {UE} Scheduling for Unlicensed MmWave
                  NR-U/WiGig Coexistence Based on Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8919--8934},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3356442},
	doi = {10.1109/TMC.2024.3356442},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YeF24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unlicensed millimeter-wave (mmWave) communication is a promising technique for the New Radio-based access to Unlicensed spectrum (NR-U) network to guarantee the ever-increasing data rate demand. A critical challenge of NR-U in unlicensed mmWave bands is to maintain equitable and harmonious coexistence with the original Wireless Gigabit (WiGig) network. In this article, we develop an intelligent joint codebook selection and user equipment (UE) scheduling scheme for mmWave NR-U and WiGig coexistence networks. Specifically, we first formulate the joint problem as a two-time scale system, wherein the codebook selection is performed on the large-time scale whilst the UE scheduling is optimized on the small-time scale. To address the multi-time scale issue, we put forth a new deep reinforcement learning (DRL) algorithm that enables operations on different time scales to benefit each other towards the target system objective, referred to as layered deep Q-network (L-DQN). Thereafter, with the judicious definitions of the state, action, and reward in L-DQN paradigms, we propose the Deep reinforcement learning based CodeBook selection and UE scheduling (DeepCBU) scheme. DeepCBU aims to attain different trade-offs between two conflicting goals, i.e., i) maximizing the total data rate of NR-U with as little interference to WiGig as possible and ii) guaranteeing the fairness among UEs, e.g., the quality of service (QoS) requirement of each UE. To fulfill this mission, we modify the conventional deep neural network architecture of DeepCBU by introducing the target branch for each objective. The gist is that different target branches evaluate the contribution of DeepCBU's strategy to different goals, and the decision of DeepCBU is determined by all target branches in a weighted fashion. Simulation results demonstrate that compared with DRL-dirLBT, TS-dirLBT, and TS-DRL schemes, DeepCBU is more Pareto efficient even without any prior network knowledge, e.g., UE mobility, random channel fading, and transmissions of WiGig, in terms of the data rate of NR-U, the data rate of WiGig, and the number of satisfied UEs. Furthermore, DeepCBU is robust to miscellaneous QoS requirement setups.}
}


@article{DBLP:journals/tmc/WangCZL24,
	author = {Kun Wang and
                  Jiani Cao and
                  Zimu Zhou and
                  Zhenjiang Li},
	title = {SwapNet: Efficient Swapping for {DNN} Inference on Edge {AI} Devices
                  Beyond the Memory Budget},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8935--8950},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3355764},
	doi = {10.1109/TMC.2024.3355764},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangCZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends, and hardware architectures of edge AI devices. We further showcase the utility of SwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference tasks in three applications demonstrate that SwapNet achieves almost the same latency as the case with sufficient memory even when DNNs demand 2.32\\times \\,\\sim\n5.81\\times\nmemory beyond the available budget. The design of SwapNet also provides novel and feasible insights for deploying large language models (LLMs) on edge AI devices in the future.}
}


@article{DBLP:journals/tmc/MiaoMYZ24,
	author = {Wang Miao and
                  Geyong Min and
                  Zhengxin Yu and
                  Xu Zhang},
	title = {Performance Analytical Modeling of Mobile Edge Computing for Mobile
                  Vehicular Applications: {A} Worst-Case Perspective},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8951--8964},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3356443},
	doi = {10.1109/TMC.2024.3356443},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MiaoMYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantitative performance analysis plays a pivotal role in theoretically investigating the performance of Vehicular Edge Computing (VEC) systems. Although considerable research efforts have been devoted to VEC performance analysis, all of the existing analytical models were designed to derive the average system performance, paying insufficient attention to the worst-case performance analysis, which hinders the practical deployment of VEC systems to support mission-critical vehicular applications, such as collision avoidance. To bridge this gap, we develop an original performance analytical model by virtue of Stochastic Network Calculus (SNC) to investigate the worst-case end-to-end performance of VEC systems. Specifically, to capture the bursty feature of task generation, an innovative bivariate Markov Chain is first established and rigorously analysed to derive the stochastic task envelope. Then, an effective service curve is created to investigate the severe resource competition among vehicular applications. Driven by the stochastic task envelope and effective service curve, a closed-form end-to-end analytical model is derived to obtain the latency bound for VEC systems. Extensive simulation experiments are conducted to validate the accuracy of the proposed analytical model under different system configurations. Furthermore, we exploit the proposed analytical model as a cost-effective tool to investigate the resource allocation strategies in VEC systems.}
}


@article{DBLP:journals/tmc/WuCS24,
	author = {Kefeng Wu and
                  Kwan{-}Wu Chin and
                  Sieteng Soh},
	title = {Multi-UAVs Network Design Algorithms for Computed Rate Maximization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8965--8980},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3355391},
	doi = {10.1109/TMC.2024.3355391},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers a network design problem using Unmanned Aerial Vehicles (UAVs). It aims to create a network to provide communication and computation service to a set of source-destination ground node pairs. The main performance metric is the minimum amount of computed data among a set of source-destination pairs. To optimize this metric, we outline two mixed Integer Linear Programs (MILPs), namely S-MILP and NS-MILP, which are designed respectively for splittable and non-splittable traffic flow models. They jointly optimize the placement of UAVs, assignment of Virtualized Network Functions (VNFs), and routing of unprocessed and processed flow. Further, NS-MILP optimizes the path selection of each source-destination pair. A key challenge is that these MILPs require an exhaustive collection of network topologies. To this end, this paper outlines two heuristic algorithms, called Resource-Aware Location Selection (RALS) and Resource-Aware Path and Location Selection (RAPLS), respectively for each traffic flow model. The simulation results show that RALS and RAPLS achieve on average 83% and 80% of the amount of computed flow of S-MILP and NS-MILP, respectively. Lastly, RALS and RAPLS require 45% and 53% less computation time as compared to S-MILP and NS-MILP, respectively.}
}


@article{DBLP:journals/tmc/TongGTLLQ24,
	author = {Xinyu Tong and
                  Weiping Ge and
                  Yichen Tian and
                  Zijuan Liu and
                  Xiulong Liu and
                  Wenyu Qu},
	title = {NNE-Tracking: {A} Neural Network Enhanced Framework for Device-Free
                  Wi-Fi Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8981--8998},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357569},
	doi = {10.1109/TMC.2024.3357569},
	timestamp = {Thu, 22 Aug 2024 20:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/TongGTLLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution of Wi-Fi to next-generation 802.11bf demonstrates the potential of device-free Wi-Fi sensing applications, where we can remotely infer the behaviors of users without bringing into physical contact with them. Among these sensing applications, Wi-Fi tracking is critical to provide location based services. Recent Wi-Fi tracking systems can be cataloged into model-based and data-based approaches: (1) the model-based approach is to build the mathematical tracking model. However, this method is sensitive to environmental noise, and spends more execution time; (2) the data-based approach is to train a neural network. However, this method requires a lot of efforts to collect training dataset, and cannot handle all types of trajectories well. To resolve these issues, we propose the NNE-Tracking, a Neural Network Enhanced tracking framework. The core design principle of NNE-Tracking is as follows: we improve the tracking accuracy based on the data-based approach, and utilize the model-based approach to supervise whether the neural network is already working well. Moreover, we also design a framework to estimate unknown parameters of the tracking model, so that the system can automatically generate the Wi-Fi map. We take the Wi-Fi passive tracking as a specific example to explain how to apply NNE-Tracking in practical applications. Experimental results demonstrate that our design can reduce 59.4\\% \\sim 85.3\\% tracking errors while significantly saving execution time. As for deployment costs, we can automatically infer the Wi-Fi map without manual calibration; As for stability, when we repeat the training process with different hidden layers and random seeds, the tracking standard deviation of these neural networks is only \\text{1.4}\\,\\text{cm}.}
}


@article{DBLP:journals/tmc/YangYWSZWX24,
	author = {Chen Yang and
                  Jinliang Yuan and
                  Yaozong Wu and
                  Qibo Sun and
                  Ao Zhou and
                  Shangguang Wang and
                  Mengwei Xu},
	title = {Communication-Efficient Satellite-Ground Federated Learning Through
                  Progressive Weight Quantization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {8999--9011},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3358804},
	doi = {10.1109/TMC.2024.3358804},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangYWSZWX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large constellations of Low Earth Orbit (LEO) satellites have been launched for Earth observation and satellite-ground communication, which collect massive imagery and sensor data. These data can enhance the AI capabilities of satellites to address global challenges such as real-time disaster navigation and mitigation. Prior studies proposed leveraging federated learning (FL) across satellite-ground to collaboratively train a share machine learning (ML) model in a privacy-preserving mechanism. However, they mostly focus on single unique challenges such as limited ground-to-satellite bandwidth, short connection window, and long connection cycle, while ignoring the completeness of these challenges in deploying efficient FL frameworks in space. In this paper, we propose an efficient satellite-ground FL framework, SatelliteFL, to address these three challenges collectively. Its key idea is to ensure that each satellite must complete per-round training within each connection window. Moreover, we design a progressive block-wise quantization algorithm that determines a unique bitwidth for each block of the ML model to maximize the model utility while not exceeding the connection window. We evaluate SatelliteFL by plugging an implemented FL platform into real-world satellite networks and satellite images. The results show that SatelliteFL highly accelerates the convergence by up to 2.8× and improves the bandwidth utilization ratio by up to 9.3× compared to the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/LinCLKG24,
	author = {Deyu Lin and
                  Zhijie Chen and
                  Xuan Liu and
                  Linghe Kong and
                  Yong Liang Guan},
	title = {{ESWCM:} {A} Novel Energy-sustainable Approach for SWIPT-enabled {WSN}
                  with Constrained {MEAP} Configurations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {9012--9028},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357771},
	doi = {10.1109/TMC.2024.3357771},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinCLKG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combination with the Simultaneous Wireless Information and Power Transfer (SWIPT) technology is expected as a promising solution to the issue of energy constraint in Wireless Sensor Networks (WSN). However, little attention is paid to the energy sustainability in SWIPT-enabled WSN with limited Mobile Energy Access Point (MEAP) configurations. To this end, a novel Energy-sustainable approach for SWIPT-enabled WSN with Constrained MEAP (ESWCM) configurations is proposed in this article. To be specific, an Optimal Ring Width Determination (ORWD) algorithm is proposed, with the aim to ensure all the sensor nodes lie in the energy coverage of limited MEAPs from the perspective of the entire network. Subsequently, a GA-based Clustering algorithm constrained by cluster Head (GCH) connectivity is presented to promote the survivability of Cluster Heads (CHs) and the effectiveness of data transmission through reasonable cluster size. Additionally, the optimal parameters of each sensor node are determined through an Energy-efficient Parameter Optimization (EPO) algorithm to achieve energy sustainability of each individual sensor node. Experimental results indicate that ESWCM exhibits higher node survival rate by 124.7%, 2%, 29.9%, and 40.5% compared with LEACH, S-LEACH, R-LEACH, and DEEC after 1200 iterations respectively. Moreover, it also shows a survival rate of 100% after adjustments.}
}


@article{DBLP:journals/tmc/LiaoTGWL24,
	author = {Hanlong Liao and
                  Guoming Tang and
                  Deke Guo and
                  Kui Wu and
                  Lailong Luo},
	title = {EV-Assisted Computing for Energy Cost Saving at Edge Data Centers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {9},
	pages = {9029--9041},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3358890},
	doi = {10.1109/TMC.2024.3358890},
	timestamp = {Sun, 19 Jan 2025 14:43:32 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiaoTGWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-distributed edge data centers (EDCs) are expected to handle a large portion of tasks offloaded from cloud data centers for various emerging edge services. However, the high energy consumption and cost add a huge burden to edge service providers (ESPs). This presents a unique challenge as traditional energy-saving strategies applicable in cloud data centers fail to apply to EDCs, given the latency-sensitive nature of edge services. In response, we put forward an innovative electric vehicle (EV)-assisted edge computing architecture that leverages idle computing resources and stored energy of EVs. Our design aims to decrease energy expenditures for ESPs by choosing EVs with more economical service costs to handle a portion of the edge services during critical periods. We construct an energy cost-aware workload offloading model and discretize the original model into multiple small-scale solvable forms in both temporal and spatial dimensions. Furthermore, we reconfigure the Kuhn-Munkres algorithm to produce an online joint matching solution to counter QoS decline, generating a mutually advantageous situation for ESPs and EV participants. Upon experimentation with real-world traces, our design demonstrates a significant reduction in total energy cost (up to 31%) and offers considerable incentives for EV participants.}
}


@article{DBLP:journals/tmc/ZhangZLLL24,
	author = {Ziyang Zhang and
                  Yang Zhao and
                  Huan Li and
                  Changyao Lin and
                  Jie Liu},
	title = {{DVFO:} Learning-Based {DVFS} for Energy-Efficient Edge-Cloud Collaborative
                  Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9042--9059},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357218},
	doi = {10.1109/TMC.2024.3357218},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to limited resources on edge and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and end-to-end latency. In addition to dynamic voltage frequency scaling (DVFS) technique, edge-cloud architecture provides a collaborative approach for efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which co-optimizes DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) the CPU, GPU and memory frequencies of edge devices, and 2) the offloaded feature map. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatial-channel attention mechanism to identify the less important DNN feature map for efficient offloading. This approach improves inference performance for different DNN models under various edge-cloud network conditions. Extensive evaluations using two datasets and six widely-deployed DNN models on five heterogeneous edge devices show that DVFO significantly reduces the energy consumption by 33% on average, compared to state-of-the-art schemes. Moreover, DVFO achieves up to 28.6%\\sim59.1% end-to-end latency reduction, while maintaining accuracy within 1% loss on average.}
}


@article{DBLP:journals/tmc/LiLFHWL24,
	author = {Hui Li and
                  Xiuhua Li and
                  Qilin Fan and
                  Qiang He and
                  Xiaofei Wang and
                  Victor C. M. Leung},
	title = {Distributed {DNN} Inference With Fine-Grained Model Partitioning in
                  Mobile Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9060--9074},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357874},
	doi = {10.1109/TMC.2024.3357874},
	timestamp = {Sun, 06 Oct 2024 21:41:31 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLFHWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model partitioning is a promising technique for improving the efficiency of distributed inference by executing partial deep neural network (DNN) models on edge servers (ESs) or Internet-of-Things (IoT) devices. However, due to heterogeneous resources of ESs and IoT devices in mobile edge computing (MEC) networks, it is non-trivial to guarantee the DNN inference speed to satisfy specific delay constraints. Meanwhile, many existing DNN models have a deep and complex architecture with numerous DNN blocks, which leads to a huge search space for fine-grained model partitioning. To address these challenges, we investigate distributed DNN inference with fine-grained model partitioning, with collaborations between ESs and IoT devices. We formulate the problem and propose a multi-task learning based asynchronous advantage actor-critic approach to find a competitive model partitioning policy that reduces DNN inference delay. Specifically, we combine the shared layers of actor-network and critic-network via soft parameter sharing, and expand the output layer into multiple branches to determine the model partitioning policy for each DNN block individually. Experiment results demonstrate that the proposed approach outperforms state-of-the-art approaches by reducing total inference delay, edge inference delay and local inference delay by an average of 4.76%, 10.04% and 8.03% in the considered MEC networks.}
}


@article{DBLP:journals/tmc/TomarT24,
	author = {Ashish Tomar and
                  Sachin Tripathi},
	title = {A Chebyshev Polynomial-Based Authentication Scheme Using Blockchain
                  Technology for Fog-Based Vehicular Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9075--9089},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357599},
	doi = {10.1109/TMC.2024.3357599},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TomarT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing number of vehicles has resulted in a tremendous growth of data in vehicular communications. Cloud-based models are inefficient for handling this large data due to high latency and bandwidth requirements. To address this, fog computing-based vehicular network models have been proposed for low latency and immediate responses. However, securing data transfer between fog servers and vehicles on public channels is essential to prevent malicious attacks. Existing authenticated key agreement schemes provide security but often incur high computational costs and vulnerability to attacks. Thus, this paper proposes a blockchain-based authenticated key agreement scheme for the fog computing-enabled vehicular network. The proposed scheme integrates blockchain into fog-based Vehicular Ad-hoc Network (VANET), where fog servers and the cloud servers serve as blockchain nodes for seamless re-authentication of a moving vehicle. For secure communication, the proposed scheme uses the Chebyshev polynomial to achieve low computational cost and establishes a common session key between cloud server, fog server, and vehicle. The formal security proof of the proposed scheme is carried out using Real-Or-Random (ROR) model. Finally, the Hyperledger Fabric and cryptographic libraries have been used for the experimental analysis to demonstrate the proposed scheme's communication and computational efficiency.}
}


@article{DBLP:journals/tmc/WangYWYWN24,
	author = {Ran Wang and
                  Xue Yu and
                  Qiang Wu and
                  Changyan Yi and
                  Ping Wang and
                  Dusit Niyato},
	title = {Efficient Deployment of Partial Parallelized Service Function Chains
                  in CPU+DPU-Based Heterogeneous {NFV} Platforms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9090--9107},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357796},
	doi = {10.1109/TMC.2024.3357796},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangYWYWN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The introduction of network function virtualization (NFV) leads to service function chain (SFC) deployment problems, promoting the idea of composing network services as virtualized network functions (VNFs). Meanwhile, the rapid development of edge computing, artificial intelligence and Big Data has led to a surge in data volume and explosive growth in computing and forwarding demands. As such, a traditional central processing unit (CPU)-based data forwarding mode in the NFV network appears to be a bottleneck, and a CPU-only computing framework can no longer meet the forwarding needs of diverse business scenarios and services. The data processing unit (DPU)-based architecture allows better forwarding performance to be achieved more cost-effectively, largely alleviating the computing pressure of the CPU and reducing the node forwarding delay. Therefore, in this paper, a heterogeneous CPU+DPU architecture is investigated to solve the SFC deployment problem. To handle diverse service needs, we establish a multi-objective SFC deployment scheme to optimize the service latency, deployment cost and service acceptance rate. Because extreme services require better real-time performance, DPUs are adopted for fast processing according to the requirement of service requests. To address the unacceptable delay in sequential mode, a parallel strategy is proposed to process SFCs. To solve the multi-objective SFC deployment problem, a deep reinforcement learning (DRL)-based heterogeneous algorithm that includes multiple subalgorithms is designed, named parallelizable, shared and horizontally scaled service function chain deployment (PSHD), which uses diverse processing algorithms to deploy SFCs and break the delay bottleneck in NFV-based networks. The performance of PSHD is evaluated through extensive experiments. PSHD is found to be time-efficient, and it achieves a higher request acceptance rate and 37.73% and 34.26% lower latencies than state-of-the-art methods.}
}


@article{DBLP:journals/tmc/LiZZDNM24,
	author = {Xingwang Li and
                  Yike Zheng and
                  Jianhua Zhang and
                  Shuping Dang and
                  Arumugam Nallanathan and
                  Shahid Mumtaz},
	title = {Finite {SNR} Diversity-Multiplexing Trade-Off in Hybrid ABCom/RCom-Assisted
                  {NOMA} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9108--9119},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3357753},
	doi = {10.1109/TMC.2024.3357753},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiZZDNM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The upcoming sixth generation (6G) driven Internet-of-Things (IoT) will face the great challenges of extremely low power demand, high transmission reliability and massive connectivities. To meet these requirements, we propose a novel hybrid ambient backscatter communication (ABCom) or relay communication (RCom)assisted non-orthogonal multiple access (NOMA) network, which simultaneously enables traditional relay networks and ABCom-assisted IoT networks. Specifically, we investigate the reliability and the finite signal-to-noise ratio (SNR) diversity-multiplexing trade-off (f-DMT) of the proposed system to characterize the outage performance of the proposed system in the non-asymptotic SNR region. We derive the outage probability (OP) and the finite SNR diversity gain when two sources aim to communicate through either ABCom or RCom. On the basis that the results of Monte Carlo simulation and analysis are in perfect agreement, we discover that in the high SNR regime, the OP for ABCom tends to be a constant, leading to a zero diversity gain and an error floor, while the OP for RCom is monotone decreasing with respect to the SNR. Also, compared with the imperfect successive interference cancellation (ipSIC) mode, the reliability of the system under the ideal condition is significantly improved; Moreover, in the lower multiplexing gain regime, for both ABCom and RCom, the higher finite SNR diversity gain results in better system reliability, which provides good opportunities for ABCom to adapt f-DMT and improve relevant performance metrics by adapting the reflection parameter.}
}


@article{DBLP:journals/tmc/ZhangSZLLG24,
	author = {Xing Zhang and
                  Wei Sun and
                  Jin Zheng and
                  Anping Lin and
                  Jian Liu and
                  Shuzhi Sam Ge},
	title = {Wi-Fi-Based Indoor Localization With Interval Random Analysis and
                  Improved Particle Swarm Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9120--9134},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359669},
	doi = {10.1109/TMC.2024.3359669},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangSZLLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of the Internet of Things has spurred the growth of wireless applications, particularly Wi-Fi-based indoor localization, which is gaining prominence owing to its cost-effectiveness. Nevertheless, the accuracy of Wi-Fi-based indoor localization is hindered by signal instability. To address this limitation, we introduce an interval random analysis approach for uncertain Wi-Fi-based indoor localization. Specifically, this approach employs an interval random parameter lognormal shadowing model for radio map enhancement and adaptive Bayesian comprehensive learning (IRPLS-ABCL) particle swarm optimization (PSO) for location estimation accuracy enhancement. The process comprises two stages: offline training and online localization. During the offline phase, we establish the interval random parameter lognormal shadowing model, considering the parameters as interval random variables, rather than precise values, in a sparse reference point scenario. In the online phase, we use a double-panel fingerprint homogeneity model to assess fingerprint similarity and apply the adaptive Bayesian comprehensive learning PSO algorithm to enhance localization precision. The experimental results show that the proposed algorithm can achieve the best performance in terms of localization accuracy based on the predicted average received signal strength (RSS), reaching 1.89 m.}
}


@article{DBLP:journals/tmc/ZhouYZDX24,
	author = {Jian Zhou and
                  Qi Yang and
                  Lu Zhao and
                  Haipeng Dai and
                  Fu Xiao},
	title = {Mobility-Aware Computation Offloading in Satellite Edge Computing
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9135--9149},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359759},
	doi = {10.1109/TMC.2024.3359759},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouYZDX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite edge computing, as an extension of ground edge computing, is a key technology for achieving seamless global computing coverage. However, the low earth orbit (LEO) satellites have limited computing resources and are moving at a high speed. This naturally poses a challenge to find more suitable computation offloading strategies with minimum network latency and energy consumption, especially when a large number of co-existing users are to offload their tasks. In this paper, therefore, we mainly focus on computation offloading in the satellite edge computing network (SECN) by jointly considering LEO satellites’ mobility and SECN's heterogeneous resource constraints to explore more practical computation offloading strategies. We first formulate the problem of Mobility-aware Computation Offloading (MCO) in the SECN via specifying the effect of LEO satellites’ high-speed movement on the computation offloading, aiming to minimize the network latency and energy consumption. Considering the MCO problem is discrete and non-convex as the objective function and constraints are associated with the binary decision variables. We then convert the original non-convex problem into a continuous convex problem which is proved to be feasible. To avoid a high computational complexity incurred by the extensive co-existing user offloading, we design MCO-A, a distributed algorithm based on ADMM (alternating direction method of multipliers) to solve the MCO problem efficiently. Finally, the performance of MCO-A is evaluated via extensive experiments including small-scale and large-scale scenarios. The experimental results show that MCO-A can achieve a lower network latency and energy consumption in an efficient way compared with the baseline and state-of-the-art approaches.}
}


@article{DBLP:journals/tmc/VillaMRBJPM24,
	author = {Davide Villa and
                  Miead Tehrani Moayyed and
                  Clifton Paul Robinson and
                  Leonardo Bonati and
                  Pedram Johari and
                  Michele Polese and
                  Tommaso Melodia},
	title = {Colosseum as a Digital Twin: Bridging Real-World Experimentation and
                  Wireless Network Emulation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9150--9166},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359596},
	doi = {10.1109/TMC.2024.3359596},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/VillaMRBJPM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless network emulators are being increasingly used for developing and evaluating new solutions for Next Generation (NextG) wireless networks. However, the reliability of the solutions tested on emulation platforms heavily depends on the precision of the emulation process, model design, and parameter settings. To address, obviate, or minimize the impact of errors of emulation models, in this work, we apply the concept of Digital Twin (DT) to large-scale wireless systems. Specifically, we demonstrate the use of Colosseum, the world's largest wireless network emulator with hardware-in-the-loop, as a DT for NextG experimental wireless research at scale. As proof of concept, we leverage the Channel emulation scenario generator and Sounder Toolchain (CaST) to create the DT of a publicly available over-the-air indoor testbed for sub-6 GHz research, namely, Arena. Then, we validate the Colosseum DT through experimental campaigns on emulated wireless environments, including scenarios concerning cellular networks and jamming of Wi-Fi nodes, on both the real and digital systems. Our experiments show that the DT is able to provide a faithful representation of the real-world setup, obtaining an average similarity of up to 0.987 in throughput and 0.982 in Signal to Interference plus Noise Ratio (SINR).}
}


@article{DBLP:journals/tmc/AsheralievaN24,
	author = {Alia Asheralieva and
                  Dusit Niyato},
	title = {Multi-Access Edge Computing for Real-Time Applications With Sporadic
                  {DAG} Tasks - {A} Graphical Game Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9167--9190},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3362912},
	doi = {10.1109/TMC.2024.3362912},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AsheralievaN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a multi-operator multi-access edge computing (MEC) network for applications with dependent tasks. Each task includes jobs executed based on logical precedence modelled as a directed acyclic graph, where each vertex is a job, each edge – precedence constraint, such that the job can be started only after its preceding jobs are completed. Tasks are executed by MEC servers with the assistance of workers – nearby edge devices. Each MEC server acts as a master deciding on jobs assigned to its workers. The master's decision problem is complex, as its workers can be associated with other masters in proximity. Thus, the available workers’ resources depend on job assignments of all neighboring masters. Yet, as masters select their decisions simultaneously, no master knows concurrent decisions of its neighbors. Besides, some masters can belong to competing operators that have no incentives to exchange information about their decisions. To address these challenges, we formulate a novel framework based on the graphical stochastic Bayesian game, where masters play under uncertainty about their neighbors’ decisions. We prove that the game admits a perfect Bayesian equilibrium (PBE), and develop new Bayesian reinforcement learning and Bayesian deep reinforcement learning algorithms enabling each master to reach the PBE independently.}
}


@article{DBLP:journals/tmc/CilfoneDF24,
	author = {Antonio Cilfone and
                  Luca Davoli and
                  Gianluigi Ferrari},
	title = {LoRa Meets {IP:} {A} Container-Based Architecture to Virtualize LoRaWAN
                  End Nodes},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9191--9207},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359150},
	doi = {10.1109/TMC.2024.3359150},
	timestamp = {Sun, 19 Jan 2025 14:43:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CilfoneDF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, a container-based architecture for the integration of Long Range Wide Area Network (LoRaWAN) end nodes—e.g., used to monitor industrial machines or mobile entities in specific environments—with Internet Protocol (IP)-based networks is proposed and its performance is investigated. To this end, we exploit the native service and resource discovery support of the Constrained Application Protocol (CoAP), as well as its light traffic requirements, owing to its use of User Datagram Protocol (UDP) rather than Transmission Control Protocol (TCP). This approach (i) adapts transparently (with no impact) to both private and public LoRaWAN networks, (ii) enables seamless interaction between LoRaWAN-based and CoAP-based nodes, through a logical “virtualization” of LoRaWAN nodes at server side, and (iii) enables routing among LoRaWAN end nodes, overcoming LoRaWAN's absence of inter-node communication and lack of compliance (at the end nodes’ side) with IP. Two virtualization approaches are proposed: (i) virtualization of a single end node (represented as a CoAP server) per container and (ii) virtualization of multiple end nodes (as CoAP servers) per container. Finally, deployments of the proposed virtualization architectures, using both a laptop and an Internet of Things (IoT) device (e.g., a Raspberry Pi), are considered, highlighting how the best solution relies on the use of several containers, with more than one CoAP server per container.}
}


@article{DBLP:journals/tmc/ZhouZJZS24,
	author = {Ruiting Zhou and
                  Yifan Zeng and
                  Lei Jiao and
                  Yi Zhong and
                  Liujing Song},
	title = {Online and Predictive Coordinated Cloud-Edge Scrubbing for DDoS Mitigation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9208--9223},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3360077},
	doi = {10.1109/TMC.2024.3360077},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouZJZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To mitigate Distributed Denial-of-Service (DDoS) attacks towards enterprise networks, we study the problem of scheduling DDoS traffic through on-premises scrubbing at the local edge and on-demand scrubbing in the remote clouds. We model this problem as a nonlinear mixed-integer program, which is characterized by the inputs of arbitrary dynamics and the trade-offs between staying at suboptimal scrubbing locations and using different best locations with switching overhead. We first design a prediction-oblivious online algorithm which consists of a carefully-designed fractional algorithm to pursue the long-term total cost minimization but avoid excessive switching overhead over time, and a randomized rounding algorithm to derive the flow-based, integral decisions. We next design a prediction-aware online algorithm which leverages the predicted inputs and can make even better scheduling decisions through invoking our prediction-oblivious online algorithm and improving its solutions via re-solving the original problem slice over each prediction window. We further extend our study to prioritize local scrubbing, and adapt our algorithms to this case correspondingly. Then, we rigorously prove the worst-case, constant competitive performance guarantees of our online algorithms. Finally, we conduct extensive evaluations and validate the superiority of our approach over multiple existing alternatives approaches.}
}


@article{DBLP:journals/tmc/LinZDCGHF24,
	author = {Zheng Lin and
                  Guangyu Zhu and
                  Yiqin Deng and
                  Xianhao Chen and
                  Yue Gao and
                  Kaibin Huang and
                  Yuguang Fang},
	title = {Efficient Parallel Split Learning Over Resource-Constrained Wireless
                  Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9224--9239},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3359040},
	doi = {10.1109/TMC.2024.3359040},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinZDCGHF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple edge devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and a large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of activations’ gradients for backpropagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabilities at edge devices, we jointly optimize subchannel allocation, power control, and cut layer selection to minimize the per-round latency. Simulation results show that the proposed EPSL framework significantly decreases the training latency needed to achieve a target accuracy compared with the state-of-the-art benchmarks, and the tailored resource management and layer split strategy can considerably reduce latency than the counterpart without optimization.}
}


@article{DBLP:journals/tmc/TangPW24,
	author = {Ming Tang and
                  Fu Peng and
                  Vincent W. S. Wong},
	title = {A Blockchain-Empowered Incentive Mechanism for Cross-Silo Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9240--9253},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3361089},
	doi = {10.1109/TMC.2024.3361089},
	timestamp = {Sun, 19 Jan 2025 14:43:29 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangPW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cross-silo federated learning (FL), organizations cooperatively train a global model with their local datasets. However, some organizations may act as free riders such that they only contribute a small amount of resources but can obtain a high-accuracy global model. Meanwhile, some organizations can be business competitors, and they do not trust each other or any third-party entity. In this work, our goal is to design a framework that motivates efficient cooperation among organizations without the coordination of a central entity. To this end, we propose a blockchain-empowered incentive mechanism framework for cross-silo FL. Under this incentive mechanism framework, we develop a distributed algorithm that enables organizations to achieve social efficiency, individual rationality, and budget balance without private information of the organizations. Our proposed algorithm has a proven convergence guarantee and empirically achieves a higher convergence rate than a benchmark method. Moreover, we propose a transaction minimization algorithm to reduce the number of transactions made among organizations in the blockchain. This algorithm is proven to achieve a performance no worse than twice the minimum value. The experimental results in a testbed show that our proposed framework enables organizations to achieve social efficiency within a relatively short iterative process.}
}


@article{DBLP:journals/tmc/YanZJCQL24,
	author = {Yuting Yan and
                  Sheng Zhang and
                  Yibo Jin and
                  Fangwen Cheng and
                  Zhuzhong Qian and
                  Sanglu Lu},
	title = {Spatial and Temporal Detection With Attention for Real-Time Video
                  Analytics at Edges},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9254--9270},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3361016},
	doi = {10.1109/TMC.2024.3361016},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YanZJCQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The detection of objects via neural networks plays a key role in various video analytics, but consumes huge resources. Due to the limited computing capability at edges, such real-time detections should be precisely used for the objects that need the most attention. Unfortunately, as the target objects keep moving, existing systems fail to consider both the distribution of objects and object movements over regions, and existing tracking mechanisms are easily affected by background content. Therefore, we propose spatial and temporal detection with attention for analytics, to increase the quality of detections for those targets. However, the attention shift over regions, the uncertainty of detections, and the constrained edge resources essentially hamper us from efficient analytics. We propose an adaptive partition planner to divide the frame into regions to achieve spatial attention. Afterwards, we design a detection planner to orchestrate the detection model temporally for each region by an online mechanism, via a queue-based adaptation. The spatial and temporal attention are integrated to maximize the accumulative detection accuracy. Via rigorous proof, both dynamic regret regarding detection accuracy and the real-time requirement for the video analytics are ensured. The testbed experiments confirm the superiority of our approach over multiple state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/FengHZZLG24,
	author = {Chuan Feng and
                  Pengchao Han and
                  Xu Zhang and
                  Qihan Zhang and
                  Yejun Liu and
                  Lei Guo},
	title = {Dependency-Aware Task Reconfiguration and Offloading in Multi-Access
                  Edge Cloud Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9271--9288},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3360978},
	doi = {10.1109/TMC.2024.3360978},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FengHZZLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access Edge Cloud (MEC) networks are powerful for providing emerging computation-intensive and latency-sensitive applications with low latency leveraging ubiquitous edge devices. These networks enable complex applications to be split into multiple components/subtasks and deployed among multiple edge servers with limited computation and communication resources. However, multiple subtasks within an application are dependent on each other. They cannot be executed in parallel, resulting in non-trivial resource waste when allocating resources to every subtask throughout the lifetime of the application. This article investigates the multi-component task offloading problem in MEC networks that addresses the dependencies among components and three-dimensional (3D) resource allocation, i.e., computation, communication, and time slots. The problem is NP-hard and challenging to solve due to the complex task dependencies, including triangular dependencies among multiple subtasks and the routing of edges between dependent subtasks. To address the challenge, we first propose a non-destructive task reconfiguration algorithm that transforms a task call graph into multiple sequential layers, breaking out the triangular dependency. Then, we develop a dePendency-awaRe task offloAding algorithm wIth taSk rEconfiguration (PRAISE) algorithm to maximize the total offloading benefit. PRAISE decouples the original problem into task offloading and 3D convex resource optimization. Simulation results show that PRAISE outperforms baselines with higher system benefits and lower resource costs.}
}


@article{DBLP:journals/tmc/WangDMCLDL24,
	author = {Ting Wang and
                  Yuxiang Deng and
                  Jiawei Mao and
                  Mingsong Chen and
                  Gang Liu and
                  Jieming Di and
                  Keqin Li},
	title = {Towards Intelligent Adaptive Edge Caching Using Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9289--9303},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3361083},
	doi = {10.1109/TMC.2024.3361083},
	timestamp = {Sun, 19 Jan 2025 14:43:33 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangDMCLDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tremendous expansion of edge data traffic poses great challenges to network bandwidth and service responsiveness for mobile computing. Edge caching has emerged as a promising method to alleviate these issues by storing a portion of data at the network edge. However, existing caching approaches suffer from either poor caching efficiency with low content-hit ratio or unintelligence of caching policies lacking self-adjustability. In this article, we propose ICE, a novel Intelligent Edge Caching scheme using a deep reinforcement learning (DRL) method to capture specific valuable information from the requested data. With the benefit of our proposed popularity model based on Newton's law of cooling, ICE fully takes into account the popularity of the contents to be cached and leverages the formulated Markov decision model to decide whether or not the contents should be cached. Moreover, to further improve the caching efficiency, we propose a novel distributed multi-node caching framework, named DCCC, assisted by a multi-tiered caching hierarchy. Comprehensive experiments show that the single-node ICE scheme greatly improves the cache hit rate and contents exchanging time in comparison with both DRL-based and legacy approaches, and our distributed multi-node caching scheme DCCC further significantly improves the overall utilization of caching space.}
}


@article{DBLP:journals/tmc/LiLDLL24,
	author = {Deng Li and
                  Chaojie Li and
                  Xiaoheng Deng and
                  Hui Liu and
                  Jiaqi Liu},
	title = {Familiar Paths are the Best: Incentive Mechanism Based on Path-Dependence
                  Considering Space-Time Coverage in Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9304--9323},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3360859},
	doi = {10.1109/TMC.2024.3360859},
	timestamp = {Sun, 19 Jan 2025 14:43:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLDLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location Dependent Mobile Crowdsensing (LDMC) often needs to collect data at different time points in various regions to ensure the coverage of sensing data. An incentive mechanism is needed to encourage participants to move to sparse areas and improve coverage. However, there are two problems: 1) most incentive mechanisms assume that the participants can get accurate information about tasks; 2) those mechanisms encourage participants through absolute utility so that the platform can obtain an improvement of incentive effect by increasing the reward. However, nodes usually get inaccurate information in reality. Moreover, behavioral economics finds that decision-making is often affected by relative utility rather than absolute utility. Path-dependence means that choices made on the basis of transitory conditions can persist long after those conditions change, which can solve the above problems. This study uses cognitive bias and the reference effect to explain the principle of path-dependence, and proposes a mechanism called Task Coverage promotion based on Path-dependence (TCPD). TCPD cultivates the cognitive bias of participants, causing an overestimation of expected utility. Then, it sets dynamic reference points to prevent participants from quitting early. The simulation results show that TCPD can improve the coverage and effectiveness of the platform.}
}


@article{DBLP:journals/tmc/MalekiMMR24,
	author = {Erfan Farhangi Maleki and
                  Weibin Ma and
                  Lena Mashayekhy and
                  Humberto J. La Roche},
	title = {QoS-Aware Content Delivery in 5G-Enabled Edge Computing: Learning-Based
                  Approaches},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9324--9336},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3363143},
	doi = {10.1109/TMC.2024.3363143},
	timestamp = {Sun, 19 Jan 2025 14:43:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MalekiMMR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for high-volume multimedia services through mobile user equipment (UEs) has imposed a significant burden on mobile networks. To cope with this growth in demand, it is necessary to extend the 5G network's ability to meet quality-of-service (QoS) requirements. The integration of Multi-access Edge Computing (MEC) with 5G technology, 5G-MEC, emerges as a pivotal solution, offering ultra-low latency, ultra-high reliability, and continuous connectivity to support various latency-sensitive applications for UEs. Despite these advancements, the mobility of UEs introduces significant spatio-temporal uncertainties, posing a major challenge on optimizing content delivery routes and directly impacting both latency and service continuity for UEs. Addressing this challenge necessitates suitable approaches for selecting optimal 5G-MEC components, with the goal of minimizing latency and reducing the frequency of handovers, ultimately ensuring a seamless content delivery experience. This paper proposes two learning-based approaches to tackle the problem of 5G-MEC component selection to facilitate QoS-aware content delivery in the absence of complete information about the dynamics of the 5G-MEC environment. First, we design an online sequential decision-making approach, called QCS-MAB, to decide on the content delivery routes in real-time while achieving a bounded performance. We then propose a deep learning approach, called QCS-DNN, to efficiently solve large-scale 5G-MEC component selection problems. We evaluate the effectiveness of our proposed approaches through extensive experiments using a real-world dataset. The results demonstrate that both QCS-MAB and QCS-DNN achieve near-optimal latency and significantly reduced handover times, significantly enhancing the 5G-MEC content delivery experience.}
}


@article{DBLP:journals/tmc/MaoMLC24,
	author = {Wuxing Mao and
                  Qian Ma and
                  Guocheng Liao and
                  Xu Chen},
	title = {Game Analysis and Incentive Mechanism Design for Differentially Private
                  Cross-Silo Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9337--9351},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364372},
	doi = {10.1109/TMC.2024.3364372},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaoMLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-silo federated learning (FL) is a distributed learning method where clients collaboratively train a global model without exchanging local data. However, recent works reveal that potential privacy leakage occurs when clients upload their local updates. Although some works have studied privacy-preserving mechanisms in FL, the selfish privacy-preserving behaviors of clients are yet to be explored. In this paper, we formulate clients’ privacy-preserving behaviors in cross-silo FL as a multi-stage privacy preservation game, where each stage game corresponds to one training iteration. Specifically, clients selfishly perturb their local updates in each training iteration to trade off between convergence performance and privacy loss. To analyze the game, we first derive a novel theoretical bound to characterize the impact of clients’ local perturbations on the convergence of FL through analyzing the corrective effect of gradient descent in model training. With the novel convergence bound, we prove that the multi-stage privacy preservation game admits a unique subgame perfect Nash equilibrium (SPNE). We show that at the SPNE, the magnitude of each client's local perturbation decreases geometrically with training iterations. We then show that the efficiency decreases with the number of clients in some cases. To tackle this problem, we propose a socially efficient incentive mechanism that guarantees individual rationality, budget balance, and social efficiency. We further propose a truthful mechanism that achieves approximate social efficiency. Simulation results show that our proposed mechanisms can decrease clients’ total cost by up to 58.08% compared with that at the SPNE.}
}


@article{DBLP:journals/tmc/LiSSMWZ24,
	author = {Zijian Li and
                  Yuchang Sun and
                  Jiawei Shao and
                  Yuyi Mao and
                  Jessie Hui Wang and
                  Jun Zhang},
	title = {Feature Matching Data Synthesis for Non-IID Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9352--9367},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3365295},
	doi = {10.1109/TMC.2024.3365295},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiSSMWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed (non-IID) data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis (HFMDS) method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentation to relieve data heterogeneity. The theoretical analysis highlights the effectiveness of our proposed data synthesis method in solving the non-IID challenge. Simulation results further demonstrate that our proposed HFMDS-FL algorithm outperforms the baselines in terms of accuracy, privacy preservation, and complexity saving on various benchmark datasets.}
}


@article{DBLP:journals/tmc/WuSWLXWJGL24,
	author = {Zhiyuan Wu and
                  Sheng Sun and
                  Yuwei Wang and
                  Min Liu and
                  Ke Xu and
                  Wen Wang and
                  Xuefeng Jiang and
                  Bo Gao and
                  Jinda Lu},
	title = {FedCache: {A} Knowledge Cache-Driven Federated Learning Architecture
                  for Personalized Edge Intelligence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9368--9382},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3361876},
	doi = {10.1109/TMC.2024.3361876},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuSWLXWJGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge Intelligence (EI) allows Artificial Intelligence (AI) applications to run at the edge, where data analysis and decision-making can be performed in real-time and close to data sources. To protect data privacy and unify data silos distributed among end devices in EI, Federated Learning (FL) is proposed for collaborative training of shared AI models across multiple devices without compromising data privacy. However, the prevailing FL approaches cannot guarantee model generalization and adaptation on heterogeneous clients. Recently, Personalized Federated Learning (PFL) has drawn growing awareness in EI, as it enables a productive balance between local-specific training requirements inherent in devices and global-generalized optimization objectives for satisfactory performance. However, most existing PFL methods are based on the Parameters Interaction-based Architecture (PIA) represented by FedAvg, which suffers from unaffordable communication burdens due to large-scale parameters transmission between devices and the edge server. In contrast, Logits Interaction-based Architecture (LIA) allows to update model parameters with logits transfer and gains the advantages of communication lightweight and heterogeneous on-device model allowance compared to PIA. Nevertheless, previous LIA methods attempt to achieve satisfactory performance either relying on unrealistic public datasets or increasing communication overhead for additional information transmission other than logits. To tackle this dilemma, we propose a knowledge cache-driven PFL architecture, named FedCache, which reserves a knowledge cache on the server for fetching personalized knowledge from the samples with similar hashes to each given on-device sample. During the training phase, ensemble distillation is applied to on-device models for constructive optimization with personalized knowledge transferred from the server-side knowledge cache. Empirical experiments on four datasets demonstrate that FedCache achieves comparable performance with state-of-art PFL approaches, with more than two orders of magnitude improvements in communication efficiency.}
}


@article{DBLP:journals/tmc/LiGLWJ24,
	author = {Yandi Li and
                  Jianxiong Guo and
                  Yupeng Li and
                  Tian Wang and
                  Weijia Jia},
	title = {Adversarial Bandits With Multi-User Delayed Feedback: Theory and Application},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9383--9397},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3362237},
	doi = {10.1109/TMC.2024.3362237},
	timestamp = {Tue, 11 Feb 2025 16:38:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiGLWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The multi-armed bandit (MAB) models have attracted significant research attention due to their applicability and effectiveness in various real-world scenarios such as resource allocation in uncertain environments, online advertising, and dynamic pricing. As an important branch, the adversarial multi-armed bandit problems with delayed feedback have been proposed and studied by many researchers recently where a conceptual adversary strategically selects the reward distributions associated with each arm to challenge the learning algorithm and the agent experiences a bunch of delays in receiving the corresponding reward feedback from different users after taking an action on them. However, the existing models restrict the feedback to being generated from only one user, which makes models inapplicable to the prevailing scenarios of multiple users (e.g., ad recommendation for a group of users). In this paper, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution while the feedback delay is arbitrary and unknown to the player in advance. Also, for different users in a round, the delays in feedback have no assumption of latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index\nT\n, the number of users\nM\n, the number of arms\nN\n, and upper bound of delay\nd\nmax\n, we prove a regret of\nO(\nT\nM\n2\nlnN(Ne+4\nd\nmax\n)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n√\n)\n. Furthermore, for the more common case of unknown\nT\n, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret concerning\nT\n. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms in dynamic environments.}
}


@article{DBLP:journals/tmc/ChenW24,
	author = {Suhong Chen and
                  Xudong Wang},
	title = {PtrTasking: Pointer Network Based Task Scheduling for Multi-Connectivity
                  Enabled {MEC} Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9398--9409},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3363662},
	doi = {10.1109/TMC.2024.3363662},
	timestamp = {Sun, 19 Jan 2025 14:43:34 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive services of mobile edge computing demand low latency in task handling, which cannot be easily satisfied due to limited per-user computing power on an edge server. Fortunately, a user can establish links with multiple base stations and the co-located edge servers in 5G and beyond. By leveraging multi-connectivity, tasks of a computing service can be dispatched to multiple edge servers. This approach can also improve the quality of services, since tasks allocated to edge servers can be prioritized according to link quality. To exploit the benefits of multi-connectivity, a task scheduling problem is formulated to minimize latency and maximize reliability of the application, subject to task dependency and resource constraints. This problem is hard to solve due to NP-hardness and time-varying conditions. To this end, a pointer network based scheme called PtrTasking is developed to obtain a deep-learning model for the schedules and then infer new schedules on-line. To support time-varying conditions (e.g., a new application or evident change of computing load), a training strategy called PtrTrain is designed to retrain PtrTasking in a fast and efficient way. Experiments based on real-world datasets demonstrate that PtrTasking significantly improves latency and reliability, as compared to the baseline schemes.}
}


@article{DBLP:journals/tmc/WangZZGGHM24,
	author = {Yizong Wang and
                  Dong Zhao and
                  Huanhuan Zhang and
                  Teng Gao and
                  Zixuan Guo and
                  Chenghao Huang and
                  Huadong Ma},
	title = {Bandwidth-Efficient Mobile Volumetric Video Streaming by Exploiting
                  Inter-Frame Correlation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9410--9423},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367750},
	doi = {10.1109/TMC.2024.3367750},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZGGHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Volumetric videos offer viewers more immersive experiences, enabling a variety of applications. However, state-of-the-art streaming systems still need hundreds of Mbps bandwidth to transmit volumetric videos, exceeding the common bandwidth capabilities of mobile devices. We find a research gap in reusing inter-frame redundant information to reduce bandwidth consumption, while the existing inter-frame compression methods rely on the so-called explicit correlation, i.e., the redundancy from the same/adjacent locations in the previous frame, which does not apply to highly dynamic frames or dynamic viewports. This paper introduces a new concept called implicit correlation, i.e., the consistency of topological structures, which stably exists in dynamic frames and is beneficial for reducing bandwidth consumption. We design a mobile volumetric video streaming system Hermes consisting of an implicit correlation encoder to reduce bandwidth consumption and a hybrid streaming method that adapts to dynamic viewports. Experiments on public datasets show that Hermes achieves a frame rate of 30+ FPS over daily networks and on commodity smartphones, with at least 3.64× and 3.34× improvement compared with two state-of-the-art baselines, respectively.}
}


@article{DBLP:journals/tmc/ZhangLLCTHY24,
	author = {Ran Zhang and
                  Fangqi Liu and
                  Jiang Liu and
                  Mingzhe Chen and
                  Qinqin Tang and
                  Tao Huang and
                  F. Richard Yu},
	title = {CPPer-FL: Clustered Parallel Training for Efficient Personalized Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9424--9436},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3365951},
	doi = {10.1109/TMC.2024.3365951},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLLCTHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a clustered parallel training algorithm is designed for personalized federated learning (Per-FL), called CPPer-FL. CPPer-FL improves the communication and training efficiency of Per-FL from two perspectives, namely, less burden for the central server and lower interaction idling delay. CPPer-FL adopts a client-edge-center learning architecture, which offloads the central server's model aggregation and communication burden to distributed edge servers. Also, CPPer-FL redesigns the cascading model synchronization and updating procedure in conventional Per-FL and changes it to a parallel manner, thus improving the interaction efficiency in the training process. Further, for the proposed hierarchical architecture, two approaches are proposed to cater to Per-FL: similarity-based clustering for client-edge association and personalized model aggregation for parallel model updating, such that clients’ personal features can be preserved in the training process. The convergence of CPPer-FL has been formally analyzed and proved. Evaluation results validate the communication efficiency, model convergence, and model accuracy improvement.}
}


@article{DBLP:journals/tmc/LiuTDWQZ24,
	author = {Zhicheng Liu and
                  Meng Tian and
                  Mianxiong Dong and
                  Xiaofei Wang and
                  Chao Qiu and
                  Cheng Zhang},
	title = {MoEI: Mobility-Aware Edge Inference Based on Model Partition and Service
                  Migration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9437--9450},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366186},
	doi = {10.1109/TMC.2024.3366186},
	timestamp = {Mon, 30 Sep 2024 07:54:36 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuTDWQZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks are the cornerstone of many mobile intelligent systems, and their inference processes bring about computation-intensive tasks. Device-edge cooperative inference in mobile edge computing provides a fine-grained processing method to migrate the burden of inference computation. However, the geographical dispersion of resources and the mobility pattern of devices pose scheduling issues to be considered. In this paper, we propose a task scheduling framework for such device-edge systems to improve the pipeline time of model inference. First, we consider the resource provisioning strategy with a pre-fetching service migration setting in the environment of multiple mobile devices and edge nodes. Then, we leverage game theory to analyze the property of the decision-making process and propose an offline algorithm under complete information. Next, we propose an algorithm based on proximal policy optimization to enable mobile devices to make decisions in a distributed online manner. Further, we adopt a memory mechanism into the online algorithm to improve the decision-makers’ understanding of the system environment. Experiments demonstrate the effectiveness of the two algorithms. The average pipeline time of the proposed online algorithm is only 61.44% of that of local processing, which is 1.196 times that of the proposed offline algorithm.}
}


@article{DBLP:journals/tmc/HuangLYLS24,
	author = {Cheng Huang and
                  Dongxiao Liu and
                  Anjia Yang and
                  Rongxing Lu and
                  Xuemin Shen},
	title = {{PPRP:} Preserving Location Privacy for Range-Based Positioning in
                  Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9451--9468},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366340},
	doi = {10.1109/TMC.2024.3366340},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangLYLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a privacy-preserving range-based positioning scheme, named PPRP, which can preserve the location privacy of both user equipment (UE) and anchors (ACs) in mobile networks. Specifically, PPRP is established on a decentralized trust-based framework that divides trust between two location management function (LMF) servers. With such a framework, UE and ACs are allowed to securely upload their range/range-difference measurement data to LMF servers using lightweight additive secret sharing techniques (ASS) instead of cumbersome cryptographic operations. Then, PPRP takes secret-shared measurement data as inputs and decomposes UE's location estimation procedures into secure two-party matrix computation sub-protocols, which are elaborately crafted using somewhat homomorphic encryption and randomization techniques to ensure both efficiency and privacy preservation in positioning. Furthermore, to mitigate the negative effects arising from non-line-of-sight (NLoS) ACs, PPRP achieves privacy-preserving residual-based NLoS analysis. To this end, we additionally propose a series of secure two-party sub-protocols to support various non-linear functions, including comparison, division, square root computation, oblivious shuffle and sorting. These sub-protocols serve as fundamental modules that can be effectively combined to perform sophisticated operations of NLoS analysis in a privacy-preserving manner. A comprehensive simulation-based security analysis demonstrates that PPRP can achieve location privacy preservation. Finally, we develop a proof-of-concept prototype and conduct extensive experiments to show PPRP's high performance in terms of positioning accuracy, computational efficiency, and communication complexity.}
}


@article{DBLP:journals/tmc/HuangWFCLWL24,
	author = {Ziyao Huang and
                  Weiwei Wu and
                  Chenchen Fu and
                  Vincent Chau and
                  Xiang Liu and
                  Jianping Wang and
                  Junzhou Luo},
	title = {AoI-Guaranteed Bandit: Information Gathering Over Unreliable Channels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9469--9486},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367869},
	doi = {10.1109/TMC.2024.3367869},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWFCLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many IoT applications, information needs to be gathered from multiple heterogeneous sources to the base station for real-time processing and follow-up actions. Undoubtedly, information freshness, measured by age of information (AoI), is critical in taking responsive actions. Recent studies have taken AoI into the consideration of transmission scheduling over wireless channels. However, existing studies on guaranteeing AoI either assume error-free wireless channels or priorly known link reliability, which is unrealistic. In this article, we tackle the AoI-guaranteed transmission scheduling problem over an unreliable channel with the aim of throughput maximization, which is modelled as an AoI-Guaranteed Multi-Armed Bandit (AG-MAB) problem. Since the problem has not been studied in the literature even for the oracle case with given link reliability, we first propose an optimal stationary randomized sampling (SRS) policy for the oracle case. For the AG-MAB problem with unknown link reliability, we propose learning algorithms that meet the AoI requirements with probability 1 and incur sublinear regret compared to Oracle SRS, which can also detect the unsatisfiability of the AoI constraint and switch to the fallback policy promptly with guaranteed accuracy. Numerical results show that our algorithm outperforms the AoI-constraint-aware baselines on throughput with per-source AoI requirement guaranteed.}
}


@article{DBLP:journals/tmc/LiQGCHGPZ24,
	author = {Yixuan Li and
                  Xiaoqi Qin and
                  Jiaxiang Geng and
                  Rui Chen and
                  Yanzhao Hou and
                  Yanmin Gong and
                  Miao Pan and
                  Ping Zhang},
	title = {{REWAFL:} Residual Energy and Wireless Aware Participant Selection
                  for Efficient Federated Learning Over Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9487--9501},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3365477},
	doi = {10.1109/TMC.2024.3365477},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiQGCHGPZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Participant selection (PS) helps to accelerate federated learning (FL) convergence, which is essential for the practical deployment of FL over mobile devices. While most existing PS approaches focus on improving training accuracy and efficiency rather than residual energy of mobile devices, which fundamentally determines whether the selected devices can participate. Meanwhile, the impacts of mobile devices heterogeneous wireless transmission rates on PS and FL training efficiency are largely ignored. Moreover, PS causes the staleness issue. Prior research exploits isolated functions to force long-neglected devices to participate, which is decoupled from original PS designs. In this paper, we propose a residual energy and wireless aware PS design for efficient FL training over mobile devices (REWAFL). REWAFL introduces a novel PS utility function that jointly considers global FL training utilities and local energy utility, which integrates energy consumption and residual battery energy of candidate mobile devices. Under the proposed PS utility function framework, REWAFL further presents a residual energy and wireless aware local computing policy. Besides, REWAFL buries the staleness solution into its utility function and local computing policy. The experimental results show that REWAFL is effective in improving training accuracy and efficiency, while avoiding flat battery of mobile devices.}
}


@article{DBLP:journals/tmc/JinSJ24,
	author = {Zhu Jin and
                  Tiecheng Song and
                  Wen{-}Kang Jia},
	title = {An Adaptive Cooperative Caching Strategy for Vehicular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9502--9517},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367543},
	doi = {10.1109/TMC.2024.3367543},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JinSJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge caching has emerged as an effective solution to the challenges posed by massive content delivery in the vehicular network. In vehicular networks, vehicles and roadside units (RSUs) can serve as intermediate relays with caching capabilities. However, due to the mobility of vehicles, the topology of the edge network changes frequently, which leads to frequent link interruptions and increases the transmission delay. This paper proposes an adaptive cooperative caching (ACC) strategy to adapt the frequent changes in the vehicular edge network topology and describes an optimization problem to minimize the average transmission delay. Then, the optimization problem is transformed into two sub-optimization problems: multiple-choice knapsack (MCK) problem and multiple minimum-weight dominating set (MMWDS) problem. Finally, two greedy algorithms with low complexity are designed to solve the above two optimization problems and obtain approximate solutions to the optimal caching decision. Simulation results show that ACC can effectively improve the cache hit rate and reduce the average transmission delay and the communication overhead compared with other caching strategies.}
}


@article{DBLP:journals/tmc/MaZW24,
	author = {Fei Ma and
                  Renbo Zhu and
                  Ping Wang},
	title = {{PTT:} Piecewise Transformation Technique for Analyzing Numerical
                  Data Under Local Differential Privacy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9518--9531},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364496},
	doi = {10.1109/TMC.2024.3364496},
	timestamp = {Sun, 19 Jan 2025 14:43:30 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local differential privacy (LDP for short), as an emerging standard privacy-preserving technique that is suitable for privacy preserving data analysis, has been widely deployed into various real-world scenarios to analyze massive data. In this study, we are mainly concerned with piecewise transformation technique (PTT for short) for analyzing numerical data under LDP. We first provide the principled framework for PTT in the context of LDP. Then, we show that (1) many PTTs are asymptotically optimal when used to obtain an unbiased estimator for mean of numerical data and (2) there is PTT that reaches the theoretical lower bound on variance. Next, we prove that (1) there does not exist strictly better PTT than the well-used Duchi's scheme in terms of the consistency-noisy-variance, also, the latter is not always better than arbitrary PTT and (2) however, one has the ability to find a great number of PTTs that are consistently better than the latter according to the worst-case-noisy-variance. When we are restricted to the high privacy level \\epsilon \\in (0,1]\n, enough PTTs turn out to have smaller variance than the well-known Laplace mechanism. Finally, we demonstrate that for a family of PTTs, the correspondingly theoretical lower bound of noisy variance follows O(\\epsilon ^{-2})\nwhen considering \\epsilon \\in (0,1]\n.}
}


@article{DBLP:journals/tmc/DuDLWLRCZ24,
	author = {Yicong Du and
                  Huan Dai and
                  Hongbo Liu and
                  Yan Wang and
                  Guyue Li and
                  Yanzhi Ren and
                  Yingying Chen and
                  Ke Zhang},
	title = {Secret Key Generation Based on Manipulated Channel Measurement Matching},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9532--9548},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364909},
	doi = {10.1109/TMC.2024.3364909},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/DuDLWLRCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The physical layer secret key generation exploiting wireless channel reciprocity has demonstrated its viability and effectiveness in various wireless scenarios, such as the Internet of Things (IoT) network, mobile communication network, and industrial control system. Most of the existing studies rely on the quantization technique to convert channel measurements into secret bits for confidential communications. However, non-simultaneous packet exchanges in time-division duplex systems and noise effects usually induce inconsistent quantization results and mismatched secret bits. Although recent research has spent significant effort mitigating such non-reciprocity, it is still far from practical error-free key generation. Unlike previous quantization-based approaches, we take a different viewpoint to match the randomly manipulated (i.e., permuted or edited) channel measurements between a pair of users by minimizing their discrepancy holistically. Specifically, two novel secret key generation algorithms based on bipartite graph matching (BM-SKG) and edited sequence alignment (SA-SKG) are developed. BM-SKG allows two users to generate the same secret key based on the permutation order of channel measurements, while SA-SKG aims to align the edited channel measurements between a pair of users for secret key agreement. In both algorithms, one user can preset the secret key and embed encrypted messages in the exchanged data packets, which reduces communication overheads in key generation. Extensive experimental results show that both BM-SKG and SA-SKG algorithms achieve error-free key agreement on channel measurements at a low cost under various scenarios.}
}


@article{DBLP:journals/tmc/EleftherakisSRCG24,
	author = {Stavros Eleftherakis and
                  Giuseppe Santaromita and
                  Maurizio Rea and
                  Xavier Costa{-}P{\'{e}}rez and
                  Domenico Giustiniano},
	title = {{SPRING+:} Smartphone Positioning From a Single WiFi Access Point},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9549--9566},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367241},
	doi = {10.1109/TMC.2024.3367241},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/EleftherakisSRCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor positioning is a major challenge for location-based services. WiFi deployments are often used to address indoor positioning. Yet, they require multiple access points, which may not be available or accessible for localization in all scenarios, or they make unrealistic assumptions for practical deployments. In this paper we present SPRING+, a positioning system that extracts and processes Channel State Information (CSI) and Fine Time Measurements (FTM) from a single Access Point (AP) to localize commercial smartphones. First, we propose an adaptive method for estimating the Angle of Arrival (AOA) from CSI that works on single packets and leverages information from the estimated number of paths. Second, we present a new method to detect the first path using FTM measurements, robust to multipath scenarios. We evaluate SPRING+ in an extensive experimental campaign consisting of four different testbeds: i) generic indoor spaces, ii) generic indoor spaces with obstacles, iii) office environments and iv) home environments. Our results show that SPRING+ is able to achieve a median 2D positioning error between 1 and 1.8 meters with a single WiFi AP.}
}


@article{DBLP:journals/tmc/LiuYX24,
	author = {Linfeng Liu and
                  Jiaqi Yan and
                  Jia Xu},
	title = {On Exploring the Carrying-Charging Demand Balance in Cruising Route
                  Recommendation for Vacant Electric Taxis},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9567--9581},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364654},
	doi = {10.1109/TMC.2024.3364654},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the gasoline taxis are gradually restricted due to the increased environmental awareness, electric taxis (E-taxis) have become a more environmentally friendly choice to provide the transportation service. When some E-taxis are vacant, they typically cruise along roads without any specific destinations, and two major concerns should be considered for vacant E-taxis: In order to increase the business profits of E-taxis, it is vital to recommend the profitable cruising routes along which vacant E-taxis could pick up passengers as early as possible and earn more profits. Besides, the residual electricity of E-taxis is continuously consumed on travels, and E-taxis must be timely charged before their residual electricity is exhausted (i.e., the breakdowns of E-taxis). Thus, the cruising route recommendation for vacant E-taxis should take into account both passenger-carrying demand and charging demand, and the carrying-charging demand balance should be properly made. To this end, we propose a cruising Route Recommendation Method based on Carrying-charging Demand Balance (RRM-CDB) for vacant E-taxis. The passenger-carrying demand and charging demand are first formulated to reflect their changes and interrelationships, and the historical cruising trajectories of vacant E-taxis (with the two types of demand) are locally learned to recommend the future cruising routes, because the historical cruising trajectories contain the distribution of taxi demand of passengers and the trend of vacant E-taxis gradually approaching the charging stations with the decrease of residual electricity. Particularly, in RRM-CDB each vacant E-taxi trains a local learning model in a distributed manner, thus significantly reducing the computational complexity of cruising route recommendation. Extensive simulations and comparisons demonstrate that RRM-CDB can help to increase the business profits of E-taxis and avoid the breakdowns of E-taxis as much as possible.}
}


@article{DBLP:journals/tmc/WangTXX24,
	author = {Jiliang Wang and
                  Shuai Tong and
                  Zhenqiang Xu and
                  Pengjin Xie},
	title = {Real-Time Concurrent LoRa Transmissions Based on Peak Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9582--9594},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3365797},
	doi = {10.1109/TMC.2024.3365797},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangTXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa, as a representative Lower Power Wide Area Network (LPWAN) technology, shows great potential in providing low power and long range wireless communication. Real LoRa deployments, however, suffer from severe collisions. Existing collision decoding methods cannot work well for low SNR LoRa signals. Most LoRa collision decoding methods process collisions offline and cannot support real-time collision decoding in practice. To address these problems, we propose Pyramid, a real-time LoRa collision decoding approach. To the best of our knowledge, this is the first real-time multi-packet LoRa collision decoding approach in low SNR. Pyramid exploits the subtle packet offset to separate packets in a collision. The core of Pyramid is to combine signals in multiple windows and transfers variation of chirp length in multiple windows to robust features in the frequency domain that are resistant to noise. We address practical challenges including accurate peak recovery and feature extraction in low SNR signals of collided packets. We theoretically prove that Pyramid incurs a very small SNR loss (<0.56 dB) to original LoRa transmissions. We implement Pyramid using USRP N210 and evaluate its performance in a 20-nodes network. Evaluation results show that Pyramid achieves real-time collision decoding and improves the throughput by 2.11×.}
}


@article{DBLP:journals/tmc/ZhaoYY24,
	author = {Yi Zhao and
                  Zhanwei Yu and
                  Di Yuan},
	title = {Caching With Personalized and Incumbent-Aware Recommendation: Modeling
                  and Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9595--9613},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3365465},
	doi = {10.1109/TMC.2024.3365465},
	timestamp = {Thu, 26 Sep 2024 17:09:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Caching popular contents at cell edge has been recognized as a promising way to facilitate rapid content delivery and alleviate backhaul burden. The content popularity is greatly influenced by recommendations by content providers. In this paper, we leverage this fact to jointly optimize caching and recommendation towards higher caching efficiency. We focus on both personalized and incumbent-aware recommendation. The incumbent content refers to the content that a user is currently browsing, resulted by the user's short-term interest. We model and formulate the resulting cache efficiency maximization problem subject to user satisfaction requirements. We prove the NP-hardness of the problem, and reformulate it using integer linear programming, enabling to solve optimally small-scale instances. Based on problem analysis with a graph representation, we derive three polynomial-time algorithms, where the recommendation sub-problem is solved to global optimum. Among these algorithms, the first two are based on sub-modularity, with 1-e^{-1}\napproximation guarantee under mild conditions, while the last one is an alternation-based algorithm with fast convergence. Numerical results show the close-to-optimal performance of the proposed algorithms.}
}


@article{DBLP:journals/tmc/LiuZWCXTGL24,
	author = {Xiulong Liu and
                  Bojun Zhang and
                  Lizhang Wang and
                  Sheng Chen and
                  Xin Xie and
                  Xinyu Tong and
                  Tao Gu and
                  Keqiu Li},
	title = {Fine-Grained Recognition of Manipulation Activities on Objects via
                  Multi-Modal Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9614--9628},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364522},
	doi = {10.1109/TMC.2024.3364522},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZWCXTGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fine-grained recognition of human manipulation activities on objects is crucial in the era of human-computer-object integration. However, there is a lack of solutions for simultaneous recognition of human identity, manipulation activities (including drawing and rotation), and manipulated objects. Therefore, we propose an RF-Camera system that combines RFID and computer vision techniques to address this challenge in multi-person and multi-object scenarios. In RF-Camera, we employ a skeleton-assisted method to extract facial images of target individuals, enabling precise recognition of their identities. To identify manipulation activities, we analyze the 3D hand trajectory and fingertip vector angle, differentiating drawing and rotation manipulation activities. Additionally, we model target person's hand movements to predict phase data of the target tag, enabling the determination of person-object relationships. Implementing RF-Camera using COTS RFID and Kinect devices involves overcoming challenges such as extracting effective data from noisy streams, predicting virtual phase data considering hand-tag offset, and ensuring high tag reading rates in tag-dense scenarios. We conducted experiments involving six participants performing object manipulation activities, including drawing letters/symbols and rotating movements. Extensive experimental results show that RF-Camera achieves over 90% accuracy in recognizing person identity, manipulation activities, and person-object matching in most conditions.}
}


@article{DBLP:journals/tmc/YinCT24,
	author = {Benshun Yin and
                  Zhiyong Chen and
                  Meixia Tao},
	title = {Knowledge Distillation and Training Balance for Heterogeneous Decentralized
                  Multi-Modal Learning Over Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9629--9644},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364673},
	doi = {10.1109/TMC.2024.3364673},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YinCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized learning is widely employed for collaboratively training models using distributed data over wireless networks. Existing decentralized learning methods primarily focus on training single-modal networks. For the decentralized multi-modal learning (DMML), the modality heterogeneity and the non-independent and non-identically distributed (non-IID) data across devices make it difficult for the training model to capture the correlated features across different modalities. Moreover, modality competition can result in training imbalance among different modalities, which can significantly impact the performance of DMML. To improve the training performance in the presence of non-IID data and modality heterogeneity, we propose a novel DMML with knowledge distillation (DMML-KD) framework, which decomposes the extracted feature into the modality-common and the modality-specific components. In the proposed DMML-KD, a generator is applied to learn the global conditional distribution of the modality-common features, thereby guiding the modality-common features of different devices towards the same distribution. Meanwhile, we propose to decrease the number of local iterations for the modalities with fast training speed in DMML-KD to address the imbalanced training. We design a balance metric based on the parameter variation to evaluate the training speed of different modalities in DMML-KD. Using this metric, we optimize the number of local iterations for different modalities on each device under the constraint of remaining energy on devices. Experimental results demonstrate that the proposed DMML-KD with training balance can effectively improve the training performance of DMML.}
}


@article{DBLP:journals/tmc/WangWDQZGCF24,
	author = {Lihao Wang and
                  Weijun Wang and
                  Haipeng Dai and
                  Yuben Qu and
                  Jiaqi Zheng and
                  Rong Gu and
                  Guihai Chen and
                  Xiaoming Fu},
	title = {Joint Deployment of Truck-Drone Systems for Camera-Based Object Monitoring},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9645--9662},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367849},
	doi = {10.1109/TMC.2024.3367849},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangWDQZGCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Truck-drone systems, wherein trucks carrying drones drive to pre-planned positions and then free drones equipped with cameras to monitor a known number of objects with reported positions, have been used for various scenarios. An object's quality of monitoring (QoM) by a camera is defined as a function of camera focal length and monitoring distance. Improving the QoM would help downstream tasks, including object detection and recognition. The monitoring utility is the fusion of all the QoMs of an object from multiple cameras. This article optimizes the Deployment Of Trucks And Drones for Object monitoring (DOTADO) problem, i.e., deploying a truck-drone system, where each drone is equipped with a varifocal camera, to maximize the overall monitoring utility for all objects. First, we model the hybrid system and define monitoring quality and utility. Then, we discretize the solution space into deployment strategies with performance bound. To select deployment strategies, we prove the submodularity of the problem and propose a two-level greedy algorithm with a bounded approximation ratio. Finally, we devise an optimal method to adjust the strategy for energy saving and communication improvement without losing monitoring utility. We perform both simulations and field experiments to verify the proposed framework.}
}


@article{DBLP:journals/tmc/LuoXWHT24,
	author = {Bing Luo and
                  Wenli Xiao and
                  Shiqiang Wang and
                  Jianwei Huang and
                  Leandros Tassiulas},
	title = {Adaptive Heterogeneous Client Sampling for Federated Learning Over
                  Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9663--9677},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3368473},
	doi = {10.1109/TMC.2024.3368473},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoXWHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited. Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity (e.g., diverse computation and communication capacities) and statistical heterogeneity (e.g., unbalanced and non-i.i.d. data). This article aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time. We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability. Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem. We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem. Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design. Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth. Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes. Notably, for EMNIST dataset, our scheme in hardware prototype spends 71% less time than the baseline uniform sampling for reaching the same target loss.}
}


@article{DBLP:journals/tmc/LanCLCS24,
	author = {Wenjun Lan and
                  Kongyang Chen and
                  Yikai Li and
                  Jiannong Cao and
                  Yuvraj Sahni},
	title = {Deep Reinforcement Learning for Privacy-Preserving Task Offloading
                  in Integrated Satellite-Terrestrial Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9678--9691},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366928},
	doi = {10.1109/TMC.2024.3366928},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LanCLCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite communication networks have attracted widespread attention for seamless network coverage and collaborative computing. In satellite-terrestrial networks, ground users can offload computing tasks to visible satellites that with strong computational capabilities. Existing solutions on satellite-assisted task computing generally focused on system performance optimization such as task completion time and energy consumption. However, due to the high-speed mobility pattern and unreliable communication channels, existing methods still suffer from serious privacy leakages. In this article, we present an integrated satellite-terrestrial network to enable satellite-assisted task offloading under dynamic mobility nature. We also propose a privacy-preserving task offloading scheme to bridge the gap between offloading performance and privacy leakage. In particular, we balance two offloading privacy, called the usage pattern privacy and the location privacy, with different offloading targets (e.g., completion time, energy consumption, and communication reliability). Finally, we formulate it into a joint optimization problem, and introduce a deep reinforcement learning-based privacy-preserving algorithm for an optimal offloading policy. Experimental results show that our proposed algorithm outperforms other benchmark algorithms in terms of completion time, energy consumption, privacy-preserving level, and communication reliability. We hope this work could provide improved solutions for privacy-persevering task offloading in satellite-assisted edge computing.}
}


@article{DBLP:journals/tmc/ChenXXMW24,
	author = {Suo Chen and
                  Yang Xu and
                  Hongli Xu and
                  Zhenguo Ma and
                  Zhiyuan Wang},
	title = {Enhancing Decentralized and Personalized Federated Learning With Topology
                  Construction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9692--9707},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367872},
	doi = {10.1109/TMC.2024.3367872},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXXMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Federated Learning (FL) permits all workers (e.g., mobile devices) to cooperatively train a model using their local data at the network edge. In order to avoid the possible bottleneck of conventional parameter server architecture, the decentralized federated learning (DFL) is developed on the peer-to-peer (P2P) communication. Non-IID issue is a key challenge in FL and will significantly degrade the model training performance. To this end, we propose a personalized solution called TOPFL, in which only parts of the local models (not the entire models) are shared and aggregated. Moreover, considering the limited communication bandwidth on workers, we propose a topology construction algorithm to accelerate the training process. To verify the convergence of the decentralized training framework, we theoretically analyze the impact of the data heterogeneity and topology on the convergence upper bound. Extensive simulation results show that TOPFL can achieve 2.2× speedup when reaching convergence and 5.8% higher test accuracy under the same resource consumption, compared with the baseline solutions.}
}


@article{DBLP:journals/tmc/JiangJZCL24,
	author = {Hongbo Jiang and
                  Panyi Ji and
                  Taiyuan Zhang and
                  Hangcheng Cao and
                  Daibo Liu},
	title = {Two-Factor Authentication for Keyless Entry System via Finger-Induced
                  Vibrations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9708--9720},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3368331},
	doi = {10.1109/TMC.2024.3368331},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangJZCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Keyless entry systems (KES) have become popular due to their high user-friendliness, while fingerprint and digital password authentication are two of the most widely used unlocking ways. However, current KES are vulnerable to security threats, such as fingerprint films that deceive fingerprint sensors and stolen passcodes. To address these issues, this paper presents {\\sf Fingerbeat}, a two-factor authentication system to defend the security risks of the current widely deployed KES devices. {\\sf Fingerbeat} combines original credentials, such as fingerprints and passcodes, with unique and persistent finger-induced vibrations to create a two-factor secure authentication model, while ensuring user-friendliness. {\\sf Fingerbeat} leverages the fact that each person's finger structure is distinct and can be represented in distinct vibration patterns. During authentication, FIV is triggered and embodied in the mechanical vibration of the force-bearing body (i.e., KES panel), which can be captured by a low-cost accelerometer. We develop a proof-of-concept prototype of {\\sf Fingerbeat}, extracting FIV features from mixed vibration recordings and eliminating the impacts of variable behaviors and external disturbance. Finally, we conduct extensive experiments to demonstrate its security and effectiveness.}
}


@article{DBLP:journals/tmc/GaoZ24,
	author = {Xingxia Gao and
                  Linbo Zhai},
	title = {Service Experience Oriented Cooperative Computing in Cache-Enabled
                  UAVs Assisted {MEC} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9721--9736},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366944},
	doi = {10.1109/TMC.2024.3366944},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/GaoZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The unmanned aerial vehicle (UAV)-enabled multi-access edge computing (MEC) technology is opening up new opportunities in the integrated space-air-ground in the 5G era and beyond. However, providing low-latency services solely from an overall perspective cannot ensure a high quality of experience (QoE) for user equipments (UEs). Therefore, we propose a service experience-oriented cooperative caching framework, where the UAVs can effectively serve each UE by providing communication and computing resources. A novel metric called service experience ratio is defined to reflect the experience at the UEs. Under the constraints of UAV's energy budget and delay requirements, we consider jointly optimizing task offloading, resource allocation, trajectory planning, and service caching placement to maximize the service experience ratio. Since the original problem is a mixed-integer non-convex programming problem with a fractional structure, it is challenging to be solved in polynomial time. Based on Dinkelbach's method and convex optimization theory, we simplify the problem model and propose a four-stage alternating iterative service ratio maximization algorithm to solve this problem. Besides, we also analyze the convergence and complexity of our proposed algorithm. Numerical results demonstrate that the service experience ratio achieved by the proposed algorithm is 19–34% higher than the comparative works.}
}


@article{DBLP:journals/tmc/MaXXLH24,
	author = {Qianpiao Ma and
                  Yang Xu and
                  Hongli Xu and
                  Jianchun Liu and
                  Liusheng Huang},
	title = {FedUC: {A} Unified Clustering Approach for Hierarchical Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9737--9756},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3366947},
	doi = {10.1109/TMC.2024.3366947},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaXXLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an effective approach to train models collaboratively among distributed edge nodes (i.e., workers) while facing three crucial challenges, edge heterogeneity, resource constraint, and Non-IID data. Under the parameter server (PS) architecture, a single parameter server may become the system bottleneck and cannot well deal with the edge heterogeneity, while the peer-to-peer (P2P) architecture causes significant communication consumption to achieve satisfactory training performance. To this end, hierarchical aggregation (HA) architecture is proposed to cluster workers to tackle the edge heterogeneity and reduce communication consumption for FL. However, the existing researches on HA architecture cannot provide a unified clustering approach for various inter-cluster aggregation patterns (e.g., centralized or decentralized structure, synchronous or asynchronous mode). In this paper, we explore the quantitative relationship between the convergence bounds of different inter-cluster patterns and several factors, e.g., data distribution, frequency of clusters participating in inter-cluster aggregation (for asynchronous modes), and inter-cluster topology (for decentralized structures). Based on the convergence bounds, we design a unified clustering algorithm FedUC to organize workers for different patterns. Experimental results on classical models and datasets show that FedUC can greatly accelerate the model training of different patterns by 1.79-7.39× compared with the state-of-the-art clustering methods.}
}


@article{DBLP:journals/tmc/JiangXXWLQ24,
	author = {Zhida Jiang and
                  Yang Xu and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Jianchun Liu and
                  Chunming Qiao},
	title = {Semi-Supervised Decentralized Machine Learning With Device-to-Device
                  Cooperation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9757--9771},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3369188},
	doi = {10.1109/TMC.2024.3369188},
	timestamp = {Fri, 20 Sep 2024 14:01:37 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/JiangXXWLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The massive data from mobile and embedded devices have huge potential for training machine learning models. Decentralized machine learning (DML) can avoid the inherent bottleneck of the parameter server (PS) by collaboratively training models in a device-to-device (D2D) fashion. However, the previous DML works often assume that the local data are fully annotated with ground-truth labels, which is unrealistic for many Internet of Things (IoT) applications. This arises a new practical DML scenario, namely semi-supervised DML, where the local data of distributed workers are partially labeled in the D2D network. The existing semi-supervised learning techniques are proposed for standalone or the PS architecture, which ignore the impact of D2D topology on the performance of semi-supervised learning. Thus, they cannot adequately leverage the unlabeled data of decentralized workers, leading to performance degradation. Herein, we propose a novel framework, called SSD, to address the problem of semi-supervised DML by exploiting D2D cooperation. The key insight behind SSD is that neighbor selection has a crucial impact on pseudo-label quality and communication overhead. In SSD, each worker adaptively selects its neighbors with high-quality models and similar data distribution under communication resource constraints, which helps to generate high-confidence pseudo-labels for local unlabeled data and further boosts the DML performance. Extensive empirical evaluations on both testbed and simulated environments show that SSD significantly outperforms other baselines.}
}


@article{DBLP:journals/tmc/ZhouWLZYS24,
	author = {Man Zhou and
                  Qian Wang and
                  Qi Li and
                  Wenyu Zhou and
                  Jingxiao Yang and
                  Chao Shen},
	title = {Securing Face Liveness Detection on Mobile Devices Using Unforgeable
                  Lip Motion Patterns},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9772--9788},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367781},
	doi = {10.1109/TMC.2024.3367781},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouWLZYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Face authentication usually utilizes deep learning models to verify users with high accuracy. However, it is vulnerable to various attacks that cheat the models by manipulating the digital counterparts of human faces. So far, lots of liveness detection schemes have been developed to prevent such attacks. Unfortunately, the attacker can still bypass them by constructing sophisticated attacks. We study the security of existing face authentication services and typical liveness detection approaches. Particularly, we develop a new type of attack, i.e., the low-cost 3D projection attack that projects manipulated face videos on a 3D face model, which can easily evade these face authentication services and liveness detection approaches. To this end, we propose FaceLip, a novel face liveness detection scheme on mobile devices, which utilizes lip motion patterns built upon well-designed acoustic signals to enable a strong security guarantee. The unique lip motions for each user are unforgeable because FaceLip verifies the patterns by analyzing acoustic signals that are dynamically generated according to random challenges, which ensures that our signals for liveness detection cannot be manipulated. We prototype FaceLip on off-the-shelf smartphones and conduct extensive experiments under different settings. Our evaluation with 44 participants validates the effectiveness and robustness of FaceLip.}
}


@article{DBLP:journals/tmc/AlHabobTW24,
	author = {Ahmed A. Al{-}Habob and
                  Hina Tabassum and
                  Omer Waqar},
	title = {Non-Orthogonal Age-Optimal Information Dissemination in Vehicular
                  Networks: {A} Meta Multi-Objective Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9789--9803},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367166},
	doi = {10.1109/TMC.2024.3367166},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AlHabobTW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article considers minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. We consider non-orthogonal multi-modal information dissemination, which is based on superposed message transmission from RSU and successive interference cancellation (SIC) at vehicles. The formulated problem is a multi-objective mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is very challenging to obtain. First, we leverage the weighted-sum approach to decompose the multi-objective problem into a set of multiple single-objective sub-problems corresponding to each predefined objective preference weight. Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy gradient (DDPG) model to solve each optimization sub-problem respective to predefined objective-preference weight. The DQN optimizes the decoding order, while the DDPG solves the continuous power allocation. The model needs to be retrained for each sub-problem. We then present a two-stage meta-multi-objective reinforcement learning solution to estimate the Pareto front with a few fine-tuning update steps without retraining the model for each sub-problem. Simulation results illustrate the efficacy of the proposed solutions compared to the existing benchmarks and that the meta-multi-objective reinforcement learning model estimates a high-quality Pareto frontier with reduced training time.}
}


@article{DBLP:journals/tmc/HoangNLTH24,
	author = {Tran Manh Hoang and
                  Ba Cao Nguyen and
                  Thi Thanh Huyen Le and
                  Xuan Nam Tran and
                  Pham Thanh Hiep},
	title = {Finite Block Length {NOMA} {MU} Pairing UAV-Enable System: Performance
                  Analysis and Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9804--9820},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3368159},
	doi = {10.1109/TMC.2024.3368159},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HoangNLTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper analyzes and solves the optimization problem of the performance of a multi-antenna unmanned aerial vehicle (UAV)-aided NOMA multi-user pairing system. Based on the approximation of the Gaussian-Chebyshev quadrature and incomplete Gamma function, we successfully derive the exact and asymptotic closed-form expressions for the block error rate, throughput, and goodput of the system. Moreover, we also provide other metrics such as the latency, reliability, and age of information (AoI). The results indicate that the considered system meets the requirements of ultra-reliable and low-latency communications (URLLC). Particularly, its reliability is higher than 99.99% and latency is less than 1 ms. The optimization of the UAV's altitude, block length, and the number of transmission bits to maximize the throughput and minimize the latency and AoI of the considered system is also studied by using iterative algorithms. Furthermore, we propose an algorithm to optimize the transmission power of UAVs to maximize the system's energy efficiency. The greedy algorithm is used to schedule paired users and achieves better performance compared to the random schedule. Moreover, applying multiple antennas and maximum ratio transmission can significantly improve the latency and AoI performance of the considered system. The accuracy of analytical frameworks is verified by simulation results.}
}


@article{DBLP:journals/tmc/MaRCWZ24,
	author = {Buyun Ma and
                  Zhiyuan Ren and
                  Wenchi Cheng and
                  Jingqing Wang and
                  Wei Zhang},
	title = {Latency-Constrained Multi-User Efficient Task Scheduling in Large-Scale
                  Internet of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9821--9834},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367459},
	doi = {10.1109/TMC.2024.3367459},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/MaRCWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by the tremendous demand for real-time data processing in the Internet of Vehicles (IoV), edge computing is envisioned as a promising solution to alleviate the resource limitation on vehicles. Current works on edge task scheduling simply optimize the total system cost and ignore the various constraints of applications, which will result in the reduction of the task completion rate and even cause security accidents. Although few works studied the multi-task deadline-constrained scheduling problem, their complexity is too high, resulting in the explosive growth of the runtime. Spurred by the above issues, the multi-task scheduling problem is formulated to maximize the task completion rate. Further, a Multi-user Efficient Task Scheduling (METS) algorithm is proposed to solve the formulated problem, which consists of three key components: 1) the dominating set-based network clustering that aims to reduce the network scale, 2) the matching-based task assignment to assign tasks that are modeled by the Directed Acyclic Graph (DAG) to their proper clusters, and 3) the intra-cluster DAG scheduling to schedule DAGs to the proper network nodes. Simulation results show that the proposed METS algorithm can significantly improve the task completion rate and reduce the algorithm runtime in an IoV environment with thousand-level network scale and thousand-level task requests.}
}


@article{DBLP:journals/tmc/WangHZHCJZL24,
	author = {Hongbo Wang and
                  Jingyang Hu and
                  Tianyue Zheng and
                  Jingzhi Hu and
                  Zhe Chen and
                  Hongbo Jiang and
                  Yuanjin Zheng and
                  Jun Luo},
	title = {MuKI-Fi: Multi-Person Keystroke Inference With BFI-Enabled Wi-Fi Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9835--9850},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3368339},
	doi = {10.1109/TMC.2024.3368339},
	timestamp = {Sun, 06 Oct 2024 21:41:31 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangHZHCJZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The contact-free sensing nature of Wi-Fi has been leveraged to achieve privacy breaches such as keystroke inference (KI). However, the use of channel state information (CSI) in existing attacks is highly questionable due to its signal instability and hardness to acquire. Moreover, such Wi-Fi-based attacks are confined to only one victim because Wi-Fi sensing offers insufficient range resolution to physically differentiate multiple victims. To this end, we propose MuKI-Fi to enable, for the first time, multi-person KI, leveraging beamforming feedback information (BFI), a new feature offered by latest Wi-Fi hardware, transmitted in clear-text by smartphones. BFI's characteristics, clear-text communication and signal stability, make it readily acquirable and usable by any other Wi-Fi devices switching to monitor mode without the need for low-level hacking on hardware. Moreover, to improve upon existing KI methods offering very limited generalizability across diversified application scenarios, MuKI-Fi innovates in an adversarial learning scheme to enable its inference generalizable towards unseen scenarios. Finally, we discover that, as a smartphone is in close proximity to a victim, the variations of BFI caused by that victim's keystrokes in such near-field substantially outweigh those caused by other distant victims; this phenomenon naturally allows for multi-person KI. Our extensive evaluations clearly demonstrate that MuKI-Fi can effectively eavesdrop on the keystrokes of multiple subjects, achieving 87.1% accuracy for individual keystrokes and up to 81% top-100 accuracy for stealing passwords from mobile applications (e.g., WeChat) on average.}
}


@article{DBLP:journals/tmc/WuZYG24,
	author = {Jiehong Wu and
                  Jianzhou Zhou and
                  Lei Yu and
                  Lijun Gao},
	title = {{MAC} Optimization Protocol for Cooperative {UAV} Based on Dual Perception
                  of Energy Consumption and Channel Gain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9851--9862},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3372253},
	doi = {10.1109/TMC.2024.3372253},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {FANET (Fly-Adhoc-Network) does not rely on pre-built infrastructure, and can form a temporary network through wireless links anytime and anywhere, which has been widely used in emergency communication and disaster relief. In order to solve the problem of signal transmission fading caused by heavy fog, heavy smoke and other harsh environments, and data packet transmission failure caused by high speed movement of unmanned aerial vehicle (UAV), a cooperative transmission mode is adopted to select a relay at the data link layer for packet forwarding, which effectively improves the network communication performance. Aiming at the problem of limited communication of UAV swarm in harsh environment, a cooperative medium access control protocol named energy consumption and channel gain cooperative medium access control protocol (EC-CMAC) for FANET is proposed. The protocol estimate the transmit power according to the channel propagation model, and further propose a relay selection strategy according to the estimate transmit power, the residual energy of the node, the direction and position of the node, and adaptively selecting the transmission mode. During cooperative transmission, it selected one-hop neighbors to forward the message, and extended the network lifetime by optimizing the energy consumption of the UAVs. The protocol is verified both statically and dynamically in MATLAB environment. The evaluation results of end-to-end delay, network throughput, network lifetime and packet transmission ratio testify that EC-CMAC can achieve longer network lifetime and higher packet delivery rate with little delay and throughput loss in harsh transmission environment and high speed movement of UAVs.}
}


@article{DBLP:journals/tmc/ZhangGLSYC24,
	author = {Tong Zhang and
                  Yu Gou and
                  Jun Liu and
                  Shanshan Song and
                  Tingting Yang and
                  Jun{-}Hong Cui},
	title = {Joint Link Scheduling and Power Allocation in Imperfect and Energy-Constrained
                  Underwater Wireless Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9863--9880},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3368425},
	doi = {10.1109/TMC.2024.3368425},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangGLSYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater wireless sensor networks (UWSNs) stand as promising technologies facilitating diverse underwater applications. However, the major design issues of the considered system are the severely limited energy supply and unexpected node malfunctions. This paper aims to provide fair, efficient, and reliable (FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs). Therefore, we formulate a FER-communication optimization problem (FERCOP) and propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through joint link scheduling and power allocation, which automatically learns scheduling algorithms without human intervention. However, conventional RL methods are unable to address the challenges posed by underwater environments and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs and propose an advanced training mechanism to deal with complex acoustic channels, limited energy supplies, and unexpected node malfunctions. Simulation results demonstrate the superiority of the proposed ICRL-JSA scheme with an advanced training mechanism compared to various benchmark algorithms.}
}


@article{DBLP:journals/tmc/HeYQ24,
	author = {Xingqiu He and
                  Chaoqun You and
                  Tony Q. S. Quek},
	title = {Age-Based Scheduling for Mobile Edge Computing: {A} Deep Reinforcement
                  Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9881--9897},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3370101},
	doi = {10.1109/TMC.2024.3370101},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HeYQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of Mobile Edge Computing (MEC), various real-time applications have been deployed to benefit people's daily lives. The performance of these applications relies heavily on the freshness of collected environmental information, which can be quantified by its Age of Information (AoI). In the traditional definition of AoI, it is assumed that the status information can be actively sampled and directly used. However, for many MEC-enabled applications, the desired status information is updated in an event-driven manner and necessitates data processing. To better serve these applications, we propose a new definition of AoI and, based on the redefined AoI, we formulate an online AoI minimization problem for MEC systems. Notably, the problem can be interpreted as a Markov Decision Process (MDP), thus enabling its solution through Reinforcement Learning (RL) algorithms. Nevertheless, the traditional RL algorithms are designed for MDPs with completely unknown system dynamics and hence usually suffer long convergence times. To accelerate the learning process, we introduce Post-Decision States (PDSs) to exploit the partial knowledge of the system's dynamics. We also combine PDSs with deep RL to further improve the algorithm's applicability, scalability, and robustness. Numerical results demonstrate that our algorithm outperforms the benchmarks under various scenarios.}
}


@article{DBLP:journals/tmc/KimSKJ24,
	author = {Cheonyong Kim and
                  Walid Saad and
                  Taehyoung Kim and
                  Minchae Jung},
	title = {Asymptotic Achievable Rate and Scheduling Gain in RIS-Aided Massive
                  {MIMO} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9898--9912},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3371534},
	doi = {10.1109/TMC.2024.3371534},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/KimSKJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) is a promising solution to support a large volume of data traffic and massive connectivity for future wireless mobile networks. Especially, RIS can mitigate the drawbacks in massive multiple-input multiple-output (MIMO) systems, such as the blockage caused by obstacles and the signal processing overhead by constructively and passively reflecting the incident wave toward the destination. In this paper, we provide an asymptotic analysis of the distribution of sum rate (SR) in RIS-aided massive MIMO systems. Using the asymptotic distribution of SR, the achievable scheduling gain and the optimal number of users are determined. In addition, we examined the channel hardening effect and outage probability through the achievable scheduling gain, and the optimal number of users is utilized to develop a low-complexity scheduling algorithm. Simulation results reveal that the SR obtained from our analysis closely aligns with the actual SR. The results also show that the channel hardening effect can vanish with many users thereby achieving the multiuser diversity gain, and an RIS-aided system is more reliable than a conventional massive MIMO system in terms of the outage probability. Furthermore, the proposed scheduling algorithm is shown to reduce computational complexity compared to the conventional scheduling algorithm.}
}


@article{DBLP:journals/tmc/WuDJW24,
	author = {Haiqin Wu and
                  Boris D{\"{u}}dder and
                  Shunrong Jiang and
                  Liangmin Wang},
	title = {VP{\textdollar}{\^{}}\{2\}{\textdollar}2-Match: Verifiable Privacy-Aware
                  and Personalized Crowdsourcing Task Matching via Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9913--9930},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3369085},
	doi = {10.1109/TMC.2024.3369085},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuDJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-aware task allocation/matching has been an active research focus in crowdsourcing. However, existing studies focus on an honest-but-curious assumption and a single-attribute matching model. There is a lack of adequate attention paid to scheme designs against malicious behaviors and supporting user-side personalized task matching over multiple attributes. A few recent works employ blockchain and cryptographic techniques to decentralize the matching procedure with verifiable and privacy-preserving on-chain executions. However, they still bear expensive on-chain overhead. In this paper, we propose VP^{2}-Match, a blockchain-assisted (publicly) verifiable privacy-aware crowdsourcing task matching scheme with personalization. VP^{2}-Match extends symmetric hidden vector encryption for user-side expressive matching without compromising their privacy. It avoids costly on-chain matching by letting the blockchain only store evidence/proofs for public verifiability of the matching correctness and for enforcing fair interactions against misbehaviors. Specifically, we construct extended attribute sets and solve matching verification by an algorithmic reduction into subset verification with an accumulator for proof generation. Formal security proof and extensive comparison experiments on Ethereum demonstrate the provable security and better performance of VP^{2}-Match, respectively.}
}


@article{DBLP:journals/tmc/SunWYSYJT24,
	author = {Ranran Sun and
                  Huihui Wu and
                  Bin Yang and
                  Yulong Shen and
                  Weidong Yang and
                  Xiaohong Jiang and
                  Tarik Taleb},
	title = {On Covert Rate in Full-Duplex D2D-Enabled Cellular Networks With Spectrum
                  Sharing and Power Control},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9931--9945},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3371377},
	doi = {10.1109/TMC.2024.3371377},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/SunWYSYJT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the fundamental covert rate performance in a D2D-enabled cellular network consisting of a cellular user Alice, a base station BS, an active warden Willie, and a D2D pair with a transmitter D_{t} and a full-duplex receiver D_{r}. To conduct covert communication between Alice and BS, the full-duplex D_{r} transmits jamming signal to confuse the active Willie and also receives signal from D_{t} simultaneously. With spectrum sharing, D_{t} can operate over either an underlay mode reusing cellular spectrum or an overlay mode using dedicated spectrum. With power control, D_{r} can send jamming signal to confuse Willie's detection of the transmission from Alice. We first provide theoretical results for the outage probabilities of the cellular and D2D transmissions, the average minimum detection error probability at Willie, and the achievable covert rate from Alice to BS. We then explore the power control for covert rate maximization (CRM) under the underlay mode, as well as the joint designs of power control and spectrum partition for CRM under the overlay mode. We further consider a mode selection that flexibly switches between these two modes with a probability, and also investigate the covert rate modeling and joint designs of power control, spectrum partition and mode selection probability for CRM. Finally, numerical results are presented to illustrate the covert rate performances of the network under the underlay mode, overlay mode and mode selection.}
}


@article{DBLP:journals/tmc/ShiWSLWYG24,
	author = {Yifan Shi and
                  Kang Wei and
                  Li Shen and
                  Jun Li and
                  Xueqian Wang and
                  Bo Yuan and
                  Song Guo},
	title = {Efficient Federated Learning With Enhanced Privacy via Lottery Ticket
                  Pruning in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9946--9958},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3370967},
	doi = {10.1109/TMC.2024.3370967},
	timestamp = {Mon, 30 Sep 2024 07:54:34 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ShiWSLWYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) can train collaboratively with several mobile terminals (MTs), which faces critical challenges in communication, resource, and privacy. Existing privacy-preserving methods usually adopt instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: performance degradation, transmission overhead, and resource constraints. Therefore, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with Lottery Ticket Hypothesis (LTH) and zero-concentrated DP (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global model structure; (ii) the iterative pruning further shrinks the size of the pruned model. Meanwhile, the performance of Fed-LTP is boosted via model validation based on the Laplace mechanism. On the client side, we use sparse-to-sparse training to solve the resource-constraints issue and provide tighter privacy analysis to reduce the privacy budget. We evaluate the effectiveness of Fed-LTP on several real-world datasets in both independent and identically distributed (IID) and non-IID settings. The results confirm the superiority of Fed-LTP over state-of-the-art (SOTA) methods in communication, computation, and memory efficiencies while realizing a better utility-privacy trade-off.}
}


@article{DBLP:journals/tmc/XueCNLX24,
	author = {Huansheng Xue and
                  Honglong Chen and
                  Zhichen Ni and
                  Xiaolong Liu and
                  Feng Xia},
	title = {Towards Maximizing Coverage of Targets for WRSNs by Multiple Chargers
                  Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9959--9970},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3369054},
	doi = {10.1109/TMC.2024.3369054},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XueCNLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, wireless rechargeable sensor networks (WRSNs) have gained significant attention in the research community due to the current advancements in wireless power transfer technology. In mobile charger scheduling, previous works primarily emphasized the survival rate of sensor nodes. However, the primary task of a WRSN is to monitor targets in a given area. Therefore, the coverage of targets (CoT) maximization should be the primary objective of mobile charger scheduling. In this paper, we shift the focus to the CoT maximization on-demand charging scheduling problem, and formulate it as a multi-objective optimization problem, aiming to simultaneously enhance the average coverage and energy efficiency. We prove that the problem is NP-hard by reformulating it as a Multiple Travelling Salesman Problem with Deadline. We first propose the multiple chargers scheduling scheme for maximizing coverage of targets called MaxCov, which is designed to optimize the charging scheduling process and improve network performance in terms of coverage. Then, we further propose the multiple chargers scheduling scheme based on requests grouping called MaxCov-RG, which can well balance the trade-off between the performance and computational complexity. Finally, we validate the effectiveness of the proposed schemes via extensive simulations.}
}


@article{DBLP:journals/tmc/ZilbermanDS24,
	author = {Aviram Zilberman and
                  Amit Dvir and
                  Ariel Stulman},
	title = {{SPRINKLER:} {A} Multi-RPL Man-in-the-Middle Identification Scheme
                  in IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9971--9988},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3370894},
	doi = {10.1109/TMC.2024.3370894},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZilbermanDS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber-threat protection is one of the most challenging research branches of Internet-of-Things (iot). With the exponential increase of tiny connected devices, the battle between friend and foe intensifies. Unfortunately, iot devices offer very limited security features, laying themselves wide open to new attacks, inhibiting the expected global adoption of iot technologies. Moreover, existing prevention and mitigation techniques and intrusion detection systems handle attack anomalies rather than the attack itself while using a significant amount of the network resources. rpl, the de-facto routing protocol for iot, proposes minimal security features that cannot handle internal attacks. Hence, in this paper, we propose sprinkler, which identifies the specific thing that is under attack by an adversarial Man-in-The-Middle. sprinkler uses the multi-instance feature of rpl to identify the adversary. The proposed solution adheres to two basic principles: it only uses pre-existing standard routing protocols and does not rely on a centralized or trusted third-party node such as a certificate authority. All information must be gleaned by each node using only primitives that already exist in the underlying communication protocol, which excludes any training dataset. Simulations show that sprinkler adds minimal maintenance and energy expenditure while pinpointing deterministically the attacker in the network. In particular, sprinkler has a message delivery rate and detection rate of 100%.}
}


@article{DBLP:journals/tmc/WeiG24,
	author = {Zhuangkun Wei and
                  Weisi Guo},
	title = {Control Layer Security: Exploiting Unobservable Cooperative States
                  of Autonomous Systems for Secret Key Generation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {9989--10000},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3369754},
	doi = {10.1109/TMC.2024.3369754},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WeiG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid growth of autonomous systems (ASs) with data sharing means new cybersecurity methods have to be developed for them. Existing computational complexity-based cryptography does not have information-theoretical bounds and poses threats to superior computational attackers. This post-quantum cryptography issue indeed motivated the rapid advances in using common physical layer properties to generate symmetrical cipher keys (known as PLS). However, PLS remains sensitive to attackers (e.g., jamming) that destroy its prerequisite wireless channel reciprocity. When ASs are in cooperative tasks (e.g., rescuing searching, and formation flight), they will behave cooperatively in the control layer. Inspired by this, we propose a new security mechanism called control layer security (CLS), which exploits the correlated but unobservable states of cooperative ASs to generate symmetrical cipher keys. This idea is then realized in the linearized UAV cooperative control scenario. The theoretical correlation coefficients between Alice's and Bob's states are computed, based on which common feature selection and key quantization steps are designed. The results from simulation and real UAV experiments show i) an approximately 90% key agreement rate is achieved, and ii) even an Eve with the known observable states and systems fails to estimate the unobservable states and the secret keys relied upon, due to the multiple-to-one mapping from unobservable states (pitch, roll and yaw angles) to the observable states (3D trajectory). This demonstrates CLS as a promising candidate to secure the communications of ASs, especially in the adversarial radio environment with attackers that destroys the prerequisite for current PLS.}
}


@article{DBLP:journals/tmc/LiZWQYNS24,
	author = {Peichun Li and
                  Hanwen Zhang and
                  Yuan Wu and
                  Li Ping Qian and
                  Rong Yu and
                  Dusit Niyato and
                  Xuemin Shen},
	title = {Filling the Missing: Exploring Generative {AI} for Enhanced Federated
                  Learning Over Heterogeneous Mobile Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10001--10015},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3371772},
	doi = {10.1109/TMC.2024.3371772},
	timestamp = {Sat, 11 Jan 2025 00:33:55 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZWQYNS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Artificial Intelligence (AI) model training over mobile edge networks encounters significant challenges due to the data and resource heterogeneity of edge devices. The former hampers the convergence rate of the global model, while the latter diminishes the devices’ resource utilization efficiency. In this paper, we propose a generative AI-empowered federated learning to address these challenges by leveraging the idea of FIlling the MIssing (FIMI) portion of local data. Specifically, FIMI can be considered as a resource-aware data augmentation method that effectively mitigates the data heterogeneity while ensuring efficient FL training. We first quantify the relationship between the training data amount and the learning performance. We then study the FIMI optimization problem with the objective of minimizing the device-side overall energy consumption subject to required learning performance constraints. The decomposition-based analysis and the cross-entropy searching method are leveraged to derive the solution, where each device is assigned suitable AI-synthetic data and resource utilization policy. Experiment results demonstrate that FIMI can save up to 50% of the device-side energy to achieve the target global test accuracy in comparison with the existing methods. Meanwhile, FIMI can significantly enhance the converged global accuracy under the non-independently-and-identically distribution (non-IID) data.}
}


@article{DBLP:journals/tmc/WuCXZSYLJ24,
	author = {Cong Wu and
                  Hangcheng Cao and
                  Guowen Xu and
                  Chengjie Zhou and
                  Jianfei Sun and
                  Ran Yan and
                  Yang Liu and
                  Hongbo Jiang},
	title = {It's All in the Touch: Authenticating Users With {HOST} Gestures
                  on Multi-Touch Screen Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10016--10030},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3371014},
	doi = {10.1109/TMC.2024.3371014},
	timestamp = {Thu, 26 Sep 2024 17:09:54 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuCXZSYLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As smartphones proliferate, secure and user-friendly authentication methods are increasingly critical. Existing behavioral biometrics, however, are often compromised by behavior variability, leading to poor authentication accuracy and an unsatisfactory user experience. To fill this gap, we propose BioHold, a new robust and reliable user authentication method, fusing finger behavior and hand geometry, captured via a smartphone's multitouch screen during natural holding gestures. It synergistically fuses behavioral and physiological biometrics. In contrast to traditional methods that require restrictive, unnatural user patterns, our approach utilizes a stable, natural gesture for authentication, effectively mitigating behavior variability. It enables one-handed authentication through familiar smartphone-holding and unlocking gestures. During this interaction, hand geometry and behavioral characteristics are recorded for subsequent authentication. We evaluate our method using a dataset collected from 20 subjects, demonstrating its resilience against behavioral variability over time while maintaining a high level of distinctiveness. With only 10 training samples, our method achieves an equal error rate of 3.59%, which improves to 1.25% with 40 training samples. Importantly, our method is resistant to common security threats such as zero-effort attacks, smudge attacks, and shoulder surfing attacks. A usability study confirms the method's high user acceptance, as measured by the system usability score.}
}


@article{DBLP:journals/tmc/YaoLXWQL24,
	author = {Zhiwei Yao and
                  Jianchun Liu and
                  Hongli Xu and
                  Lun Wang and
                  Chen Qian and
                  Yunming Liao},
	title = {Ferrari: {A} Personalized Federated Learning Framework for Heterogeneous
                  Edge Clients},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10031--10045},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3370961},
	doi = {10.1109/TMC.2024.3370961},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/YaoLXWQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated semi-supervised learning (FSSL) has been proposed to address the insufficient labeled data problem by training models with pseudo-labeling. In previous FSSL systems, a single global model is always trained without an equivalent generalization ability for the clients under the non-IID setting. Accordingly, model personalization methods have been proposed to overcome this problem. Intuitively, seeking labeling assistance from other clients with similar data distribution, i.e., model migration, can effectively improve the personalization on the clients with scarce labeled data. However, previous works require to migrate a pre-fixed number of models among the clients, causing unnecessary resource waste and accuracy degradation due to resource heterogeneity. Considering that the number of model migrations and the quality of pseudo-labels have a significant impact on the training performance (e.g., efficiency and accuracy), we propose a novel personalized FSSL system, called Ferrari, to boost the efficiency of pseudo-labeling and training accuracy through adaptive model migrations among the clients. Specifically, Ferrari first generates the similarity-based ranking using a Gaussian KD-Tree, considering the varied data distributions among the clients. Combined with the ranking and clients’ heterogeneous resource constraints, Ferrari then adaptively determines the proper model migration policy and confidence thresholds for high-quality pseudo-labeling and personalized training for clients. Extensive experiments on a physical platform show that Ferrari provides a 1.2\\sim 5.5\\times\nspeedup without sacrificing model accuracy, compared to existing methods.}
}


@article{DBLP:journals/tmc/WangZWDZ24,
	author = {Qingshan Wang and
                  Zhiwen Zheng and
                  Qi Wang and
                  Dazhu Deng and
                  Jiangtao Zhang},
	title = {Generalizations of Wearable Device Placements and Sentences in Sign
                  Language Recognition With Transformer-Based Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10046--10059},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373472},
	doi = {10.1109/TMC.2024.3373472},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WangZWDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sign language is widely used among deaf. Many existing studies on Sign Language Recognition (SLR) focus on addressing communication barriers between deaf and hearing people. However, current studies face two challenges: the recognition result is highly dependent on the wearable device placements, and the existence of sentences in the training set. To address the challenges, this paper proposed EasyHear – a Transformer-based Chinese SLR system with a generalization approach. For generalization of wearable device placements, an anchor-based signal rotation correction algorithm is proposed to eliminate the impact of variations in wearing positions. In terms of generalization of sentences, a gesture code is constructed to reflect the closeness of gestures after defining an intimate entropy of gestures. Moreover, a semantic code is developed by training a neural network on a large corpus to reflect the intimacy of Chinese Sign Language (CSL) words on unseen sentences in the training set. A Transformer-based model combined with the rotated gesture signals, as well as gesture and semantic codes is suggested to improve recognition performance across various wearing positions and sentences. The results show that EasyHear achieved an average word error rate of 21.60% for samples of 712 commonly used CSL sentences.}
}


@article{DBLP:journals/tmc/FuXZWZ24,
	author = {Luoyi Fu and
                  Jiasheng Xu and
                  Lei Zhou and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Analyzing Information Cascading in Large Scale Networks: {A} Fixed
                  Point Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10060--10076},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373622},
	doi = {10.1109/TMC.2024.3373622},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/FuXZWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information cascading, referred as the phenomenon of an individual following the behavior of the preceding individual after observing its actions, is prevalent in real social networks and triggers intense research interests for the purpose of monitoring and controlling network epidemics. One of the typical lines of information cascading study belongs to the Influence Maximization Problem, which aims to algorithmically find the optimal seeds that can spread the information to the maximum number of nodes. Regardless of the tremendous efforts made in various algorithm design of finding such optimal seeds, it has not yet been well understood how the absolute influence power of the “optimal” source set affects the ultimate cascading, i.e., under which conditions the seeds are able or unable to influence an substantial fraction of the entire network. Most existing works have investigated the conditions of network scale influence under linear threshold model, where the activation of a node requires a large number of infected neighbors. Instead, in this paper we focus on the case of single source cascading, which is only possible to occur under the independent cascading model. We launch information cascading analysis from two aspects, i.e., the influence scale and network-scale cascading probability. Firstly, percolation analysis of the cascading outcome shows that estimating influence scale is equivalent to solving fixed point equations. Then, we investigate the speed and stability of information cascading based on fixed point analysis, which shows that the information cascading process almost surely terminates within logarithmic time complexity. Furthermore, the results are generalized to the stochastic block model, where we find that network-scale cascading is determined by the spectral radius of the community matrix. The analysis presented in this paper could help us better understand the conditions for different information cascading outcomes.}
}


@article{DBLP:journals/tmc/LuoWY24,
	author = {Fucai Luo and
                  Haiyan Wang and
                  Xingfu Yan},
	title = {Re-PAEKS: Public-Key Authenticated Re-Encryption With Keyword Search},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10077--10092},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373626},
	doi = {10.1109/TMC.2024.3373626},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LuoWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of cloud computing and the exponential growth of data have led to an increasing demand for secure data sharing and querying. Proxy re-encryption (PRE) addresses the issue of secure data sharing, since it enables controlled data sharing and delegation of access rights without revealing the actual content of the encrypted data stored in the cloud; public-key encryption with keyword search (PEKS) tackles the issue of secure data querying, since it allows resource-constrained clients to effectively search over encrypted data stored in the cloud. As a combination of PRE and PEKS, proxy re-encryption with keyword search (PRES) enables both secure data sharing and querying. Despite their merits, existing PRES schemes are vulnerable to quantum computer attacks, keyword guessing attacks (KGAs), or incur high end-to-end delay. To address these vulnerabilities, this paper introduces a novel cryptographic primitive called public-key authenticated re-encryption with keyword search (Re-PAEKS), which combines the strengths of PRE and public-key authenticated encryption with keyword search (PAEKS). Our Re-PAEKS has low end-to-end delay, and is resistant to both quantum computer attacks and KGAs. Technically, we improve the previous lattice-based PAEKS scheme and achieve the delegation of access rights by exploiting the lattice-based identity-based encryption (IBE) techniques, which are widely believed to be secure against quantum computer attacks. In addition, we formalize the security model of the Re-PAEKS and prove its security in the random oracle model. Finally, we conduct a comprehensive performance evaluation of the Re-PAEKS, and the experimental results show that the Re-PAEKS is computationally efficient and practical. Particularly, the Re-PAEKS enjoys the lowest end-to-end delay compared to current state-of-the-art PRES.}
}


@article{DBLP:journals/tmc/ZhanWDSLXTWC24,
	author = {Zhongwei Zhan and
                  Yingjie Wang and
                  Peiyong Duan and
                  Akshita Maradapu Vera Venkata Sai and
                  Zhaowei Liu and
                  Chaocan Xiang and
                  Xiangrong Tong and
                  Weilong Wang and
                  Zhipeng Cai},
	title = {Enhancing Worker Recruitment in Collaborative Mobile Crowdsourcing:
                  {A} Graph Neural Network Trust Evaluation Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10093--10110},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373469},
	doi = {10.1109/TMC.2024.3373469},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanWDSLXTWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Mobile Crowdsourcing (CMCS) allows platforms to recruit worker teams to collaboratively execute complex sensing tasks. The efficiency of such collaborations could be influenced by trust relationships among workers. To obtain the asymmetric trust values among all workers in the social network, the Trust Reinforcement Evaluation Framework (TREF) based on Graph Convolutional Neural Networks (GCNs) is proposed in this paper. The task completion effect is comprehensively calculated by considering the workers’ ability benefits, distance benefits, and trust benefits in this paper. The worker recruitment problem is modeled as an Undirected Complete Recruitment Graph (UCRG), for which a specific Tabu Search Recruitment (TSR) algorithm solution is proposed. An optimal execution team is recruited for each task by the TSR algorithm, and the collaboration team for the task is obtained under the constraint of privacy loss. To enhance the efficiency of the recruitment algorithm on a large scale and scope, the Mini-Batch K-Means clustering algorithm and edge computing technology are introduced, enabling distributed worker recruitment. Lastly, extensive experiments conducted on five real datasets validate that the recruitment algorithm proposed in this paper outperforms other baselines. Additionally, TREF proposed herein surpasses the performance of state-of-the-art trust evaluation methods in the literature.}
}


@article{DBLP:journals/tmc/XuLYWCLXH24,
	author = {Zhuqing Xu and
                  Junzhou Luo and
                  Zhimeng Yin and
                  Shuai Wang and
                  Ciyuan Chen and
                  Jingkai Lin and
                  Runqun Xiong and
                  Tian He},
	title = {Leveraging Imperfect-Orthogonality Aware Scheduling for High Scalability
                  in {LPWAN}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10111--10129},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373413},
	doi = {10.1109/TMC.2024.3373413},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/XuLYWCLXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging Low-Power Wide Area Networks (LPWAN) technology, LoRa is dedicated to providing long-range connections for pervasive Internet-of-Things devices. As LoRa operates in the unlicensed spectrum with an ALOHA-based MAC-layer protocol stack, transmissions from multiple LoRa end-devices inevitably collide with each other, leading to packet losses and increased transmission delay. Targeting at collisions caused by interferences under the same spreading factor (SF) settings, researchers introduce multiple lines of techniques. Despite their efforts, these techniques commonly neglect the potential collisions caused by interferences under different SF settings, resulting in imperfect orthogonality. Given the disparate transmission power configurations and diverse deployed locations, the collisions under different SFs commonly exist in practical networks and significantly limit the LoRa reliability. This paper presents X-MAC, the first scheduler aware of imperfect orthogonality. Technically, X-MAC detects the collisions under different SFs via tracking historical transmissions, and performs dynamic channel scheduling to avoid collisions caused by interferences under the same and different SFs. Extensive evaluations on testbed devices show that, compared with the state-of-the-art methods, X-MAC boosts the network scalability (number of concurrent end-devices) by 1.26× to 2.41× with packet reception rate requirement of > 95%.}
}


@article{DBLP:journals/tmc/LiuHX24,
	author = {Linfeng Liu and
                  Zhuo Huang and
                  Jia Xu},
	title = {Multi-Agent Deep Reinforcement Learning Based Scheduling Approach
                  for Mobile Charging in Internet of Electric Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10130--10145},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373410},
	doi = {10.1109/TMC.2024.3373410},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile charging stations (MCSs) have become an indispensable complement of fixed charging stations. In the regions where fixed charging stations are sparsely deployed or even absent, the main concern is that how to properly schedule MCSs to charge the electric vehicles with insufficient electricity (EVCs). In this paper, we focus on the scheduling of idle MCSs and pending EVCs. To increase the charging revenue of MCSs and enhance the proportion of successfully charged EVCs, we schedule idle MCSs to proactively track some EVCs with potential charging demand, and schedule pending EVCs to approach some busy MCSs for potential charging opportunities. To this end, a Scheduling Approach based on Multi-Agent Deep Reinforcement Learning (SA-MADRL) is proposed to train the scheduling models for agents (idle MCSs and pending EVCs). In SA-MADRL, the agents obtain the local observations to make the scheduling decisions. Both idle MCSs and pending EVCs can independently make the scheduling decisions, and thus SA-MADRL can realize the fully distributed scheduling and has a good scalability. Extensive simulations and comparisons demonstrate the performance superiority of SA-MADRL, i.e., the charging revenue of MCSs can be significantly increased, and the proportion of successfully charged EVCs can be effectively enhanced.}
}


@article{DBLP:journals/tmc/AlQuraanZCSMIM24,
	author = {Mohammad Al{-}Quraan and
                  Ahmed Zoha and
                  Anthony Centeno and
                  Haythem Bany Salameh and
                  Sami Muhaidat and
                  Muhammad Ali Imran and
                  Lina S. Mohjazi},
	title = {Enhancing Reliability in Federated mmWave Networks: {A} Practical
                  and Scalable Solution Using Radar-Aided Dynamic Blockage Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10146--10160},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373529},
	doi = {10.1109/TMC.2024.3373529},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/AlQuraanZCSMIM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article introduces a new method to improve the dependability of millimeter-wave (mmWave) and terahertz (THz) network services in dynamic outdoor environments. In these settings, line-of-sight (LoS) connections are easily interrupted by moving obstacles like humans and vehicles. The proposed approach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leverages radar measurements and federated learning (FL) to train a dual-output neural network (NN) model capable of simultaneously predicting blockage status and time. This enables determining the optimal point for proactive handover (PHO) or beam switching, thereby reducing the latency introduced by 5G new radio procedures and ensuring high quality of experience (QoE). The framework employs radar sensors to monitor and track object movement, generating range-angle and range-velocity maps that are useful for scene analysis and predictions. Moreover, FL provides additional benefits such as privacy protection, scalability, and knowledge sharing. The framework is assessed using an extensive real-world dataset comprising mmWave channel information and radar data. The evaluation results show that RaDaR substantially enhances network reliability, achieving an average success rate of 94% for PHO compared to existing reactive HO procedures that lack proactive blockage prediction. Additionally, RaDaR maintains a superior QoE by ensuring sustained high throughput levels and minimising PHO latency.}
}


@article{DBLP:journals/tmc/WuZ24,
	author = {Chuxiong Wu and
                  Qiang Zeng},
	title = {Turning Noises to Fingerprint-Free "Credentials": Secure
                  and Usable Drone Authentication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10161--10174},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373503},
	doi = {10.1109/TMC.2024.3373503},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/WuZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones have been widely used in various services, such as delivery and surveillance. Authentication forms the foundation of the security of these services. However, drones are expensive and may carry important payloads. To avoid being captured by attackers, drones should keep a safe distance from the verifier before authentication succeeds. This makes authentication methods that only work in very close proximity not applicable. Our work leverages drone noises for authentication. While using sounds for authentication is highly usable, how to handle various attacks that manipulate sounds is an unresolved challenge. It is also unclear how to ensure robustness under various environmental sounds. Being the first in the literature, we address the two major challenges by exploiting unique characteristics of drone noises. We thereby build an authentication system that does not rely on any drone sound fingerprints, keeps resilient to attacks, and is robust under environmental sounds. An extensive evaluation demonstrates its security and usability.}
}


@article{DBLP:journals/tmc/LiLYZZW24,
	author = {Youqi Li and
                  Fan Li and
                  Song Yang and
                  Chuan Zhang and
                  Liehuang Zhu and
                  Yu Wang},
	title = {A Cooperative Analysis to Incentivize Communication-Efficient Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10175--10190},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373501},
	doi = {10.1109/TMC.2024.3373501},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/LiLYZZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has achieved state-of-the-art performance in training a global model in a decentralized and privacy-preserving manner. Many recent works have demonstrated that incentive mechanism is of paramount importance for the success of FL. Existing incentives to FL either neglect communication efficiency, or consider communication efficiency but design the incentive mechanisms using non-cooperative games under complete information assumption, or study incentive mechanism under incomplete information but only apply to the sequential interaction setting. We shed light on this problem from the cooperative perspective and propose an incentive mechanism for communication-efficient FL based on the Nash bargaining theory. Specially, we formulate our incentive mechanism as a one-to-many concurrent bargaining game among the aggregator and clients, and systematically analyze the Nash bargaining solution (NBS, game equilibrium) to design the incentive mechanism. It should be noted that the existing sequential bargaining is not suitable for incentivizing FL due to high (exponential) time complexity, which deteriorates the straggler problem in FL. Our formulated bargaining game is challenging due to the NP-hardness. We propose a probabilistic greedy-based client selection algorithm and derive an analytical payment solution as an approximate NBS. We prove the convergence guarantee of our incentive mechanism for communication-efficient FL. Finally, we conduct experiments over real-world datasets to evaluate the performance of our incentive mechanism.}
}


@article{DBLP:journals/tmc/HuangDWWG24,
	author = {Zhanxiang Huang and
                  Yuan Ding and
                  Dapeng Oliver Wu and
                  Shuai Wang and
                  Wei Gong},
	title = {Bitalign: Bit Alignment for Bluetooth Backscatter Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {10},
	pages = {10191--10201},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3374815},
	doi = {10.1109/TMC.2024.3374815},
	timestamp = {Fri, 20 Sep 2024 14:01:38 +0200},
	biburl = {https://dblp.org/rec/journals/tmc/HuangDWWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past decade, backscatter communications have drawn significant attention as they are an ultra-low-power solution to transmit IoT sensor data, including video and audio. However, most of the state-of-the-art backscatter systems that are fully compatible with commodity radios suffer from poor synchronization accuracy and low throughput, being unable to support various multimedia sensors. In this paper, we propose Bitalign, a Bluetooth backscatter system that can make use of uncontrolled ambient signals as excitations and deliver high throughput for multimedia streaming applications. To do so, we identify several backscatter bottlenecks and employ a set of techniques to considerably boost backscatter throughput. In particular, we introduce an identification-based synchronization method that can efficiently distinguish various ambient signals and accurately decide where to modulate. We further propose a matching-based synchronization method with higher synchronization accuracy. In addition, we propose header reconstruction to make Bitalign truly compatible with commercial Bluetooth devices. Finally, we implement a tag prototype using FPGAs and conduct extensive experiments. Results show that the minimum bit error rate of Bitalign is 0.5%, which is 60 times better than that of FreeRider, a state-of-the-art Bluetooth system that features uncontrolled excitors. The maximal theoretical throughput of Bitalign is 1.98 Mbps.}
}


@article{DBLP:journals/tmc/ZhengMXW24,
	author = {Zhenzhe Zheng and
                  Weichao Mao and
                  Yidan Xing and
                  Fan Wu},
	title = {On Designing Market Model and Pricing Mechanisms for IoT Data Exchange},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10202--10218},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373811},
	doi = {10.1109/TMC.2024.3373811},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengMXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data is becoming an important kind of commercial good, and many online marketplaces are set up to facilitate the exchange of data. However, most existing data market models and the corresponding pricing mechanisms fail to capture the unique economic properties of data. In this paper, we first characterize the new features of IoT data as a digital commodity, and then present a market model for IoT data exchange, from an information design perspective. We further propose a family of data pricing mechanisms for maximizing revenue under different information asymmetry settings. Our MSimple mechanism extracts full surplus for the model with one type of buyer in the market. When multiple types of buyers coexist, our MGeneral mechanism optimally solves the problem of revenue maximization by formulating it as a convex program with polynomial size. For a more practical setting where buyers have bounded rationality, we design the MPractical mechanism with a tight logarithmic approximation ratio. We also show that the seller can further increase revenue by offering a free data trial to the buyers. We evaluate our pricing mechanisms on a real-world ambient sound dataset. Evaluation results demonstrate that our pricing mechanisms achieve good performance and approach the optimal revenue.}
}


@article{DBLP:journals/tmc/XieSXCFB24,
	author = {Liang Xie and
                  Zhou Su and
                  Qichao Xu and
                  Nan Chen and
                  Yixin Fan and
                  Abderrahim Benslimane},
	title = {A Secure {UAV} Cooperative Communication Framework: Prospect Theory
                  Based Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10219--10234},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3367124},
	doi = {10.1109/TMC.2024.3367124},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XieSXCFB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicles (UAVs) have attracted extensive attention from both industry and academia owing to their high mobility, line-of-sight (LoS) characteristics of air-to-ground (A2G) channels, and low cost. However, the broadcast nature of wireless transmission and the LoS characteristics of A2G channels are vulnerable to eavesdropping attacks, which leads to severe security issues. To enhance the security of UAV communication, we propose a framework that multiple UAVs cooperate to resist attacks (MURA). Specifically, we first introduce an efficient incentive scheme based on the coalition game to motivate UAVs to participate in a coalition. By forming a grand coalition, we demonstrate that each individual UAV can maximize its utility. Additionally, we propose a secure UAV communication scheme designed to counter eavesdropping attacks. Two distinct types of scenarios are considered in UAV communication. In a completely rational scenario, in which participants make decisions aiming to maximize their utility, we utilize the Stackelberg game to model the interactions between UAVs and attacker. The existence and uniqueness of the equilibrium solution are proved, and the equilibrium solution is obtained. In the context of an imperfectly rational scenario, we employ the prospect theory (PT) to effectively capture the underlying rationality exhibited by the players. The PT valuations of the players, i.e., UAV and attacker, are deduced in detail. Meanwhile, the convergence of the PT valuations of UAV and attacker is proved. Finally, extensive simulation results show that the proposed scheme can effectively improve the utility of legal UAVs and ensure the security of the UAV networks compared with benchmarks.}
}


@article{DBLP:journals/tmc/WangW24a,
	author = {Cong Wang and
                  Hongyi Wu},
	title = {Energy Optimization for Federated Learning on Consumer Mobile Devices
                  With Asynchronous {SGD} and Application Co-Execution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10235--10250},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379236},
	doi = {10.1109/TMC.2024.3379236},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangW24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning relies on distributed training on mobile device. The previous research mainly focuses on addressing the heterogeneity from computation and data distributions. As battery life remains to be the performance bottleneck on mobile devices, energy consumption from the persistent training tasks poses great challenges. In this paper, we propose an online scheduler to optimize energy usage by leveraging application co-execution and asynchronous gradient updates. Motivated by a series of empirical studies, we find that placing the training process in the background while co-running a foreground application gives the system a large energy discount. Based on these findings, we first study an offline baseline assuming all the application occurrences are known in advance, and propose a dynamic programming solution. Then we propose an online scheduler using the Lyapunov framework to exploit the energy-staleness/slowdown trade-offs and prove the convergence at the rate of 1/\\sqrt{K}. We conduct extensive experiments on a mobile testbed with devices from different vendors. The results indicate 10-30% energy saving and much faster convergence compared to FedAvg and FedProx with 3-4% higher testing accuracy under the non-IID data setting. The design is also validated in terms of resource utilization, memory bandwidth and Frame-Per-Second rates.}
}


@article{DBLP:journals/tmc/MaM24,
	author = {Weibin Ma and
                  Lena Mashayekhy},
	title = {Video Offloading in Mobile Edge Computing: Dealing With Uncertainty},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10251--10264},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3369685},
	doi = {10.1109/TMC.2024.3369685},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MaM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Videos are projected to account for roughly 80% of global mobile data traffic by 2028. Many camera-equipped mobile devices, such as surveillance drones, require realtime video analytics, encompassing tasks like object detection and action recognition. These devices, restricted by limited resource constraints, need to offload videos to Mobile Edge Computing (MEC) to simultaneously optimize video analytics performance and minimize delay. However, MEC is facing significant challenges in providing efficient video offloading solutions, especially due to uncertainties caused by dynamic device mobility and the associated trade offs in selecting video quality for offloading. Offloading high-quality videos enhances video analytics performance, such as object detection accuracy. Yet, as a mobile device relocates, lower video quality or serving by a different MEC cloudlet may be required (triggering a service migration) to maintain a satisfactory service performance. In this paper, we study the Video Offloading Problem (VOP) in MEC to address these challenges. We propose two uncertainty-aware approaches that model the uncertainties in the environment to solve VOP. Our first approach, focusing on system side, is based on Two-stage Stochastic Program and proposing a unique clustering-based Sample Average Approximation to effectively solve TSP-VOP. The second approach, focusing on device side, employs an online learning algorithm based on a multi-armed bandit to learn and select the optimal offloading solution online. Through extensive experiments, we show that our proposed approaches significantly enhance video offloading decisions, with high video quality and reduced service migration costs under uncertain device mobility, compared to other benchmarks.}
}


@article{DBLP:journals/tmc/ZhengYXLM24,
	author = {Xiaolong Zheng and
                  Kun Yang and
                  Jie Xiong and
                  Liang Liu and
                  Huadong Ma},
	title = {Pushing the Limits of WiFi Sensing With Low Transmission Rates},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10265--10279},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3374046},
	doi = {10.1109/TMC.2024.3374046},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengYXLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing WiFi sensing systems transmit dedicated high-rate packets for accurate sensing. These “sensing packets” greatly affect the main data communication function of WiFi and significantly counteract the promised benefit of reusing WiFi communication for sensing. In this work, we propose WiImg2.0, a lightweight system which involves machine learning techniques to enable WiFi sensing under low packet rate, pushing WiFi sensing one step towards real-life adoption. The key idea is to convert the WiFi CSI samples into images and employ the Generative Adversarial Network (GAN) for CSI image inpainting, relaxing the requirement of high sample rate for sensing. We first recover the sensing data from the antenna spatial domain and then from the sample time domain. To avoid the large training overhead of GAN, we design a lightweight GAN that leverages samples of only three rates in a fixed window to recover the CSI traces of arbitrary rates and varying duration. Experiments show that with just 25 packets per second, WiImg2.0 is able to increase the recognition accuracy for hand gesture recognition and daily activity tracking from the state-of-the-art 59.1% and 65.9% to 86.7% and 96.4%, respectively.}
}


@article{DBLP:journals/tmc/DingLXLGD24,
	author = {Zhi Ding and
                  Yuxiang Lin and
                  Weifeng Xu and
                  Jiamei Lv and
                  Yi Gao and
                  Wei Dong},
	title = {Energy Optimization for Mobile Applications by Exploiting 5G Inactive
                  State},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10280--10295},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3377696},
	doi = {10.1109/TMC.2024.3377696},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DingLXLGD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high energy consumption of 5G New Radio (NR) poses a major challenge to user experience. A major source of energy consumption in User Equipments (UE) is the radio tail, during which the UE remains in a high-power state to release the radio sources. Existing energy optimization approaches cut radio tails by forcing the UE to enter a low-power state. However, these approaches introduce extra promotion delays and energy consumption with soon-coming data transmissions. In this paper, we first conduct an empirical study to reveal that the 5G radio tail introduces significant energy waste on UEs. Then we propose 5GSaver, a two-phase energy-saving approach that utilizes the inactive state of New Radio to better eliminate the tail phenomenon in 5G cellular networks. 5GSaver identifies the end of App communication events in the first phase and predicts the next packet arrival time in the second phase. With the learning results, 5GSaver can automatically help the UE determine which radio resource control state to enter for saving energy. We evaluate 5GSaver using 15 mobile Apps on commercial smartphones. Evaluation results show that 5GSaver can reduce radio energy consumption by 9.5% and communication delay by 12.4% on average compared to the state-of-the-art approach.}
}


@article{DBLP:journals/tmc/YanLXWQ24,
	author = {Jiaming Yan and
                  Jianchun Liu and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Chunming Qiao},
	title = {Peaches: Personalized Federated Learning With Neural Architecture
                  Search in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10296--10312},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373506},
	doi = {10.1109/TMC.2024.3373506},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YanLXWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge computing (EC), federated learning (FL) enables numerous distributed devices (or workers) to collaboratively train AI models without exposing their local data. Most works of FL adopt a predefined architecture on all participating workers for model training. However, since workers’ local data distributions vary heavily in EC, the predefined architecture may not be the optimal choice for every worker. It is also unrealistic to manually design a high-performance architecture for each worker, which requires intense human expertise and effort. In order to tackle this challenge, neural architecture search (NAS) has been applied in FL to automate the architecture design process. Unfortunately, the existing federated NAS frameworks often suffer from the difficulties of system heterogeneity and resource limitation. To remedy this problem, we present a novel framework, termed Peaches, to achieve efficient searching and training in the resource-constrained EC system. Specifically, the local model of each worker is stacked by base cell and personal cell, where the base cell is shared by all workers to capture the common knowledge and the personal cell is customized for each worker to fit the local data. We determine the number of base cells, shared by all workers, according to the bandwidth budget on the parameters server. Besides, to relieve the data and system heterogeneity, we find the optimal number of personal cells for each worker based on its computing capability. In addition, we gradually prune the search space during training to mitigate the resource consumption. We evaluate the performance of Peaches through extensive experiments, and the results show that Peaches can achieve an average accuracy improvement of about 6.29% and up to 3.97× speed up compared with the baselines.}
}


@article{DBLP:journals/tmc/ZhouLSGY24,
	author = {Huan Zhou and
                  Mingze Li and
                  Peng Sun and
                  Bin Guo and
                  Zhiwen Yu},
	title = {Accelerating Federated Learning via Parameter Selection and Pre-Synchronization
                  in Mobile Edge-Cloud Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10313--10328},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376636},
	doi = {10.1109/TMC.2024.3376636},
	timestamp = {Wed, 06 Nov 2024 22:18:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLSGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed machine learning methodology that can achieve collaborative model training among clients without collecting their private training data. Despite the great benefits in privacy protection, FL still faces challenges like limited computation capabilities of clients (e.g., end devices) and significant communication overheads when applied to mobile edge-cloud networks. To address these issues, this paper proposes a novel three-layer FL framework with Parameter Selection and Pre-synchronization (PSPFL) to achieve fast and accurate model training in mobile edge-cloud networks. The basic idea of PSPFL is that clients can select partial model parameters for transmission. Then, base stations aggregate these model parameters cooperatively (i.e., pre-synchronization) and send the aggregated results to the server for global model update periodically. However, there is an intrinsic trade-off between parameter transmission overhead and model training loss. To strike a desirable balance between them, we investigate the optimal parameter pre-synchronization round and local training round under PSPFL. Specifically, we propose an Alternating Minimization (AM) algorithm to obtain the initial local training round and parameter pre-synchronization round. Moreover, we integrate Deep Q-network with AM (namely DQNAM) to explore and update the optimal solution. Finally, extensive simulations are conducted to evaluate the performance of the proposed method on commonly used datasets. The results show that the proposed method can reduce the sum of FL completion time and training loss by an average of 20.72% - 69.25% compared to benchmark methods.}
}


@article{DBLP:journals/tmc/RenDQLLWN24,
	author = {Xiaoxu Ren and
                  Hongyang Du and
                  Chao Qiu and
                  Tao Luo and
                  Zejun Liu and
                  Xiaofei Wang and
                  Dusit Niyato},
	title = {Dual-Level Resource Provisioning and Heterogeneous Auction for Mobile
                  Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10329--10343},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3377211},
	doi = {10.1109/TMC.2024.3377211},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RenDQLLWN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of the mobile Metaverse has garnered increasing attention in the next-generation Internet, fueled by the rapid advancements of mobile Internet, communication, and computing technologies. With the resource limitations faced by mobile Metaverse users (MUs), the mobile Metaverse market is flourishing. This market enables MUs to access high-quality immersive experiences by trading resources with Metaverse service providers (MSPs) across geographically distributed resource pools. However, the mobile Metaverse market still faces several challenges, including the hierarchical mobile Metaverse service structure, temporal dependencies, and heterogeneous incentive mechanisms. To address these problems, this paper proposes a dual-level resources trading approach for mobile Metaverse based on blockchain. This approach employs a dual-level structure consisting of resource provisioning and heterogeneous auction mechanisms. Specifically, we formulate the resource provisioning as a temporal-dependent average delay minimization problem at the low level. To solve this low-level problem, we introduce a novel algorithm called LyDif, which leverages Lyapunov optimization techniques and diffusion models. At the high level, we propose a price-guided double dutch auction (PG-DDA) mechanism to match heterogeneous resources and determine pricing strategies. The PG-DDA smart contract is deployed on a consortium blockchain platform, facilitating resource trading management and transaction monitoring. Based on a real trace of edge-cloud service requests, our experimental results demonstrate the effectiveness of our proposed scheme in achieving optimal latency and social welfare.}
}


@article{DBLP:journals/tmc/WangDNKXRMS24,
	author = {Jiacheng Wang and
                  Hongyang Du and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Deepu Rajan and
                  Shiwen Mao and
                  Xuemin Shen},
	title = {A Unified Framework for Guiding Generative {AI} With Wireless Perception
                  in Resource Constrained Mobile Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10344--10360},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3377226},
	doi = {10.1109/TMC.2024.3377226},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangDNKXRMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the significant advancements in artificial intelligence (AI) technologies and computational capabilities, generative AI (GAI) has become a pivotal digital content generation technique for offering superior digital services. However, due to the inherent instability of AI models, directing GAI towards the desired output remains a challenging task. Therefore, in this paper, we design a novel framework that utilizes wireless perception to guide GAI (WiPe-GAI) in delivering AI-generated content (AIGC) service, within resource-constrained mobile edge networks. Specifically, we first propose a new sequential multi-scale perception (SMSP) algorithm to predict user skeleton based on the channel state information (CSI) extracted from wireless signals. This prediction then guides GAI to provide users with AIGC, i.e., virtual character generation. To ensure the efficient operation of the proposed framework in resource constrained networks, we further design a pricing-based incentive mechanism and propose a diffusion model based approach to generate an optimal pricing strategy for the service provisioning. The strategy maximizes the user's utility while incentivizing the participation of the virtual service provider (VSP) in AIGC provision. The experimental results demonstrate the effectiveness of the designed framework in terms of skeleton prediction and optimal pricing strategy generation, outperforming other existing solutions.}
}


@article{DBLP:journals/tmc/SuWYHR24,
	author = {Hairong Su and
                  Shibo Wang and
                  Shusen Yang and
                  Tianchi Huang and
                  Xuebin Ren},
	title = {Reducing Traffic Wastage in Video Streaming via Bandwidth-Efficient
                  Bitrate Adaptation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10361--10377},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3373498},
	doi = {10.1109/TMC.2024.3373498},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SuWYHR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bitrate adaptation (also known as ABR) is a crucial technique to improve the quality of experience (QoE) for video streaming applications. However, existing ABR algorithms suffer from severe traffic wastage, which refers to the traffic cost of downloading the video segments that users do not finally consume, for example, due to early departure or video skipping. In this paper, we carefully formulate the dynamics of buffered data volume (BDV), a strongly correlated indicator of traffic wastage, which, to the best of our knowledge, is the first time to rigorously clarify the effect of downloading plans on potential wastage. To reduce wastage while keeping a high QoE, we present a bandwidth-efficient bitrate adaptation algorithm (named BE-ABR), achieving consistently low BDV without distinct QoE losses. Specifically, we design a precise, time-aware transmission delay prediction model over the Transformer architecture, and develop a fine-grained buffer control scheme. Through extensive experiments conducted on emulated and real network environments including WiFi, 4G, and 5G, we demonstrate that BE-ABR performs well in both QoE and bandwidth savings, enabling a 60.87% wastage reduction and a comparable, or even better, QoE, compared to the state-of-the-art methods.}
}


@article{DBLP:journals/tmc/BentalebLABZ24,
	author = {Abdelhak Bentaleb and
                  May Lim and
                  Mehmet N. Akcay and
                  Ali C. Begen and
                  Roger Zimmermann},
	title = {Bitrate Adaptation and Guidance With Meta Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10378--10392},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376560},
	doi = {10.1109/TMC.2024.3376560},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BentalebLABZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive bitrate (ABR) schemes enable streaming clients to adapt to time-varying network/device conditions for a stall-free viewing experience. Most ABR schemes use manually tuned heuristics or learning-based methods. Heuristics are easy to implement but do not always perform well, whereas learning-based methods generally perform well but are difficult to deploy on low-resource devices. To make the most out of both worlds, we earlier developed Ahaggar, a learning-based scheme executing on the server side that provides quality-aware bitrate guidance to streaming clients running their own heuristics. Ahaggar's novelty is the meta reinforcement learning approach taking network conditions, clients’ statuses and device resolutions, and streamed content as input features to perform bitrate guidance. Ahaggar uses the new Common Media Client/Server Data (CMCD/SD) protocols to exchange the necessary metadata between the servers and clients. While Ahaggar was a significant step forward, in this study, we focus on three open areas, namely, (i) exploring the performance of Ahaggar in a heterogeneous environment including both Ahaggar and non-Ahaggar clients with varied network conditions and device resolutions, and (ii) quantifying the impact of device resolutions on QoE with Ahaggar. We thoroughly investigate these areas and report our findings. We also (iii) discuss the Ahaggar design choices. Experiments on an open-source system show that Ahaggar adapts to unseen conditions fast and outperforms its competitors in several viewer experience metrics.}
}


@article{DBLP:journals/tmc/WangYNSWYSLZ24,
	author = {Xuanzhi Wang and
                  Anlan Yu and
                  Kai Niu and
                  Weiyan Shi and
                  Junzhe Wang and
                  Zhiyun Yao and
                  Rahul C. Shah and
                  Hong Lu and
                  Daqing Zhang},
	title = {Understanding the Diffraction Model in Static Multipath-Rich Environments
                  for WiFi Sensing System Design},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10393--10410},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3377708},
	doi = {10.1109/TMC.2024.3377708},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangYNSWYSLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although WiFi-based contactless sensing has made significant progress in the past decade, most prior work still focus on the reflection zone far from WiFi transceivers, while few studies explore the diffraction zone near transceivers. Additionally, previous diffraction models only consider the CSI amplitude signal and ignore the impact of multipath. In this work, we develop an accurate diffraction model to characterize the relationship between both CSI amplitude and phase and target's movement in the diffraction zone. We further put forward the deformation forms of the model under static multipath conditions and find that the CSI patterns vary significantly with multipath. Consequently, the common assumption of a one-to-one mapping between CSI patterns and activities in existing work fails due to multipaths, degrading sensing performance when multipath changes. To address this challenge, we propose to extract a relative change pattern from CSI signals to recover the one-to-one mapping relations and eliminate the impact of static multipath. Extensive experiments under various multipath conditions demonstrate an accuracy higher than 96% for the coarse-grained intrusion detection and an average error rate of 0.6 bpm for the fine-grained respiration monitoring.}
}


@article{DBLP:journals/tmc/HuangZMDP24,
	author = {Hualong Huang and
                  Wenhan Zhan and
                  Geyong Min and
                  Zhekai Duan and
                  Kai Peng},
	title = {Mobility-Aware Computation Offloading With Load Balancing in Smart
                  City Networks Using {MEC} Federation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10411--10428},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376377},
	doi = {10.1109/TMC.2024.3376377},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangZMDP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet-of-Things (IoT) has played a critical role in developing sustainable smart cities and emerging numerous latency-sensitive IoT applications. Mobile edge computing (MEC) federation has the capability to incorporate a transparent resource management approach, which enables the sharing and utilization of MEC services from edge infrastructure providers (EIPs) and provides agile access services to mobile devices (MDs). In this paper, we investigate the joint optimization problem of computation offloading, task migration, and resource allocation in the MEC federation. The objective is to minimize the weighted sum of latency and energy consumption while maintaining load balancing under the constraint of the long-term migration cost budget of EIPs. To address the problem, we decompose it into two sub-problems: 1) the MDs clustering sub-problem and 2) the sub-problem of joint computation offloading, task migration, and resource allocation. First, an MDs clustering matching (MDCM) algorithm is proposed to cluster the MDs in edge servers (ESs) according to the differences in channel gains. Afterward, the second sub-problem is simplified by the Lyapunov optimization technique, and then we propose a Transformer-based mobility prediction model and a decentralized deep deterministic policy gradient (DDPG)-based framework to solve it. Extensive simulation results demonstrate the cost-efficiency of the proposed algorithm.}
}


@article{DBLP:journals/tmc/ZhangY24,
	author = {Zhe Zhang and
                  Li Yan},
	title = {A Dispatching Strategy of Autonomous Robotic Charger Boats for Charging
                  Electric Vessels},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10429--10442},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3374789},
	doi = {10.1109/TMC.2024.3374789},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous robotic boats equipped with chargers or swappable batteries can serve as Mobile Energy Disseminators (MED) and proactively maintain the State-of-Charge of electric vessels. However, previous methods are either incapable of keeping the vessels continuously driving without recharge downtime, or not directly applicable for the scheduling of chargers on city-scale transportation networks. We propose RoboCharger: a Robotic Charger boat scheduling system that adaptively determines the number of serving MEDs, and the optimal routes of the MEDs according to vessel traffic. We studied a metropolitan-scale vessel mobility dataset provided by MarineTraffic and analyzed the spatial-temporal characteristics of vessel movements in Amsterdam's canals. Based on the analysis insights, we developed a MinHash and spatial-temporal similarity comparison based method for vessel traffic estimation, a Chinese Postman Problem based method for determining the cruising routes of the MEDs, and formulated and solved a multi-objective optimization problem to determine the number of serving MEDs, the driving route of each MED and maintain the SoC of the vessels above zero. Our trace-driven experiments demonstrate that compared with previous methods, RoboCharger increases the average SoC of vessels over all time slots throughout a day by almost 42%, and the number of charges of vessels by almost 53%.}
}


@article{DBLP:journals/tmc/LiZHMWL24,
	author = {Xishuo Li and
                  Shan Zhang and
                  Yuejiao Huang and
                  Xiao Ma and
                  Zhiyuan Wang and
                  Hongbin Luo},
	title = {Towards Timely Video Analytics Services at the Network Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10443--10459},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376769},
	doi = {10.1109/TMC.2024.3376769},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZHMWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time video analytics services aim to provide users with accurate recognition results timely. However, existing studies usually fall into the dilemma between reducing delay and improving accuracy. The edge computing scenario imposes strict transmission and computation resource constraints, making balancing these conflicting metrics under dynamic network conditions difficult. In this regard, we introduce the age of processed information (AoPI) concept, which quantifies the time elapsed since the generation of the latest accurately recognized frame. AoPI depicts the integrated impact of recognition accuracy, transmission, and computation efficiency. We derive closed-form expressions for AoPI under preemptive and non-preemptive computation scheduling policies w.r.t. the transmission/computation rate and recognition accuracy of video frames. We then investigate the joint problem of edge server selection, video configuration adaptation, and bandwidth/computation resource allocation to minimize the long-term average AoPI over all cameras. We propose an online method, i.e., Lyapunov-based block coordinate descent (LBCD), to solve the problem, which decouples the original problem into two subproblems to optimize the video configuration/resource allocation and edge server selection strategy separately. We prove that LBCD achieves asymptotically optimal performance. According to the testbed experiments and simulation results, LBCD reduces the average AoPI by up to 10.94X compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/TangLZCXSL24,
	author = {Jin Tang and
                  Jian Li and
                  Lan Zhang and
                  Xianhao Chen and
                  Kaiping Xue and
                  Qibin Sun and
                  Jun Lu},
	title = {Opportunistic Content-Aware Routing in Satellite-Terrestrial Integrated
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10460--10474},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3377729},
	doi = {10.1109/TMC.2024.3377729},
	timestamp = {Tue, 17 Dec 2024 16:57:28 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangLZCXSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising complement to terrestrial cellular networks, satellite networks have recently drawn increasing attention, offering seamless coverage cost-effectively. However, with the rapidly increasing users’ demand for multimedia content, how to achieve efficient content transmission seamlessly becomes a critical but knotty problem. To provide an efficient solution from the routing perspective, in this paper, we propose an opportunistic content-aware routing scheme. Our scheme combines the features of in-network caching and content awareness of information-centric networking (ICN) architecture. The basic idea of the proposed scheme is to sense users’ requests and find the optimal route solution with the largest potential gain. Moreover, considering the limitation of real-time signaling collection in satellite networks, we design a cached content prediction method. The method is capable of inferring the probability of content being cached based on historical popularity information, providing essential information for measuring potential gains. Extensive simulation results demonstrate that the proposed opportunistic content-aware routing scheme outperforms baseline approaches with significantly reduced delay and traffic consumption.}
}


@article{DBLP:journals/tmc/YiSWZWL24,
	author = {Liping Yi and
                  Xiaorong Shi and
                  Nan Wang and
                  Jinsong Zhang and
                  Gang Wang and
                  Xiaoguang Liu},
	title = {FedPE: Adaptive Model Pruning-Expanding for Federated Learning on
                  Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10475--10493},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3374706},
	doi = {10.1109/TMC.2024.3374706},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YiSWZWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, federated learning (FL) as a new learning paradigm allows multi-party to collaboratively train a shared global model with privacy protection. However, vanilla FL running on heterogeneous mobile edge devices still faces three crucial challenges: communication efficiency, statistical heterogeneity, and system heterogeneity. To tackle them simultaneously, we devise FedPE, a communication-efficient and personalized federated learning framework, which allows each client to search for personalized optimal local subnets adaptive to system capacity in each round of FL. It consists of three core components: a) adaptive pruning-expanding controls model pruning or expanding according to the accuracy variations of local models, b) error compensation strategy promotes the pruned or expanded subnets to be Lottery Ticket Networks (LTNs), c) the fair aggregation rule aggregates local models with their real-time contributions as coefficients to boost the performance of the aggregated global model. The integration of the three components facilitates that only personalized optimal subnets with different footprints interact between the server and clients, which effectively reduces communication costs and enhances the robustness of FL to statistical and system heterogeneity. We also prove the convergence of FedPE and design an optimal hyperparameter searching (OHS) algorithm based on Pareto optimization to search for optimal hyperparameters for FedPE. Extensive experiments evaluated on five real-world datasets with IID or Non-IID distributions demonstrate that FedPE configured with found optimal hyperparameters achieves 1.86\\times -121\\times\ncommunication efficiency improvement with almost no accuracy degradation, presenting the best trade-off between model accuracy and communication cost.}
}


@article{DBLP:journals/tmc/ZhangMMC24,
	author = {Ning Zhang and
                  Qian Ma and
                  Wuxing Mao and
                  Xu Chen},
	title = {Coalitional {FL:} Coalition Formation and Selection in Federated Learning
                  With Heterogeneous Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10494--10508},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3375325},
	doi = {10.1109/TMC.2024.3375325},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangMMC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The model accuracy achieved by federated learning (FL) depends significantly on devices’ data distributions. To improve the model accuracy of FL with heterogeneous data distributions on devices, existing works propose some device sampling methods for the central server, but face the problem that the selected devices may still have unbalanced data. In this article, we propose a novel coalitional FL framework for FL with heterogeneous data. Specifically, devices can cooperate and form device coalitions to reduce the data unbalancedness, and we formulate devices’ interactions as a coalition formation game. Then the server selects an optimal subset of device coalitions to improve the model accuracy. Analyzing the coalition formation and selection framework is challenging since the relationship between model accuracy and data heterogeneity is not clear, and devices’ coalition formation decisions and the server's coalition selection strategy are coupled in a highly non-trivial manner. We first derive a novel theoretical characterization of the relationship between model accuracy loss and data heterogeneity which follows an inverse function. With the novel theoretical relationship, we analyze devices’ coalition formation game. We characterize the conditions under which the Nash stable partition exists, and propose an accelerated algorithm for devices to reach the Nash stable partition. For the server's device coalition selection problem, we show that the model accuracy loss depends on both data heterogeneity and the number of data samples of device coalitions in a non-monotonous way, and we propose a low-complexity algorithm for the server to select device coalitions efficiently. We conduct extensive simulations and show that our proposed coalition formation and selection framework reduces the data heterogeneity of selected device coalitions by up to 58.6% and increases the model accuracy by up to 6.8% compared with four existing benchmarks.}
}


@article{DBLP:journals/tmc/LiLSMZ24,
	author = {Zijian Li and
                  Zehong Lin and
                  Jiawei Shao and
                  Yuyi Mao and
                  Jun Zhang},
	title = {FedCiR: Client-Invariant Representation Learning for Federated Non-IID
                  Features},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10509--10522},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376697},
	doi = {10.1109/TMC.2024.3376697},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLSMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client set and representations conditioned on labels to promote representations of clients to be client-invariant. We further incorporate two regularizers into the FL framework to bound the mutual information terms with an approximate global representation distribution to compensate for the absence of the ground-truth global representation distribution, thus achieving informative and client-invariant feature extraction. To achieve global representation distribution approximation, we propose a data-free mechanism performed by the server without compromising privacy. Extensive experiments demonstrate the effectiveness of our approach in achieving client-invariant representation learning and solving the data heterogeneity issue.}
}


@article{DBLP:journals/tmc/XuGLZJW24,
	author = {Changfu Xu and
                  Jianxiong Guo and
                  Yupeng Li and
                  Haodong Zou and
                  Weijia Jia and
                  Tian Wang},
	title = {Dynamic Parallel Multi-Server Selection and Allocation in Collaborative
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10523--10537},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3376550},
	doi = {10.1109/TMC.2024.3376550},
	timestamp = {Tue, 11 Feb 2025 16:38:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuGLZJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Mobile Edge Computing (MEC) has emerged as a promising approach to provide low service latency for computation-intensive Internet of Things applications, facilitated by the cooperation of edge-edge and edge-cloud resources. However, existing collaborative MEC methods typically restrict the collaborative processing between any two Edge Servers (ESs) or one ES and the cloud server for a task request, limiting the exploitation of available resources on other ESs. Moreover, these conventional methods rely on offline task partitioning, potentially leading to extended make-span, especially when ES computing capacities exhibit heterogeneity. In this paper, we propose an innovative method named SMCoEdge. This method performs dynamic parallel multi-ES selection and workload allocation in heterogeneous collaborative MEC environments, thus simultaneously enabling multiple ESs’ idle resources to accelerate task processing. We formulate our problem into an online linear programming problem, with the objective of minimizing task computing and transmission make-spans. To enhance computational efficiency, we decompose the problem into two stages: multi-ES selection and workload allocation. Then, we propose an online Deep Reinforcement Learning based Simultaneous Multi-ES Offloading (DRL-SMO) algorithm along with a top-$k$ deep Q-learning network model to effectively solve our problem, where an efficient algorithm is proposed to achieve the optimal solution for the workload allocation stage. Furthermore, we provide a theoretical performance analysis, demonstrating that the DRL-SMO algorithm achieves a near-optimal solution for our problem within an approximate linear time complexity. Finally, our extensive experimental results demonstrate the substantial advantages of our method. It consistently reduces the average make-span by 19.63% and keeps a lower offloading failure rate, when compared to state-of-the-art methods. These findings underline the efficacy of our method in enhancing collaborative MEC performance.}
}


@article{DBLP:journals/tmc/LiuLCW24,
	author = {Xiaochen Liu and
                  Fan Li and
                  Yetong Cao and
                  Yu Wang},
	title = {BrailleReader: Braille Character Recognition Using Wearable Motion
                  Sensor},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10538--10553},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379569},
	doi = {10.1109/TMC.2024.3379569},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing demand for improving communication and independence for visually impaired people, automatic Braille recognition has gained increasing attention in facilitating Braille learning and reading. However, current approaches mainly require high-cost hardware, involve inconvenient operation, and disturb the normal touch function. In this paper, we propose BrailleReader as a low-cost and effortless Braille character recognition system without disturbing normal Braille touching. It exploits the wrist motion of Braille reading captured by the motion sensor available in the ubiquitous wrist-worn device to infer the encoded character information. To address the noise caused by other body and hand movements, we propose a novel noise cancellation method using the wavelet packet decomposition and reconstruction technique to separate clean wrist movement induced by the Braille dot. Moreover, we further explore the unique wrist movement pattern in three aspects to extract a novel and effective feature set. Based on this, BrailleReader leverages a spiking neural network-based model to robustly recognize Braille characters across different people and different surface materials. Extensive experiments with 48 participants demonstrate that BrailleReader can perform accurate and robust recognition of 26 Braille characters.}
}


@article{DBLP:journals/tmc/YangZLHSC24,
	author = {Shuai Yang and
                  Dongheng Zhang and
                  Yadong Li and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {iSense: Enabling Radar Sensing Under Mutual Device Interference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10554--10569},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379570},
	doi = {10.1109/TMC.2024.3379570},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangZLHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) radar has been widely used in wireless sensing due to its non-contact nature, privacy preservation, and immunity to adverse lighting conditions. However, with more and more mmWave radars working in the same frequency band, mutual device interference among them is inevitable and has become a serious problem. The device interference reduces the signal-to-interference-plus-noise ratio (SINR) and significantly degrades the detection performance. Existing works mainly focus on the vital sign monitoring in different practical scenarios (e.g., device movement, human movement, multi-person interference, and in-car scenario), and the vital sign monitoring in the presence of mutual device interference is still not well resolved. In this paper, we propose a novel interference mitigation framework, iSense, to enable radar vital sign sensing under device interference. By exploiting one-way propagation characteristic of device interference, iSense can effectively detect and suppress the interference. We evaluate iSense under a variety of complex device interference scenarios, including different distances, angles, and numbers of aggressor radars, as well as the impact of different environments. Experimental results show that the accuracy of respiration and heartbeat estimation of iSense can reach over 99.2% and 98.6%, indicating that iSense takes an important step towards the practical development of radar sensing.}
}


@article{DBLP:journals/tmc/ShenZCCCZWG24,
	author = {Jiangang Shen and
                  Hongzi Zhu and
                  Yunxiang Cai and
                  Shan Chang and
                  Haibin Cai and
                  Bangzhao Zhai and
                  Xudong Wang and
                  Minyi Guo},
	title = {Taming Distributed One-Hop Multicasting in Millimeter-Wave VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10570--10583},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3381955},
	doi = {10.1109/TMC.2024.3381955},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShenZCCCZWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient one-hop multicasting (OHM) of high-volume sensor data plays a pivotal role in the success of cooperative autonomous driving applications. Although millimeter-Wave (mmWave) bands demonstrate huge potential for high-bandwidth OHM data transmission, the challenge lies in enabling individual vehicles to locate and communicate with suitable neighbors in a fully distributed and highly dynamic scenario. This paper introduces mmV2V, a fully distributed OHM scheme designed for vehicular networks, comprising three tightly integrated protocols. Initially, synchronized vehicles perform a probabilistic neighbor discovery procedure, wherein randomly divided transmitters (or receivers) clockwise scan (or listen to) the surroundings in synchronization with heterogeneous Tx (or Rx) beams. This approach facilitates the identification of the vast majority of neighbors within a few repeated rounds. Subsequently, vehicles engage in negotiations with their neighbors to establish an optimal communication schedule in evenly distributed slots. Finally, matched pairs of neighboring vehicles commence high data rate transmissions using refined beams. We implement a prototype testbed to validate the feasibility of the main components of mmV2V. Extensive simulations based on generated and real-world traffic traces are conducted and the results demonstrate that mmV2V consistently achieves a high completion ratio in demanding OHM tasks across various traffic conditions.}
}


@article{DBLP:journals/tmc/YuZCSLLG24,
	author = {Zhiwen Yu and
                  Lele Zhao and
                  Helei Cui and
                  Yongbo Song and
                  Yimeng Liu and
                  Yixuan Luo and
                  Bin Guo},
	title = {CrowdKit: {A} Generic Programming Framework for Mobile Crowdsensing
                  Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10584--10597},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3381578},
	doi = {10.1109/TMC.2024.3381578},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuZCSLLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Crowdsensing (MCS) has become a popular sensing paradigm, where a number of participants use their mobile devices to collectively share and extract information related to a certain common interest. In this trend, many typical applications, such as environmental monitoring, intelligent transportation, and public safety, are emerging in our daily lives, and the need to quickly develop various new applications is becoming more urgent. However, existing programming frameworks for MCS applications either target specific scenarios that lack extensibility or require considerable development effort and expertise, hindering innovation in this direction. In order to reduce the burden of developing new MCS applications, we devise a developer-oriented generic programming framework, namely CrowdKit. It abstracts the common and fundamental data models and functions of MCS applications and makes them reusable. Meanwhile, it follows the principles of modular design, visual development, and automatic code generation to further bring extensibility and drastically reduce the difficulty and time cost of developers. Moreover, its algorithm modules can accommodate various advanced MCS algorithms, thus narrowing the gap between theory and practice. We implement and release a full-fledged version of CrowdKit, and conduct comprehensive case study and user study to demonstrate its simplicity, generality, extensibility and high efficiency.}
}


@article{DBLP:journals/tmc/LiaoLFZC24,
	author = {Guocheng Liao and
                  Bing Luo and
                  Yutong Feng and
                  Meng Zhang and
                  Xu Chen},
	title = {Optimal Mechanism Design for Heterogeneous Client Sampling in Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10598--10609},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379659},
	doi = {10.1109/TMC.2024.3379659},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiaoLFZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) provides a collaborative paradigm for distributedly training a global model while protecting clients’ privacy. In addition to communication bottlenecks and non-i.i.d. data distributions, the FL framework introduces two fundamental economic challenges: first, clients are self-interested and strategic in practice, requiring specific incentives to participate in FL; second, each client can misreport its private information to its advantage. Although existing studies have proposed economic mechanisms, they are often restricted to a “binary” participation scenario, leading to communication overheads or biased models due to client heterogeneity. In this paper, we first analyze the convergence bound under arbitrary client sampling probability with a varying number of clients. Then, we consider an optimal mechanism design problem: the FL convergence bound minimization subject to budget constraint, incentive compatibility, and individual rationality. We derive the optimal sampling probability function in a close form. To overcome the unknown prior distribution challenge, we introduce a prior-independent mechanism design, and show how it gradually learns cost distributions by exploiting the incentive compatibility property. We perform extensive experiments and show that, while outperforming the uniform sampling scheme, two proposed schemes (prior-based and prior-independent ones) perform closely to the ideal complete information upper bound.}
}


@article{DBLP:journals/tmc/TianWWTLQ24,
	author = {Yichen Tian and
                  Yunliang Wang and
                  Yufan Wang and
                  Xinyu Tong and
                  Xiulong Liu and
                  Wenyu Qu},
	title = {Device-Free Human Tracking and Gait Recognition Based on the Smart
                  Speaker},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10610--10627},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379647},
	doi = {10.1109/TMC.2024.3379647},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TianWWTLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The smart speakers benefit from the ability to localize and identify users. Specifically, we can analyze the user's habits from the historical trajectory to provide better voice-based services. However, current voice localization method requires the user to actively issue voice commands, which makes smart speakers unable to track and identify silent users most of the time. This paper introduces WSTrack+, a system that combines Wi-Fi and Sound to track human movement and recognize gait patterns. In particular, current smart speakers naturally support both Wi-Fi and acoustic functions. As a result, we are able to construct the system using just one router and a smart speaker, which is a more promising approach compared to existing systems that rely on multiple routers for sensing. To track and identify the silent user, our insights are twofold: 1) the smart speakers can hear the sound of the user's footstep, and then extract which direction the user is in; 2) we can extract the reflected path change rate from the Wi-Fi signals, and the acoustic signal can help us convert the path change rate into the actual user's velocity. Our implementation and evaluation on commodity devices demonstrate that WSTrack+ can realize simultaneous tracking and gait recognition, where the median tracking error is \\text{0.34}\\;\\text{m} and the recognition accuracy is 88.6% for 12 users.}
}


@article{DBLP:journals/tmc/XiaoYYL24,
	author = {Yang Xiao and
                  Ying Yang and
                  Huihan Yu and
                  Jun Liu},
	title = {Scalable QoS-Aware Multipath Routing in Hybrid Knowledge-Defined Networking
                  With Multiagent Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10628--10646},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379191},
	doi = {10.1109/TMC.2024.3379191},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath routing remains a challenging issue in traffic engineering (TE) as existing solutions are incapable of handling the evolving network dynamics and stringent quality-of-service (QoS) requirements. To address it, multiagent deep reinforcement learning (MADRL) is a promising technique that provides more elaborate multipath routing strategies. However, prevalent MADRL-based solutions still suffer inapplicability as they fail to ensure both scalability and QoS awareness. In this paper, we leverage the emerging hybrid knowledge-defined networking (KDN) architecture, and propose a collaborative MADRL-based multipath routing algorithm. Two novel mechanisms, i.e., parallel agent replication and periodic policy synchronization, are devised for agent design to ensure the practicality of the proposed method. In addition, an efficient communication mechanism is established to facilitate multiagent collaboration by enabling scalable observation and reward exchange. Featuring a multiagent twin-actor-critic (MA-TAC) learning structure and a proximal policy optimization (PPO) -based training process, the proposed algorithm consists of alternately scheduled execution and training phases for practical deployment. We compare the performance of our proposed method with those of several benchmark methods. Extensive simulation results demonstrate that the proposed method achieves significantly better scalability, QoS awareness, and stability than the benchmark methods under various environment settings.}
}


@article{DBLP:journals/tmc/JiangGHL24,
	author = {Changkun Jiang and
                  Lin Gao and
                  Fen Hou and
                  Jianqiang Li},
	title = {Economic Analysis of Edge Caching Enabled Mobile Internet Ecosystem},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10647--10664},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3382013},
	doi = {10.1109/TMC.2024.3382013},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JiangGHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge caching is promising to improve content delivery and alleviate backbone burden by caching contents at the network edges. The commercial deployment relies on a comprehensive understanding of the economic interactions involved. This paper studies the edge caching enabled Internet ecosystem including a Content Provider (CP), a Global ISP (G-ISP) providing backbone services, a Local ISP (L-ISP) providing access services, and End-Users (EUs). The CP serves EUs via Internet servers or L-ISP's edge cache. We formulate their multi-tiered interactions as a three-stage dynamic game. In Stage I, CP determines edge cache storage to purchase from L-ISP and cache access fee to charge EUs. In Stage II, G-ISP and L-ISP determine backbone and access prices. In Stage III, EUs decide whether to choose edge cache services, considering cache hit probability, cache access fee, and backbone access prices. We analyze the subgame perfect equilibrium by elaborately designing five cases of EUs’ choices, five regions of ISPs’ pricing, and three patterns of CP's caching, under cooperative and competitive ISP pricing scenarios. Our analysis demonstrates that adopting edge caching leads to win-win outcomes for all parties involved. Furthermore, we find that competitive pricing is more advantageous for CP's profit when cache costs are low, while cooperative pricing is more beneficial when cache costs are high.}
}


@article{DBLP:journals/tmc/LiWHWLZLD24,
	author = {Kexin Li and
                  Xingwei Wang and
                  Qiang He and
                  Jielei Wang and
                  Jie Li and
                  Siyu Zhan and
                  Guoming Lu and
                  Schahram Dustdar},
	title = {Computation Offloading in Resource-Constrained Multi-Access Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10665--10677},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3383041},
	doi = {10.1109/TMC.2024.3383041},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiWHWLZLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, computation offloading methods have greatly improved the Quality of Experience (QoE) in Multi-access Edge Computing (MEC) by offloading tasks to the edge servers. Since well-coordinated actions of Terminal Devices (TDs) are critical to improving the performance of the entire individual system, many practical MEC-based applications, i.e., firefighting robots and unmanned aerial vehicles, require great teamwork among TDs. However, real-world scenarios are usually bound by resource conditions. For instance, network connectivity may weaken or experience interruptions during emergency situations. In cases where the communication medium is utilized by multiple TDs, achieving effective coordination poses a significant challenge. In this paper, we propose a computation offloading scheme based on Scheduled Multi-agent Deep Reinforcement Learning (SMDRL) to make the most efficient decision in a resource-constrained scenario. First, we design a virtual energy queue based on the MEC system and maximize the QoE (related to service delay and energy consumption) in a real-time manner. Subsequently, we propose a scheduled multi-agent deep reinforcement learning algorithm to support each TD in learning how to encode messages, select actions, and schedule itself based on the received messages. Furthermore, a TopK mechanism is introduced. This mechanism chooses the most crucial TDs to broadcast their messages, and then the computation offloading problem in a communication-constrained MEC environment can be solved in a low-communication manner. Also, we prove that even under limited communication conditions, our proposed methods can still lead to the close-to-optimal performance. The final performance analysis shows that the developed scheme has significant advantages over other representative schemes.}
}


@article{DBLP:journals/tmc/FengSP24,
	author = {Xin'ao Feng and
                  Yaohua Sun and
                  Mugen Peng},
	title = {Distributed Satellite-Terrestrial Cooperative Routing Strategy Based
                  on Minimum Hop-Count Analysis in Mega {LEO} Satellite Constellation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10678--10693},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3380891},
	doi = {10.1109/TMC.2024.3380891},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FengSP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mega low earth orbit (LEO) satellite constellation is promising in achieving global coverage with high capacity. However, forwarding packets in mega constellation faces long end-to-end delay caused by multi-hop routing and high-complexity routing table construction, which will detrimentally impair the network transmission efficiency. To overcome this issue, a distributed low-complexity satellite-terrestrial cooperative routing approach is proposed in this paper, and its core idea is that each node forwards packets to next-hop node under the constraints of minimum end-to-end hop-count and queuing delay. Particularly, to achieve an accurate and low-complexity minimum end-to-end hop-count estimation in satellite-terrestrial cooperative routing scenario, we first introduce a satellite real-time position based graph (RTPG) to simplify the description of three-dimensional constellation, and further abstract RTPG into a key node based graph (KNBG). Considering the frequent regeneration of KNBG due to satellite movement, a low complexity generation method of KNBG is studied as well. Finally, utilizing KNBG as input, we design the minimum end-to-end hop-count estimation method (KNBG-MHCE). Meanwhile, the computational complexity, routing path survival probability and practical implementation of our proposal are all deeply discussed. Extensive simulations are also conducted in systems with Ka and laser band inter-satellite links to verify the superiority of our proposal.}
}


@article{DBLP:journals/tmc/PandeyTRR24,
	author = {Chandrasen Pandey and
                  Vaibhav Tiwari and
                  Joel J. P. C. Rodrigues and
                  Diptendu Sinha Roy},
	title = {5GT-GAN-NET: Internet Traffic Data Forecasting With Supervised Loss
                  Based Synthetic Data Over 5G},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {11},
	pages = {10694--10705},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3364655},
	doi = {10.1109/TMC.2024.3364655},
	timestamp = {Wed, 06 Nov 2024 22:18:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PandeyTRR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an era of 5G smart cities, precise traffic prediction remains elusive due to limited real-world data. Our paper introduces a novel approach using Generative Adversarial Networks (GANs) to create synthetic traffic data that closely mimics real-world statistics. This artificial dataset enhances our new 5GT-GAN-NET-based prediction model. The result is a significant boost in prediction accuracy, with Mean Square Error (MSE) reduced to 0.000346 and Mean Absolute Error (MAE) to 0.00685. Compared to benchmarks, our model improves MSE and MAE by up to 95.45% with respect to the ARIMA model and 87.31% with respect to the NARNN model respectively. User privacy remains a cornerstone of our approach, crucial for smart city applications. Our predictive capabilities enable more efficient resource allocation by service providers, increasing communication infrastructure reliability. Although tailored for smart cities, the approach is adaptable to other fields facing data scarcity and privacy concerns. Our research highlights the potential of GANs in generating large, accurate datasets for traffic prediction in 5G environments while prioritizing user privacy.}
}


@article{DBLP:journals/tmc/TanC24,
	author = {Tianxiang Tan and
                  Guohong Cao},
	title = {Thermal-Aware Scheduling for Deep Learning on Mobile Devices With
                  {NPU}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10706--10719},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3379501},
	doi = {10.1109/TMC.2024.3379501},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TanC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Deep Neural Networks (DNNs) have been successfully applied to various fields, there is a tremendous demand for running DNNs on mobile devices. Although mobile GPU can be leveraged to improve performance, it consumes a large amount of energy. After a short period of time, the mobile device may become overheated and the processors are forced to reduce the clock speed, significantly reducing the processing speed. A different approach to support DNNs on mobile device is to leverage the Neural Processing Units (NPUs). Compared to GPU, NPU is much faster and more energy efficient, but with lower accuracy due to the use of low precision floating-point numbers. We propose to combine these two approaches to improve the performance of running DNNs on mobile devices by studying the thermal-aware scheduling problem, where the goal is to achieve a better tradeoff between processing time and accuracy while ensuring that the mobile device is not overheated. To solve the problem, we propose a heuristic-based scheduling algorithm to determine when to run DNNs on GPU and when to run DNNs on NPU based on the current states of the mobile device. The heuristic-based algorithm makes scheduling decisions greedily and ignores their future impacts. Thus, we propose a deep reinforcement learning based scheduling algorithm to further improve performance. Extensive evaluation results show that the proposed algorithms can significantly improve the performance of running DNNs on mobile devices while avoiding overheating.}
}


@article{DBLP:journals/tmc/XiaRLOLCFL24,
	author = {Zhiliang Xia and
                  Yanzhi Ren and
                  Siyi Li and
                  Jiachen Ou and
                  Hongbo Liu and
                  Yingying Chen and
                  Shu Fu and
                  Hongwei Li},
	title = {Indoor Location Identification for Smart Speakers Leveraging 3-D Acoustic
                  Images},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10720--10733},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3380162},
	doi = {10.1109/TMC.2024.3380162},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaRLOLCFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The indoor location awareness has drawn increasing attention for smart speakers as they become essential to provide function-location services. Existing indoor localization solutions either require add-on equipment or could only achieve room-level accuracy, which could not provide a function-location service for smart speakers. In this work, we propose a location identification system utilizing 3-D acoustic images, which are derived from the smart speaker by emitting a beep signal and sensing echoes created by objects in the surrounding environment with its microphone array, as the proof to identify some pre-defined indoor locations. Given the recorded acoustic samplings captured by the microphone array, our image construction component constructs a virtual imaging hemisphere and steers the array towards each grid of the hemisphere to generate a 3-D acoustic image of the surrounding environment. Moreover, we design a transfer-learning based model to derive effective features from the constructed images, and propose a data augmentation scheme for generating synthesized training images. To achieve a more accurate location identification, we further design a distance estimation scheme to identify the distances between the smart speaker and some major surrounding objects by utilizing the constructed 3-D acoustic image, and then adopt such distance information for location identification. Our experimental results show that our proposed system is accurate and robust for location identification under various real world scenarios.}
}


@article{DBLP:journals/tmc/XiaCPYYQ24,
	author = {Songpengcheng Xia and
                  Lei Chu and
                  Ling Pei and
                  Jiarui Yang and
                  Wenxian Yu and
                  Robert C. Qiu},
	title = {Timestamp-Supervised Wearable-Based Activity Segmentation and Recognition
                  With Contrastive Learning and Order-Preserving Optimal Transport},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10734--10751},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3381171},
	doi = {10.1109/TMC.2024.3381171},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaCPYYQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition (HAR) with wearables is one of the serviceable technologies in ubiquitous and mobile computing applications. The sliding-window scheme is widely adopted while suffering from the multi-class windows problem. As a result, there is a growing focus on joint segmentation and recognition with deep-learning methods, aiming at simultaneously dealing with HAR and time-series segmentation issues. However, obtaining the full activity annotations of wearable data sequences is resource-intensive or time-consuming, while unsupervised methods yield poor performance. To address these challenges, we propose a novel method for joint activity segmentation and recognition with timestamp supervision, in which only a single annotated sample is needed in each activity segment. However, the limited information of sparse annotations exacerbates the gap between recognition and segmentation tasks, leading to sub-optimal model performance. Therefore, the prototypes are estimated by class-activation maps to form a sample-to-prototype contrast module for well-structured embeddings. Moreover, with the optimal transport theory, our approach generates the sample-level pseudo-labels that take advantage of unlabeled data between timestamp annotations for further performance improvement. Comprehensive experiments on four public HAR datasets demonstrate that our model trained with timestamp supervision is superior to the state-of-the-art weakly-supervised methods and achieves comparable performance to the fully-supervised approaches.}
}


@article{DBLP:journals/tmc/ZhangK24,
	author = {Zhengguang Zhang and
                  Marwan Krunz},
	title = {Preamble Forgery and Injection in Wi-Fi Networks: Attacks and Defenses},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10752--10769},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3383065},
	doi = {10.1109/TMC.2024.3383065},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Wi-Fi networks, the preamble plays a crucial role in frame detection, synchronization, and channel estimation. It also ensures compatibility and interoperability across devices that operate different versions of Wi-Fi (e.g., IEEE 802.11a/g/n/ac/ax/be). Despite its significance, the preamble lacks authenticity and confidentiality guarantees, relying solely on weak integrity protection. In this paper, we introduce novel Preamble Injection and Spoofing (PrInS) attacks that exploit these vulnerabilities. Specifically, we show how an adversary can inject forged preambles without payloads to disrupt legitimate receptions or force legitimate users to defer transmissions. We demonstrate the impact of PrInS attacks both via experiments using software-defined radios (SDRs) and via system-level simulations. Our results show that the adversary can almost silence the channel, degrading the throughput of a legitimate user down to 2% of its normal throughput. Even at 30\\,\ndB less power than the legitimate signal, the adversary still causes 87% reduction in throughput. Even when the attacker targets only a fraction of legitimate frames, the average packet latency and packet loss rate significantly increase. As a countermeasure, we propose preamble customization and randomization using group keys and timestamps, along with preamble authentication in the receive state machine. Our countermeasure detects forged preambles with nearly 100% accuracy while maintaining low false alarm rates in most scenarios. Most importantly, it remains backward-compatible with existing 802.11 standards and does not impact the synchronization and frame error rates of the Wi-Fi system.}
}


@article{DBLP:journals/tmc/FurutanpeyRD24,
	author = {Alireza Furutanpey and
                  Philipp Raith and
                  Schahram Dustdar},
	title = {FrankenSplit: Efficient Neural Feature Compression With Shallow Variational
                  Bottleneck Injection for Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10770--10786},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3381952},
	doi = {10.1109/TMC.2024.3381952},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FurutanpeyRD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. Split Computing (SC) alleviates resource inefficiency by partitioning DNN layers across devices, but current methods are overly specific and only marginally reduce bandwidth consumption. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.}
}


@article{DBLP:journals/tmc/ZhuCWHLY24,
	author = {Yujiao Zhu and
                  Mingzhe Chen and
                  Sihua Wang and
                  Ye Hu and
                  Yuchen Liu and
                  Changchuan Yin},
	title = {Collaborative Reinforcement Learning Based Unmanned Aerial Vehicle
                  {(UAV)} Trajectory Design for 3D {UAV} Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10787--10802},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3382913},
	doi = {10.1109/TMC.2024.3382913},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuCWHLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the problem of using one active unmanned aerial vehicle (UAV) and four passive UAVs to localize a 3D target UAV in real time is investigated. In the considered model, each passive UAV receives reflection signals from the target UAV, which are initially transmitted by the active UAV. The received reflection signals allow each passive UAV to estimate the signal transmission distance which will be transmitted to a base station (BS) for the estimation of the position of the target UAV. Due to the movement of the target UAV, each active/passive UAV must optimize its trajectory to continuously localize the target UAV. Meanwhile, since the accuracy of the distance estimation depends on the signal-to-noise ratio of the transmission signals, the active UAV must optimize its transmit power. This problem is formulated as an optimization problem whose goal is to jointly optimize the transmit power of the active UAV and trajectories of both active and passive UAVs so as to maximize the target UAV positioning accuracy. To solve this problem, a Z function decomposition based reinforcement learning (ZD-RL) method is proposed. Compared to value function decomposition based RL (VD-RL), the proposed method can find the probability distribution of the sum of future rewards to accurately estimate the expected value of the sum of future rewards thus finding better transmit power of the active UAV and trajectories for both active and passive UAVs and improving target UAV positioning accuracy. Simulation results show that the proposed ZD-RL method can reduce the positioning errors by up to 39.4% and 64.6%, compared to VD-RL and independent deep RL methods, respectively.}
}


@article{DBLP:journals/tmc/TongCFZH24,
	author = {Jingwen Tong and
                  Zhenzhen Chen and
                  Liqun Fu and
                  Jun Zhang and
                  Zhu Han},
	title = {From Learning to Analytics: Improving Model Efficacy With Goal-Directed
                  Client Selection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10803--10817},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3383038},
	doi = {10.1109/TMC.2024.3383038},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TongCFZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an appealing paradigm for learning a global model among distributed clients while preserving data privacy. Driven by the demand for high-quality user experiences, evaluating the well-trained global model after the FL process is crucial. In this paper, we propose a closed-loop model analytics framework that allows for effective evaluation of the trained global model using clients’ local dataset. To address the challenges posed by system and data heterogeneities in the FL process, we study a goal-directed client selection problem based on the model analytics framework by selecting a subset of clients for the model training. This problem is formulated as a stochastic multi-armed bandit (SMAB) problem. We first put forth a quick initial upper confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under the federated analytics (FA) framework. Then, we further propose a belief propagation-based UCB (BP-UCB) algorithm under the democratized analytics (DA) framework. Moreover, we derive two regret upper bounds for the proposed algorithms, which increase logarithmically over the time horizon. The numerical results demonstrate that the proposed algorithms achieve nearly optimal performance, with a gap of less than 1.44% and 3.12% under the FA and DA frameworks, respectively.}
}


@article{DBLP:journals/tmc/QaisarYBLHZA24,
	author = {Muhammad Umar Farooq Qaisar and
                  Weijie Yuan and
                  Paolo Bellavista and
                  Fan Liu and
                  Guangjie Han and
                  Rabiu Sale Zakariyya and
                  Adeel Ahmed},
	title = {Poised: Probabilistic On-Demand Charging Scheduling for ISAC-Assisted
                  WRSNs With Multiple Mobile Charging Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10818--10834},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3382668},
	doi = {10.1109/TMC.2024.3382668},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QaisarYBLHZA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) and wireless sensor networks (WSNs) face an energy shortage challenge that could be overcome by the novel wireless power transfer (WPT) technology. The combination of WSNs and WPT is known as wireless rechargeable sensor networks (WRSNs), with the charging efficiency and charging scheduling being the primary concerns. Therefore, this paper proposes a probabilistic on-demand charging scheduling for integrated sensing and communication (ISAC)-assisted WRSNs with multiple mobile charging vehicles (MCVs) that addresses three parts. First, it considers the four attributes with their probability distributions to balance the charging load on each MCV. The attributes are residual energy of charging node, distance from MCV to charging node, degree of charging node, and charging node betweenness centrality. Second, it considers the efficient charging factor strategy to partially charge network nodes. Finally, it employs the ISAC concept to efficiently utilize the wireless resources to reduce the traveling cost of each MCV and to avoid the charging conflicts between them. The simulation results show that the proposed protocol outperforms cutting-edge protocols in terms of energy usage efficiency, charging delay, charging coverage, survival rate, travel distance, queue length, and service time.}
}


@article{DBLP:journals/tmc/YangDZ24,
	author = {Yang Yang and
                  Shuping Dang and
                  Zhenrong Zhang},
	title = {An Adaptive Compression and Communication Framework for Wireless Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10835--10854},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3382776},
	doi = {10.1109/TMC.2024.3382776},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangDZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed privacy-preserving paradigm of machine learning that enables efficient and secure model training through the collaboration of multiple clients. However, imperfect channel estimation and resource constraints of edge devices severely hinder the convergence of typical wireless FL, while the trade-off between communications and computation still lacks in-depth exploration. These factors lead to inefficient communications and hinder the full potential of FL from being unleashed. In this regard, we formulate a joint optimization problem of communications and learning in wireless networks subject to dynamic channel variations. For addressing the formulated problem, we propose an integrated adaptive n\n-ary compression and resource management framework (ANC) that is capable of adjusting the selection of edge devices and compression schemes, and allocates the optimal resource blocks and transmit power to each participating device, which effectively improves the energy efficiency and scalability of FL in resource-constrained environments. Furthermore, an upper bound on the expected global convergence rate is derived in this paper to quantify the impacts of transmitted data volume and wireless propagation on the convergence of FL. Simulation results demonstrate that the proposed adaptive framework achieves much faster convergence while maintaining considerably low communication overhead.}
}


@article{DBLP:journals/tmc/ZhaoN24,
	author = {Ming Zhao and
                  Mohammad Reza Nakhai},
	title = {A Unified Federated Deep {Q} Learning Caching Scheme for Scalable
                  Collaborative Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10855--10866},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3382824},
	doi = {10.1109/TMC.2024.3382824},
	timestamp = {Sat, 30 Nov 2024 21:08:14 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge caching-enabled networks can efficiently alleviate data traffic and improve quality of service. However, effectively adapting to users’ heterogeneous requests and coordinating among multiple edge servers remains a challenge. In this paper, we address the collaborative cache update and request delivery problem in an edge caching system, aiming to minimize the long-term average system cost under uncertainties of users’ heterogeneous demands and dynamic content popularity. To overcome the curse of dimensionality, we decompose the formulated problem into two subproblems: the coordinated proactive cache updating and local request processing. Next, we propose a unified federated deep Q learning (DQL) caching scheme to tackle and coordinate these two subproblems. Particularly, our scheme features a scalable DQL approach with a two-phase action selection procedure to learn the heterogeneous user requests across distributed servers in an online manner. Furthermore, we develop a federated learning (FL)-empowered training process to improve coordination among multiple servers, in which a Thompson sampling (TS)-based algorithm is introduced for smart server selection. We evaluate the performance of our proposed caching scheme in both small-scale and large-scale scenarios through comprehensive experiments, which highlights the advantages of the proposed scheme in terms of caching performance, scalability and robustness.}
}


@article{DBLP:journals/tmc/SongDXLYX24,
	author = {Fuhong Song and
                  Mingsen Deng and
                  Huanlai Xing and
                  Yanping Liu and
                  Fei Ye and
                  Zhiwen Xiao},
	title = {Energy-Efficient Trajectory Optimization With Wireless Charging in
                  UAV-Assisted {MEC} Based on Multi-Objective Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10867--10884},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3384405},
	doi = {10.1109/TMC.2024.3384405},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SongDXLYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the problem of energy-efficient trajectory optimization with wireless charging (ETWC) in an unmanned aerial vehicle (UAV)-assisted mobile edge computing system. A UAV is dispatched to collect computation tasks from specific ground smart devices (GSDs) within its coverage while transmitting energy to the other GSDs. In addition, a high-altitude platform with a laser beam is deployed in the stratosphere to charge the UAV, so as to maintain its flight mission. The ETWC problem is characterized by multi-objective optimization, aiming to maximize both the energy efficiency of the UAV and the number of tasks collected via optimizing the UAV's flight trajectories. The conflict between the two objectives in the problem makes it quite challenging. Recently, some single-objective reinforcement learning (SORL) algorithms have been introduced to address the aforementioned problem. Nevertheless, these SORLs adopt linear scalarization to define the user utility, thus ignoring the conflict between objectives. Furthermore, in dynamic MEC scenarios, the relative importance assigned to each objective may vary over time, posing significant challenges for conventional SORLs. To solve the challenge, we first build a multi-objective Markov decision process that has a vectorial reward mechanism. There is a corresponding relationship between each component of the reward and one of the two objectives. Then, we propose a new trace-based experience replay scheme to modify sample efficiency and reduce replay buffer bias, resulting in a modified multi-objective reinforcement learning algorithm. The experiment results validate that the proposed algorithm can obtain better adaptability to dynamic preferences and a more favorable balance between objectives compared with several algorithms.}
}


@article{DBLP:journals/tmc/LiuPL24,
	author = {Mingyu Liu and
                  Li Pan and
                  Shijun Liu},
	title = {Collaborative Storage for Tiered Cloud and Edge: {A} Perspective of
                  Optimizing Cost and Latency},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10885--10902},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3384264},
	doi = {10.1109/TMC.2024.3384264},
	timestamp = {Mon, 03 Mar 2025 18:09:08 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuPL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge storage is emerging as a novel storage paradigm, which offers the advantage of low latency and low cost, but has the disadvantage of limiting the scope of services to a certain area. In contrast, cloud storage offers anywhere services but has disadvantages in terms of latency and cost. In this paper, a collaborative storage scheme is proposed to leverage their complementary advantages. Considering the impact of collaboration on the cloud and the edge, we propose collaborative optimization models for cost and latency. To address the challenge of requiring future information when performing optimization for cost and latency, we first transform the long-term optimization into individual optimizations in each time slot using the Lyapunov optimization technique, based on which our algorithm decides whether a replica should be created at the edge and which cloud tier a replica should be migrated to. Then, we prove that the proposed algorithm can obtain near-optimal costs and guaranteed latencies. Finally, we conduct extensive simulations driven by real-world traces, and show that our algorithm can achieve a trade-off between cost and latency and outperform other benchmark algorithms.}
}


@article{DBLP:journals/tmc/TangMLJWZ24,
	author = {Zhiqing Tang and
                  Fangyi Mou and
                  Jiong Lou and
                  Weijia Jia and
                  Yuan Wu and
                  Wei Zhao},
	title = {Joint Resource Overbooking and Container Scheduling in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10903--10917},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3386936},
	doi = {10.1109/TMC.2024.3386936},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangMLJWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containers have gained popularity in Edge Computing (EC) networks due to their lightweight and flexible deployment advantage. In resource-constrained EC environments, overbooking container resources can substantially improve resource utilization. However, existing work overlooks the complex interplay between resource provisioning and container scheduling, which may result in performance degradation or inefficient resource utilization due to highly dynamic resource heterogeneity in EC. To address this issue, this paper presents a novel joint Resource Overbooking and Container Scheduling (ROCS) algorithm. Our approach accounts for resource heterogeneity and the geographical distribution of edge nodes, and we formulate the ROCS problem to consolidate various costs and revenues into a single profit metric for service providers. To enhance resource utilization and maximize the profit of the service providers, we develop an efficient algorithm that operates within a hybrid action space scheme by leveraging soft actor-critic reinforcement learning. Furthermore, we introduce a risk assessment mechanism to mitigate overbooking risks. Large-scale simulations with real-world data traces demonstrate the efficacy of our proposed ROCS algorithm, validating its advantage of improving resource utilization within EC networks.}
}


@article{DBLP:journals/tmc/GuiC24,
	author = {Jinsong Gui and
                  Fujian Cai},
	title = {Coverage Probability and Throughput Optimization in Integrated mmWave
                  and Sub-6 GHz Multi-UAV-Assisted Disaster Relief Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10918--10937},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3386550},
	doi = {10.1109/TMC.2024.3386550},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuiC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the disaster-hit areas where ground network infrastructure has been severely damaged, one challenging problem for multi-UAV-assisted disaster relief networks is how to improve the coverage probability of each UAV. On the basis of solving this problem, the second challenging problem is how to design a channel and power-beam allocation scheme to optimize system throughput while meeting spectrum-energy efficiency constraint. In this article, we first propose a new method for measuring single UAV coverage quality, which considers both the ratio of effective coverage time to single loop flight time and that of the ground terminals with effective coverage time to the total ground terminals. Then, we develop a set of new algorithms to take advantage of the uneven distribution of ground terminals, which can achieve the total coverage probability improvement and the reduction of deployment costs of UAVs. Finally, we formulate the second problem as Markov decision process (MDP) and develop a solution based on deep deterministic policy gradient (DDPG). Simulation results demonstrate the validity and superiority of our proposed solutions compared with other benchmark strategies in different perspectives.}
}


@article{DBLP:journals/tmc/ChenYYSHSC24,
	author = {Yuhao Chen and
                  Yuxuan Yan and
                  Qianqian Yang and
                  Yuanchao Shu and
                  Shibo He and
                  Zhiguo Shi and
                  Ji{-}Ming Chen},
	title = {AccEPT: An Acceleration Scheme for Speeding up Edge Pipeline-Parallel
                  Training},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10938--10951},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389779},
	doi = {10.1109/TMC.2024.3389779},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenYYSHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is usually infeasible to fit and train an entire large deep neural network (DNN) model using a single edge device due to the limited resources. To facilitate intelligent applications across edge devices, researchers have proposed partitioning a large model into several sub-models, and deploying each of them to a different edge device to collaboratively train a DNN model. However, the communication overhead caused by the large amount of data transmitted from one device to another during training, as well as the sub-optimal partition point due to the inaccurate latency prediction of computation at each edge device can significantly slow down training. In this paper, we propose AccEPT, an acceleration scheme for accelerating the edge collaborative pipeline-parallel training. In particular, we propose a light-weight adaptive latency predictor to accurately estimate the computation latency of each layer at different devices, which also adapts to unseen devices through continuous learning. Therefore, the proposed latency predictor leads to better model partitioning which balances the computation loads across participating devices. Moreover, we propose a bit-level computation-efficient data compression scheme to compress the data to be transmitted between devices during training. Our numerical results demonstrate that our proposed acceleration approach is able to significantly speed up edge pipeline parallel training up to 3 times faster in the considered experimental settings.}
}


@article{DBLP:journals/tmc/ShaoFCZMZ24,
	author = {Wei Shao and
                  Zejun Fan and
                  Chia{-}Ju Chen and
                  Zhaofeng Zhang and
                  Jiaqi Ma and
                  Junshan Zhang},
	title = {Impact of Sensing Errors on Headway Design: From {\textdollar}{\textbackslash}alpha{\textdollar}{\(\alpha\)}-Fair
                  Group Safety to Traffic Throughput},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10952--10965},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389987},
	doi = {10.1109/TMC.2024.3389987},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShaoFCZMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Headway, namely the distance between vehicles, is a key design factor for ensuring the safe operation of autonomous driving systems. There have been studies on headway optimization based on the speeds of leading and trailing vehicles, assuming perfect sensing capabilities. In practical scenarios, however, sensing errors are inevitable, calling for a more robust headway design to mitigate the risk of collision. Undoubtedly, augmenting the safety distance would reduce traffic throughput, thus highlighting the need for headway design to incorporate both sensing errors and risk tolerance models. In addition, prioritizing group safety over individual safety is often deemed unacceptable because no driver should sacrifice their safety for the safety of others. In this study, we propose a multi-objective optimization framework that examines the impact of sensing errors on both traffic throughput and the fairness of safety among vehicles. The proposed framework provides a solution to determine the Pareto frontier for traffic throughput and vehicle safety. ComDrive, a communication-based autonomous driving simulation platform, is developed to validate the proposed approach. Extensive experiments demonstrate that the proposed approach outperforms existing baselines.}
}


@article{DBLP:journals/tmc/LiuDNKXJS24,
	author = {Yinqiu Liu and
                  Hongyang Du and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Abbas Jamalipour and
                  Xuemin Shen},
	title = {ProSecutor: Protecting Mobile {AIGC} Services on Two-Layer Blockchain
                  via Reputation and Contract Theoretic Approaches},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10966--10983},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3390208},
	doi = {10.1109/TMC.2024.3390208},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuDNKXJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile AI-Generated Content (AIGC) has achieved great attention in unleashing the power of generative AI and scaling the AIGC services. By employing numerous Mobile AIGC Service Providers (MASPs), ubiquitous and low-latency AIGC services for clients can be realized. Nonetheless, the interactions between clients and MASPs in public mobile networks, pertaining to three key mechanisms, namely MASP selection, payment scheme, and fee-ownership transfer, are unprotected. In this paper, we design the above mechanisms in a systematic approach and present the first blockchain to protect mobile AIGC, called {\\sf ProSecutor}. Specifically, by roll-up and layer-2 channels, {\\sf ProSecutor} forms a two-layer architecture, realizing tamper-proof data recording and atomic fee-ownership transfer with high resource efficiency. Then, we present the Objective-Subjective Service Assessment (\\text{OS}^{2}\\mathrm{A}) framework, which effectively evaluates the AIGC services by fusing the objective service quality with the reputation-based subjective experience of the service outcome (i.e., AIGC outputs). Deploying \\text{OS}^{2}\\mathrm{A} on {\\sf ProSecutor}, firstly, the MASP selection can be realized by sorting the reputation. Afterward, the contract theory is adopted to optimize the payment scheme and help clients avoid moral hazards in mobile networks. We implement the prototype of {\\sf ProSecutor} on BlockEmulator. Extensive experiments demonstrate that {\\sf ProSecutor} achieves 12.5× throughput and saves 67.5% storage resources compared with BlockEmulator. Moreover, the effectiveness and efficiency of the proposed mechanisms are validated.}
}


@article{DBLP:journals/tmc/HuangTTXZY24,
	author = {Tao Huang and
                  Sha Tan and
                  Qinqin Tang and
                  Renchao Xie and
                  Chen Zhang and
                  F. Richard Yu},
	title = {Coordinating Services and Networks With NaaS Tickets Towards Service
                  Customization in Distributed Clouds},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {10984--10999},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3386811},
	doi = {10.1109/TMC.2024.3386811},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangTTXZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed clouds decentralize cloud resources, moving from a single high-level point in the network to multiple low-level points, allowing for the dynamic distribution of services across the “cloud-edge-end”. Nevertheless, the “best-effort” traditional networks suffer from unpredictable service quality and limited collaboration between services and networks. To address these shortcomingxvvs, we present a novel solution named “Network-as-a-Service (NaaS) Tickets,” inspired by traffic tickets in transportation systems to empower distributed clouds with customized service capabilities. Specifically, we first propose NaaS Tickets-enabled service-customized distributed clouds (NT-SCDC) to realize on-demand and service-oriented interconnection in a wide area. To establish a solid connection between services and networks, we introduce an auction-driven matching mechanism for NaaS Tickets. Then, the matching problem is formulated via an online framework MatOnline, which translates the long-term market problem into a series of one-shot auctions for NaaS Tickets. Based on the Vickrey-Clarke-Groves (VCG) mechanism, we develop MatVCG algorithm to handle one-shot matching problems, guaranteeing truthfulness, individual rationality, and social welfare. Moreover, we improve the performance of MatOnline to find the minimum feasible scale-down ratio with reduced budget expenditure. Experimental results demonstrate our algorithm achieves a stable competitive ratio on social welfare, effectively meeting customized demands.}
}


@article{DBLP:journals/tmc/QinYTYZK24,
	author = {Yeguang Qin and
                  Yilin Yang and
                  Fengxiao Tang and
                  Xin Yao and
                  Ming Zhao and
                  Nei Kato},
	title = {Differentiated Federated Reinforcement Learning Based Traffic Offloading
                  on Space-Air-Ground Integrated Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11000--11013},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389011},
	doi = {10.1109/TMC.2024.3389011},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QinYTYZK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Space-Air-Ground Integrated Network (SAGIN) plays a pivotal role as a comprehensive foundational network communication infrastructure, presenting opportunities for highly efficient global data transmission. Nonetheless, given SAGIN's unique characteristics as a dynamically heterogeneous network, conventional network optimization methodologies encounter challenges in satisfying the stringent requirements for network latency and stability inherent to data transmission within this network environment. Therefore, this paper proposes the use of differentiated federated reinforcement learning (DFRL) to solve the traffic offloading problem in SAGIN, i.e., using multiple agents to generate differentiated traffic offloading policies. Considering the differentiated characteristics of each region of SAGIN, DFRL models the traffic offloading policy optimization process as the process of solving the Decentralized Partially Observable Markov Decision Process (DEC-POMDP) problem. The paper proposes a novel Differentiated Federated Soft Actor-Critic (DFSAC) algorithm to solve the problem. The DFSAC algorithm takes the network packet delay as the joint reward value and introduces the global trend model as the joint target action-value function of each agent to guide the update of each agent's policy. The simulation results demonstrate that the traffic offloading policy based on the DFSAC algorithm achieves better performance in terms of network throughput, packet loss rate, and packet delay compared to the traditional federated reinforcement learning approach and other baseline approaches.}
}


@article{DBLP:journals/tmc/XiaoZTWLW24,
	author = {Yunpeng Xiao and
                  Qunqing Zhang and
                  Fei Tang and
                  Rong Wang and
                  Qian Li and
                  Guoyin Wang},
	title = {Cycle-Fed: {A} Double-Confidence Unlabeled Data Augmentation Method
                  Based on Semisupervised Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11014--11028},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3388731},
	doi = {10.1109/TMC.2024.3388731},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoZTWLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) generates a substantial volume of unlabeled personal privacy data in finance and healthcare, distributed across diverse locations and networks, which is currently underutilized. Semisupervised federated learning emerges as a promising solution by conducting model training on local devices without transmitting raw data to a central server. This approach enhances the security and efficiency of IoT systems. First, to overcome traditional data augmentation limitations in simulating raw data distribution, we introduce a data augmentation module using a uniformly distributed dropout (Uout) layer. This module enhances data diversity by mitigating sensitivity to variance shifts. Furthermore, considering the insufficiency of pseudo-label availability under the condition of low-density separation, we propose the Cycle-Fed model with dual-reliability. This model enhances its performance through the effects of data augmentation by incorporating pseudo-labeled positive samples subjected to secondary validation by discriminators provided by the data augmentation module. Finally, we propose a client-side optimal value avoidance strategy based on an adaptive local proximal term, which is denoted as \\mu _{t}\n. Experimental results on a public dataset indicate that the Cycle-Fed model surpasses the baseline with a 4.52%–8.76% reduction in loss, 1.62%–6.09% accuracy improvement, and a 1.285%–1.396% increase in area under the curve.}
}


@article{DBLP:journals/tmc/ZhaoBLYZS24,
	author = {Zhihui Zhao and
                  Haoyu Bin and
                  Hong Li and
                  Nan Yu and
                  Hongsong Zhu and
                  Limin Sun},
	title = {FeaShare: Feature Sharing for Computation Correctness in Edge Preprocessing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11029--11044},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3391294},
	doi = {10.1109/TMC.2024.3391294},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoBLYZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge preprocessing is a critical service type in edge computing. However, untrusted edges may be malicious to provide incorrect computational results (i.e., edge tampering). Although some studies have considered the correctness of results, they have limitations when applied to edge preprocessing. We present FeaShare, a feature-sharing approach, to verify edge results. The process is integrated into normal service operations. Meanwhile, to overcome feature-based limitations, terminals obtain partial edge results for a set of data by executing a small number of computations. These partial results are leveraged to construct shared features, facilitating the detection of edge tampering even when the tampered portion is not directly related to the features. Subsequently, the shared features are mapped to pseudo-data and added to the terminal’s data sequence, preventing features from influencing the results of terminal data. To resist edge attacks, both feature construction and placement are time-dependent and dynamic. FeaShare is not confined to specific edge tasks. We evaluate FeaShare using 3 typical scenes encompassing 5 applications. For instance, the evaluation utilizing the VGG model and CIFAR-10 dataset demonstrates a detection rate of 97%. Terminals perform approximately 10% of the edge’s computation operations, and its overhead growth rate is less than 10%.}
}


@article{DBLP:journals/tmc/LiLDYSM24,
	author = {Yunxin Li and
                  Fan Liu and
                  Zhen Du and
                  Weijie Yuan and
                  Qingjiang Shi and
                  Christos Masouros},
	title = {Frame Structure and Protocol Design for Sensing-Assisted {NR-V2X}
                  Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11045--11060},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389697},
	doi = {10.1109/TMC.2024.3389697},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLDYSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of the fifth-generation (5G) New Radio (NR) technology has provided unprecedented opportunities for vehicle-to-everything (V2X) networks, enabling enhanced quality of services. However, high-mobility V2X networks require frequent handovers and acquiring accurate channel state information (CSI) necessitates the utilization of pilot signals, leading to increased overhead and reduced communication throughput. To address this challenge, integrated sensing and communications (ISAC) techniques have been employed at the base station (gNB) within vehicle-to-infrastructure (V2I) networks, aiming to minimize overhead and improve spectral efficiency. In this study, we propose novel frame structures that incorporate ISAC signals for three crucial stages in the NR-V2X system: initial access, connected mode, and beam failure and recovery. These new frame structures employ 75% fewer pilots and reduce reference signals by 43.24%, capitalizing on the sensing capability of ISAC signals. Through extensive link-level simulations, we demonstrate that our proposed approach enables faster beam establishment during initial access, higher throughput and more precise beam tracking in connected mode with reduced overhead, and expedited detection and recovery from beam failures. Furthermore, the numerical results obtained from our simulations showcase enhanced spectrum efficiency, improved communication performance and minimal overhead, validating the effectiveness of the proposed ISAC-based techniques in NR V2I networks.}
}


@article{DBLP:journals/tmc/MehmetiPKP24,
	author = {Fidan Mehmeti and
                  Arled Papa and
                  Wolfgang Kellerer and
                  Thomas F. La Porta},
	title = {Minimizing Rate Variability With Effective Resource Utilization in
                  Cellular Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11061--11079},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389589},
	doi = {10.1109/TMC.2024.3389589},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MehmetiPKP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While one of the main features of 5G networks is provisioning very high rates with low (or no) variability to cellular users, it has been shown that this turns out to be very ineffective for operators because it leads to an abundance of unused network resources. Yet, reallocating the unused resources to the same users, after providing them with the same constant rate, increases back the variability in data rates. A more efficient way would be to provide different low-variability data rates to the users depending on their channel conditions while trying to bring the wasted resources to the lowest possible extent. To that end, in this paper, two approaches are considered; one with reserved resources for every user and the other where the amount of resources is decided on the fly, depending on their current channel conditions. Then, for each approach, we look at different allocation policies and derive the corresponding maximum achievable constant rate for every user jointly with the level of resource utilization, showing which policy is more beneficial. Further, the performance is evaluated on a real 5G trace using both extensive simulations and real measurements conducted on OpenAirInterface. Results show that no-resource reservation policies increase the utilization of resources and data rates at the expense of increased rate variability across all the users. Moreover, all the policies proposed in this paper outperform state-of-the-art approaches by at least \\text{2}\\times\n, bringing the waste of resources down to 15%.}
}


@article{DBLP:journals/tmc/WuFZYC24,
	author = {Axin Wu and
                  Dengguo Feng and
                  Min Zhang and
                  Anjia Yang and
                  Jialin Chi},
	title = {Privacy-Preserving Bilateral Multi-Receiver Matching With Revocability
                  for Mobile Social Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11080--11090},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3390036},
	doi = {10.1109/TMC.2024.3390036},
	timestamp = {Thu, 13 Feb 2025 14:31:48 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuFZYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile social networks (MSNs) offer convenient and ubiquitous services to expand social circles, share information, etc. These services require strict security measures to prevent the spread of deceptive content, misleading information, and malicious behavior. Achieving bilateral access control, message confidentiality and authenticity, and identity privacy can establish a positive network environment. Identity-based matchmaking encryption (IB-ME) with all the above features is a promising cryptographic primitive for MSNs. However, IB-ME can only specify one receiver. To share data with multiple users, the sender needs to encrypt the same message many times, resulting in higher frequencies of communication. Moreover, in multi-receiver scenarios, revocation of decryption permission may be necessary due to the possibility of malicious behavior, organization changes, or discontinuing subscription services. To our knowledge, no cryptographic primitives have been developed that satisfy these requirements. To address these challenges, we introduce the concept of revocable multi-receiver IB-ME and formalize its syntax and security definitions. We propose a revocable multi-receiver IB-ME scheme that provides privacy and authenticity in the random oracle model. Our evaluation demonstrates that it is efficient, and the sizes of system parameters and secret keys are independent of the number of receivers and revoked receivers.}
}


@article{DBLP:journals/tmc/LiuZCW24,
	author = {Li Liu and
                  Tengfei Zhao and
                  Sammy Chan and
                  Changmao Wu},
	title = {Continuous Object Tracking via Joint Global-Local Binary Tree Topological
                  Transformation in Underwater Acoustic Sensor Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11091--11104},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3389670},
	doi = {10.1109/TMC.2024.3389670},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent activities in marine energy exploration and transportation have led to the ongoing presence of continuous objects, such as oil spills and radioactive waste, in the ocean. This article focuses on enhancing the understanding of these objects’ boundaries for accurate assessment of their shape, coverage, and evolution. We introduce a continuous object tracking algorithm named JGL-COT, based on joint global-local binary tree topological transformations and specifically designed for underwater acoustic sensor networks. The contribution of JGL-COT lies in its ability to leverage the correlation between the morphologies of a continuous object's boundaries over time, alternating between global and local binary tree topological transformations. When the present boundary features a strong resemblance to its previous form, JGL-COT switches to a local transformation by establishing a semi-infinite region. Otherwise, it transitions to a global transformation. Following this, JGL-COT chooses a group of binary tree-structured cells for boundary mapping, creating virtual boundary nodes as sampling points for boundary fitting. Building upon graph theory, we derive the lower bound on the effectiveness and time complexity of the proposed joint global and local binary tree topological transformations when applied to object boundary tracking. Experiments in both realistic and simulated settings confirm that JGL-COT provides highly accurate tracking and significantly reduces network energy consumption.}
}


@article{DBLP:journals/tmc/RenYXLYLL24,
	author = {Yanzhi Ren and
                  Tingyuan Yang and
                  Zhiliang Xia and
                  Hongbo Liu and
                  Jiadi Yu and
                  Bo Liu and
                  Hongwei Li},
	title = {Robust Mobile Two-Factor Authentication Leveraging Acoustic Fingerprinting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11105--11120},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3391184},
	doi = {10.1109/TMC.2024.3391184},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RenYXLYLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The two-factor authentication (2FA) has become pervasive as the mobile devices become prevalent. In this work, we propose a secure 2FA that utilizes the individual acoustic fingerprint of the speaker/microphone on enrolled device as the second proof. The main idea behind our system is to use both magnitude and phase fingerprints derived from the frequency response of the enrolled device by emitting acoustic beep signals alternately from both enrolled and login devices and receiving their direct arrivals for 2FA. Given the input microphone samplings, our system designs an arrival time detection scheme to accurately identify the beginning point of the beep signal from the received signal. To achieve a robust authentication, we develop a new distance mitigation scheme to eliminate the impact of transmission distances from the sound propagation model for extracting stable fingerprint in both magnitude and phase domain. Our device authentication component then calculates a weighted correlation value between the device profile and fingerprints extracted from run-time measurements to conduct the device authentication for 2FA. Moreover, to thwart the possible co-located attacks, our proximity detection component further makes the enrolled phone to generate an active random vibration signal by its built-in motor, and then matches the signal received by the microphone of login device with the signal received by the accelerometer of enrolled phone to verify the proximity of two devices. Our experimental results show that our proposed system is accurate and robust to various attacks across different scenarios and device models.}
}


@article{DBLP:journals/tmc/HouGHYZW24,
	author = {Lu Hou and
                  Yunxing Geng and
                  Lingyi Han and
                  Haojun Yang and
                  Kan Zheng and
                  Xianbin Wang},
	title = {Masked Token Enabled Pre-Training: {A} Task-Agnostic Approach for
                  Understanding Complex Traffic Flow},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11121--11132},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3390941},
	doi = {10.1109/TMC.2024.3390941},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HouGHYZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate analysis of traffic flow (TF) data is crucial for the vehicular applications. Conventional deep learning models require task-specific training and are susceptible to high-frequency disturbances, degrading the feature representation capability. To overcome these limitations, this paper proposes a Token-based Self-Supervised Network (TSSN) that can learn TF features in both tokenization and task-agnostic manners. It provides a properly bootstrapped pre-training model for various downstream tasks. In support of the edge computing and vehicular cloud computing, the pooled computational resources facilitate real-time inferences of downstream models. In TSSN, TF data are segmented into tokens. A pretext task, named as Masked Token Prediction (MTP), is then developed to allow TSSN to understand the underlying correlations of TF by predicting randomly masked tokens. By utilizing MTP, TSSN is able to extract the high-level intrinsic semantics of TF, and provide general-purpose token embeddings, leading to improved overall performance and enhanced ability to adapt to different tasks. By substituting the last fully-connected layers with a group of untrained new layers and fine-tuning using small-scale task-specific data, TSSN can be utilized for a variety of downstream tasks in vehicular applications. Simulation results indicate that the TSSN enhances overall performance in comparison to state-of-the-art models.}
}


@article{DBLP:journals/tmc/LeeZPNA24,
	author = {Sunwoo Lee and
                  Tuo Zhang and
                  Saurav Prakash and
                  Yue Niu and
                  Salman Avestimehr},
	title = {Embracing Federated Learning: Enabling Weak Client Participation via
                  Partial Model Training},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11133--11143},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3392212},
	doi = {10.1109/TMC.2024.3392212},
	timestamp = {Fri, 31 Jan 2025 08:23:11 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LeeZPNA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Federated Learning (FL), clients may have weak devices that cannot train the full model or even hold it in their memory space. To implement large-scale FL applications, thus, it is crucial to develop a distributed learning method that enables the participation of such weak clients. We propose \\mathtt{EmbracingFL}, a general FL framework that allows all available clients to join the distributed training regardless of their system resource capacity. The framework is built upon a novel form of partial model training method in which each client trains as many consecutive output-side layers as its system resources allow. Our study demonstrates that \\mathtt{EmbracingFL} encourages each layer to have similar data representations across clients, improving FL efficiency. The proposed partial model training method guarantees convergence to a neighbor of stationary points for non-convex and smooth problems. We evaluate the efficacy of \\mathtt{EmbracingFL} under a variety of settings with a mixed number of strong, moderate (\\sim\\! 40\\% memory), and weak (\\sim\\! 15\\% memory) clients, datasets (CIFAR-10, FEMNIST, and IMDB), and models (ResNet20, CNN, and LSTM). Our empirical study shows that \\mathtt{EmbracingFL} consistently achieves high accuracy as like all clients are strong, outperforming the state-of-the-art width reduction methods (i.e., HeteroFL and FjORD).}
}


@article{DBLP:journals/tmc/ChenBZLQX24,
	author = {Lixing Chen and
                  Yang Bai and
                  Pan Zhou and
                  Youqi Li and
                  Zhe Qu and
                  Jie Xu},
	title = {On Adaptive Edge Microservice Placement: {A} Reinforcement Learning
                  Approach Endowed With Graph Comprehension},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11144--11158},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396510},
	doi = {10.1109/TMC.2024.3396510},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenBZLQX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microservice (MS) structures a service application as a collection of independently deployable service modules, making it particularly suitable for delivering complex applications in distributed computing systems. This article investigates MS architecture over Mobile Edge Computing (MEC) networks (hereafter referred to as EdgeMS) and studies an EdgeMS placement problem that aims to deploy MS modules over the MEC network in a manner that maximizes the reward of MS application providers. A novel algorithm called Dual-GNN Deep Deterministic Policy Gradient (DG-DDPG) is proposed to establish an intelligent EdgeMS placement policy for optimizing the location of MS modules and performing fractional computing resource allocation. DG-DDPG leverages the graph neural network (GNN) to comprehend the graph-structured information encapsulated in the MS application structure and MEC network. A dual-GNN core is constructed in DG-DDPG, one GNN for MS applications to distill knowledge from intricate connections between MS modules, and the other GNN for MEC networks to capture complicated interactions between edge sites when providing EdgeMS. DG-DDPG embeds the dual-GNN core in a DDPG-based reinforcement learning framework, which not only handles temporal dependencies between EdgeMS placement decisions for maximizing long-term reward but also supports continuous action space for enabling fractional resource allocation. In particular, the learning process of DG-DDPG is tailored to address hard constraints (i.e., computing capacity and MS application completeness) in the EdgeMS placement problem. We design constraint-based regularization terms and add them to the objective of DG-DDPG, which facilitates the identification of feasible placement decisions during learning. We carry out systematic experiments to evaluate the performance of DG-DDPG, and the results show that DG-DDPG outperforms state-of-the-art benchmarks in terms of reward, service delay and deployment cost.}
}


@article{DBLP:journals/tmc/LiD24,
	author = {Hongbo Li and
                  Lingjie Duan},
	title = {Human-in-the-Loop Learning for Dynamic Congestion Games},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11159--11171},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3391697},
	doi = {10.1109/TMC.2024.3391697},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Google Maps and Waze). Yet such platforms simply cater to selfish users’ myopic interests to recommend the shortest path, and do not encourage enough users to travel and learn other paths for future others. Prior studies focus on one-shot congestion games without considering users’ information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a human-in-the-loop manner. In a typical parallel routing network with one deterministic path and multiple stochastic paths, our analysis shows that the myopic routing policy (used by Google Maps and Waze) leads to severe under-exploration of stochastic paths. This results in a price of anarchy (PoA) greater than 2, as compared to the socially optimal policy achieved through optimal exploration-exploitation tradeoff in minimizing the long-term social cost. Besides, the myopic policy fails to ensure the correct learning convergence about users’ traffic hazard beliefs. To address this, we focus on informational (non-monetary) mechanisms as they are easier to implement than pricing. We first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even $\\text{PoA}=\\infty$. Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group. Our CHAR mechanism successfully ensures PoA less than $\\frac{5}{4}$, which cannot be further reduced by any other informational (non-monetary) mechanism. Besides the parallel network, we further extend our analysis and CHAR mechanism to more general linear path graphs with multiple intermediate nodes, and we prove that the PoA results remain unchanged. Additionally, we carry out experiments with real-world datasets to further extend our routing graphs and verify the close-to-optimal performance of our CHAR mechanism.}
}


@article{DBLP:journals/tmc/LiLCSLYJ24,
	author = {Zhao Li and
                  Siwei Le and
                  Jie Chen and
                  Kang G. Shin and
                  Jia Liu and
                  Zheng Yan and
                  Riku J{\"{a}}ntti},
	title = {Decomposed and Distributed Modulation to Achieve Secure Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11172--11190},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3391344},
	doi = {10.1109/TMC.2024.3391344},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLCSLYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid deployment and wide use of mobile services and applications, more and more sensitive user information is being transmitted wirelessly. Due to the broadcast nature of wireless transmissions, they are exposed to all surrounding entities and thus vulnerable to eavesdropping. To counter this vulnerability, we propose a new physical-layer secure transmission scheme, called DDM-Sec, based on decomposed and distributed modulation (DDM). We show that a high-order modulation can be decomposed into multiple quadrature phase shift keying (QPSK) modulations, each of which can be further represented by two mutually orthogonal binary phase shift keying (BPSK) modulations. Therefore, traditional modulation can be realized by two cooperative transmitters (Txs), each generating a BPSK signal, in a distributed manner. The legitimate receiver (Rx) can decode the desired/intended information from the mixed two received BPSK signals while preventing the eavesdropper from accessing the legitimate user’s information. DDM-Sec can effectively exploit the randomness of wireless channels to secure data transmission, enrich the spatial signatures of the legitimate user’s transmission by employing two cooperative Txs, and then distribute the user’s information to two transmissions so that none of the decomposed signals alone carry the legitimate user’s full information. Moreover, due to random deployment of the two Txs and Rx, delay difference of the two transmissions is introduced. This can be further utilized to make eavesdropping difficult. Our theoretical analysis and simulation have shown that DDM-Sec can effectively prevent the eavesdropping, and hence guarantee the secrecy of the legitimate user’s data transmission.}
}


@article{DBLP:journals/tmc/LiuZLLML24,
	author = {Jianqi Liu and
                  Zhiwei Zhao and
                  Xiangyang Luo and
                  Pan Li and
                  Geyong Min and
                  Huiyong Li},
	title = {SlaugFL: Efficient Edge Federated Learning With Selective GAN-Based
                  Data Augmentation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11191--11208},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3397585},
	doi = {10.1109/TMC.2024.3397585},
	timestamp = {Tue, 25 Feb 2025 07:53:20 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuZLLML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has been widely used to facilitate distributed and privacy-preserving machine learning in recent years. Different from centralized training that usually has independent and identically distributed (IID) distribution of all users’ data, FL suffers from significant communication cost and model performance degradation due to the non-IID data from individual edge devices. Existing work calibrates the local models using a global anchor or sharing global data. However, these studies either assume that the central server has the global dataset or require participating devices to share raw data, which incurs additional communication costs and privacy concerns. In this paper, we propose SlaugFL, a novel selective GAN-based data augmentation scheme for communication-efficient edge FL, which selects representative devices to share specific local class prototypes with the central server for GAN model training and improves FL performance with the trained GAN. Specifically, on the server side, we generate diverse labeled candidate data with the help of powerful generative models (the stable diffusion model and ChatGPT). To ensure that the GAN-generated data possesses a similar domain to the devices’ local data, we leverage these selected local class prototypes to pick desired GAN training samples from the labeled candidate data. On the device side, we propose a dual-calibration approach consisting of two calibration manners. Concretely, we augment devices’ non-IID data with the trained GAN model, where devices utilize the trained GAN model to generate the IID dataset. Thus, the device's local model can be directly calibrated with the augmented data. With the generated IID data, we yield privacy-free (p-f) global class prototypes which can be employed to further calibrate devices’ local models. Combining these two calibrations effectively improves devices’ local models. Extensive experimental results show that SlaugFL can significantly reduce the communication cost (up to 52.49%) while achieving the same accuracy, compared to the state-of-the-art work.}
}


@article{DBLP:journals/tmc/ZhuHQTS24,
	author = {Yuanwei Zhu and
                  Yakun Huang and
                  Xiuquan Qiao and
                  Jian Tang and
                  Xiang Su},
	title = {HiVAT: Improving QoE for Hybrid Video Streaming Service With Adaptive
                  Transcoding},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11209--11226},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399398},
	doi = {10.1109/TMC.2024.3399398},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHQTS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile video streaming enables flexible delivery of videos to mobile devices, supporting emerging video formats. The transition from conventional 2D videos to immersive formats, such as virtual reality and holographic videos, significantly increases the demand for computation and network resources. Existing streaming techniques are predominantly developed for specific video types, neglecting fair adaptive transmission and optimal resource utilization in services involving multiple video types. This paper investigates hybrid video streaming, encompassing 2D, 360-degree, and volumetric videos. To accommodate resource-intensive hybrid video streaming on mobile devices, we propose HiVAT, an adaptive transcoding-based system that ensures Quality of Experience (QoE) for each stream type. We contribute 1) a transcoding-based framework to address the challenges of high bandwidth and decoding overhead on mobile devices; 2) a universal QoE model involving traditional factors, viewport smoothness, degree of immersion, etc., for transcoded video streams; 3) a multi-agent adaptive bitrate controller that collaboratively determines hybrid video quality levels to achieve high and fair QoE across multiple streams; and 4) a learning-based task scheduler to optimize computation resource usage, thereby improving the overall serviceability of the system. We evaluate HiVAT against state-of-the-art methods, witnessing an average QoE improvement of 5.9% and 9.9% on linear and logarithmic metrics, respectively.}
}


@article{DBLP:journals/tmc/WuGDWXZK24,
	author = {Leijie Wu and
                  Song Guo and
                  Yaohong Ding and
                  Junxiao Wang and
                  Wenchao Xu and
                  Yufeng Zhan and
                  Anne{-}Marie Kermarrec},
	title = {Rethinking Personalized Client Collaboration in Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11227--11239},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396218},
	doi = {10.1109/TMC.2024.3396218},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuGDWXZK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has gained considerable attention recently, as it allows clients to cooperatively train a global machine learning model without sharing raw data. However, its performance can be compromised due to the high heterogeneity in clients’ local data distributions, commonly known as Non-IID (non-independent and identically distributed). Moreover, collaboration among highly dissimilar clients exacerbates this performance degradation. Personalized FL seeks to mitigate this by enabling clients to collaborate primarily with others who have similar data characteristics, thereby producing personalized models. We noticed that existing methods for assessing model similarity often do not capture the genuine relevance of client domains. In response, our paper enhances personalized client collaboration in FL by introducing a metric for domain relevance between clients. Specifically, to facilitate optimal coalition formation, we measure the marginal contributions of client models using coalition game theory, providing a more accurate representation of potential client domain relevance within the FL privacy-preserving framework. Based on this metric, we then adjust each client's coalition membership and implement a personalized FL aggregation algorithm that is robust to Non-IID data domain. We provide a theoretical analysis of the algorithm's convergence and generalization capabilities. Our extensive evaluations on multiple datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, and under varying Non-IID data distributions (Pathological and Dirichlet), demonstrate that our personalized collaboration approach consistently outperforms contemporary benchmarks in terms of accuracy for individual clients.}
}


@article{DBLP:journals/tmc/ZhangWSZWW24,
	author = {Dayou Zhang and
                  Lai Wei and
                  Kai Shen and
                  Hao Zhu and
                  Dan Wang and
                  Fangxin Wang},
	title = {TrimStream: Adaptive Realtime Video Streaming Through Intelligent
                  Frame Retrospection in Adverse Network Conditions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11240--11252},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396810},
	doi = {10.1109/TMC.2024.3396810},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWSZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Realtime video streaming (RVS) services are gaining popularity in various applications, such as video conferencing, online education, and mixed reality. However, adverse network conditions can significantly damage video transmission, leading to a decline in users’ Quality of Experience (QoE). Existing approaches have made considerable efforts to address these problems, including bitrate adaptation, FEC (forward error correction) encoding, and super-resolution techniques. Nevertheless, these methods either focus solely on adjusting transmission configurations (ABR) or consume additional network and computational resources to enhance QoE (FEC or super-resolution), making them suboptimal for adverse network conditions. In this paper, we analyze the limitations of conventional RVS systems when confronted with adverse network conditions and propose TrimStream, a novel RVS solution based on intelligent frame retrospection, to effectively handle such scenarios. Our approach leverages the high similarity observed between frames in realtime video streaming. The core idea is to store a subset of correctly received frames and exploit frame similarity to minimize transmission while breaking down frame-level dependencies. We formulate the frame caching problem to maximize QoE in RVS and present an online frame cache algorithm. Furthermore, we design a vision-transformer-based, cost-effective frame matching framework that combines different levels of frame information. Our evaluation results demonstrate that TrimStream outperforms state-of-the-art solutions by 14.8\\% \\sim 21.1\\%\nimprovement in overall QoE.}
}


@article{DBLP:journals/tmc/HeFYL24,
	author = {Ying He and
                  Jingcheng Fang and
                  F. Richard Yu and
                  Victor C. M. Leung},
	title = {Large Language Models (LLMs) Inference Offloading and Resource Allocation
                  in Cloud-Edge Computing: An Active Inference Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11253--11264},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3415661},
	doi = {10.1109/TMC.2024.3415661},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HeFYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing popularity and demands for large language model applications on mobile devices, it is difficult for resource-limited mobile terminals to run large-model inference tasks efficiently. Traditional deep reinforcement learning (DRL) based approaches have been used to offload large language models (LLMs) inference tasks to servers. However, existing DRL solutions suffer from data inefficiency, insensitivity to latency requirements, and non-adaptability to task load variations, which will degrade the performance of LLMs. In this paper, we propose a novel approach based on active inference for LLMs inference task offloading and resource allocation in cloud-edge computing. Extensive simulation results show that our proposed method has superior performance over mainstream DRLs, improves in data utilization efficiency, and is more adaptable to changing task load scenarios.}
}


@article{DBLP:journals/tmc/GroenDDBVPMC24,
	author = {Joshua Groen and
                  Salvatore D'Oro and
                  Utku Demir and
                  Leonardo Bonati and
                  Davide Villa and
                  Michele Polese and
                  Tommaso Melodia and
                  Kaushik R. Chowdhury},
	title = {Securing {O-RAN} Open Interfaces},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11265--11277},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3393430},
	doi = {10.1109/TMC.2024.3393430},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GroenDDBVPMC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of cellular networks will be characterized by openness, intelligence, virtualization, and distributed computing. The Open Radio Access Network (Open RAN) framework represents a significant leap toward realizing these ideals, with prototype deployments taking place in both academic and industrial domains. While it holds the potential to disrupt the established vendor lock-ins, Open RAN's disaggregated nature raises critical security concerns. Safeguarding data and securing interfaces must be integral to Open RAN's design, demanding meticulous analysis of cost/benefit tradeoffs. In this paper, we embark on the first comprehensive investigation into the impact of encryption on two pivotal Open RAN interfaces: the E2 interface, connecting the base station with a near-real-time RAN Intelligent Controller, and the Open Fronthaul, connecting the Radio Unit to the Distributed Unit. Our study leverages a full-stack O-RAN ALLIANCE compliant implementation within the Colosseum network emulator and a production-ready Open RAN and 5G-compliant private cellular network. This research contributes quantitative insights into the latency introduced and throughput reduction stemming from using various encryption protocols. Furthermore, we present four fundamental principles for constructing security by design within Open RAN systems, offering a roadmap for navigating the intricate landscape of Open RAN security.}
}


@article{DBLP:journals/tmc/SongYDXLYLX24,
	author = {Fuhong Song and
                  Qixun Yang and
                  Mingsen Deng and
                  Huanlai Xing and
                  Yanping Liu and
                  Xi Yu and
                  Kaiju Li and
                  Lexi Xu},
	title = {AoI and Energy Tradeoff for Aerial-Ground Collaborative {MEC:} {A}
                  Multi-Objective Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11278--11294},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3394568},
	doi = {10.1109/TMC.2024.3394568},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SongYDXLYLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the age of information (AoI) and energy tradeoff (AET) problem in an aerial-ground collaborative mobile edge computing system, where a high-altitude platform and an unmanned aerial vehicle (UAV) work together to offer computing services for ground devices (GDs). The AET problem is formulated as a multi-objective optimization problem (MOP) that aims at simultaneously minimizing the total AoI of GDs and total energy consumption of the UAV by optimizing its flight paths and task offloading ratios. Addressing the AET problem poses a significant challenge due to the inherent conflict between the two objectives. The existing methods cannot well address the MOP because they adopt the linear combination to transform an MOP into a single-objective optimization problem using fixed weights (i.e., preferences), ignoring the conflict between objectives. Moreover, user preferences may change over time in dynamic MEC systems. To overcome these challenges, we first build a multi-objective Markov decision process model with a vectorial reward for the AET problem. There are one-to-one relationships between each component of the reward and one of the two objectives. Then, we propose a multi-objective learning algorithm based on proximal policy optimization (PPO), which primarily comprises a training phase and an evolutionary phase. The former adopts multi-objective PPO to iteratively optimize multiple learning individuals, aiming to obtain a nondominated policy set. The latter employs a genetic operator to further improve the quality of each policy in the set. Specifically, the crossover and mutation operators operate at the parameter level of policy networks, avoiding stagnation and premature convergence. The experiment results validate that the proposed approach obtains a set of excellent nondominated policies and a favorable balance between objectives. Moreover, the proposed approach achieves improvements of at least 39.8%, 2.1%, and 15.3% regarding AoI, energy consumption, and cost compared with several algorithms.}
}


@article{DBLP:journals/tmc/ZhangLXJ24,
	author = {Yuncan Zhang and
                  Weifa Liang and
                  Zichuan Xu and
                  Xiaohua Jia},
	title = {Mobility-Aware Service Provisioning in Edge Computing via Digital
                  Twin Replica Placements},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11295--11311},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3394839},
	doi = {10.1109/TMC.2024.3394839},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangLXJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin (DT) has been emerging as an enabling technology to provide seamless interactions between the virtual cyber world and the real world. The explosion of IoT devices (objects) further fuels the development of the DT technology, and paves the way to real-time monitoring, behavior simulations and decisive predictions on objects through their digital counterparts. Meanwhile, Mobile Edge Computing (MEC) has been envisioned as a promising computing paradigm for various IoT applications with stringent delay requirements. In this paper, we study mobility-aware, delay-sensitive service provisioning in a DT-empowered MEC network with the mobility of both users and objects through DT replica placements of mobile objects. To this end, we first formulate two novel optimization problems: the DT replica placement problem and the dynamic DT replica placement problem, respectively, and show NP-hardness of the two problems. We then formulate an Integer Linear Programming (ILP) solution to the DT replica placement problem when the problem size is small or medium; otherwise we devise a randomized algorithm with high probability, provided that the mobility profiles of each object and each user are given. Meanwhile, we also develop an online algorithm for the dynamic DT replica placement problem, where for a given time horizon, service requests arrive one by one without the knowledge of future arrivals, each arrived request must be responded immediately by accepting or rejecting it. However, the heterogeneity and dynamics of user requests on resource demands may lead to the removals and re-instantiations of DT instances frequently. To mitigate this, we propose an efficient prediction mechanism to reserve a certain number of DTs for future by introducing the timestamp concept. We finally evaluate the performance of the proposed algorithms by simulations. Simulation results show that the proposed algorithms are promising, and outperform the performance of other comparison counterparts.}
}


@article{DBLP:journals/tmc/LeJL24,
	author = {Tien Thanh Le and
                  Yusheng Ji and
                  John C. S. Lui},
	title = {{MFTTS:} {A} Mean-Field Transfer Thompson Sampling Approach for Distributed
                  Power Allocation in Unsourced Multiple Access},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11312--11325},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399876},
	doi = {10.1109/TMC.2024.3399876},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LeJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsourced multiple access (UMA) is a novel approach to support a large number of devices in a massive Machine-Type Communication (mMTC) system. UMA enables devices to concurrently encode their data using the same codebook to transmit without being individually identified, resulting in reduced signaling and computational overhead at the base station. Hybrid-domain non-orthogonal multiple access (NOMA), which combines power-domain NOMA with code-domain NOMA, is another technique that enhances the spectral efficiency of mMTC. While the study of hybrid-domain NOMA has been conducted, its integration with UMA has not been thoroughly investigated. Considering that mMTC traffic primarily consists of sporadic short packets in the uplink direction, employing a fully distributed mMTC multiple access protocol can substantially decrease signaling overhead and latency. In this work, a multi-armed bandits (MAB) paradigm is adopted to create a distributed power selection policy for devices that using UMA. Particularly, an MAB algorithm called Thompson Sampling (TS) is used to allow mMTC devices to minimize the transmission power without violating the minimum receiving signal-to-noise constraint needed to correctly decode the UMA codewords back to the original messages. A mean-field modeling technique is used to approximate the learned policies. The knowledge gained from the approximated policies can be transferred to new devices by initializing their prior distribution, which is called Mean-field Transfer Thompson Sampling (MFTTS). Simulations show that the mean-field approximation is indeed accurate and effective. Interestingly, MFTTS performs better than TS without knowledge transfer as well as other distributed power allocation methods.}
}


@article{DBLP:journals/tmc/PallewattaKB24,
	author = {Samodha Pallewatta and
                  Vassilis Kostakos and
                  Rajkumar Buyya},
	title = {Reliability-Aware Proactive Placement of Microservices-Based IoT Applications
                  in Fog Computing Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11326--11341},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3394486},
	doi = {10.1109/TMC.2024.3394486},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PallewattaKB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fog computing paradigm is rapidly gaining popularity for latency-critical and bandwidth-hungry IoT application deployment. Meanwhile, MicroService Architecture (MSA) is increasingly adopted for developing IoT applications due to its high scalability and extensibility. For mission-critical IoT services in fog, reliability remains one of the most critical QoS requirements due to less dependability of fog resources. Granular microservices with independent deployment and scaling exhibit great potential in utilising resource-constrained fog resources to improve reliability through redundant placement. However, current research on service placement lacks reliability-aware holistic approaches that combine the MSA features and failure characteristics of fog resources under independent and correlated failures. Hence, we analyse MSA and formulate the reliability-aware placement problem by modelling composite services as k-out-of-n serial-parallel systems in a throughput-aware manner for placement under fog resource failures. Our proposed Reliability-aware Placement Method (RPM) is a hierarchical policy combining improved PSO and NSGA-II algorithms. We integrate it with Monte Carlo reliability calculations to produce redundant placements reaching a trade-off between reliability and cost. The performance results reveal that compared to the benchmarks, our algorithm shows significant improvements in reliability satisfaction (up to 25%) and time to first failure (up to 40%), thus providing a robust placement method.}
}


@article{DBLP:journals/tmc/LiuHZLNS24,
	author = {Lingshuang Liu and
                  Cheng Huang and
                  Dan Zhu and
                  Dongxiao Liu and
                  Jianbing Ni and
                  Xuemin Shen},
	title = {Enabling Efficient and Distributed Access Control for Pervasive Edge
                  Computing Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11342--11356},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3395388},
	doi = {10.1109/TMC.2024.3395388},
	timestamp = {Wed, 12 Feb 2025 20:32:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuHZLNS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an efficient and distributed service access control framework (E-DAC) in the pervasive edge computing (PEC) environment, where the resources of peer devices at the network edge are integrated to provide latency-sensitive computing services to the nearby devices on behalf of edge servers. E-DAC addresses the challenge of efficient and distributed service access control, comprising edge service authorization, service access authorization, and mutual authentication between edge servers and edge devices. In dong so, E-DAC first extends a key-aggregate cryptosystem to enable batch service authorization, in which a service provider can aggregate the authorization keys of different services to produce a constant-size aggregate key for an edge server. Second, E-DAC enables users to acquire authorization from the service provider for service access on edge servers by using efficient secret sharing. Third, edge servers and users can authenticate with each other without interacting with a centralized server, while enabling secure zero-round trip communication, so that the service data is protected and the communication bandwidth cost is low. In addition, the service provider is capable of efficiently revoking the authorization of the dropout or compromised edge servers or users in response to the dynamics of the PEC environment. Finally, we prove the security of service access control in E-DAC, including unforgeability of service authorization and confidentiality of service data, and conduct extensive analysis and experiments to demonstrate that E-DAC is highly computational and communication-efficient on service authorization, authentication, and revocation.}
}


@article{DBLP:journals/tmc/YangZGXNH24,
	author = {Yaoqi Yang and
                  Bangning Zhang and
                  Daoxing Guo and
                  Zehui Xiong and
                  Dusit Niyato and
                  Zhu Han},
	title = {Can We Realize Data Freshness Optimization for Privacy Preserving-Mobile
                  Crowdsensing With Artificial Noise?},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11357--11374},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396993},
	doi = {10.1109/TMC.2024.3396993},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangZGXNH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By utilizing intelligent mobile terminals, mobile crowdsensing (MCS) can realize the sensing data collection effectively and economically. However, the privacy security and freshness quality of the obtained sensing data are two major concerns to be addressed in MCS, as they directly impact the system security and timeliness performance. In this regard, we focus on improving the data freshness performance and protecting sensing data content, sensing terminals’ identification, and location information simultaneously. Accordingly, based on the artificial noise (AN)-based differential privacy and covert communication technologies, we aim to jointly minimize the Age of Information (AoI) metric and weighted privacy preservation budget in the single terminal scenario. Besides, we achieve the goal of average AoI optimization with data computing requirements in multiple terminal systems, where the privacy preservation budget is treated as the critical constraint. Furthermore, by using the backward induction (BI) method and block successive upper-bound minimization (BSUM) approach, we solve the above two optimization problems, respectively. Finally, compared with the listed baselines, the results evaluate the proposed schemes’ effectiveness under various simulation settings.}
}


@article{DBLP:journals/tmc/YaoWWY24,
	author = {Jiamin Yao and
                  Junli Wang and
                  Cheng Wang and
                  Chungang Yan},
	title = {DRL-Based {VNF} Cooperative Scheduling Framework With Priority-Weighted
                  Delay},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11375--11388},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3394370},
	doi = {10.1109/TMC.2024.3394370},
	timestamp = {Fri, 21 Feb 2025 12:56:50 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YaoWWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective Service Function Chains (SFCs) mapping and Virtual Network Functions (VNFs) scheduling are crucial to ensure high-quality service provision for Internet of Things (IoT) tasks. Meeting the varying demands of multiple SFCs poses a significant challenge, particularly when working with the limited resources available in edge computing networks. Most existing working focuses on uniformly mapping and scheduling service requests in a batch processing manner within a given time period, without taking the diversity and priority of VNFs into account. When there is a sudden surge in demand, the issues of VNF queueing waiting and resources imbalance become prominent. To address the mentioned issues, this paper proposes a Deep Reinforcement Learning (DRL)-based VNF cooperative scheduling framework with priority-weighted delay. In light of the urgency of VNFs with higher priorities and the limitations of available resources, we begin by modeling an average queuing delay with priority weight based on the shortest remaining time priority technique. We then formulate a mathematical optimization problem to minimize the modeled delay in VNF scheduling process while providing suitable multidimensional resources in the edge network. Finally, a DRL method with experience replay and target Q-network is designed to effectively obtain the optimal solutions of the optimization problem from experience. The experimental results show that our proposed method outperforms its peers in terms of SFC request acceptance, delay, load balance, and resource utilization.}
}


@article{DBLP:journals/tmc/WangDJRZ24,
	author = {Ziyuan Wang and
                  Jun Du and
                  Chunxiao Jiang and
                  Yong Ren and
                  Xiao{-}Ping Zhang},
	title = {UAV-Assisted Target Tracking and Computation Offloading in USV-Based
                  {MEC} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11389--11405},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396121},
	doi = {10.1109/TMC.2024.3396121},
	timestamp = {Tue, 03 Dec 2024 17:09:07 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangDJRZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, unmanned aerial vehicles (UAVs) have been widely used in ocean target tracking and image acquisition for processing. Due to the limited energy of the UAV and the high computational complexity associated with image processing tasks, a lightweight energy-saving target tracking scheme is designed for the UAV, and the unmanned surface vehicle (USV) based mobile edge computing (MEC) networks are adopted to share the computing load of the UAV. Due to the randomness of the environment, we formulate data processing, computation offloading, resource allocation, and target-tracking as a joint stochastic optimization problem. This paper investigates a two-stage optimization scheme to address the problem. First, we employ a Lyapunov-based approach to convert the stochastic optimization problem into a deterministic per-time slot problem under communication and computing resources constraints. Then, we develop a real-time target tracking scheme for the UAV based on the Elman neural network. Numerical results validate that the designed tracking scheme can effectively minimize propulsion energy consumption while maintaining a high success rate in tracking. Furthermore, the proposed method balances data-related energy consumption, image detection accuracy, and stability of the data storage queue.}
}


@article{DBLP:journals/tmc/XuLWXJZ24,
	author = {Yang Xu and
                  Yunming Liao and
                  Lun Wang and
                  Hongli Xu and
                  Zhida Jiang and
                  Wuyang Zhang},
	title = {Overcoming Noisy Labels and Non-IID Data in Edge Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11406--11421},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398801},
	doi = {10.1109/TMC.2024.3398801},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuLWXJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables edge devices to cooperatively train models without exposing their raw data. However, implementing a practical FL system at the network edge mainly faces three challenges: label noise, data non-IIDness, and device heterogeneity, which seriously harm model performance and slow down convergence speed. Unfortunately, none of the existing works tackle all three challenges simultaneously. To this end, we develop a novel FL system, called Aorta, which features adaptive dataset construction and aggregation weight assignment. On each client, Aorta first calibrates potentially noisy labels and then constructs a training dataset with low noise, balanced distribution, and proper size. To fully utilize limited data on clients, we propose a global model guided method to select clean data and progressively correct noisy labels. To achieve balanced class distribution and proper dataset size, we propose a distribution-and-capability-aware data augmentation method to generate local training data. On the server, Aorta assigns aggregation weights based on the quality of local models to ensure that high-quality models have a greater influence on the global model. The model quality is measured through its cosine similarity with a benchmark model, which is trained on a clean and balanced dataset. We conduct extensive experiments on four datasets with various settings, including different noise types/ratios and non-IID types/levels. Compared to the baselines, Aorta improves model accuracy up to 9.8% on the datasets with moderate noise and non-IIDness, while providing a speedup of 4.2× on average when achieving the same target accuracy.}
}


@article{DBLP:journals/tmc/ZhengDDKNZ24,
	author = {Jie Zheng and
                  Baoxia Du and
                  Hongyang Du and
                  Jiawen Kang and
                  Dusit Niyato and
                  Haijun Zhang},
	title = {Energy-Efficient Resource Allocation in Generative AI-Aided Secure
                  Semantic Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11422--11435},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396860},
	doi = {10.1109/TMC.2024.3396860},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengDDKNZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of semantic communication with Internet of Things (IoT) technologies has advanced the development of Semantic IoT (SIoT), with edge mobile networks playing an increasingly vital role. This paper presents a framework for SIoT-based image retrieval services, focusing on the application in automotive market analysis. Here, semantic information in the form of textual representations is transmitted to users, such as automotive companies, and stored as knowledge graphs, instead of raw imagery. This approach reduces the amount of data transmitted, thereby lowering communication resource usage, and ensures user privacy. We explore potential adversarial attacks that could disrupt image transmission in SIoT and propose a defense mechanism utilizing Generative Artificial Intelligence (GAI), specifically the Generative Diffusion Models (GDMs). Unlike methods that necessitate adversarial training with specifically crafted adversarial example samples, GDMs adopt a strategy of adding and removing noise to negate adversarial perturbations embedded in images, offering a more universally applicable defense strategy. The GDM-based defense aims to protect image transmission in SIoT. Furthermore, considering mobile devices’ resource constraints, we employ GDM to devise resource allocation strategies, optimizing energy use and balancing between image transmission and defense-related energy consumption. Our numerical analysis reveals the efficacy of GDM in reducing energy consumption during adversarial attacks. For instance, in a scenario, GDM-based defense lowers energy consumption by 5.64%, decreasing the number of image retransmissions from 18 to 6, thus underscoring GDM's role in bolstering network security.}
}


@article{DBLP:journals/tmc/LiuLCQJ24,
	author = {Mei Liu and
                  Yutong Li and
                  Yingqi Chen and
                  Yimeng Qi and
                  Long Jin},
	title = {A Distributed Competitive and Collaborative Coordination for Multirobot
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11436--11448},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3397242},
	doi = {10.1109/TMC.2024.3397242},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuLCQJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enlightened by competitive and collaborative coordination behaviors widely observed in natural swarm systems, this work emphasizes these coordinating modes in multirobot systems and optimizes system stability along with resource utilization. Then, schemes are constructed to describe and model these two modes, where a k\n-winner-take-all concept is introduced as the driving principle of multirobot competition. In addition, a distributed coordination approach is established to effectively handle the above schemes aided with optimality theory, which is developed by a fusion of a recurrent neural dynamics solver and a distributed solver. The former is a single-layer neural dynamics model with a simple structure, and the latter transforms the involved global information to a distributed type via consensus. Both of them are carried out in the discrete-time domain to fit the actual application. Finally, the convergence and stability of the proposed coordination approach are proved via theoretical analysis and further demonstrated through simulations and experiments.}
}


@article{DBLP:journals/tmc/QiuCLNWLLHHZ24,
	author = {Yu Qiu and
                  Min Chen and
                  Weifa Liang and
                  Dusit Niyato and
                  Yue Wang and
                  Yizhe Li and
                  Victor C. M. Leung and
                  Yixue Hao and
                  Long Hu and
                  Yin Zhang},
	title = {Reliable or Green? Continual Individualized Inference Provisioning
                  in Fabric Metaverse via Multi-Exit Acceleration},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11449--11465},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396612},
	doi = {10.1109/TMC.2024.3396612},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QiuCLNWLLHHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fabric metaverse employs intelligence fibers embedded with flexible sensors to unknowingly gather and transmit massive hypermodal data around humans to a deep neural network-based metaverse inference service (DMS) for continual and real-time analysis. Each DMS has one primary branch and multiple side branches that allow early termination of service with differential accuracy and energy consumption. However, the continual provisioning of compute-intensive DMS with varying requirements for service model, accuracy, delay, and reliability poses a challenge for edge servers characterized by restricted computing resources and intermittent green energy. In this paper, we focus on a continual individualized DMS provisioning problem in the fabric metaverse consisting of a side branch insertion subproblem and a server activation and service deployment subproblem, and formulate them as Integer linear Programming and Markov Decision Process, respectively. Then, we propose a green continual inference (GCI) system, where a pruner with provable approximation ratios trims superfluous branches of every model to the given number K to minimize total overflow accuracy between accuracy demands and reserved branches assigned to users. Based on this exit result, each DMS is further divided into several blocks with dependencies to exploit constrained resources of computing and energy in a fine-grained manner. Finally, a learning-based scheduler is merged into GCI to maximize request throughput while minimizing the activation number of edge servers on different demand scenarios, by adaptively activating suitable servers and deploying required blocks and their corresponding backups on selected servers. Theoretical analyses, simulations, and experiments demonstrate that the GCI is promising compared with baseline algorithms.}
}


@article{DBLP:journals/tmc/ChenOKIG24,
	author = {Ying Chen and
                  Sasamon Omoma and
                  Hojung Kwon and
                  Hazer Inaltekin and
                  Maria Gorlatova},
	title = {Quantifying and Exploiting {VR} Frame Correlations: An Application
                  of a Statistical Model for Viewport Pose},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11466--11482},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399770},
	doi = {10.1109/TMC.2024.3399770},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenOKIG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In virtual reality (VR), users’ head pose, that is, the location and the orientation of users’ viewport, determines the view of the virtual world that is shown to the users. The importance of the viewport pose to VR experiences calls for the development of VR viewport pose models. However, no study has obtained a full pose (the position and the orientation) model applicable to modeling the viewport pose in VR experiences. In this paper, informed by our experimental measurements of viewport trajectories across 4 different types of VR interfaces, we first develop a statistical model of viewport poses in VR environments. Based on the developed model, we examine the correlations between pixels in VR frames that correspond to different viewport poses, and obtain an analytical expression for the visibility similarity (ViS) of the pixels across different VR frames. We then propose a lightweight ViS-based algorithm (ALG-ViS) that adaptively splits VR frames into the background and the foreground, reusing the background across different frames. Our implementation of ALG-ViS in two Oculus Quest 2 rendering systems demonstrates ALG-ViS running in real time, supporting the full VR frame rate, and outperforming baselines on measures of frame quality and bandwidth consumption.}
}


@article{DBLP:journals/tmc/ZhangZLTYLM24,
	author = {Zhengyuan Zhang and
                  Dong Zhao and
                  Renhao Liu and
                  Kuo Tian and
                  Yuxing Yao and
                  Yuanchun Li and
                  Huadong Ma},
	title = {CamoNet: On-Device Neural Network Adaptation With Zero Interaction
                  and Unlabeled Data for Diverse Edge Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11483--11497},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398202},
	doi = {10.1109/TMC.2024.3398202},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZLTYLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying deep learning models to edge devices for low-latency and privacy-preserving applications has become a trend. To adapt to heterogeneous devices and data, it is significant to generate customized models. However, existing model adaptation approaches require edge devices to make interactions (collecting hardware information or local data) with the cloud, which raises privacy concerns, increases communication costs, and burdens the cloud. By contrast, we propose CamoNet, a universal on-device model adaptation framework with zero interaction between devices and the cloud. In CamoNet, a lightweight on-device neural architecture search module is utilized to quickly generate a customized model for subsequent on-device training, followed by an on-device contrastive transfer learning module to effectively leverage unlabeled data for fine-tuning the customized model. Extensive experimental results show that CamoNet can effectively run on various edge devices. Compared with the SOTA model adaptation approaches, CamoNet achieves significant accuracy improvement by 25.2% on average for image classification, 10.1% on average for object detection, and reduces the training memory by 4.8-11.4×. We will open-source our models and tools for edge AI developers.}
}


@article{DBLP:journals/tmc/AbantoLeonKGSSH24,
	author = {Luis F. Abanto{-}Leon and
                  Aravindh Krishnamoorthy and
                  Andres Garcia{-}Saavedra and
                  Gek Hong Sim and
                  Robert Schober and
                  Matthias Hollick},
	title = {Radio Resource Management Design for {RSMA:} Optimization of Beamforming,
                  User Admission, and Discrete/Continuous Rates With Imperfect {SIC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11498--11518},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396389},
	doi = {10.1109/TMC.2024.3396389},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AbantoLeonKGSSH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC). Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources. Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve. Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions. We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains. Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate. Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics. Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable.}
}


@article{DBLP:journals/tmc/ZhouWKWXQ24,
	author = {Yizhi Zhou and
                  Junxiao Wang and
                  Xiangyu Kong and
                  Shan Wu and
                  Xin Xie and
                  Heng Qi},
	title = {Exploring Amplified Heterogeneity Arising From Heavy-Tailed Distributions
                  in Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11519--11534},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398052},
	doi = {10.1109/TMC.2024.3398052},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouWKWXQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has emerged as a privacy-preserving paradigm enabling collaborative model training among distributed clients. However, current FL methods operate under the closed-world assumption, i.e., all local training data originates from a global labeled dataset balanced across classes, which is often invalid for practical scenarios. In contrast, in many open-world settings, data have been observed to exhibit heavy-tailed distributions, particularly in the realm of mobile computing and Internet of Things (IoT). Heavy-tailed data can have a significant negative impact on the performance of learning algorithms due to amplifying the heterogeneity in the FL environment. To this end, we introduce a novel framework to counter biased training caused by diverse and imbalanced classes. This framework includes a balance-aware reward aggregation mechanism addressing local majority and global minority class disparities. Rewards are assigned based on client class prevalence for fair aggregation. A calibration module supplements global aggregation to manage conflicts from inconsistent data distribution among clients. Using reward aggregation and calibration, we effectively mitigate heavy-tailed distribution effects, enhancing FL model performance. This framework seamlessly integrates with leading FL methods, demonstrated through extensive experiments on benchmark and real-world datasets.}
}


@article{DBLP:journals/tmc/LuWHLZL24,
	author = {Yuyang Lu and
                  Xingfu Wang and
                  Ammar Hawbani and
                  Ping Liu and
                  Liang Zhao and
                  Zhi Liu},
	title = {{EHTA:} An Environment-Cost-Based Heterogeneous Task Allocation in
                  Vehicular Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11535--11548},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396512},
	doi = {10.1109/TMC.2024.3396512},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuWHLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular crowd sensing (VCS), emerging as a new paradigm within mobile crowd sensing, leverages vehicles as the participator, which can obtain broader sensing coverage and higher sensing flexibility. Previous works ignored the strong impact of environmental factors on workers’ travel costs, as well as improper gains from speculative behavior (i.e., workers detour or delay to get more compensation), resulting in unfair income of workers. Moreover, these works focused solely on sensing tasks within specific domains, lacking generalization ability. Therefore, our work is dedicated to providing a fair and universal VCS platform, which is called Environment-cost-based Heterogeneous Task Allocation (EHTA) framework. Our work differs from previous works in the following aspects: 1) We introduce the Environment Cost (EC) based on the investigation of traffic conditions to accurately quantify workers’ efforts, and propose a straightforward yet efficacious detection methods to identify speculative behavior of malicious workers, both of which could guarantee the fairness in workers’ income. 2) We design a spatial-temporal fair incentive mechanism based on monetary reward to ensure the fair execution of tasks in both space and time dimensions. 3) We summarize the characteristics of three kinds of sensing tasks and propose a universal task allocation algorithm to assign multiple types of tasks simultaneously. The effectiveness of our framework was validated by simulations, which are conducted on a data set comprising 13,000 taxi trajectories from Shanghai in April 2015. We compared our framework against four baseline algorithms, and the results shows that EHTA framework outperforms in terms of task expenditure, task utility and fairness.}
}


@article{DBLP:journals/tmc/HuangCCRJFZ24,
	author = {Wenbin Huang and
                  Hanyuan Chen and
                  Hangcheng Cao and
                  Ju Ren and
                  Hongbo Jiang and
                  Zhangjie Fu and
                  Yaoxue Zhang},
	title = {Manipulating Voice Assistants Eavesdropping via Inherent Vulnerability
                  Unveiling in Mobile Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11549--11563},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3401096},
	doi = {10.1109/TMC.2024.3401096},
	timestamp = {Mon, 10 Feb 2025 14:42:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangCCRJFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous mobile devices are equipped with voice assistants to facilitate contactless user-device interaction. However, the widespread availability of voice assistants also raises security and privacy concerns, as they can be maliciously triggered to perform voice eavesdropping. Although diverse attacks have been taken to manipulate voice assistants for eavesdropping, they exhibit deficiencies of limited attack scopes and conspicuous attack behaviors because they target specific voice assistants or require extra voice commands to activate them. To manipulate arbitrary voice assistants for covert eavesdropping attack, we conduct a comprehensive analysis of voice assistant implementation in the Android system and refine a universal workflow. Through meticulous analysis and experimental verification, we uncover an inherent vulnerability that in voice assistants across device types that can be awakened by an artificial faking Intent. Building on this significant discovery, we propose an attack termed VoiceEar. It leverages a malicious event generation file and a first-in-first-out Intent generation algorithm to trigger voice assistants within the normal workflow for eavesdropping, without voice commands. Finally, we deploy the VoiceEar attacks on 25 mainstream mobile devices, and invite 95 volunteers for eavesdropping activity perception testing. The results unequivocally demonstrate the seamless execution of VoiceEar attacks, with neither users nor devices awareness.}
}


@article{DBLP:journals/tmc/BayarSIA24,
	author = {Ferhat Bayar and
                  Onur Salan and
                  Haci Ilhan and
                  Erdogan Aydin},
	title = {Space-Time Block Coded Reconfigurable Intelligent Surface-Based Received
                  Spatial Modulation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11564--11575},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398539},
	doi = {10.1109/TMC.2024.3398539},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BayarSIA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) structures reflect incident signals by adjusting phase adaptively according to the channel condition during transmission to increase signal quality at the receiver. Spatial modulation (SM) technique is a potential candidate for future energy-efficient wireless communications due to its ability to provide better throughput, low-cost implementation, and good error performance. Also, Alamouti's space-time block coding (ASBC) is an important space and time coding technique for the diversity gain and simplified maximum likelihood (ML) detection. In this paper, we propose the RIS-assisted received spatial modulation (RSM) scheme with ASBC, namely RIS-RSM-ASBC, for next-generation wireless networks. The RIS in the proposed system model is separated into two parts, each used as an access point (AP) to transmit its Alamouti coded information while reflecting passive signals to the selected receive antenna. We design the optimal ML detector for the proposed RIS-RSM-ASBC scheme. Furthermore, we present the greedy detector (GD) to reduce the complexity of the ML detector. Extensive computer simulations are performed to corroborate theoretical derivations. The results show that the RIS-RSM-ASBC system is highly reliable and provides data rate enhancement compared to traditional RIS-assisted transmit SM (RIS-TSM), RIS-assisted transmit quadrature SM (RIS-TQSM), RIS-assisted received SM (RIS-RSM), RIS-assisted transmit space shift keying with ASBC (RIS-TSSK-ASBC), and RIS-assisted TSSK with Vertical-Bell Laboratories Layered Space-Time (RIS-TSSK-VBLAST) schemes.}
}


@article{DBLP:journals/tmc/FengLSHWN24,
	author = {Shaohan Feng and
                  Xiao Lu and
                  Sumei Sun and
                  Ekram Hossain and
                  Guiyi Wei and
                  Zhengwei Ni},
	title = {Covert Communication in Large-Scale Multi-Tier {LEO} Satellite Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11576--11587},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396793},
	doi = {10.1109/TMC.2024.3396793},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FengLSHWN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We leverage covert communication to enhance the security of a large-scale multi-tier Low Earth Orbit (LEO) satellite network against vigilant adversarial terrestrial Base Stations (BSs) aiming at detecting satellite transmissions. This approach involves deploying massive LEO satellites at different altitudes around Earth to form a multi-tier network serving as a backhaul for near-ground Unmanned Aerial Vehicles (UAVs) that provide network services to terrestrial mobile users. Meanwhile, terrestrial BSs attempt to detect satellite transmissions based on their own received signal powers. To evade detection, the LEO satellite network performs power control to obscure the satellite transmission within the co-channel interference among the LEO satellites. We formulate a two-stage Stackelberg game to model the conflict dynamics between the terrestrial BSs and the LEO satellite network. In this game, the terrestrial BSs act as non-cooperative followers at the lower stage aiming to minimize their detection errors. On the other hand, the LEO satellite network acts as the leader at the upper stage aiming to maximize its utility while ensuring communication covertness. In contrast to existing works that focus on a small set of network nodes, our study considers a large-scale multi-tier LEO satellite network and employs stochastic geometry to model the spatial distribution of network nodes. To achieve the Stackelberg equilibrium, we develop a bi-level algorithm based on Successive Convex Approximation (SCA) and golden-section search. Our numerical results provide practical insights, revealing a trade-off in leveraging co-channel interference (i.e., while it improves the communication covertness of satellite transmission, it simultaneously degrades the link reliability).}
}


@article{DBLP:journals/tmc/GhoshGDM24,
	author = {Shreya Ghosh and
                  Soumya K. Ghosh and
                  Sajal K. Das and
                  Prasenjit Mitra},
	title = {Mobilytics: Mobility Analytics Framework for Transferring Semantic
                  Knowledge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11588--11603},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3413589},
	doi = {10.1109/TMC.2024.3413589},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GhoshGDM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of sensor-equipped smartphones has led to the generation of vast amounts of GPS data, such as timestamped location points, enabling a range of location-based services. However, deciphering the spatio-temporal dynamics of mobility to understand the underlying motivations behind travel patterns presents a significant challenge. This paper focuses on how individuals’ GPS traces (latitude, longitude, timestamp) interpret the connection and correlations among different entities such as people, locations or point-of-interests (POIs), and semantic contexts (trip-purpose). We introduce a mobility analytics framework, named Mobilytics designed to identify trip purposes from individual GPS traces by leveraging a “mobility knowledge graph” (MKG) and a deep learning architecture that automatically annotates the GPS log. Additionally, we propose a novel “transfer learning” approach to explore movement dynamics in a geographically distant area by leveraging knowledge obtained from a comparable region, such as an academic campus. In terms of major contributions and novelty, this is the first work to present end-to-end daily mobility trip purpose extraction and mobility knowledge transfer for trip annotation and POI-tagging where the labeled data are insufficient. Experimental results on real-life datasets of five different regions demonstrate the efficacy of our proposed Mobilytics framework which outperforms the baselines for trip-purpose extraction and POI annotations by a significant margin (\\approx 18% to \\approx 30%). Moreover, the analysis on huge volume of simulated traces (10,000 users) illustrates the scalability and robustness of the framework.}
}


@article{DBLP:journals/tmc/ChenXCML24,
	author = {Zheyi Chen and
                  Bing Xiong and
                  Xing Chen and
                  Geyong Min and
                  Jie Li},
	title = {Joint Computation Offloading and Resource Allocation in Multi-Edge
                  Smart Communities With Personalized Federated Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11604--11619},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3396511},
	doi = {10.1109/TMC.2024.3396511},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenXCML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Through deploying computing resources at the network edge, Mobile Edge Computing (MEC) alleviates the contradiction between the high requirements of intelligent mobile applications and the limited capacities of mobile End Devices (EDs) in smart communities. However, existing solutions of computation offloading and resource allocation commonly rely on prior knowledge or centralized decision-making, which cannot adapt to dynamic MEC environments with changeable system states and personalized user demands, resulting in degraded Quality-of-Service (QoS) and excessive system overheads. To address this important challenge, we propose a novel Personalized Federated deep Reinforcement learning based computation Offloading and resource Allocation method (PFR-OA). This innovative PFR-OA considers the personalized demands in smart communities when generating proper policies of computation offloading and resource allocation. To relieve the negative impact of local updates on global model convergence, we design a new proximal term to improve the manner of only optimizing local Q-value loss functions in classic reinforcement learning. Moreover, we develop a new partial-greedy based participant selection mechanism to reduce the complexity of federated aggregation while endowing sufficient exploration. Using real-world system settings and testbed, extensive experiments demonstrate the effectiveness of the PFR-OA. Compared to benchmark methods, the PFR-OA achieves better trade-offs between delay and energy consumption and higher task execution success rates under different scenarios.}
}


@article{DBLP:journals/tmc/YangCSP24,
	author = {Wenjun Yang and
                  Lin Cai and
                  Shengjie Shu and
                  Jianping Pan},
	title = {Mobility-Aware Congestion Control for Multipath {QUIC} in Integrated
                  Terrestrial Satellite Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11620--11634},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3397164},
	doi = {10.1109/TMC.2024.3397164},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangCSP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Integrated Terrestrial and LEO Satellite Network (ITSN) has a high bandwidth-delay product (BDP) and high-speed movement, which makes congestion control difficult. We develop a Mobility-Aware COngestion control (MACO) algorithm for multipath QUIC (MPQUIC) in ITSN. MACO models the dynamic interactions between MPQUIC subflows and LEO networks, including handovers and outages triggered by satellite movement, and changes in network topology and link conditions. With the knowledge of network dynamics influenced by mobility, MACO can estimate changes in path BDP without solely relying on lengthy network probing. It employs a quick start (QS) and an effective congestion avoidance (CA) mechanism based on a multipath fluid model. The QS sets an appropriate initial cwnd to shorten the slow start duration. The CA applies a square root function to quickly increase the cwnd to the equilibrium and conservatively increase when approaching the BDP. We conduct a series of experiments to evaluate MACO using network simulator 3 (ns-3) based on collected data traces on Starlink. Simulation results demonstrate that MACO can achieve upto three times higher throughput and improve the convergence performance by 70.67% against benchmark algorithms.}
}


@article{DBLP:journals/tmc/LiZZZCA24,
	author = {Meng Li and
                  Mingwei Zhang and
                  Liehuang Zhu and
                  Zijian Zhang and
                  Mauro Conti and
                  Mamoun Alazab},
	title = {Decentralized and Privacy-Preserving Smart Parking With Secure Repetition
                  and Full Verifiability},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11635--11654},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3397687},
	doi = {10.1109/TMC.2024.3397687},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZZZCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart Parking Services (SPSs) enable cruising drivers to find the nearest parking lot with available spots, reducing the traveling time, gas, and traffic congestion. However, drivers risk the exposure of sensitive location data during parking query to an untrusted Smart Parking Service Provider (SPSP). Our motivation arises from a repetitive query to an updated database, i.e., how a driver can be repetitively paired with a previously-matched-but-forgotten lot. Meanwhile, we aim to achieve repetitive query in an oblivious and unlinkable manner. In this work, we present Mnemosyne^{2}: decentralized and privacy-preserving smart parking with secure repetition and full verifiability. Specifically, we design repetitive, oblivious, and unlinkable Secure k Nearest Neighbor (Sk NN) with basic verifiability (correctness and completeness) for encrypted-and-updated databases. We build a local Ethereum blockchain to perform driver-lot matching via smart contracts. To adapt to the lot count update, we resort to the immutable blockchain for advanced verifiability (truthfulness). Last, we utilize decentralized blacklistable anonymous credentials to guarantee identity privacy. Finally, we formally define and prove privacy and security. We conduct extensive experiments over a real-world dataset and compare Mnemosyne^{2} with existing work. The results show that a query only needs 8 seconds (175 ms) on average for service waiting (verification) among 500 drivers.}
}


@article{DBLP:journals/tmc/SalehiRGDC24,
	author = {Batool Salehi and
                  Debashri Roy and
                  Jerry Gu and
                  Chris Dick and
                  Kaushik R. Chowdhury},
	title = {FLASH-and-Prune: Federated Learning for Automated Selection of High-Band
                  mmWave Sectors using Model Pruning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11655--11669},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3401046},
	doi = {10.1109/TMC.2024.3401046},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SalehiRGDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast sector-steering in the mmWave band for vehicular mobility scenarios is a challenge because standard-defined exhaustive search over predefined antenna sectors cannot be assuredly completed within short contact times. This paper proposes machine learning to speed up sector selection using data from multiple non-RF sensors, such as LiDAR, GPS, and camera images in the mmWave radios with large codebooks. The contributions in this paper are threefold: First, we propose a multimodal deep learning architecture that fuses the inputs from these data sources and locally predicts the sectors for best alignment at a vehicle. Second, we propose FLASH-and-Prune, which combines the knowledge from multiple vehicles by aggregating the local model parameters and exploits model pruning to optimize the model parameter exchange overhead. Third, we present a pruning strategy that takes into account the distributed nature of federated learning to adaptively prune or retrieve model weights. We validate the proposed architecture on a real-world multimodal dataset collected from an autonomous car. We observe that FLASH-and-Prune incurs 29.25% and 35.89% less overhead in the uplink and downlink, respectively, compared to standard federated learning.}
}


@article{DBLP:journals/tmc/MonemiFRL24,
	author = {Mehdi Monemi and
                  Mohammad Amir Fallah and
                  Mehdi Rasti and
                  Matti Latva{-}aho},
	title = {6G Fresnel Spot Beamfocusing using Large-Scale Metasurfaces: {A} Distributed
                  DRL-Based Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11670--11684},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398296},
	doi = {10.1109/TMC.2024.3398296},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MonemiFRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel approach to smart spot-beamforming (SBF) in the Fresnel zone leveraging extremely large-scale programmable metasurfaces (ELPMs). A smart SBF scheme aims to adaptively concentrate the aperture's radiating power exactly at a desired focal point (DFP) in the 3D space utilizing some Machine Learning (ML) method. This offers numerous advantages for next-generation networks including ultra-high-speed wireless communication, location-based multiple access (LDMA), efficient wireless power transfer (WPT), interference mitigation, and improved information security. SBF necessitates ELPMs with precise channel state information (CSI) for all ELPM elements. However, obtaining exact CSI for ELPMs is not feasible in all environments; we alleviate this by developing a novel CSI-independent ML scheme based on the TD3 deep-reinforcement-learning (DRL) method. While the proposed ML-based scheme is well-suited for relatively small-size arrays, the computational complexity is unaffordable for ELPMs. To overcome this limitation, we introduce a modular highly scalable structure composed of multiple sub-arrays, each equipped with a TD3-DRL optimizer. This setup enables collaborative optimization of the radiated power at the DFP, significantly reducing computational complexity while enhancing learning speed. The proposed structure's benefits in terms of 3D spot-like power distribution, convergence rate, and scalability are validated through simulation results.}
}


@article{DBLP:journals/tmc/LinYDOWWZ24,
	author = {Chi Lin and
                  Wei Yang and
                  Haipeng Dai and
                  Mohammad S. Obaidat and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Maximizing Charging Utility With Fresnel Diffraction Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11685--11699},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398026},
	doi = {10.1109/TMC.2024.3398026},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinYDOWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefitting from the recent breakthrough of wireless power transfer technology, Wireless Rechargeable Sensor Networks (WRSNs) have become an important research topic. Most prior arts focus on system performance enhancement in the ideal environment that ignores the impact of obstacles. This contradicts the practical applications in which obstacles can be found almost anywhere and have dramatic impacts on energy transmission. In this paper, we concentrate on the problem of charging a practical WRSN in the presence of obstacles to maximize the charging utility under specific energy constraints. First, we propose a new theoretical charging model with obstacles based on the Fresnel diffraction model and conduct experiments to verify its effectiveness. Then, we propose a spatial discretization scheme to obtain a finite feasible charging position set for mobile charger (MC), which largely reduces computation overhead. Afterwards, we re-formalize charging utility maximization with energy constraints as a submodular function maximization problem and propose a cost-efficient algorithm with an approximation guarantee to solve it. In addition, we present a theoretical analysis and a relevant mathematical proof of our algorithm. Finally, we demonstrate that our scheme outperforms other competing algorithms by 20.5% on average in terms of charging utility through test-bed experiments and extensive simulations.}
}


@article{DBLP:journals/tmc/ZhuLGZL24,
	author = {Shengtong Zhu and
                  Yan Liu and
                  Lingfeng Guo and
                  Yuming Zhang and
                  Jack Y. B. Lee},
	title = {On Rate-Limiting in Mobile Data Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11700--11718},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3398125},
	doi = {10.1109/TMC.2024.3398125},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLGZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid deployment of LTE/5G services, mobile subscribers now have access to high-speed services approaching Gbps. However, most mobile data plans have data quota from a few GBs up, beyond which the subscriber will be restricted to much lower bandwidth (e.g., 1 Mbps) – rate-limited service. Rate limiting not only poses a significant challenge to service providers, as it is often mistaken for network problems, triggering false alarms at the providers, but may also cause significant performance anomalies at the application layer and transport layer. This work tackles two central problems in mobile network rate-limiting, namely rate-limiting classification and parameter estimation, through a novel model-based online rate-limiter (MODRL) detector that can detect the presence of rate limiting and estimate its parameters passively from transport layer ACK. Experiments in controlled network testbed and production 4G/5G mobile networks show that MODRL can achieve remarkably high and consistent classification accuracy across a wide range of networks. Preliminary results from integrating MODRL into adaptive video streaming and QUIC transport demonstrate that it can effectively eliminate the performance anomalies caused by rate limiting, and open new avenues to further optimize protocol performance over rate-limited mobile networks.}
}


@article{DBLP:journals/tmc/SunXXGZ24,
	author = {He Sun and
                  Mingjun Xiao and
                  Yin Xu and
                  Guoju Gao and
                  Shu Zhang},
	title = {Crowdsensing Data Trading for Unknown Market: Privacy, Stability,
                  and Conflicts},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11719--11734},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399816},
	doi = {10.1109/TMC.2024.3399816},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunXXGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Crowdsensing Data Trading (CDT) has emerged as a new data trading paradigm, where buyers crowdsource data collection tasks to a group of mobile users with sensing devices (a.k.a., sellers) who sell the collected data to them, through a platform as the broker for a long-term data trading. One of the most critical issues in CDT is ensuring the stability of the matching between buyers and sellers in the data trading market. In this paper, we focus on privacy protection and the stability problems in the CDT market with unknown preference sequences of buyers. The goal is to protect sellers’ data qualities and ensure the CDT market's stability while maximizing the cumulative data quality for each task. We model this problem as a differentially private multi-player multi-armed competing bandit problem and propose a novel metric of the approximate stability, called \\delta\n-stability. We propose a privacy-preserving stable CDT mechanism called DPS-CB to solve this problem in the centralized setting, which is based on stable matching theory, and competing bandit strategy. Moreover, we extend it into decentralized setting in order to avoid the competitive matching conflicts caused in this setting and propose a Conflicts-avoiding DPS-CB mechanism, called CDPS-CB, by using Bernoulli probability and selecting feasible sets of sellers. In addition, we prove the security and stability of the CDT market under privacy concerns and analyze the regret performance of DPS-CB and CDPS-CB mechanisms, respectively. Finally, the significant performance of these two mechanisms is demonstrated through extensive simulations on a real-world dataset.}
}


@article{DBLP:journals/tmc/JinMLWPF24,
	author = {Can Jin and
                  Xiangzhu Meng and
                  Xuanheng Li and
                  Jie Wang and
                  Miao Pan and
                  Yuguang Fang},
	title = {Rodar: Robust Gesture Recognition Based on mmWave Radar Under Human
                  Activity Interference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11735--11749},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3402356},
	doi = {10.1109/TMC.2024.3402356},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JinMLWPF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using mmWave radar to conduct gesture recognition is a promising solution for human-computer interaction. Although many studies have shown initial success, two-fold problems still remain unsolved, namely, the high-strength human activity interference and the difficulty in handling similar gestures. In light of these, we develop a robust mmWave radar based gesture recognition system, Rodar, to achieve accurate recognition of similar gestures under high-strength human activity interference, where a Multi-view De-interference Transformer (MvDeFormer) network is proposed. Specifically, to deal with the strong human activity interference, we design a DeFormer module to capture the useful gesture features by learning different patterns between gestures and interference, thereby reducing the impact of interference. Then, we develop a hierarchical multi-view fusion module to first extract the enhanced features within each view, and effectively fuse them across various views for final recognition. To evaluate the proposed Rodar system, we construct a dataset with seven similar gestures under three common human activity interference scenarios. Experimental results show that the accuracy can achieve up to 93.01%.}
}


@article{DBLP:journals/tmc/WangZZJWCGZT24,
	author = {Bingbing Wang and
                  Fengyuan Zhu and
                  Linling Zhong and
                  Meng Jin and
                  Xinbing Wang and
                  Cailian Chen and
                  Xinping Guan and
                  Chenghu Zhou and
                  Xiaohua Tian},
	title = {Enabling Dual-Band Wi-Fi Backscatter},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11750--11764},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3400013},
	doi = {10.1109/TMC.2024.3400013},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZJWCGZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents dual-band Wi-Fi backscatter (DBscatter), which is the first system supporting 2.4 GHz and 5 GHz Wi-Fi backscatter simultaneously in a single tag. Our key insight is that most existing Wi-Fi devices communicate in the clean 5 GHz band. The 5 GHz band provides more chances for “free riding” with less interference, while the 2.4 GHz band presents better NLoS performance. DBscatter combines the strengths of the existing 2.4 GHz band with the unexplored 5 GHz backscatter in a unified design, developing a robust and high-throughput ambient Wi-Fi backscatter system. We make the following technical contributions: (1) We design a dual-band RF frontend to support dual-band Wi-Fi signals. (2) We propose a tag data demodulation algorithm, which merges the common phase error in multi-antenna received signals, improving the tag transmission reliability while reducing the number of required receivers. (3) We build a prototype of DBscatter system using COTS FPGAs and SDRs. Compared to TiScatter and FreeRider, DBscatter boosts Wi-Fi backscatter throughput by 3.74X and 7.35X, and energy efficiency by 1.78X and 1.38X respectively.}
}


@article{DBLP:journals/tmc/TangZZPWSYC24,
	author = {Zijie Tang and
                  Tianming Zhao and
                  Tianfang Zhang and
                  Huy Phan and
                  Yan Wang and
                  Cong Shi and
                  Bo Yuan and
                  Yingying Chen},
	title = {{RF} Domain Backdoor Attack on Signal Classification via Stealthy
                  Trigger},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11765--11780},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3404341},
	doi = {10.1109/TMC.2024.3404341},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangZZPWSYC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) has recently become a key technology supporting radio frequency (RF) signal classification applications. Given the heavy DL training requirement, adopting outsourced training is a practical option for RF application developers. However, the outsourcing process exposes a security vulnerability that enables a backdoor attack. While backdoor attacks have been explored in the vision domain, it is rarely explored in the RF domain. In this work, we present a stealthy backdoor attack that targets DL-based RF signal classification. To realize such an attack, we extensively explore the characteristics of the RF data in different applications, which include RF modulation classification and RF fingerprint-based device identification. Then, we design a training-based backdoor trigger generation approach with different optimization procedures for two backdoor attack scenarios (i.e., poison-label and clean-label). Extensive experiments on two RF signal classification datasets show that the attack success rate is over 99.2%, while its classification accuracy for the clean data remains high (i.e., less than a 0.6% drop compared to the clean model). The low NMSE (less than 0.091) indicates the stealthiness of the attack. Additionally, we demonstrate that our attack can bypass existing defense strategies, such as Neural Cleanse and STRIP.}
}


@article{DBLP:journals/tmc/ChenSZSZCY24,
	author = {Dayin Chen and
                  Xiaodan Shi and
                  Haoran Zhang and
                  Xuan Song and
                  Dongxiao Zhang and
                  Yuntian Chen and
                  Jinyue Yan},
	title = {A Phone-Based Distributed Ambient Temperature Measurement System With
                  an Efficient Label-Free Automated Training Strategy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11781--11793},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399843},
	doi = {10.1109/TMC.2024.3399843},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenSZSZCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enhancing the energy efficiency of buildings significantly relies on monitoring indoor ambient temperature. The potential limitations of conventional temperature measurement techniques, together with the omnipresence of smartphones, have redirected researchers’ attention towards the exploration of phone-based ambient temperature estimation methods. However, existing phone-based methods face challenges such as insufficient privacy protection, difficulty in adapting models to various phones, and hurdles in obtaining enough labeled training data. In this study, we propose a distributed phone-based ambient temperature estimation system which enables collaboration among multiple phones to accurately measure the ambient temperature in different areas of an indoor space. This system also provides an efficient, cost-effective approach with a few-shot meta-learning module and an automated label generation module. It shows that with just 5 new training data points, the temperature estimation model can adapt to a new phone and reach a good performance. Moreover, the system uses crowdsourcing to generate accurate labels for all newly collected training data, significantly reducing costs. Additionally, we highlight the potential of incorporating federated learning into our system to enhance privacy protection. We believe this study can advance the practical application of phone-based ambient temperature measurement, facilitating energy-saving efforts in buildings.}
}


@article{DBLP:journals/tmc/YuJ24,
	author = {Lixing Yu and
                  Tianxi Ji},
	title = {Efficient Federated Learning With Channel Status Awareness and Devices'
                  Personal Touch},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11794--11806},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3403754},
	doi = {10.1109/TMC.2024.3403754},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YuJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a widely used distributed learning framework. However, constrained wireless environment and intrinsically heterogeneous data across devices can hinder the FL framework being practical. In this paper, we propose a communication-efficient FL framework that helps boost the training process by considering the transmission power of each device and the local models’ personalized training. In each round of training, we select the participating devices that can minimize the upper bound of the convergence rate plus the corresponding communication overhead while subjecting to the transmit power constraint. Besides, each device update a personalized and sparse model that only consumes limited computation resources. We validate our proposed FL framework on various dataset, and experiment results show that our framework speeds up the training process by taking \\sim\n40% less time than the existing frameworks. Also, the communication time can be significantly decreased by employing our framework, e.g., we achieve as high as a 42.7% increase in test accuracy and save up to 74.3\\%\nin the communication cost compared with FedAvg.}
}


@article{DBLP:journals/tmc/QinLCCW24,
	author = {Langtian Qin and
                  Hancheng Lu and
                  Yuang Chen and
                  Baolin Chong and
                  Feng Wu},
	title = {Toward Decentralized Task Offloading and Resource Allocation in User-Centric
                  {MEC}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11807--11823},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399766},
	doi = {10.1109/TMC.2024.3399766},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QinLCCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the traditional cellular-based mobile edge computing (MEC), users at the edge of the cell are prone to suffer severe inter-cell interference and signal attenuation, leading to low throughput even transmission interruptions. Such edge effect severely obstructs offloading of tasks to MEC servers. To address this issue, we propose user-centric mobile edge computing (UCMEC), a novel MEC architecture integrating user-centric transmission, which can ensure high throughput and reliable communication for task offloading. Then, we formulate an long-term delay minimization problem by jointly optimizing task offloading, power allocation, and computing resource allocation in UCMEC. To solve the intractable problem, we propose two decentralized joint optimization schemes based on multi-agent deep reinforcement learning (MADRL) and convex optimization, which consider both cooperation and non-cooperation among network nodes. Simulation results demonstrate that the proposed schemes in UCMEC can significantly improve the uplink transmission rate by at least 176.99% and reduce the long-term average total delay by at least 16.36% compared to traditional cellular-based MEC.}
}


@article{DBLP:journals/tmc/LiuFZLCLZ24,
	author = {Xuyang Liu and
                  Kaiyu Feng and
                  Zijian Zhang and
                  Meng Li and
                  Xi Chen and
                  Wenqian Lai and
                  Liehuang Zhu},
	title = {Dolphin: Efficient Non-Blocking Consensus via Concurrent Block Generation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11824--11838},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3399772},
	doi = {10.1109/TMC.2024.3399772},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuFZLCLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain technology has become a research hotspot in distributed systems, aiming to sustain a decentralized ledger via consensus. Traditional consensus solutions exhibit slow processing speed and response time, resulting in poor performance. To address this issue, several consensus protocols have been proposed. One such popular protocol is HotStuff, a Byzantine fault-tolerant consensus (BFT) that achieves high throughput at the cost of latency. However, its throughput suffers from a proportional decrease with the increase in latency, posing a significant challenge. In this paper, we propose a new protocol called Dolphin that builds upon HotStuff. It operates in a partially synchronous network with n replicas, up to f byzantine faults, where n \\geq 3f+1, and achieves higher throughput in high-latency environments by leveraging non-blocking concurrent block generation. Specifically, we formalize our strategy as a generic Asynchronization Procedure Patch and prove that it does not affect the execution process of the original protocol. Theoretical analysis validates that Dolphin preserves the safety, liveness, and responsiveness properties while enhancing the throughput. The evaluation demonstrates that Dolphin typically achieves more than 10x higher throughput in Wide Area Network (WAN) environments with lower latency compared to HotStuff and its variants, and exhibits similar bandwidth utilization to DAG-based protocols such as Narwhal.}
}


@article{DBLP:journals/tmc/RuanCHSL24,
	author = {Na Ruan and
                  Jikun Chen and
                  Tu Huang and
                  Zekun Sun and
                  Jie Li},
	title = {Fool Attackers by Imperceptible Noise: {A} Privacy-Preserving Adversarial
                  Representation Mechanism for Collaborative Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11839--11852},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3405548},
	doi = {10.1109/TMC.2024.3405548},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RuanCHSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance of deep learning models highly depends on the amount of training data. It is common practice for today's data holders to merge their datasets and train models collaboratively, which yet poses a threat to data privacy. Different from existing methods, such as secure multi-party computation (MPC) and federated learning (FL), we find representation learning has unique advantages in collaborative learning due to its low privacy budget, wide applicability to tasks and lower communication overhead. However, data representations face the threat of model inversion attacks. In this article, we formally define the collaborative learning scenario, and present ARS (for adversarial representation sharing), a collaborative learning framework wherein users share representations of data to train models, and add imperceptible adversarial noise to data representations against reconstruction or attribute extraction attacks. By theoretical analysis and evaluating ARS in different contexts, we demonstrate that our mechanism is effective against model inversion attacks, and can achieve great utility and low communication complexity while preserving data privacy. Moreover, the ARS framework has wide applicability, which can be easily extended to the vertical data partitioning scenario and utilized in different tasks.}
}


@article{DBLP:journals/tmc/GuoWGSCXH24,
	author = {Zhengxin Guo and
                  Dongzi Wang and
                  Linqing Gui and
                  Biyun Sheng and
                  Hui Cai and
                  Fu Xiao and
                  Jinsong Han},
	title = {UWTracking: Passive Human Tracking Under {LOS/NLOS} Scenarios Using
                  {IR-UWB} Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11853--11870},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3404460},
	doi = {10.1109/TMC.2024.3404460},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoWGSCXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Passive human tracking plays a critical role in the field of ubiquitous sensing, offering customized services such as real-time location tracking for vital sign monitoring and motion detection. Traditional contact-free tracking systems are primarily designed for Line-of-Sight (LOS) scenarios, requiring a direct path between the radio device and the target. However, in Non-Line-of-Sight (NLOS) scenarios, where obstacles obstruct this direct path, these systems suffer from sensing failures and are unable to accurately obtain the motion trajectory of the sensing target. In this paper, we propose the UWTracking system, which utilizes the Commercial Off-the-Shelf (COTS) Impulse Radio-Ultra Wideband (IR-UWB) radars to enable precise indoor passive human tracking in both LOS and NLOS scenarios. To effectively capture the motion information of a moving target in NLOS scenarios, we present the Reconstructed Distributed-Doppler Frequency Shift (RD-DFS) features. We then binarize the RD-DFS features and design the Distance Extraction Algorithm (DEA) to obtain the target's distance in both scenarios. Subsequently, the Circle Intersection Method with Distance Stretching (CIM-DS) algorithm is developed to determine the indoor position of the sensing target, and the Scanning Angle and Velocity Particle Filter (SAV-PF) algorithm facilitates high-precision trajectory tracking. We implement a prototype of UWTracking system and conduct extensive evaluations to showcase its trajectory tracking performance under various scenarios. The results demonstrate that UWTracking achieves effective real-time tracking, with a median tracking error of 17.65 cm in the LOS scenario and 23.34 cm in the NLOS scenario, outperforming the state-of-the-art trajectory tracking systems based on COTS IR-UWB Radars.}
}


@article{DBLP:journals/tmc/LinDH24,
	author = {Qinqi Lin and
                  Lingjie Duan and
                  Jianwei Huang},
	title = {Learning From Social Interactions: Personalized Pricing and Buyer
                  Manipulation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11871--11888},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3411111},
	doi = {10.1109/TMC.2024.3411111},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the sociological theory of homophily suggests, people tend to interact with those of similar preferences. Motivated by this well-established phenomenon, today's online sellers, such as Amazon, seek to learn a new buyer's private preference from his friends’ purchase records. Although such learning allows the seller to enable personalized pricing and boost revenue, buyers are also increasingly aware of these practices and may alter their social behaviors accordingly. This paper presents the first study regarding how buyers strategically manipulate their social interaction signals considering their preference correlations, and how a seller can take buyers’ strategic social behaviors into consideration when designing the pricing scheme. Starting with the basic two-buyer network, we propose and analyze a parsimonious model that uniquely captures the double-layered information asymmetry between the seller and buyers, integrating both individual buyer information and inter-buyer correlation information. Our analysis reveals that only high-preference buyers tend to manipulate their social interactions to evade the seller's personalized pricing, but surprisingly, their payoffs may actually worsen as a result. Additionally, we demonstrate that the seller can considerably benefit from the learning practice, regardless of whether the buyers are aware of this fact or not. Indeed, our analysis reveals that buyers’ learning-aware strategic manipulation has only a slight impact on the seller's revenue. In light of the tightening regulatory policies concerning data access, it is advisable for sellers to maintain transparency with buyers regarding their access to buyers’ social interaction data for learning purposes. This finding aligns well with current informed-consent industry practices for data sharing. Finally, we explore the seller's dynamic learning process across multiple interconnected buyers, and show that learning previous buyers’ preferences may not necessarily help infer other buyers’ preferences in the seller's subsequent learning phase.}
}


@article{DBLP:journals/tmc/ZhuoLHLCHL24,
	author = {Weipeng Zhuo and
                  Shiju Li and
                  Tianlang He and
                  Mengyun Liu and
                  S.{-}H. Gary Chan and
                  Sangtae Ha and
                  Chul{-}Ho Lee},
	title = {Online Path Description Learning Based on {IMU} Signals From IoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11889--11906},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406436},
	doi = {10.1109/TMC.2024.3406436},
	timestamp = {Tue, 11 Feb 2025 13:57:23 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuoLHLCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A user's movement path can be precisely and concisely described as a concatenation of straight lines having the user's turns as their end points. Learning such a path description or representation from inertial measurement unit (IMU) sensors enables various mobile and IoT applications, as it allows efficient processing of the movement path data. It is, however, non-trivial to learn a succinct yet accurate path description from IMU sensor readings in the mobile device of a moving user on the fly due to the dynamically changing behaviors and the technical difficulty in detecting the user's turns. We propose PATHLIT, a novel online path description learning system based on IMU signals. PATHLIT learns position vectors of a user from IMU sensor readings by our custom-made self-attention network model. Once each position vector is learned, PATHLIT also decides whether or not to take it as a part of the resulting path description by our efficient online algorithm developed under the minimum description length principle, which essentially detects the user's turns along the path. We conduct extensive experiments on two large datasets. The experiment results show that PATHLIT achieves superior performance over state-of-the-art algorithms by up to 50% in absolute trajectory error using only 15% of trajectory data points.}
}


@article{DBLP:journals/tmc/FengLBJ24,
	author = {Fangzheng Feng and
                  Guanghua Liu and
                  Ting Bi and
                  Tao Jiang},
	title = {Edge Selective Sharing for Massive Mobile Video Streaming With Cross-Layer
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11907--11919},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3402237},
	doi = {10.1109/TMC.2024.3402237},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FengLBJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an edge selective sharing architecture (ESSA) for massive mobile video streaming (MMVS) and develop cross-layer optimizations for joint user scheduling and power allocation (JUSPA) in ESSA. This work aims to ensure users’ high quality of experience (QoE) by efficiently using network resources over mobile edge computing (MEC)-assisted massive multiple-input and multiple-output (MIMO) networks. Initially, by taking advantage of the analytical and empirical characteristics of video streaming, ESSA is able to maintain MMVS by activating only a portion of facilities in wireless access networks (WANs) without sacrificing video quality. Following this, a cross-layer optimization problem is formulated for JUSPA. We simplify the problem into a generalized assignment (GA) problem by discussing the peculiarities of MMSV, whose approximate solution can be obtained in polynomial time. Moreover, load balancing based on user density is integrated to reformulate a capacitated facility location (CFL) problem solvable with low overall time complexity, effectively mitigating the increased overheads of MEC units and enhancing the overall performance of MMVS. Numerical results indicate that our ESSA with JUSPA schemes for MMVS achieves better streaming fluency and transmission efficiency than alternative methods.}
}


@article{DBLP:journals/tmc/ZhengYWD24,
	author = {Shensheng Zheng and
                  Wenhao Yuan and
                  Xuehe Wang and
                  Lingjie Duan},
	title = {Adaptive Federated Learning via New Entropy Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11920--11936},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3402080},
	doi = {10.1109/TMC.2024.3402080},
	timestamp = {Mon, 09 Dec 2024 10:42:35 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhengYWD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has emerged as a prominent distributed machine learning framework that enables geographically discrete clients to train a global model collaboratively while preserving their privacy-sensitive data. However, due to the non-independent-and-identically-distributed (Non-IID) data generated by heterogeneous clients, the performances of the conventional federated optimization schemes such as FedAvg and its variants deteriorate, requiring the design to adaptively adjust specific model parameters to alleviate the negative influence of heterogeneity. In this paper, by leveraging entropy as a new metric for assessing the degree of system disorder, we propose an adaptive FEDerated learning algorithm based on ENTropy theory (FedEnt) to alleviate the parameter deviation among heterogeneous clients and achieve fast convergence. Nevertheless, given the data disparity and parameter deviation of heterogeneous clients, determining the optimal dynamic learning rate for each client becomes a challenging task as there is no communication among participating clients during the local training epochs. To enable a decentralized learning rate for each participating client, we first introduce the mean-field terms to estimate the components associated with other clients’ local parameters. Furthermore, we provide rigorous theoretical analysis on the existence and determination of the mean-field estimators. Based on the mean-field estimators, the closed-form adaptive learning rate for each client is derived by constructing the Hamilton equation. Moreover, the convergence rate of our proposed FedEnt is proved. The extensive experimental results on the real-world datasets (i.e., MNIST, EMNIST-L, CIFAR10, and CIFAR100) show that our FedEnt algorithm surpasses FedAvg and its variants (i.e., FedAdam, FedProx, and FedDyn) under Non-IID settings and achieves a faster convergence rate.}
}


@article{DBLP:journals/tmc/MingYT24,
	author = {Zhao Ming and
                  Hao Yu and
                  Tarik Taleb},
	title = {Federated Deep Reinforcement Learning for Prediction-Based Network
                  Slice Mobility in 6G Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11937--11953},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3404125},
	doi = {10.1109/TMC.2024.3404125},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MingYT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slices are generally coupled with services and face service continuity/unavailability concerns due to the high mobility and dynamic requests from users. Network slice mobility (NSM), which considers user mobility, service migration, and resource allocation from a holistic view, is witnessed as a key technology in enabling network slices to respond quickly to service degradation. Existing studies on NSM either ignored the trigger detection before NSM decision-making or didn't consider the prediction of future system information to improve the NSM performance, and the training of deep reinforcement learning (DRL) agents also faces challenges with incomplete observations. To cope with these challenges, we consider that network slices migrate periodically and utilize the prediction of system information to assist NSM decision-making. The periodical NSM problem is further transformed into a Markov decision process, and we creatively propose a prediction-based federated DRL framework to solve it. Particularly, the learning processes of the prediction model and DRL agents are performed in a federated learning paradigm. Based on extensive experiments, simulation results demonstrate that the proposed scheme outperforms the considered baseline schemes in improving long-term profit, reducing communication overhead, and saving transmission time.}
}


@article{DBLP:journals/tmc/YangGTLLZ24,
	author = {Lu Yang and
                  Songtao Guo and
                  Chen{-}Khong Tham and
                  Mingyan Li and
                  Guiyan Liu and
                  Pengzhan Zhou},
	title = {PreM-FedIoV: {A} Novel Federated Reinforcement Learning Framework
                  for Predictive Maintenance in IoV},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11954--11970},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3404249},
	doi = {10.1109/TMC.2024.3404249},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangGTLLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Vehicles (IoV) enhances data availability by equipping a plethora of sensors, driving the automotive industry towards data-driven Predictive Maintenance (PreM) models. However, traditional centralized PreM solutions, requiring complete access to training data, raise concerns about data privacy. PreM in the automotive domain is more challenging than in many other fields, partly due to the varying distribution nature of data samples and the limited network connectivity time caused by vehicle mobility. To address these challenges, we propose the PreM-FedIoV framework, extending single-agent Double Deep Q-Network (DDQN) to Multi-Agent Double Deep Q-Network (MADDQN). In each round, each vehicle client uploads a data packet to the server based on the current contention window, containing its local model, local test Mean Absolute Error (MAE), and a timestamp. The server initially performs federated aggregation on the received local models. The MADDQN module then dynamically adjusts the contention window of each vehicle for the next round based on the local test MAE and communication statistical state, aiming to optimize communication costs and predictive performance. Additionally, we utilize NS-3 to create IoV simulations and deploy the PreM-FedIoV framework within NS3-gym. We choose Federated Averaging (FedAvg) and FedAdam following the IEEE 802.11p standard as baselines. The experiments demonstrate significant improvements in our framework compared to state-of-the-art algorithms. On the C-MAPSS dataset, we achieve reductions of up to 10.2% in MAE, 26.31% in average communication clock time per round, and 65.6% in the number of participating clients per round. For the Random Battery Usage dataset, with up to 4.55%, 24.44%, and 36.58% improvements in the respective metrics.}
}


@article{DBLP:journals/tmc/WangCZWWM24,
	author = {Zhenning Wang and
                  Yue Cao and
                  Huan Zhou and
                  Libing Wu and
                  Wei Wang and
                  Geyong Min},
	title = {Fairness-Aware Two-Stage Hybrid Sensing Method in Vehicular Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11971--11988},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3408751},
	doi = {10.1109/TMC.2024.3408751},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangCZWWM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By utilizing on-board sensors and computing resources in intelligent vehicles, vehicular crowdsensing can collect a series of sensing data. Typically, sensing vehicles can be divided into opportunistic vehicles with fixed trajectories and participatory vehicles with changeable trajectories. Therefore, to complete sensing tasks more effectively, how to combine the advantages of the mobility characteristics of the two vehicles is a challenging problem. To solve this problem, this paper innovatively proposes a joint scheduling and incentive-driven two-stage hybrid sensing method. Specifically, the method is divided into two stages: opportunistic vehicle selection and participatory vehicle scheduling. In particular, both types of vehicles are managed through the Crowd Sensing Platform (CSP). For the first stage, this paper proposes a reverse auction-based incentive mechanism to select the lowest-cost set of vehicles to complete sensing tasks. This mechanism mainly consists of two steps: winning vehicle selection and reward payment. It is also verified that the proposed mechanism can ensure the individual rationality and truthfulness of opportunistic vehicles. For the second stage, based on the first-stage sensing results, this paper proposes a Soft Actor-Critic (SAC) based approach to scheduling participatory vehicle trajectories to complete sensing tasks. In addition, this paper also considers sensing fairness to ensure the balance of sensing task completion in different sub-regions. Through the two-stage hybrid sensing method, this paper aims to minimize the CSP overhead while ensuring sensing fairness. Finally, extensive evaluation results based on Roma taxi data sets demonstrate that the proposed method works effectively and outperforms other benchmark schemes in different working scenarios.}
}


@article{DBLP:journals/tmc/AlamM24,
	author = {Muhammad Morshed Alam and
                  Sangman Moh},
	title = {Joint Trajectory Control, Frequency Allocation, and Routing for {UAV}
                  Swarm Networks: {A} Multi-Agent Deep Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {11989--12005},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3403890},
	doi = {10.1109/TMC.2024.3403890},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AlamM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative unmanned aerial vehicle (UAV) swarm networks can effectively execute various emerging missions such as surveillance and communication coverage. However, due to high mobility and constrained transmission range, packet routing encounters mutual interferences, link breakages, and unexpected delays. In such networks, routing performance is coupled with trajectory control, frequency allocation, and relay selection. In this study, we propose a joint trajectory control, frequency allocation, and packet routing (JTFR) algorithm, in which link utility is maximized by considering the link stability, signal-to-interference-plus-noise ratio, queuing delay, and residual energy of UAVs. The proposed JTFR employs adaptive distributed multi-agent deep deterministic policy gradient coupled with the swarming behavior to obtain the optimal solution. For each UAV, an actor network is established by utilizing a long short-term memory-based state representation layer containing two-hop neighbor information to adopt the dynamic time-varying topology. Subsequently, a scalable multi-head attentional critic network is set up to adaptively adjust the actor network policy of each UAV by collaborating with neighbors. The extensive simulation results show that JTFR outperforms existing routing protocols by 30–60% less end-to-end delay, 15–32% better packet delivery ratio, and 20–46% less energy consumption.}
}


@article{DBLP:journals/tmc/AlbaneseDSRBC24,
	author = {Antonio Albanese and
                  Francesco Devoti and
                  Vincenzo Sciancalepore and
                  Marco Di Renzo and
                  Albert Banchs and
                  Xavier Costa{-}P{\'{e}}rez},
	title = {{ARES:} Autonomous {RIS} Solution With Energy Harvesting and Self-Configuration
                  Towards 6G},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12006--12019},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3405076},
	doi = {10.1109/TMC.2024.3405076},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AlbaneseDSRBC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surfaces (RISs) are expected to play a crucial role in reaching the key performance indicators (KPIs) for future 6G networks. Their competitive edge over conventional technologies lies in their ability to control the propagation properties of the wireless environment at will, thus revolutionizing the traditional communication paradigm that perceives the communication channel as an uncontrollable black box. As RISs transition from research to market, practical deployment issues arise. Major roadblocks for commercially viable RISs are i) the need for a fast and complex control channel to adapt to the ever-changing wireless channel conditions, and ii) an extensive grid to supply power to each deployed RIS. In this paper, we question the established RIS practices and propose a novel RIS design combining self-configuration and energy self-sufficiency capabilities. We analyze the feasibility of devising fully-autonomous RISs that can be easily and seamlessly installed throughout the environment, according to the new Internet-of-Surfaces (IoS) paradigm, requiring modifications neither to the deployed mobile network nor to the power distribution system. In particular, we introduce ARES, an Autonomous RIS solution with Energy harvesting and Self-configuration. ARES achieves outstanding communication performance while demonstrating the feasibility of energy harvesting (EH) for RISs power supply in future deployments.}
}


@article{DBLP:journals/tmc/KangMLLYZ24,
	author = {Xin Kang and
                  Zhuo Ma and
                  Yang Liu and
                  Teng Li and
                  Zuobin Ying and
                  Bingsheng Zhang},
	title = {Multi-Party Private Edge Computing for Collaborative Quantitative
                  Exposure Detection of Endemic Diseases},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12020--12034},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3403664},
	doi = {10.1109/TMC.2024.3403664},
	timestamp = {Thu, 13 Feb 2025 09:22:27 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KangMLLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facing the global threat of endemic diseases, utilizing edge computing for exposure detection enables efficient monitoring of the dynamic distribution of infected patient groups across regions, enhancing the management and control of these diseases. Employing the quantitative exposure detection of endemic diseases, regions seek to reconcile patient information collected through mobile devices, aiming to obtain statistical and analytical results based on the intersection of patient lists. In this paper, we propose a privacy-preserving scheme for the collaborative quantitative exposure detection of endemic diseases, which ensures each region to only learn the statistical results, without any information about other regions’ datasets. Our scheme is fundamentally achieved through Circuit-based Private Set Intersection (Circuit-PSI) that can compute functions over the set intersection without disclosing the intersection itself. However, the state-of-the-art solution involves a laborious process in which one party iteratively compares its elements with those of others, which leads to a significantly high communication complexity. Therefore, we introduce a novel multi-party protocol that can diminish the communication overhead of circuit-PSI through a skillful decoupling of the comparison complexity from the number of parties. Furthermore, to address scenarios involving patient information with additional attributes, we extend our protocol to include payloads by developing a lightweight multiparty data mapping algorithm. Our extensive experiments show that compared to prior works, our protocol achieves a substantial reduction in communication overhead by 6.4×, and runs 1.2× faster in the LAN setting and 3.1× in the WAN setting.}
}


@article{DBLP:journals/tmc/JeonSW24,
	author = {Kang Eun Jeon and
                  James She and
                  Simon Wong},
	title = {Energy Status Recovery Using Recurrent {SVR} Framework With Data Loss
                  Conditions},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12035--12045},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3405988},
	doi = {10.1109/TMC.2024.3405988},
	timestamp = {Sat, 07 Dec 2024 15:55:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JeonSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To address the short-lived battery lifetime of Bluetooth low energy (BLE) beacons, researchers proposed solar-powered designs, equipped with rechargeable energy storage such as a supercapacitor. However, accurately monitoring the energy status - an essential step for device maintenance - has shown to be a major concern. Existing energy status monitoring methods, which are either crowd-assisted or require on-site data collection, suffer from severe losses of energy status information. This paper presents an energy status recovery framework with support vector regression (SVR) to address this issue. The proposed framework leverages recurrence training of SVR with lost energy status information to capture features from discharge behavior, achieving high accuracy while minimizing training and prediction time. Multiple real-life BLE beacon energy level records are evaluated to demonstrate that our proposed framework can recover the energy information with at least 98% accuracy under a data loss rate of up to 99%.}
}


@article{DBLP:journals/tmc/GongHJ24,
	author = {Hao Gong and
                  Baoqi Huang and
                  Bing Jia},
	title = {Energy-Efficient 3-D {UAV} Ground Node Accessing Using the Minimum
                  Number of UAVs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12046--12060},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3405494},
	doi = {10.1109/TMC.2024.3405494},
	timestamp = {Sat, 30 Nov 2024 21:08:15 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GongHJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative multiple unmanned aerial vehicles (UAVs) have been widely exploited in various applications, including data collection, forest monitoring, edge computing, and so on. Due to limited onboard storage and expensive hardware costs, reducing both the energy consumption and the number of UAVs is critical for these multi-UAV applications. However, existing studies primarily revolved around energy minimization in two-dimensional (2-D) scenarios, given a sufficient but fixed number of UAVs, and most of them considered specific application scenarios, resulting in poor generality. In contrast, this paper defines a generalized application scenario, in which multiple ground nodes (GNs) are accessed by multiple UAVs in three-dimensional (3-D) scenarios, and aims to minimize the energy consumption by employing the necessary (or equivalently minimum) number of UAVs and formulating a mix-integer nonconvex problem. To this end, this paper decomposes the problem into two subproblems: energy consumption minimization for a single UAV consecutively accessing any two GNs and energy-efficient multi-UAV GN-accessing path planning employing the minimum number of UAVs. The first subproblem is solved by applying the successive convex approximation (SCA) technique and the path discretization method, while the second subproblem is addressed by designing a three-stage approximation framework based on modified particle swarm optimization (MPSO) and greedy path assignment (GPA). Comprehensive simulations demonstrate the superior performance of the proposed method in terms of optimality and efficiency compared to several other counterparts.}
}


@article{DBLP:journals/tmc/TangWYXZ24,
	author = {Jine Tang and
                  Sen Wang and
                  Song Yang and
                  Yong Xiang and
                  Zhangbing Zhou},
	title = {Federated Learning-Assisted Task Offloading Based on Feature Matching
                  and Caching in Collaborative Device-Edge-Cloud Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12061--12079},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3403851},
	doi = {10.1109/TMC.2024.3403851},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TangWYXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing provides relatively rich computation resources for Internet-of-Things (IoT) task offloading at the edge of networks. As time goes on, user tasks present diverse requirements in function, type, dependency, urgency, etc., which makes edge servers take on dynamically diversified service features to adapt to the requirements of user tasks. Moreover, cache has been studied a lot in recent years for reducing the execution cost of related or dependent tasks. However, jointly considering which result data required to be cached and where to cache is still an intractable problem in task offloading due to dynamically diversified and sensitive features of task and edge servers for prediction. To provide more comprehensive consideration, we propose a multiple features matching scheme, coupled with federated learning-assisted collaborative caching, to enhance the efficiency of task offloading. Specifically, we first build a common features of historical tasks based FI-tree to help search for an edge server that best matches the requested task features. This helps to obtain optimal task allocation and improve offloading performance. Further, the results of tasks related to or dependent on cached results can be obtained directly through the collaborative edge cache prediction model trained by two-stage federated learning. In this way, the amount of data executed for offloaded tasks is reduced, thereby speeding up the return of final results as well as reducing the delay and energy of task execution. Meanwhile, it avoids massive transmission of task results correlated data and also protects the privacy of these data when training the prediction model. Experimental results show that our proposed method outperforms the benchmark approaches through reducing the time delay and energy consumption by at least 15.6% and 18.2%.}
}


@article{DBLP:journals/tmc/HuangTJZ24,
	author = {Wenbin Huang and
                  Wenjuan Tang and
                  Hongbo Jiang and
                  Yaoxue Zhang},
	title = {Recognizing Voice Spoofing Attacks via Acoustic Nonlinearity Dissection
                  for Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12080--12096},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3411791},
	doi = {10.1109/TMC.2024.3411791},
	timestamp = {Mon, 10 Feb 2025 14:42:45 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangTJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millions of mobile devices are currently equipped with voice assistant (VA) for robust identity authentication. Regrettably, VA authentication remains susceptible to voice spoofing attacks, encompassing playback, synthesis, and conversion attacks. Despite numerous proposed defense schemes, these solutions exhibit deficiencies such as limited versatility and cumbersome implementation. Many are specialized in detecting only one specific type of attack, necessitate additional equipment, or mandate placing the device in specific locations. In this study, we introduce a versatile and user-friendly scheme designed to counteract voice spoofing attacks by analyzing common nonlinear features inherent in vocalization systems. Initially, we demonstrate the nonlinear nature of both human and mobile device vocalization by scrutinizing the mechanisms and processes of voice generation. Subsequently, we develop a comprehensive nonlinear model and extract a universal acoustic nonlinear property to discern sounds produced by humans from those generated by loudspeakers, thereby enhancing resistance against spoofing attacks. Finally, we conduct extensive experiments utilizing a real-world collected dataset and the supplementary ASVspoof2017 dataset. Evaluation results reveal that the proposed scheme significantly improves accuracy and computation cost by nearly 40% and 15%, respectively.}
}


@article{DBLP:journals/tmc/WangGZLL24,
	author = {Yao Wang and
                  Tao Gu and
                  Yu Zhang and
                  Minjie Lyu and
                  Hui Li},
	title = {Exploring a Secure Device Pairing Using Human Body as a Conductor},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12097--12112},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406016},
	doi = {10.1109/TMC.2024.3406016},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangGZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research has been exploring ways to streamline device pairing by introducing touch-to-access that minimizes user interaction. It generates pairing keys by extracting features from a shared information source to ascertain if two devices are being held by the same person. While these solutions focus on verifying the authenticity of the device, they do not consider the legitimacy and pairing intent of the device holder. Moreover, the pairing keys exchanged over an open wireless link may be susceptible to eavesdropping attacks. In this paper, we propose a secure device pairing mechanism that utilizes the unique electrical responses of the human body to generate and transmit user-specific pairing keys, ensuring both the user's legitimacy and pairing intent while also improving key transmission reliability. We accomplish this by using the device's built-in microphone to capture ambient sound as entropy and converting it into an electrical signal transmitted by the body for device pairing. We have built a prototype and conducted extensive experiments with 31 participants to evaluate its security and usability. The results demonstrate that our proposed mechanism offers a more secure and reliable option for user-specific pairing keys, contributing to the field of device pairing.}
}


@article{DBLP:journals/tmc/KangLLFC24,
	author = {Hong Kang and
                  Minghao Li and
                  Lehao Lin and
                  Sizheng Fan and
                  Wei Cai},
	title = {Bridging Incentives and Dependencies: An Iterative Combinatorial Auction
                  Approach to Dependency-Aware Offloading in Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12113--12130},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407958},
	doi = {10.1109/TMC.2024.3407958},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KangLLFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As mobile applications grow increasingly computation-intensive, the challenges arising from the limitations of mobile devices in terms of computing resources and battery life become more pronounced. Mobile Edge Computing (MEC) provides a promising avenue to address these challenges and enhance user experience. While existing studies have extensively explored resource allocation and task scheduling in MEC, most treat tasks as monolithic entities, overlooking the nuanced subtasks/components that often make up mobile applications. This paper endeavors to bridge the gap between the need for incentive mechanisms and the offloading of dependent computation tasks in MEC. Drawing inspiration from auction theory, we introduce a novel Multi-stage Iterative Combinatorial Double Auction (MICDA) mechanism, specifically tailored for dependent tasks in a cloud-edge-end cooperative computing scenario. Through theoretical analysis, the MICDA mechanisms demonstrate truthfulness, individual rationality, budget balance, and computational efficiency. Comprehensive experiment results further confirm its superior performance in improving application makespan and social welfare compared to other existing offloading strategies. This work validates the effective integration of dependency-aware computation offloading and auction mechanisms in overcoming economic and computational challenges in MEC systems, thereby paving the way for their potential application in broader real-world scenarios.}
}


@article{DBLP:journals/tmc/ZhouLZT24,
	author = {Tailin Zhou and
                  Zehong Lin and
                  Jun Zhang and
                  Danny H. K. Tsang},
	title = {Understanding and Improving Model Averaging in Federated Learning
                  on Heterogeneous Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12131--12145},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406554},
	doi = {10.1109/TMC.2024.3406554},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouLZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model averaging is a widely adopted technique in federated learning (FL) that aggregates multiple client models to obtain a global model. Remarkably, model averaging in FL yields a superior global model, even when client models are trained with non-convex objective functions and on heterogeneous local datasets. However, the rationale behind its success remains poorly understood. To shed light on this issue, we first visualize the loss landscape of FL over client and global models to illustrate their geometric properties. The visualization shows that the client models encompass the global model within a common basin, and interestingly, the global model may deviate from the basin's center while still outperforming the client models. To gain further insights into model averaging in FL, we decompose the expected loss of the global model into five factors related to the client models. Specifically, our analysis reveals that the global model loss after early training mainly arises from i) the client model's loss on non-overlapping data between client datasets and the global dataset and ii) the maximum distance between the global and client models. Based on the findings from our loss landscape visualization and loss decomposition, we propose utilizing iterative moving averaging (IMA) on the global model at the late training phase to reduce its deviation from the expected minimum, while constraining client exploration to limit the maximum distance between the global and client models. Our experiments demonstrate that incorporating IMA into existing FL methods significantly improves their accuracy and training speed on various heterogeneous data setups of benchmark datasets.}
}


@article{DBLP:journals/tmc/SunJJZGLD24,
	author = {Tong Sun and
                  Bowen Jiang and
                  Lewei Jin and
                  Wenzhao Zhang and
                  Yi Gao and
                  Zhendong Li and
                  Wei Dong},
	title = {Understanding Differencing Algorithms for Mobile Application Updates},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12146--12160},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407867},
	doi = {10.1109/TMC.2024.3407867},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunJJZGLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile application updates occur frequently, and they continue to add considerable traffic over the Internet. Differencing algorithms, which compute a small delta between the new version and the old version, are often employed to reduce the update overhead. Researchers have proposed many differencing algorithms over the years. Unfortunately, it is currently unknown how these algorithms quantitatively perform for different categories of applications. It is also challenging to know the impacts of different techniques and whether a technique in one algorithm can be integrated into another algorithm for further performance improvement. This paper conducts the first systematic study to understand the performance of four widely used differencing algorithms for mobile application updates, including xdelta3, bsdiff, archive-patcher, and HDiffPatch with respect to five key metrics, including compression ratio, differencing time/memory overhead, and reconstruction time/memory overhead. We perform measurements for 200 mobile applications, and analyze key techniques (such as decompressing-before-differencing, sliding window, and copy instructions merging) that influence the performance of these algorithms. We have provided four important findings which give insights to further optimize for performance improvement. Guided by these insights, we have also proposed a novel algorithm, sdiff, which achieves the smallest compression ratio to state-of-the-art algorithms by combining an appropriately chosen set of key techniques.}
}


@article{DBLP:journals/tmc/LiCLLL24,
	author = {Zhizhen Li and
                  Mingzhe Chen and
                  Gaolei Li and
                  Xi Lin and
                  Yuchen Liu},
	title = {Map-Driven mmWave Link Quality Prediction With Spatial-Temporal Mobility
                  Awareness},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12161--12175},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406728},
	doi = {10.1109/TMC.2024.3406728},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The susceptibility of millimeter-wave (mmWave) links to blockages poses challenges for maintaining consistent high-rate performance. By predicting link quality in advance at specific locations or times of interest, proactive resource allocation techniques, such as link-quality-aware scheduling, can be employed to optimize the utilization of network resources. In this paper, we introduce a map-driven link quality prediction framework that divides the problem into long-term and short-term link quality predictions to cater to the needs of mobile computing. The first stage aims to predict a long-term radio map considering static network characteristics. We propose to separate LoS and NLoS scenarios, and build an analytical model and a regression-based approach to construct a complete link quality map in the spatial domain. Next, short-term link quality prediction is explored to anticipate future variations in link quality through a spatial-temporal attention-based prediction framework. The essence of this approach lies in capturing the spatial correlation and temporal dependency of mmWave wireless characteristics, followed by an attention mechanism to complement the dynamic link quality prediction task. On top of that, we also design a regional training mechanism with a weighted loss function to address the classical data imbalance problem of map-driven prediction. Extensive experimental and simulation results show that our integrated framework effectively captures comprehensive spatial-temporal knowledge and achieves significantly higher accuracy than other baseline prediction methods, making it a promising solution for a wide range proactive configuration tasks in mobile mmWave networks.}
}


@article{DBLP:journals/tmc/ZuoGCGXJ24,
	author = {Yiping Zuo and
                  Linqing Gui and
                  Kaiyan Cui and
                  Jiajia Guo and
                  Fu Xiao and
                  Shi Jin},
	title = {Mobile Blockchain-Enabled Secure and Efficient Information Management
                  for Indoor Positioning With Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12176--12194},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407792},
	doi = {10.1109/TMC.2024.3407792},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZuoGCGXJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional indoor location information management methods based on centralized servers have problems such as safe and reliable transmission, personal privacy leaks, location information tampering, and computing and storage loads. These problems have seriously affected the development of personalized services based on indoor location information. In this paper, a novel mobile blockchain-enabled federated learning (MBFL) information management framework for indoor positioning is presented, comprising the mobile blockchain model, the federated learning (FL) model, and the InterPlanetary file storage model. Then, we design the MBFL algorithm, establishing a robust foundation for collaborative model training, efficient block mining, and secure data storage. Moreover, we derive training and mining latency as well as the individual user rewards, and formulate latency-limited resource allocation strategies as a non-cooperative game. We propose an efficient alternating iterative algorithm to achieve the Nash equilibrium of this game. Numerical results demonstrate that the proposed alternating iterative algorithm achieves rapid convergence and strikes an effective balance between economic and time efficiency. Furthermore, when confronted with model poisoning attacks, the MBFL algorithm exhibits superior security performance compared to the traditional FL algorithm. Future work will focus on adapting the MBFL framework for various indoor environments and enhancing consumption and computational efficiency with hybrid consensus mechanisms.}
}


@article{DBLP:journals/tmc/CuiXZZWBH24,
	author = {Jie Cui and
                  Jietian Xiao and
                  Hong Zhong and
                  Jing Zhang and
                  Lu Wei and
                  Irina Bolodurina and
                  Debiao He},
	title = {{LH-IDS:} Lightweight Hybrid Intrusion Detection System Based on Differential
                  Privacy in VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12195--12210},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407709},
	doi = {10.1109/TMC.2024.3407709},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CuiXZZWBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular Ad hoc Networks (VANETs) are vulnerable to various types of attacks. Intrusion Detection System (IDS) based on machine learning can effectively detect malicious network attacks in VANETs. However, machine learning training necessitates ample data which contain significant ample private information, increasing the risk of privacy disclosure. The privacy protection of training data for machine learning used in the IDS of VANETs is rarely investigated. Meanwhile, Differential Privacy (DP) is one of the most secure privacy protection methods based on perturbations. Therefore, we propose a lightweight hybrid IDS (LH-IDS) based on machine learning and DP. It uses algorithms based on unsupervised learning to detect anomalous network behaviour with high performance, especially unknown attacks in VANETs, while protecting data privacy. The DP is used to secure the privacy of the training data. Noise from different privacy budgets is added to datasets to obtain DP datasets. Subsequently, LH-IDS is used to verify the utility of the DP datasets. Extensive experiments confirm LH-IDS can not only detect anomalous and normal traffic with excellent performance but can also protect the private information of the training data. Additionally, the proposed model incurs only minimal CPU and memory overhead, making it a lightweight solution.}
}


@article{DBLP:journals/tmc/ShaoCG24,
	author = {Yulin Shao and
                  Qi Cao and
                  Deniz G{\"{u}}nd{\"{u}}z},
	title = {A Theory of Semantic Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12211--12228},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406375},
	doi = {10.1109/TMC.2024.3406375},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShaoCG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication is an emerging research area that has gained a wide range of attention recently. Despite this growing interest, there remains a notable absence of a comprehensive and widely-accepted framework for characterizing semantic communication. This paper introduces a new conceptualization of semantic communication and formulates two fundamental problems, which we term language exploitation and language design. Our contention is that the challenge of language design can be effectively situated within the broader framework of joint source-channel coding theory, underpinned by a comprehensive end-to-end distortion metric. To tackle the language exploitation problem, we put forth three approaches: semantic encoding, semantic decoding, and a synergistic combination of both in the form of combined semantic encoding and decoding. Furthermore, we establish the semantic distortion-cost region as a critical framework for assessing the language exploitation problem. For each of the three proposed approaches, the achievable distortion-cost region is characterized. Overall, this paper aims to shed light on the intricate dynamics of semantic communication, paving the way for a deeper understanding of this evolving field.}
}


@article{DBLP:journals/tmc/VuCPHND24,
	author = {Thai T. Vu and
                  Nam Hoai Chu and
                  Khoa Tran Phan and
                  Dinh Thai Hoang and
                  Diep N. Nguyen and
                  Eryk Dutkiewicz},
	title = {Energy-Based Proportional Fairness in Cooperative Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12229--12246},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406721},
	doi = {10.1109/TMC.2024.3406721},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/VuCPHND24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By executing offloaded tasks from mobile users, edge computing augments mobile devices with computing/communications resources from edge nodes (ENs), thus enabling new services/applications (e.g., real-time gaming, virtual/augmented reality). However, despite being more resourceful than mobile devices, allocating ENs’ computing/communications resources to a given favorable set of users (e.g., closer to edge nodes) may block other devices from their services. This is often the case for most existing task offloading and resource allocation approaches that only aim to maximize the network social welfare or minimize the total energy consumption but do not consider the computing/battery status of each mobile device. This work develops an energy-based proportionally fair task offloading and resource allocation framework for a multi-layer cooperative edge computing network to serve all user equipments (UEs) while considering both their service requirements and individual energy/battery levels. The resulting optimization involves both binary (offloading decisions) and continuous (resource allocation) variables. To tackle the NP-hard mixed integer optimization problem, we leverage the fact that the relaxed problem is convex and propose a distributed algorithm, namely the dynamic branch-and-bound Benders decomposition (DBBD). DBBD decomposes the original problem into a master problem (MP) for the offloading decisions and multiple subproblems (SPs) for resource allocation. To quickly eliminate inefficient offloading solutions, the MP is integrated with powerful Benders cuts exploiting the ENs’ resource constraints. We then develop a dynamic branch-and-bound algorithm (DBB) to efficiently solve the MP considering the load balance among ENs. The SPs can either be solved for their closed-form solutions or be solved in parallel at ENs, thus reducing the complexity. The numerical results show that the DBBD returns the optimal solution in maximizing the proportional fairness among UEs. The DBBD has higher fairness indexes, i.e., Jain's index and min-max ratio, in comparison with the existing ones that minimize the total consumed energy.}
}


@article{DBLP:journals/tmc/ZhuW24,
	author = {Botao Zhu and
                  Xianbin Wang},
	title = {Hypergraph-Aided Task-Resource Matching for Maximizing Value of Task
                  Completion in Collaborative IoT Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12247--12261},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3408425},
	doi = {10.1109/TMC.2024.3408425},
	timestamp = {Mon, 09 Dec 2024 22:46:24 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growing scale and intrinsic heterogeneity of Internet of Things (IoT) systems, distributed device collaboration becomes essential for effective task completion by dynamically utilizing limited communication and computing resources. However, the separated design and situation-agnostic operation of computing, communication and application layers create a fundamental challenge for rapid task-resource matching, which further deteriorate the overall task completion effectiveness. To overcome this challenge, we utilize hypergraph as a new tool to vertically unify computing, communication, and task aspects of IoT systems for an effective matching by accurately capturing the relationships between tasks and communication and computing resources. Specifically, a state-of-the-art task-resource matching hypergraph (TRM-hypergraph) model is proposed in this paper, which is used to effectively transform the process of allocating complex heterogeneous resources to convoluted tasks into a hypergraph matching problem. Taking into account computational complexity and storage, a game-theoretic hypergraph matching algorithm is proposed via considering the hypergraph matching problem as a non-cooperative multi-player clustering game. Numerical results demonstrate that the proposed TRM-hypergraph model achieves superior performance in matching of tasks and resources compared with comparison algorithms.}
}


@article{DBLP:journals/tmc/YangSYCKNS24,
	author = {Yuye Yang and
                  You Shi and
                  Changyan Yi and
                  Jun Cai and
                  Jiawen Kang and
                  Dusit Niyato and
                  Xuemin Shen},
	title = {Dynamic Human Digital Twin Deployment at the Edge for Task Execution:
                  {A} Two-Timescale Accuracy-Aware Online Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12262--12279},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406607},
	doi = {10.1109/TMC.2024.3406607},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangSYCKNS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human digital twin (HDT) is an emerging paradigm that bridges physical twins (PTs) with powerful virtual twins (VTs) for assisting complex task executions in human-centric services. In this paper, we study a two-timescale online optimization for building HDT under an end-edge-cloud collaborative framework. As a unique feature of HDT, we consider that PTs’ corresponding VTs are deployed on edge servers, consisting of not only generic models placed by downloading experiential knowledge from the cloud but also customized models updated by collecting personalized data from end devices. To maximize task execution accuracy with stringent energy and delay constraints, and by taking into account HDT's inherent mobility and status variation uncertainties, we jointly and dynamically optimize VTs’ construction and PTs’ task offloading, along with communication and computation resource allocations. Observing that decision variables are asynchronous with different triggers, we propose a novel two-timescale accuracy-aware online optimization approach (TACO). Specifically, TACO utilizes an improved Lyapunov method to decompose the problem into multiple instant ones, and then leverages piecewise McCormick envelopes and block coordinate descent based algorithms, addressing two timescales alternately. Theoretical analyses and simulations show that the proposed approach can reach asymptotic optimum within a polynomial-time complexity, and demonstrate its superiority over counterparts.}
}


@article{DBLP:journals/tmc/LiHLSLLW24,
	author = {Weihe Li and
                  Jiawei Huang and
                  Yu Liang and
                  Qichen Su and
                  Jingling Liu and
                  Wenjun Lyu and
                  Jianxin Wang},
	title = {Optimizing Video Streaming in Dynamic Networks: An Intelligent Adaptive
                  Bitrate Solution Considering Scene Intricacy and Data Budget},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12280--12297},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3406409},
	doi = {10.1109/TMC.2024.3406409},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiHLSLLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive Bitrate (ABR) algorithms have become increasingly important for delivering high-quality video content over fluctuating networks. Considering the complexity of video scenes, video chunks can be separated into two categories: those with intricate scenes and those with simple scenes. In practice, it has been observed that improving the quality of intricate chunks yields more substantial improvements in Quality of Experience (QoE) compared with focusing solely on simple chunks. However, the current ABR schemes either treat all chunks equally or rely on fixed linear-based reward functions, which limits their ability to meet real-world requirements. To tackle these limitations, this paper introduces a novel ABR approach called CAST (Complex-scene Aware bitrate algorithm via Self-play reinforcemenT learning), which considers the scene complexity and formulates the bitrate adaptation task as an explicit objective. Leveraging the power of parallel computing with multiple agents, CAST trains a neural network to achieve superior video playback quality for intricate scenes while minimizing playback freezing time. Moreover, we also introduce a new variant of our proposed approach called CAST-DU, to address the critical issue of efficiently managing users’ limited cellular data budgets while ensuring a satisfactory viewing experience. Furthermore, we present CAST-Live, tailored for live streaming scenarios with constrained playback buffers and considerations for energy costs. Extensive trace-driven evaluations and subjective tests demonstrate that CAST, CAST-DU, and CAST-Live outperform existing off-the-shelf schemes, delivering a superior video streaming experience over fluctuating networks while efficiently utilizing data resources. Moreover, CAST-Live demonstrates effectiveness even under limited buffer size constraints while incurring minimal energy costs.}
}


@article{DBLP:journals/tmc/HuangYZHFX24,
	author = {Yongming Huang and
                  Xiaohu You and
                  Hang Zhan and
                  Shiwen He and
                  Ningning Fu and
                  Wei Xu},
	title = {Learning Wireless Data Knowledge Graph for Green Intelligent Communications:
                  Methodology and Experiments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12298--12312},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3408142},
	doi = {10.1109/TMC.2024.3408142},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangYZHFX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Native artificial intelligence (AI) has played a pivotal role in shaping the evolution of 6G networks. It must meet stringent real-time requirements and therefore deploying lightweight AI models is necessary. However, as wireless networks generate a multitude of data fields and only a fraction of them imposes significant impact on the AI models, it is essential to accurately identify a small amount of critical data that significantly impacts communication performance. In this paper, we propose the pervasive multi-level (PML) native AI architecture, which incorporates knowledge graph (KG) into mobile network operations to establish a wireless data KG. Leveraging the wireless data KG, we analyze the relationships among various data fields and provide the on-demand generation of minimal and effective datasets, referred to as feature datasets. Consequently, it not only enhances AI training, inference, and validation processes but also significantly reduces resource wastage and overhead for communication networks. The proposed solution includes a spatio-temporal heterogeneous graph attention neural network model (STREAM) and a feature dataset generation algorithm. Experimental results validate the exceptional capability of STREAM in handling spatio-temporal data and demonstrate that the proposed architecture effectively reduces data scale and computational costs of AI training by almost an order of magnitude.}
}


@article{DBLP:journals/tmc/DuLSRLDY24,
	author = {Yicong Du and
                  Hongbo Liu and
                  Ziyu Shao and
                  Yanzhi Ren and
                  Shuai Li and
                  Huan Dai and
                  Jiadi Yu},
	title = {Secure and Controllable Secret Key Generation Through {CSI} Obfuscation
                  Matrix Encapsulation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12313--12329},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407062},
	doi = {10.1109/TMC.2024.3407062},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DuLSRLDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Physical-layer key generation has emerged as a promising avenue for establishing secret keys using reciprocal channel measurements between wireless devices. However, channel reciprocity may suffer degradation from ambient noise and cause mismatched secret bits, while existing methods mitigating this issue may yet face limitations in key efficiency. The root cause behind such limitations is the heavy reliance on channel measurements, which can be naturally susceptible to channel non-reciprocity attributed to environmental factors. Instead of direct key extraction from channel measurements, we seek to share a pre-defined key and utilize channel measurements as a bearer to facilitate key transmission. We propose an accurate and efficient key generation method (KeyCome) to ensure secure key sharing by encapsulating it with channel state information (CSI) obfuscation matrices through circulant convolution. To this end, we develop a reliable key derivation through a quadratic programming method with matrix equilibration, ensuring stable and rapid solutions. Notably, the transmitter can control the key beforehand for enhanced communication efficiency and combine it with an error correction mechanism for accurate key derivation. Furthermore, a lightweight reconciliation scheme is designed to minimize mismatched bits caused by occasional non-reciprocity. Comprehensive experiments demonstrate KeyCome's high accuracy and efficiency in key generation.}
}


@article{DBLP:journals/tmc/EHHXLCL24,
	author = {Jinlong E and
                  Fangshuo Han and
                  Lin He and
                  Wei Xu and
                  Zhenhua Li and
                  Yunpeng Chai and
                  Yunhao Liu},
	title = {WiseCam: {A} Systematic Approach to Intelligent Pan-Tilt Cameras for
                  Moving Object Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12330--12344},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3410645},
	doi = {10.1109/TMC.2024.3410645},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/EHHXLCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the desired functionality of moving object tracking, wireless pan-tilt cameras are able to play critical roles in a growing diversity of surveillance environments. However, today's pan-tilt cameras oftentimes underperform when tracking frequently moving objects like humans – they are prone to lose sight of objects and bring about excessive mechanical rotations that are especially detrimental to those energy-constrained outdoor scenarios. The ineffectiveness and high cost of all state-of-the-art tracking approaches are rooted in their adherence to the industry's simplicity principle, which leads to their stateless nature, performing gimbal rotations based only on the latest object detection. To address the issues, we design and implement WiseCam that wisely tunes the pan-tilt cameras to minimize mechanical rotation costs while maintaining long-term object tracking. This systematic tracking approach also tackles issues of motion-rotation speed gap and scattered moving objects, which is universally applicable to complex tracking scenarios. We examine the performance of WiseCam by experiments on two types of pan-tilt cameras with different motors. Results show that it significantly outperforms the state-of-the-art tracking approaches on both tracking duration and power consumption.}
}


@article{DBLP:journals/tmc/ValiantiMKE24,
	author = {Panayiota Valianti and
                  Kleanthis Malialis and
                  Panayiotis Kolios and
                  Georgios Ellinas},
	title = {Cooperative Multi-Agent Jamming of Multiple Rogue Drones Using Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12345--12359},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3409050},
	doi = {10.1109/TMC.2024.3409050},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ValiantiMKE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The wide adoption and use of unmanned aerial vehicles (UAVs) has created not only opportunities but also threats to the security of sensitive areas. Thus, effective and efficient counter-drone systems are required to protect these areas. This work tackles this issue by developing cooperative multi-agent jamming techniques using reinforcement learning (RL) to counter the operation of one or multiple rogue drones flying over a sensitive area. The aim of the proposed RL approach is to optimize the joint mobility and power control actions of the pursuer UAVs in order to maximize the received jamming power at the rogue drones aiming at disrupting communication links and sensing circuitry, while at the same time keeping the interference to surrounding pursuer agents below a predefined threshold. The effectiveness of the proposed approach in terms of scalability, learning speed, and agents’ final joint performance is demonstrated through extensive simulation experiments for various agent and target configurations.}
}


@article{DBLP:journals/tmc/WangZHX24,
	author = {Yiqian Wang and
                  Jie Zhu and
                  Haiping Huang and
                  Fu Xiao},
	title = {Bi-Objective Ant Colony Optimization for Trajectory Planning and Task
                  Offloading in UAV-Assisted {MEC} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12360--12377},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3408603},
	doi = {10.1109/TMC.2024.3408603},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the paper, the Unmanned Aerial Vehicle (UAV) path planning and task offloading problem in UAV-assisted mobile edge computing (MEC) systems is investigated. A bi-criterion ant colony optimization (bi-ACO) framework is proposed for the considered problem with the objectives of minimizing the total cost and the completion time, meanwhile satisfying the energy, deadline, location, and priority constraints. In the bi-ACO framework, multiple heterogeneous colonies are introduced with different preferences of objectives. Each colony maintains five pairs of pheromone matrices for constructing feasible solutions. Besides the colony settings, three key components of bi-ACO are delicately designed: feasible solution generation method (FSGM) to construct a feasible solution, solution division method (SDM) to improve obtained solutions of good quality, and pheromone update method (PUM) to updates pheromone matrices by pheromone evaporation operation and pheromone enhancement operation based on the preferences of colonies. Four Pareto-based metrics are introduced to evaluate the performance of the compared algorithms. Experimental results show that the proposal outperforms the compared baseline algorithms in effectiveness and robustness.}
}


@article{DBLP:journals/tmc/GuoCLLLXZ24,
	author = {Suiming Guo and
                  Chao Chen and
                  Zhetao Li and
                  Chengwu Liao and
                  Yaxiao Liu and
                  Ke Xu and
                  Daqing Zhang},
	title = {Privacy Leakage From Dynamic Prices: Trip Purpose Mining as an Example},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12378--12395},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3408419},
	doi = {10.1109/TMC.2024.3408419},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoCLLLXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic prices are used in many scenarios, e.g., flight ticketing, hotel room booking and ride-on-demand (RoD) service such as Uber and DiDi, and while they are beneficial for service providers, practitioners or users, they lead to the concern of privacy leakage – the possibility of learning user information from dynamic prices. In this paper, we aim to study this possibility and choose trip purpose mining in RoD service as an attack example, based on real-world large datasets. We discuss the criteria of choosing datasets – ubiquitous, collective and easily accessible – from the perspective of an attacker, and extract features describing trip information, spatio-temporal and dynamic prices context. The trip purpose mining problem is then solved as a multi-class classification problem and multiple binary-class problems. In the multi-class problem, we verify that dynamic prices information results in a 17.1% improvement in classification accuracy; in the binary-class problems, we quantify feature contributions and explain the different extents of privacy leakage in identifying different trip purposes. Our hope is that the study not only serves as a case study demonstrating the privacy leakage problem in RoD service, but also sheds light on such privacy problem in other services using dynamic prices and triggers more research efforts.}
}


@article{DBLP:journals/tmc/ZhangCWWL24,
	author = {Rongyu Zhang and
                  Yun Chen and
                  Chenrui Wu and
                  Fangxin Wang and
                  Bo Li},
	title = {Multi-Level Personalized Federated Learning on Heterogeneous and Long-Tailed
                  Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12396--12409},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3409159},
	doi = {10.1109/TMC.2024.3409159},
	timestamp = {Wed, 19 Feb 2025 12:54:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangCWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) offers a privacy-centric distributed learning framework, enabling model training on individual clients and central aggregation without necessitating data exchange. Nonetheless, FL implementations often suffer from non-i.i.d. and long-tailed class distributions across mobile applications, e.g., autonomous vehicles, which leads models to overfitting as local training may converge to sub-optimal. In our study, we explore the impact of data heterogeneity on model bias and introduce an innovative personalized FL framework, Multi-level Personalized Federated Learning (MuPFL), which leverages the hierarchical architecture of FL to fully harness computational resources at various levels. This framework integrates three pivotal modules: Biased Activation Value Dropout (BAVD) to mitigate overfitting and accelerate training; Adaptive Cluster-based Model Update (ACMU) to refine local models ensuring coherent global aggregation; and Prior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster classification and personalize models in accord with skewed local data with shared knowledge. Extensive experiments on diverse real-world datasets for image classification and semantic segmentation validate that MuPFL consistently outperforms state-of-the-art baselines, even under extreme non-i.i.d. and long-tail conditions, which enhances accuracy by as much as 7.39% and accelerates training by up to 80% at most, marking significant advancements in both efficiency and effectiveness.}
}


@article{DBLP:journals/tmc/ParkCPJCCK24,
	author = {Soohyun Park and
                  Jaehyun Chung and
                  Chanyoung Park and
                  Soyi Jung and
                  Minseok Choi and
                  Sungrae Cho and
                  Joongheon Kim},
	title = {Joint Quantum Reinforcement Learning and Stabilized Control for Spatio-Temporal
                  Coordination in Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12410--12427},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3407883},
	doi = {10.1109/TMC.2024.3407883},
	timestamp = {Mon, 03 Feb 2025 08:09:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ParkCPJCCK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to build realistic metaverse systems, enabling high synchronization between physical-space and virtual meta-space is essentially required. For this purpose, this paper proposes a novel system-wide coordination algorithm for high synchronization under characteristics (i.e., highly realistic meta-space construction under the constraints of physical-space). The proposed algorithm consists of the following three stages. The first stage is quantum multi-agent reinforcement learning (QMARL)-based scheduling for low-delay temporal-synchronization using differentiated age-of-information (AoI) during data gathering in physical-space by observers for meta-space construction. This is beneficial for scalability according to action dimension reduction in reinforcement learning computation. The second stage is for creating virtual contents under delay constraints in meta-space based on the gathered data. When rendering regions that have received more user attention, avatar-popularity is considered for spatio-synchronization. Thus, a stabilized control mechanism is designed for time-average reality quality maximization for each region. The last stage is for caching based on avatar-popularity and AoI which can be helpful in constructing low-delay realistic meta-space. Furthermore, the concept of AoI is divided into two separate sub-concepts of physical AoI and virtual AoI such that the AoI in virtual meta-space can be thoroughly implemented.}
}


@article{DBLP:journals/tmc/GuoSLWYH24,
	author = {Jiani Guo and
                  Shanshan Song and
                  Jun Liu and
                  Lei Wan and
                  Yang Yu and
                  Guangjie Han},
	title = {An MC-CDMA-Based {MAC} Protocol for Efficient Concurrent Communication
                  in Mobile Underwater Acoustic Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12428--12443},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3409562},
	doi = {10.1109/TMC.2024.3409562},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoSLWYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Underwater Acoustic Networks (UANs) leverage Autonomous Underwater Vehicles (AUVs) to enhance flexibility and mobility, playing an essential role in ocean research. Similar to static UANs, the Medium Access Control (MAC) protocol is still critical for mobile UANs to achieve efficient communication. However, mobile UANs are delay-sensitive and suffer from low Signal-to-Noise Ratio (SNR), which presents significant challenges for the design of MAC protocols. As a hybrid technology combining spread spectrum and multi-carrier modulation, Multi-Carrier Code Division Multiple Access (MC-CDMA) offers simple multi-path channel equalization and flexible multi-user access, aiding the MAC protocol in achieving robust communication in mobile UANs. Along this line, we propose an MC-CDMA-based MAC (MC-MAC) protocol, which considers both characteristics of mobile UANs and MC-CDMA to achieve efficient concurrent communication. Specifically, to adequately utilize the limited underwater communication resources, we design an adaptive node clustering algorithm, classifying nodes based on propagation distance, relative mobile velocity, data size, and data grade. Meanwhile, the algorithm determines non-random initial center nodes and adaptively decides the optimal number of clusters to decrease the computational complexity. Based on the clustering results, we present a many-objective optimization algorithm, which jointly allocates specific spreading code length, spreading code number, subcarrier range, and transmission power to optimize throughput, delay, and energy consumption in mobile UANs. Extensive simulation results demonstrate that MC-MAC fully leverages the advantages of MC-CDMA, providing efficient concurrent communication with lower energy consumption for mobile UANs compared to state-of-the-art protocols.}
}


@article{DBLP:journals/tmc/TaoCZWYCD24,
	author = {Youming Tao and
                  Shuzhen Chen and
                  Congwei Zhang and
                  Di Wang and
                  Dongxiao Yu and
                  Xiuzhen Cheng and
                  Falko Dressler},
	title = {Private Over-the-Air Federated Learning at Band-Limited Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12444--12460},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3411295},
	doi = {10.1109/TMC.2024.3411295},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TaoCZWYCD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate over-the-air federated learning (OTA-FL) that exploits over-the-air computing (AirComp) to integrate communication and computation seamlessly for FL. Privacy presents a serious obstacle for OTA-FL, as it can be compromised by maliciously manipulating channel state information (CSI). Moreover, the limited band at edge hinders OTA-FL from training large-scale models. It remains open how to enable a multitude of devices with constrained resources and sensitive data to collaboratively train a global model at band-limited edge. To tackle this, we design a novel algorithm PROBE building upon a lightweight over-the-air gradients aggregation rule PB-O-GAR. Specifically, PB-O-GAR combines a random sparsification-like dimension reduction with Gaussian perturbation to provide rigorous privacy and band-adapted communication. It elaborately calibrates the transmission signal according to devices’ perceived CSI for heterogeneous power constraints accommodation and CSI attack resilience. We show that by utilizing the common randomness, which deviates from the conventional FL, random sparsification-like dimension reduction can augment privacy in addition to the intrinsic privacy amplification effect of AirComp. We establish near-optimal convergence rates and explicit trade-offs among privacy, communication and utility for PROBE. Finally, extensive experiments on benchmark datasets are conducted to validate our theoretical findings and showcase the superiority of PROBE in realistic settings.}
}


@article{DBLP:journals/tmc/WangLLJDL24,
	author = {Ping Wang and
                  Saiqin Long and
                  Haolin Liu and
                  Kun Jiang and
                  Qingyong Deng and
                  Zhetao Li},
	title = {Propagation Verification Under Social Relationship Privacy Awareness
                  in Mobile Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12461--12476},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3412214},
	doi = {10.1109/TMC.2024.3412214},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLLJDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsourcing aims to recruit enough workers holding mobile devices to collect data. Nevertheless, the platform will have cold start problems when the number of workers is limited. Existing studies have proposed solving this problem by propagating tasks to social networks for social recruitment. However, they neglect to verify workers’ propagation, leading to malicious workers reducing the platform's utility. Furthermore, during propagation verification, it is imperative to protect the privacy of social relationships among workers, as it can significantly influence the propagation. Therefore, this paper proposes Zero-knowledge Propagation Verification based on Social Relationship Encryption (ZPV-SRE) to improve the platform's utility. Specifically, we transform the propagation verification problem into a problem of computing the solution of the function. Then, the Zero-knowledge proof is used to prove the propagation, in which the worker's social relationship is protected through homomorphic encryption. Considering that ZPV-SRE will incur a significant time cost, we propose Trust-guided Zero-knowledge Propagation Verification based on Social Relationship Encryption (TZPV-SRE), which updates the worker's trust based on the verification results and selects suspicious workers for verification. The experimental results show ZPV-SRE improves the platform's utility as high as 104.05% over the state-of-the-art methods, while TZPV-SRE reduces time costs and ensures improvement.}
}


@article{DBLP:journals/tmc/ZhangZDWZC24,
	author = {Ganlin Zhang and
                  Dongheng Zhang and
                  Hongyu Deng and
                  Yun Wu and
                  Fengquan Zhan and
                  Yan Chen},
	title = {Practical Passive Indoor Localization With Intelligent Reflecting
                  Surface},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12477--12490},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3414861},
	doi = {10.1109/TMC.2024.3414861},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZDWZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface has gained significant attention for supporting integrated sensing and communication (ISAC) by manipulating wireless signals. However, existing IRS-based sensing systems commonly utilize directional horn antennas for signal transmission, which is incompatible with commercial WiFi devices and contradicts the concept of ISAC. In this paper, we propose an IRS-aided localization system with omnidirectional antennas to overcome these limitations. We reveal two critical challenges for implementing such a system. First, precise distance between the IRS and transmitter is requisite for IRS codebook design, yet practical measurement introduces centimeter-scale biases, causing severe localization errors. We derive a linear relationship between measurement bias and localization error, which serves as the basis for a calibration method. Second, in practice, we can only obtain the relative phase change of the IRS under different bias voltages, but cannot measure the phase offset introduced by IRS at zero voltage. Consequently, an unknown fixed offset appears when calculating the channel response of the signal reflected by the IRS, which destroys subsequent IRS codebook design. We resolve this problem by eliminating the signals that are not related to the IRS. Extensive experiments validate the proposed calibration techniques and demonstrate that our system achieves accurate passive localization.}
}


@article{DBLP:journals/tmc/KatsarosFTN24,
	author = {George N. Katsaros and
                  Marcin Filo and
                  Rahim Tafazolli and
                  Konstantinos Nikitopoulos},
	title = {MIMO-SoftiPHY: {A} Software-Based {PHY} Design and Implementation
                  Framework for Highly-Efficient Open-RAN {MIMO} Radios},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12491--12504},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3411788},
	doi = {10.1109/TMC.2024.3411788},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KatsarosFTN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open Radio Access Networks (Open-RAN) trigger a shift from conventional monolithic RAN architectures to disaggregated designs with open interfaces, diversifying the 5G supply chain and boosting innovation. It is envisaged that Open-RAN deployments will be heavily software-based, allowing for higher flexibility and faster integration of new features. However, existing software-based solutions, seem to be unable to realize practical and standard-compliant Multiple-Input, Multiple-Output (MIMO) designs with a large number of concurrently transmitted information streams, as the 5G New Radio standard requires in order to substantially improve connectivity and throughput. In this context, we introduce MIMO-SoftiPHY, the first 3GPP and Open-RAN compliant, software-based physical layer (PHY) design and implementation framework that can practically realize MIMO designs with large numbers of information streams in a power-efficient manner. MIMO-SoftiPHY is inherently integrated with OpenAirInterface, enabling practical, software-based MIMO deployments with commercial-off-the-shelf user equipment. Specifically, MIMO-SoftiPHY achieves real-time performance for 12 MU-MIMO streams at a 10 MHz bandwidth and 8 streams at a 20 MHz. In addition, and in contrast to existing designs, MIMO-SoftiPHY can also support non-linear base-station processing in real-time that, as we show, can result in substantial power savings at the radio side, by obviating the need for a “massive” number of base-station antennas.}
}


@article{DBLP:journals/tmc/WeiZCZBH24,
	author = {Lu Wei and
                  Yongjuan Zhang and
                  Jie Cui and
                  Hong Zhong and
                  Irina Bolodurina and
                  Debiao He},
	title = {A Threshold-Based Full-Decentralized Authentication and Key Agreement
                  Scheme for VANETs Powered by Consortium Blockchain},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12505--12521},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3412106},
	doi = {10.1109/TMC.2024.3412106},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WeiZCZBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The authentication and key agreement (AKA) scheme for VANETs can produce a series of short-term session keys, which can be used to secure the vehicular communications across open and insecure wireless channels. Traditional VANETs AKA schemes tend to employ the centralized trust architecture as the core authentication backend, which raises concerns about system security and reliability. Recently, several VANETs AKA schemes that are constructed on decentralized trust architecture have been proposed. However, these schemes do not achieve full decentralization and tend to suffer from key exposure issues, insufficient performance, and lack of optimization for on-chain storage costs. To address these shortcomings, we propose a threshold-based full-decentralized VANETs AKA scheme that is powered by consortium blockchain. In our proposed scheme, the threshold-based voting concept is employed to mitigate the key exposure issue inherent to the network infrastructure. Furthermore, we leverage lightweight cryptography in conjunction with the Cuckoo filter to reduce computational, communication, and on-chain operation costs brought by cryptographic operations and smart contracts. The security proof together with the cryptographic protocol validation tool prove the security of our proposed scheme, whereas the simulation experiment demonstrates the efficiency of our proposed scheme.}
}


@article{DBLP:journals/tmc/QiLWLGJJ24,
	author = {Houyi Qi and
                  Minghui Liwang and
                  Xianbin Wang and
                  Li Li and
                  Wei Gong and
                  Jian Jin and
                  Zhenzhen Jiao},
	title = {Bridge the Present and Future: {A} Cross-Layer Matching Game in Dynamic
                  Cloud-Aided Mobile Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12522--12539},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3412751},
	doi = {10.1109/TMC.2024.3412751},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QiLWLGJJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud-aided mobile edge networks (CAMENs) allow edge servers (ESs) to purchase resources from remote cloud servers (CSs), while overcoming resource shortage when handling computation-intensive tasks of mobile users (MUs). Conventional trading mechanisms (e.g., onsite trading) confront many challenges, including decision-making overhead (e.g., latency) and potential trading failures. This paper investigates a series of cross-layer matching mechanisms to achieve stable and cost-effective resource provisioning across different layers (i.e., MUs, ESs, CSs), seamlessly integrated into a novel hybrid paradigm that incorporates futures and spot trading. In futures trading, we explore an overbooking-driven aforehand cross-layer matching (OA-CLM) mechanism, facilitating two future contract types: contract between MUs and ESs, and contract between ESs and CSs, while assessing potential risks under historical statistical analysis. In spot trading, we design two backup plans respond to current network/market conditions: determination on contractual MUs that should switch to local processing from edge/cloud services; and an onsite cross-layer matching (OS-CLM) mechanism that engages participants in real-time practical transactions. We next show that our matching mechanisms theoretically satisfy stability, individual rationality, competitive equilibrium, and weak Pareto optimality. Comprehensive simulations in real-world and numerical network settings confirm the corresponding efficacy, while revealing remarkable improvements in time/energy efficiency and social welfare.}
}


@article{DBLP:journals/tmc/ManjangZSTZ24,
	author = {Ousman Manjang and
                  Yanlong Zhai and
                  Jun Shen and
                  Jude Tchaye{-}Kondi and
                  Liehuang Zhu},
	title = {Anchor Model-Based Hybrid Hierarchical Federated Learning With Overlap
                  {SGD}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12540--12557},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3414999},
	doi = {10.1109/TMC.2024.3414999},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ManjangZSTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed machine learning framework where multiple clients collaboratively train a model without sharing their data. Despite advancements, traditional FL methods encounter challenges including communication overhead, extended latency, and slow convergence. To address these issues, this paper introduces Anchor-HHFL, a novel approach that combines the strengths of synchronous and asynchronous FL. Anchor-HHFL employs multi-tier edge servers which conduct partial model aggregation and reduce the frequency of communication with the central server. Anchor-HHFL implements a novel divergence control method through hierarchical pullback. It orchestrates the sequence of each client's stochastic gradient descent (SGD) updates to pull the locally trained models towards an anchor model, ensuring alignment and minimizing divergence. Simultaneously, a secondary process collects client models without disrupting their ongoing local computations and transmits them to edge servers, thereby overlapping computation with communication, substantially enhancing the training speed. Additionally, to effectively handle asynchronous updates across clusters, Anchor-HHFL uses a heuristic weight assignment for global aggregation, weighting clients’ updates based on the degree of their divergence from the global model. Extensive experiments on MNIST and CIFAR-10 datasets demonstrate Anchor-HHFL's superiority, achieving up to 3 \\times\nfaster convergence and higher test accuracy compared to the baselines.}
}


@article{DBLP:journals/tmc/XiangRCLJ24,
	author = {Shuzhen Xiang and
                  Huigui Rong and
                  Jianguo Chen and
                  Daibo Liu and
                  Hongbo Jiang},
	title = {{TSBG:} {A} Two-Stage Stackelberg Game Algorithm for QoE-Awareness
                  Video Streaming Transmission},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12558--12571},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3412860},
	doi = {10.1109/TMC.2024.3412860},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiangRCLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic Adaptive Streaming over HTTP (DASH) stands as a leading streaming technology embraced by major video platforms and smart TV manufacturers worldwide. Despite its widespread use, the inherent diversity in both the video content and the client devices poses challenges, hindering DASH from consistently delivering top-notch playback quality for all users. This oversight often leads to network congestion, compromising the playback quality for users. To tackle these issues, we propose a Two-stage Stackelberg Game (TSBG) algorithm for personalized video streaming transmission in Edge Computing (EC) environments. The TSBG algorithm aims to optimize the Quality of Experience (QoE) of users by tailoring video streaming services between EC servers and clients. Initially, we establish the system model and define the video stream transmission problem as a multi-objective optimization problem, balancing server downlink resource scheduling and client adaptive bit rate. Subsequently, we design the TSBG algorithm, where an edge server allocation mechanism is adopted in the first stage to maximize overall user QoE, while users adjust their video bit rates based on the edge server's distribution plan to enhance their individual QoE in the second stage. We prove the existence and uniqueness of the equilibrium solution of the two-stage Starkelberg game and design an optimal pricing algorithm to maximize the benefits of edge servers. Extensive simulation experiments validate the effectiveness of the TSBG algorithm, showcasing its superiority in achieving enhanced QoE, network efficiency, and fairness compared to alternative approaches.}
}


@article{DBLP:journals/tmc/TchayeKondiZSTZ24,
	author = {Jude Tchaye{-}Kondi and
                  Yanlong Zhai and
                  Jun Shen and
                  Akbar Telikani and
                  Liehuang Zhu},
	title = {Adaptive Period Control for Communication Efficient and Fast Convergent
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12572--12586},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3416312},
	doi = {10.1109/TMC.2024.3416312},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TchayeKondiZSTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning is particularly challenging in IoT environments, where edge and cloud nodes have imbalanced computation capacity and networking bandwidth. The main scalability barrier in distributed stochastic gradient descent-based machine learning frameworks is the communication overhead from frequent model parameter exchanges between workers and the central server. One way to reduce this overhead is by employing constant and periodic averaging, which sends model parameters to the server after a few iterations of local updates from workers. However, investigations have shown that the optimal communication period for balancing communication and convergence is not constant. Although some studies have explored the effectiveness of federated learning with a constant period, dynamically adjusting the period for optimal convergence remains under-explored. To address this, we investigate the impact of the period on global model convergence and propose an adaptive period control mechanism (AdaPC). This mechanism adaptively adjusts the aggregation period of the federated learning framework to achieve fast convergence with minimal communication. Our theoretical and empirical findings demonstrate that our proposed solution achieves faster convergence, lower final training loss, and minimized communication overhead compared to the constant period averaging strategy and other existing solutions.}
}


@article{DBLP:journals/tmc/LiuXQOJ24,
	author = {Yu Liu and
                  Zejun Xu and
                  Zheng Qin and
                  Lu Ou and
                  Wenqiang Jin},
	title = {TouchAccess: Unlock IoT Devices on Touching by Leveraging Human-Induced
                  {EM} Emanations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12587--12602},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3414992},
	doi = {10.1109/TMC.2024.3414992},
	timestamp = {Mon, 02 Dec 2024 08:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuXQOJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) devices play essential roles in both industry and daily scenarios. However, unlike smartphones and computers, IoT devices typically lack conventional user interfaces (UIs) such as keyboards and touchscreens. It renders the traditional user authentication designs, e.g., PINs and patterns, inapplicable. In this article, we propose TouchAccess that enables users to unlock an arbitrary IoT device by applying a simple touch. Our design is motivated by the key observation that IoT devices unavoidably generate electromagnetic emanations (EMM) while they are functioning. When the user touches the device, it causes time-varying coupling between those two and generates unique EMMs. Our feasibility studies further reveal that these human-induced EMMs are distinct and strongly correlated with the circuitry properties of the user and the device, but are susceptible to environmental EM noises, thus lowering the authentication accuracy. To address this challenge, we develop signal processing techniques with a Siamese network learning scheme that clears the ambient electromagnetic (EM) noises, extracts robust signal features, and builds noise-resistant classifiers, enabling users to be correctly recognized. A significant advantage of TouchAccess is that it requires only a low-cost analog-to-digital converter (ADC) to sense the EM signal. We implement TouchAccess on commercial off-the-shelf (COTS) IoT devices, which vary significantly in terms of UIs, sizes, and hardware designs. The performance evaluations show that TouchAccess achieves an average authentication accuracy as high as 97.85%.}
}


@article{DBLP:journals/tmc/HuangWLC24,
	author = {Guangjing Huang and
                  Qiong Wu and
                  Jingyi Li and
                  Xu Chen},
	title = {{IMFL-AIGC:} Incentive Mechanism Design for Federated Learning Empowered
                  by Artificial Intelligence Generated Content},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12603--12620},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3414138},
	doi = {10.1109/TMC.2024.3414138},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a promising paradigm that enables clients to collaboratively train a shared global model without uploading their local data. To alleviate the heterogeneous data quality among clients, artificial intelligence-generated content (AIGC) can be leveraged as a novel data synthesis technique for FL model performance enhancement. Due to various costs incurred by AIGC-empowered FL (e.g., costs of local model computation and data synthesis), however, clients are usually reluctant to participate in FL without adequate economic incentives, which leads to an unexplored critical issue for enabling AIGC-empowered FL. To fill this gap, we first devise a data quality assessment method for data samples generated by AIGC and rigorously analyze the convergence performance of FL model trained using a blend of authentic and AI-generated data samples. We then propose a data quality-aware incentive mechanism to encourage clients’ participation. In light of information asymmetry incurred by clients’ private multi-dimensional attributes, we investigate clients’ behavior patterns and derive the server's optimal incentive strategies to minimize server's cost in terms of both model accuracy loss and incentive payments for both complete and incomplete information scenarios. Numerical results demonstrate that our proposed mechanism exhibits highest training accuracy and reduces up to 53.34% of the server's cost with real-world datasets, compared with existing benchmark mechanisms.}
}


@article{DBLP:journals/tmc/SunZPLWHCW24,
	author = {Sheng Sun and
                  Zengqi Zhang and
                  Quyang Pan and
                  Min Liu and
                  Yuwei Wang and
                  Tianliu He and
                  Yali Chen and
                  Zhiyuan Wu},
	title = {Staleness-Controlled Asynchronous Federated Learning: Accuracy and
                  Efficiency Tradeoff},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12621--12634},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3416216},
	doi = {10.1109/TMC.2024.3416216},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunZPLWHCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is an emerging distributed learning paradigm with the privacy-preserving advantage of collaboratively training a shared model across multiple participants. Considering the prevailing device heterogeneity circumstance in practice, asynchronous interaction is introduced into FL to break the straggler barrier of synchronization, at the cost of significant accuracy degradation derived from model staleness. Although quite a few works attempt to partially mitigate the detrimental impact after occurring staleness issue, they neglect to control the overall staleness degree of clients-side local models from the whole training perspective, resulting in highly-stale models for aggregation and slow convergence speed. To this end, we propose a Staleness-Controlled Asynchronous Federated Learning (SC-AFL) method, which enables to restrict staleness degree of local models within a certain bound via dynamically tuning the aggregated strategy of each round, aiming to strike a good balance between accuracy guarantee and convergence acceleration. Specifically, we leverage the Lyapunov optimization framework to decouple the troublesome round-coupling problem into the single-round sequential solving problem, and further develop a deterministic algorithm that selects the aggregated number of clients to minimize training time under the constraint of maintaining staleness queue stability. Besides, we derive the theoretical convergence analysis of SC-AFL and also present the upper bound of the performance gap with the optimum. Extensive experiments on three datasets demonstrate the superiority of SC-AFL in terms of time-to-accuracy speedup on both IID and Non-IID data distributions, achieving a good balance between model accuracy and convergence efficiency in AFL system.}
}


@article{DBLP:journals/tmc/HuDWPZSR24,
	author = {Jiahui Hu and
                  Jiacheng Du and
                  Zhibo Wang and
                  Xiaoyi Pang and
                  Yajie Zhou and
                  Peng Sun and
                  Kui Ren},
	title = {Does Differential Privacy Really Protect Federated Learning From Gradient
                  Leakage Attacks?},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12635--12649},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417930},
	doi = {10.1109/TMC.2024.3417930},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuDWPZSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is susceptible to the gradient leakage attack (GLA), which can recover local private training data from the shared gradients or model updates. To ensure privacy, differential privacy is applied in FL by clipping and adding noise to local gradients (i.e., Local Differential Privacy (LDP)) or the global model update (i.e., Central Differential Privacy (CDP)). However, the effectiveness of DP in defending GLAs needs to be thoroughly investigated since some works briefly verify that DP can guard FL against GLAs while others question its defense capability. In this paper, we empirically evaluate CDP and LDP on the resistance of GLAs, and pay close attention to the trade-offs between privacy and utility in FL. Our findings reveal that: 1) existing GLAs can be defended by CDP using a per-layer clipping strategy and LDP with a reasonable privacy guarantee and 2) both CDP and LDP ensure the trade-off between privacy and utility in training shallow model, but cannot guarantee this trade-off in deeper model training (e.g., ResNets). Triggered by the crucial role of clipping operation for DP, we propose an improved attack that incorporates the clipping operation into existing GLAs without requiring additional information. The experimental results show our attack can destruct the protection of CDP and weaken the effectiveness of LDP. Overall, our work validates the effectiveness as well as reveals the vulnerability of DP under GLAs. We hope this work can provide guidance on utilizing DP for defending against GLA in FL and inspire the design of future privacy-preserving FL.}
}


@article{DBLP:journals/tmc/WangJHZJLL24,
	author = {Penghao Wang and
                  Ruobing Jiang and
                  Jingyang Hu and
                  Yanmin Zhu and
                  Hongbo Jiang and
                  Minglu Li and
                  Chao Liu},
	title = {AMT{\textdollar}{\^{}}+{\textdollar}+: Acoustic Multi-Target Tracking
                  With Smartphone {MIMO} System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12650--12665},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417474},
	doi = {10.1109/TMC.2024.3417474},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangJHZJLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic target tracking has shown great advantages for device-free human-machine interaction over vision/RF-based mechanisms. However, existing approaches for portable devices solely track a single target, incapable of the ubiquitous and highly challenging multi-target situations such as double-hand multimedia controlling and multi-player gaming. In this paper, we propose AMT^+, a pioneering smartphone MIMO system to achieve centimeter-level multi-target tracking. The challenge of multi-target occlusion is effectively addressed by employing multiple speaker-microphone pairs. However, the unique challenge raised by MIMO is the superposition of multi-source signals due to the cross-correlation among speakers. Initially, we tackle this challenge by designing a weak cross-correlation signal to reduce interference passively. In AMT^+, we’ve further integrated self-interference cancellation for active minimize interference. The most distinguishing advantage of AMT^+ lies in the elimination of the raised multipath effect, which is commonly ignored in previous work by hastily assuming targets as particles. AMT^+ employs Doppler filtering over delay subtraction for echo suppression. Further, by non-particle target reflections modeling results, we introduce a distance-projection-based method for continuous target identification and tracking. Implemented on commercial smartphones, AMT^+ achieves on average 0.54 cm, 1.37 cm, and 2.13 cm errors for single, double, and triple target tracking respectively, and on average 97.0% classification accuracy for 14 controlling gestures.}
}


@article{DBLP:journals/tmc/VannellaPJJ24,
	author = {Filippo Vannella and
                  Alexandre Prouti{\`{e}}re and
                  Yassir Jedra and
                  Jaeseong Jeong},
	title = {Learning Optimal Antenna Tilt Control Policies: {A} Contextual Linear
                  Bandits Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12666--12679},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3424192},
	doi = {10.1109/TMC.2024.3424192},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/VannellaPJJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Controlling antenna tilts in cellular networks is critical to achieve a good trade-off between network coverage and capacity. We devise algorithms learning optimal tilt control policies from existing data (passive learning setting) or from data actively generated by the algorithms (active learning setting). We formalize the design of such algorithms as a Best Policy Identification problem in Contextual Linear Bandits (CLB). In CLB, an action represents an antenna tilt update; the context captures current network conditions; the reward corresponds to an improvement of performance, mixing coverage and capacity. The objective is to identify an approximately optimal policy (a function mapping the context to an action with maximal reward). For both active and passive learning, we derive information-theoretical lower bounds on the number of samples required by any algorithm returning an approximately optimal policy with a given level of certainty, and devise algorithms achieving these fundamental limits. We apply our algorithms to the Remote Electrical Tilt optimization problem in cellular networks, and show that they can produce optimal tilt update policy using much fewer data samples than naive or existing rule-based learning algorithms. This paper is an extension of work presented at IEEE International Conference on Computer Communications (INFOCOM) 2022 (Vannella et al. 2022).}
}


@article{DBLP:journals/tmc/WangLXPF24,
	author = {Li Wang and
                  Liang Li and
                  Lianming Xu and
                  Xian Peng and
                  Aiguo Fei},
	title = {Failure-Resilient Distributed Inference With Model Compression Over
                  Heterogeneous Edge Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12680--12692},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419831},
	doi = {10.1109/TMC.2024.3419831},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLXPF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The distributed inference paradigm enables the computation workload to be distributed across multiple devices, facilitating the implementation of deep learning based intelligent services on extremely resource-constrained Internet of Things (IoT) scenarios. Yet it raises great challenges to perform complicated inference tasks relying on a cluster of IoT devices that are heterogeneous in their computing/communication capacity and prone to crash or timeout failures. In this paper, we present RoCoIn, a robust cooperative inference mechanism for locally distributed execution of deep neural network-based inference tasks over heterogeneous edge devices. It creates a set of independent and compact student models that are learned from a large model using knowledge distillation for distributed deployment. In particular, the devices are strategically grouped to redundantly deploy and execute the same student model such that the inference process is resilient to any local failures, while a joint knowledge partition and student model assignment scheme are designed to minimize the response latency of the distributed inference system in the presence of devices with diverse capacities. Extensive simulations are conducted to corroborate the superior performance of our RoCoIn for distributed inference compared to several baselines, and the results demonstrate its efficacy in timely inference and failure resilience.}
}


@article{DBLP:journals/tmc/WangYYPJ24,
	author = {Chen Wang and
                  Mengting Yuan and
                  Yang Yang and
                  Kai Peng and
                  Hongbo Jiang},
	title = {Revisiting Long- and Short-Term Preference Learning for Next {POI}
                  Recommendation With Hierarchical {LSTM}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12693--12705},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417405},
	doi = {10.1109/TMC.2024.3417405},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangYYPJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Point-of-interest (POI) recommendation has drawn much attention with the widespread popularity of location-based social networks (LBSNs). Previous works define long- and short-term trajectories via long short-term memory (LSTM) to capture user's stable and current preference, and incorporate context factors to improve recommendation effectiveness. However, these factors have different impacts on POI recommendation, and meanwhile, they are mutually influenced. Existing studies either model all the factors separately, or feed them into the same LSTM model, which are less meticulous for modeling the LBSNs trajectories. To address such issues, we revisit the long- and short-term preference learning for next POI recommendation by presenting a novel framework that can model both POI level and semantic level check-in trajectories. We develop a hierarchical LSTM to learn the two-level representations and consider the interplay of the two-level features by adding factors to the gates of LSTMs for each trajectory. We further construct a semantic filter to improve the recommendation efficacy. Experimental results using two real-world check-in datasets indicate that the proposed framework outperforms four state-of-the-art baselines regarding two commonly used metrics.}
}


@article{DBLP:journals/tmc/PanZGF24,
	author = {Yuhan Pan and
                  Zhipeng Zhou and
                  Wei Gong and
                  Yuguang Fang},
	title = {{SAT:} {A} Selective Adversarial Training Approach for WiFi-Based
                  Human Activity Recognition},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12706--12716},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3420405},
	doi = {10.1109/TMC.2024.3420405},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PanZGF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the continuous evolution of deep learning has opened up promising avenues to groundbreaking advancements in wireless sensing systems, which significantly enhance the practical applications of WiFi-based Human Activity Recognition (HAR) systems. However, despite these strides, such systems remain susceptible to adversarial attacks. This article unveils the vulnerability of existing WiFi-based HAR systems to common adversaries, revealing their insufficient robustness. While the intuitive approach is to employ adversarial training to fortify the models, our investigation exposes inherent deficiencies in the current approach. Specifically, we confirm that the strength of perturbations directly influences training outcomes. Moreover, even when confined within a specified perturbation radius, the perturbation strength exhibits variability within a prescribed range, potentially giving rise to “extreme” samples that could compromise training results. To address this challenge, we propose a two-stage Selective Adversarial Training (SAT) approach that integrates model confidence calibration and sample selection. Specifically, we start with calibrating the model and then selectively choose samples from all adversarial examples based on the calibrated confidence outputs that align with the desired criteria for adversarial training. This sample-wise perturbation intensity control effectively prevents the inclusion of inappropriate samples in training, a capability lacking in previous domain-wise perturbation control. Our experiments demonstrate that the proposed fine-grained training method, SAT, is both straightforward and effective in augmenting adversarial training results.}
}


@article{DBLP:journals/tmc/GuoSLCXC24,
	author = {Jiani Guo and
                  Shanshan Song and
                  Jun Liu and
                  Hao Chen and
                  Yuanbo Xu and
                  Jun{-}Hong Cui},
	title = {Exploring Applicable Scenarios and Boundary of {MAC} Protocols: {A}
                  {MAC} Performance Analysis Framework for Underwater Acoustic Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12717--12730},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417895},
	doi = {10.1109/TMC.2024.3417895},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoSLCXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Medium Access Control (MAC) protocols are critical for scheduling resources to access multiple users without collisions in Underwater Acoustic Networks (UANs). Due to harsh marine environments and limited communication resources, UANs lack a standard MAC protocol to adapt to various scenarios. The specific UAN scenario suffers from how to analyze multiple basic MAC protocols’ performance boundaries and modify the most potential one. A practical solution is to evaluate MAC protocols’ performance by modeling data loss (collisions and packet errors) and service time. However, existing models provide inaccurate performance results, since they ignore the effects of unique UANs’ characteristics and MAC protocol diversity on data loss and service time. In this paper, we propose a MAC Performance Analysis Framework (MPAF) for UANs to consider both unique UANs’ characteristics and MAC protocols’ diversity. We design Successful Transmission Probability (STP) model and Packet Service Time (PST) model in MPAF to estimate nodal throughput, delay, and energy consumption. STP model analyzes data loss types of different MAC protocols by considering long propagation delay, half-duplex communication, and random backoff to achieve a superior STP result from a view of real underwater communication conditions. Based on STP model, we employ Markov chain to deduce the retransmission number in PST model. In this way, MPAF ensures effectiveness and applicability in real-ocean environments. Extensive simulation results show that MPAF can accurately evaluate different MAC protocols’ performance boundaries, select the most appropriate basic protocol, and provide modified suggestions for a specific UAN scenario.}
}


@article{DBLP:journals/tmc/HuangMWCS24,
	author = {Jiwei Huang and
                  Bowen Ma and
                  Yuan Wu and
                  Ying Chen and
                  Xuemin Shen},
	title = {A Hierarchical Incentive Mechanism for Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12731--12747},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3423399},
	doi = {10.1109/TMC.2024.3423399},
	timestamp = {Sat, 07 Dec 2024 11:06:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangMWCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive development of mobile computing, federated learning (FL) has been considered as a promising distributed training framework for addressing the shortage of conventional cloud based centralized training. In FL, local model owners (LMOs) individually train their respective local models and then upload the trained local models to the task publisher (TP) for aggregation to obtain the global model. When the data provided by LMOs do not meet the requirements for model training, they can recruit workers to collect data. In this paper, by considering the interactions among the TP, LMOs and workers, we propose a three-layer hierarchical game framework. However, there are two challenges. First, information asymmetry between workers and LMOs may result in that the workers hide their types. Second, incentive mismatch between TP and LMOs may result in a lack of LMOs’ willingness to participate in FL. Therefore, we decompose the hierarchical-based framework into two layers to address these challenges. For the lower-layer, we leverage the contract theory to ensure truthful reporting of the workers’ types, based on which we simplify the feasible conditions of the contract and design the optimal contract. For the upper-layer, the Stackelberg game is adopted to model the interactions between the TP and LMOs, and we derive the Nash equilibrium and Stackelberg equilibrium solutions. Moreover, we develop an iterative Hierarchical-based Utility Maximization Algorithm (HUMA) to solve the coupling problem between upper-layer and lower-layer games. Extensive numerical experimental results verify the effectiveness of HUMA, and the comparison results illustrate the performance gain of HUMA.}
}


@article{DBLP:journals/tmc/WuLDWW24,
	author = {Panlong Wu and
                  Qi Liu and
                  Yanjie Dong and
                  Zhaorui Wang and
                  Fangxin Wang},
	title = {LMaaS: Exploring Pricing Strategy of Large Model as a Service for
                  Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12748--12760},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3418881},
	doi = {10.1109/TMC.2024.3418881},
	timestamp = {Mon, 09 Dec 2024 09:51:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuLDWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the most important features of next-generation communication is to incorporate intelligence towards semantic communication, where highly condensed semantic information considering both source and channel features will be extracted and transmitted. The recent popular large models such as GPT4 and the boosting learning techniques are envisioned to accelerate its practical implementation in the near future. Given the characteristics of “training once and widely use” of those multimodal large language models, we argue that a pay-as-you-go service mode will be suitable in this context, referred to as Large Model as a Service (LMaaS). However, the trading and pricing problem is quite complex with heterogeneous and dynamic customer environments, making the pricing optimization problem challenging in seeking on-hand solutions. In this paper, we optimize the profit of both the seller and customers. We formulate the LMaaS market trading as a Stackelberg game with two steps. In the first step, we optimize the seller's pricing decision and propose an Iterative Model Pricing (IMP) algorithm that optimizes the prices of large models iteratively by reasoning customers’ future rental decisions, which achieves a near-optimal pricing solution. In the second step, we optimize customers’ selection decisions by designing a robust selecting and renting (RSR) algorithm, which is guaranteed to be optimal with rigorous theoretical proof. Extensive experiments confirm the effectiveness and robustness of our algorithms, outperforming the state-of-the-art solution by 43.96% in profit at the customer side and achieving the near-optimal profit at the seller side.}
}


@article{DBLP:journals/tmc/DaiCWLG24,
	author = {Penglin Dai and
                  Yangyang Chao and
                  Xiao Wu and
                  Kai Liu and
                  Songtao Guo},
	title = {Context-Aware Offloading for Edge-Assisted On-Device Video Analytics
                  Through Online Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12761--12777},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3418608},
	doi = {10.1109/TMC.2024.3418608},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiCWLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing has emerged as a powerful technology for enhancing the performance of on-device video analytics, which is critical to support real-time applications. Nevertheless, there still lack of effective metrics to guide the offloading decision of video analytics tasks between device and edge server. Additionally, these existing optimization mechanisms either presume prior knowledge of the ground-truth of previous inferences or involve high training overheads, thereby rendering them unsuitable for real-time situations. To address these challenges, this paper presents a system model of edge-assisted online video analytics, where a lightweight object tracking module and a complex DNN-based model are deployed at the device and edge server, respectively. We formulate the resolution and deviation-based offloading (RDO) problem by considering heterogeneous computation resources and dynamic network bandwidth, aiming at maximizing inference accuracy and processing rate concurrently. We propose a context-aware offloading (CO) algorithm based on Bayesian optimization, which learns the optimal parameter settings by evaluating reward based on Gaussian process. Notably, the CO is proved to offer near-optimal solution with sublinear regret. Finally, we build a testbed and test algorithm performance on three realistic video datasets. The simulation results illustrate that the proposed CO outperforms other existing solutions in various service scenarios.}
}


@article{DBLP:journals/tmc/NaharMDB24,
	author = {Ankur Nahar and
                  Koustav Kumar Mondal and
                  Debasis Das and
                  Rajkumar Buyya},
	title = {Clouds on the Road: {A} Software-Defined Fog Computing Framework for
                  Intelligent Resource Management in Vehicular Ad-Hoc Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12778--12792},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419016},
	doi = {10.1109/TMC.2024.3419016},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NaharMDB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of software-defined networking (SDN) and cloud radio access networks (CRANs) into vehicular ad hoc networks (VANETs) presents intricate challenges to achieving stringent service level objectives (SLOs). These objectives include optimizing data flow and resource management, achieving low latency and rapid response times, and ensuring network resilience under fluctuating conditions. Traditional load balancing and clustering approaches, designed for more static environments, fall short in the dynamic and variable context of VANETs. This necessitates a paradigm shift towards more adaptive and robust strategies to meet these advanced SLOs reliably. This paper proposes a software-defined vehicular fog computing (SDFC) framework that refines resource allocation in VANETs. Our SDFC framework utilizes an intelligent controller placement that strategically positions decision-making entities within the network to optimize data flow and resource distribution. This placement is governed by a dynamic clustering algorithm that responds to variable network conditions, an advancement over the static mappings used by traditional methods. By incorporating parallel processing principles, the framework ensures that computational tasks are distributed effectively across network nodes, reducing bottlenecks and enhancing overall network agility. Empirical evaluations (testbed) and simulation results of our framework indicate a substantial increase in network efficiency: a 28% improvement in average response time, a 23% decrease in network latency, and a 25% faster convergence to optimal resource distribution compared to state-of-the-art methods. These improvements testify to the framework's ability to underscore its potential to refine operational efficacy within VANETs.}
}


@article{DBLP:journals/tmc/WangHS24,
	author = {Weizheng Wang and
                  Zhaoyang Han and
                  Chunhua Su},
	title = {Comments on "EAKE-WC: Efficient and Anonymous Authenticated Key
                  Exchange Scheme for Wearable Computing"},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12793--12794},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417181},
	doi = {10.1109/TMC.2024.3417181},
	timestamp = {Thu, 27 Feb 2025 22:37:47 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the above paper, Tu et al. proposed an efficient and anonymous authenticated key exchange scheme optimized for wearable computing environments, utilizing lightweight cryptographic primitives like XOR, ASCON, and hash functions. They claimed the employed Authenticated Key Exchange (AKE) scheme is robust against prevalent security threats. However, our analysis reveal a critical vulnerability to replay attacks that could undermine the protocol's security; specifically, an attacker could intercept messages and induce unauthorized server-side password updates, effectively blocking further legitimate user communications. Upon dissecting the root causes of this vulnerability, we offer targeted recommendations to mitigate such attacks and reinforce the protocol's defenses.}
}


@article{DBLP:journals/tmc/PeiWHH24,
	author = {Jian Pei and
                  Gang Wang and
                  Dominic K. C. Ho and
                  Lei Huang},
	title = {Moving Transceivers Aided Localization of a Far-Field Object},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12795--12810},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419123},
	doi = {10.1109/TMC.2024.3419123},
	timestamp = {Sat, 01 Mar 2025 10:58:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PeiWHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the localization problem in unique coordinates of a far-field object. Using moving transceivers as relays for the sensor signals in reaching the object, we utilize the range measurements from the sensors through the transceivers to the object to determine its position. The transceivers have no self-localization capability such that their motion parameters are unknown. Moreover, neither transceiver relay times nor object reflection delay are known, causing additional unknown range offsets. We propose an effective three-step method for this localization problem. The first step eliminates the object position and range offsets by formulating a constrained weighted least squares (CWLS) problem and estimates only the transceiver motion parameters. Using the preliminary estimate of the motion parameters, the second step formulates another CWLS problem to obtain the object position estimate, which is used to determine the range offsets by a linear WLS estimator. Finally, a refinement procedure follows in the third step by formulating a different CWLS problem to compensate for the performance loss. To solve the non-convex CWLS problems, we introduce semidefinite relaxation to transform them into convex semidefinite programs. Both mean square error analysis and simulation results show that the refined CWLS solution is able to achieve the Cramer–Rao lower bound performance when the SNR is not very low.}
}


@article{DBLP:journals/tmc/ZouWDWHW24,
	author = {Yongpan Zou and
                  Yunshu Wang and
                  Haozhi Dong and
                  Yaqing Wang and
                  Yanbo He and
                  Kaishun Wu},
	title = {PreGesNet: Few-Shot Acoustic Gesture Recognition Based on Task-Adaptive
                  Pretrained Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12811--12829},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419556},
	doi = {10.1109/TMC.2024.3419556},
	timestamp = {Tue, 17 Dec 2024 08:55:04 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZouWDWHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic-based human gesture recognition (HGR) applications have drawn increasing academic attention in order to overcome the shortcomings of conventional interaction methods on tiny devices. Existing techniques following a learning-based routine requires collecting massive application-specific training data. What is worse, the cross-domain problem induces additional retraining overhead to enable the systems recognize unseen gestures in different environments. This obviously decreases their scalability and prevent them from real-world deployment. Although some recent works propose different few-shot learning solutions to deal with the cross-domain problem in HGR, they possess shortcomings of being application-specific, high training overhead, and/or incapability to recognize unseen gestures. In this paper, we propose PreGesNet, a few-shot acoustic gesture recognition framework based on task-adaptive pretrained networks whose novelty lies in three aspects: i) leveraging pretrained feature extractor which captures generic knowledge of our collected and open-source large-scale gesture datasets; ii) designing task-specific parameter adaptation mechanism to efficiently update the feature extractor to adapt the pretrained feature extractor to each target task; iii) discovering suitable distance metric and task generation strategy which fit HGR application. According to the experiments, when the model is trained with 10 digit gestures, its recognition accuracies of 26 kinds of letter gestures and 8 kinds of other hand gestures can be up to 80.5% and 93.4% with only two shots, respectively. In addition, the average recognition latency of PreGesNet is less than 0.4 second.}
}


@article{DBLP:journals/tmc/ZhaoLWFGG24,
	author = {Rui Zhao and
                  Yun Li and
                  Kui Wang and
                  Yuze Fan and
                  Fei Gao and
                  Zhenhai Gao},
	title = {Centralized Cooperation for Connected Autonomous Vehicles at Intersections
                  by Safe Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12830--12847},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3417441},
	doi = {10.1109/TMC.2024.3417441},
	timestamp = {Wed, 15 Jan 2025 17:29:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoLWFGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connected and automated vehicles (CAVs) have the potential to transform traffic management, especially at intersections. Traditional traffic signals might become obsolete with the implementation of autonomous intersection management (AIM) systems, which aim for efficient and safe vehicle flow. Current AIM methods often rely on optimization control algorithms, which are not computationally efficient. Some methods use reinforcement learning (RL) but compromise safety for rewards and simplify traffic scenarios by designating specific turn lanes. This paper introduces a novel approach, the risk situation-aware constrained policy optimization (RSCPO), to enhance RL training with safety assurance. It uses Kullback-Leibler (KL) divergence to form a trust region, identifying risk levels in policy updates that could lead to dangerous situations, and suggests safe policy update mechanisms. Furthermore, the paper presents a safety reinforced all-directional autonomous intersection management (SafeR-ADAIM) algorithm. This algorithm accounts for the complexity of unpredictable all-direction turn lanes and collaboratively ensures the safety, efficiency, and smooth operation of CAVs at intersections. In simulations, our method surpasses the model predictive control (MPC)-based method in computational and traffic efficiency by 67.81 and 1.46 times, respectively. Additionally, it significantly reduces the mean collision rate from at most 35.01% to 0% compared to non-safety aware RL methods.}
}


@article{DBLP:journals/tmc/FuWZCRZ24,
	author = {Yongjian Fu and
                  Shuning Wang and
                  Linghui Zhong and
                  Lili Chen and
                  Ju Ren and
                  Yaoxue Zhang},
	title = {UltraSR: Silent Speech Reconstruction via Acoustic Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12848--12865},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419170},
	doi = {10.1109/TMC.2024.3419170},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FuWZCRZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Silent Speech Interfaces (SSI) have been developed to convert silent articulatory gestures into speech, aiding communication in public spaces and assisting individuals with aphasia. Previous SSIs, which rely on wearable devices or cameras, often pose issues like prolonged contact or privacy risks. Recent advancements in acoustic sensing present new opportunities for gesture sensing, but they typically focus on content classification rather than reconstructing audible speech. This results in the loss of crucial speech characteristics such as rate, intonation, and emotion.In this paper, we propose UltraSR, a novel sensing system designed for accurate audible speech reconstruction by analyzing the disturbance of tiny articulatory gestures on reflected ultrasound signals. UltraSR employs a multi-scale feature extraction scheme to aggregate information from multiple views and introduces a new model that maps ultrasound to speech signals, enabling the reconstruction of audible speech from silent gestures.Instead of the laborious collection of massive training data, UltraSR constructs an inverse task to generate virtual gestures from widely available audio (e.g., phone calls) for efficient model training. Additionally, it incorporates a finetuning mechanism using unlabeled data for user adaptation.We implemented UltraSR on a portable smartphone and evaluated it in various environments. Results show that UltraSR can achieve a Character Error Rate (CER) as low as 5.22% and reduce the CER from 80.13% to 6.31% for new users with only 1 hour of ultrasound data, outperforming state-of-the-art acoustic-based approaches while preserving rich speech information.}
}


@article{DBLP:journals/tmc/WangLJ24,
	author = {Chenlong Wang and
                  Guanghua Liu and
                  Tao Jiang},
	title = {Malicious Node Detection in Wireless Weak-Link Sensor Networks Using
                  Dynamic Trust Management},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12866--12877},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3418826},
	doi = {10.1109/TMC.2024.3418826},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The application of Wireless Sensor Networks (WSNs) in extreme environments is becoming increasingly widespread. Within these extreme environments, communication links between WSN nodes become more fragile. We refer to such WSNs as Wireless Weak-link Sensor Networks (WWSNs). The characteristics of WWSNs make them more vulnerable to internal attacks. During the data transmission process from source nodes to destination nodes, intermediary nodes could act as malicious entities capable of intercepting or manipulating data. Therefore, detecting malicious nodes is of utmost importance. This paper proposes a malicious node detection strategy based on dynamic trust management to address these challenges. The dynamic trust management algorithm integrates type-2 fuzzy logic and considers various trust factors to comprehensively evaluate node trust within WWSNs. Additionally, a dynamic trust value updating mechanism is proposed to accommodate the dynamic environmental changes inherent to WWSNs. Experimental results emphasize the effectiveness of the proposed approach in dynamically adapting to the network environment while achieving a high level of performance in detecting malicious nodes.}
}


@article{DBLP:journals/tmc/WenLLLWYG24,
	author = {Yingkun Wen and
                  Lei Liu and
                  Junhuai Li and
                  Yilan Li and
                  Kan Wang and
                  Shui Yu and
                  Mohsen Guizani},
	title = {Covert Communications Aided by Cooperative Jamming in Overlay Cognitive
                  Radio Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12878--12891},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419071},
	doi = {10.1109/TMC.2024.3419071},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WenLLLWYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper examines integrating jamming and secondary signals for covert communications in cognitive radio networks (CRNs), aiming to enhance covertness by using jamming and secondary signals in an overlay cooperative CRN. The scenario involves a primary base station (PBS) transmitting to a primary user (PU), with a secondary user transmitter (SU-Tx) acting as a cooperative jammer to obscure the message from a malevolent secondary user named “Willie.” During idle intervals on the primary channel, the SU-Tx opportunistically accesses it to transmit secondary signals, reinforcing the covert communication of primary signals. The study quantifies the detection error probability (DEP) experienced by Willie, considering perfect and statistical channel state information (CSI) scenarios. In the perfect CSI scenario, optimization has two phases. Phase I aims to maximize the signals-to-interference-plus-noise ratio (SINR) of the PU, subject to the warden DEP exceeding a specified threshold. Phase II uses an iterative search algorithm to optimize beamforming vectors, enhancing SINR. In the statistical CSI scenario, the goal is to maximize effective transmission throughput (ETT), measuring the information transmitted from PBS to PU under covert constraints. Numerical results validate the theoretical analysis.}
}


@article{DBLP:journals/tmc/QiuZNT24,
	author = {Houming Qiu and
                  Kun Zhu and
                  Dusit Niyato and
                  Bin Tang},
	title = {Resilient, Secure, and Private Coded Distributed Convolution Computing
                  for Mobile-Assisted Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12892--12906},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3418449},
	doi = {10.1109/TMC.2024.3418449},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QiuZNT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Metaverse is recognized as the next-generation Internet that provides immersive interaction experiences for users. Convolutional neural networks (CNNs) play a crucial role in providing strong immersive experiences in the Metaverse. However, the Metaverse faces challenges in meeting the escalating demands for computing and storage resources due to the explosive growth of convolution tasks, resulting in severe performance degradation. To tackle these issues, coded distributed computing (CDC) is commonly employed. In this paper, we first propose an efficient and reliable mobile-assisted CDC framework to perform large-scale CNN training tasks for the Metaverse. In this framework, the various mobile devices act as workers contributing their resources to collaborate with each other to complete convolution operation tasks. Furthermore, we design a novel resilient, secure, and private coded convolution (RSPCC) scheme for the proposed framework. The RSPCC scheme achieves several significant performances. First, it substantially reduces computation latency compared to conventional convolution. Second, it efficiently mitigates an adverse impact of straggling workers returning results exceedingly slow. Third, we integrate a verifiable computing approach into the encoding/decoding process to check the correctness of the final computation results. Fourth, the PSPCC scheme considers the existence of colluding workers, providing information-theoretic privacy protection for input data. Finally, experimental results demonstrate that our proposed RSPCC scheme can significantly reduce execution time while ensuring the correctness of computation results within the CDC-based Metaverse framework.}
}


@article{DBLP:journals/tmc/WuBD24,
	author = {Mengfan Wu and
                  Mate Boban and
                  Falko Dressler},
	title = {Flexible Training and Uploading Strategy for Asynchronous Federated
                  Learning in Dynamic Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12907--12921},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3418613},
	doi = {10.1109/TMC.2024.3418613},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuBD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning is a fast-developing distributed learning scheme with promising applications in vertical domains such as industrial automation and connected automated driving. The heterogeneity of devices in data distribution, communication, and computation, when deployed in dynamic environments typically with wireless communication, poses challenges to traditional federated learning solutions, where successful learning depends on balanced contribution from participants. In this paper, we propose a flexible communication strategy for devices in asynchronous federated learning, which adapts the training and uploading actions based on the condition of the communication link. We propose a novel method of computing aggregation weight based on model distances and number of local optimizations, to control errors introduced in asynchronous aggregation while maximizing learning speed. We prove the convergence of the learning tasks analytically under the new scheme. The improved performance is rooted in the increased number of optimizations during training, which grows by 12% through opportunistically condensing model uploading during good link condition periods. By facilitating timely communication between devices and server, combined with the novel aggregation weight design, our method reduces the communication resources in dynamic environments by at least 5% while even slightly increasing the learning accuracy.}
}


@article{DBLP:journals/tmc/LiuWGLPF24,
	author = {Jinyu Liu and
                  Jie Wang and
                  Qinghua Gao and
                  Xuanheng Li and
                  Miao Pan and
                  Yuguang Fang},
	title = {Diversity-Enhanced Robust Device-Free Vital Signs Monitoring Using
                  mmWave Signals},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12922--12937},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419005},
	doi = {10.1109/TMC.2024.3419005},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWGLPF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-free vital signs monitoring is an emerging technology that utilizes the unique influence of chest vibrations on surrounding wireless signals to achieve vital signs monitoring in a device-free and contact-free manner. Existing methods could achieve good monitoring performance when high-quality reflected signals can be obtained. However, in daily vital signs monitoring at home, the received reflected signals are often very weak due to factors such as obstruction and attenuation, resulting in a sharp decrease in the monitoring performance. To address the aforementioned challenges, in this paper, we develop a diversity-enhanced robust device-free vital signs monitoring system using mmWave signals. Specifically, inspired by the concept of diversity in the field of communications, we propose a diversity-enhanced wireless sensing strategy that comprehensively utilizes multi-dimensional physical layer resources, including antennas, chirps, and space, to improve the signal-to-noise ratio of vital signs. Additionally, inspired by cameras that achieve clear images by prolonging exposure time, we propose an accumulation-enhanced localization method to lock onto the chest of the human body in complex scenarios. Extensive experiments on a 60 GHz mmWave testbed demonstrate that our developed system could guarantee robust vital signs monitoring performance in various challenging scenarios, even at distances of up to 40 m.}
}


@article{DBLP:journals/tmc/LiZX24,
	author = {Yu Li and
                  Jiaheng Zhang and
                  Ning Xie},
	title = {Demodulation Scheme Against Phase Noise Using an Ensemble Clustering
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12938--12951},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419021},
	doi = {10.1109/TMC.2024.3419021},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most wireless systems involve time-varying multipath channels, where the negative effect of the phase noise cannot be ignored. The phase noise can be introduced by imperfect phase-locked loop circuitry, imperfect channel estimation, or both. This paper considers the demodulation problem of wireless systems with phase noise. We propose an ensemble clustering algorithm to address the limitations of the existing demodulation schemes. The proposed ensemble clustering algorithm is named as the Ensemble Clustering algorithm using the Matrix Factorization and Information Theory (MFIT-EC). The MFIT-EC algorithm improves the performance of existing ensemble clustering algorithms, which neglect different clustering effects of different base-clustering algorithms or different effects of different clusters of the same base-clustering algorithm. Based on the MFIT-EC algorithm, we design a demodulation scheme in a coherent wireless system with phase noise, referred to as the MFIT-EC demodulation scheme. Specifically, we first utilize several base-clustering algorithms to obtain different base-clustering results. Second, we use a weighted ensemble mechanism to allocate different weights to different base-clustering results, and calculate the certainty of the clusters of each base-clustering algorithm to obtain a better and more robust demodulation performance. We implement the proposed approach and conduct extensive performance comparisons through simulations, which show that the proposed approach has better performance than the prior approaches in terms of both clustering performance and demodulation performance.}
}


@article{DBLP:journals/tmc/DaiAPY24,
	author = {Donghui Dai and
                  Zhenlin An and
                  Qingrui Pan and
                  Lei Yang},
	title = {Harnessing {NFC} to Generate Standard Optical Barcodes for NFC-Missing
                  Smartphones},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12952--12968},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3420410},
	doi = {10.1109/TMC.2024.3420410},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DaiAPY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile payments have grown significantly recently, driven by their contactless feature that minimizes COVID-19 transmission risks. While NFC offers more security and convenience than barcodes and benefits those with amblyopia, many smartphones lack NFC due to module shortages or security decisions. In this work, we present {\\sf MagCode}\n, an innovative method connecting NFC readers with cameras, allowing users to enjoy NFC payment security using prevalent camera technology. At the heart of {\\sf MagCode}\nis the harmless magnetic interference on the CMOS image sensor of a smartphone placed nearby the NFC reader, resulting in a group of barcode-like stripes appearing on the captured images. We take advantage of these stripes to encode the data and achieve simplex communication from an NFC reader to an NFC-denied or NFC-disabled smartphone. In particular, we developed a comprehensive suite of protocols spanning from the physical layer to the transport layer, and we rigorously tested our proof-of-concept prototype on 11 different smart devices. Our extensive evaluations showcase a maximum throughput of 2.58 kbps–surpassing magnetometer-based alternatives by a factor of 58–and demonstrate an average data exchange time of 1.3 seconds for mobile payment transactions between an NFC reader and a smartphone.}
}


@article{DBLP:journals/tmc/LuYCCLL24,
	author = {Yanyan Lu and
                  Lei Yang and
                  Hao{-}Rui Chen and
                  Jiannong Cao and
                  Wanyu Lin and
                  Saiqin Long},
	title = {Federated Class-Incremental Learning With Dynamic Feature Extractor
                  Fusion},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12969--12982},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419096},
	doi = {10.1109/TMC.2024.3419096},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuYCCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated class-incremental learning (FCIL) allows multiple clients in a distributed environment to learn models collaboratively from evolving data streams, where new classes arrive continually at each client. Some existing works in FCIL combine traditional federated learning methods with class-incremental methods. However, the global model affected by data heterogeneity can aggravate local forgetting through the direct combination of traditional methods. To tackle this issue, we propose FCIDF, a novel Federated Class-Incremental learning approach based on Dynamic feature extractor Fusion. FCIDF learns personalized and incremental models for each client by introducing personalized fusion rates to integrate global knowledge into local features. Leveraging meta-learning during each incremental round, FCIDF ensures involvement of both old and new task knowledge in personalized training. Besides, we further propose a new Storing strategy based on Accumulated Global Feature Means (AGFMS), which helps the model review unbiased old knowledge and compensates for local forgetting. Experiment results show that FCIDF outperforms the baseline methods in both accuracy and forgetting on most settings, and AGFMS improves the performance of FCIDF on most evaluated scales.}
}


@article{DBLP:journals/tmc/LiZXZSMDY24,
	author = {Danyang Li and
                  Yishujie Zhao and
                  Jingao Xu and
                  Shengkai Zhang and
                  Longfei Shangguan and
                  Qiang Ma and
                  Xuan Ding and
                  Zheng Yang},
	title = {Reshaping Edge-Assisted Visual {SLAM} by Embracing On-Chip Intelligence},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12983--12997},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3424452},
	doi = {10.1109/TMC.2024.3424452},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZXZSMDY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-assisted visual SLAM plays a crucial role in enabling innovative mobile applications, such as autonomous swarm inspection, search-and-rescue, and smart logistics. Constrained by the computational capacities of lightweight mobile devices, current approaches delegate lightweight, time-sensitive tracking tasks to the mobile end while offloading resource-intensive, latency-tolerant map optimization tasks to the edge. However, our pilot study reveals several limitations of the tracking-optimization decoupled paradigm, stemming from the disruption of inter-dependencies between the two tasks. In this paper, we design and implement edgeSLAM2, an innovative system that reshapes the edge-assisted visual SLAM paradigm by tightly integrating tracking and partial-yet-crucial optimization on mobile. edgeSLAM2 harnesses the heterogeneous computing units offered by the commercial systems-on-chip (SoCs) to enhance the computational capacity of mobile devices, which in turn, allows edgeSLAM2 to design a suit of novel algorithms for map sync, optimization, and tracking that accommodate such architectural upgrade. By capitalizing on the full potential of on-chip intelligence, edgeSLAM2 supports both solitary and collaborative SLAM with accuracy and immediacy, underpinned by a cohesive software-hardware co-design. We deploy edgeSLAM2 on drones for industrial inspection. Comprehensive experiments in one of the world’s largest oil fields over three months demonstrate its superior performance.}
}


@article{DBLP:journals/tmc/YangDLLWLZSX24,
	author = {Huanqi Yang and
                  Di Duan and
                  Hongbo Liu and
                  Chengwen Luo and
                  Yuezhong Wu and
                  Wei Li and
                  Albert Y. Zomaya and
                  Linqi Song and
                  Weitao Xu},
	title = {Scenario-Adaptive Key Establishment Scheme for LoRa-Enabled IoV Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {12998--13014},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3421659},
	doi = {10.1109/TMC.2024.3421659},
	timestamp = {Mon, 02 Dec 2024 08:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangDLLWLZSX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the Internet of Vehicles (IoV) has experienced significant growth, but the lack of effective secret key establishment remains a security concern due to the dynamic and ad-hoc nature of IoV communications. Physical layer key generation has emerged as a promising solution for establishing a pair of cryptographic keys in a lightweight and information-theoretic secure manner. However, previous works have primarily focused on legacy communication technologies, such as Wi-Fi, ZigBee, and 5 G, which are limited to short-range IoV communications. With the emergence of Long-range (LoRa) communication technology, which features long-range, low power, and extremely low data rates, new challenges arise for key generation in long-range IoV scenarios. This paper presents Vehicle-Key, a secret key generation system designed to secure LoRa-enabled IoV communications. Vehicle-Key presents an innovative scenario adaptive deep learning model that performs channel prediction and quantization concurrently while reducing the training cost through a data augmentation pipeline and enhancing the model's generalization using a domain-adaption method. Additionally, we propose a bloom filter-assisted autoencoder-based reconciliation method to significantly improve the key agreement rate. Comprehensive real-world experiments show that Vehicle-Key surpasses the State-of-the-Art, achieving a 15.26%–50.35% improvement in key agreement rate and a 9–15× increase in key generation rate. Moreover, the proposed method attains a 4.37--9.33% improvement when adapted to new scenarios with limited data sizes. A security analysis demonstrates that Vehicle-Key is resilient against several common attacks. Furthermore, we implement Vehicle-Key on a Raspberry Pi and demonstrate its ability to execute within 3.5 ms.}
}


@article{DBLP:journals/tmc/LiuSLLWWN24,
	author = {Saichao Liu and
                  Geng Sun and
                  Jiahui Li and
                  Shuang Liang and
                  Qingqing Wu and
                  Pengfei Wang and
                  Dusit Niyato},
	title = {UAV-Enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13015--13032},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3419915},
	doi = {10.1109/TMC.2024.3419915},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuSLLWWN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate an unmanned aerial vehicle (UAV)-assistant air-to-ground communication system, where multiple UAVs form a UAV-enabled virtual antenna array (UVAA) to communicate with remote base stations by utilizing collaborative beamforming. To improve the work efficiency of the UVAA, we formulate a UAV-enabled collaborative beamforming multi-objective optimization problem (UCBMOP) to simultaneously maximize the transmission rate of the UVAA and minimize the energy consumption of all UAVs by optimizing the positions and excitation current weights of all UAVs. This problem is challenging because these two optimization objectives conflict with each other, and they are non-concave to the optimization variables. Moreover, the system is dynamic, and the cooperation among UAVs is complex, making traditional methods take much time to compute the optimization solution for a single task. In addition, as the task changes, the previously obtained solution will become obsolete and invalid. To handle these issues, we leverage the multi-agent deep reinforcement learning (MADRL) to address the UCBMOP. Specifically, we use the heterogeneous-agent trust region policy optimization (HATRPO) as the basic framework, and then propose an improved HATRPO algorithm, namely HATRPO-UCB, where three techniques are introduced to enhance the performance. Simulation results demonstrate that the proposed algorithm can learn a better strategy compared with other methods. Moreover, extensive experiments also demonstrate the effectiveness of the proposed techniques.}
}


@article{DBLP:journals/tmc/HanFWHZ24,
	author = {Guangjie Han and
                  Zixiao Feng and
                  Hao Wang and
                  Yun Hou and
                  Fan Zhang},
	title = {Underwater Multi-Target Node Path Planning in Hybrid Action Space:
                  {A} Deep Reinforcement Learning Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13033--13047},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3421541},
	doi = {10.1109/TMC.2024.3421541},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanFWHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Path planning is a basic requirement for Autonomous Underwater Vehicles (AUVs) to accomplish underwater missions. However, previous studies often have limitations, such as ignoring the basic condition that the AUV operates in an ocean current environment and discretizing its actions without considering the action space, which results in the simulation being far from the actual situation. To solve the above problems, this paper proposes a method of using a Parametrized Deep Q-Network (PDQN) to output hybrid actions for path planning, which can output a hybrid action space based on the current local observation, flexibly avoid obstacles under limited sensor observations, and realize the refined operation of AUV actions. According to the setup of the simulation environment, the AUV needs to visit multiple target nodes underwater and decelerate within the communication range of the nodes to have enough time to communicate with the nodes. The PDQN enables the AUV to easily learn the connection between the current state and discrete actions. It outputs the corresponding continuous actions based on the current discrete actions, which realizes a time-saving strategy of accelerating and then decelerating among the nodes. Meanwhile, we also utilize the actual current data and terrain data to restore the simulation environment as accurately as possible, and the simulation results prove the superiority and robustness of the algorithm.}
}


@article{DBLP:journals/tmc/WuLZMXX24,
	author = {Qingshun Wu and
                  Yafei Li and
                  Guanglei Zhu and
                  Baolong Mei and
                  Jianliang Xu and
                  Mingliang Xu},
	title = {Prediction-Aware Adaptive Task Assignment for Spatial Crowdsourcing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13048--13061},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3423396},
	doi = {10.1109/TMC.2024.3423396},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuLZMXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless networks and smart devices, spatial crowdsourcing (SC) has become increasingly prevalent. The key issue in SC is efficiently assigning spatial tasks, such as parcel and food delivery, to mobile workers in order to maximize platform utility. Existing works mainly focus on task assignment based on real-time spatio-temporal constraints of workers and tasks, neglecting the influence of future spatio-temporal distributions of tasks on current assignments. In this paper, we propose a novel problem in SC called Prediction-aware Task Assignment (PTA), where the platform adaptively assigns spatial tasks to workers by considering their current and future spatio-temporal constraints to maximize overall platform revenue. To address this problem, we introduce a two-stage framework composed of task prediction and task assignment. In the task prediction stage, we develop a powerful Bilateral Spatial-Temporal Graph Convolutional Network (BSTGCNet) to predict the time and location where potential tasks may appear in the future. In the task assignment stage, we present a Deep Reinforcement Learning (DRL) approach to dynamically partition tasks into batches based on the current and future status of tasks, and conduct bipartite graph matching for spatial tasks and workers in a batch-wise manner. Finally, extensive experiments on real-world datasets validate the effectiveness and efficiency of our proposed solution.}
}


@article{DBLP:journals/tmc/LiSLWMDC24,
	author = {Wei Li and
                  Zichen Shen and
                  Xiulong Liu and
                  Mingfeng Wang and
                  Chao Ma and
                  Chuntao Ding and
                  Jiannong Cao},
	title = {Representative Kernels-Based {CNN} for Faster Transmission in Federated
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13062--13075},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3423448},
	doi = {10.1109/TMC.2024.3423448},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiSLWMDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the contradiction between limited bandwidth and huge transmission parameters, federated Learning (FL) has been an ongoing challenge to reduce the model parameters that need to be transmitted to server in clients for fast transmission. Existing works that attempt to reduce the amount of transmitted parameters have limitations: 1) the reduced number of parameters is not significant; 2) the performance of the global model is limited. In this paper, we propose a novel method called Fed-KGF that significantly reduces the amount of model parameters while improving the global model performance. Our goal is to reduce those transmitted parameters by reducing the number of convolution kernels. Specifically, we construct an incomplete model with a few representative convolution kernels, and propose Kernel Generation Function (KGF) to generate other convolution kernels to render the incomplete model to be a complete one. We discard those generated kernels after training local models, and solely transmit those representative kernels during training, thereby significantly reducing the transmitted parameters. Furthermore, there is a client-drift in the traditional FL because of the averaging method, which hurts the global model performance. We innovatively select one or few modules from all client models in a permutation way, and only aggregate the uploaded modules rather than averaging all modules to reduce client-drift, thus improving the global model performance and further reducing the transmitted parameters. Experimental results on both non-Independent and Identically Distributed (non-IID) and IID scenarios for image classification and object detection tasks demonstrate that our Fed-KGF outperforms SOTA FL models.}
}


@article{DBLP:journals/tmc/LiLKWZLS24,
	author = {Yiran Li and
                  Zhen Liu and
                  Ze Kou and
                  Yannan Wang and
                  Guoqiang Zhang and
                  Yidong Li and
                  Yongqi Sun},
	title = {Real-Time Adaptive Partition and Resource Allocation for Multi-User
                  End-Cloud Inference Collaboration in Mobile Environment},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13076--13094},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3430103},
	doi = {10.1109/TMC.2024.3430103},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLKWZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of Deep Neural Networks (DNNs) requires significant computational and storage resources, which is challenging for resource-constrained end devices. To this end, collaborative deep inference is proposed, in which the DNN is divided into two parts and executed on the end device and cloud respectively. The selection of DNN partition point is the key challenge to realize end-cloud collaborative deep inference, especially in mobile environments with unstable networks. In this paper, we propose a Real-time Adaptive Partition (RAP) framework, in which a fast split point decision algorithm is proposed to realize real-time adaptive DNN model partition in the mobile network. A weighted joint optimization of DNN quantization loss, inference and transmission latency is performed. We further propose a Joint Multi-user Model Partition and Resource Allocation (JM-MPRA) algorithm under RAP framework. JM-MPRA aims to guarantee the optimized latency, accuracy and resource utilization in the multi-user scene. Experimental evaluations have demonstrated the effectiveness of RAP with JM-MPRA in improving the performance of real-time end-cloud collaborative inference in both stable and unstable mobile networks. Compared with the state-of-the-art methods, the proposed approaches can achieve up to 5.06x decrease in inference latency and bring performance improvement of 1.52% in inference accuracy.}
}


@article{DBLP:journals/tmc/LiuSSXTS24,
	author = {Jia Liu and
                  Jianbo Shao and
                  Min Sheng and
                  Yang Xu and
                  Tarik Taleb and
                  Norio Shiratori},
	title = {Mobile Crowdsensing Ecosystem With Combinatorial Multi-Armed Bandit-Based
                  Dynamic Truth Discovery},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13095--13113},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3428542},
	doi = {10.1109/TMC.2024.3428542},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuSSXTS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsensing (MCS) has emerged as a popular and promising paradigm for solving challenging problems by utilizing collective wisdom and resources. However, the system architecture and operational rules for MCS have not been well-defined, and obtaining accurate and reliable results from conflicting data collected by workers is difficult due to discrepancies in sensor quality and privacy protection requirements. In this paper, we combine the methodologies of Dynamic Truth Discovery (DTD), Combinatorial Multi-Armed Bandit (CMAB), and Multi-Attribute Reverse Auction to develop a novel MCS ecosystem, with the objective of maximizing the sensing accuracy-aware utility under the budget constraint. We first establish the data collection model by jointly considering the task completion duration as well as the deviation caused by both endogenous errors and privacy protection-oriented injected noise. Then, we theoretically evaluate the accuracy of truth discovery and quantify the contribution of each worker to MCS to form the worker selection criterion. As the qualities of workers are initially unknown, the platform faces the exploration-exploitation dilemma. Therefore, we apply CMAB to transform the worker recruitment problem into a combinatorial arm-pulling problem and elaborately design an Upper Confidence Bound (UCB) algorithm to achieve a desirable exploration-exploitation tradeoff. Moreover, we design an auction-based payment method for the platform, stimulating workers to provide their quoted price honestly while enabling individual rationality. Extensive simulations and comparison results demonstrate the feasibility and effectiveness of our proposed MCS ecosystem.}
}


@article{DBLP:journals/tmc/LiZWYLCHSC24,
	author = {Jiamu Li and
                  Dongheng Zhang and
                  Zhi Wu and
                  Cong Yu and
                  Yadong Li and
                  Qi Chen and
                  Yang Hu and
                  Qibin Sun and
                  Yan Chen},
	title = {{SBRF:} {A} Fine-Grained Radar Signal Generator for Human Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13114--13130},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427406},
	doi = {10.1109/TMC.2024.3427406},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiZWYLCHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While deep learning-based RF perception has received significant attention in recent years, the requirement for massive labeled RF data has hindered its further advancement. Despite existing efforts in synthesizing signals, they fail to accurately calculate the Radar Cross Section (RCS) of the target, leading to less practicality of the synthesized signals. In this paper, we introduce Simulated Body Radio Frequency (SBRF), a novel signal synthesis framework for calculating more realistic RCS by combining ray tracing with electromagnetic computation. SBRF involves three key components: a grid-based Shooting and Bouncing Ray (SBR) algorithm to calculate fine-grained human body RCS, a novel ray partitioning algorithm to improve the efficiency of ray tracing, and a coordinate transformation method to sense moving targets. Furthermore, we also design unique data augmentation techniques to improve the efficiency and generalizability of signal synthesis. Extensive experimental evaluations conducted on two publicly available datasets, involving wide-scale activity recognition and fine-grained gesture recognition, demonstrate the effectiveness of SBRF-generated signals in improving RF perception performance and alleviating the challenge of RF data collection.}
}


@article{DBLP:journals/tmc/AzizB24,
	author = {Zagroz Aziz and
                  Robert Bestak},
	title = {Modeling Voice Traffic Patterns for Anomaly Detection and Prediction
                  in Cellular Networks Based on {CDR} Data},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13131--13143},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3430032},
	doi = {10.1109/TMC.2024.3430032},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AzizB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The available telecommunication services nowadays make connecting users easier. In return, vast streams of data are generated every day. However, mining useful information in data requires relevant techniques and procedures. In this article, a novel study is proposed consisting of a multi-algorithm approach to understand, detect and predict anomalies in cellular networks. The study holds 37 million Call Detail Records data from a cellular network with deployment of the latest network generations including 5G. The research is divided into two phases. In the first phase, we outline the voice traffic profile, where utilizing certain algorithms are meant to target specific attributes and scenarios in the data to understand the typical voice traffic patterns. Gaussian Mixture model is used to define the regular groups of call duration and Mean Shift clustering algorithm is employed to detect the peak hours on a daily basis. In the second phase, we deseasonalize the data for higher accuracy followed by the distribution function to comprehend the patterns in the data. We introduce three algorithms to detect and predict anomalies in the cellular network. The performance evaluation shows that DBSCAN and Isolation Forest algorithms provide the highest accuracy with 98% compared to the Z-score algorithm.}
}


@article{DBLP:journals/tmc/ShiLWLZ24,
	author = {Fang Shi and
                  Weiwei Lin and
                  Xiumin Wang and
                  Keqin Li and
                  Albert Y. Zomaya},
	title = {The Analysis and Optimization of Volatile Clients in Over-the-Air
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13144--13157},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427709},
	doi = {10.1109/TMC.2024.3427709},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShiLWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the implementation of Federated Learning (FL) in an over-the-air computation system with volatile clients, where each client operates under a limited energy budget and may unexpectedly drop out during local training sessions. The dropout of clients not only wastes energy but also diminishes their participation frequency, necessitating careful client selection by the server in each communication round. However, the diversity of training tasks and the random nature of client dropout present challenges such as the absence of an explicit objective function and the unavailability of client performance metrics. To address these challenges, we first analyze the convergence of the over-the-air federated learning system with volatile clients to identify the key factor influencing the model's convergence speed. Building upon this analysis, we propose an approximation of the objective function as the optimization goal for client selection. To mitigate energy waste, we introduce a dynamic client selection strategy termed DCSE, based on Exp3 with multiple plays and energy constraints, aiming to reconcile the dilemma of unknown local training states and limited resource constraints. Theoretical analysis demonstrates that our proposed solution maintains a constant bound on the difference from the optimal solution, affirming its theoretical feasibility. Furthermore, experimental results validate the effectiveness of the proposed strategy in enhancing FL by accelerating convergence speed, improving test accuracy, and reducing wasted energy.}
}


@article{DBLP:journals/tmc/SkocajALV24,
	author = {Marco Skocaj and
                  Lorenzo Mario Amorosa and
                  Michele Lombardi and
                  Roberto Verdone},
	title = {{GUMBLE:} Uncertainty-Aware Conditional Mobile Data Generation Using
                  Bayesian Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13158--13171},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438208},
	doi = {10.1109/TMC.2024.3438208},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SkocajALV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of mobile and Internet of Things (IoT) networks, data naturally originates at the edge, making crowdsourcing a convenient and inherent approach to data collection. However, crowdsourcing presents challenges related to privacy, sampling bias, statistical sufficiency, and the need for time-consuming post-processing. To this end, generating synthetic data using deep learning techniques emerges as a promising solution to overcome such limitations. In this study, we propose an innovative framework that transcends applications and data types, enabling the conditional generation of crowdsourced datasets with location information in mobile and IoT networks. A crucial aspect of our methodology lies in the ability to assess uncertainty in newly generated samples and produce calibrated predictions through approximate Bayesian methods. Without loss of generality, we ascertain the validity of our method on the task of minimization of drive test (MDT) data generation, presenting for the first time a comparison of synthetically generated data with an original large-scale MDT set collected from a mobile network operator's network infrastructure. By offering a versatile solution to data generation, our framework contributes to overcoming challenges associated with crowdsourced data, opening up possibilities for advanced analytics and experimentation in mobile and IoT networks.}
}


@article{DBLP:journals/tmc/LinWWKZG24,
	author = {Deyu Lin and
                  Jun Wan and
                  Jing Wang and
                  Linghe Kong and
                  Yufei Zhao and
                  Yong Liang Guan},
	title = {A Novel Topology-Scale-Adaptive and Energy-Efficient Clustering Scheme
                  for Energy Sustainable Large-Scale SWIPT-Enabled WSNs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13172--13188},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3429235},
	doi = {10.1109/TMC.2024.3429235},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinWWKZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of the Simultaneous Wireless Information and Power Transfer (SWIPT) technology makes it possible to achieve energy sustainability in the Wireless Sensor Networks (WSNs). However, little attention was paid to the large-scale SWIPT-enabled WSNs. To this end, we synthesize the network Energy Efficiency (EE), energy sustainability conditions, and network throughput in the large-scale SWIPT-enabled WSNs, and propose a Topology-Scale-Adaptive and Energy Efficient Clustering Scheme (TSA-EECS). To be specific, the EE maximization problem is formulated as a fractional programming problem, which jointly optimizes the transmission power and the power splitting ratio of sensors. Subsequently, a Dinkelbach-based iterative algorithm is proposed to transform the problem and a Lagrangian function is presented to obtain a near-optimal solution through the gradient descent method. In addition, a BFOA-based Cluster Head (CH) selection algorithm is proposed to adapt to the large-scale network and reduce energy consumption for CH selection. Finally, extensive simulations are conducted to evaluate the effectiveness of TSA-EECS. Experimental results demonstrate that the proposed iterative algorithm converges after 15 iterations on average. In addition, compared with recent energy harvesting schemes, the network throughput of TSA-EECS is improved by 3–8%.}
}


@article{DBLP:journals/tmc/LiuCP24,
	author = {Xiuwen Liu and
                  Yanjiao Chen and
                  Shanchen Pang},
	title = {Trading off Coverage and Emergency for Hybrid Task Scheduling in Traffic
                  Anomaly Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13189--13206},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3430389},
	doi = {10.1109/TMC.2024.3430389},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuCP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic anomaly detection in road networks is vital for patrol participants to adaptively optimize patrol routes for hybrid task scheduling. In most cases, the routine patrol routing problem comes down to offline task scheduling with a maximal coverage requirement of traffic violation hotspots. Meanwhile, emergent anomalies (e.g., accidents, crowded events) can happen with the unpredictable duration times and locations, leading to real-time requirements of online task scheduling. In practice, trading off coverage and emergency is usually inapplicable in hybrid scenarios, especially with decentralized routing decision makers in highly stochastic traffic environments. In this paper, we propose a Crowd-enhanced hybrid task scheduling framework with the balance of Coverage and Emergency, called CCE-patrol. CCE-patrol consists of offline violation hotspot patrolling, online emergency monitoring. For the offline subproblem, a contact-based integer linear problem modeling is proposed to adaptively find optimal patrol routes. Guided by offline routing, we identify and incorporate online emergent anomalies into traffic data completion for urban crowdsensing, with up-to-date importance estimation by multi-agent cooperative reinforcement learning. The secretary problem can be utilized to select important monitoring tasks in an online manner. Extensive experiments show the superiority of our proposals in comparison with baseline algorithms (i.e., HASH (Sun et al. 2022)) by 21.08\\%\nimprovement.}
}


@article{DBLP:journals/tmc/HanLZZZAA24,
	author = {Shoufei Han and
                  Xiaojing Liu and
                  MengChu Zhou and
                  Kun Zhu and
                  Liang Zhao and
                  Aiiad Albeshri and
                  Abdullah Abusorrah},
	title = {Joint Association, Deployment and Flight Trajectory Optimization for
                  Multi-UAV-Enabled Large-Scale Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13207--13221},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3426945},
	doi = {10.1109/TMC.2024.3426945},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanLZZZAA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work investigates how multiple unmanned aerial vehicles (UAVs) assist the large-scale IoT devices (its count \\geq 100) in the edge computing system in accomplishing their tasks. The UAVs serve the latter as edge servers, and fly to footholds to collect task data from the latter, execute tasks locally and return results to the latter. The goal of this work is to minimize overall energy consumption by jointly optimizing the association between each UAV and ground-based IoT devices, deployments of UAVs, and their flight trajectories. To achieve this, this work proposes a joint optimization approach (JOA). It has three parts: 1) an improved k-means method is designed to handle the association between each UAV and ground-based IoT devices, where the number of clusters is equal to that of UAVs, which means that each UAV is responsible for the IoT devices within a cluster; 2) for the deployments of UAVs, an improved fireworks algorithm (IFWA) with variable-length encoding strategy and population size update strategy is proposed to optimize the number and locations of footholds of each UAV, where each member of the population symbolizes a UAV foothold, and each firework and its offspring are considered as the deployment of UAV. Also, the population size update strategy is employed to dynamically change the number of footholds; and 3) regarding UAV flight trajectory, a pre-computed greedy algorithm based on the footholds of UAVs obtained by IFWA is proposed to minimize the total UAV distance. The proposed approach is verified on ten large-scale instances, and the results demonstrate its effectiveness in achieving minimal energy consumption when compared to other state-of-the-art methods.}
}


@article{DBLP:journals/tmc/TanLXGAHWXL24,
	author = {Xinrui Tan and
                  Hongjia Li and
                  Xiaofei Xie and
                  Lu Guo and
                  Nirwan Ansari and
                  Xueqing Huang and
                  Liming Wang and
                  Zhen Xu and
                  Yang Liu},
	title = {Reinforcement Learning Based Online Request Scheduling Framework for
                  Workload-Adaptive Edge Deep Learning Inference},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13222--13239},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3429571},
	doi = {10.1109/TMC.2024.3429571},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TanLXGAHWXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent advances of deep learning in various mobile and Internet-of-Things applications, coupled with the emergence of edge computing, have led to a strong trend of performing deep learning inference on the edge servers located physically close to the end devices. This trend presents the challenge of how to meet the quality-of-service requirements of inference tasks at the resource-constrained network edge, especially under variable or even bursty inference workloads. Solutions to this challenge have not yet been reported in the related literature. In the present paper, we tackle this challenge by means of workload-adaptive inference request scheduling: in different workload states, via adaptive inference request scheduling policies, different models with diverse model sizes can play different roles to maintain high-quality inference services. To implement this idea, we propose a request scheduling framework for general-purpose edge inference serving systems. Theoretically, we prove that, in our framework, the problem of optimizing the inference request scheduling policies can be formulated as a Markov decision process (MDP). To tackle such an MDP, we use reinforcement learning and propose a policy optimization approach. Through extensive experiments, we empirically demonstrate the effectiveness of our framework in the challenging practical case where the MDP is partially observable.}
}


@article{DBLP:journals/tmc/LiLZZZHJLOSS24,
	author = {Ruoyu Li and
                  Qing Li and
                  Qingsong Zou and
                  Dan Zhao and
                  Xiangyi Zeng and
                  Yucheng Huang and
                  Yong Jiang and
                  Feng Lyu and
                  Gaston Ormazabal and
                  Aman Singh and
                  Henning Schulzrinne},
	title = {IoTGemini: Modeling IoT Network Behaviors for Synthetic Traffic Generation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13240--13257},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3426600},
	doi = {10.1109/TMC.2024.3426600},
	timestamp = {Tue, 14 Jan 2025 14:24:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLZZZHJLOSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Synthetic traffic generation can produce sufficient data for model training of various traffic analysis tasks for IoT networks with few costs and ethical concerns. However, with the increasing functionalities of the latest smart devices, existing approaches can neither customize the traffic generation of various device functions nor generate traffic that preserves the sequentiality among packets as the real traffic. To address these limitations, this paper proposes IoTGemini, a novel framework for high-quality IoT traffic generation, which consists of a Device Modeling Module and a Traffic Generation Module. In the Device Modeling Module, we propose a method to obtain the profiles of the device functions and network behaviors, enabling IoTGemini to customize the traffic generation like using a real IoT device. In the Traffic Generation Module, we design a Packet Sequence Generative Adversarial Network (PS-GAN), which can generate synthetic traffic with high fidelity of both per-packet fields and sequential relationships. We set up a real-world IoT testbed to evaluate IoTGemini. The experiment result shows that IoTGemini can achieve great effectiveness in device modeling, high fidelity of synthetic traffic generation, and remarkable usability to downstream tasks on different traffic datasets and downstream traffic analysis tasks.}
}


@article{DBLP:journals/tmc/SunQDDLZWG24,
	author = {Hao Sun and
                  Yuben Qu and
                  Chao Dong and
                  Haipeng Dai and
                  Zhenhua Li and
                  Lei Zhang and
                  Qihui Wu and
                  Song Guo},
	title = {All-Sky Autonomous Computing in {UAV} Swarm},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13258--13274},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427420},
	doi = {10.1109/TMC.2024.3427420},
	timestamp = {Sat, 30 Nov 2024 21:08:16 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunQDDLZWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) play an essential role in emergency cases and adverse environments for applications like disaster detection and mine exploration. To process the massive volume of sensing data generated by various sensory payloads in these applications, existing works either compress deep learning (DL) models to conduct onboard computing, or offload raw data back to the resourceful ground station with the help of relay UAVs due to base station damage. However, the former sacrifices the inference accuracy of DL models (up to 10% accuracy loss), while the latter achieves high accuracy at the cost of significant latency, due to limited wireless communication resources in the multi-hop transmission. To address the problem, exploiting the resources of the UAV swarm including both task UAVs and relay UAVs, we build up an all-sky autonomous computing (ASAP) system to autonomously conduct collaborative computing in the swarm, to achieve both high accuracy and low latency of sensing data processing. In detail, we first propose a novel UAV swarm-native collaborative computing architecture, considering the general hierarchy and clustering structure of UAV swarms, as well as the characteristic of DL model execution. We then design an elastic efficient task scheduler to allocate computing tasks for UAVs, and update the scheduling scheme online when some UAVs are unavailable, with the aid of a lightweight and accurate DL inference performance predictor. Finally, we design an adaptive inter-UAV data compressor, to adapt to the limited and dynamic communication resources between UAVs. Experiment results on 24 airborne computers and five real-world UAVs show that, the proposed system can perform collaborative computing in a timely manner and effectively deal with situations when some UAVs become unavailable.}
}


@article{DBLP:journals/tmc/LiuBYNYCZ24,
	author = {Yufei Liu and
                  Yuanguo Bi and
                  Xiaoming Yuan and
                  Dusit Niyato and
                  Kaiqi Yang and
                  Xiangyi Chen and
                  Liang Zhao},
	title = {A Novel Multimodal Long-Term Trajectory Prediction Scheme for Heterogeneous
                  User Behavior Patterns},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13275--13291},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427862},
	doi = {10.1109/TMC.2024.3427862},
	timestamp = {Mon, 24 Feb 2025 11:31:02 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuBYNYCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prediction of user trajectories is a fundamental component to support urban traffic management and various advanced transportation applications, such as traffic optimization and location-based services. Trajectory data typically contains multiple behavioral patterns and contexts, including different travel purposes, modes of transportation, time intervals, and geographic regions. These complex factors collectively influence the prediction of user trajectories. However, trajectory prediction models face challenges in effectively distinguishing between these various patterns. In this paper, we propose a novel stack Transformer-based multimodal long-term trajectory prediction (SMTTP) scheme for heterogeneous user behavior patterns. First, a learnable trajectory similarity measure method is proposed to estimate the relative distance between multi-attribute variable-length trajectories. Then, to address the instability of trajectory clustering caused by random initialization, a cluster head initialization algorithm based on high confidence nodes is developed to improve clustering stability and reduce convergence time. In addition, a Transformer-based trajectory prediction model with multi-dimensional feature fusion is proposed to achieve accurate and efficient long-term trajectory prediction. Experimental results on the real telecom dataset in Shanghai, China show that the proposed SMTTP scheme can achieve improved performance in trajectory prediction in terms of prediction error, and also has high accuracy and stability in unsupervised trajectory clustering.}
}


@article{DBLP:journals/tmc/LiBXWLZYY24,
	author = {Yingxi Li and
                  Xiaowei Bai and
                  Liang Xie and
                  Xiaodong Wang and
                  Feng Lu and
                  Feitian Zhang and
                  Ye Yan and
                  Erwei Yin},
	title = {Real-Time Gaze Tracking via Head-Eye Cues on Head Mounted Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13292--13309},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425928},
	doi = {10.1109/TMC.2024.3425928},
	timestamp = {Wed, 04 Dec 2024 07:33:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiBXWLZYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Gaze is a crucial element in human-computer interaction and plays an increasingly vital role in promoting the adoption of head-mounted devices (HMDs). Existing gaze tracking methods for HMDs either demand user calibration or face challenges in balancing accuracy and speed, compromising the overall user experience. In this paper, we introduce a novel strategy for real-time, calibration-free gaze tracking using joint head-eye cues on HMDs. Initially, we create a multimodal gaze tracking dataset named HE-Gaze, encompassing synchronized eye images and 6DoF head movement data, addressing a gap in the current data landscape. Statistical analyses unveil the correlation between head movements and gaze positions. Building on these insights, we introduce the hierarchical head-eye coordinated gaze tracking model (HHE-Tracker), which incorporates two lightweight branches to encode input eye images and head sequences efficiently. It combines encoded head velocity and posture features with eye features across various scales to infer gaze position. HHE-Tracker was implemented on a commercial HMD, and its performance was assessed in unconstrained scenarios. The results demonstrate the HHE-Tracker's capability to accurately estimate gaze positions in real-time. In comparison to the state-of-the-art gaze tracking algorithm, HHE-Tracker exhibits commendable accuracy (3.47^{\\circ }\n) and a 40-fold speedup (81 FPS on a Snapdragon 845 SoC).}
}


@article{DBLP:journals/tmc/ChenLLWZQ24,
	author = {Yanan Chen and
                  Peilin Li and
                  Ang Li and
                  Dan Wu and
                  Liang Zhou and
                  Yi Qian},
	title = {Toward Low-Latency Cross-Modal Communication: {A} Flexible Prediction
                  Scheme},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13310--13324},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425733},
	doi = {10.1109/TMC.2024.3425733},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenLLWZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To ensure the users’ immersive experience in cross-modal communication, overcoming the end-to-end (E2E) latency through prediction has attracted attention and shown its superiority. However, existing prediction schemes encounter formidable challenges in the presence of multi-modal signals, primarily to adapt and satisfy the prediction requirements of diverse multi-modal services, as well as to fully exploit and effectively utilize the correlation features of multi-modal signals for precise prediction. To this end, this work presents a flexible prediction scheme for low-latency cross-modal communication. Specifically, we first propose an adaptive prediction-aware cross-modal communication framework, which reduces the delay by predicting and transmitting the future multi-modal signals in advance, and flexibly adjusts the prediction horizon to satisfy the prediction accuracy of different multi-modal services. Next, we design an information gain-assisted graph attention (IGGA) method for cross-modal signal prediction, which leverages the graph attention block to extract the intra-modal, inter-modal spatial and temporal correlation features, and effectively optimize and utilize these features with the information gain (IG), thereby facilitating precise cross-modal signal prediction. Finally, numerical experiments conducted on a self-built dataset, a public dataset, and a multi-modal acupuncture platform demonstrate the superiority of the proposed scheme in low-latency cross-modal communication.}
}


@article{DBLP:journals/tmc/LinSAFWWFW24,
	author = {Chi Lin and
                  Zhouhe Sun and
                  Asfandeyar Ahmad and
                  Xinxin Fan and
                  Yi Wang and
                  Lei Wang and
                  Xin Fan and
                  Guowei Wu},
	title = {AirWrite: An Aerial Handwriting Trajectory Tracking and Recognition
                  System With mmWave},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13325--13341},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425709},
	doi = {10.1109/TMC.2024.3425709},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinSAFWWFW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the field of human-computer interaction (HCI), handwriting trajectory tracking and recognition have attracted significant attention due to their wide range of applications. However, many existing approaches rely on handheld devices and are highly susceptible to factors such as environmental conditions, location, and writing style. To overcome these limitations, we propose AirWrite, a novel contactless aerial system for handwriting trajectory tracking and recognition using mmWave technology. We introduce a signal clipping method based on the Doppler effect caused by user actions to accurately remove non-handwriting signals in the time domain. Additionally, we analyze power variations within the signal frequency interval to determine the handwriting frequency and employ a band-pass filter to eliminate dynamic environmental noise effectively. Through extensive experiments, we demonstrate that AirWrite can precisely track handwriting trajectories in noisy environments regardless of distance, angle, handwriting speed, character size, or in the presence of obstacles. Furthermore, we present an effective handwritten character recognition method for AirWrite that recognizes alphabets, numbers, and words. AirWrite can achieve an average accuracy of over 96% with only a 34 KB small dataset within 0.15 s for recognition.}
}


@article{DBLP:journals/tmc/MengZLDZS24,
	author = {Yan Meng and
                  Yuxia Zhan and
                  Jiachun Li and
                  Suguo Du and
                  Haojin Zhu and
                  Xuemin Shen},
	title = {De-Anonymizing Avatars in Virtual Reality: Attacks and Countermeasures},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13342--13357},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3426046},
	doi = {10.1109/TMC.2024.3426046},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MengZLDZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By providing users with an immersive visual and acoustic experience, virtual reality (VR) serves as a foundational technique for the emerging metaverse. One of the most promising aspects of VR is its ability to protect users’ identities by transforming their physical appearances into avatars with arbitrary appearances in the virtual world. However, the increasing threat of de-anonymization attacks that seek to reveal users’ identities poses significant privacy risks. We propose AvatarHunter, a non-intrusive and user-unaware de-anonymization attack leveraging victims’ inherent movement signatures. AvatarHunter discreetly collects the avatar's gait information by recording videos in the VR scenario without requiring any permissions. Notably, we designed a Unity-based feature extractor that maintains the avatar's movement signature while enabling AvatarHunter to be resistant to changes in the avatar's appearance. We conduct real-world experiments on VRChat to evaluate AvatarHunter's effectiveness. The results demonstrate that in commercial settings, AvatarHunter achieves attack success rates (ASR) of 92.1% and 66.9% in closed-world and open-world avatar scenarios, respectively, significantly surpassing existing benchmarks. Additionally, simulations using an open-source dataset confirm that AvatarHunter can attain over 78% ASR in full-body tracking scenarios. Finally, we discuss several countermeasures and implement an obfuscation mechanism during the avatar rendering phase, significantly reducing the ASR.}
}


@article{DBLP:journals/tmc/ShaoYXSCX24,
	author = {Ziling Shao and
                  Helin Yang and
                  Liang Xiao and
                  Wei Su and
                  Yifan Chen and
                  Zehui Xiong},
	title = {Deep Reinforcement Learning-Based Resource Management for UAV-Assisted
                  Mobile Edge Computing Against Jamming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13358--13374},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3432491},
	doi = {10.1109/TMC.2024.3432491},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ShaoYXSCX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mobile edge computing (MEC) systems, multiple unmanned aerial vehicles (UAVs) can be utilized as aerial servers to provide computing, communication, and storage services for edge users, called UAV-assisted MEC, which has emerged as a promising technology to improve both the computing and communication performances. Unlike existing works without considering jamming attacks, we investigate a multi-UAV-assisted-MEC scenario under multiple malicious jammers and then propose a resource management approach with the objective of minimizing both the system energy consumption and latency. Due to the time-varying nature of communication environments, we design a multi-agent deep reinforcement learning (MADRL)-based resource management approach to dynamically adjust the CPU frequency, communication bandwidth, and channel access selection of UAVs to enhance the system performance against jamming attacks. On this basis, in order to enhance the algorithm learning efficiency, we propose a multi-agent twin-delayed deep deterministic policy algorithm in combination with the prioritized experience replay mechanism (PER-MATD3) to effectively search for the joint resource management strategy under high-dimensional state and action spaces, where the time-varying channel state information and imperfect attack behavior information are also effectively trained to improve the learning capacity and convergence speed. Simulation and experimental results verify that the proposed approach can significantly decrease the overall system latency (i.e., computing and communication latency) and energy consumption compared to other benchmark algorithms under different real-world settings.}
}


@article{DBLP:journals/tmc/QiuCHLLHN24,
	author = {Yu Qiu and
                  Min Chen and
                  Hebin Huang and
                  Weifa Liang and
                  Junbin Liang and
                  Yixue Hao and
                  Dusit Niyato},
	title = {Spotlighter: Backup Age-Guaranteed Immersive Virtual Vehicle Service
                  Provisioning in Edge-Enabled Vehicular Metaverse},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13375--13391},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425896},
	doi = {10.1109/TMC.2024.3425896},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QiuCHLLHN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-enabled Vehicular Metaverse (EVM) is a new paradise supported by various compute-intensive Virtual Vehicle Services (VVSs), where users can immerse and enjoy their spiritual world. User immersion is critical during VVS provisioning in the EVM, yet it can be weakened or curtailed by a sense of disengagement caused by unknown failures. Providing redundant backups VVSs (BVVSs) and keeping the Age of Backup Information (AoBI) could effectively resist and avoid this disengagement when failures occur. However, the trajectories of mobile vehicles are unknown and dynamic, which makes it challenging to optimally migrate VVSs and BVVSs or adjust the update frequency of backup information in real-time, so as to ensure service reliability and AoBI while minimizing the cost of accepting VVS-based metaverse services. In this paper, the above long-term issue is first decomposed into discrete single-slot sub-problems that are modeled as integer linear programming problems. Then, a comprehensive resource explorer named spotlighter is designed, where the first and second parts are a metaverse service home prediction algorithm based on deep learning and a VVS migration algorithm based on randomized rounding, respectively. By tracking the dynamical locations of service homes based on current and historical information, the former can help the latter to adaptively minimize migration costs on VVS re-instantiation and traffic transmission among services and moving vehicles. Finally, a cost-adaptive AoBI guarantee algorithm is merged in spotlighter to ensure the freshness of backup status, by trading-off synchronization cost on BVVS migration, backup update, and backup synchronization. Theoretical analyses and experiments based on real databases show that our algorithms are promising compared with baseline algorithms.}
}


@article{DBLP:journals/tmc/BoljanovicSC24,
	author = {Veljko Boljanovic and
                  Shamik Sarkar and
                  Danijela Cabric},
	title = {Joint User Association and Beam Scheduling With Interference Management
                  in Dense Millimeter-Wave Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13392--13405},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3426657},
	doi = {10.1109/TMC.2024.3426657},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BoljanovicSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hybrid arrays enable millimeter-wave (mmW) base stations and users to steer multiple beams simultaneously. However, in dense mmW networks with small inter-site distances and many users, a large number of serving beams can lead to significant inter- and intra-cell interference that prevents data-hungry users from satisfying their rate requirements. In this work, we address this problem by designing a linear multi-step optimization framework for user association and beam scheduling with interference management. In the first step, the framework aims to maximize the number of users with fully satisfied rate requirements by scheduling a minimal number of non-interfering beams. In the second step, any remaining non-interfering beams are distributed among other users to maximize the number of them with at least partially satisfied requirements. Hybrid precoders and combiners are then designed to remove any excess sidelobe interference among the scheduled beams. Finally, power allocation is optimized on a network level to boost the data rates of the partially satisfied users. Given that the framework includes NP-hard optimization problems, we propose an algorithm that attains a sub-optimal solution in polynomial-time. The proposed framework is numerically evaluated in realistic mmW channels and the results reveal its advantages over the baseline user association schemes.}
}


@article{DBLP:journals/tmc/LiuW24,
	author = {Gaosheng Liu and
                  Lin Wang},
	title = {Data on the Go: Seamless Data Routing for Intermittently-Powered Battery-Free
                  Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13406--13419},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3429636},
	doi = {10.1109/TMC.2024.3429636},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rising demand for sustainable IoT has promoted the adoption of battery-free devices intermittently powered by ambient energy for sensing. However, the intermittency poses significant challenges in sensing data collection. Despite recent efforts to enable one-to-one communication, routing data across multiple intermittently-powered battery-free devices, a crucial requirement for a sensing system, remains a formidable challenge. This paper fills this gap by introducing Swift, which enables seamless data routing in intermittently-powered battery-free sensing systems. Swift overcomes the challenges posed by device intermittency and heterogeneous energy conditions through three major innovative designs. First, Swift incorporates a reliable node synchronization protocol backed by number theory, ensuring successful synchronization regardless of energy conditions. Second, Swift adopts a low-latency message forwarding protocol, allowing continuous message forwarding without repeated synchronization. Finally, Swift features a simple yet effective mechanism for routing path construction, enabling nodes to obtain the optimal path to the sink node with minimum hops. We implement Swift and perform large-scale experiments representing diverse real-world scenarios. The results demonstrate that Swift achieves an order of magnitude reduction in end-to-end message delivery time compared with the state-of-the-art approaches for intermittently-powered battery-free sensing systems.}
}


@article{DBLP:journals/tmc/WangLGYZ24,
	author = {Ziqi Wang and
                  Sicong Liu and
                  Bin Guo and
                  Zhiwen Yu and
                  Daqing Zhang},
	title = {CrowdLearning: {A} Decentralized Distributed Training Framework Based
                  on Collectives of Trusted AIoT Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13420--13437},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427636},
	doi = {10.1109/TMC.2024.3427636},
	timestamp = {Fri, 14 Feb 2025 11:54:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLGYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of Artificial Intelligence of Things (AIoT), integrating deep neural networks (DNNs) into mobile and embedded devices has become a significant trend, enhancing the data collection and analysis capabilities of IoT devices. Traditional integration paradigms rely on cloud-based training and terminal deployment, but they often suffer from delayed model updates, decreased accuracy, and increased communication overhead in dynamic real-world environments. Consequently, on-device training methods have garnered research focus. However, the limited local perception data and computational resources pose bottlenecks to training efficiency. To address these challenges, Federated Learning emerged but faces issues such as slow model convergence and reduced accuracy due to data privacy concerns that restrict sharing data or model details. In contrast, we propose the concept of trusted clusters in the real world (such as personal devices in smart spaces, trusted devices from the same organization/company, etc.), where devices in trusted clusters focus more on computational efficiency and can also share privacy. We propose CrowdLearning, a decentralized distributed training framework based on trusted AIoT device collectives. This framework comprises two collaborative modules: A heterogeneous resource-aware task offloading module aimed at alleviating training latency bottlenecks, and an efficient communication data reallocation module responsible for determining the timing, manner, and recipients of data transmission, thereby enhancing DNN training efficiency and effectiveness. Experimental results demonstrate that in various scenarios, CrowdLearning outperforms existing federated learning and distributed training baselines on devices, reducing training latency by 55.8% and lowering communication costs by 67.1%.}
}


@article{DBLP:journals/tmc/RenWSGSZCGYZZ24,
	author = {Zhihui Ren and
                  Zhu Wang and
                  Zhuo Sun and
                  Yifan Guo and
                  Wenchao Song and
                  Hualei Zhang and
                  Chao Chen and
                  Bin Guo and
                  Zhiwen Yu and
                  Xingshe Zhou and
                  Daqing Zhang},
	title = {Characterizing the Through-Wall Sensing Mechanism of Wi-Fi Signals
                  With a Refraction-Aware Fresnel Zone Model},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13438--13454},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425847},
	doi = {10.1109/TMC.2024.3425847},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RenWSGSZCGYZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {During the last decade, there have been lots of efforts on wireless sensing using Wi-Fi signals, which can be divided into two categories, i.e., the pattern-based approach and the model-based approach. Recently, more and more attention has been paid on the model-based approach, mainly due to its superiority of no need for collecting a large dataset or retraining the model for new environments. However, existing models are mainly designed for Line-of-Sight (LoS) scenarios, which are not applicable to Non-Line-of-Sight (NLoS) scenarios, such as through-wall sensing. To bridge this gap, we put forward a through-wall wireless sensing model to reveal the sensing mechanism of Wi-Fi signals in NLoS scenarios. In particular, a refraction-aware Fresnel zone model is developed by taking into account both the reflection propagation and the refraction propagation of Wi-Fi signals. For the first time, we discover that the geometric distribution of Fresnel zones becomes uneven, due to the difference in dielectric constants between the air and the wall. Specifically, some areas become denser and other areas become sparser, leading to the squeeze effect and stretch effect of Fresnel zones. Inspired by the insight, we further put forward a new metric named compression-ratio to quantify the through-wall sensing capability of Wi-Fi signals. Meanwhile, a set of algorithms are developed to guide the deployment of Wi-Fi sensing systems. To validate the proposed model, we implement a through-wall respiration sensing prototype system. Experiments show that the respiration detection performance varies significantly when the user locates in different areas. Specifically, for two sensing locations (one in the compression area and the other in the expansion area) symmetrically distributed on both sides of the transceivers’ connection line, the difference in mean absolute errors (MAE) can exceed 3 times.}
}


@article{DBLP:journals/tmc/Fan24a,
	author = {Wenhao Fan},
	title = {Hybrid Deep Reinforcement Learning-Based Task Offloading for D2D-Assisted
                  Cloud-Edge-Device Collaborative Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13455--13471},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3425723},
	doi = {10.1109/TMC.2024.3425723},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/Fan24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In D2D (Device to Device)-assisted cloud-edge-device collaborative networks, the tasks of a busy device can be processed locally, offloaded to an idle device through D2D transmission, offloaded to the ES (Edge Server) of the affiliated BS (Base Station), further to the ES of another BS through ES-ES transmission, or further to the CS (Cloud Server) through ES-CS transmission. However, existing works did not fully consider both the D2D task offloading and cloud-edge-device collaboration. Moreover, the very high complexity of the joint resource optimization problem makes it extremely challenging to be solved efficiently by relying solely on numerical methods or machine-learning-based methods. In this paper, we propose a task offloading scheme to minimize the total system cost considering the time and energy consumption of all the devices. The task offloading decision, transmission power allocation, transmission rate allocation, and computational resource allocation are jointly optimized. We design a DRL (deep reinforcement learning)-based algorithm to solve the optimization problem efficiently through a hybrid approach, which decomposes the problem into sub-problems, and then jointly leverages an SD3 (Softmax Deep Double Deterministic Policy Gradients)-based DRL method to handle the task offloading sub-problem and uses multiple numerical methods to handle the other small-scale sub-problems. Extensive simulations are conducted in 7 scenarios. The superiority of our scheme is demonstrated in comparison with 4 reference schemes.}
}


@article{DBLP:journals/tmc/HashemianA24,
	author = {Seyed Ali Hashemian and
                  Farid Ashtiani},
	title = {Analytical Modeling and Improvement of Interference-Coupled {RAN}
                  Slicing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13472--13486},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3431194},
	doi = {10.1109/TMC.2024.3431194},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HashemianA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing, a key component of 5G, enables simultaneously running incompatible service types on a common infrastructure. Inter-slice isolation, as a key requirement of slicing, ensures that slice activity, i.e., containing a flow under transmission, does not affect the activity of other slices. Isolation in radio access network (RAN) slices is challenging due to interaction between slices. In fact, due to direct or indirect overlap among frequency channels in RAN slices, interference in inevitable, leading to interaction. In this paper, we propose an analytical model to analyze interference-coupled multi-cell RAN slicing where the interaction among slices results in dynamic behavior of slices. To this end, we map our scenario onto a suitable state-dependent queueing network, propose an iterative algorithm to obtain approximately the network steady-state probability distribution, and derive conventional QoS metrics (average delay and throughput). To quantify isolation, we define some new key performance indicators (KPIs) that show how changes in interfering slices affect the QoS metrics. Finally, we propose an interference-aware slice channel allocation policy that significantly reduces overlapping frequency channels. Numerical results demonstrate the accuracy of our analysis and the efficacy of the proposed policy in improving isolation-based KPIs compared to some other allocation policies.}
}


@article{DBLP:journals/tmc/ModekurthyIRS24,
	author = {Venkata P. Modekurthy and
                  Dali Ismail and
                  Mahbubur Rahman and
                  Abusayeed Saifullah},
	title = {Extending Coverage Through Integrating Multiple Low-Power Wide-Area
                  Networks: {A} Latency Minimizing Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13487--13504},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3428989},
	doi = {10.1109/TMC.2024.3428989},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ModekurthyIRS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Industrial and agricultural Internet of Things (IoT) are emerging in very large-scale and wide-area applications (e.g., oil-field management, smart farming) that may spread over hundreds of square miles (e.g., 45 mi × 12 mi East Texas Oil-field). Although a single Low-Power Wide-Area Network (LPWAN) covers several miles, it faces coverage challenge in such extremely large-area IoT applications, especially in rural or remote areas with no/limited infrastructure, requiring an in-band integration of multiple LPWANs. We consider a seamless integration of multiple SNOW LPWANs. SNOW (Sensor Network Over White spaces) is an LPWAN architecture over the TV white spaces, avoiding overcrowding problems in the limited ISM band and the cost of licensed band and infrastructure. It offers high scalability through concurrent and bi-directional communication between a base station and numerous nodes. Existing integration of multiple SNOW LPWANs does not consider minimizing network latency and is less suitable for delay-sensitive or real-time applications. In this work, we propose the first latency-minimizing scalable in-band integration of multiple SNOWs. Considering the impact of bandwidth on latency and base station power dissipation, low-latency integration of multiple SNOWs as a constrained spectrum allocation problem is formulated. A novel greedy latency- and traffic- aware spectrum allocation to allocate each link's bandwidth is proposed, achieving an integrated network. To enable low-latency integration, we propose two medium access control protocols for multiple SNOWs, RI-TDMA and TDMA, and estimate their latency. We have implemented the proposed integration both on SNOW hardware and in NS-3 simulator. The physical experiments show up to 44% reduction in the maximum network latency under our approach compared to existing approach. The simulation results show at least 62.3% reduction of maximum network latency by the proposed approach with RI-TDMA and 86.7% with TDMA.}
}


@article{DBLP:journals/tmc/CaiZLQS24,
	author = {Qing Cai and
                  Yiqing Zhou and
                  Ling Liu and
                  Yanli Qi and
                  Jinglin Shi},
	title = {Prioritized Assignment With Task Dependency in Collaborative Mobile
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13505--13521},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427380},
	doi = {10.1109/TMC.2024.3427380},
	timestamp = {Sat, 07 Dec 2024 15:55:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaiZLQS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative mobile edge computing enables resource-constrained edge facilities to work cooperatively for computation-intensive tasks. However, as the number of tasks demanded by various applications increases, resource competition is inevitable in edge facilities. Existing works tackle the resource competition problem with a first come first served (FCFS) scheme, which is blind to different delay requirements among tasks. This may result in tasks with higher delay requirements waiting a long time for service, thereby reducing overall service quality. This paper proposes a prioritized queuing scheme with task dependency (PQTD), which allows high-prioritized sub-tasks with higher delay requirements to jump into the queue ahead of low-prioritized sub-tasks with lower delay requirements. To describe the complicated delay change caused by queue-jumping, a joint DAG-queue delay (JDQD) model is proposed, which analyzes the chain reaction of delay changes caused by the processing queue on the server and the task dependency. With JDQD, a multi-task assignment optimization problem is formulated to maximize the average satisfaction degree (AvgSatD), which is defined according to the priorities of the tasks and their delay requirements. Then, a tree-based algorithm is proposed to solve the NP-hard optimization problem, i.e., Monte Carlo Tree Search (MCTS). Simulation results demonstrate the effectiveness of the PQTD queuing scheme and tree search mechanism of MCTS. Overall, PQTD + MCTS can increase AvgSatD by at least 45.8% with an acceptable complexity.}
}


@article{DBLP:journals/tmc/ChenHSWH24,
	author = {Wei Chen and
                  Ru Huo and
                  Chuang Sun and
                  Shuo Wang and
                  Tao Huang},
	title = {Efficient and Non-Repudiable Data Trading Scheme Based on State Channels
                  and Stackelberg Game},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13522--13538},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3429501},
	doi = {10.1109/TMC.2024.3429501},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChenHSWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the Internet of Things gathers pace and popularity, more and more data is collected at the edge. To unleash the value of data and make it tradable, data markets have been proposed. However, existing data markets generally depend on broker or blockchain, which inevitably raises concerns about one or more aspects of fairness, security, or efficiency. In addition, to promote data trading in the data market, a data trading incentive mechanism is also essential. In this paper, we propose a novel data trading scheme based on state channels and Stackelberg game. First, we propose a State Channels-based Data Trading (SCDT) framework to support non-repudiable and efficient data trading. The framework can arbitrate disputes arising from off-chain data trading through state channels, enabling traders to conduct efficient transactions off-chain without worrying about security issues. Second, we propose an optimal incentive mechanism to solve the pricing and purchasing problems. The tripartite interactions among the data seller, resource seller, and user service platform are formulated as a Stackelberg game to maximize the profits of all participants. Finally, we implement the data trading framework and analyze the incentive mechanism, which reveals the feasibility of the framework and the rationality of the incentive mechanism.}
}


@article{DBLP:journals/tmc/SuWWATTZ24,
	author = {Weijian Su and
                  Pengfei Wang and
                  Zhiyi Wang and
                  Muhammad Ameen and
                  Tiwei Tao and
                  Xiangrong Tong and
                  Qiang Zhang},
	title = {{F2UL:} Fairness-Aware Federated Unlearning for Data Trading},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13539--13555},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3429228},
	doi = {10.1109/TMC.2024.3429228},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SuWWATTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) offers a credible solution for distributed data trading since it could train machine learning models in a distributed manner thereby enhancing data privacy without sharing local data. However, it is still challenging to trade data through FL due to unfair model allocation issues arising from the unreliability of user-provided data. To tackle it, we propose F2UL, a Fairness-aware Federated UnLearning solution that distributes models to users commensurate with their data quality. F2UL is trained in a two-stage (TST) way and contains three main components: 1) Label-free model quality assessment (LMQA) promotes fairness by evaluating models without user-specific data, ensuring uniform assessment standards. 2) Fair model distribution (FMD) addresses the issue of unfair model distribution by allocating models with feature mapping deviation, ensuring that users who contribute low-quality models do not receive enhanced models. 3) User data federated unlearning (UDFU) ensures fairness in model distribution by employing rapid recovery federated unlearning, safeguarding regular users from the adverse effects of low-quality data on model performance. In experiments on CIFAR10, F2UL reduces the low-quality data user accuracy to 9.09% and increases regular users’ accuracy by 3.02%, thereby demonstrating F2UL's capacity to ensure fairness in data trading.}
}


@article{DBLP:journals/tmc/QianZHLYHB24,
	author = {Hui Qian and
                  Liang Zhao and
                  Ammar Hawbani and
                  Zhi Liu and
                  Keping Yu and
                  Qiang He and
                  Yuanguo Bi},
	title = {Collaborative Overtaking Strategy for Enhancing Overall Effectiveness
                  of Mixed Connected and Connectionless Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13556--13572},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427677},
	doi = {10.1109/TMC.2024.3427677},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/QianZHLYHB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent Transportation Systems (ITS) aim to enhance traffic management by improving connectivity and data sharing among vehicles and road infrastructure. In a Mixed Connected and Connectionless Vehicles (MCCV) scenario consisting of connected vehicles equipped with On-Board Units (OBUs) and non-connected vehicles lacking OBUs, communication disparities create challenges in critical lane-changing overtaking decisions. These discrepancies hinder the adaptation of fully connected scenarios to dynamic interactions among these different types of vehicles. Considering the diversity in decision-making ways and capabilities of non-connected vehicles in MCCV scenarios, ensuring the coordinated execution of safe and efficient lane-changing overtaking maneuvers by multiple connected vehicles is crucial for enhancing traffic efficiency. Therefore, we propose a collaborative strategy to facilitate safer and more efficient lane-changing overtaking maneuvers for connected vehicles in the MCCV scenario. First, we design a multi-criteria priority detection, and a dynamic event-triggered mechanism based on confidence intervals to foster efficient collaboration among connected vehicles, optimizing decision-making and reducing conflicts. Second, to accommodate diverse driving styles of autonomous and human-driven vehicles, we introduce an Improved Dynamic Precise Fuzzy C-Means (IDP-FCM) algorithm to dynamically identify and adapt to different driving styles, thereby improving safety. Finally, tackling the challenge of multiple connected vehicles performing lane-changing overtaking involving hybrid action space, our proposed Multi-agent Contrastive Parameterized Dueling Deep Q-Network (MCPDDQN) algorithm incorporates contrastive learning to improve strategy stability in complex driving scenarios. Experimental results demonstrate the effectiveness of our strategy in improving road safety and traffic efficiency of the MCCV scenario.}
}


@article{DBLP:journals/tmc/YanSLT24,
	author = {Huanyu Yan and
                  Chenxi Sun and
                  Huanxin Liao and
                  Xiaoying Tang},
	title = {Optimal Pricing and Charging Strategy Design for Non-Cooperative Battery
                  Swapping Stations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13573--13588},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3427784},
	doi = {10.1109/TMC.2024.3427784},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YanSLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Battery swapping is a rapid way to recharge electric vehicles (EVs). As more and more entities are involved in building Battery Swapping Stations (BSSs), how non-cooperative BSSs maximize their profit in a competitive market needs further investigation. In this paper, we focus on a practical scenario where competitive BSSs are coordinated by the same aggregator. To study the optimal pricing and battery charging, we formulate a hierarchical game model, where BSSs determine the swapping price in the day-ahead market in the first stage, and then determine the optimal battery charging strategy in the real-time market in the second stage. We rigorously prove the existence and uniqueness of the Subgame Perfect Nash Equilibrium (SPNE). In particular, the uniqueness property provides theoretical support that the strategy under equilibrium is optimal in the competitive environment. Based on the unique SPNE, we propose an optimal pricing and charging strategy for each BSS to maximize profit in the competitive market. A prediction error handling method is also proposed to deal with unexpected fluctuations in swapping demand. Our simulation with a 12-BSS system based on real-life data from Xi’an, China shows that our pricing and charging strategy increases the individual BSS profit by at least 18.1%.}
}


@article{DBLP:journals/tmc/ZhaoTLLWRL24,
	author = {Lei Zhao and
                  Guanghua Tan and
                  Jiewen Lai and
                  Chwee Ming Lim and
                  Weng Kin Wong and
                  Hongliang Ren and
                  Kenli Li},
	title = {Needle Trajectory Prediction for Percutaneous Kidney Biopsy in 5G-Powered
                  Teleultrasound Navigation System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13589--13602},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3435040},
	doi = {10.1109/TMC.2024.3435040},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoTLLWRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Needle insertion is a critical component of many remote surgical procedures, including biopsies, injections, neurosurgery, and brachytherapy cancer treatments. However, precise visualization of the biopsy needle trajectory remains challenging due to specular reflection, speckle noise, and needle-like anatomical features. This paper proposes a visual feedback prediction framework for ultrasound-assisted percutaneous kidney biopsy in 5G remote surgery, aiming to enhance operator confidence, reduce procedure time, and minimize the risk of unintended bleeding. Building upon this framework, we design a Lightweight-Accuracy Needle Trajectory Prediction (LA-NTP) model by minimizing the backbone and optimizing the multi-module prediction process, incorporating innovative training strategies (i.e., angle-aware geometric and trajectory augmentation losses). The experimental results demonstrate that it achieves competitive performance with only 20.3% of the model size of the previous best real-time method and a 3.7-fold increase in inference speed. Even in challenging scenarios involving large insertion depths and steep angles, our method provides stable and precise navigation.}
}


@article{DBLP:journals/tmc/ZhangZDYDYZ24,
	author = {Xiaoya Zhang and
                  Yuyang Zhang and
                  Ping Dong and
                  Dong Yang and
                  Xiaojiang Du and
                  Chengxiao Yu and
                  Hongke Zhang},
	title = {{TA2LS:} {A} Traffic-Aware Multipath Scheduler for Cost-Effective
                  QoE in Dynamic HetNets},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13603--13620},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3440415},
	doi = {10.1109/TMC.2024.3440415},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZDYDYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath transmission is a critical enabling technology to enhance QoE for edge users. The packet scheduler plays an irreplaceable role in overcoming heterogeneity and dynamicity in multipath transmission. However, current schedulers depend on an inaccurate delay estimation and lack systematic traffic intensity awareness, performing poorly in wireless heterogeneous networks (HetNets). In this paper, we propose a novel traffic-aware two-level packet scheduler (TA2LS) to address the problem and improve aggregated bandwidth while trading off delay. In particular, we design a multipath transmission state machine (MTSM) to perceive link traffic intensity. MTSM replaces network prediction algorithms by identifying the contribution of each link in multipath transmission in a cost-effective way. Further, we propose a scheduling mechanism based on a two-level optimal-path evaluation method (2LOSM) to adjust the packet scheduling policy adaptively. 2LOSM increases the priority of links with low traffic intensity during scheduling, improving aggregated bandwidth performance and reducing end-to-end delay. We have built a real-world 4G/5G/WiFi testbed and deployed 47 dynamic scenarios to evaluate TA2LS and other five schedulers. In 4G/5G/WiFi scenarios, TA2LS improves aggregated bandwidth by 10.32%–48.27% compared to the second-best scheduler and reduces end-to-end delay by 5.04%–39.98% under the premise of fewer or equivalent overheads.}
}


@article{DBLP:journals/tmc/WangYHYYSQWZ24,
	author = {Pengfei Wang and
                  Hao Yang and
                  Guangjie Han and
                  Ruiyun Yu and
                  Leyou Yang and
                  Geng Sun and
                  Heng Qi and
                  Xiaopeng Wei and
                  Qiang Zhang},
	title = {Decentralized Navigation With Heterogeneous Federated Reinforcement
                  Learning for UAV-Enabled Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13621--13638},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439696},
	doi = {10.1109/TMC.2024.3439696},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangYHYYSQWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicle (UAV)-enabled mobile edge computing has been proposed as an efficient task-offloading solution for user equipments (UEs). Nevertheless, the presence of heterogeneous UAVs makes centralized navigation policies impractical. Decentralized navigation policies also face significant challenges in knowledge sharing among heterogeneous UAVs. To address this, we present the soft hierarchical deep reinforcement learning network (SHDRLN) and dual-end federated reinforcement learning (DFRL) as a decentralized navigation policy solution. It enhances overall task-offloading energy efficiency for UAVs while facilitating knowledge sharing. Specifically, SHDRLN, a hierarchical DRL network based on maximum entropy learning, reduces policy differences among UAVs by abstracting atomic actions into generic skills. Simultaneously, it maximizes the average efficiency of all UAVs, optimizing coverage for UEs and minimizing task-offloading waiting time. DFRL, a federated learning (FL) algorithm, aggregates policy knowledge at the cloud server and filters it at the UAV end, enabling adaptive learning of navigation policy knowledge suitable for the UAV's performance parameters. Extensive simulations demonstrate that the proposed solution not only outperforms other baseline algorithms in overall energy efficiency but also achieves more stable navigation policy learning under different levels of heterogeneity of different UAV performance parameters.}
}


@article{DBLP:journals/tmc/ZhuHLT24,
	author = {Shengchao Zhu and
                  Guangjie Han and
                  Chuan Lin and
                  Qiuzi Tao},
	title = {Underwater Target Tracking Based on Hierarchical Software-Defined
                  Multi-AUV Reinforcement Learning: {A} Multi-AUV Advantage-Attention
                  Actor-Critic Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13639--13653},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437376},
	doi = {10.1109/TMC.2024.3437376},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuHLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of underwater robots, underwater communication techniques, etc., the Autonomous Underwater Vehicle (AUV) cluster network has emerged as a candidate paradigm to perform underwater civil and military applications, e.g., underwater target tracking. In this paper, we focus on how to utilize networking and multi-agent artificial intelligence technique to improve underwater target tracking. In particular, to improve the flexibility and scalability of the AUV cluster network, we employ Software-Defined Networking (SDN) and Centralized Training with Decentralized Execution (CTDE)-based Multi-Agent Reinforcement Learning (MARL) technologies, to propose a Hierarchical Software-Defined Multiple AUVs Reinforcement Learning (HSD-MARL) framework. For the MARL mechanism in HSD-MARL, we propose an advantage-attention mechanism and present the architecture of Multi-AUV Advantage-Attention Actor-Critic (MA-A3C), to address slow convergence and poor scalability issues on the AUV cluster network of large-scale. Further, to improve the utilization rate of advantage samples especially when the MA-A3C is utilized to perform AUV cluster network-based underwater tracking, we propose an ‘advantage resampling’ method based on experience replay buffer. Evaluation results showcase that our proposed approaches can perform exact underwater target tracking based on AUV cluster network systems and outperform some recent research products in terms of convergence speed, tracking accuracy, etc.}
}


@article{DBLP:journals/tmc/SangYXHSC24,
	author = {Qianlong Sang and
                  Jinqi Yan and
                  Rui Xie and
                  Chuang Hu and
                  Kun Suo and
                  Dazhao Cheng},
	title = {QoS-Aware Power Management via Scheduling and Governing Co-Optimization
                  on Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13654--13669},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438267},
	doi = {10.1109/TMC.2024.3438267},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SangYXHSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scheduling and governing are two key technologies to trade off the Quality of Service (QoS) against the power consumption on mobile devices with heterogeneous cores. However, there are still defects in the use of them, among which two of the decoupling issues are critical and need to be resolved. First, both the scheduling and governing decouple from QoS, one of the most important metrics of user experience on mobile platforms. Second, scheduling and governing also decouple from each other in mobile systems and they might weaken each other when being effective at the same time. To address the above issues, we propose Orthrus, a comprehensive QoS-aware power management approach that involves a governing approach based on deep reinforcement learning to adjust the frequency of heterogeneous cores, a scheduling algorithm based on finite state machine that assigns cores to QoS-related threads, and expert fuzzy control-based coordination mechanism between the two to manage the impact between scheduling and governing. Our proposed approach aims to minimize power consumption while guaranteeing the QoS. We implement Orthrus on Google Pixel 3 as the system service of Android and evaluate it using several widespread mobile applications. The performance evaluation demonstrates that Orthrus reduces the average power consumption by up to 35.7% compared to three state-of-the-art techniques while ensuring the QoS on mobile platforms.}
}


@article{DBLP:journals/tmc/WangLY24,
	author = {Shuoyao Wang and
                  Jiawei Lin and
                  Fangwei Ye},
	title = {Imitation Learning for Adaptive Video Streaming With Future Adversarial
                  Information Bottleneck Principle},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13670--13683},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437455},
	doi = {10.1109/TMC.2024.3437455},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive video streaming plays a crucial role in ensuring high-quality video streaming services. Despite extensive research efforts devoted to Adaptive BitRate (ABR) techniques, the current reinforcement learning (RL)-based ABR algorithms may benefit the average Quality of Experience (QoE) but suffers from fluctuating performance in individual video sessions. In this paper, we present a novel approach that combines imitation learning with the information bottleneck technique, to learn from the complex offline optimal scenario rather than inefficient exploration. In particular, we leverage the deterministic offline bitrate optimization problem with the future throughput realization as the expert and formulate it as a mixed-integer non-linear programming (MINLP) problem. To enable large-scale training for improved performance, we propose an alternative optimization algorithm that efficiently solves the formulated MINLP problem. To address the overfitting issues due to the future information leakage in MINLP, we incorporate an adversarial information bottleneck framework. By compressing the video streaming state into a latent space, we retain only action-relevant information. Additionally, we introduce a future adversarial term to mitigate the influence of future information leakage, where Model Prediction Control (MPC) policy without any future information is employed as the adverse expert. Experimental results demonstrate the effectiveness of our proposed approach in significantly enhancing the quality of adaptive video streaming, providing a 7.30% average QoE improvement and a 30.01% average ranking reduction.}
}


@article{DBLP:journals/tmc/LiuMYYL24,
	author = {Pei Liu and
                  Kai Ma and
                  Jie Yang and
                  Bo Yang and
                  Zhixin Liu},
	title = {A Secure Transmission Strategy for Smart Grid Communication Infrastructure-Assisted
                  Two Tier Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13684--13695},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443056},
	doi = {10.1109/TMC.2024.3443056},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuMYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to the openness and diversification of heterogeneous communication network, communication security becomes a pressing problem. In this paper, we consider a heterogeneous communication network in which spectrum resources are shared by electric power communication network and licensed network. First, we establish the utility companies’ cost model based on Taguchi loss function. Next, we utilize cooperative relay strategy to enhance the transmission quality and achieve high-speed information transmission in smart grids. Under the premise of ensuring high-quality transmission of electric power communication services, we propose a secure transmission strategy for information resource sharing and interference price trading that utilizes smart grid infrastructure and relay to interfere with eavesdroppers to improve the security rate of licensed user (LU), which achieves mutual benefits. Furthermore, the bernstein approximation method and the successive convex approximation are adopted to obtain the open-form expression of the constraint and transform the non-convex problem into the convex problem, respectively. A distributed robust power control algorithm is then proposed to obtain the optimal solutions. Finally, numerical results verify that the proposed secure scheme and algorithm can increase the secrecy rate at LU, reduce the total electricity cost, and improve both the profit of relay and the social welfare.}
}


@article{DBLP:journals/tmc/HuangWZDQTC24,
	author = {Rui Huang and
                  Wushao Wen and
                  Zhi Zhou and
                  Chongwu Dong and
                  Cheng Qiao and
                  Zhihong Tian and
                  Xu Chen},
	title = {Dynamic Task Offloading for Multi-UAVs in Vehicular Edge Computing
                  With Delay Guarantees: {A} Consensus ADMM-Based Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13696--13712},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437785},
	doi = {10.1109/TMC.2024.3437785},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangWZDQTC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the paradigm of forthcoming 6G network infrastructures, unmanned aerial vehicles (UAVs), functioning as principal conveyances, are projected to emerge as pivotal enablers in the nascent domain of the low-altitude economy. UAVs are poised to embrace various innovative applications, including latency-sensitive and compute-intensive services. However, UAVs are constrained by their energy capacity and computational resources, rendering them insufficient for fulfilling the increasingly rigorous service demands in the future. To address these challenges, our investigation focuses on the innovative UAV-based Vehicular Edge Computing (UVEC) framework, incorporating Vehicular Edge Computing (VEC) in UAV systems to bolster service reliability. A UAV can enhance its mission duration by dynamically selecting suitable vehicles for computation offloading and adaptively adjusting the task offloading ratio between vehicles and the edge server. By integrating vehicle selection and task offloading scheduling in the UVEC framework, we investigate the optimization of energy efficiency while satisfying the statistical delay and the buffer constraints for UAVs. To deal with the proposed problem, a distributed algorithm is designed by jointly considering the vehicle selection for task offloading radio to vehicles and the edge server. The stochastic network calculus (SNC) is employed to derive performance bounds for the statistical delay and constraints, enabling robust analysis and optimization of network performance. After that, we leverage linear transformation techniques to reformulate the original problem into a linear framework, enabling the application of the Alternating Direction Method of Multipliers (ADMM) algorithm to efficiently solve the transformed problem. Theoretical analysis and simulation results show that our algorithm converges while effectively satisfying service reliability constraints within the desired targets, outperforming benchmark schemes in terms of efficiency while meeting task delay and error-rate bounded constraints.}
}


@article{DBLP:journals/tmc/LiuJZ24,
	author = {Yimin Liu and
                  Peng Jiang and
                  Liehuang Zhu},
	title = {SecVerGSSL: Proof-Based Verified Computation for Graph Self-Supervised
                  Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13713--13727},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437388},
	doi = {10.1109/TMC.2024.3437388},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuJZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As graph data becomes increasingly prevalent in mobile computing scenarios, deploying Graph Convolutional Network-based self-supervised learning (GCN-SSL) models on mobile devices provides a powerful solution for analyzing graph data and enhancing the intelligence of various mobile services. However, ensuring the legitimacy and security of these models is crucial to protect against compromised or unauthorized versions that could lead to security vulnerabilities or intellectual property issues. In this work, we propose PoGSSL, a verifiable proof of GCN-SSL model training that authenticates model integrity and provenance by checking the reproducibility of the specific model training process. We then introduce SecVerGSSL from PoGSSL to provide privacy-preserving verified computation services. SecVerGSSL offloads the entire computation to the cloud and equips servers with customized secure components, enabling effective verified computation over secret-sharing encrypted training data and PoGSSL. Extensive experiments demonstrate that SecVerGSSL offers verification accuracy indistinguishable from plaintext results, with overhead on the verifier-side requiring at most 10.35 milliseconds and 65.98 KB per epoch.}
}


@article{DBLP:journals/tmc/ZhaoXXLFPX24,
	author = {Bowen Zhao and
                  Huanlai Xing and
                  Lexi Xu and
                  Yang Li and
                  Li Feng and
                  Jincheng Peng and
                  Zhiwen Xiao},
	title = {On Forecasting-Oriented Time Series Transmission: {A} Federated Semantic
                  Communication System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13728--13744},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438285},
	doi = {10.1109/TMC.2024.3438285},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoXXLFPX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series data widely exist in public services, industrial environments, and military applications. Traditionally, the transmission of a huge volume of data for analytic tasks poses challenges, particularly in mobile environments with limited computing and communication resources. Semantic communication emerges as a solution for intelligently extracting various features from source data and efficiently transmitting task-related information to receivers, thereby reducing bandwidth consumption significantly. In this paper, we introduce a novel federated semantic communication system tailored for forecasting-oriented time series transmission tasks. The correlation of source data collected from terminal devices is mined and the corresponding semantic information is transmitted to an edge server for collaborative inference. To optimize the semantic analysis process, we devise a deep decomposition block at the transmitter side, decomposing time series into trend and multiple period components. This reduces noise interference from wireless channels, enhancing the overall transmission quality. For effective training and collaborative inference, we propose a Federated Mixture of period Routers (FedMoR) architecture. Within each channel encoder, period routers are divided into private and public ones. Private routers extract specialized features from individually collected data, mitigating accuracy degradation. Public routers share knowledge across all transmitters, enhancing temporal analysis robustness. Simulation results demonstrate that the proposed system outperforms two traditional technique-based and two semantic communication-based baselines under three common channels. The system achieves low mean square errors on five widely-used real-world time series forecasting datasets, particularly in the low signal-to-noise ratio regime.}
}


@article{DBLP:journals/tmc/ZhouPCCGJ24,
	author = {Junyi Zhou and
                  Henglin Pu and
                  Hangcheng Cao and
                  Chao Cai and
                  Peng Guo and
                  Hongbo Jiang},
	title = {{CORA:} Continuous Respiration Monitoring Using Analytical Signal
                  Processing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13745--13759},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437675},
	doi = {10.1109/TMC.2024.3437675},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouPCCGJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic-based respiration sensing is promising due to its ubiquitous device support and great freedom in signal design. However, existing proposals often either fail to function properly when a target is non-static or is under multipath interference, or address it in an algorithmic manner. To this end, in this paper, we propose CORA, a COntinuous RespirAtion monitoring system using purely analytical signal processing methods. CORA is the first approach that achieves physical separation between motion artifacts and respiration, other than existing algorithmic solutions, and hence can obtain results that are closer to ground truth. CORA leverages the edges of Orthogonal Time Frequency Space signals in monitoring motion states and addressing multipath interference. The ability to tackle these challenges can help to compensate motion-induced artifacts for FMCW-based sensing techniques, enabling continuous respiration monitoring even in non-static scenarios. To achieve high-quality compensation, a pipeline of signal processing techniques is proposed, including robust moving target tracking, accurate frequency bin selection, and effective phase denoising. Unlike existing deep learning-based approaches, CORA is explainable and is readily deployable, without sophisticated adaptation or exhausted training processes. We have implemented a system prototype and evaluated its performance. Experiment results demonstrate a median error of 0.86 respiration per minute.}
}


@article{DBLP:journals/tmc/ZhangWLHYG24,
	author = {Rongrong Zhang and
                  Shuai Wang and
                  Hao Liu and
                  He Huang and
                  Jihong Yu and
                  Yong Guan},
	title = {When Noise Can Help: Anonymous Group Writing in RFID-Enabled Backscatter
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13760--13772},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439024},
	doi = {10.1109/TMC.2024.3439024},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWLHYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient and secure group writing plays a crucial role in RFID-enabled multi-task backscatter systems. The prior works lay emphasis on the time efficiency of the group data transmission, but neglect its security. This paper is devoted to providing anonymous group writing. We propose the Overlapped Bloom Filter-based protocol (OBF) and its enhanced version (OBF+). The core is to construct an approximately random sequence as a noise by making transmission data for different tag groups overlap with each other, thus hiding the original information with a low computational complexity. The compact filter can guarantee the time efficiency while improving the security of the group writing. To make tags aware of the correctness of the decoded group data, the OBF+ introduces the complementary code-based check mechanism to eliminate the fault data. We prototype the system with USRP and programmable WISP tags, and conduct extensive simulations to evaluate our approaches in terms of the time efficiency, the accuracy, and the anonymity of the data transmission.}
}


@article{DBLP:journals/tmc/WuWZSSY24,
	author = {Guowen Wu and
                  Hui Wang and
                  Hong Zhang and
                  Yizhou Shen and
                  Shigen Shen and
                  Shui Yu},
	title = {Mean-Field Game-Based Task-Offloaded Load Balance for Industrial Mobile
                  Edge Computing Systems Using Software-Defined Networking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13773--13786},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437761},
	doi = {10.1109/TMC.2024.3437761},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuWZSSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart devices (SDs) used in the Industrial Internet of Things can generate computational tasks for processing the data generated during production. However, due to the limited processing power of SDs, it is necessary to transfer these computational tasks to more powerful devices for processing. To this end, we propose a Mobile Edge Computing (MEC) system based on a Software Defined Network (SDN) for SDs to offload their computational tasks. This MEC system includes multiple MEC servers to handle numerous SDs, which leads to load-balancing challenges among these servers. To tackle this problem, we develop a computational offloading model based on mean-field game theory and introduce a mean-field game-based load-balancing algorithm (MFGLB), which reduces processing latency and facilitates task scheduling through Multi-Agent Deep Reinforcement Learning. Each SD in the MEC system is considered a participant in the mean-field game, simplifying the complex stochastic game into a more manageable dual-agent game. We then prove the existence of Nash Equilibrium for this mean-field game. To evaluate the effectiveness of our MFGLB algorithm, we compare its performance with traditional load-balancing algorithms and a stochastic game-based load-balancing algorithm. Our experimental results demonstrate the superiority of MFGLB in reducing processing latency and addressing load imbalances.}
}


@article{DBLP:journals/tmc/CuiYSZXH24,
	author = {Kaiyan Cui and
                  Qiang Yang and
                  Leming Shen and
                  Yuanqing Zheng and
                  Fu Xiao and
                  Jinsong Han},
	title = {Towards ISAC-Empowered mmWave Radars by Capturing Modulated Vibrations},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13787--13803},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443404},
	doi = {10.1109/TMC.2024.3443404},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CuiYSZXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated Sensing and Communication (ISAC) has emerged as a promising technology for next-generation mobile networks. Towards ISAC, we develop mmRipple that empowers commodity mmWave radars with communication capabilities through smartphone vibrations. In mmRipple, a smartphone (transmitter) sends messages by modulating smartphone vibrations, while a mmWave radar (receiver) receives the messages by detecting and decoding the smartphone vibrations. By doing so, a smartphone user can not only be passively sensed by a mmWave radar, but also actively send messages to the radar without any hardware modifications. Although promising, the data rate of mmRipple is limited by Morse-style communication. To address this, we present mmRipple+, which leverages the Pulse Width and Amplitude Modulation (PWAM) technique and suppresses inter-symbol interference to enable faster communication. We prototype mmRipple and mmRipple+ on commodity mmWave radars and different types of smartphones. Experimental results show that mmRipple achieves an average vibration pattern recognition accuracy of 98.60% within a 2 m communication range, and 97.74% within 3 m. The maximum communication range extends to 5 m. Meanwhile, mmRipple+ achieves a bit rate of 100 bps with a BER of less than 3%, improving the data rate by 4× over mmRippe with the same symbol duration. This work pioneers smartphone-to-COTS mmWave radar communication via vibrations, unlocking diverse applications.}
}


@article{DBLP:journals/tmc/ZhanHLWF24,
	author = {Cheng Zhan and
                  Han Hu and
                  Zhi Liu and
                  Jing Wang and
                  Rongfei Fan},
	title = {Interference-Aware Online Optimization for Cellular-Connected Multiple
                  {UAV} Networks With Energy Constraints},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13804--13820},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438759},
	doi = {10.1109/TMC.2024.3438759},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhanHLWF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The incorporation of Unmanned Aerial Vehicles (UAVs) into cellular networks opens up new possibilities to enhance their ubiquitous operations and establish superior performance owing to the high probability of line-of-sight (LoS) for air-to-ground channels. However, this also results in the UAV inducing more significant uplink interference to non-associated Base Stations (BSs). This paper explores the online design policy in cellular-connected multiple UAV communications in the absence of channel conditions, focusing on wireless resource allocation and dynamic three-dimensional (3-D) path planning. Our objective is to maximize the minimum uplink throughput for all UAVs while considering the energy constraints of the UAVs. First, we implement an online design utilizing the achievable rate based on the estimated instantaneous channel state information (CSI) for the current time slot, and the expected data rate for future time slots based on channel distribution information (CDI). Our solution employs the exact penalty method along with alternating optimization and successive convex optimization methods. Second, we formulate an online design by merely using the achievable rate based on the estimated instantaneous CSI for the current time slot. We introduce an energy-triggered penalty term to regulate the energy consumption of the UAVs, resulting in a low-complexity solution even if the CDI is unavailable before the flight. Lastly, we conduct extensive simulations to corroborate our findings and provide comprehensive comparisons with other baseline schemes to underline the effectiveness of the proposed designs.}
}


@article{DBLP:journals/tmc/XuYZMHXN24,
	author = {Jiaqi Xu and
                  Haipeng Yao and
                  Ru Zhang and
                  Tianle Mai and
                  Shan Huang and
                  Zehui Xiong and
                  Dusit Niyato},
	title = {Semantic-Aware {UAV} Swarm Coordination in the Metaverse: {A} Reputation-Based
                  Incentive Mechanism},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13821--13833},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438152},
	doi = {10.1109/TMC.2024.3438152},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuYZMHXN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) swarms have found extensive applications owing to their flexibility, mobility, cost-effectiveness, and capacity for collaborative and autonomous service delivery. Empowered by intelligent algorithms, UAV swarm can exhibit cohesive behaviors and autonomously coordinate to achieve collective objectives. Nonetheless, in real-world scenarios with uncertainty and stochasticity, its performance suffers from the unstable information exchange among UAVs and inefficient data sampling. In this paper, we introduce a metaverse-based UAV swarm system, where monitoring, observation, analysis, and simulation can be realized collaboratively and virtually. Within the metaverse, virtual service providers (VSPs) utilize digital twin (DT) to generate and render virtual sub-worlds, while providing diverse virtual services. In particular, the VSP trains the learning model using high-fidelity data from the physical world, formulates optimal decisions for diverse tasks, and returns these decisions to the UAV swarm for the execution of the corresponding tasks. Since synchronization between two worlds needs frequent data exchange, we employ the semantic communication technique in our system which could reduce communication latency by transmitting only the semantic information. In such design, UAVs as workers are employed to collect data and provide extracted semantic information to the VSPs. Moreover, we propose a hierarchical framework to investigate the reliability and sustainability of the metaverse-based UAV swarm system. In the lower layer, we design a worker selection scheme to determine reliable UAVs for data synchronization. In the upper layer, we consider deep learning (DL)-based auction as the incentive mechanism for resource allocation in semantic information trading between UAV swarm and VSPs.}
}


@article{DBLP:journals/tmc/TianZWG24,
	author = {Fengsen Tian and
                  Xinglin Zhang and
                  Xiumin Wang and
                  Yue{-}Jiao Gong},
	title = {Two-Layer Optimization With Utility Game and Resource Control for
                  Federated Learning in Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13834--13850},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437435},
	doi = {10.1109/TMC.2024.3437435},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TianZWG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed machine learning paradigm that can be organized in two layers. In the outer layer of users, there is a model interaction process between the task publisher and users, through which all parties obtain their respective utilities. However, these parties’ utilities are coupled, both depending on the training sample size and local iterations. In the inner layer of users, a user's multiple devices (e.g., computers and smart phones) can be used to jointly train local models efficiently. Yet, due to device heterogeneity, it is challenging for users to determine which devices to participate in local training and allocate how many computing and communication resources to minimize training costs. In this paper, we tackle this novel two-layer optimization problem by designing utility game and resource control strategies. In the outer layer, we model the relationship between the task publisher and users as a Stackelberg game and obtain the optimal solution for both parties by solving a unique Stackelberg equilibrium point; while in the inner layer, we formulate the optimization problem as a mixed integer nonlinear programming problem, which is decomposed into sub-problems and solved by devising resource control algorithm based on successive convex approximation. Finally, extensive experiments show that the proposed algorithms outperform baseline algorithms.}
}


@article{DBLP:journals/tmc/ZhaoGTQPL24,
	author = {Bowen Zhao and
                  Weibin Guo and
                  Bo Tian and
                  Cheng Qiao and
                  Qingqi Pei and
                  Ximeng Liu},
	title = {{RATE:} Privacy-Preserving Task Assignment With Bi-Objective Optimization
                  for Mobile Crowdsensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13851--13865},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439584},
	doi = {10.1109/TMC.2024.3439584},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoGTQPL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Assigning sensing tasks to appropriate task participants is critical for mobile crowdsensing (MCS) and is an essential optimization problem. However, existing task assignment (or say participant selection) solutions for MCS generally support a single-objective optimization (e.g., minimizing travel distance or maximizing social welfare). Additionally, task assignment for MCS usually requires task participants’ and a task requester's location information, which compromises their location privacy and hinders participation willingness. To achieve task assignment with bi-objective optimization and safeguard bilateral privacy, in this paper, we propose RATE, a privacy-preserving task assignment with bi-objective optimization for MCS. RATE features the following characteristics. First, RATE enables task assignment with bi-objective optimization including maximizing the social welfare and the requester's revenue, simultaneously. Second, RATE achieves bilateral privacy-preserving task assignment with bi-objective optimization by carefully designing underlyingly secure computing protocols. Third, RATE approximates optimal results of task assignments without sacrificing privacy. Theoretical analyses show that RATE protects the location privacy of both the task requester and the task participants. Meanwhile, experimental evaluations demonstrate that RATE outperforms traditional task assignment solutions and generates the task assignment result effectively and efficiently.}
}


@article{DBLP:journals/tmc/WangJLW24,
	author = {Jiazhao Wang and
                  Wenchao Jiang and
                  Ruofeng Liu and
                  Shuai Wang},
	title = {Towards Efficient and Portable Software Modulator via Neural Networks
                  for IoT Gateways},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13866--13881},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3444768},
	doi = {10.1109/TMC.2024.3444768},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangJLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A physical-layer modulator is crucial for IoT gateways, but current solutions face issues like limited extensibility and platform-specificity due to soldered chipsets for specific technologies or diverse software toolkits for software radios. With the rapid expansion of the Internet of Things (IoT), such limitations are hard to ignore as the demand for versatile wireless technologies has increased. This paper introduces a novel approach using neural networks as an abstraction layer for these modulators in IoT gateways, termed NN-defined modulators. This method overcomes the challenges of extensibility and portability across different hardware platforms. The NN-defined modulator employs a model-driven approach based on mathematical principles, resulting in a lightweight, hardware-acceleration-friendly structure. These modulators are containerized with necessary runtime, facilitating agile deployment on varied platforms. We tested NN-defined modulators on platforms like Nvidia Jetson Nano and Raspberry Pi, showing they perform comparably to traditional modulators while offering efficiency improvements. The implementation is memory-efficient and adds minimal latency. Additionally, we demonstrate real-world applications of our NN-defined modulators in generating ZigBee and WiFi packets, compatible with standard TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC) devices.}
}


@article{DBLP:journals/tmc/WangZCZWBH24,
	author = {Li Wang and
                  Hong Zhong and
                  Jie Cui and
                  Jing Zhang and
                  Lu Wei and
                  Irina Bolodurina and
                  Debiao He},
	title = {Privacy-Preserving and Secure Distributed Data Sharing Scheme for
                  VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13882--13897},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3441595},
	doi = {10.1109/TMC.2024.3441595},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZCZWBH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data sharing is one of the essential services of vehicular ad hoc networks (VANETs), which primarily requires data security and access control, and ciphertext-policy attribute-based encryption (CP-ABE) is a promising tool. However, data sharing schemes of distributed CP-ABE have concerns about the single-point performance bottleneck and privacy leakage. The factor for the former is that the authority manages a disjoint attribute set. The latter is because the user's identity and attributes are required to submit to authorities, which targets to bind this information to decryption keys for collusion-resistant. We propose a privacy-preserving distributed data sharing scheme for VANETs. This scheme introduces asymmetric group key agreement to distributed CP-ABE, which realizes that multiple authorities manage an attribute, and the user can obtain the attribute key bound with his identity from any authority in the group. To match up to the requirement of privacy-preserving, a key extract protocol provided user anonymity is proposed, which implements that attribute keys can be obtained without revealing the user's identity and attributes. Moreover, partial policy hiding is satisfied. Finally, we analyze and evaluate the proposed scheme, and the results indicate that our scheme is secure and efficient.}
}


@article{DBLP:journals/tmc/WangZZYSC24,
	author = {Guanzhong Wang and
                  Dongheng Zhang and
                  Tianyu Zhang and
                  Shuai Yang and
                  Qibin Sun and
                  Yan Chen},
	title = {Learning Domain-Invariant Model for WiFi-Based Indoor Localization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13898--13913},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438454},
	doi = {10.1109/TMC.2024.3438454},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZYSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based indoor localization has gained widespread attention due to the pervasive availability of WiFi Access Points (APs). While signal processing-based methods can achieve decimeter-level localization, their performance is constrained by the limited spatial resolution of WiFi systems, especially in complex environments with strong interference. By contrast, deep learning-based methods have achieved impressive performance even in complex environments, which however often fail to generalize to new environments. In this paper, we propose a novel framework to learn domain-invariant model for WiFi-based indoor localization, which maintains impressive performance across different environments. The key insight is to design a deep learning-based WiFi localization system through the perspective of signal processing. Specifically, we let the neural network estimate APs-centered polar coordinates to avoid fitting the coordinates of APs strongly correlated with the environment, enabling us to obtain the domain-invariant model. To unleash the potential of neural networks in regressing high-precision parameters, we design a beamforming layer to integrate the knowledge of signal processing. Furthermore, we propose a multi-task learning scheme to further improve localization accuracy. Extensive experiments on diverse datasets have demonstrated that the localization performance of our method outperforms state-of-the-art methods and demonstrates superiority under cross-domain conditions.}
}


@article{DBLP:journals/tmc/LiHXHL24,
	author = {Xulong Li and
                  Wei Huangfu and
                  Xinyi Xu and
                  Jiahao Huo and
                  Keping Long},
	title = {Secure Offloading With Adversarial Multi-Agent Reinforcement Learning
                  Against Intelligent Eavesdroppers in UAV-Enabled Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13914--13928},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439016},
	doi = {10.1109/TMC.2024.3439016},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiHXHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has attracted widespread attention due to its ability to effectively alleviate the cloud computing load and significantly reduce latency. However, the potential eavesdroppers challenge the security of the MEC systems and the rapid development of artificial intelligence (AI) has made this security situation more severe. In most existing studies, the eavesdroppers are non-intelligent and it is assumed that they are fixed or move in a simple manner. Obviously, there is a gap from such an assumption to the real conditions that the eavesdropping unmanned aerial vehicles (UAVs) may adjust their flight paths intelligently. To better reflect real-world scenarios, we consider a multi-UAV-assisted MEC system in the presence of intelligent eavesdroppers and propose an adversarial multi-agent reinforcement learning (MARL)-based scheme for secure computational offloading and resource allocation. With this scheme, we aim to solve the zero-sum game between the legitimate UAVs and the eavesdropping UAVs, in which the two types of UAVs take turns acting as the agents of MARL to alternately optimize their respective opposing objectives. The simulation experimental results indicate that the proposed scheme significantly outperforms the existing baseline methods in dealing with the intelligent eavesdropping UAVs, and ensures high energy efficiency of Internet of Things (IoT) devices even in the worst-case scenario when dealing with potential eavesdropping threats.}
}


@article{DBLP:journals/tmc/LiCL24,
	author = {Lingang Li and
                  Yongrui Chen and
                  Zhijun Li},
	title = {{QCC:} Driver-Queue Based Congestion Control for Data Uploading in
                  Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13929--13944},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437409},
	doi = {10.1109/TMC.2024.3437409},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data uploading applications in wireless networks may suffer from the degrade of Quality of Experiences (QoEs), due to the untimely adjustment of congestion window (cwnd) in face of the rapid change of wireless channel. To mitigate this problem, we analyzed the relationship between the NIC driver queue length at the wireless sender and the end-to-end transmission performances, and found a strong correlation between them, since the bottleneck mostly occurs at the wireless link. Based on this observation, we designed QCC, a congestion control algorithm that adjusts cwnd according to the residual queue length after each round of NIC transmission. Since obtaining congestion information locally at the sender leads to a much shorter feedback path than waiting for the end-to-end ACK feedback, QCC can track the time-varying wireless links much faster and more accurately. In addition, QCC also presents adaptive slow start mechanism and MAC layer-assisted fast recovery mechanism, both of which make efficient use of residual queue length to further improve transmission performances. Experiment results on both real-world Wi-Fi and cellular networks reveal that QCC can achieve at least 2.36X lower delay than that of BBR while ensuring 98.5% throughput of BBR.}
}


@article{DBLP:journals/tmc/WangLKHMTYJ24,
	author = {Hanling Wang and
                  Qing Li and
                  Haidong Kang and
                  Dieli Hu and
                  Lianbo Ma and
                  Gareth Tyson and
                  Zhenhui Yuan and
                  Yong Jiang},
	title = {ParaLoupe: Real-Time Video Analytics on Edge Cluster via Mini Model
                  Parallelization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13945--13962},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438155},
	doi = {10.1109/TMC.2024.3438155},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLKHMTYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time video analytics on edge devices has gained increasing attention across a wide range of business areas. However, edge devices usually have limited computing resources. Consequently, conventional approaches to video analytics either deploy simplified models on the edge (resulting in low accuracy) or transmit video content to the cloud (resulting in high latency and network overheads) to enable deep learning inference (e.g., object detection). In this paper, we introduce ParaLoupe, a novel real-time video analytics system that parallelizes deep learning inference in the edge cluster with task-oriented mini models. These mini models do not attain State-of-the-Art accuracy individually, but collectively can achieve much better accuracy-latency tradeoff than State-of-the-Art models. To achieve this, ParaLoupe crops multiple single-object patches from a given video frame. These single-object patches are then sent to multiple edge devices for parallel inference with specifically designed mini models. A patch-based task scheduling algorithm is further proposed to leverage the computing resources of the edge cluster to meet the service-level objectives. Our experimental results on real-world datasets show that ParaLoupe significantly outperforms baseline methods, achieving up to 14.1× inference speedup with accuracy on par with state-of-the-art models, or improving accuracy up to 45.1% under the same latency constraints.}
}


@article{DBLP:journals/tmc/NguyenNKRN24,
	author = {Nhat Nguyen and
                  Duong Nguyen and
                  Junae Kim and
                  Gianluca Rizzo and
                  Hung X. Nguyen},
	title = {Decentralized Coordination for Multi-Agent Data Collection in Dynamic
                  Environments},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13963--13978},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437360},
	doi = {10.1109/TMC.2024.3437360},
	timestamp = {Sun, 24 Nov 2024 18:58:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NguyenNKRN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coordinated multi-robot systems are an effective way to harvest data from sensor networks and implement active perception strategies. However, achieving efficient coordination in a way that guarantees a target QoS while adapting dynamically to changes (in the environment and/or in the system) is a key open issue. In this paper, we propose a novel decentralized Monte Carlo Tree Search (MCTS) algorithm for dynamic environments that allows agents to optimize their own actions while achieving some form of coordination. Its main underlying idea is to balance adaptively the exploration-exploitation trade-off to deal effectively with changes in the environment while filtering out outdated and irrelevant samples via a sliding window mechanism. We show both theoretically and through simulations that in dynamic environments our algorithm provides a log-factor (in terms of time steps) smaller regret than state-of-the-art decentralized multi-agent planning methods. We instantiate our approach to the problem of underwater data collection, showing in a variety of different settings that our approach greatly outperforms the best-competing approaches, both in terms of convergence speed and global utility.}
}


@article{DBLP:journals/tmc/ZhangTGD24,
	author = {Wenzhao Zhang and
                  Yixiao Teng and
                  Yi Gao and
                  Wei Dong},
	title = {Reducing End-to-End Latency of Trigger-Action IoT Programs on Containerized
                  Edge Platforms},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13979--13990},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439533},
	doi = {10.1109/TMC.2024.3439533},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangTGD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT rule engines are important middlewares that allow users to easily create custom trigger-action programs (TAPs) and interact with the physical world. Users expect their TAPs to give a timely response within a certain deadline. Existing works provide this support by boosting the process of trigger event identification. Many IoT rule engines now run in containerized environments, bringing about new challenges and opportunities. Prior solutions can no longer satisfy the need of mitigating the end-to-end latency of containerized TAPs. In this work, we propose EdgeRuler, which couples the IoT rule engine and the container runtime to assure the performance of latency-critical TAPs. To enable such capability, EdgeRuler precisely models the end-to-end latency by exploiting information from both the physical and the cyber world. EdgeRuler then enforces a deadline-aware life-cycle control and resource provision for meeting the TAP constraints in a lightweight and efficient way. We prototype and evaluate EdgeRuler on top of production-ready open-source components, which shows that EdgeRuler reduces the end-to-end latency by 28.6%-96.2% compared to existing scheduling algorithms and 68.4%-89.1% to that of the state-of-the-art IoT rule engines, incurring negligible runtime overhead.}
}


@article{DBLP:journals/tmc/ZhaoCLLWL24,
	author = {Zeyu Zhao and
                  Yue Ling Che and
                  Sheng Luo and
                  Gege Luo and
                  Kaishun Wu and
                  Victor C. M. Leung},
	title = {On Designing Multi-UAV Aided Wireless Powered Dynamic Communication
                  via Hierarchical Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {13991--14004},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439556},
	doi = {10.1109/TMC.2024.3439556},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhaoCLLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel design on the wireless powered communication network (WPCN) in dynamic environments under the assistance of multiple unmanned aerial vehicles (UAVs). Unlike the existing studies, where the low-power wireless nodes (WNs) often conform to the coherent harvest-then-transmit protocol, under our newly proposed double-threshold based WN type updating rule, each WN can dynamically and repeatedly update its WN type as an E-node for non-linear energy harvesting over time slots or an I-node for transmitting data over sub-slots. To maximize the total transmission data size of all the WNs over T\nslots, each of the UAVs individually determines its trajectory and binary wireless energy transmission (WET) decisions over times slots and its binary wireless data collection (WDC) decisions over sub-slots, under the constraints of each UAV's limited on-board energy and each WN's node type updating rule. However, due to the UAVs’ tightly-coupled trajectories with their WET and WDC decisions, as well as each WN's time-varying battery energy, this problem is difficult to solve optimally. We then propose a new multi-agent based hierarchical deep reinforcement learning (MAHDRL) framework with two tiers to solve the problem efficiently, where the soft actor critic (SAC) policy is designed in tier-1 to determine each UAV's continuous trajectory and binary WET decision over time slots, and the deep-Q learning (DQN) policy is designed in tier-2 to determine each UAV's binary WDC decisions over sub-slots under the given UAV trajectory from tier-1. Both of the SAC policy and the DQN policy are executed distributively at each UAV. Finally, extensive simulation results are provided to validate the outweighed performance of the proposed MAHDRL approach over various state-of-the-art benchmarks.}
}


@article{DBLP:journals/tmc/HuangGD24,
	author = {Jiaming Huang and
                  Yi Gao and
                  Wei Dong},
	title = {Elastic {DNN} Inference With Unpredictable Exit in Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14005--14016},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3441946},
	doi = {10.1109/TMC.2024.3441946},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangGD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-exit neural networks have gained popularity in edge computing to leverage the computing power of diverse devices. However, real-time tasks in edge applications often face frequent unpredictable exits caused by power outages or high-priority preemptions, which have been largely overlooked by multi-exit models. To address this challenge, it is crucial to determine the appropriate exit point in the multi-exit model to ensure desirable results during unpredictable exits. In this paper, we propose EINet, a sample-wise planner for real-time multi-exit deep neural networks. EINet enables efficient Elastic Inference with unpredictable exits while ensuring best-effort accuracy on various edge platforms. Our approach involves partitioning a trained deep neural network into multiple blocks, each with its exit. Furthermore, EINet utilizes block-wise model profiles, which include accuracy and inference time information for each block. By leveraging these profiles, EINet dynamically determines the optimal exit plan for each sample during the inference process. We introduce Confidence Score Predictors to adapt to the unique characteristics of input samples and employ the Search Engine to efficiently find near-optimal plans for elastic inference. Extensive evaluations of EINet using multiple deep neural networks and datasets with unpredictable exits demonstrate its superior performance. EINet exhibits significant accuracy improvements: 0.13%–16.5% compared to static plans, 0.79%–4.1% compared to other dynamic plans, and over 50% compared to predictable inference in typical scenarios.}
}


@article{DBLP:journals/tmc/HuTGJZC24,
	author = {Chuang Hu and
                  Tianyu Tu and
                  Yili Gong and
                  Jiawei Jiang and
                  Zhigao Zheng and
                  Dazhao Cheng},
	title = {Tackling Multiplayer Interaction for Federated Generative Adversarial
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14017--14030},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438148},
	doi = {10.1109/TMC.2024.3438148},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuTGJZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generative Adversarial Networks (GANs) have become predominant in mobile computing for their ability to generate data. The concern for data privacy has made it arduous to collect large-scale datasets for GAN training on centralized servers. Federated Learning (FL) has emerged as a promising solution to address data privacy concerns. In this paper, we propose Oasis, a multiplayer-oriented federated GAN training system. We present a motivation, highlighting the Nash Equilibrium (NE) shift in vanilla federated GANs, exacerbated by data heterogeneity, leading to poor training performance with issues of vanishing gradient and mode collapse. To address mode collapse, Oasis extracts privacy-preserving data representations and generates a similarity table for clustering clients. Each group independently trains a GAN model and conducts distribution and fusion. By introducing a coordinator, Oasis generalizes intra-group games into Separable Zero-sum Multiplayer Games to tackle vanishing gradient. Thus, Oasis considers the overall federated GAN training as Group-wise Separable Zero-sum Multiplayer Games. Practically, we evaluate our theoretical results both on a hardware prototype and in a simulated environment. Evaluation results demonstrate the effectiveness of Oasis, with an average improvement of 23.13% and 26.33% in terms of FID and NDB/K respectively, compared to three state-of-the-art FL approaches over three datasets.}
}


@article{DBLP:journals/tmc/BasuCJ24,
	author = {Amartya Basu and
                  Ayon Chakraborty and
                  Kush Jajal},
	title = {Ubiquitous Indoor Mapping Using Mobile Radio Tomography},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14031--14043},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442439},
	doi = {10.1109/TMC.2024.3442439},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BasuCJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for real-time and accurate mapping is ubiquitous, particularly in complex indoor settings. While SLAM-based methods are popular, Radio Tomographic Imaging (RTI) offers an essential set of advantages, including mapping inaccessible or enclosed spaces, shorter scanning trajectories, or even identifying material properties of structures on the map. However, existing RTI systems typically depend on pre-deployed, precisely calibrated infrastructure with ample computing power, making it challenging to deploy in a ubiquitous setting. We design UbiqMap, a lightweight RTI-based end-to-end system capable of mapping indoor spaces in real-time, with minimal to zero reliance over pre-deployed infrastructure. We evaluate the performance of UbiqMap in various scenarios, including two real deployments - a moderately complex residential apartment (800 sq. ft) and a large building foyer area (3000 sq. ft) and a few simulated scenarios. We demonstrate how UbiqMap can benefit over traditional SLAM-based techniques in specific contexts and advocate the fusion of RTI methods with SLAM to improve future mapping technologies. Overall, UbiqMap improves the quality of the estimated map by 30%–40% over the state-of-the-art with equivalent resource availability.}
}


@article{DBLP:journals/tmc/GuoGW24,
	author = {Tao Guo and
                  Song Guo and
                  Junxiao Wang},
	title = {Explore and Cure: Unveiling Sample Effectiveness With Context-Aware
                  Federated Prompt Tuning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14044--14054},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439864},
	doi = {10.1109/TMC.2024.3439864},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using pre-trained vision-language models like CLIP with federated training prompts has shown great potential in federated learning (FL) by offering significant benefits in computation, communication, and privacy over existing frameworks. However, existing researches overlook the internal mechanisms underlying federated prompt tuning and comply with the traditional context-unaware tuning mechanism. Our experiments, on the other hand, demonstrate that federated prompting is a data-efficient but data-sensitive paradigm, and therefore, the samples that involved in the prompt tuning process holds significant importance. To address the above issue, we propose Context-aware Federated Prompt Tuning (CaFPT), which facilitates the retrieval process by conditioning on the examples capable of activating the most pertinent knowledge inside the pre-trained models with information theory. Moving in this direction steers the behavior of pre-trained neurons precisely and improves performance on the local task. Informative vectors are built by pruning clients’ training data based on their \\mathcal {V}\n-usable information. The study shows that these vectors can be updated and combined through operations like FedAVG, and the resulting model's behavior is steered accordingly on multiple clients’ tasks. Extensive experiments have demonstrated that informative vectors offer promising robustness, making it a simple yet effective way to enhance the performance of federated prompting.}
}


@article{DBLP:journals/tmc/AkbariV24,
	author = {Shiva Akbari and
                  Shahrokh Valaee},
	title = {A Binary Structured Sparsity Approach for Multi-Anchor Direct Localization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14055--14070},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439099},
	doi = {10.1109/TMC.2024.3439099},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AkbariV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structured sparsity improves on traditional sparse modeling by suggesting that only a limited number of input variables is needed to describe the output variable. These methods go a step further by incorporating structured patterns in variable selection, such as groups or networks of input variables. This paper introduces a novel binary approximation method leveraging structured sparsity to enhance multi-anchor direct localization performance. By reformulating the sparse recovery as a quadratic unconstrained binary optimization (QUBO) problem, we address the significant computational complexity inherent in NP-hard problems. Employing compressed sensing and binary programming, our method reduces approximation errors and ensures that the line-of-sight component is consistently identified from a coherent grid point among anchors, thus enhancing localization accuracy. Our findings demonstrate a significant improvement in the accuracy of direct localization methods, underscoring the potential of structured sparsity and QUBO formulation to advance localization technologies in multipath environments.}
}


@article{DBLP:journals/tmc/WangWLC24,
	author = {Yatong Wang and
                  Zhongyi Wen and
                  Yunjie Li and
                  Bin Cao},
	title = {Learn to Collaborate in {MEC:} An Adaptive Decentralized Federated
                  Learning Framework},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14071--14084},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439588},
	doi = {10.1109/TMC.2024.3439588},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized federated learning (DFL) has emerged as a conducive paradigm, facilitating a distributed privacy-preserving data collaboration mode in mobile edge computing (MEC) systems to bolster the expansion of artificial intelligence applications. Nevertheless, the dynamic wireless environment and the heterogeneity among collaborating nodes, characterized by skewed datasets and uneven capabilities, present substantial challenges for efficient DFL model training in MEC systems. Consequently, the design of an efficient collaboration strategy becomes essential to facilitate practical distributed knowledge sharing and cost reduction for MEC. In this paper, we propose an adaptive decentralized federated learning framework that enables heterogeneous nodes to learn tailored collaboration strategies, thereby maximizing the efficiency of the DFL training process in collaborative MEC systems. Specifically, we present an effective option critic-based collaboration strategy learning (OCSL) mechanism by decomposing the collaboration strategy model into two sub-strategies: local training strategy and resource scheduling strategy. In addressing inherent issues such as large-scale action space and overestimation in collaboration strategy learning, we introduce the option framework and a dual critic network-based approximation method within the OCSL design. We theoretically prove that the learned collaboration strategy achieves the Nash equilibrium. Extensive numerical results demonstrate the effectiveness of the proposed method in comparison with existing baselines.}
}


@article{DBLP:journals/tmc/ZhangZQW24,
	author = {Songwei Zhang and
                  Xiaobo Zhou and
                  Tie Qiu and
                  Dapeng Oliver Wu},
	title = {Quantum-Inspired Robust Networking Model With Multiverse Co-Evolution
                  for Scale-Free IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14085--14098},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439511},
	doi = {10.1109/TMC.2024.3439511},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZQW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The robustness of scale-free Internet of Things (IoT) topology is seriously affected by malicious attacks. Improving the tolerance to node failures is critical to the stability of IoT systems. Heuristic algorithms, especially genetic algorithms, enhance the stability of network topology through the evolution of population chromosomes. However, the loss of genetic diversity makes the optimization easily fall into local optimum. Although the problem can be alleviated by adjusting population size and genetic probability, the genetic diversity is still not guaranteed in the limited number of iterations. Inspired by the quantum superposition that simultaneously operates on an exponential number of states, we propose a quantum-inspired robust networking model with multiverse co-evolution for the scale-free IoT (Q-Robust). This model designs quantum chromosomes with double-chain structures to represent the connections between all nodes. Then we present the quantum measurement method of quantum chromosomes based on the degree distribution of nodes. Furthermore, this model constructs a primary-secondary quantum multiverse co-evolution mechanism to improve the convergence efficiency of topology evolution. The experimental results show that the topology robustness optimized by Q-Robust is about 60% and 10% higher than the initial topology and the state-of-the-art topology evolution algorithm, respectively.}
}


@article{DBLP:journals/tmc/CaoLHL24,
	author = {Jiani Cao and
                  Yang Liu and
                  Lixiang Han and
                  Zhenjiang Li},
	title = {Finger Tracking Using Wrist-Worn {EMG} Sensors},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14099--14110},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439018},
	doi = {10.1109/TMC.2024.3439018},
	timestamp = {Mon, 03 Mar 2025 22:25:36 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoLHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces WETrak, a finger tracking system using wrist-worn electromyography (EMG) sensors. Recent finger tracking methods mainly employ EMGs on armbands. Compared to a range of contactless methods using cameras or wireless, they are not limited by high computational costs, privacy concerns, and mobility, while unlike other wearable-based approaches, they do not require the deployment of sensors on the user's hands. However, users need to wear an additional armband on their forearm each time solely for tracking purpose, which hinders the widespread adoption of finger tracking in practice. This paper investigates the feasibility of moving EMG sensors from the forearm to the wrist for finger tracking. WETrak inherits the advantages of existing EMG-based armband tracking while avoiding the limitation of requiring additional armbands, which brings a strong incentive for integrating EMG sensors into wrist-worn wearables in the future. As sensor placement varies, we find new challenges in determing good locations to place sensors to gather useful information to capture all finger movements and using low-quality signals to still ensure accurate tracking. In this paper, we introduce new, efficient solutions to these problems. We develop a prototype, and the results show that WETrak outperforms the state-of-the-art method and performs consistently well under various settings.}
}


@article{DBLP:journals/tmc/HuangFTXCY24,
	author = {Tao Huang and
                  Zeru Fang and
                  Qinqin Tang and
                  Renchao Xie and
                  Tianjiao Chen and
                  F. Richard Yu},
	title = {Dual-Timescales Optimization of Task Scheduling and Resource Slicing
                  in Satellite-Terrestrial Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14111--14126},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3440066},
	doi = {10.1109/TMC.2024.3440066},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuangFTXCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we optimize network computational performance and ensure diverse quality of service (QoS) for tasks by developing a dual-timescale joint optimization framework for satellite-terrestrial integrated edge computing networks (STECN). In our architecture, STECN can handle intelligent tasks for the Internet of remote things (IoRT) devices based on multiple configured applications deployed. Specifically, we formulate task scheduling as a Markov decision process (MDP) to minimize network energy consumption and task processing delay at small timescales. A deep reinforcement learning (DRL) framework is designed for policy learning. Recognizing the impact of resource slicing on task scheduling in STECN and the deployment overhead from frequent changes, we further optimize resource slicing at larger timescales. To enhance network service capability under dynamic demand, we establish a resource slice gap index, characterizing the difference between actual resources and service demand. By a heuristic-based artificial electric field (AEF) approach, we obtain an optimal strategy with low complexity. Considering the correlation between two timescales, the optimal solution is found by iteratively constructing a hierarchical solution. In addition, to guarantee the global load balancing of the network, we introduce a self-attention mechanism, which allows the knowledge of other satellites to be taken into account when slicing the satellite resources. Finally, extensive simulations confirm the effectiveness and superiority of the proposed scheme.}
}


@article{DBLP:journals/tmc/ZhouXZYW24,
	author = {Xuanhan Zhou and
                  Jun Xiong and
                  Haitao Zhao and
                  Chao Yan and
                  Jibo Wei},
	title = {Symmetry-Augmented Multi-Agent Reinforcement Learning for Scalable
                  {UAV} Trajectory Design and User Scheduling},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14127--14144},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437679},
	doi = {10.1109/TMC.2024.3437679},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouXZYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) as mobile base stations are recognized as effective means for emergency communications. The performance of such systems depends on the movement of UAVs and scheduling of ground users (GUs). However, devising an efficient algorithm to jointly optimize UAV trajectories and user scheduling is still challenging, especially in real-time scenarios lacking central controllers. Multi-agent deep reinforcement learning (MADRL) provides a promising solution to this problem. Nevertheless, as the numbers of UAVs and GUs increase, existing MADRL algorithms encounter scalability and sample efficiency issues. In this paper, we develop a novel symmetry-augmented MADRL approach for learning scalable UAV trajectory design and user scheduling policies. The core idea is to utilize symmetries to reduce the multi-agent state-action space and enhance sample efficiency. Specifically, we design a family of neural networks to learn individual policies, namely entity permutation equivariant policy networks (EP2Nets). EP2Nets effectively leverage the permutation symmetry to reduce redundancy in the state-action space. Additionally, we achieve data augmentation by exploiting rotational and reflection symmetries, further boosting sample efficiency. Finally, a Symmetric QMIX (SymmQMIX) algorithm is proposed by integrating the EP2Net and data augmentation method into the QMIX algorithm. Simulation results indicate that SymmQMIX significantly outperforms QMIX and other symmetry-enhanced algorithms, achieving a 4.5-fold increase in converged performance and a 100-fold improvement in sample efficiency.}
}


@article{DBLP:journals/tmc/WangZZT24,
	author = {Tingqi Wang and
                  Xu Zheng and
                  Jinchuan Zhang and
                  Ling Tian},
	title = {Federal Graph Contrastive Learning With Secure Cross-Device Validation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14145--14158},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437468},
	doi = {10.1109/TMC.2024.3437468},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed mobile devices collect unlabeled graph data from environment. Introducing popular graph contrastive learning (GCL) methods can learn node representations better. However, training high-performance GCL requires large-scale data and graph data collected by the single device is insufficient. Meanwhile, transmitting local data for centralized training suffers from non-negotiate privacy leakage and bandwidth consumption. Federated learning (FL), as a distributed learning paradigm, is commonly used for such issues. Nevertheless, direct combination of FL and GCL struggles to supplement global graph information. This absence results in neighbor information missing, thus causing the local GCL to learn biased node representations. Moreover, the combination also triggers potential gradient explosion owing to the lack of unified learning criteria. In this paper, we propose a federal GCL framework that complements missing structural information and provides unified learning criteria. The key idea is to achieve cross-client node alignment on server through local graph structural importance to reason about the global graph information. We design a hierarchical structural importance scoring method to comprehensively evaluate structural importance, thus server performs effective cross-client aggregation while maintaining local graph privacy. We demonstrate the security and prove the bandwidth-reducing advantage of the proposed framework. Extensive experiments on 3 datasets show the superior performance of our method.}
}


@article{DBLP:journals/tmc/MishraK24a,
	author = {Soumya Nandan Mishra and
                  Manas Khatua},
	title = {Game Theoretic Congestion Control to Achieve Hard Reliability in Mission-Critical
                  IoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14159--14170},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3437483},
	doi = {10.1109/TMC.2024.3437483},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/MishraK24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mission-critical Internet of Things (MC-IoT) applications span from the industrial field to the battlefield and from smart homes to healthcare. Reliability is one of the stringent requirements of such applications. Routing in low-power and Lossy Networks (RPL) is a standard routing protocol traditionally used in IoT applications. Unlike the traditional single-path-based RPL, the reliable multi-path RPL (RMP-RPL) selects k parents for each transmitting node to achieve the hard reliability requirement in MC-IoT applications. However, the RMP-RPL fails to achieve the reliability requirement when the number of source nodes and traffic rate increases. In this paper, a multi-path game theoretic congestion control (GTMP-RPL) approach is proposed on top of the RMP-RPL to reduce congestion at the parent nodes of any child. In case of congestion at any one of the selected k parents, its child node replaces the congested parent by a non-congested parent with minimum rank from the set of remaining nodes. The rank of a node is calculated based on the Data Packet Drop Ratio (DPDR), Expected Transmission Count (ETX), and Node Mobility (NM) of that node. If no non-congested parent is available for that child node, a non-cooperative game is played among siblings to adjust their data transmission rates based on congestion occurrence at the parent, energy spent by the child, and parent connectivity of the child node. The solution obtained for the proposed game, using the Lagrange multiplier and Karush-Kuhn-Tucker (KKT) conditions, is used to reduce congestion at the parent node. The proposed scheme is validated using the cooja simulator in Contiki OS. Simulation results show that GTMP-RPL achieves a 99% packet delivery ratio for at most 50 source nodes with a traffic rate of 30 pkts/min present in a random topology of 101 nodes. In addition, it outperforms the other benchmark schemes by a significant margin to achieve hard reliability.}
}


@article{DBLP:journals/tmc/AhmedYMKRHPH24,
	author = {Awais Ahmed and
                  Panlong Yang and
                  Adeel Feroz Mirza and
                  Taha Khan and
                  Muhammad Rizwan and
                  Ammar Hawbani and
                  Miao Pan and
                  Zhu Han},
	title = {SipDeep: Swallowing-Based Transparent Authentication via Bone-Conducted
                  In-Ear Acoustics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14171--14185},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450919},
	doi = {10.1109/TMC.2024.3450919},
	timestamp = {Mon, 03 Mar 2025 22:25:36 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/AhmedYMKRHPH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing use of smart devices requires improving privacy and security. Conventional biometrics confront false positives and unauthorized access, stressing cautious user input. We enhance security by analyzing distinctive human physiological characteristics rather than relying on conventional methods susceptible to spoof attacks. Drinking, a common physiological activity, can provide continuous authentication. SipDeep, proposed innovative system, utilizes bone-conducted liquid intake sound, incorporating unique biometrics from bone and pharyngeal characteristics. The system captures these elements in the external auditory canal, offering a novel transparent authentication applicable to a diverse user range. Our noise filtering system eliminates environmental and anatomical interferences during drinking, including subtle body movements. The study introduces a hybrid event detection technique integrating wavelet transform with start/end points detection. Next, we extract physiological features from bone structure, liquid intake sound, and liquid intake pattern. We used the physiological features to train a deep learning algorithm based on a Triplet-Siamese network to classify authentication. The proposed model has been thoroughly compared with advanced models such as DenseNet169, ResNet18, and VGG16. Following extensive experimentation involving multiple users across various environments, SipDeep demonstrates 96.5% authentication accuracy, coupled with a 98.33% resistance to spoof attacks.}
}


@article{DBLP:journals/tmc/ZouW24,
	author = {Rui Zou and
                  Wenye Wang},
	title = {FLuMe: Understanding Differential Spectrum Mobility Features in High
                  Resolution},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14186--14200},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442151},
	doi = {10.1109/TMC.2024.3442151},
	timestamp = {Mon, 03 Mar 2025 22:25:39 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZouW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing measurements and modeling of radio spectrum usage have shown that exclusive access leads to low efficiency. Thus, the next generation of wireless networks is adopting new paradigms of spectrum sharing and coexistence among heterogeneous networks. However, two significant limitations in current spectrum tenancy models hinder the development of essential functions in nonexclusive spectrum access. First, these models rely on data with much coarser resolutions than those required for wireless scheduling, rendering them ineffective for spectrum prediction or characterizing spectrum access behavior in a wireless coexistence setting. Second, due to a lack of detailed data, current models cannot describe the access dynamics of individual users, leading to unjustified adoption of simplistic traffic models, such as the on/off model and the M/G/1 queue, in spectrum access algorithm research. To address these limitations, we propose the Frame-Level spectrum Model (FLuMe), a data-driven model that characterizes individual spectrum usage based on high-resolution data. This lightweight model tracks the spectrum tenancy movements of individual users using four variables. The proposed model is applied to high-resolution LTE spectrum tenancy data, from which model parameters are extracted. Comprehensive validations demonstrate the goodness-of-fit of the model and its applicability to spectrum prediction.}
}


@article{DBLP:journals/tmc/JinLTWZ24,
	author = {Meng Jin and
                  Kexin Li and
                  Xiaohua Tian and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Graph Based {RFID} Grouping for Fast and Robust Inventory Tracking},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14201--14217},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3439430},
	doi = {10.1109/TMC.2024.3439430},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JinLTWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the design, implementation, and evaluation of TaGroup, a fast, fine-grained, and robust grouping technique for RFIDs. It can achieve a nearly 100% accuracy in distinguishing multiple groups of closely located RFIDs, within only a few seconds. It would benefit many inventory tracking applications, such as self-checkout in retails and packaging quality control in logistics. We make two technical innovations. First, we propose a novel method which can measure the channels between multiple pairs of commercial RFID tags simultaneously, and then estimate the proximity relations between them based on the channel information. Second, we introduce a spatio-temporal graph model which captures a full picture of proximity relations among all the tags, based on which TaGroup can perform a robust grouping of the tags. These two designs together boost the grouping speed and accuracy of TaGroup. Our experiments show that in grouping 120 tags into 4 closely located groups, TaGroup can achieve a nearly 100% accuracy, at the cost of only 2 seconds.}
}


@article{DBLP:journals/tmc/WeiXLZ24,
	author = {Bo Wei and
                  Weitao Xu and
                  Chengwen Luo and
                  Jin Zhang},
	title = {FaceFinger: Embracing Variance for Heartbeat Based Symmetric Key Generation
                  System},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14218--14232},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3440263},
	doi = {10.1109/TMC.2024.3440263},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WeiXLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Symmetric key generation methods are recently designed for wireless communication based on similar and unique observations of sensor measurements, such as wireless radio channels, inaudible sound channels, etc. Heartbeats, as unique biometrics, have been used for symmetric key generation. However, current solutions are designed for wearable devices with the integration of the same types of touchable heartbeat measurement equipment and fail because of the significant difference from different devices or the same devices with different deployment locations, which limits its large scale of deployment and application. To solve this problem, we propose a general heartbeat-based symmetric key generation solution by embracing observation variance from different devices, i.e., using an optical heart sensor on one finger and facing the camera of the second device to the user's face. We propose a novel data processing method to mitigate the significant difference and exploit key reconciliation to generate symmetric keys for paring devices and securing wireless communication. We have conducted extensive evaluations and shown our proposed method has good key matching rates up to 100% as well as good randomness. Security analysis has also been conducted to ensure the robustness of the proposed method.}
}


@article{DBLP:journals/tmc/HaoHJM24a,
	author = {Lifei Hao and
                  Baoqi Huang and
                  Bing Jia and
                  Guoqiang Mao},
	title = {Heterogeneous Dual-Attentional Network for WiFi and Video-Fused Multi-Modal
                  Crowd Counting},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14233--14247},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3444469},
	doi = {10.1109/TMC.2024.3444469},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HaoHJM24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowd counting aims to estimate the number of individuals in targeted areas. However, mainstream vision-based methods suffer from limited coverage and difficulty in multi-camera collaboration, which limits their scalability, whereas emerging WiFi-based methods can only obtain coarse results due to signal randomness. To overcome the inherent limitations of unimodal approaches and effectively exploit the advantage of multi-modal approaches, this paper presents an innovative WiFi and video-fused multi-modal paradigm by leveraging a heterogeneous dual-attentional network, which jointly models the intra- and inter-modality relationships of global WiFi measurements and local videos to achieve accurate and stable large-scale crowd counting. First, a flexible hybrid sensing network is constructed to capture synchronized multi-modal measurements characterizing the same crowd at different scales and perspectives; second, differential preprocessing, heterogeneous feature extractors, and self-attention mechanisms are sequentially utilized to extract and optimize modality-independent and crowd-related features; third, the cross-attention mechanism is employed to deeply fuse and generalize the matching relationships of two modalities. Extensive real-world experiments demonstrate that our method can significantly reduce the error by 26.2%, improve the stability by 48.43%, and achieve the accuracy of about 88% in large-scale crowd counting when including the videos from two cameras, compared to the best WiFi unimodal baseline.}
}


@article{DBLP:journals/tmc/LiuWLCZS24,
	author = {Shu Liu and
                  Dingzhu Wen and
                  Da Li and
                  Qimei Chen and
                  Guangxu Zhu and
                  Yuanming Shi},
	title = {Energy-Efficient Optimal Mode Selection for Edge {AI} Inference via
                  Integrated Sensing-Communication-Computation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14248--14262},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3440581},
	doi = {10.1109/TMC.2024.3440581},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuWLCZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing edge inference methods only consider one paradigm, i.e., one of on-device inference, on-server inference, or edge-device cooperative inference. Each paradigm has its pros and cons as well as dominant application scopes. For example, the on-device paradigm is the best choice when the inference task is not computationally intensive, the on-server paradigm is suitable if the communication capacity is strong, and the edge-device cooperative mode should be selected in the scenario of weak on-device communication and computation. However, each paradigm suffers from poor performance if deployed outside of its application scope, thus leading to limited potential and flexibility. This paper proposes an edge AI inference framework, which makes the first attempt to jointly consider the three modes for making full use of their benefits. In addition, sensing for data acquisition is enabled at both the edge server and the device. This can effectively improve the inference accuracy with rich information on the target area from two different views. On the other hand, energy cost minimization turns out to be a key target all over the world and a significant issue in wireless networks. To this end, we target minimizing the system energy cost under a given inference accuracy guarantee and other network resource constraints, by coordinating sensing, communication, and computation in different modes. By optimally solving the optimization problem, an integrated sensing-communication-computation (ISCC) based task-oriented mode selection scheme is proposed. A practical ISCC platform is built and extensive experiments are conducted to verify our theoretical analysis.}
}


@article{DBLP:journals/tmc/HuWXXSCA24,
	author = {Guojie Hu and
                  Qingqing Wu and
                  Donghui Xu and
                  Kui Xu and
                  Jiangbo Si and
                  Yunlong Cai and
                  Naofal Al{-}Dhahir},
	title = {Movable Antennas-Assisted Secure Transmission Without Eavesdroppers'
                  Instantaneous {CSI}},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14263--14279},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3438795},
	doi = {10.1109/TMC.2024.3438795},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HuWXXSCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Movable antenna (MA) technology is highly promising for improving communication performance, due to its advantage of flexibly adjusting positions of antennas to reconfigure channel conditions. In this paper, we investigate MAs-assisted secure transmission under a legitimate transmitter Alice, a legitimate receiver Bob and multiple eavesdroppers. Specifically, we consider a practical scenario where Alice has no any knowledge about the instantaneous non-line-of-sight component of the wiretap channel. Under this setup, we evaluate the secrecy performance by adopting the secrecy outage probability metric, the tight approximation of which is first derived by interpreting the Rician fading as a special case of Nakagami fading and concurrently exploiting the Laguerre series approximation. Then, we minimize the secrecy outage probability by jointly optimizing the transmit beamforming and positions of antennas at Alice. However, the problem is highly non-convex because the objective includes the complex incomplete gamma function. To tackle this challenge, we, for the first time, effectively approximate the inverse of the incomplete gamma function as a simple linear model. Based on this approximation, we arrive at a simplified problem with a clear structure, which can be solved via the developed alternating projected gradient ascent (APGA) algorithm. Considering the high complexity of the APGA, we further design another scheme where the zero-forcing based beamforming is adopted by Alice, and then we transform the problem into minimizing a simple function which is only related to positions of antennas at Alice. Such problem is well-solved via another projected gradient descent algorithm developed with a lower complexity. As demonstrated by simulations, our proposed schemes achieve significant performance gains compared to conventional schemes based on fixed-position antennas.}
}


@article{DBLP:journals/tmc/LiHCJLYYL24,
	author = {Xueliang Li and
                  Shicong Hong and
                  Junyang Chen and
                  Junkai Ji and
                  Chengwen Luo and
                  Guihai Yan and
                  Zhibin Yu and
                  Jianqiang Li},
	title = {Satisfying Energy-Efficiency Constraints for Mobile Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14280--14296},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447026},
	doi = {10.1109/TMC.2024.3447026},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiHCJLYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy-efficiency is one of the most important design criteria for mobile systems, such as smartphones and tablets. But current mobile systems always over-provision resources to satisfy users. The root cause is that, we have no knowledge on how much of system performance/energy will exactly satisfy users. Psychophysics defines the quantified link between physical stimuli and human-perceived stimuli. So, we will leverage psychophysics to study the quantified correlation between computer architecture resources (i.e., physical stimuli) and user satisfaction (i.e., human-perceived stimuli). We then exploit such correlation to precisely apportion resources to operate tasks and accurately satisfy users. Benefiting from our precisely-defined user satisfaction criteria and well-designed algorithms, we can reduce energy consumption of computer architectures by up to 42.9% without harming user experience. To the best of our knowledge, we for the first time theoretically and accurately model such substantial correlation. Our work opens a new research domain for fundamentally improving mobiles’ energy-efficiency.}
}


@article{DBLP:journals/tmc/HanWS24,
	author = {Qiaomei Han and
                  Xianbin Wang and
                  Weiming Shen},
	title = {Communication-Dependent Computing Resource Management for Concurrent
                  Task Orchestration in IoT Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14297--14312},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3444597},
	doi = {10.1109/TMC.2024.3444597},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in distributed machine learning and wireless network technologies are bringing new opportunities for Internet of Things (IoT) systems, where smart devices are often wirelessly connected to collaborate, jointly completing tasks known as communication-dependent computing (CDC) tasks. However, due to the dependence of computing on communication and the presence of concurrent tasks, it remains a challenge to optimize CDC task performance and efficiency while fulfilling multi-dimensional requirements, particularly with incomplete system information and dynamic environmental impacts. To overcome these, we present a concurrent CDC task framework to model the correlated communication and computing stages and multi-dimensional requirements of CDC tasks. We then formulate a task orchestration and resource management problem to optimize overall utility, where each task's utility is designed as a joint metric including the cumulative computing deviation and time efficiency of task completion. To solve this, we employ auxiliary graphs to capture the topological information of tasks and resources, and update weights based on the utility in dynamic environments. Subsequently, a multi-agent reinforcement learning algorithm is leveraged to make distributed decisions with incomplete information. Experiments demonstrate the proposed approach outperforms baselines in terms of task performance and efficiency, indicating our solution holds great potential.}
}


@article{DBLP:journals/tmc/BiCLWWQ24,
	author = {Suzhi Bi and
                  Haoguo Chen and
                  Xian Li and
                  Shuoyao Wang and
                  Yuan Wu and
                  Li Ping Qian},
	title = {A Two-Stage Deep Reinforcement Learning Framework for MEC-Enabled
                  Adaptive 360-Degree Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14313--14329},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443200},
	doi = {10.1109/TMC.2024.3443200},
	timestamp = {Sat, 07 Dec 2024 11:06:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/BiCLWWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging multi-access edge computing (MEC) technology effectively enhances the wireless streaming performance of 360-degree videos. By connecting a user's head-mounted device (HMD) to a smart MEC platform, the edge server (ES) can efficiently perform adaptive tile-based video streaming to improve the user's viewing experience. Under constrained wireless channel capacity, the ES can predict the user's field of view (FoV) and transmit to the HMD high-resolution video tiles only within the predicted FoV. In practice, the video streaming performance is challenged by the random FoV prediction error and wireless channel fading effects. For this, we propose in this paper a novel two-stage adaptive 360-degree video streaming scheme that maximizes the user's quality of experience (QoE) to attain stable and high-resolution video playback. Specifically, we divide the video file into groups of pictures (GOPs) of fixed playback interval, where each GOP consists of a number of video frames. At the beginning of each GOP (i.e., the inter-GOP stage), the ES predicts the FoV of the next GOP and allocates an encoding bitrate for transmitting (precaching) the video tiles within the predicted FoV. Then, during the real-time video playback of the current GOP (i.e., the intra-GOP stage), the ES observes the user's true FoV of each frame and transmits the missing tiles to compensate for the FoV prediction errors. To maximize the user's QoE under random variations of FoV and wireless channel, we propose a double-agent deep reinforcement learning framework, where the two agents operate in different time scales to decide the bitrates of inter- and intra-GOP stages, respectively. Experiments based on real-world measurements show that the proposed scheme can effectively mitigate FoV prediction errors and maintain stable QoE performance under different scenarios, achieving over 22.1% higher QoE than some representative benchmark methods.}
}


@article{DBLP:journals/tmc/ZhouSGR24,
	author = {Tianlong Zhou and
                  Tianyi Shi and
                  Hongye Gao and
                  Weixiong Rao},
	title = {Learning to Optimize State Estimation in Multi-Agent Reinforcement
                  Learning-Based Collaborative Detection},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14330--14343},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445583},
	doi = {10.1109/TMC.2024.3445583},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouSGR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the collaborative detection problem in a multi-agent environment. By exploiting onboard range-bearing sensors, mobile agents make sequential control decisions such as moving directions to gather information of movable targets. To estimate target states, i.e., target location and velocity, the classic works such as Kalman Filter (KF) and Extended Kalman Filter (EKF) impractically assume that the underlying state space model is fully known, and some recent learning-based works, i.e., KalmanNet, estimate target states alone but without estimation uncertainty, and cannot make robust control decision. To tackle such issues, we first propose a neural network-based state estimator, namely TWo-phase KALman Filter with Uncertainty quanTification (WALNUT), to explicitly give both target states and estimation uncertainty. The developed multi-agent reinforcement learning (MARL) model then takes the learned target states and uncertainty as input and makes robust actions to track movable targets. Our extensive experiments demonstrate that our work outperforms the state-of-the-art by higher tracking ability and lower localization error.}
}


@article{DBLP:journals/tmc/XuXLZHJL24,
	author = {Mengwei Xu and
                  Daliang Xu and
                  Chiheng Lou and
                  Li Zhang and
                  Gang Huang and
                  Xin Jin and
                  Xuanzhe Liu},
	title = {Efficient, Scalable, and Sustainable {DNN} Training on SoC-Clustered
                  Edge Servers},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14344--14360},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442430},
	doi = {10.1109/TMC.2024.3442430},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XuXLZHJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of industrial edge computing, a novel server architecture known as SoC-Cluster, characterized by its aggregation of numerous mobile systems-on-chips (SoCs), has emerged as a promising solution owing to its enhanced energy efficiency and seamless integration with prevalent mobile applications. Despite its advantages, the utilization of SoC-Cluster servers remains unsatisfactory, primarily attributed to the tidal patterns of user-initiated workloads. To address such inefficiency, we introduce SoCFlow+, a pioneering framework designed to facilitate the co-location of deep learning training tasks on SoC-Cluster servers, thereby optimizing resource utilization. SoCFlow+ incorporates three novel techniques tailored to mitigate the inherent limitations of commercial SoC-Cluster servers. First, it employs group-wise parallelism complemented by delayed aggregation, a strategy engineered to enhance the training efficiency and scalability of deep learning models, effectively circumventing network bottlenecks. Second, it integrates a data-parallel mixed-precision training algorithm, optimized to exploit the heterogeneous processing capabilities inherent to mobile SoCs fully. Third, SoCFlow+ employs an underclocking-aware workload re-balanacing mechanism to tackle the training performance degradation caused by the thermal control of mobile SoCs. Through rigorous experimental validation, SoCFlow+ achieves a convergence speedup ranging from 1.6× to 740× across 32 SoCs, compared to conventional benchmarks. Furthermore, when juxtaposed with commodity GPU servers (e.g., NVIDIA V100) under identical power constraints, SoCFlow+ not only exhibits comparable training speed but also achieves a remarkable reduction in energy consumption by a factor of 2.31× to 10.23×, all while preserving convergence accuracy.}
}


@article{DBLP:journals/tmc/YangTZZXH24,
	author = {Ningbin Yang and
                  Chunming Tang and
                  Tianqi Zong and
                  Zhikang Zeng and
                  Zehui Xiong and
                  Debiao He},
	title = {{RIC-SDA:} {A} Reputation Incentive Committee-Based Secure Conditional
                  Dual Authentication Scheme for VANETs},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14361--14376},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442933},
	doi = {10.1109/TMC.2024.3442933},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YangTZZXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular ad hoc networks (VANETs) establish wireless connections among all vehicles, enabling seamless mobile communication. However, existing conditional privacy protection VANETs authentication schemes fail to address the issue of potential key-exposure and do not provide accelerated vehicle authentication. In this paper, we propose a reputation incentive committee-based secure conditional dual authentication scheme for VANETs called RIC-SDA. Our proposed scheme incorporates dual authentication of the consensus committee and vehicle-to-vehicle (V2V) communication. It enables the rapid provision of dynamic vehicle epoch-key from consensus committee authentication for V2V authentication through our designed reputation incentive mechanism. To mitigate the potential key-exposure problem, we introduce a novel concept of secure vehicle epoch communication, which means V2V authentication is valid for only one epoch blockchain unit time. The proposed scheme achieves lightweight computation and incurs minimal communication overheads, with the signature size being just 137 bytes. The RIC-SDA scheme supports fast batch verification. We prove that our proposed scheme is unforgeable security under random oracle and demonstrate its feasibility by implementing it in a test network based on Ethereum Sepolia. The results demonstrate that our RIC-SDA solution outperforms the existing state-of-the-art authentication VANET schemes regarding efficiency and communication costs.}
}


@article{DBLP:journals/tmc/HanLLG24,
	author = {Pengchao Han and
                  Bo Liu and
                  Yejun Liu and
                  Lei Guo},
	title = {Cell-Less Offloading of Distributed Learning Tasks in Multi-Access
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14377--14395},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442242},
	doi = {10.1109/TMC.2024.3442242},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanLLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access edge computing (MEC) is a powerful technology that facilitates the provision of services to 6G users with ultra-low latency and high reliability, particularly in supporting artificial intelligence (AI) applications that rely on distributed machine learning (DL). However, the mobility of users poses challenges in offloading DL tasks to the MEC networks while ensuring satisfactory delay and blocking rates. Task replication emerges as a promising technique for achieving a cell-less design for mobile users. Nevertheless, existing research overlooks the replication of DL tasks involving multiple subtasks and users, as well as the high resource cost of task replication. Towards this challenge, this paper investigates the Mobility-awarE mulTi-replicA (META) DL task offloading problem in MEC networks. First, we propose a hybrid resource allocation mechanism that allocates resources to a replica with high access probability in a static manner and dynamically allocates resources to replicas with low access probabilities. Then, we develop an access base station (BS) clustering algorithm for each user to determine the optimal number of replicas. Additionally, we propose the META DL task offloading algorithms with proved approximation ratios to minimize the overall resource cost. Through simulations based on generated and real-world mobile users, we demonstrate the effectiveness of our proposed algorithms.}
}


@article{DBLP:journals/tmc/XiaoZYWHZ24,
	author = {Xuedou Xiao and
                  Yingying Zuo and
                  Mingxuan Yan and
                  Wei Wang and
                  Jianhua He and
                  Qian Zhang},
	title = {Task-Oriented Video Compressive Streaming for Real-Time Semantic Segmentation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14396--14413},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3446185},
	doi = {10.1109/TMC.2024.3446185},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaoZYWHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time semantic segmentation (SS) is a major task for various vision-based applications such as self-driving. Due to the limited computing resources and stringent performance requirements, streaming videos from camera-embedded mobile devices to edge servers for SS is a promising approach. While there are increasing efforts on task-oriented video compression, most SS-applicable algorithms apply more uniform compression, as the sensitive regions are less obvious and concentrated. Such processing results in low compression performance and significantly limits the capacity of edge servers supporting real-time SS. In this paper, we propose STAC, a novel task-oriented DNN-driven video compressive streaming algorithm tailed for SS, to strike accuracy-bitrate balance and adapt to time-varying bandwidth. It exploits DNN's gradients as sensitivity metrics for fine-grained spatial adaptive compression and includes a temporal adaptive scheme that integrates spatial adaptation with predictive coding. Furthermore, we design a new bandwidth-aware neural network, serving as a compatible configuration tuner to fit time-varying bandwidth and content. STAC is evaluated in a system with a commodity mobile device and an edge server with real-world network traces. Experiments show that STAC can save up to 63.7–75.2% of bandwidth or improve accuracy by 3.1–9.5% compared to state-of-the-art algorithms, while capable of adapting to time-varying bandwidth.}
}


@article{DBLP:journals/tmc/NiCGLWY24,
	author = {Zhichen Ni and
                  Honglong Chen and
                  Birong Gao and
                  Kai Lin and
                  Liantao Wu and
                  Jiguo Yu},
	title = {Reward-Oriented Task Offloading in Energy Harvesting Collaborative
                  Edge Computing Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14414--14426},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443868},
	doi = {10.1109/TMC.2024.3443868},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NiCGLWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread deployment of Internet of Things (IoT) devices brings more and more computation intensive or delay sensitive tasks, causing a series of challenges to efficient services. Collaborative edge computing is an effective way to solve them, where the tasks will be processed in the devices, edge servers, and cloud server in parallel. However, the above collaborative paradigm requires dense deployment of base stations (BSs) and consumes lots of energy. To address this problem, in this paper, we introduce energy harvesting technology and construct a collaborative edge computing system powered by hybrid energy. Considering the highly variable task execution delay caused by the resource contention and the unstable energy state, we further introduce the Holt Linear Exponential Smoothing Prediction to predict the delay and then propose an Online Server Control schedule called OSC based on Lyapunov optimization to obtain the optimized offloading decision without the knowledge of the future system state. The extensive simulations illustrate that the proposed OSC outperforms other benchmark ones.}
}


@article{DBLP:journals/tmc/ZhangKFH24,
	author = {Yuhao Zhang and
                  Zhufang Kuang and
                  Yanyan Feng and
                  Fen Hou},
	title = {Task Offloading and Trajectory Optimization for Secure Communications
                  in Dynamic User Multi-UAV {MEC} Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14427--14440},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442909},
	doi = {10.1109/TMC.2024.3442909},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangKFH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advantages of high mobility and flexible deployment, Unmanned Aerial Vehicle (UAV) combines with Mobile Edge Computing (MEC) is a promising technology. When dynamic Terminal Users (TUs) offload tasks to UAVs, eavesdroppers may eavesdrop on the channel information. The offloading decisions, trajectory plannings of UAVs and resource allocation with the objective of high-capacity secure communication is a challenging problem. In this paper, we design a multi-UAVs MEC system, where the original region is divided into several sub-regions and TUs offload tasks to UAVs which provide computing services for these TUs. Meanwhile, A joint optimization problem of offloading decision, resource allocation and trajectory planning is formulated, where TUs move with the Gauss-Markov random model. In addition, the Base Station (BS) emits jamming signals to evade the eavesdropping of offloading information from eavesdroppers. The goal of the optimization problem is to maximize the TUs’ minimum secure calculation capacity, and a Joint Dynamic Programming and Bidding (JDPB) algorithm is proposed to solve it. The Successive Convex Approximation (SCA) and Block Coordinate Descent (BCD) algorithms are used to handle the resource allocation and trajectory planning problems, and the bidding method is used to address the task offloading decision problem. Simulation results show that JDPB has better performance and better robustness under different parameter settings than other schemes.}
}


@article{DBLP:journals/tmc/ZhangC24,
	author = {Longji Zhang and
                  Kwan{-}Wu Chin},
	title = {{VNF} Scheduling and Sampling Rate Maximization in Energy Harvesting
                  IoT Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14441--14458},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3442809},
	doi = {10.1109/TMC.2024.3442809},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies virtual network function (VNF) scheduling in energy harvesting virtualized Internet of Things (IoT) networks. Unlike prior works, sensor devices leverage imprecise computation to vary their computational workload to conserve energy at the expense of computation quality. In this respect, an optimization problem of interest is to maximize the minimum VNF computation/execution quality. To this end, this paper presents the first mixed integer linear program (MILP) that optimizes i) the VNFs executed by each sensor device, ii) the computational resources allocated to VNFs, iii) sampling rate or amount of data supplied by sensor devices to VNFs, iv) the routing of samples to VNFs and forwarding of computation results, and v) link scheduling. In addition, this paper also proposes a heuristic, called sampling control and computation scheduling (SCACS), for large-scale networks. The simulation results show that SCACS reaches 81.66% of the optimal quality. In addition, the application completion rate when using SCACS is at most 39% higher than a benchmark that randomly selects nodes to sample targets and execute VNFs.}
}


@article{DBLP:journals/tmc/LiuDJ24,
	author = {Jingping Liu and
                  Xiujuan Du and
                  Long Jin},
	title = {A Localization Algorithm for Underwater Acoustic Sensor Networks With
                  Improved Newton Iteration and Simplified Kalman Filter},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14459--14470},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443992},
	doi = {10.1109/TMC.2024.3443992},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuDJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater acoustic localization is a crucial technique for most underwater applications. However, in highly dynamic marine environments, underwater acoustic localization faces many challenges, such as the stratification effect, the clock asynchronization, the node drift, and environmental noises. Concerning above problems, we propose a new underwater localization algorithm for mobile underwater acoustic sensor networks (UASNs). At first, the measurement biases are modeled as the combination of constant biases and random biases according to the physical mechanism of their generation and distribution characteristics in measured data. Then, an error-summation-incorporated Newton iteration (ESINI) algorithm is designed to compute the localization result along the direction of constant biases decrease, and a Taylor expansion is used to approach the actual localization result along the direction of random biases decrease. Subsequently, a simplified Kalman filter (SKF) fuses the two localization results and enhances the localization accuracy. In this way, the proposed algorithm effectively increases the accuracy of localization results without adding extra measurement. Finally, theoretical analyses, simulations, and lake experiments are provided to verify the proposed algorithm's effectiveness and noise resistance performance.}
}


@article{DBLP:journals/tmc/LiCNGL24,
	author = {Zhe Li and
                  Honglong Chen and
                  Zhichen Ni and
                  Yudong Gao and
                  Wei Lou},
	title = {Towards Adaptive Privacy Protection for Interpretable Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14471--14483},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443862},
	doi = {10.1109/TMC.2024.3443862},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCNGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an effective privacy-preserving mechanism that collaboratively trains the global model in a distributed manner by solely sharing model parameters rather than data from local clients, like mobile devices, to a central server. Nevertheless, recent studies have illustrated that FL still suffers from gradient leakage as adversaries try to recover training data by analyzing shared parameters from local clients. To address this issue, differential privacy (DP) is adopted to add noise to the parameters of local models before aggregation occurs on the server. It, however, results in the poor performance of gradient-based interpretability, since some important weights capturing the salient region in feature maps will be perturbed. To overcome this problem, we propose a simple yet effective adaptive gradient protection (AGP) mechanism that selectively adds noisy perturbations to certain channels of each client model that have a relatively small impact on interpretability. We also offer a theoretical analysis of the convergence of FL using our method. The evaluation results on both IID and Non-IID data demonstrate that the proposed AGP can achieve a good trade-off between privacy protection and interpretability in FL. Furthermore, we verify the robustness of the proposed method against two different gradient leakage attacks.}
}


@article{DBLP:journals/tmc/CaoCZLHLZL24,
	author = {Zhiqiang Cao and
                  Yun Cheng and
                  Zimu Zhou and
                  Anqi Lu and
                  Youbing Hu and
                  Jie Liu and
                  Min Zhang and
                  Zhijun Li},
	title = {Patching in Order: Efficient On-Device Model Fine-Tuning for Multi-DNN
                  Vision Applications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14484--14501},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443057},
	doi = {10.1109/TMC.2024.3443057},
	timestamp = {Sat, 30 Nov 2024 21:08:17 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoCZLHLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing deployment of multiple deep neural networks (DNNs) on edge devices is revolutionizing mobile vision applications, spanning autonomous vehicles, augmented reality, and video surveillance. These applications demand adaptation to contextual and environmental drifts, typically through fine-tuning on edge devices without cloud access, due to increasing data privacy concerns and the urgency for timely responses. However, fine-tuning multiple DNNs on edge devices faces significant challenges due to the substantial computational workload. In this paper, we present PatchLine, a novel framework tailored for efficient on-device training in the form of fine-tuning for multi-DNN vision applications. At the core of PatchLine is an innovative lightweight adapter design called patches coupled with a strategic patch updating approach across models. Specifically, PatchLine adopts drift-adaptive incremental patching, correlation-aware warm patching, and entropy-based sample selection, to holistically reduce the number of trainable parameters, training epochs, and training samples. Experiments on four datasets, three vision tasks, four backbones, and two platforms demonstrate that PatchLine reduces the total computational cost by an average of 55% without sacrificing accuracy compared to the state-of-the-art.}
}


@article{DBLP:journals/tmc/XiaCHLLCBY24,
	author = {Xiaoyu Xia and
                  Feifei Chen and
                  Qiang He and
                  Ruikun Luo and
                  Bowen Liu and
                  Caslon Chua and
                  Rajkumar Buyya and
                  Yun Yang},
	title = {EdgeShield: Enabling Collaborative DDoS Mitigation at the Edge},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14502--14513},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3443260},
	doi = {10.1109/TMC.2024.3443260},
	timestamp = {Mon, 10 Feb 2025 14:01:42 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/XiaCHLLCBY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing (EC) enables low-latency services by pushing computing resources to the network edge. Due to the geographic distribution and limited capacities of edge servers, EC systems face the challenge of edge distributed denial-of-service (DDoS) attacks. Existing systems designed to fight cloud DDoS attacks cannot mitigate edge DDoS attacks effectively due to new attack characteristics. In addition, those systems are typically activated upon detected attacks, which is not always realistic in EC systems. DDoS mitigation needs to be cohesively integrated with workload migration at the edge to ensure timely responses to edge DDoS attacks. In this paper, we present EdgeShield, a novel DDoS mitigation system that leverages edge servers’ computing resources collectively to defend against edge DDoS attacks without the need for attack detection. Aiming to maximize system throughput over time without causing significant service delays, EdgeShield monitors service delays and migrates workloads across an EC system with adaptive mitigation strategies. The experimental results show that EdgeShield significantly outperforms state-of-the-art solutions in both system throughput and service delays.}
}


@article{DBLP:journals/tmc/NiuDQG24,
	author = {Ziru Niu and
                  Hai Dong and
                  A. Kai Qin and
                  Tao Gu},
	title = {FLrce: Resource-Efficient Federated Learning With Early-Stopping Strategy},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14514--14529},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447000},
	doi = {10.1109/TMC.2024.3447000},
	timestamp = {Wed, 04 Dec 2024 16:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/NiuDQG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of the network also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like energy and communication resources like bandwidth. To comprehensively address these challenges, in this paper, we present FLrce, an efficient FL framework with a relationship-based client selection and early-stopping strategy. FLrce accelerates the FL process by selecting clients with more significant effects, enabling the global model to converge to a high accuracy in fewer rounds. FLrce also leverages an early stopping mechanism that terminates FL in advance to save communication and computation resources. Experiment results show that, compared with existing efficient FL frameworks, FLrce improves the computation and communication efficiency by at least 30% and 43% respectively.}
}


@article{DBLP:journals/tmc/DangJFLPCLL24,
	author = {Fan Dang and
                  Xinqi Jin and
                  Qi{-}An Fu and
                  Lingkun Li and
                  Guanyan Peng and
                  Xinlei Chen and
                  Kebin Liu and
                  Yun{-}Hao Liu},
	title = {StreamingTag: {A} Scalable Piracy Tracking Solution for Mobile Streaming
                  Services},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14530--14543},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445411},
	doi = {10.1109/TMC.2024.3445411},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DangJFLPCLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Streaming services have billions of mobile subscribers, yet video piracy has cost service providers billions. Digital Rights Management (DRM), however, is still far from satisfactory. Unlike DRM, which attempts to prohibit the creation of pirated copies, fingerprinting may be used to track out the source of piracy. Nevertheless, existing fingerprinting-based streaming systems are not widely used since they fail to serve numerous users. In this paper, we present the design and evaluation of StreamingTag, a scalable piracy tracing system for mobile streaming services. StreamingTag adopts a segment-level fingerprint embedding scheme to remove the need of re-embedding the fingerprint into the video for each new viewer. The key innovations of StreamingTag include a scalable and CDN-friendly delivery framework, an accurate and lightweight temporal synchronization scheme, a polarized and randomized SVD watermarking scheme, and a collusion-resistant fingerprinting scheme. Experiment results show the good QoS of StreamingTag in terms of preparation latency, bandwidth consumption, and video fidelity. Compared with existing methods, the proposed three schemes improve the re-identification accuracy by 4-49x, the watermark extraction accuracy by 2.25x at most and 1.5x on average, and the recall rate of catching colluders by 26%.}
}


@article{DBLP:journals/tmc/WangLXYW24,
	author = {En Wang and
                  Dongming Luan and
                  Yuanbo Xu and
                  Yongjian Yang and
                  Jie Wu},
	title = {Distributed Task Selection for Crowdsensing: {A} Game-Theoretical
                  Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14544--14560},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449039},
	doi = {10.1109/TMC.2024.3449039},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangLXYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile CrowdSensing (MCS) is a promising sensing paradigm that leverages users’ mobile devices to collect and share data for various applications. A key challenge in MCS is task allocation, which aims to assign sensing tasks to suitable users efficiently and effectively. Existing task allocation approaches are mostly centralized, requiring users to disclose their private information and facing high computational complexity. Moreover, centralized approaches may not satisfy users’ preferences or incentives. To address these issues, we propose a novel distributed task allocation scheme based on route navigation systems. We consider two scenarios: time-tolerant tasks and time-sensitive tasks, and formulate them as potential games. We design distributed algorithms to achieve Nash equilibria while considering users’ individual preferences and the platform’s task allocation objectives. We also analyze the convergence and performance of our algorithm theoretically. In the time-sensitive task scenario, the problem becomes even more intricate due to temporal conflicts among tasks. We prove the task selection problem is NP-hard and propose a distributed task selection algorithm. In contrast to existing distributed approaches that require users to deviate from their regular routes, our method ensures task completion while minimizing disruption to users. Trace-based simulation results validate that the proposed algorithm attains a Nash equilibrium and offers a total user profit performance closely aligned with that of the optimal solution.}
}


@article{DBLP:journals/tmc/LinZLJDYWC24,
	author = {Shaohui Lin and
                  Xiaoxi Zhang and
                  Yupeng Li and
                  Carlee Joe{-}Wong and
                  Jingpu Duan and
                  Dongxiao Yu and
                  Yu Wu and
                  Xu Chen},
	title = {Online Management for Edge-Cloud Collaborative Continuous Learning:
                  {A} Two-Timescale Approach},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14561--14574},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3451715},
	doi = {10.1109/TMC.2024.3451715},
	timestamp = {Tue, 11 Feb 2025 16:38:19 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LinZLJDYWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) powered real-time applications usually need continuous training using data streams generated over time and across different geographical locations. Enabling data offloading among computation nodes through model training is promising to mitigate the problem that devices generating large datasets may have low computation capability. However, offloading can compromise model convergence and incur communication costs, which must be balanced with the long-term cost spent on computation and model synchronization. Therefore, this paper proposes EdgeC3, a novel framework that can optimize the frequency of model aggregation and dynamic offloading for continuously generated data streams, navigating the trade-off between long-term accuracy and cost. We first provide a new error bound to capture the impacts of data dynamics that are varying over time and heterogeneous across devices, as well as quantifying varied data heterogeneity between local models and the global one. Based on the bound, we design a two-timescale online optimization framework. We periodically learn the synchronization frequency to adapt with uncertain future offloading and network changes. In the finer timescale, we manage online offloading by extending Lyapunov optimization techniques to handle an unconventional setting, where our long-term global constraint can have abruptly changed aggregation frequencies that are decided in the longer timescale. Finally, we theoretically prove the convergence of EdgeC3 by integrating the coupled effects of our two-timescale decisions, and we demonstrate its advantage through extensive experiments performing distributed DL training for different domains.}
}


@article{DBLP:journals/tmc/ChiZXLQ24,
	author = {Jiancheng Chi and
                  Xiaobo Zhou and
                  Fu Xiao and
                  Yuto Lim and
                  Tie Qiu},
	title = {Task Offloading via Prioritized Experience-Based Double Dueling {DQN}
                  in Edge-Assisted IIoT},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14575--14591},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452502},
	doi = {10.1109/TMC.2024.3452502},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ChiZXLQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Industrial Internet of Things (IIoT), Multi-access Edge Computing (MEC) emerges as a transformative paradigm for managing computation-intensive tasks, where task offloading plays an important role. However, due to the complex environment of IIoT, existing deep reinforcement learning-based schemes suffer from significant shortcomings in accuracy and convergence speed during model training when addressing the issue of task offloading. In this paper, to solve this problem, we propose an online task offloading scheme based on reinforcement learning, leveraging the double deep Q network (DQN) and dueling DQN with a prioritized experience replay mechanism, called the Prioritized experience-based Double Dueling DQN task offloading scheme (P-D3QN). P-D3QN enhances action selection accuracy using double DQN and mitigates Q-value overestimation by decomposing state and advantage using dueling DQN. Additionally, we adopt the prioritized experience replay mechanism to enhance the convergence speed of model training by selecting transitions that induce a higher training error between the evaluation network and the target network. Experimental results demonstrate that P-D3QN outperforms several state-of-the-art schemes, achieving a reduction of 21.0% in the average cost of the task and improving the completion rate of the task by 19.5%.}
}


@article{DBLP:journals/tmc/HanYJXYHLCH24,
	author = {Mingda Han and
                  Huanqi Yang and
                  Mingda Jia and
                  Weitao Xu and
                  Yanni Yang and
                  Zhijian Huang and
                  Jun Luo and
                  Xiuzhen Cheng and
                  Pengfei Hu},
	title = {Seeing the Invisible: Recovering Surveillance Video With {COTS} mmWave
                  Radar},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14592--14606},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445507},
	doi = {10.1109/TMC.2024.3445507},
	timestamp = {Sun, 24 Nov 2024 18:58:06 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanYJXYHLCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video surveillance systems play a crucial role in ensuring public safety and security by capturing and monitoring critical events in various areas. However, traditional surveillance cameras face limitations when it comes to malicious physical damage or obscuring by offenders. To overcome this limitation, we propose m^{2}2 Vision, which is the first millimeter-wave (mmWave)-based video reconstruction system designed to enhance existing video surveillance cameras. m^{2}2 Vision utilizes mmWave to sense the profile and motion signature of the target, integrating it with previously acquired visual data about the environment and the target's appearance, thereby facilitating the reconstruction of surveillance video. Specifically, our proposed system incorporates a dual-stage mmWave signal denoising algorithm to efficiently eliminate the noise and multiple-input multiple-output virtual antenna enhanced heatmap generation (MVAE-HG) method to obtain fine-grained mmWave heatmaps responsive to the target's profile and motion information. Moreover, we design the mm2Video generative network that first employs a multi-modal fusion module to fuse the mmWave and pre-acquired visual data, then use a conditional generative adversarial network (cGAN)-based video reconstruction module for surveillance video reconstruction. We conducted comprehensive experiments on m^{2}2 Vision using a commercial mmWave radar and four surveillance cameras across various environments, with the participation of seven individuals. Evaluation results show that m^{2}2 Vision can achieve an average structural similarity index measure (SSIM) of 0.93, demonstrating its effectiveness and potential.}
}


@article{DBLP:journals/tmc/LiGLWCHXXX24,
	author = {Jing Li and
                  Song Guo and
                  Weifa Liang and
                  Jianping Wang and
                  Quan Chen and
                  Zicong Hong and
                  Zichuan Xu and
                  Wenzheng Xu and
                  Bin Xiao},
	title = {AoI-Aware Service Provisioning in Edge Computing for Digital Twin
                  Network Slicing Requests},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14607--14621},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449818},
	doi = {10.1109/TMC.2024.3449818},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiGLWCHXXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twins are poised to enter our lives with Industry 4.0. The Digital Twin Network (DTN) paradigm is projected to deliver upon the promise of efficient collaboration among digital twins to enable complicated and systematic services across many domains, through depicting an overall picture of a group of physical objects. To achieve timely data processing of digital twins, Mobile Edge Computing (MEC) shifts the computational power towards the network edge, and network slicing is well-suited to bundle heterogeneous physical resources to build logical networks based on edge servers for accommodating DTNs. In light of this, in this paper we investigate DTN slicing-enabled service provisioning in MEC, where each DTN slice consists of one master digital twin and a set of worker digital twins, and each worker digital twin is synchronized through collecting data from a respective object periodically. The master digital twin aggregates the processed data from worker digital twins to model the DTN continuously for user query services, whilst meeting delay requirements of users. We capture the utility gain of a DTN slicing request based on the DTN model quality at its master digital twin that is impacted by the Age of Information (AoI), and we focus on two novel optimization problems: the utility maximization problem for a single DTN slicing request, and the dynamic utility maximization problem for multiple DTN slicing requests. We propose an approximation algorithm for the former, and an online algorithm with a provable competitive ratio for the latter. We also evaluate the performance of the proposed algorithms through simulations. Experimental results demonstrate that the proposed algorithms are promising, outperforming their counterparts by at least 10.2%.}
}


@article{DBLP:journals/tmc/OuyangWLDJW24,
	author = {Jinhui Ouyang and
                  Mingzhu Wu and
                  Xinglin Li and
                  Hanhui Deng and
                  Zhanpeng Jin and
                  Di Wu},
	title = {NeuroBCI: Multi-Brain to Multi-Robot Interaction Through EEG-Adaptive
                  Neural Networks and Semantic Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14622--14637},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3446829},
	doi = {10.1109/TMC.2024.3446829},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/OuyangWLDJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in EEG-based BCI technologies have been explored to assist individuals in executing brain-to-robot tasks. In the future, using BCI systems in both personal and professional domains is anticipated in widespread adoption. One promising application is facing home life, to enable people to use commercial EEG equipment to implement daily tasks. However, current BCI studies mainly focus on single-brain-to-single-robot interaction, which has limitations in representing diverse human intentions. A generalized BCI system with multiple EEG devices allows users to more precisely or collaboratively control robots. Therefore, it is imperative to extend the BCI techniques to future collaboration scenarios. In this paper, we present a new system, NeuroBCI, for multi-brain-to-multi-robot interaction through the integration of sensing, computing, communication, and control. To improve sensing efficiency, NeuroBCI employs a sparse attention mechanism to extract joint features from heterogeneous EEG data. Parallel computation and transmission for multi-user multi-task scenarios are handled by semantic autoencoder and autodecoder communications. A code map is designed to ensure concurrent control and model compression methods are used on both transmitter and receiver sides. Our experiments in comparison with state-of-the-art works show the superior performance of NeuroBCI on sensing, computing, communication, and control as a holistic system.}
}


@article{DBLP:journals/tmc/SharmaGBP24,
	author = {Sachin Sharma and
                  Saptarshi Ghosh and
                  Manav R. Bhatnagar and
                  Bijaya K. Panigrahi},
	title = {Power Efficient Handoff Management in Hybrid {V2X} Communication:
                  Game-Theoretic Approach to Resource Allocation With Load Reduction},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14638--14655},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449374},
	doi = {10.1109/TMC.2024.3449374},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SharmaGBP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, we jointly investigate the connectivity probability and the number of required resource blocks (RBs) for load reduction during handoff (HO) decision for hybrid vehicle-to-everything (V2X) communication. Frequent HOs is a major issue in vehicular communication, causing various concerns such as loading problems in the overlapping area, signalling overhead, and decreasing the bandwidth efficiency of the network. To combat the loading problem due to HO in the overlapping area and the low convergence issues, we propose a game-theoretic approach to resolve the imbalance of resource allocation amongst the clusters at the edge of the coverage area. The proposed game-based Flexible Resource Allocation (FRA) approach optimally redistributes the resources to all clusters within a g-NodeB (g-NB). We obtain a closed-form expression of the transmit power for a cellular-vehicle-to-everything (C-V2X) standard of 5G network, besides analyzing the effects of connectivity probability and required resources for vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) during HO. We explore the implications of non-uniform speeds among the clusters and perform an analysis of time complexity. Additionally, we compare two types of cost functions. A linear cost function links a player’s cost to their action proportionally, while a nonlinear cost function shows a non-proportional relationship between a player’s cost and their action. The simulation results indicate that, with an increasing number of clusters, the linear cost case outperforms the non-linear case in both connectivity probability (which increases by 30.18%) and the number of required resources (which is reduced by 39%) in the HO region. The analytical derivation has been verified through simulation results.}
}


@article{DBLP:journals/tmc/LuXXWQW24,
	author = {Xiaozhen Lu and
                  Liang Xiao and
                  Yilin Xiao and
                  Wei Wang and
                  Nan Qi and
                  Qian Wang},
	title = {Risk-Aware Federated Reinforcement Learning-Based Secure IoV Communications},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14656--14671},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447019},
	doi = {10.1109/TMC.2024.3447019},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuXXWQW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth in the number of high-mobility vehicles and booming enhanced applications with restricted latency requirements, downlink communication in Internet of Vehicles (IoV) systems has become increasingly vulnerable to active eavesdropping attacks. This paper proposes a federated learning-enabled secure communication framework for IoV against active eavesdropping, in which the roadside units (RSUs) apply reinforcement learning (RL) model to optimize their downlink transmit power levels, and the server helps update the RL models of the RSUs. First, we design a multi-agent deep RL algorithm for each RSU, which designs a punishment and a blacklist mechanism to mitigate risky explorations related to severe data leakage or communication outages. Second, this framework designs a risk-aware RL for the server, which uses a two-level hierarchical structure to choose the number of participated RSUs and the corresponding local training data size for higher optimization speed. This framework considers both the reward and risk in the selection of policies to reduce the probability of exploring the risky training policies that cause defense failure of the RSUs against active eavesdropping. Third, we analyze the convergence performance, computational complexity, and reward upper bound, which reveals how the power constraint, radio bandwidth and data size affect the secure communication performance. Simulation and experimental results validate the effectiveness of our schemes, such as the reductions of the eavesdropping rate, training latency, and the loss of local models compared to the benchmarks.}
}


@article{DBLP:journals/tmc/LuLCXWW24,
	author = {Xiaozhen Lu and
                  Zhibo Liu and
                  Yuhan Chen and
                  Liang Xiao and
                  Wei Wang and
                  Qihui Wu},
	title = {Risk-Aware Reinforcement Learning-Based Federated Learning for IoV
                  Systems},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14672--14688},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447034},
	doi = {10.1109/TMC.2024.3447034},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LuLCXWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) that improves data privacy reduces the computational overhead for Internet of Vehicles (IoV) systems but has difficulty in defending against selfish attacks due to the restricted quality of service requirements and the high mobility of vehicles. In this paper, we design a risk-aware hierarchical reinforcement learning-based FL framework for IoV to resist selfish attacks. By designing a two-level hierarchical policy selection module that consists of two deep neural networks, this framework divides the training policy into two sub-policies, i.e., the selection of FL participants and the corresponding local training data size, which are chosen based on the previous training performance and vehicle participation performance. This framework designs a risk-aware safety guide to avoid dangerous states such as local task failure resulting from risky training policies. Specifically, the guide uses a warning signal to evaluate the short-term risk of each state-action pair, applies an R-network to estimate the long-term risks for modifying the chosen training policy, and designs a punishment function for the modified training policy to revise the immediate reward to further enhance the safe exploration. We analyze the convergence performance and computational complexity of our scheme. Experimental results on MNIST, CIFAR-10, and Stanford Cars datasets verify the effectiveness of our scheme, including the global model accuracy, training latency, detection success rate, and convergence speed compared with the benchmarks FedAvg, MFL, DQNPS, and SHRL.}
}


@article{DBLP:journals/tmc/CaoCLLWL24,
	author = {Jiani Cao and
                  Jiesong Chen and
                  Chengdong Lin and
                  Yang Liu and
                  Kun Wang and
                  Zhenjiang Li},
	title = {Practical Gaze Tracking on Any Surface With Your Phone},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14689--14707},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445373},
	doi = {10.1109/TMC.2024.3445373},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CaoCLLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces ASGaze, a novel gaze tracking system using the RGB camera of smartphones. ASGaze improves the accuracy of existing methods and uniquely tracks gaze points on various surfaces, including phone screens, computer displays, and non-electronic surfaces like whiteboards or paper - a situation that is challenging for existing methods. To achieve this, we revisit the 3D geometric eye model, commonly used in high-end commercial trackers, and it has the potential to achieve our goals. To avoid the high cost of commercial solutions, we identify three fundamental issues when processing the eye model with an RGB camera, including how to accurately extract iris boundary that is the meta-information in our design, how to remove ambiguity from iris boundary to gaze point transformation, and how to map gaze points onto the target surface. Furthermore, as we consider deploying ASGaze in real-world applications, two additional challenges should be addressed: how to automatically and accurately annotate the training dataset to reduce manual labor and time costs, and how to accelerate the inference speed of ASGaze on mobile devices to improve user experience. We propose effective techniques to resolve these issues. Our prototype and experiments on three tracking surfaces demonstrate significant performance gains.}
}


@article{DBLP:journals/tmc/LiangTWWJ24,
	author = {Yonghui Liang and
                  Huijun Tang and
                  Huaming Wu and
                  Yixiao Wang and
                  Pengfei Jiao},
	title = {Lyapunov-Guided Offloading Optimization Based on Soft Actor-Critic
                  for ISAC-Aided Internet of Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14708--14721},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445350},
	doi = {10.1109/TMC.2024.3445350},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiangTWWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to numerous computation-intensive and delay-sensitive tasks in the Internet of Vehicles (IoV), Vehicular Edge Computing (VEC) is increasingly playing a crucial role as a key solution in the IoV. However, how to concurrently enhance communication quality and reduce the cost of latency and energy has emerged as a critical challenge in VEC. To tackle the above problem, we propose a Lyapunov-guided offloading based on the Soft Actor-Critic (SAC) algorithm, named LySAC, to minimize the average cost of the Integrated Sensing and Communications (ISAC) technology-aided IoV, where ISAC technology can effectively improve the communication quality by harnessing high-frequency waveforms to seamlessly integrate communication and sensing functionalities. First, we model the offloading process of ISAC-Aided IoV as an optimization problem of the joint cost of delay and energy with long-term energy consumption and queue stability. Then we formulate the optimization problem as a Lyapunov optimization and utilize the SAC method to find the optimal offloading decisions. Finally, we conduct extensive experiments and the results demonstrate the effectiveness and superiority of the proposed LySAC in minimizing total cost while maintaining queue stability and meeting long-term energy requirements compared with other several baseline schemes.}
}


@article{DBLP:journals/tmc/WangQHBC24,
	author = {Gang Wang and
                  Qi Qi and
                  Rui Han and
                  Lin Bai and
                  Jinho Choi},
	title = {{P2CEFL:} Privacy-Preserving and Communication Efficient Federated
                  Learning With Sparse Gradient and Dithering Quantization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14722--14736},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3445957},
	doi = {10.1109/TMC.2024.3445957},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangQHBC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) offers a promising framework for obtaining a global model by aggregating trained parameters from participating clients without transmitting their local private data. To further enhance privacy, differential privacy (DP)-based FL can be considered, wherein certain amounts of noise are added to the transmitting parameters, inevitably leading to a deterioration in communication efficiency. In this paper, we propose a novel Privacy-Preserving and Communication Efficient Federated Learning (P2CEFL) algorithm to reduce communication overhead under DP guarantee, utilizing sparse gradient and dithering quantization. Through gradient sparsification, the upload overhead for clients decreases considerably. Additionally, a subtractive dithering approach is employed to quantize sparse gradient, further reducing the bits for communication. We conduct theoretical analysis on privacy protection and convergence to verify the effectiveness of the proposed algorithm. Extensive numerical simulations show that the P2CEFL algorithm can achieve a similar level of model accuracy and significantly reduce communication costs compared to existing conventional DP-based FL methods.}
}


@article{DBLP:journals/tmc/WangYLMWTL24,
	author = {Chenyang Wang and
                  Hao Yu and
                  Xiuhua Li and
                  Fei Ma and
                  Xiaofei Wang and
                  Tarik Taleb and
                  Victor C. M. Leung},
	title = {Dependency-Aware Microservice Deployment for Edge Computing: {A} Deep
                  Reinforcement Learning Approach With Network Representation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14737--14753},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453069},
	doi = {10.1109/TMC.2024.3453069},
	timestamp = {Mon, 03 Mar 2025 22:25:38 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangYLMWTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of microservices in industry has sparked much attention in the research community. Despite significant progress in microservice deployment for resource-intensive services and applications at the network edge, the intricate dependencies among microservices are often overlooked, and some studies underestimate the importance of system context extraction in deployment strategies. This paper addresses these issues by formulating the microservice deployment problem as a max-min problem, considering system cost and quality of service (QoS) jointly. We first study the attention-based microservice representation (AMR) method to achieve effective system context extraction. In this way, the contributions of different computing power providers (users, edge servers, or cloud servers) in the networks can be effectively paid attention to. Subsequently, we propose the attention-modified soft actor-critic (ASAC) algorithm to tackle the microservice deployment problem. ASAC leverages attention mechanisms to enhance decision-making and adapt to changing system dynamics. Our simulation results demonstrate ASAC's effectiveness, prioritizing average system cost and reward compared to the other state-of-the-art algorithms.}
}


@article{DBLP:journals/tmc/YanLLZYXR24,
	author = {Hao Yan and
                  Feng Lin and
                  Jin Li and
                  Meng Zhang and
                  Zhisheng Yan and
                  Jian Xiao and
                  Kui Ren},
	title = {Shadow Based Non-Line-of-Sight Pedestrian Rushing Detection for Automated
                  Driving},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14754--14767},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447907},
	doi = {10.1109/TMC.2024.3447907},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/YanLLZYXR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Among the foremost contributors to compromised driving safety is the abrupt emergence of obstacles or pedestrians within drivers’ non-line-of-sight regions. Previous investigations into non-line-of-sight imaging have predominantly depended on costly apparatus or have been confined to controlled laboratory settings (e.g., extensive planar reflectors and regulated illumination). Consequently, these technological approaches prove impractical within intricate driving environments. In this paper, we introduce a shadow based non-line-of-sight moving obstacle detection system devised to augment Advanced Driver Assistance Systems (ADAS), ensuring adequate time for safe response and halting. Our approach incorporates a shadow signal discriminator tailored to evaluate faint shadows generated by moving obstacles, such as pedestrians within blind spots. Note that we merely use commercial onboard sensors and our system is robust to various lighting scenarios and planar reflectors. We comprehensively assess our methodology's adaptability by employing datasets acquired from real-world driving scenarios encompassing diverse road surfaces and lighting conditions. The results substantiate the system's efficacy in detecting pedestrians in motion within NLOS regions, showcasing an impressive detection range of 22 meters. This proficiency enables the system to pre-emptively forewarn the ADAS, facilitating the maintenance of a safe distance from the pedestrian.}
}


@article{DBLP:journals/tmc/ZhuLCSL24,
	author = {Yuchen Zhu and
                  Min Liu and
                  Yali Chen and
                  Sheng Sun and
                  Zhongcheng Li},
	title = {SkyOrbs: {A} Fast 3-D Directional Neighbor Discovery Algorithm for
                  {UAV} Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14768--14786},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3451991},
	doi = {10.1109/TMC.2024.3451991},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhuLCSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neighbor discovery (ND) is a critical network initialization stage, particularly challenging for highly-dynamic unmanned aerial vehicle (UAVs) with directional antennas. Considering that directional antennas focus signal energy in one direction, successful ND requires a pair of UAVs to point antennas towards each other simultaneously. However, due to the inherent constraints of autonomous UAVs (e.g., high mobility and decentralized coordination), spatial alignment of directional beams is difficult. Existing works resort to ideal assumptions (e.g., clock synchronization, assistance of omni-directional antennas and prior information) for simplification. Moreover, previous ND algorithms assume unlimited switching capability for directional antennas, often unrealistic for traditional mechanically steered antennas. In this paper, we propose SkyOrbs, a fast directional ND algorithm for UAV networks without these ideal assumptions. To reduce ND latency, SkyOrbs presents a skip scanning strategy, dynamically adjusting antenna rotation speed to enhance discovery probability. Furthermore, to mitigate the uncertain rotation overhead induced by time-variant angular speed, SkyOrbs designs a novel antenna scanning path that accommodates limited mechanical rotation capacity. We analyze the theoretical delay performance of SkyOrbs, and expand its applicability to broader scenarios. Evaluation results show that SkyOrbs can reduce discovery latency by 40.8% and rotation overhead by 55.0% compared to the baseline method.}
}


@article{DBLP:journals/tmc/DengWZTDKLN24,
	author = {Dongshang Deng and
                  Xuangou Wu and
                  Tao Zhang and
                  Xiangyun Tang and
                  Hongyang Du and
                  Jiawen Kang and
                  Jiqiang Liu and
                  Dusit Niyato},
	title = {FedASA: {A} Personalized Federated Learning With Adaptive Model Aggregation
                  for Heterogeneous Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14787--14802},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3446271},
	doi = {10.1109/TMC.2024.3446271},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DengWZTDKLN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) opens a new promising paradigm for the Industrial Internet of Things (IoT) since it can collaboratively train machine learning models without sharing private data. However, deploying FL frameworks in real IoT scenarios faces three critical challenges, i.e., statistical heterogeneity, resource constraint, and fairness. To address these challenges, we design a fair and efficient FL method, termed FedASA, which can address the challenge of statistical heterogeneity in resource-constrained scenarios by determining the shared architecture adaptively. In FedASA, we first present a cell-wised shared architecture selection strategy, which can adaptively construct the shared architecture for each device. We then design a cell-based aggregation algorithm for aggregating heterogeneous shared architectures. In addition, we provide a theoretical analysis of the federated error bound, which provides a theoretical guarantee for the fairness. At the same time, we prove the convergence of FedASA at the first-order stationary point. We evaluate the performance of FedASA through extensive simulation and experiments. Experimental results in cross-location scenarios show that FedASA outperformed the state-of-the-art approaches, improving accuracy by up to 13.27% with better fairness and faster convergence and communication requirement has been reduced by 81.49%.}
}


@article{DBLP:journals/tmc/SunWSWKNL24,
	author = {Geng Sun and
                  Yixian Wang and
                  Zemin Sun and
                  Qingqing Wu and
                  Jiawen Kang and
                  Dusit Niyato and
                  Victor C. M. Leung},
	title = {Multi-Objective Optimization for Multi-UAV-Assisted Mobile Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14803--14820},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3446819},
	doi = {10.1109/TMC.2024.3446819},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunWSWKNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent developments in unmanned aerial vehicles (UAVs) and mobile edge computing (MEC) have provided users with flexible and resilient computing services. However, meeting the computation-intensive and delay-sensitive demands of users poses a significant challenge due to the limited resources of UAVs. To address this challenge, we consider a multi-UAV-assisted MEC system. Based on this system, we formulate a multi-objective optimization problem aiming at minimizing the total task completion delay, reducing the total UAV energy consumption, and maximizing the total number of offloaded tasks. Since the problem is a mixed-integer non-linear programming (MINLP) and NP-hard problem, we propose a joint task offloading, computation resource allocation, and UAV trajectory control (JTORATC) approach. The problem is split into three components to cope with the coupling of these decision variables, and then solved individually to obtain the corresponding decisions. Specifically, the sub-problem of task offloading is solved by using distributed splitting and threshold rounding methods, the sub-problem of computation resource allocation is solved by adopting the Karush-Kuhn-Tucker (KKT) method, and the sub-problem of UAV trajectory control is solved by employing the successive convex approximation (SCA) method. Simulation results show that the proposed JTORATC has superior performance compared with the other benchmark methods.}
}


@article{DBLP:journals/tmc/WangZCLCZG24,
	author = {Zhenxi Wang and
                  Hongzi Zhu and
                  Yunxiang Cai and
                  Quan Liu and
                  Shan Chang and
                  Liang Zhang and
                  Minyi Guo},
	title = {Enabling Long Range Point Cloud Registration in Vehicular Networks
                  via Muti-Hop Relays},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14821--14833},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3446828},
	doi = {10.1109/TMC.2024.3446828},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZCLCZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Point cloud registration (PCR) can significantly extend the visual field and enhance the point density on distant objects, thereby improving driving safety. However, it is very challenging for vehicles to perform online registration between long-range point clouds. In this paper, we propose an online long-range PCR scheme in VANETs, called LoRaPCR, where vehicles achieve long-range registration through multi-hop short-range highly-accurate registrations. Given the NP-hardness of the problem, a heuristic algorithm is developed to determine best registration paths while leveraging the reuse of registration results to reduce computation costs. Moreover, we utilize an optimized dynamic programming algorithm to determine the transmission routes while minimizing the communication overhead. To the best of our knowledge, LoRaPCR is the first solution to achieve multi-vehicle point cloud long-range registration. Results of extensive experiments demonstrate that LoRaPCR can achieve high PCR accuracy with low relative translation and rotation errors of 0.55 meters and 1.43{}^{\\circ }, respectively, at a distance of over 100 meters, and reduce the computation overhead by more than 50% compared to the state-of-the-art method.}
}


@article{DBLP:journals/tmc/PengXXLDFSZX24,
	author = {Jincheng Peng and
                  Huanlai Xing and
                  Lexi Xu and
                  Shouxi Luo and
                  Penglin Dai and
                  Li Feng and
                  Jing Song and
                  Bowen Zhao and
                  Zhiwen Xiao},
	title = {Adversarial Reinforcement Learning Based Data Poisoning Attacks Defense
                  for Task-Oriented Multi-User Semantic Communication},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14834--14851},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3447087},
	doi = {10.1109/TMC.2024.3447087},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/PengXXLDFSZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-user semantic communication (MUSC) has emerged as a promising paradigm for future 6G networks and applications, where massive clients (e.g., mobile devices) collaboratively construct a global semantic decoder without sharing their local data. However, due to the lack of direct access to clients’ data, MUSC is vulnerable to data poisoning attacks (DPAs), wherein malicious participants send updates derived from poisoned training samples. Current defense techniques against DPAs are designed for traditional networks and are not directly applicable to MUSC. In this paper, we propose an effective attack-defense game framework, denoted as DPAD-MUSC, tailored to defend against DPAs during image transmission for MUSC. First, we determine each attack-type's optimal attack policy based on reinforcement learning, with the aim of strengthening the attack while avoiding detection. To generate adversarial samples accordingly, we devise an adversarial samples generator (ADV-Generator) based on conditional generative adversarial network (CGAN). Then, we introduce an attack defender (DPA-Defender) to detect data poisoning attacks and exclude poisoned samples from the target model's learning process, with the adversarial samples generated under the guidance of the optimal attack policy to enhance the detector's robustness. Simulation results demonstrate that the DPAD-MUSC can find optimal attack policies that cause a greater accuracy drop in the target model while maintaining a higher evasion rate. The ADV-Generator can generate effective adversarial samples and the DPA-Defender outperforms five state-of-the-art methods on three widely used image datasets under additive white Gaussian noise (AWGN) channel in terms of Top-1 accuracy.}
}


@article{DBLP:journals/tmc/LiCWGYXQ24,
	author = {Zijian Li and
                  Zihan Chen and
                  Xiaohui Wei and
                  Shang Gao and
                  Hengshan Yue and
                  Zhewen Xu and
                  Tony Q. S. Quek},
	title = {Exploiting Complex Network-Based Clustering for Personalization-Enhanced
                  Hierarchical Federated Edge Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14852--14870},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449129},
	doi = {10.1109/TMC.2024.3449129},
	timestamp = {Wed, 12 Feb 2025 08:30:41 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCWGYXQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has been extensively applied in urban environmental prediction tasks of mobile edge computing by training a global machine learning model without data sharing. However, the training of FL faces the challenges such as the poor generalization capability of a single global model over heterogeneous data and hefty communication overhead caused by the frequent model exchange between massive edge servers and remote cloud servers. To address such issues, we propose HPFL-CN, a novel communication-efficient Hierarchical Personalized Federated edge Learning framework with Complex Network clustering. HPFL-CN introduces Privacy-preserving Feature Clustering (PFC) to extract privacy-preserving low-dimensional feature representations of each edge server via mapping the environmental data to different complex network domains for clustering similar edge servers accurately. Based on the clustering results of PFC, an edge-mediator-cloud hierarchical architecture is proposed to realize personalization at the cluster level by Effective Hierarchical Scheduling (EHS). Furthermore, to adapt to dynamic scenarios of new edge servers joining and streaming data generation, we further extend HPFL-CN to Adaptive personalized federated learning with dynamic grouping (Ada-HPFL-CN), which can flexibly re-group edge servers and adjust mixed model weights and the model aggregation frequency adaptively. Our extensive experiments on real-world datasets demonstrate the efficacy of our framework, which outperforms state-of-the-art FL methods regarding personalization and communication efficiency performance.}
}


@article{DBLP:journals/tmc/LiuDNKXMZS24,
	author = {Yinqiu Liu and
                  Hongyang Du and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Shiwen Mao and
                  Ping Zhang and
                  Xuemin Shen},
	title = {Cross-Modal Generative Semantic Communications for Mobile {AIGC:}
                  Joint Semantic Encoding and Prompt Engineering},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14871--14888},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449645},
	doi = {10.1109/TMC.2024.3449645},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiuDNKXMZS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Employing massive Mobile AI-Generated Content (AIGC) Service Providers (MASPs) with powerful models, high-quality AIGC services become accessible for resource-constrained end users. However, this advancement, referred to as mobile AIGC, also introduces a significant challenge: users should download large AIGC outputs from the MASPs, leading to substantial bandwidth consumption and potential transmission failures. In this paper, we apply cross-modal Generative Semantic Communications (G-SemCom) in mobile AIGC to overcome wireless bandwidth constraints. Specifically, we utilize cross-modal attention maps to indicate the correlation between user prompts and each part of AIGC outputs. In this way, the MASP can analyze the prompt context and filter the most semantically important content efficiently. Only semantic information is transmitted, with which users can recover the entire AIGC output with high quality while saving mobile bandwidth. Since the transmitted information not only preserves the semantics but also prompts the recovery, we formulate a joint semantic encoding and prompt engineering problem to optimize the bandwidth allocation among users. Particularly, we present a human-perceptual metric named Joint Perceptual Similarity and Quality (JPSQ), which is fused by two learning-based measurements regarding semantic similarity and aesthetic quality, respectively. Furthermore, we develop the Attention-aware Deep Diffusion (ADD) algorithm, which learns attention maps and leverages the diffusion process to enhance the environment exploration ability of traditional deep reinforcement learning (DRL). Extensive experiments demonstrate that our proposal can reduce the bandwidth consumption of mobile users by 49.4% on average, with almost no perceptual difference in AIGC output quality. Moreover, the ADD algorithm shows superior performance over baseline DRL methods, with 1.74× higher overall reward.}
}


@article{DBLP:journals/tmc/LiHSJW24,
	author = {Weihe Li and
                  Jiawei Huang and
                  Qichen Su and
                  Wanchun Jiang and
                  Jianxin Wang},
	title = {{VASE:} Enhancing Adaptive Bitrate Selection for VBR-Encoded Audio
                  and Video Content With Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14889--14902},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3448370},
	doi = {10.1109/TMC.2024.3448370},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiHSJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive BitRate (ABR) algorithms have become increasingly prevalent in modern streaming platforms, offering users significant improvements in the Quality of Experience (QoE). With streaming providers like YouTube and Netflix shifting to high-fidelity audio formats such as stereophonic sound and Dolby Atoms, ensuring proper audio and video adaptation has become a critical aspect of modern streaming platforms. Additionally, Variable Bitrate (VBR) encoding has gained great popularity in encoding audio and video content, given its higher quality-to-bits ratio. However, the considerable variability in network bandwidth, in combination with VBR features such as significantly fluctuating audio/video chunk sizes and diverse content complexity, makes existing ABR schemes formidable to make optimal bitrate selection due to their overlook of audio adaptation or oblivious to VBR features. In this paper, we introduce a new ABR approach for VBR-based Audio-aware video StrEaming named VASE, which harnesses deep reinforcement learning (DRL) and exploits parallel computing with multiple agents to swiftly and adeptly manage fluctuations in video/audio chunk sizes, network bandwidth, and varying content complexity, all while operating without any assumptions. Besides, two variants are proposed to mitigate the download energy cost and handle audio and video content in finer granularity. Extensive trace-driven, testbed, and subjective evaluations show that our scheme surpasses existing advanced adaptation schemes regarding the overall QoE, effectively demonstrating its superiority.}
}


@article{DBLP:journals/tmc/TengXGGMZ24,
	author = {Xiaoqiang Teng and
                  Shibiao Xu and
                  Deke Guo and
                  Yulan Guo and
                  Weiliang Meng and
                  Xiaopeng Zhang},
	title = {DTTCNet: Time-to-Collision Estimation With Autonomous Emergency Braking
                  Using Multi-Scale Transformer Network},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14903--14917},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3454122},
	doi = {10.1109/TMC.2024.3454122},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TengXGGMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of autonomous driving technologies has brought the significance of Autonomous Emergency Braking (AEB) systems, which are paramount in mitigating collision risk and elevating road safety by preemptively applying brakes when a potential collision is detected. Within the core mechanisms of AEB systems, the Time-to-Collision (TTC) estimation plays a pivotal role, in quantitatively determining the criticality and timing for initiating braking interventions. However, existing TTC estimation approaches exhibit sensitivity to diverse driving scenarios, compromising the performance of AEB systems, especially in instantaneous situations. To address these issues, this paper presents DTTCNet, a novel supervised deep learning model for TTC estimation that leverages multi-scale transformer architectures and multi-task losses, thereby enhancing precision and boosting system performance. The DTTCNet first extracts spatiotemporal features from raw sensor data and utilizes a supervised training strategy. The multi-scale transformer architecture effectively captures variations across different scales, while the multi-task loss function optimizes the network training performance. Our experimental results on a challenging dataset demonstrate that DTTCNet achieves approximately 20% performance improvements over existing methods in terms of accuracy. This signifies a promising approach to augmenting the safety of autonomous driving systems with the integration of aftermarket mobile devices (e.g., Mobileye and Bosch products).}
}


@article{DBLP:journals/tmc/GuoTGSHC24,
	author = {Xiuzhen Guo and
                  Long Tan and
                  Chaojie Gu and
                  Yuanchao Shu and
                  Shibo He and
                  Jiming Chen},
	title = {MagWear: Vital Sign Monitoring Based on Biomagnetism Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14918--14933},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452499},
	doi = {10.1109/TMC.2024.3452499},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoTGSHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the design, implementation, and evaluation of MagWear, a novel biomagnetism-based system that can accurately and inclusively monitor the heart rate, respiration rate, and blood pressure of users. MagWear's contributions are twofold. First, we build a mathematical model that characterizes the magnetic coupling effect of blood flow under the influence of an external magnetic field. This model uncovers the variations in accuracy when monitoring vital signs among individuals. Second, leveraging insights derived from this mathematical model, we present a software-hardware co-design that effectively handles the impact of human diversity on the performance of vital sign monitoring, pushing this generic solution one big step closer to real adoptions. Following IRB protocols, our extensive experiments involving 30 volunteers demonstrate that MagWear achieves high monitoring accuracy with a mean percentage error (MPE) of 1.55% for heart rate (HR), 1.79% for respiration rate (RR), 3.35% for systolic blood pressure (SBP), and 3.89% for diastolic blood pressure (DBP). MagWear can also be extended to detect anemia and blood oxygen saturation, which is also our ongoing work.}
}


@article{DBLP:journals/tmc/DongZLQZ24,
	author = {Liran Dong and
                  Yiqing Zhou and
                  Ling Liu and
                  Yanli Qi and
                  Yu Zhang},
	title = {Age of Information Based Client Selection for Wireless Federated Learning
                  With Diversified Learning Capabilities},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14934--14945},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450549},
	doi = {10.1109/TMC.2024.3450549},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/DongZLQZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) empowers wireless intelligent applications, by leveraging distributed data of edge clients for training without compromising privacy. Client selection is inevitable in FL, since clients have diversified learning capabilities arising from heterogeneous computing and communication resources. Existing methods like fair-selection and dropping-straggler are either inefficient or unfair (resulting in a less effective trained model). Therefore, we propose FedAoI, an Age-of-Information (AoI) based client selection policy. FedAoI ensures fairness by allowing all clients, including stragglers, to submit their model updates while maintaining high training efficiency by keeping round completion times short. This trade-off is achieved by minimizing Peak-AoI (PAoI), the interval between a client's consecutive participations. An optimization problem is formulated by minimizing the Expected-Weighted-Sum-of-PAoI. This NP-hard problem is addressed with a two-step sub-optimal algorithm, PriorS. It first calculates client priority in a round using Lyapunov optimization and then selects the highest-priority clients through G-FPFC (Greedy minimization of the round weighted-sum-of-PAoI with First-Priority-First-Considered). Simulation results demonstrate that, compared to fair-selection, FedAoI improves average efficiency by 83.8% and achieves an average model accuracy of 97.3% (or at the cost of averaging 2.7% degradation in model accuracy). Compared to dropping-straggler, FedAoI reduces the average model accuracy degradation from 9.5% to 2.7%.}
}


@article{DBLP:journals/tmc/JeonCJJP24,
	author = {Youbin Jeon and
                  Hongrok Choi and
                  Hyeonjae Jeong and
                  Daeyoung Jung and
                  Sangheon Pack},
	title = {CheckBullet: {A} Lightweight Checkpointing System for Robust Model
                  Training on Mobile Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14946--14958},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450283},
	doi = {10.1109/TMC.2024.3450283},
	timestamp = {Mon, 03 Mar 2025 22:25:37 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JeonCJJP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training on time-series data generated from mobile networks is a resource-intensive and time-consuming task that encounters various training failures. To cope with this issue, we propose CheckBullet, a lightweight checkpoint system to minimize storage requirements and enable fast recovery in mobile networks. First, CheckBullet determines a checkpointing interval based on the characteristics of the model and the timing of failure occurrences. This approach ensures fast recovery while preserving the existing training runtime. Second, CheckBullet quantizes the weight tensor and eliminates duplicate weights, which significantly reduces the overall checkpoint size, leading to a substantial decrease in storage requirements. Third, CheckBullet selects the minimum training loss among the deduplicated checkpoints and merges the selected checkpoints. This approach reduces recovery time while preserving existing training loss. The experimental results show that CheckBullet can reduce the recovery time by 6\\times to 11\\times barely increasing the training runtime. Furthermore, CheckBullet can save storage requirements by up to 70% while maintaining the minimum training loss.}
}


@article{DBLP:journals/tmc/ZhangZXZYZ24,
	author = {Duo Zhang and
                  Xusheng Zhang and
                  Yaxiong Xie and
                  Fusang Zhang and
                  Hongliu Yang and
                  Daqing Zhang},
	title = {From Single-Point to Multi-Point Reflection Modeling: Robust Vital
                  Signs Monitoring via mmWave Sensing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14959--14974},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450318},
	doi = {10.1109/TMC.2024.3450318},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangZXZYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Long-term monitoring of human vital signs like respiration and heartbeat is crucial for the early detection of diverse diseases and overall health monitoring. Contact-free vital signs monitoring using wireless signals, particularly mmWave-based methods, has gained attention due to its sensitivity and privacy-preserving benefits. However, we observe that even minor human movements could lead to significant mutations in the signal-to-noise ratio (SNR) of the wireless signal, which cannot be explained by the commonly used model that represents the human chest as a single reflection point. These fluctuations challenge the robustness of heart rate and heart rate variability (HRV) monitoring due to the vulnerability of faint heartbeats to noise interference. To tackle this, we introduce a multi-point reflection model to understand the underlying causes of SNR fluctuations and propose a frequency diversity based algorithm to enhance sensing SNR. Our solution, Robust-Vital, was rigorously evaluated using commercial mmWave radar systems and demonstrated superior performance on long-term heart rate and heart rate variability tracking in a user study with 12 participants.}
}


@article{DBLP:journals/tmc/ZuoZYZ24,
	author = {Changqi Zuo and
                  Xu Zhang and
                  Liang Yan and
                  Zuyu Zhang},
	title = {{GUGEN:} Global User Graph Enhanced Network for Next {POI} Recommendation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14975--14986},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3455107},
	doi = {10.1109/TMC.2024.3455107},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZuoZYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning the next Point-of-Interest (POI) is a highly context-dependent human movement behavior prediction task, which has gained increasing attention with the consideration of massive spatial-temporal trajectories data or check-in data. The spatial dependency, temporal dependency, sequential dependency and social network dependency are widely considered pivotal to predict the users’ next location in the near future. However, most existing models fail to consider the influence of other users’ movement patterns and the correlation with the POIs the user has visited. Therefore, we propose a Global User Graph Enhanced Network (GUGEN) for the next POI recommendation from a global and a user perspectives. First, a trajectory learning network is designed to model the users’ short-term preference. Second, a geographical learning module is designed to model the global and user context information. From the global perspective, two graphs are designed to represent the global POI features and the geographical relationships of all POIs. From the user perspective, a user graph is constructed to describe each users’ historical POI information. We evaluated the proposed model on three real-world datasets. The experimental evaluations demonstrate that the proposed GUGEN method outperforms the state-of-the-art approaches for the next POI recommendation.}
}


@article{DBLP:journals/tmc/RenLYLLLW24,
	author = {Hualing Ren and
                  Kai Liu and
                  Guozhi Yan and
                  Chunhui Liu and
                  Yantao Li and
                  Chuzhao Li and
                  Weiwei Wu},
	title = {Truthful Auction Mechanisms for Dependent Task Offloading in Vehicular
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {14987--15002},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450504},
	doi = {10.1109/TMC.2024.3450504},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/RenLYLLLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work investigates the truthful auction for dependent task offloading in vehicular edge computing by considering the selfishness and rationality of participating nodes. Specifically, we first illustrate a truthfulness-guaranteed dependent task offloading architecture. Then, we formulate the Truthfulness-Guaranteed Dependent Task Offloading problem, aiming at maximizing the system utility (SU) while ensuring truthfulness and individual rationality in dynamic environments. Further, we design both centralized and distributed auction mechanisms to derive the optimal and approximate solutions, respectively. For centralized auction mechanism, we adopt the branch-and-price algorithm to determine the offloaded nodes, which yields maximum SU. Then, we adopt VCG mechanism to determine the payment of buyers. For distributed auction mechanism, each seller independently chooses the winning bid, and the buyer greedily chooses the offloaded node with maximum utility. Then, a novel payment mechanism regarding the cost of failed buyers is designed to guarantee the truthfulness and individual rationality. Finally, we build the simulation model and conduct the performance evaluation based on realistic vehicular trajectories. The results demonstrate that the proposed distributed auction mechanism achieves performance within approximately 4% of the optimal method, while significantly reducing computational complexity. Additionally, it significantly outperforms other methods in terms of system utility across various task requirements.}
}


@article{DBLP:journals/tmc/FangHAZWCCF24,
	author = {Zhengru Fang and
                  Senkang Hu and
                  Haonan An and
                  Yuang Zhang and
                  Jingjing Wang and
                  Hangcheng Cao and
                  Xianhao Chen and
                  Yuguang Fang},
	title = {{PACP:} Priority-Aware Collaborative Perception for Connected and
                  Autonomous Vehicles},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15003--15018},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3449371},
	doi = {10.1109/TMC.2024.3449371},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/FangHAZWCCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird's Eye View has been employed to accurately capture spatial relationships among vehicles. However, severe inherent limitations of BEV, like blind spots, have been identified. Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles. While most existing collaborative perception strategies adopt a fully connected graph predicated on fairness in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy. To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception. By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics. Moreover, we deploy a deep learning-based adaptive autoencoder to modulate the image reconstruction quality under dynamic channel conditions. Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union.}
}


@article{DBLP:journals/tmc/LiTHLZ24,
	author = {Dongxu Li and
                  Yuanming Tian and
                  Chuan Huang and
                  Qingwen Liu and
                  Shengli Zhou},
	title = {Design and Performance of Resonant Beam Communications - Part {II:}
                  Mobile Scenario},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15019--15030},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3451657},
	doi = {10.1109/TMC.2024.3451657},
	timestamp = {Fri, 14 Feb 2025 15:01:46 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiTHLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios. Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case. In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference. With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation. By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method. Finally, simulation results validate the analysis of our proposed method in some typical scenarios.}
}


@article{DBLP:journals/tmc/ZhouXPLZYX24,
	author = {Chengjin Zhou and
                  Qiao Xiang and
                  Lingjun Pu and
                  Zheli Liu and
                  Yuan Zhang and
                  Xinjing Yuan and
                  Jingdong Xu},
	title = {{\textdollar}\{{\textbackslash}sf NetDPI\}{\textdollar}NetDPI: Efficient
                  Deep Packet Inspection via Filtering-Plus-Verification in Programmable
                  5G Data Plane for Multi-Access Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15031--15047},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450691},
	doi = {10.1109/TMC.2024.3450691},
	timestamp = {Mon, 02 Dec 2024 08:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhouXPLZYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we advocate {\\sf NetDPI}, a novel and efficient Deep Packet Inspection (DPI) solution built-in 5G Data Plane for multi-access edge computing, leveraging the unique forwarding while computing capability of emerging programmable switches. As the cornerstone, we propose {\\sf FIVE}, the first Filtering-plus-Verification algorithm tailored to programmable switches to achieve efficient multiple pattern matching (i.e., the core of DPI). Briefly, the filtering phase introduces a multi-window parallel shift-or algorithm to rapidly screen out all the “suspicious” packet payloads. Meanwhile, the verification phase innovates a level-based state encoding scheme for the Aho–Corasick (AC) algorithm, which substantially increases the number of supported patterns and consequently figures out more “guilty” payloads. We implement the prototype of {\\sf NetDPI} in both software and hardware programmable switches (i.e., BMv2 and Barefoot Tofino2) and make them publicly available. Extensive evaluations indicate that {\\sf NetDPI} provides orders of magnitude improvement in throughput compared to the typical cloud-delivered DPI solutions, and besides {\\sf FIVE} greatly reduces the memory consumption compared to the alternative in-network exact match algorithms under a variety of system settings including different DPI pattern sets and malware-packet percentages.}
}


@article{DBLP:journals/tmc/LiLYQS24,
	author = {Wentong Li and
                  Hang Li and
                  Long Yang and
                  Lei Qiao and
                  Liang Shi},
	title = {{MTPS:} {A} Multi-Task Perceiving and Scheduling Framework Across
                  Multiple Mobile Devices},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15048--15061},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450577},
	doi = {10.1109/TMC.2024.3450577},
	timestamp = {Fri, 07 Feb 2025 20:27:05 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiLYQS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalence of cross-device resource sharing enables users to utilize various device resources of the connected mobile devices seamlessly. Since there are often numerous connected mobile devices under the same network, cross-device tasks are often executed concurrently. However, the existing resource sharing schemes suffer from significant performance degradation for the parallel cross-device tasks due to competition for limited system resources (e.g., network and CPU). This paper first analyzes the performance penalty in parallel execution of the cross-device resource sharing tasks. Then, a novel multi-task perceiving and scheduling framework (MTPS) is proposed to guarantee the quality of service of the parallel tasks. The basic idea of MTPS is to first build a master-slave system model to reorganize mobile devices under the same network. Then, MTPS perceives the running cross-device resource sharing tasks and schedules the parallel execution of multiple tasks to avoid mutual interference. Experimental results on real devices show that MTPS can reduce the average completion time of file sharing by 63.5%, and maintain at least 24 frames per second for screen casting at optimal levels in the presence of other tasks.}
}


@article{DBLP:journals/tmc/WangYCSY24,
	author = {Xigui Wang and
                  Haiyang Yu and
                  Yuwen Chen and
                  Richard O. Sinnott and
                  Zhen Yang},
	title = {PrVFL: Pruning-Aware Verifiable Federated Learning for Heterogeneous
                  Edge Computing},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15062--15079},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3450542},
	doi = {10.1109/TMC.2024.3450542},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangYCSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era emphasizing the privacy of personal data, verifiable federated learning has garnered significant attention as a machine learning approach to safeguard user privacy while simultaneously validating aggregated result. However, there are some unresolved issues when deploying verifiable federated learning in edge computing. Due to the constraint resources, edge computing demands cost saving measurements in model training such as model pruning. Unfortunately, there is currently no protocol capable of enabling users to verify pruning results. Therefore, in this paper, we introduce PrVFL, a verifiable federated learning framework that supports model pruning verification and heterogeneous edge computing. In this scheme, we innovatively utilize zero-knowledge range proof protocol to achieve pruning result verification. Additionally, we first propose a heterogeneous delayed verification scheme supporting the validation of aggregated result for pruned heterogeneous edge models. Addressing the prevalent scenario of performance-heterogeneous edge clients, our scheme empowers each edge user to autonomously choose the desired pruning ratio for each training round based on their specific performance. By employing a global residual model, we ensure that every parameter has an opportunity for training. The extensive experimental results demonstrate the practical performance of our proposed scheme.}
}


@article{DBLP:journals/tmc/SunSYNA24,
	author = {Qingshuang Sun and
                  Denis Steckelmacher and
                  Yuan Yao and
                  Ann Now{\'{e}} and
                  Rapha{\"{e}}l Avalos},
	title = {Dynamic Size Message Scheduling for Multi-Agent Communication Under
                  Limited Bandwidth},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15080--15097},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452986},
	doi = {10.1109/TMC.2024.3452986},
	timestamp = {Sun, 22 Dec 2024 15:49:03 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SunSYNA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication plays a vital role in multi-agent systems, fostering collaboration and coordination. However, in real-world scenarios where communication is bandwidth-limited, existing multi-agent reinforcement learning (MARL) algorithms often provide agents with a binary choice: either transmitting a fixed amount of data or no information at all. This rigid communication strategy hinders the ability to effectively utilize bandwidth. To overcome this challenge, we present the Dynamic Size Message Scheduling (DSMS) method, which introduces finer-grained communication scheduling by considering the actual size of the information being exchanged. Our approach lies in adapting message sizes using Fourier transform-based compression techniques with clipping, enabling agents to tailor their messages to match the allocated bandwidth according to importance weights. This method realizes a balance between information loss and bandwidth utilization. Receiving agents reliably decompress the messages using the inverse Fourier transform. We evaluate DSMS in cooperative tasks where the agent has partial observability. Experimental results demonstrate that DSMS significantly improves performance by optimizing the utilization of bandwidth and effectively balancing information importance.}
}


@article{DBLP:journals/tmc/JinZXWL24,
	author = {Yili Jin and
                  Wenyi Morty Zhang and
                  Zihan Xu and
                  Fangxin Wang and
                  Xue (Steve) Liu},
	title = {Privacy-Preserving Gaze-Assisted Immersive Video Streaming},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15098--15113},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452510},
	doi = {10.1109/TMC.2024.3452510},
	timestamp = {Thu, 30 Jan 2025 09:09:53 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/JinZXWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Immersive videos, also known as 360^{\\circ } videos, have gained significant attention in recent years due to their ability to provide an interactive and engaging experience. However, the development of immersive video streaming faces several challenges, including privacy concerns, the need for accurate viewport prediction, and efficient bandwidth allocation. In this paper, we propose a comprehensive system that integrates three specialized modules: the Privacy Protection module, the Viewport Prediction module, and the Bitrate Allocation module. The Privacy Protection module introduces a novel approach to differential privacy tailored for immersive video environments, considering the spatial and temporal correlations in viewport and gaze motion data. The Viewport Prediction module leverages a crossmodal attention mechanism based on the transformer to predict user viewport movements by analyzing the complex interactions between historical data, video content, and gaze patterns. The Bitrate Allocation module employs an adaptive tile-based bitrate allocation strategy using an exponential decay function to optimize video quality and maximize user quality of experience. Experimental results demonstrate that our proposed framework outperforms three state-of-the-art integrated frameworks, achieving an average QoE improvement of 21.61%. This paper offers substantial novelty in addressing privacy concerns, leveraging gaze information for viewport prediction, and utilizing underlying correlations between different features.}
}


@article{DBLP:journals/tmc/GaoZLWJTCQLT24,
	author = {Ruipeng Gao and
                  Shuli Zhu and
                  Lingkun Li and
                  Xuyu Wang and
                  Yuqin Jiang and
                  Naiqiang Tan and
                  Hua Chai and
                  Peng Qi and
                  Jiqiang Liu and
                  Dan Tao},
	title = {Real-World Large-Scale Cellular Localization for Pickup Position Recommendation
                  at Black-Hole},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15114--15131},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453596},
	doi = {10.1109/TMC.2024.3453596},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GaoZLWJTCQLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indoor localization availability is still sporadic in industry, especially at the black-hole, i.e., there only exist cellular signals, no GPS or WiFi signals. Based on our 2-year observations at the DiDi ride-hailing platform in China, there are 68\\,\\text{k} orders everyday created at black-hole. In this paper, we present TransparentLoc, a large-scale cellular localization system for pickup position recommendation of the DiDi platform. Specifically, we design a CNN model for real-time localization based on a crowdsourcing fingerprint set constructed by outdoor trajectories and abnormal cell tower detection. Then we leverage a DeepFM model to recommend an optimal pickup position for passengers. We share our 2-year experience with 50 million orders across 13 million devices in 4541 cities to address practical challenges including sparse cell towers, unbalanced user fingerprints, temporal variations, and abnormal cell towers in terms of four major service metrics, i.e., pickup position error, over-30-meters ratio, cancel ratio, and call ratio. The large-scale evaluations show that our system achieves a 0.54\\,\\text{m} lower median pickup position error compared to the iOS built-in cellular localization system, regardless of environmental changes, smartphone brands/models, time, and cellular providers. Additionally, the over-30-meters ratio, cancel ratio, and call ratio have significant reductions of 0.88%, 0.88%, and 5.13%, respectively.}
}


@article{DBLP:journals/tmc/LiCZYWZ24,
	author = {Bing Li and
                  Wei Cui and
                  Le Zhang and
                  Qi Yang and
                  Min Wu and
                  Joey Tianyi Zhou},
	title = {Democratizing Federated WiFi-Based Human Activity Recognition Using
                  Hypothesis Transfer},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15132--15148},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3457788},
	doi = {10.1109/TMC.2024.3457788},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiCZYWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Human activity recognition (HAR) is a crucial task in IoT systems with applications ranging from surveillance and intruder detection to home automation and more. Recently, non-invasive HAR utilizing WiFi signals has gained considerable attention due to advancements in ubiquitous WiFi technologies. However, recent studies have revealed significant privacy risks associated with WiFi signals, raising concerns about bio-information leakage. To address these concerns, the decentralized paradigm, particularly federated learning (FL), has emerged as a promising approach for training HAR models while preserving data privacy. Nevertheless, FL models may struggle in end-user environments due to substantial domain discrepancies between the source training data and the target end-user environment. This discrepancy arises from the sensitivity of WiFi signals to environmental changes, resulting in notable domain shifts. As a consequence, FL-based HAR approaches often face challenges when deployed in real-world WiFi environments. Albeit there are pioneer attempts on federated domain adaptation, they typically require non-trivial communication and computation cost, which is prohibitively expensive especially considering edge-based hardware equipment of end-user environment. In this paper, we propose a model to democratize the WiFi-based HAR system by enhancing recognition accuracy in unannotated end-user environments while prioritizing data privacy. Our model leverages the hypothesis transfer and a lightweight hypothesis ensemble to mitigate negative transfer. We prove a tighter theoretical upper bound compared to existing multi-source federated domain adaptation models. Extensive experiments shows our model improves the average accuracy by approximately 10 absolute percentage points in both cross-person and cross-environment settings comparing several state-of-the-art baselines.}
}


@article{DBLP:journals/tmc/KimJCY24,
	author = {Minwoo Kim and
                  Jonggyu Jang and
                  Youngchol Choi and
                  Hyun Jong Yang},
	title = {Distributed Task Offloading and Resource Allocation for Latency Minimization
                  in Mobile Edge Computing Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15149--15166},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3458185},
	doi = {10.1109/TMC.2024.3458185},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/KimJCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growth in artificial intelligence (AI) technology has attracted substantial interests in latency-aware task offloading of mobile edge computing (MEC)—namely, minimizing service latency. Additionally, the use of MEC systems poses an additional problem arising from limited battery resources of MDs. This paper tackles the pressing challenge of latency-aware distributed task offloading optimization, where user association (UA), resource allocation (RA), full-task offloading, and battery of mobile devices (MDs) are jointly considered. In existing studies, joint optimization of overall task offloading and UA is seldom considered due to the complexity of combinatorial optimization problems, and in cases where it is considered, linear objective functions such as power consumption are adopted. Revolutionizing the realm of MEC, our objective includes all major components contributing to users’ quality of experience, including latency and energy consumption. To achieve this, we first formulate an NP-hard combinatorial problem, where the objective function comprises three elements: communication latency, computation latency, and battery usage. We derive a closed-form RA solution of the problem; next, we provide a distributed pricing-based UA solution. We simulate the proposed algorithm for various resource-intensive tasks. Our numerical results show that the proposed method Pareto-dominates baseline methods. More specifically, the results demonstrate that the proposed method can outperform baseline methods by 1.62 times shorter latency with 41.2% less energy consumption.}
}


@article{DBLP:journals/tmc/WuLWDLW24,
	author = {Panlong Wu and
                  Kangshuo Li and
                  Ting Wang and
                  Yanjie Dong and
                  Victor C. M. Leung and
                  Fangxin Wang},
	title = {FedFMSL: Federated Learning of Foundation Models With Sparsely Activated
                  LoRA},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15167--15181},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3454634},
	doi = {10.1109/TMC.2024.3454634},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WuLWDLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Foundation models (FMs) have shown great success in natural language processing, computer vision, and multimodal tasks. FMs have a large number of model parameters, thus requiring a substantial amount of data to help optimize the model during the training. Federated learning has revolutionized machine learning by enabling collaborative learning from decentralized data while still preserving clients’ data privacy. Despite the great benefits foundation models can have empowered by federated learning, their bulky model parameters cause severe communication challenges for modern networks and computation challenges especially for edge devices. Moreover, the data distribution of different clients can be different thus inducing statistical challenges. In this paper, we propose a novel two-stage federated learning algorithm called FedFMSL. A global expert is trained in the first stage and a local expert is trained in the second stage to provide better personalization. We construct a Mixture of Foundation Models (MoFM) with these two experts and design a gate neural network with an inserted gate adapter that joins the aggregation every communication round in the second stage. To further adapt to edge computing scenarios with limited computational resources, we design a novel Sparsely Activated LoRA (SAL) algorithm that freezes the pre-trained foundation model parameters inserts low-rank adaptation matrices into transformer blocks, and activates them progressively during the training. We employ extensive experiments to verify the effectiveness of FedFMSL, results show that FedFMSL outperforms other SOTA baselines by up to 59.19% in default settings while tuning less than 0.3% parameters of the foundation model.}
}


@article{DBLP:journals/tmc/SoderiBXC24,
	author = {Simone Soderi and
                  Alessandro Brighente and
                  Saiqin Xu and
                  Mauro Conti},
	title = {Multi-RIS Aided {VLC} Physical Layer Security for 6G Wireless Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15182--15195},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452963},
	doi = {10.1109/TMC.2024.3452963},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/SoderiBXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies highlighted the advantages of Visible Light Communication (VLC) over radio technology for future 6G networks. Thanks to the use of Reflective Intelligent Surfaces (RISs), researchers showed that is possible to guarantee communication secrecy in a VLC network where the adversary location is unknown. However, the problem of authenticating the transmitter with a low-complexity physical layer solution while guaranteeing communication secrecy is still open. This paper proposes a novel multi-RIS architecture to guarantee source authentication, communication secrecy, and integrity in a VLC scenario. We leverage the intuition that a signal transmitted by users located in different positions will undergo a different propagation path to discriminate between the legitimate intended transmitter and an attacker. To increase the channel's variability and reduce the chances that an adversary might be able to replicate it, we leverage the reconfiguration capabilities of RIS. We derive a statistical characterization of the non-line-of-sight VLC channel, representing the light reflected by RIS elements. Via numerical simulations, we show that the channel variability combined with the configurability capabilities of RISs provide sufficient statistics to authenticate the legitimate transmitter at the physical layer.}
}


@article{DBLP:journals/tmc/LiWHSZTY24,
	author = {Anran Li and
                  Guangjing Wang and
                  Ming Hu and
                  Jianfei Sun and
                  Lan Zhang and
                  Luu Anh Tuan and
                  Han Yu},
	title = {Joint Client-and-Sample Selection for Federated Learning via Bi-Level
                  Optimization},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15196--15209},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3455331},
	doi = {10.1109/TMC.2024.3455331},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/LiWHSZTY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables massive local data owners to collaboratively train a deep learning model without disclosing their private data. The importance of local data samples from various data owners to FL models varies widely. This is exacerbated by the presence of noisy data that exhibit large losses similar to important (hard) samples. Currently, there lacks an FL approach that can effectively distinguish hard samples (which are beneficial) from noisy samples (which are harmful). To bridge this gap, we propose the joint Federated Meta-Weighting based Client and Sample Selection (FedMW-CSS) approach to simultaneously mitigate label noise and hard sample selection. It is a bilevel optimization approach for FL client-and-sample selection and global model construction to achieve hard sample-aware noise-robust learning in a privacy preserving manner. It performs meta-learning based online approximation to iteratively update global FL models, select the most positively influential samples and deal with training data noise. To utilize both the instance-level information and class-level information for better performance improvements, FedMW-CSS efficiently learns a class-level weight by manipulating gradients at the class level, e.g., it performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Theoretically, we analyze the privacy guarantees and convergence of FedMW-CSS. Extensive experiments comparison against eight state-of-the-art baselines on six real-world datasets in the presence of data noise and heterogeneity shows that FedMW-CSS achieves up to 28.5% higher test accuracy, while saving communication and computation costs by at least 49.3% and 1.2%, respectively.}
}


@article{DBLP:journals/tmc/WangZYRL24,
	author = {Jiwen Wang and
                  Rongrong Zhang and
                  Jihong Yu and
                  Ju Ren and
                  Yun Li},
	title = {Age-Efficient Random Access With Load Adaptation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15210--15223},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453042},
	doi = {10.1109/TMC.2024.3453042},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/WangZYRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The lightweight and energy-efficient Frame Slotted Aloha (FSA) protocol has become a promising MAC protocol in large-scale IoT systems. Existing work on minimizing the age of information (AoI) of FSA protocol cannot significantly benefit from frequent packet generations when the packet generation rate \\lambda exceeds its throughput e^{-1}. To fill this gap, this paper proposes two age threshold-based algorithms to reduce the AoI of FSA systems for \\lambda > e^{-1}, namely TF and TF+. Their core ideas are to only allow the nodes with age gain over the configured thresholds to send their packets so that the FSA systems are slimmed to a stable one with \\lambda < e^{-1} and a polling system, respectively. Technically, we design the threshold configuration rules for the two algorithms and characterize the normalized average AoI. We also conduct simulation and the results show that TF and TF+ achieve lower AoI than the prior works.}
}


@article{DBLP:journals/tmc/CongZZM24a,
	author = {Rong Cong and
                  Zhiwei Zhao and
                  Linyuanqi Zhang and
                  Geyong Min},
	title = {Kite: Link-Adaptive and Real-Time Object Detection in Dynamic Edge
                  Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15224--15237},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452101},
	doi = {10.1109/TMC.2024.3452101},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/CongZZM24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vision-based real-time object detection has become a key fundamental service for smart-city applications such as auto-drive and digital twins. Due to the limited resource available at camera devices, edge-assisted object detection has attracted increasing research attention. The existing edge-assisted schemes often assume stable or averaged wireless links during the frame offloading process. However, the assumption does not hold in real-world dynamic edge networks and will lead to significant performance degradation in terms of both detection latency and accuracy. In this paper, we propose Kite, a link-adaptive scheme for real-time object detection. Based on measurement studies and systematic analysis, we devise a lightweight yet representative performance indicator – “frame-anchor” distance, to incorporate the immeasurable impact of wireless dynamics into a measurable metric. Based on this performance indicator, we model the offloading process as an integer nonlinear programming problem, and propose an online link-adaptive algorithm for frame offloading decisions. We implement Kite in a neuro-enhanced live streaming application and conduct comparative experiments with four different datasets in WiFi/LTE based edge networks. The results show that Kite can improve the detection accuracy by 40.53% in highly dynamic networks, compared to the state-of-the-art works.}
}


@article{DBLP:journals/tmc/HanYFDL24,
	author = {Feiyu Han and
                  Panlong Yang and
                  Yuanhao Feng and
                  Haohua Du and
                  Xiang{-}Yang Li},
	title = {Exploring Earable-Based Passive User Authentication via Interpretable
                  In-Ear Breathing Biometrics},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15238--15255},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453412},
	doi = {10.1109/TMC.2024.3453412},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/HanYFDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As earable devices have become indispensable smart devices in people's lives, earable-based user authentication has gradually attracted widespread attention. In our work, we explore novel in-ear breathing biometrics and design an earable-based authentication approach, named BreathSign, which takes advantage of inward-facing microphones on commercial earphones to capture in-ear breathing sounds for passive authentication. To expand the differences among individuals, we model the process of breathing sound generation, transmission, and reception. Based on that, we derive hard-to-forge physical-level features from in-ear breathing sounds as biometrics. Furthermore, to eliminate the impact of breathing behavioral patterns (e.g., duration and intensity), we design a triple network model to extract breathing behavior-independent features and design an online user template update mechanism for long-term authentication. Extensive experiments with 35 healthy subjects have been conducted to evaluate the performance of BreathSign. The results show that our system achieves the average authentication accuracy of 93.15%, 98.06%, and 99.74% via one, five, and nine breathing cycles, respectively. Regarding the resistance of spoofing attacks, BreathSign could achieve an average EER of approximately 3.5%. Compared with other behavior-based authentication schemes, BreathSign does not require users to perform complex movements or postures but only effortless breathing for authentication and can be easily implemented on commercial earphones with high usability and enhanced security.}
}


@article{DBLP:journals/tmc/GuoLLLC24,
	author = {Jialin Guo and
                  Zhetao Li and
                  Anfeng Liu and
                  Xiong Li and
                  Ting Chen},
	title = {REC-Fed: {A} Robust and Efficient Clustered Federated System for Dynamic
                  Edge Networks},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15256--15273},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3452312},
	doi = {10.1109/TMC.2024.3452312},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/GuoLLLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising approach, Clustered Federated Learning (CFL) enables personalized model aggregation for heterogeneous clients. However, facing dynamic and open edge networks, previous CFL rarely considers the impact of dynamic client data on clustering validity, or sensitively identifies low-quality parameters from highly heterogeneous client models. Moreover, the device heterogeneity in each cluster leads to unbalanced model transmission delay, thus reducing the system efficiency. To tackle the above issues, this paper proposes a Robust and Efficient Clustered Federated System (REC-Fed). First, a Hierarchical Attention based Robust Aggregation (HARA) method is designed to realize layer-wise model customization for clients, meanwhile keeping the clustering validity under dynamic client data distribution. In addition, the fine-grained parameter detection in HARA provides a natural advantage to detect low-quality parameters, which improves the robustness of CFL systems. Second, to realize efficient synchronous model transmission, an Adaptive Model Transmission Optimization (AMTO) is proposed to jointly optimize the model compression and bandwidth allocation for heterogenous clients. Finally, we theoretically analyze the convergence of REC-Fed and conduct experiments on several personalization tasks, which demonstrate that our REC-Fed has significant improvement on flexibility, robustness and efficiency.}
}


@article{DBLP:journals/tmc/TongLFZL24,
	author = {Jingwen Tong and
                  Xinran Li and
                  Liqun Fu and
                  Jun Zhang and
                  Khaled B. Letaief},
	title = {A Federated Online Restless Bandit Framework for Cooperative Resource
                  Allocation},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15274--15288},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453250},
	doi = {10.1109/TMC.2024.3453250},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/TongLFZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Restless multi-armed bandits (RMABs) have been widely utilized to address resource allocation problems with Markov reward processes (MRPs). Existing works often assume that the dynamics of MRPs are known prior, which makes the RMAB problem solvable from an optimization perspective. Nevertheless, an efficient learning-based solution for RMABs with unknown system dynamics remains an open problem. In this paper, we fill this gap by investigating a cooperative resource allocation problem with unknown system dynamics of MRPs. This problem can be modeled as a multi-agent online RMAB problem, where multiple agents collaboratively learn the system dynamics while maximizing their accumulated rewards. We devise a federated online RMAB framework to mitigate the communication overhead and data privacy issue by adopting the federated learning paradigm. Based on this framework, we put forth a Federated Thompson Sampling-enabled Whittle Index (FedTSWI) algorithm to solve this multi-agent online RMAB problem. The FedTSWI algorithm enjoys a high communication and computation efficiency, and a privacy guarantee. Moreover, we derive a regret upper bound for the FedTSWI algorithm. Finally, we demonstrate the effectiveness of the proposed algorithm on the case of online multi-user multi-channel access. Numerical results show that the proposed algorithm achieves a fast convergence rate of \\mathcal {O}(\\sqrt{T\\log (T)})\nand better performance compared with baselines. More importantly, its sample complexity reduces sublinearly with the number of agents.}
}


@article{DBLP:journals/tmc/ZhangWWWZW24,
	author = {Yudong Zhang and
                  Xu Wang and
                  Pengkun Wang and
                  Binwu Wang and
                  Zhengyang Zhou and
                  Yang Wang},
	title = {Modeling Spatio-Temporal Mobility Across Data Silos via Personalized
                  Federated Learning},
	journal = {{IEEE} Trans. Mob. Comput.},
	volume = {23},
	number = {12},
	pages = {15289--15306},
	year = {2024},
	url = {https://doi.org/10.1109/TMC.2024.3453657},
	doi = {10.1109/TMC.2024.3453657},
	timestamp = {Sat, 30 Nov 2024 21:08:18 +0100},
	biburl = {https://dblp.org/rec/journals/tmc/ZhangWWWZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatio-temporal mobility modeling plays a pivotal role in the advancement of mobile computing. Nowadays, data is frequently held by various distributed silos, which are isolated from each other and confront limitations on data sharing. Given this, there have been some attempts to introduce federated learning into spatio-temporal mobility modeling. Meanwhile, the distributional heterogeneity inherent in the spatio-temporal data also puts forward requirements for model personalization. However, the existing methods tackle personalization in a model-centric manner and fail to explore the data characteristics in various data silos, thus ignoring the fact that the fundamental cause of insufficient personalization in the model is the heterogeneous distribution of data. In this paper, we propose a novel distribution-oriented personalized Federated learning framework for Cross-silo Spatio-Temporal mobility modeling (named FedCroST), that leverages learnable spatio-temporal prompts to implicitly represent the local data distribution patterns of data silos and guide the local models to learn the personalized information. Specifically, we focus on the potential characteristics within temporal distribution and devise a conditional diffusion module to generate temporal prompts that serve as guidance for the evolution of the time series. Simultaneously, we emphasize the structure distribution inherent in node neighborhoods and propose adaptive spatial structure partition to construct the spatial prompts, augmenting the spatial information representation. Furthermore, we introduce a denoising autoencoder to effectively harness the learned multi-view spatio-temporal features and obtain personalized representations adapted to local tasks. Our proposal highlights the significance of latent spatio-temporal data distributions in enabling personalized federated spatio-temporal learning, providing new insights into modeling spatio-temporal mobility in data silo scenarios. Extensive experiments conducted on real-world datasets demonstrate that FedCroST outperforms the advanced baselines by a large margin in diverse cross-silo spatio-temporal mobility modeling tasks.}
}
