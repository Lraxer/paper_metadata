@article{DBLP:journals/tois/LiaoZLZOL25,
	author = {Weibin Liao and
                  Yifan Zhu and
                  Yanyan Li and
                  Qi Zhang and
                  Zhonghong Ou and
                  Xuesong Li},
	title = {RevGNN: Negative Sampling Enhanced Contrastive Graph Learning for
                  Academic Reviewer Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {1:1--1:26},
	year = {2025},
	url = {https://doi.org/10.1145/3679200},
	doi = {10.1145/3679200},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiaoZLZOL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acquiring reviewers for academic submissions is a challenging recommendation scenario. Recent graph learning-driven models have made remarkable progress in the field of recommendation, but their performance in the academic reviewer recommendation task may suffer from a significant false negative issue. This arises from the assumption that unobserved edges represent negative samples. In fact, the mechanism of anonymous review results in inadequate exposure of interactions between reviewers and submissions, leading to a higher number of unobserved interactions compared to those caused by reviewers declining to participate. Therefore, investigating how to better comprehend the negative labeling of unobserved interactions in academic reviewer recommendations is a significant challenge. This study aims to tackle the ambiguous nature of unobserved interactions in academic reviewer recommendations. Specifically, we propose an unsupervised Pseudo Neg-Label strategy to enhance graph contrastive learning (GCL) for recommending reviewers for academic submissions, which we call RevGNN. RevGNN utilizes a two-stage encoder structure that encodes both scientific knowledge and behavior using Pseudo Neg-Label to approximate review preference. Extensive experiments on three real-world datasets demonstrate that RevGNN outperforms all baselines across four metrics. Additionally, detailed further analyses confirm the effectiveness of each component in RevGNN.}
}


@article{DBLP:journals/tois/TavakoliTZSS25,
	author = {Leila Tavakoli and
                  Johanne R. Trippas and
                  Hamed Zamani and
                  Falk Scholer and
                  Mark Sanderson},
	title = {Online and Offline Evaluation in Search Clarification},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {2:1--2:30},
	year = {2025},
	url = {https://doi.org/10.1145/3681786},
	doi = {10.1145/3681786},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/TavakoliTZSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement. Contrary to the current understanding that offline evaluations fall short of supporting online evaluations, we indicate that when identifying the most engaging clarification questions from the user’s perspective, online and offline evaluations correspond with each other. We show that the query length does not influence the relationship between online and offline evaluations, and reducing uncertainty in online evaluation strengthens this relationship. We illustrate that an engaging clarification needs to excel from multiple perspectives, and SERP quality and characteristics of the clarification are equally important. We also investigate if human labels can enhance the performance of Large Language Models (LLMs) and Learning-to-Rank (LTR) models in identifying the most engaging clarification questions from the user’s perspective by incorporating offline evaluations as input features. Our results indicate that LTR models do not perform better than individual offline labels. However, GPT, an LLM, emerges as the standout performer, surpassing all LTR models and offline labels.}
}


@article{DBLP:journals/tois/SangLZZY25,
	author = {Lei Sang and
                  Honghao Li and
                  Yiwen Zhang and
                  Yi Zhang and
                  Yun Yang},
	title = {AdaGIN: Adaptive Graph Interaction Network for Click-Through Rate
                  Prediction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {3:1--3:31},
	year = {2025},
	url = {https://doi.org/10.1145/3681785},
	doi = {10.1145/3681785},
	timestamp = {Thu, 27 Mar 2025 18:54:49 +0100},
	biburl = {https://dblp.org/rec/journals/tois/SangLZZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The goal of click-through rate (CTR) prediction in recommender systems is to effectively work with input features. However, existing CTR prediction models face three main issues. First, many models use a basic approach for feature combinations, leading to noise and reduced accuracy. Second, there is no consideration for the varying importance of features in different interaction orders, affecting model performance. Third, current model architectures struggle to capture different interaction signals from various semantic spaces, leading to sub-optimal performance. To address these issues, we propose the Adaptive Graph Interaction Network (AdaGIN) with the Graph Neural Networks-based Feature Interaction Module (GFIM), the Multi-semantic Feature Interaction Module (MFIM), and the Negative Feedback-based Search (NFS) algorithm. GFIM explicitly aggregates information between features and assesses their importance, while MFIM captures information from different semantic spaces. NFS uses negative feedback to optimize model complexity. Experimental results show AdaGIN outperforms existing models on large-scale public benchmark datasets.}
}


@article{DBLP:journals/tois/LiSZZZ25,
	author = {Honghao Li and
                  Lei Sang and
                  Yi Zhang and
                  Xuyun Zhang and
                  Yiwen Zhang},
	title = {{CETN:} Contrast-enhanced Through Network for Click-Through Rate Prediction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {4:1--4:34},
	year = {2025},
	url = {https://doi.org/10.1145/3688571},
	doi = {10.1145/3688571},
	timestamp = {Thu, 27 Mar 2025 18:54:49 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiSZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Click-through rate (CTR) prediction is a crucial task in personalized information retrievals, such as industrial recommender systems, online advertising, and web search. Most existing CTR Prediction models utilize explicit feature interactions to overcome the performance bottleneck of implicit feature interactions. Hence, deep CTR models based on parallel structures (e.g., DCN, FinalMLP, xDeepFM) have been proposed to obtain joint information from different semantic spaces. However, these parallel subcomponents lack effective supervision and communication signals, making it challenging to efficiently capture valuable multi-views feature interaction information in different semantic spaces. To address these issues, we propose a simple yet effective novel CTR model: Contrast-enhanced Through Network (CETN). Drawing inspiration from sociology, CETN leverages the complementary nature of diversity and homogeneity to guide the model in acquiring higher-quality feature interaction information. Specifically, CETN employs product-based feature interactions and the augmentation (perturbation) concept from contrastive learning to segment different semantic spaces, each with distinct activation functions. This improves diversity in the feature interaction information captured by the model. Additionally, we introduce self-supervised signals and through connection within each semantic space to ensure the homogeneity of the captured feature interaction information. The experiments conducted on four real datasets demonstrate that our model consistently outperforms twenty baseline models in terms of AUC and Logloss.}
}


@article{DBLP:journals/tois/TangWSZXL25,
	author = {Haoran Tang and
                  Shiqing Wu and
                  Xueyao Sun and
                  Jun Zeng and
                  Guandong Xu and
                  Qing Li},
	title = {{TCGC:} Temporal Collaboration-Aware Graph Co-Evolution Learning for
                  Dynamic Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {5:1--5:27},
	year = {2025},
	url = {https://doi.org/10.1145/3687470},
	doi = {10.1145/3687470},
	timestamp = {Mon, 25 Aug 2025 08:52:58 +0200},
	biburl = {https://dblp.org/rec/journals/tois/TangWSZXL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic recommendation systems, where users interact with items continuously over time, have been widely deployed in real-world online streaming applications. The burst of interaction stream causes a rapid evolution of both users and items. To update representations dynamically, existing studies have investigated event-level and history-level dynamics by modeling the newly arrived interactions and aggregating historical interactions, respectively. However, most of them directly learn the representation evolution as new interactions occur, without exploring the collaboration between the newly arrived and historical interactions, thus failing to scrutinize whether those new interactions would benefit the evolution learning process when generating dynamic representations. Moreover, most of them model the two levels of dynamics independently, explicitly ignoring the inherent co-evolving correlation between them. In this work, we propose the  Temporal Collaboration-Aware Graph Co-Evolution Learning (TCGC)  for the dynamic recommendation scenario. First, we explore the effectiveness of collaborative information and devise the collaboration-aware indicator to guide the evolution learning process. Second, we design a temporal co-evolving graph network, enabling our framework to capture the correlation between event and history dynamics. Third, we leverage the evolution task and recommendation task together for joint training. Extensive experiments on four public datasets demonstrate the superiority and effectiveness of our proposed TCGC.}
}


@article{DBLP:journals/tois/XuWZSLTPG25,
	author = {Nuo Xu and
                  Pinghui Wang and
                  Junzhou Zhao and
                  Feiyang Sun and
                  Lin Lan and
                  Jing Tao and
                  Li Pan and
                  Xiaohong Guan},
	title = {Distinguish Confusion in Legal Judgment Prediction via Revised Relation
                  Knowledge},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {6:1--6:32},
	year = {2025},
	url = {https://doi.org/10.1145/3689628},
	doi = {10.1145/3689628},
	timestamp = {Wed, 17 Dec 2025 16:01:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuWZSLTPG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Legal Judgment Prediction (LJP) aims to automatically predict a law case’s judgment results based on the text description of its facts. In practice, the confusing law articles (or charges) problem frequently occurs, reflecting that the law cases applicable to similar articles (or charges) tend to be misjudged. Although some recent works based on prior knowledge solve this issue well, they ignore that confusion also occurs between law articles with a high posterior semantic similarity due to the data imbalance problem instead of only between the prior highly similar ones, which is this work’s further finding. This article proposes an end-to-end model named  D-LADAN  to solve the above challenges. On the one hand, D-LADAN constructs a graph among law articles based on their text definition and proposes a graph distillation operator (GDO) to distinguish the ones with a high prior semantic similarity. On the other hand, D-LADAN presents a novel momentum-updated memory mechanism to dynamically sense the posterior similarity between law articles (or charges) and a weighted GDO to adaptively capture the distinctions for revising the inductive bias caused by the data imbalance problem. We perform extensive experiments to demonstrate that D-LADAN significantly outperforms state-of-the-art methods in accuracy and robustness.}
}


@article{DBLP:journals/tois/ZhangSLMSZ25,
	author = {Xiaoyu Zhang and
                  Shaoyun Shi and
                  Yishan Li and
                  Weizhi Ma and
                  Peijie Sun and
                  Min Zhang},
	title = {Feature-Enhanced Neural Collaborative Reasoning for Explainable Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {7:1--7:33},
	year = {2025},
	url = {https://doi.org/10.1145/3690381},
	doi = {10.1145/3690381},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangSLMSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing reasonable explanations for a specific suggestion given by the recommender can help users trust the system more. As logic rule-based inference is concise, transparent, and aligned with human cognition, it can be adopted to improve the interpretability of recommendation models. Previous work that interprets user preference with logic rules merely focuses on the construction of rules while neglecting the usage of feature embeddings. This limits the model in capturing implicit relationships between features. In this article, we aim to improve both the effectiveness and explainability of recommendation models by simultaneously representing logic rules and feature embeddings. We propose a novel model-intrinsic explainable recommendation method named  Feature-Enhanced Neural Collaborative Reasoning (FENCR) . The model automatically extracts representative logic rules from massive possibilities in a data-driven way. In addition, we utilize feature interaction-based neural modules to represent logic operators on embeddings. Experiments on two large public datasets show our model outperforms state-of-the-art neural logical recommendation models. Further case analyses demonstrate that  FENCR  can derive reasonable rules, indicating its high robustness and expandability. 1}
}


@article{DBLP:journals/tois/PengXWLHW25,
	author = {Qiyao Peng and
                  Hongyan Xu and
                  Yinghui Wang and
                  Hongtao Liu and
                  Cuiying Huo and
                  Wenjun Wang},
	title = {{PEPT:} Expert Finding Meets Personalized Pre-Training},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {8:1--8:26},
	year = {2025},
	url = {https://doi.org/10.1145/3690380},
	doi = {10.1145/3690380},
	timestamp = {Tue, 20 Jan 2026 10:19:18 +0100},
	biburl = {https://dblp.org/rec/journals/tois/PengXWLHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding experts is essential in Community Question Answering (CQA) platforms as it enables the effective routing of questions to potential users who can provide relevant answers. The key is to personalized learning expert representations based on their historical answered questions, and accurately matching them with target questions. Recently, the applications of Pre-Trained Language Models (PLMs) have gained significant attraction due to their impressive capability to comprehend textual data, and are widespread used across various domains. There have been some preliminary works exploring the usability of PLMs in expert finding, such as pre-training expert or question representations. However, these models usually learn pure text representations of experts from histories, disregarding personalized and fine-grained expert modeling. For alleviating this, we present a personalized pre-training and fine-tuning paradigm, which could effectively learn expert interest and expertise simultaneously. Specifically, in our pre-training framework, we integrate historical answered questions of one expert with one target question, and regard it as a candidate-aware expert-level input unit. Then, we fuse expert IDs into the pre-training for guiding the model to model personalized expert representations, which can help capture the unique characteristics and expertise of each individual expert. Additionally, in our pre-training task, we design (1) a question-level masked language model task to learn the relatedness between histories, enabling the modeling of question-level expert interest; (2) a vote-oriented task to capture question-level expert expertise by predicting the vote score the expert would receive. Through our pre-training framework and tasks, our approach could holistically learn expert representations including interests and expertise. Our method has been extensively evaluated on six real-world CQA datasets, and the experimental results consistently demonstrate the superiority of our approach over competitive baseline methods.}
}


@article{DBLP:journals/tois/AnandM25,
	author = {Vineeta Anand and
                  Ashish Kumar Maurya},
	title = {A Survey on Recommender Systems Using Graph Neural Network},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {9:1--9:49},
	year = {2025},
	url = {https://doi.org/10.1145/3694784},
	doi = {10.1145/3694784},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/AnandM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The expansion of the Internet has resulted in a change in the flow of information. With the vast amount of digital information generated online, it is easy for users to feel overwhelmed. Finding the specific information can be a challenge, and it can be difficult to distinguish credible sources from unreliable ones. This has made recommender system (RS) an integral part of the information services framework. These systems alleviate users from information overload by analyzing users’ past preferences and directing only desirable information toward users. Traditional RSs use approaches like collaborative and content-based filtering to generate recommendations. Recently, these systems have evolved to a whole new level, intuitively optimizing recommendations using deep network models. graph neural networks (GNNs) have become one of the most widely used approaches in RSs, capturing complex relationships between users and items using graphs. In this survey, we provide a literature review of the latest research efforts done on GNN-based RSs. We present an overview of RS, discuss its generalized pipeline and evolution with changing learning approaches. Furthermore, we explore basic GNN architecture and its variants used in RSs, their applications, and some critical challenges for future research.}
}


@article{DBLP:journals/tois/CaiCXZ25,
	author = {Qiqi Cai and
                  Jian Cao and
                  Guandong Xu and
                  Nengjun Zhu},
	title = {Distributed Recommendation Systems: Survey and Research Directions},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {10:1--10:38},
	year = {2025},
	url = {https://doi.org/10.1145/3694783},
	doi = {10.1145/3694783},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/CaiCXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of online information, recommendation systems have become essential tools for alleviating information overload. In recent years, researchers have increasingly focused on centralized recommendation systems, capitalizing on the powerful computing capabilities of cloud servers and the rich historical data they store. However, the rapid development of edge computing and mobile devices in recent years has provided new alternatives for building recommendation systems. These alternatives offer advantages such as privacy protection and low-latency recommendations. To leverage the advantages of different computing nodes, including cloud servers, edge servers, and terminal devices, researchers have proposed recommendation systems that involve the collaboration of these nodes, known as distributed recommendation systems. This survey provides a systematic review of distributed recommendation systems. Specifically, we design a taxonomy for these systems from four perspectives and comprehensively summarize each study by category. In particular, we conduct a detailed analysis of the collaboration mechanisms of distributed recommendation systems. Finally, we discuss potential future research directions in this field.}
}


@article{DBLP:journals/tois/XuYXZSW25,
	author = {Chen Xu and
                  Xiaopeng Ye and
                  Jun Xu and
                  Xiao Zhang and
                  Weiran Shen and
                  Ji{-}Rong Wen},
	title = {{LTP-MMF:} Toward Long-Term Provider Max-Min Fairness under Recommendation
                  Feedback Loops},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {11:1--11:29},
	year = {2025},
	url = {https://doi.org/10.1145/3695867},
	doi = {10.1145/3695867},
	timestamp = {Fri, 07 Mar 2025 18:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuYXZSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-stakeholder recommender systems involve various roles, such as users and providers. Previous work pointed out that max-min fairness (MMF) is a better metric to support weak providers. However, when considering MMF, the features or parameters of these roles vary over time, and how to ensure long-term provider MMF has become a significant challenge. We observed that recommendation feedback loops (RFL) will influence the provider MMF greatly in the long term. RFL means that recommender systems can only receive feedback on exposed items from users and update recommender models incrementally based on this feedback. When utilizing the feedback, the recommender model will regard the unexposed items as negative. In this way, the tail provider will not get the opportunity to be exposed, and its items will always be considered negative samples. Such phenomena will become more and more serious in RFL. To alleviate the problem, this article proposes an online ranking model named Long-Term Provider Max-min Fairness (LTP-MMF). Theoretical analysis shows that the long-term regret of LTP-MMF enjoys a sub-linear bound. Experimental results on three public recommendation benchmarks demonstrated that LTP-MMF can outperform the baselines in the long term.}
}


@article{DBLP:journals/tois/LiYPXWSZY25,
	author = {Pu Li and
                  Xiaoyan Yu and
                  Hao Peng and
                  Yantuan Xian and
                  Linqin Wang and
                  Li Sun and
                  Jingyun Zhang and
                  Philip S. Yu},
	title = {Relational Prompt-Based Pre-Trained Language Models for Social Event
                  Detection},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {12:1--12:43},
	year = {2025},
	url = {https://doi.org/10.1145/3695869},
	doi = {10.1145/3695869},
	timestamp = {Fri, 07 Mar 2025 18:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiYPXWSZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social Event Detection (SED) aims to identify significant events from social streams, and has a wide application ranging from public opinion analysis to risk management. In recent years, Graph Neural Network (GNN) based solutions have achieved state-of-the-art performance. However, GNN-based methods often struggle with missing and noisy edges between messages, affecting the quality of learned message embedding. Moreover, these methods statically initialize node embedding before training, which, in turn, limits the ability to learn from message texts and relations simultaneously. In this article, we approach social event detection from a new perspective based on Pre-trained Language Models (PLMs), and present  RPLM S E D  ( R elational prompt-based  P re-trained  L anguage  M odels for  S ocial  E vent  D etection). We first propose a new pairwise message modeling strategy to construct social messages into message pairs with multi-relational sequences. Secondly, a new multi-relational prompt-based pairwise message learning mechanism is proposed to learn more comprehensive message representation from message pairs with multi-relational prompts using PLMs. Thirdly, we design a new clustering constraint to optimize the encoding process by enhancing intra-cluster compactness and inter-cluster dispersion, making the message representation more distinguishable. We evaluate the  RPLM S E D  on three real-world datasets, demonstrating that the  RPLM S E D  model achieves state-of-the-art performance in offline, online, low-resource, and long-tail distribution scenarios for social event detection tasks.}
}


@article{DBLP:journals/tois/WanWXHLLLS25,
	author = {Qizhi Wan and
                  Changxuan Wan and
                  Keli Xiao and
                  Rong Hu and
                  Dexi Liu and
                  Guoqiong Liao and
                  Xiping Liu and
                  Yuxin Shuai},
	title = {A Multifocal Graph-Based Neural Network Scheme for Topic Event Extraction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {13:1--13:36},
	year = {2025},
	url = {https://doi.org/10.1145/3696353},
	doi = {10.1145/3696353},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WanWXHLLLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event extraction is a long-standing and challenging task in natural language processing, and existing studies mainly focus on extracting events within sentences. However, a significant problem that has not been carefully investigated is whether an “event topic” can be identified to represent the main aspects of extracted events. This article formulates the “topic event” extraction problem, aiming to identify a representative event from extracted ones. Specifically, after defining the topic event, we develop a multifocal graph-based framework to handle the extraction task. To enrich the associations of events and their tokens, we construct four event graphs, including the event subgraph and three event-associated graphs (i.e., event dependency parsing graph, event organization graph, and event share token graph), that reflect the internal and external structures of events, respectively. Subsequently, we design a multi-attention event-graph neural network to capture these event graph structures and improve event subgraph embedding. Finally, the output embeddings in the last layer of each channel are concatenated and fed into a fully connected network for topic event recognition. Extensive experiments validate the effectiveness of our method, and the results confirm its superiority over state-of-the-art baselines. In-depth analyses explore the essential factors (e.g., graph structures, attentions, feature generation method, etc.) determining the extraction performance.}
}


@article{DBLP:journals/tois/WuPSKF25,
	author = {Xuyang Wu and
                  Ajit Puthenputhussery and
                  Hongwei Shang and
                  Changsung Kang and
                  Yi Fang},
	title = {Meta-Learning to Rank for Sparsely Supervised Queries},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {14:1--14:29},
	year = {2025},
	url = {https://doi.org/10.1145/3698876},
	doi = {10.1145/3698876},
	timestamp = {Thu, 09 Oct 2025 11:08:29 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WuPSKF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Supervisory signals are a critical resource for training learning to rank models. In many real-world search and retrieval scenarios, these signals may not be readily available or could be costly to obtain for some queries. The examples include domains where labeling requires professional expertise, applications with strong privacy constraints, and user engagement information that are too scarce. We refer to these scenarios as sparsely supervised queries which pose significant challenges to traditional learning to rank models. In this work, we address sparsely supervised queries by proposing a novel meta-learning to rank framework which leverages fast learning and adaption capability of meta-learning. The proposed approach accounts for the fact that different queries have different optimal parameters for their rankers, in contrast to traditional learning to rank models which only learn a global ranking model applied to all the queries. In consequence, the proposed method would yield significant advantages especially when new queries are of different characteristics with the training queries. Moreover, the proposed meta-learning to rank framework is generic and flexible. We conduct a set of comprehensive experiments on both public datasets and a real-world e-commerce dataset. The results demonstrate that the proposed meta-learning approach can significantly enhance the performance of learning to rank models with sparsely labeled queries.}
}


@article{DBLP:journals/tois/LiFLWLWCJ25,
	author = {Bobo Li and
                  Hao Fei and
                  Fei Li and
                  Shengqiong Wu and
                  Lizi Liao and
                  Yinwei Wei and
                  Tat{-}Seng Chua and
                  Donghong Ji},
	title = {Revisiting Conversation Discourse for Dialogue Disentanglement},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {15:1--15:34},
	year = {2025},
	url = {https://doi.org/10.1145/3698191},
	doi = {10.1145/3698191},
	timestamp = {Wed, 28 Jan 2026 16:50:16 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiFLWLWCJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dialogue disentanglement aims to detach the chronologically ordered utterances into several independent sessions. Conversation utterances are essentially organized and described by the underlying discourse, and thus dialogue disentanglement requires the full understanding and harnessing of the intrinsic discourse attribute. In this article, we propose enhancing dialogue disentanglement by taking full advantage of the dialogue discourse characteristics. First of all,  in feature encoding stage , we construct the heterogeneous graph representations to model the various dialogue-specific discourse structural features, including the static speaker-role structures (i.e., speaker-utterance and speaker-mentioning structure) and the dynamic contextual structures (i.e., the utterance-distance and partial-replying structure). We then develop a structure-aware framework to integrate the rich structural features for better modeling the conversational semantic context. Second,  in model learning stage , we perform optimization with a hierarchical ranking loss mechanism, which groups dialogue utterances into different discourse levels and carries training covering pairwise and session-wise levels hierarchically. Third,  in inference stage , we devise an easy-first decoding algorithm, which performs utterance pairing under the easy-to-hard manner with a global context, breaking the constraint of traditional sequential decoding order. On two benchmark datasets, our overall system achieves new state-of-the-art performances on all evaluations. In-depth analyses further demonstrate the efficacy of each proposed idea and also reveal how our methods help advance the task. Our work has great potential to facilitate broader multi-party multi-thread dialogue applications.}
}


@article{DBLP:journals/tois/ZhouZGLG25,
	author = {Qi Zhou and
                  Peng Zhang and
                  Hansu Gu and
                  Tun Lu and
                  Ning Gu},
	title = {Exploring Cross-Site User Modeling without Cross-Site User Identity
                  Linkage: {A} Case Study of Content Preference Prediction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {16:1--16:28},
	year = {2025},
	url = {https://doi.org/10.1145/3697832},
	doi = {10.1145/3697832},
	timestamp = {Wed, 29 Oct 2025 15:13:38 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhouZGLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing user modeling on two or more social media platforms collaboratively and complementing each other (cross-site user modeling) has been a significant problem in the area of social media mining in recent years. The core of this problem is to get to know a person’s identities on multiple platforms and then train user models collaboratively among these platforms. However, for privacy protection, many people do not want their identities on different platforms to be linked and disclosed. For this problem, we set cross-site Content Preference Prediction as a task and propose a cross-site user modeling method without cross-site User Identity Linkage (UIL). The core thought borrowed from privacy-preserving recommender system research is to organize social media identities into groups to hide the identity linkage among platforms. Experiments on real-world datasets suggest that our method outperforms the existing cross-site user modeling methods with cross-site UIL regarding several metrics.}
}


@article{DBLP:journals/tois/LiRSD25,
	author = {Jie Li and
                  Yongli Ren and
                  Mark Sanderson and
                  Ke Deng},
	title = {Explaining Recommendation Fairness from a User/Item Perspective},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {17:1--17:30},
	year = {2025},
	url = {https://doi.org/10.1145/3698877},
	doi = {10.1145/3698877},
	timestamp = {Mon, 25 Aug 2025 12:08:39 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiRSD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems play a crucial role in personalizing user experiences, yet ensuring fairness in their outcomes remains an elusive challenge. This work explores the impact of individual users or items on the fairness of recommender systems, thus addressing a significant knowledge gap in the field. We introduce an innovative approach called Adding-Based Counterfactual Fairness Reasoning ( ACFR ), designed to elucidate recommendation fairness from the unique perspectives of users and items. Conventional methodologies, like erasing-based counterfactual analysis, pose limitations, particularly in modern recommender systems dealing with a large number of users and items. These traditional methods, by excluding specific users or items, risk disrupting the crucial relational structure central to collaborative filtering recommendations. In contrast,  ACFR  employs an adding-based counterfactual analysis, a unique strategy allowing us to consider potential, yet-to-happen user-item interactions. This strategy preserves the core user-item relational structure, while predicting future behaviors of users or items. The commonly used feature-based counterfactual analysis, relying on gradient-based optimization to identify interference on each feature, is not directly applicable in our case. In the recommendation scenario we consider, only interactions between users and items are present during model training—no distinct features are involved. Consequently, the traditional mechanism proves impractical for identifying interference on these existing interactions. Our extensive experiments validate the superiority of  ACFR  over traditional baseline methods, demonstrating significant improvements in recommendation fairness on benchmark datasets. This work, therefore, provides a fresh perspective and a promising methodology for enhancing fairness in recommender systems.}
}


@article{DBLP:journals/tois/LanZCWPPZ25,
	author = {Wei Lan and
                  Guoxian Zhou and
                  Qingfeng Chen and
                  Wenguang Wang and
                  Shirui Pan and
                  Yi Pan and
                  Shichao Zhang},
	title = {Contrastive Clustering Learning for Multi-Behavior Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {18:1--18:23},
	year = {2025},
	url = {https://doi.org/10.1145/3698192},
	doi = {10.1145/3698192},
	timestamp = {Fri, 07 Mar 2025 18:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LanZCWPPZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increasing multiple behavior recommendation models have achieved great successes. However, many models do not consider commonalities and differences between behaviors and data sparsity of the target behavior. This article proposes a novel multi-behavior recommendation model based on contrastive clustering learning (MBRCC). Specifically, the graph convolutional network (GCN) is employed to obtain the embeddings of users and items, respectively. Then, three kinds of tasks (including behavior-level embedding, instance-level embedding, and cluster-level embedding) are designed to optimize the embeddings of users and items. In behavior-level embedding, we design an adaptive parameter learning strategy to analyze the impact of auxiliary behaviors on the target behavior. Then, the embeddings of users for each behavior are weighted to obtain the final embeddings of users. In instance-level embedding, we employ contrastive learning to analyze the instances of user and item for mitigating the issue of data sparsity. In cluster-level embedding, we design a new cluster contrastive learning method to capture the similarity between groups of user and item. Finally, we combine these three tasks to improve the quality of the embeddings of users and items. We conduct extensive experiments on three real-world datasets and experimental results indicate that the MBRCC remarkably outperforms numerous existing recommendation models.}
}


@article{DBLP:journals/tois/ZhangWCLZ25,
	author = {Zeyang Zhang and
                  Xin Wang and
                  Haibo Chen and
                  Haoyang Li and
                  Wenwu Zhu},
	title = {Disentangled Dynamic Graph Attention Network for Out-of-Distribution
                  Sequential Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {19:1--19:42},
	year = {2025},
	url = {https://doi.org/10.1145/3701988},
	doi = {10.1145/3701988},
	timestamp = {Fri, 07 Mar 2025 18:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangWCLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation, leveraging user-item interaction histories to provide personalized and timely suggestions, has drawn significant research interest recently. With the power of exploiting spatio-temporal dynamics, Dynamic Graph Neural Networks (DyGNNs) show great potential in sequential recommendation by modeling the dynamic relationship between users and items. However, spatio-temporal distribution shifts naturally exist in out-of-distribution sequential recommendation, where both user-item relationships and temporal sequences demonstrate pattern shifts. The out-of-distribution scenarios may lead to the failure of existing DyGNNs in handling spatio-temporal distribution shifts in sequential recommendation, given that the patterns they exploit tend to be variant w.r.t labels under distribution shifts. In this article, we propose Disentangled Intervention-based Dynamic graph Attention networks with Invariance Promotion ( I-DIDA ) to handle spatio-temporal distribution shifts in sequential recommendation by discovering and utilizing  invariant patterns , i.e., structures and features whose predictive abilities are stable across distribution shifts. Specifically, we first propose a disentangled spatio-temporal attention network to capture the variant and invariant patterns. By utilizing the disentangled patterns, we design a spatio-temporal intervention mechanism to create multiple interventional distributions and an environment inference module to infer the latent spatio-temporal environments, and minimize the invariance loss to leverage the invariant patterns with stable predictive abilities under distribution shifts. Extensive experiments demonstrate the superiority of our method over state-of-the-art sequential recommendation baselines under distribution shifts.}
}


@article{DBLP:journals/tois/ZhuLZXZYLC25,
	author = {Xi Zhu and
                  Fake Lin and
                  Ziwei Zhao and
                  Tong Xu and
                  Xiangyu Zhao and
                  Zikai Yin and
                  Xueying Li and
                  Enhong Chen},
	title = {Multi-Behavior Recommendation with Personalized Directed Acyclic Behavior
                  Graphs},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {20:1--20:30},
	year = {2025},
	url = {https://doi.org/10.1145/3696417},
	doi = {10.1145/3696417},
	timestamp = {Tue, 07 Oct 2025 09:43:47 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhuLZXZYLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A well-developed recommendation system can not only leverage multi-typed interactions (such as  page view ,  add-to-cart , and  purchase ) to better identify user preferences but also demonstrate high performance, low complexity, and strong interpretability. However, many existing solutions for multi-behavior recommendation fall short of intuitive modeling of real-world scenarios, leading to overly complex models with massive parameters and cumbersome components. In particular, they share two critical limitations: (1) Some pioneering models are built upon the strict assumption of cascade effects across behaviors, which contradicts multifarious behavior paths in practical applications. (2) Existing approaches fail to explicitly capture the unique idiosyncrasies of users and even neglect the inherent nature of items involved in the multi-behavior interactions. To this end, we propose a novel Directed Acyclic Graph Convolutional Network (DA-GCN) for the multi-behavior recommendation task. Specifically, we pinpoint the partial order relations within the monotonic behavior chain and extend it to personalized directed acyclic behavior graphs to exploit behavior dependencies. Then, a GCN-based directed edge encoder is employed to distill rich collaborative signals embodied by each directed edge. In light of the information flows over the directed acyclic structure, we propose an attentive aggregation module to gather messages from all potential antecedent behaviors, representing distinct perspectives to understand the terminated behavior. Thus, we obtain comprehensive representations for the follow-up behavior through learnable distributions over its preceding behaviors, explicitly reflecting personalized interactive patterns of users and underlying properties of items simultaneously. Finally, we design a customized multi-task learning objective for flexible joint optimization. Extensive experiments on public benchmarking datasets fully demonstrate the superiority of DA-GCN with significant performance improvement and computational efficiency over a wide range of state-of-the-art methods. Our code is available at  https://github.com/xizhu1022/DA-GCN .}
}


@article{DBLP:journals/tois/WuXRCMRR25,
	author = {Shiguang Wu and
                  Xin Xin and
                  Pengjie Ren and
                  Zhumin Chen and
                  Jun Ma and
                  Maarten de Rijke and
                  Zhaochun Ren},
	title = {Learning Robust Sequential Recommenders through Confident Soft Labels},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {21:1--21:27},
	year = {2025},
	url = {https://doi.org/10.1145/3700876},
	doi = {10.1145/3700876},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WuXRCMRR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommenders that are trained on implicit feedback are usually learned as a multi-class classification task through softmax-based loss functions on one-hot class labels. However, one-hot training labels are sparse and may lead to biased training and sub-optimal performance. Dense, soft labels have been shown to help improve recommendation performance. However, how to generate high-quality and confident soft labels from noisy sequential interactions between users and items is still an open question. We propose a new learning framework for sequential recommenders, CSRec, which introduces confident soft labels to provide robust guidance when learning from user–item interactions. CSRec contains a teacher module that generates high-quality and confident soft labels and a student module that acts as the target recommender and is trained on the combination of dense, soft labels and sparse, one-hot labels. We propose and compare three approaches to constructing the teacher module: (i) model-level, (ii) data-level, and (iii) training-level. To evaluate the effectiveness and generalization ability of CSRec, we conduct experiments using various state-of-the-art sequential recommendation models as the target student module on four benchmark datasets. Our experimental results demonstrate that CSRec is effective in training better-performing sequential recommenders.}
}


@article{DBLP:journals/tois/ZhangZSS25,
	author = {Yi Zhang and
                  Yiwen Zhang and
                  Lei Sang and
                  Victor S. Sheng},
	title = {Simplify to the Limit! Embedding-Less Graph Collaborative Filtering
                  for Recommender Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {22:1--22:30},
	year = {2025},
	url = {https://doi.org/10.1145/3701230},
	doi = {10.1145/3701230},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZSS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tremendous positive driving effect of Graph Convolutional Network (GCN) and Graph Contrastive Learning (GCL) for recommender systems has become a consensus. GCN encoders are extensively used in recommendation models for capturing high-order connectivities between users and items, whereas GCL accelerates the training of recommendation tasks by adding extra supervision signals from contrastive objectives. However, little attention has been paid on corresponding theories that are truly tailored to recommendation tasks. From the technical perspective, Collaborative Filtering (CF) is seen as an important factor in recommender systems. It is applied to measure user–user, item–item, and user–item similarities rather than to achieve better clustering or node classification results. Besides, heuristic-based data augmentation may not be hold true in the field of recommender systems as it requires additional training costs and introduces noises that will corrupt the interaction graph structure and the semantic information of nodes. To tackle these limitations, we propose a novel Embedding-Less Graph Collaborative Filtering (EGCF) for recommendation, which is tailor-made for the problem mentioned for CF and further simplifies existing solutions. Structurally, it consists of two parts: embedding-less GCN and embedding-less GCL. The former improves user–item affinity by streamlining user-type embeddings and carrying out iterative graph convolution. And the latter utilizes three-type contrastive objectives to directly measure the alignment and the uniformity of users, items, and interaction pairs, respectively, avoiding any type of data augmentation or multi-view construction. Even though EGCF has been extremely streamlined, extensive experimental results on three classical datasets demonstrate the effectiveness of EGCF in terms of recommendation accuracy and training efficiency. The code and used datasets are released at  https://github.com/BlueGhostYi/ID-GRec .}
}


@article{DBLP:journals/tois/WangWTLY25,
	author = {Guolong Wang and
                  Xun Wu and
                  Xun Tu and
                  Zhaoyuan Liu and
                  Junchi Yan},
	title = {Unsupervised Video Moment Retrieval with Knowledge-Based Pseudo-Supervision
                  Construction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {23:1--23:26},
	year = {2025},
	url = {https://doi.org/10.1145/3701229},
	doi = {10.1145/3701229},
	timestamp = {Thu, 25 Dec 2025 12:46:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangWTLY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video moment retrieval locates a specified moment by a sentence query. Recent approaches have made remarkable advancements with large-scale video-sentence annotations. These annotations require extensive human labor and expertise, leading to the need for unsupervised fashion. Generating pseudo-supervision from videos is an effective strategy. With the power of the large-scale pre-trained model, we introduce knowledge into constructing pseudo-supervision. The main technical challenge is improving pseudo-supervision diversity and alleviating noise brought by external knowledge. To address these problems, we propose two Knowledge-Based Pseudo-Supervision Construction (KPSC) strategies: KPSC-P and KPSC-F. They all follow two steps: generating diverse samples and alleviating knowledge chaos. The main difference is that the former first learns a representation space with prompt tuning, while the latter directly utilizes data information. KPSC-P has two modules: (1) Proposal Prompt (PP): Generate temporal proposals; (2) Verb Prompt (VP): Generate pseudo-queries with noun-verb patterns. KPSC-F also has two modules: (1) Captioner: Generating candidate queries; (2) Filter: Alleviating knowledge chaos. Thus, our KPSC involves two attempts to extract knowledge from pre-trained models. Extensive experiments show that our attempts outperform the existing unsupervised methods on two public datasets (Charades-STA and ActivityNet-Captions) and perform on par with several methods using stronger supervision.}
}


@article{DBLP:journals/tois/WangFWMLL25,
	author = {Shijie Wang and
                  Wenqi Fan and
                  Xiao{-}Yong Wei and
                  Xiaowei Mei and
                  Shanru Lin and
                  Qing Li},
	title = {Multi-Agent Attacks for Black-Box Social Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {24:1--24:26},
	year = {2025},
	url = {https://doi.org/10.1145/3696105},
	doi = {10.1145/3696105},
	timestamp = {Fri, 07 Mar 2025 18:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangFWMLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users’ decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on  targeted attacks  to promote target items on vanilla recommender systems,  untargeted attacks  to degrade the overall prediction performance are less explored on social recommendations under a  black-box  scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework  MultiAttack  based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.}
}


@article{DBLP:journals/tois/ChenZLWQXZY25,
	author = {Chaochao Chen and
                  Yizhao Zhang and
                  Yuyuan Li and
                  Jun Wang and
                  Lianyong Qi and
                  Xiaolong Xu and
                  Xiaolin Zheng and
                  Jianwei Yin},
	title = {Post-Training Attribute Unlearning in Recommender Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {25:1--25:28},
	year = {2025},
	url = {https://doi.org/10.1145/3701987},
	doi = {10.1145/3701987},
	timestamp = {Tue, 20 Jan 2026 14:46:51 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ChenZLWQXZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as  attribute  and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this article, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers. We further extend this measurement to handle multi-class attribute cases with efficient computational overhead. The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization. We use stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods.}
}


@article{DBLP:journals/tois/ZhangMZ25,
	author = {Shuo Zhang and
                  Xiangwu Meng and
                  Yujie Zhang},
	title = {Variational Type Graph Autoencoder for Denoising on Event Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {1},
	pages = {26:1--26:27},
	year = {2025},
	url = {https://doi.org/10.1145/3703156},
	doi = {10.1145/3703156},
	timestamp = {Fri, 28 Mar 2025 18:26:55 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendations for events play a pivotal role in facilitating the discovery of upcoming intriguing events within Event-Based Social Networks (EBSNs). Previous research has established the crucial significance of mining contextual features and implicit relationships to enhance recommendation performance and alleviate data sparsity issues. However, the noise inherent in contextual features exacerbates data sparsity and hampers the ability of previous methods to explore implicit relationships for mitigating data sparsity. To address this challenge, we propose a variational type graph autoencoder model that attenuates the influence of noise in different types of context features by introducing type-specific latent variables. First, we introduce a heterogeneous denoising convolution module composed of two components: (1) Denoising attention aggregation is proposed to mitigate the influence of noisy structures and uncover implicit relationships. (2) A heterogeneous normalization module leverages context features within the same type to alleviate the effects of noise in context features and data sparsity. Furthermore, we propose a learnable heterogeneous mixture prior that assists in assigning different priors to distinct types of latent variables, effectively modeling different types of contextual features. Through comprehensive experiments conducted on real-world datasets, we demonstrate the compelling performance of our model compared to state-of-the-art competitive approaches.}
}


@article{DBLP:journals/tois/LinDXLCZLWLZGYTZ25,
	author = {Jianghao Lin and
                  Xinyi Dai and
                  Yunjia Xi and
                  Weiwen Liu and
                  Bo Chen and
                  Hao Zhang and
                  Yong Liu and
                  Chuhan Wu and
                  Xiangyang Li and
                  Chenxu Zhu and
                  Huifeng Guo and
                  Yong Yu and
                  Ruiming Tang and
                  Weinan Zhang},
	title = {How Can Recommender Systems Benefit from Large Language Models: {A}
                  Survey},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {28:1--28:47},
	year = {2025},
	url = {https://doi.org/10.1145/3678004},
	doi = {10.1145/3678004},
	timestamp = {Tue, 18 Nov 2025 15:39:38 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LinDXLCZLWLZGYTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of online services and web applications, recommender systems (RS) have become increasingly indispensable for mitigating information overload and matching users’ information needs by providing personalized suggestions over items. Although the RS research community has made remarkable progress over the past decades, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-domain world knowledge, and difficulties in comprehending users’ underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities for various natural language processing (NLP) tasks, which mainly stem from their extensive open-world knowledge, logical and commonsense reasoning abilities, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of RS and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their common knowledge and capabilities to compensate for the limitations of CRM. In this article, we conduct a comprehensive survey on this research direction, and draw a bird’s-eye view from the perspective of the whole pipeline in real-world RS. Specifically, we summarize existing research works from two orthogonal aspects: where and how to adapt LLM to RS. For the “ WHERE ” question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the “ HOW ” question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not during training, and whether to involve CRM for inference. Detailed analysis and general development paths are provided for both “WHERE” and “HOW” questions, respectively. Then, we highlight the key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects.}
}


@article{DBLP:journals/tois/WangCPZGSZ25,
	author = {Xin Wang and
                  Hong Chen and
                  Zirui Pan and
                  Yuwei Zhou and
                  Chaoyu Guan and
                  Lifeng Sun and
                  Wenwu Zhu},
	title = {Automated Disentangled Sequential Recommendation with Large Language
                  Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {29:1--29:29},
	year = {2025},
	url = {https://doi.org/10.1145/3675164},
	doi = {10.1145/3675164},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangCPZGSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation aims to recommend the next items that a target user may have interest in based on the user’s sequence of past behaviors, which has become a hot research topic in both academia and industry. In the literature, sequential recommendation adopts a Sequence-to-Item or Sequence-to-Sequence training strategy, which supervises a sequential model with a user’s next one or more behaviors as the labels and the sequence of the past behaviors as the input. However, existing powerful sequential recommendation approaches employ more and more complex deep structures such as Transformer in order to accurately capture the sequential patterns, which heavily rely on hand-crafted designs on key attention mechanism to achieve state-of-the-art performance, thus failing to automatically obtain the optimal design of attention representation architectures in various scenarios with different data. Other works on classic automated deep recommender systems only focus on traditional settings, ignoring the problem of sequential scenarios. In this article, we study the problem of automated sequential recommendation, which faces two main challenges: (1) How can we design a proper search space tailored for attention automation in sequential recommendation, and (2) How can we accurately search effective attention representation architectures considering multiple user interests reflected in the sequential behavior. To tackle these challenges, we propose an automated disentangled sequential recommendation (AutoDisenSeq) model. In particular, we employ neural architecture search (NAS) and design a search space tailored for automated attention representation in attentive intention-disentangled sequential recommendation with an expressive and efficient space complexity of  O ( n 2 )  given  n  as the number of layers. We further propose a context-aware parameter sharing mechanism taking characteristics of each sub-architecture into account to enable accurate architecture performance estimations and great flexibility for disentanglement of latent intention representation. Moreover, we propose AutoDisenSeq-large language model (LLM), which utilizes the textual understanding power of LLM as a guidance to refine the candidate list for recommendation from AutoDisenSeq. We conduct extensive experiments to show that our proposed AutoDisenSeq model and AutoDisenSeq-LLM model outperform existing baseline methods on four real-world datasets in both overall recommendation and cold-start recommendation scenarios.}
}


@article{DBLP:journals/tois/WangYZZC25,
	author = {Hao Wang and
                  Mingjia Yin and
                  Luankang Zhang and
                  Sirui Zhao and
                  Enhong Chen},
	title = {{MF-GSLAE:} {A} Multi-Factor User Representation Pre-Training Framework
                  for Dual-Target Cross-Domain Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {30:1--30:28},
	year = {2025},
	url = {https://doi.org/10.1145/3690382},
	doi = {10.1145/3690382},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangYZZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the dual-target cross-domain recommendation has been an emerging research problem, which aims to improve the performances of both source and target domains by transferring the preferences of overlapping users. Most of the existing work adopted a coarse-grained manner to detach general users’ preferences and associate them with domain-specific information for enhancing user representation learning, which fails to depict the differences in users’ diverse preferences and aggregate relevant preferences with improper propagation. To this end, in this article, we propose a multi-factor user representation pre-training framework, dubbed MF-GSLAE, with a focus on fine-grained preference learning and transferring. Specifically, we first propose a fine-grained factor representation pre-training paradigm. It projects the behavior records of both domains into several subspaces and introduces a compactness regularization to generate multiple fine-grained preference factors. Furthermore, we propose a multi-factor graph structure learning method within linear complexity to efficiently construct preference connections on different scales of users, which could aggregate the intrinsic relationship of user preferences in immediate embedding spaces to capture high-order information. Following the pre-training, we subsequently design a factor selection module with the bootstrapping mechanism to adaptively choose the corresponding domain-related preferences and transfer domain-shared information through partial overlapping factors for addressing the negative transfer problem. Finally, the optimization objectives of both domains are formalized in a multi-task learning framework and derive the learned user representation in an end-to-end training manner. Extensive experimental results on several publicly available datasets have not only demonstrated the effectiveness of the learned user representations with the comparison of state-of-the-art baselines but also indicated the interpretability and robustness. The code of our work is publicly available at  https://github.com/USTC-StarTeam/MF-GSLAE .}
}


@article{DBLP:journals/tois/DiSWML25,
	author = {Yicheng Di and
                  Hongjian Shi and
                  Xiaoming Wang and
                  Ruhui Ma and
                  Yuan Liu},
	title = {Federated Recommender System Based on Diffusion Augmentation and Guided
                  Denoising},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {31:1--31:36},
	year = {2025},
	url = {https://doi.org/10.1145/3688570},
	doi = {10.1145/3688570},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/DiSWML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommender systems often struggle with accurate personalized recommendations due to data sparsity issues. Existing works use variational autoencoders and generative adversarial network methods to enrich sparse data. However, they often overlook diversity in the latent data distribution, hindering the model’s generative capacity. This characteristic of generative methods can introduce additional noise in many cases. Moreover, retaining personalized user preferences through the generation process remains a challenge. This work introduces DGFedRS, a Federated Recommender System Based on Diffusion Augmentation and Guided Denoising, designed to capture the diversity in the latent data distribution while preserving user-specific information and suppressing noise. In particular, we pre-train the diffusion model using the recommender dataset and use a diffusion augmentation strategy to generate interaction sequences, expanding the sparse user-item interactions in the discrete space. To preserve user-specific preferences in the generated interactions, we employ a guided denoising strategy to guide the generation process during reverse diffusion. Subsequently, we design a noise control strategy to reduce the damage to personalized information during the diffusion process. Additionally, a stepwise scheduling strategy is devised to input generated data into the sequential recommender model based on their challenge levels. The success of the DGFedRS approach is demonstrated by thorough experiments conduct on three real-world datasets.}
}


@article{DBLP:journals/tois/PengGZDDLLM25,
	author = {Yingtao Peng and
                  Chen Gao and
                  Yu Zhang and
                  Tangpeng Dan and
                  Xiaoyi Du and
                  Hengliang Luo and
                  Yong Li and
                  Xiaofeng Meng},
	title = {Denoising Alignment with Large Language Model for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {32:1--32:35},
	year = {2025},
	url = {https://doi.org/10.1145/3696662},
	doi = {10.1145/3696662},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/PengGZDDLLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The mainstream approach of GNN-based recommendation aggregates high-order ID information associated with the node in the user-item graph. The aggregation pattern using ID as signal has two disadvantages: lack of textual semantics and the impact of interaction noise. These disadvantages pose a threat to effectively learn user preferences, especially in capturing intricate user-item semantic relationships. Although  large language models (LLMs)  allow the integration of rich textual information into recommenders and have had groundbreaking applications in recommender systems, current works need to bridge the gap between different representation spaces. This is because LLM-based methods align the representations of GNN-based models only by using text embedding of LLM, leading to unsatisfactory results. To address this challenge, we propose a  denoising alignment framework with LLMs for GNN-based recommenders (DALR) , which aims to align structural representation with textual representation and mitigate the effects of noise. Specifically, we propose a modeling framework that integrates the representation of graph structure with textual information from LLMs to capture intricate user-item interactions. We also suggest an alignment paradigm to enhance representation performance by aligning semantic signals from LLMs and structural features from GNN models. Additionally, we introduce a contrastive learning scheme to relieve the impact of noise and improve model performance. Extensive experiments on public datasets demonstrate that our model consistently outperforms the state-of-the-art methods. DALR achieves improvements ranging from 2.82% to 12.20% in Recall@5 and from 1.04% to 3.48% in NDCG@5 compared to the strongest baseline model, using the Steam dataset as an example.}
}


@article{DBLP:journals/tois/ZhangZZYGT25,
	author = {Dan Zhang and
                  Shaojie Zheng and
                  Yifan Zhu and
                  Huihui Yuan and
                  Jibing Gong and
                  Jie Tang},
	title = {{MCAP:} Low-Pass GNNs with Matrix Completion for Academic Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {33:1--33:29},
	year = {2025},
	url = {https://doi.org/10.1145/3698193},
	doi = {10.1145/3698193},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZZYGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are commonly used and have shown promising performance in recommendation systems. A major branch, heterogeneous GNNs, models heterogeneous information by leveraging side information for academic paper recommendations. These networks use message passing and high-order propagation to learn representations for users and items. However, existing recommendation methods perform high-order propagation, leading to sub-optimal representation learning. To address this issue, this article proposes a framework called MCAP, which uses relation-aware GNNs and executes low-pass propagation with matrix completion to enhance academic paper recommendations. The framework uses an attention mechanism to learn top- U  relationships by constructing a user–user relation graph based on common authors and venues from interacted items. To efficiently and effectively capture semantic-aware similar items, MCAP builds an item–item relation graph by fusing side information of papers using text embedding models (e.g., Mistral) and large language models (e.g., GPT-3.5-Turbo, GLM-4). Finally, the relation-aware user–user and item–item graphs are incorporated into existing GNN-based models to generate representations of users and papers to enhance academic paper recommendations. The effectiveness of the MCAP is validated using four academic datasets, AMiner-PC, AMiner-WeChat, CiteULike, and DBLP, with user–item interactions and side information of papers. Comprehensive experiments show that the MCAP outperforms state-of-the-art models in terms of Recall@5, NDCG@5, and HR@5 with 69.2%, 70.5%, and 77.6% on the AMiner-WeChat dataset. The code for MCAP is available at  https://github.com/THUDM/MCAP .}
}


@article{DBLP:journals/tois/XuLXCT25,
	author = {Lixiang Xu and
                  Yusheng Liu and
                  Tong Xu and
                  Enhong Chen and
                  Yuanyan Tang},
	title = {Graph Augmentation Empowered Contrastive Learning for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {34:1--34:27},
	year = {2025},
	url = {https://doi.org/10.1145/3677377},
	doi = {10.1145/3677377},
	timestamp = {Thu, 20 Nov 2025 13:50:06 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuLXCT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The application of contrastive learning (CL) to collaborative filtering (CF) in recommender systems has achieved remarkable success. CL-based recommendation models mainly focus on creating multiple augmented views by employing different graph augmentation methods and utilizing these views for self-supervised learning. However, current CL methods for recommender systems usually struggle to fully address the problem of noisy data. To address this problem, we propose the  G raph  A ugmentation  E mpowered  C ontrastive  L earning  (GAECL)  for recommendation framework, which uses graph augmentation based on topological and semantic dual adaptation and global co-modeling via structural optimization to co-create contrasting views for better augmentation of the CF paradigm. Specifically, we strictly filter out unimportant topologies by reconstructing the adjacency matrix and mask unimportant attributes in nodes according to the PageRank centrality principle to generate an augmented view that filters out noisy data. Additionally, GAECL achieves global collaborative modeling through structural optimization and generates another augmented view based on the PageRank centrality principle. This helps to filter the noisy data while preserving the original semantics of the data for more effective data augmentation. Extensive experiments are conducted on five datasets to demonstrate the superior performance of our model over various recommendation models.}
}


@article{DBLP:journals/tois/GuoCBFCZC25,
	author = {Jiafeng Guo and
                  Yinqiong Cai and
                  Keping Bi and
                  Yixing Fan and
                  Wei Chen and
                  Ruqing Zhang and
                  Xueqi Cheng},
	title = {{CAME:} Competitively Learning a Mixture-of-Experts Model for First-stage
                  Retrieval},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {35:1--35:25},
	year = {2025},
	url = {https://doi.org/10.1145/3678880},
	doi = {10.1145/3678880},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/GuoCBFCZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The first-stage retrieval aims to retrieve a subset of candidate documents from a huge collection both effectively and efficiently. Since various matching patterns can exist between queries and relevant documents, previous work tries to combine multiple retrieval models to find as many relevant results as possible. The constructed ensembles, whether learned independently or jointly, do not care which component model is more suitable to an instance during training. Thus, they cannot fully exploit the capabilities of different types of retrieval models in identifying diverse relevance patterns. Motivated by this observation, in this article, we propose a Mixture-of-Experts (MoE) model consisting of representative matching experts and a novel competitive learning mechanism to let the experts develop and enhance their expertise during training. Specifically, our MoE model shares the bottom layers to learn common semantic representations and uses differently structured upper layers to represent various types of retrieval experts. Our competitive learning mechanism has two stages: (1) a standardized learning stage to train the experts equally to develop their capabilities to conduct relevance matching; (2) a specialized learning stage where the experts compete with each other on every training instance and get rewards and updates according to their performance to enhance their expertise on certain types of samples. Experimental results on retrieval benchmark datasets show that our method significantly outperforms the state-of-the-art baselines in the in-domain and out-of-domain settings.}
}


@article{DBLP:journals/tois/ZhangLHWLZ25,
	author = {Xu Zhang and
                  Zexu Lin and
                  Xiaoyu Hu and
                  Jianlei Wang and
                  Wenpeng Lu and
                  Deyu Zhou},
	title = {{SECON:} Maintaining Semantic Consistency in Data Augmentation for
                  Code Search},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {36:1--36:26},
	year = {2025},
	url = {https://doi.org/10.1145/3686151},
	doi = {10.1145/3686151},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangLHWLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient code search techniques are crucial in accelerating software development by aiding developers in locating specific code snippets and understanding code functionalities. This study investigates code search methodologies, focusing on the emerging significance of semantic consistency in data augmentation techniques. While existing approaches predominantly enhance raw data, often requiring additional preprocessing and incurring higher training costs, this research introduces a pioneering method operating at the code and query representation levels. By bypassing the need for extensive data processing, this novel approach fosters an interactive alignment between code and query, augmenting the semantic coherence crucial for effective code search. An extensive empirical evaluation of a diverse dataset across multiple programming languages substantiates the efficacy of this approach in significantly enhancing code search model performance compared to traditional methodologies. The implementation is publicly available on GitHub, 1  offering an accessible resource for further exploration and application.}
}


@article{DBLP:journals/tois/GeJSYL25,
	author = {Hongfei Ge and
                  Yuanchun Jiang and
                  Jianshan Sun and
                  Kun Yuan and
                  Yezheng Liu},
	title = {LLM-Enhanced Composed Image Retrieval: An Intent Uncertainty-Aware
                  Linguistic-Visual Dual Channel Matching Model},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {37:1--37:30},
	year = {2025},
	url = {https://doi.org/10.1145/3699715},
	doi = {10.1145/3699715},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/GeJSYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Composed image retrieval (CoIR) involves a multi-modal query of the reference image and modification text describing the desired changes, allowing users to express image retrieval intents flexibly and effectively. The key of CoIR lies in how to properly reason the search intent from the multi-modal query. Existing work either aligns the composite embedding of the multi-modal query and the target image embedding in the visual domain through late-fusion or converts all images into text descriptions and leverage large language models (LLM) for text semantic reasoning. However, this single-modality reasoning approach fails to comprehensively and interpretably capture the users’ ambiguous and uncertain intents in the multi-modal queries, incurring the inconsistency between retrieved results and ground truth. Besides, the expensive manually annotated datasets limit the further performance improvement of CoIR. To this end, this article proposes an LLM-enhanced Intent Uncertainty-Aware Linguistic-Visual Dual Channel Matching Model (IUDC), which combines the strengths of multi-modal late-fusion and LLMs for CoIR. We first construct an LLM-based triplet augmentation strategy to generate more synthetic training triplets. Based on this, the core of IUDC consists of two matching channels: the semantic matching channel is responsible for intent reasoning on the aspect-level attributes extracted by an LLM, and the visual matching channel accounts for the fine-grained visual matching between multi-modal fusion embedding and target images. Considering the intent uncertainty presented in the multi-modal queries, we introduce Probability Distribution Encoder (PDE) to project the intents as probabilistic distributions in the two matching channels. Consequently, a mutually enhanced module is designed to share knowledge between the visual and semantic representations for better representation learning. Finally, the matching scores of two channels are added to retrieve the target image. Extensive experiments conducted on two real datasets demonstrate the effectiveness and superiority of our model. Notably, with the help of the proposed LLM-based triplet augmentation strategy, our model achieves a new record of state-of-the-art performance among all datasets.}
}


@article{DBLP:journals/tois/JafarzadehEAZ25,
	author = {Parastoo Jafarzadeh and
                  Faezeh Ensan and
                  Mahdiyar Ali Akbar Alavi and
                  Fattane Zarrinkalam},
	title = {A Knowledge Graph Embedding Model for Answering Factoid Entity Questions},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {38:1--38:27},
	year = {2025},
	url = {https://doi.org/10.1145/3678003},
	doi = {10.1145/3678003},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/JafarzadehEAZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources, such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This article introduces the knowledge graph embedding model for FEQ (KGE-FEQ) answering. Leveraging a textual knowledge graph derived from extensive text collections, KGE-FEQ encodes textual relationships between entities. The model employs a two-step process: (1) Triple Retrieval, where relevant triples are retrieved from the textual knowledge graph based on semantic similarities to the question, and (2) Answer Selection, where a knowledge graph embedding approach is utilized for answering the question. This involves positioning the embedding for the answer entity close to the embedding of the question entity, incorporating a vector representing the question and textual relations between entities. Extensive experiments evaluate the performance of the proposed approach, comparing KGE-FEQ to state-of-the-art baselines in FEQ answering and the most advanced open-domain question answering techniques applied to FEQs. The results show that KGE-FEQ outperforms existing methods across different datasets. Ablation studies highlights the effectiveness of KGE-FEQ when both the question and textual relations between entities are considered for answering questions.}
}


@article{DBLP:journals/tois/LiWLYWYFGY25,
	author = {Xinze Li and
                  Hanbin Wang and
                  Zhenghao Liu and
                  Shi Yu and
                  Shuo Wang and
                  Yukun Yan and
                  Yukai Fu and
                  Yu Gu and
                  Ge Yu},
	title = {Building a Coding Assistant via the Retrieval-Augmented Language Model},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {39:1--39:25},
	year = {2025},
	url = {https://doi.org/10.1145/3695868},
	doi = {10.1145/3695868},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiWLYWYFGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pretrained language models have shown strong effectiveness in code-related tasks, such as code retrieval, code generation, code summarization, and code completion tasks. In this article, we propose COde assistaNt viA retrieval-augmeNted language model (CONAN), which aims to build a code assistant by mimicking the knowledge-seeking behaviors of humans during coding. Specifically, it consists of a code structure-aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and Masked Entity Prediction tasks to make language models code structure-aware and learn effective representations for code snippets and documentation. Then CONAN-G designs a dual-view code representation mechanism for implementing a retrieval-augmented code generation model. CONAN-G regards the code documentation descriptions as prompts, which help language models better understand the code semantics. Our experiments show that CONAN achieves convincing performance on different code generation tasks and significantly outperforms previous retrieval augmented code generation models. Our further analyses show that CONAN learns tailored representations for both code snippets and documentation by aligning code-documentation data pairs and capturing structural semantics by masking and predicting entities in the code data. Additionally, the retrieved code snippets and documentation provide necessary information from both program language and natural language to assist the code generation process. CONAN can also be used as an assistant for Large Language Models (LLMs), providing LLMs with external knowledge in shorter code document lengths to improve their effectiveness on various code tasks. It shows the ability of CONAN to extract necessary information and help filter out the noise from retrieved code documents.}
}


@article{DBLP:journals/tois/MaoDXGWZ25,
	author = {Yuren Mao and
                  Xuemei Dong and
                  Wenyi Xu and
                  Yunjun Gao and
                  Bin Wei and
                  Ying Zhang},
	title = {{FIT-RAG:} Black-Box {RAG} with Factual Information and Token Reduction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {40:1--40:27},
	year = {2025},
	url = {https://doi.org/10.1145/3676957},
	doi = {10.1145/3676957},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/MaoDXGWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the extraordinarily large number of parameters, fine-tuning large language models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a retrieval-augmented generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs’ preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this article proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs’ preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ, and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.}
}


@article{DBLP:journals/tois/LyuLNXTWWLXC25,
	author = {Yuanjie Lyu and
                  Zhiyu Li and
                  Simin Niu and
                  Feiyu Xiong and
                  Bo Tang and
                  Wenjin Wang and
                  Hao Wu and
                  Huanyong Liu and
                  Tong Xu and
                  Enhong Chen},
	title = {{CRUD-RAG:} {A} Comprehensive Chinese Benchmark for Retrieval-Augmented
                  Generation of Large Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {41:1--41:32},
	year = {2025},
	url = {https://doi.org/10.1145/3701228},
	doi = {10.1145/3701228},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LyuLNXTWWLXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types—create, read, update, and delete (CRUD). “Create” refers to scenarios requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete” pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub:  https://github.com/IAAR-Shanghai/CRUD_RAG .}
}


@article{DBLP:journals/tois/HuangYMZFWCPFQL25,
	author = {Lei Huang and
                  Weijiang Yu and
                  Weitao Ma and
                  Weihong Zhong and
                  Zhangyin Feng and
                  Haotian Wang and
                  Qianglong Chen and
                  Weihua Peng and
                  Xiaocheng Feng and
                  Bing Qin and
                  Ting Liu},
	title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy,
                  Challenges, and Open Questions},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {42:1--42:55},
	year = {2025},
	url = {https://doi.org/10.1145/3703155},
	doi = {10.1145/3703155},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/HuangYMZFWCPFQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.}
}


@article{DBLP:journals/tois/BevilacquaOQSZGYA25,
	author = {Marialena Bevilacqua and
                  Kezia Oketch and
                  Ruiyang Qin and
                  Will Stamey and
                  Xinyuan Zhang and
                  Yi Gan and
                  Kai Yang and
                  Ahmed Abbasi},
	title = {When Automated Assessment Meets Automated Content Generation: Examining
                  Text Quality in the Era of GPTs},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {43:1--43:36},
	year = {2025},
	url = {https://doi.org/10.1145/3702639},
	doi = {10.1145/3702639},
	timestamp = {Sun, 02 Nov 2025 21:29:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/BevilacquaOQSZGYA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models (LLMs) such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML models, human- and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that LLMs and transformer pretrained language models (PLMs) more accurately score human essay quality as compared to CNN/RNN and feature-based ML methods. Interestingly, we find that LLMs and transformer PLMs tend to score GPT-generated text 10–20% higher on average, relative to human-authored documents. Conversely, traditional deep learning and feature-based ML models score human text considerably higher. Further analysis reveals that even though the LLMs and transformer PLMs are exclusively fine-tuned on human text, they more prominently attend to certain tokens appearing only in GPT-generated text, possibly (in part) due to familiarity/overlap in pre-training. Our framework and results have implications for text classification settings where automated scoring of text is likely to be disrupted by generative AI.}
}


@article{DBLP:journals/tois/LiuLCCNK25,
	author = {Fan Liu and
                  Yaqi Liu and
                  Huilin Chen and
                  Zhiyong Cheng and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
	title = {Understanding Before Recommendation: Semantic Aspect-Aware Review
                  Exploitation via Large Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {44:1--44:26},
	year = {2025},
	url = {https://doi.org/10.1145/3704999},
	doi = {10.1145/3704999},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuLCCNK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems harness user–item interactions like clicks and reviews to learn their representations. Previous studies improve recommendation accuracy and interpretability by modeling user preferences across various aspects and intents. However, the aspects and intents are inferred directly from user reviews or behavior patterns, suffering from the data noise and the data sparsity problem. Furthermore, it is difficult to understand the reasons behind recommendations due to the challenges of interpreting implicit aspects and intents. To address these constraints, we harness the sentiment analysis capabilities of Large Language Models (LLMs) to enhance the accuracy and interpretability of the conventional recommendation methods. Specifically, inspired by the deep semantic understanding offered by LLMs, we introduce a chain-based prompting strategy to uncover semantic aspect-aware interactions, which provide clearer insights into user behaviors at a fine-grained semantic level. To incorporate the rich interactions of various aspects, we propose the simple yet effective Semantic Aspect-Based Graph Convolution Network (SAGCN). By performing graph convolutions on multiple semantic aspect graphs, SAGCN efficiently combines embeddings across multiple semantic aspects for final user and item representations. The effectiveness of the SAGCN was evaluated on four publicly available datasets through extensive experiments, which revealed that it outperforms all other competitors. Furthermore, interpretability analysis experiments were conducted to demonstrate the interpretability of incorporating semantic aspects into the model.}
}


@article{DBLP:journals/tois/CunhaFESRG25,
	author = {Washington Cunha and
                  Alejandro Moreo Fern{\'{a}}ndez and
                  Andrea Esuli and
                  Fabrizio Sebastiani and
                  Leonardo Rocha and
                  Marcos Andr{\'{e}} Gon{\c{c}}alves},
	title = {A Noise-Oriented and Redundancy-Aware Instance Selection Framework},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {45:1--45:33},
	year = {2025},
	url = {https://doi.org/10.1145/3705000},
	doi = {10.1145/3705000},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/CunhaFESRG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fine-tuning transformer-based deep-learning models are currently at the forefront of natural language processing (NLP) and information retrieval (IR) tasks. However, fine-tuning these  transformers  for specific tasks, especially when dealing with ever-expanding volumes of data, constant retraining requirements, and budget constraints, can be computationally and financially costly, requiring substantial energy consumption and contributing to carbon dioxide emissions. This article focuses on advancing the state-of-the-art (SOTA) on  instance selection  (IS)—a range of document filtering techniques designed to select the most representative documents for the sake of training. The objective is to either maintain or enhance classification effectiveness while reducing the overall training (fine-tuning) total processing time. In our prior research, we introduced the E2SC framework, a redundancy-oriented IS method focused on transformers and large datasets—currently the state-of-the-art in IS. Nonetheless, important research questions remained unanswered in our previous work, mostly due to E2SC’s sole emphasis on redundancy. In this article, we take our research a step further by proposing  biO-IS— an extended  bi - o bjective  i nstance  s election solution, a novel IS framework aimed at simultaneously removing redundant and  noisy  instances from the training. biO-IS estimates redundancy based on scalable, fast, and calibrated weak classifiers and captures noise with the support of a new entropy-based step. We also propose a novel iterative process to estimate near-optimum reduction rates for both steps. Our extended solution is able to reduce the training sets by 41% on average (up to 60%) while maintaining the effectiveness in  all  tested datasets, with speedup gains of 1.67 on average (up to 2.46x). No other baseline, not even our previous SOTA solution, was capable of achieving results with this level of quality, considering the tradeoff among training reduction, effectiveness, and speedup. To ensure reproducibility, our documentation, code, and datasets can be accessed on GitHub— https://github.com/waashk/bio-is .}
}


@article{DBLP:journals/tois/SunJZZHX25,
	author = {Ying Sun and
                  Yang Ji and
                  Hengshu Zhu and
                  Fuzhen Zhuang and
                  Qing He and
                  Hui Xiong},
	title = {Market-aware Long-term Job Skill Recommendation with Explainable Deep
                  Reinforcement Learning},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {46:1--46:35},
	year = {2025},
	url = {https://doi.org/10.1145/3704998},
	doi = {10.1145/3704998},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/SunJZZHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Continuously learning new skills is essential for talents to gain a competitive advantage in the labor market. Despite extensive efforts on relevance- or preference-based skill recommendations, little attention has been given to the practical effects of job skills in the market. To bridge this gap, we propose an explainable personalized skill learning recommendation system that considers the long-term learning benefits and costs. Specifically, we model skill learning utilities based on salary and learning cost associated with job positions and propose a multi-objective deep reinforcement learning framework to model and maximize long-term utilities. Furthermore, we propose a Self-explaining Skill Recommendation Deep Q-network (SeSRDQN) that captures and prototypes prevalent skill sets in the market into representative exemplars for decision-making. SeSRDQN quantitatively decomposes the talent’s long-term learning utility into contributions from each exemplar, offering a comprehensive and multi-factorial explanation across various skill learning options. To tackle the combinatorial complexity of the skill space, we develop an MCTS-based optimization-decoding iterative training procedure for explanation fidelity and human understandability. In this way, talents will receive a tailored roadmap of essential skills, complemented by exemplar-based explanations, to effectively plan their careers. Extensive experiments on a real-world dataset validate the effectiveness and explainability of our approach.}
}


@article{DBLP:journals/tois/ZhangZWLDX25,
	author = {Leping Zhang and
                  Xiao Zhang and
                  Yichao Wang and
                  Xuan Li and
                  Zhenhua Dong and
                  Jun Xu},
	title = {Adapting Constrained Markov Decision Process for {OCPC} Bidding with
                  Delayed Conversions},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {47:1--47:29},
	year = {2025},
	url = {https://doi.org/10.1145/3706420},
	doi = {10.1145/3706420},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZWLDX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, optimized cost-per-click (OCPC) has been widely adopted in online advertising. In OCPC, the advertiser sets an expected cost-per-conversion and pays per click, while the platform automatically adjusts the bid on each click to meet advertiser’s constraint. Existing bidding methods are based on feedback control, adjusting bids to keep the current cost-per-conversion close to the expected cost-per-conversion to avoid compensation. However, they overlook the conversion lag phenomenon: There always exists a time interval between the ad’s click time and conversion time. This interval makes existing methods overestimate the cost-per-conversion and results in over conservative bidding policies which finally hurts the revenue. To address the issue, this article proposes a novel bidding method, Bidding with Delayed Conversions (Bid-DC) which predicts the conversion probability of the clicked ads and used it to adjust the cost-per-conversion values. To ensure the bidding model can satisfy the advertiser’s constraint, constrained Markov decision process (CMDP) is adapted to automatically learn the optimal parameters from the log data. Both online and offline experiments demonstrate that Bid-DC outperforms the state-of-the-art baselines in terms of improving revenue. Empirical analysis also showed Bid-DC can accurately estimate the cost-per-conversion and make more stable bids.}
}


@article{DBLP:journals/tois/HuynhNNNYNN25,
	author = {Thanh Trung Huynh and
                  Trong Bang Nguyen and
                  Thanh Toan Nguyen and
                  Phi Le Nguyen and
                  Hongzhi Yin and
                  Quoc Viet Hung Nguyen and
                  Thanh Tam Nguyen},
	title = {Certified Unlearning for Federated Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {48:1--48:29},
	year = {2025},
	url = {https://doi.org/10.1145/3706419},
	doi = {10.1145/3706419},
	timestamp = {Mon, 12 May 2025 21:02:39 +0200},
	biburl = {https://dblp.org/rec/journals/tois/HuynhNNNYNN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems play a crucial role in providing web-based suggestion utilities by leveraging user behavior, preferences, and interests. In the context of privacy concerns and the proliferation of handheld devices, federated recommender systems have emerged as a promising solution. These systems allow each client to train a local model and exchange only the model updates with a central server, thus preserving data privacy. However, certain use cases necessitate the deduction of contributions from specific clients, a process known as “unlearning.” Existing machine unlearning methods are designed for centralized settings and do not cater to the collaborative nature of recommendation systems, thereby overlooking their unique characteristics. This article proposes CFRU, a novel federated recommendation unlearning model that enables efficient and certified removal of target clients from the global model. Instead of retraining the model, our approach rolls back and eliminates the historical updates associated with the target client. To efficiently store the learning process’s historical updates, we propose sampling strategies that reduce the number of historical updates, retaining only the most significant ones. Furthermore, we analyze the potential bias introduced by the removal of target clients’ updates at each training round and establish an estimation using the Lipschitz condition. Leveraging this estimation, we propose an efficient iterative scheme to accumulate the bias across all rounds, compensating for the removed updates from the global model and recovering its utility without requiring post-training steps. Extensive experiments conducted on two real-world datasets, incorporating two poison attack scenarios, have shown that our unlearning technique can achieve a model quality that is 99.3% equivalent to retraining the model from scratch while performing up to 1,000 times faster.}
}


@article{DBLP:journals/tois/DangLYGJZW25,
	author = {Yizhou Dang and
                  Yuting Liu and
                  Enneng Yang and
                  Guibing Guo and
                  Linying Jiang and
                  Jianzhe Zhao and
                  Xingwei Wang},
	title = {Efficient and Adaptive Recommendation Unlearning: {A} Guided Filtering
                  Framework to Erase Outdated Preferences},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {49:1--49:25},
	year = {2025},
	url = {https://doi.org/10.1145/3706633},
	doi = {10.1145/3706633},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/DangLYGJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation unlearning is an emerging task to erase the influences of user-specified data from a trained recommendation model. Most existing research follows the paradigm of partitioning the original dataset into multi-fold and then retraining corresponding sub-models while those influences are totally removed. Despite the effectiveness, two key problems remain unexplored: (i) Existing work becomes inefficient and computationally expensive to retrain all sub-models, especially when facing large amounts of unlearning data. (ii) User preferences are dynamically changing. If users express negative opinions on some interacted items they used to prefer, how can we adaptively erase the outdated preferences behind such transformation from the trained model? Although these unlearning data contain outdated information, there is still a lot of helpful knowledge worth preserving. Existing methods ignore this preservation during unlearning and may remove all the knowledge in the interactions, compromising the final performance. In light of these limitations, we propose a novel unlearning framework called GFEraser, which transforms the unlearning into an efficient guided filtering process to avoid time-consuming retraining and retain beneficial knowledge. Specifically, we develop an intra-user negative sampling strategy to learn the outdated preferences that need to be erased. Under the guidance of differential maximization agreement and attention-based fusion module, the original representations are adaptively filtered and aggregated based on the learned preferences. Besides, we leverage contrastive learning to preserve the invariant user preferences, maintaining the final performance. Finally, we devise a new metric called  Ranking Decrease Rate  to evaluate the unlearning effect. Experimental results demonstrate that GFEraser can maintain reliable recommendation performance while achieving efficient outdated preferences unlearning, up to 37 ×  acceleration.}
}


@article{DBLP:journals/tois/WangLRCMZZZBL25,
	author = {Wei Wang and
                  Yujie Lin and
                  Pengjie Ren and
                  Zhumin Chen and
                  Tsunenori Mine and
                  Jianli Zhao and
                  Qiang Zhao and
                  Moyan Zhang and
                  Xianye Ben and
                  Yujun Li},
	title = {Privacy-Preserving Sequential Recommendation with Collaborative Confusion},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {50:1--50:25},
	year = {2025},
	url = {https://doi.org/10.1145/3707204},
	doi = {10.1145/3707204},
	timestamp = {Wed, 28 Jan 2026 16:50:16 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangLRCMZZZBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation has attracted a lot of attention from both academia and industry, however the privacy risks associated with gathering and transferring users’ personal interaction data are often underestimated or ignored. Existing privacy-preserving studies are mainly applied to traditional collaborative filtering or matrix factorization rather than sequential recommendation. Moreover, these studies are mostly based on differential privacy or federated learning, which often lead to significant performance degradation, or have high requirements for communication. In this work, we address privacy-preserving from a different perspective. Unlike existing research, we capture collaborative signals of neighbor interaction sequences and directly inject indistinguishable items into the target sequence before the recommendation process begins, thereby increasing the perplexity of the target sequence. Even if the target interaction sequence is obtained by attackers, it is difficult to discern which ones are the actual user interaction records. To achieve this goal, we introduce a novel sequential recommender system called  CoLlaborative-cOnfusion seqUential recommenDer (CLOUD) , which incorporates a collaborative confusion mechanism to modify the raw interaction sequences before conducting recommendation. Specifically, CLOUD first calculates the similarity between the target interaction sequence and other neighbor sequences to find similar sequences. Then, CLOUD considers the shared representation of the target sequence and similar sequences to determine the operation to be performed: keep, delete, or insert. A copy mechanism is designed to make items from similar sequences have a higher probability to be inserted into the target sequence. Finally, the modified sequence is used to train the recommender and predict the next item. We conduct extensive experiments on three benchmark datasets. The experimental results show that CLOUD achieves a maximum modification rate of 66.57% on interaction sequences and obtains over 99% recommendation accuracy compared to the state-of-the-art sequential recommendation methods. This proves that CLOUD can effectively protect user privacy at minimal recommendation performance cost, which provides a new solution for privacy-preserving for sequential recommendation. Our implementation is available at  https://github.com/weiwang0927/CLOUD .}
}


@article{DBLP:journals/tois/ZhangZCLF25,
	author = {Qin Zhang and
                  Mengqi Zheng and
                  Shangsi Chen and
                  Han Liu and
                  Meng Fang},
	title = {Self Data Augmentation for Open Domain Question Answering},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {51:1--51:35},
	year = {2025},
	url = {https://doi.org/10.1145/3707449},
	doi = {10.1145/3707449},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZCLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information Retrieval (IR) constitutes a vital facet of Open Domain Question Answering (ODQA) systems, focusing on the exploration of pertinent information within extensive collections of passages, such as Wikipedia, to facilitate subsequent reader processing. Historically, IR relied on textual overlaps for relevant context retrieval, employing methods like BM25 and TF-IDF, which, however, lacked natural language understanding. The advent of deep learning ushered in a new era, leading to the introduction of Dense Passage Retrievers (DPR), shows superiority over traditional sparse retrievers. These dense retrievers leverage Pre-Trained Language Models (PLMs) to initialize context encoders, enabling the extraction of natural language representations. They utilize the distance between latent vectors of contexts as a metric for assessing similarity. However, DPR methods are heavily reliant on large volumes of meticulously labeled data, such as Natural Questions. The process of data labeling is both costly and time-intensive. In this article, we propose a novel data augmentation methodology Self Data Augmentation (SDA) that employs DPR models to automatically annotate unanswered questions. Specifically, we initiate the process by retrieving relevant pseudo passages for these unlabeled questions. We subsequently introduce three distinct passage selection methods to annotate these pseudo passages. Ultimately, we amalgamate the pseudo-labeled passages with the unanswered questions to create augmented data. Our experimental evaluations conducted on two extensive datasets (Natural Questions and TriviaQA), alongside a relatively small dataset (WebQuestions), utilizing three diverse base models, illustrate the significant enhancement achieved through the incorporation of freshly augmented data. Moreover, our proposed data augmentation method exhibits remarkable flexibility, which is readily adaptable to various dense retrievers. Additionally, we have conducted a comprehensive human study on the augmented data, which further supports our conclusions.}
}


@article{DBLP:journals/tois/YuanYQNYY25,
	author = {Wei Yuan and
                  Chaoqun Yang and
                  Liang Qu and
                  Quoc Viet Hung Nguyen and
                  Guanhua Ye and
                  Hongzhi Yin},
	title = {{PTF-FSR:} {A} Parameter Transmission-Free Federated Sequential Recommender
                  System},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {52:1--52:24},
	year = {2025},
	url = {https://doi.org/10.1145/3708344},
	doi = {10.1145/3708344},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YuanYQNYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommender systems, as a specialized branch of recommender systems that can capture users’ dynamic preferences for more accurate and timely recommendations, have made significant progress. Recently, due to increasing concerns about user data privacy, some researchers have implemented federated learning for sequential recommendation, a.k.a., Federated Sequential Recommender Systems (FedSeqRecs), in which a public sequential recommender model is shared and frequently transmitted between a central server and clients to achieve collaborative learning. Although these solutions mitigate user privacy to some extent, they present two significant limitations that affect their practical usability: (1) They require a globally shared sequential recommendation model. However, in real-world scenarios, the recommendation model constitutes a critical intellectual property for platform and service providers. Therefore, service providers may be reluctant to disclose their meticulously developed models. (2) The communication costs are high as they correlate with the number of model parameters. This becomes particularly problematic as the current FedSeqRec will be inapplicable when sequential recommendation marches into a large language model era. To overcome the above challenges, this article proposes a parameter transmission-free federated sequential recommendation framework (PTF-FSR), which ensures both model and data privacy protection to meet the privacy needs of service providers and system users alike. Furthermore, since PTF-FSR only transmits prediction results under privacy protection, which are independent of model sizes, this new federated learning architecture can accommodate more complex and larger sequential recommendation models. Extensive experiments conducted on three widely used recommendation datasets, employing various sequential recommendation models from both ID-based and ID-free paradigms, demonstrate the effectiveness and generalization capability of our proposed framework. To facilitate future research in this direction, we release our code at  https://github.com/hi-weiyuan/PTF-FSR .}
}


@article{DBLP:journals/tois/ChenLFLSW25,
	author = {Nuo Chen and
                  Jiqun Liu and
                  Hanpei Fang and
                  Yuankai Luo and
                  Tetsuya Sakai and
                  Xiao{-}Ming Wu},
	title = {Decoy Effect in Search Interaction: Understanding User Behavior and
                  Measuring System Vulnerability},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {53:1--53:58},
	year = {2025},
	url = {https://doi.org/10.1145/3708884},
	doi = {10.1145/3708884},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ChenLFLSW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study addresses (1) the influence of the decoy effect, a cognitive bias where the presence of an inferior item alters preferences between two options, on users’ search interactions and (2) the measurement of information retrieval systems’  vulnerability  to the decoy effect. 1  From the perspective of user behavior, this study investigates the influence of the decoy effect in information retrieval (IR) by examining how decoy results affect users’ interaction on search engine result pages (SERPs), particularly in terms of click-through likelihood, browsing dwell time, and perceived document usefulness. We conducted an experiment based upon regression analysis on user interaction logs from three user study datasets which in total encompass 24 topics, 841 unique search sessions, and 2,685 queries. The findings indicate that decoys significantly increase the likelihood of document clicks and perceived usefulness. To investigate whether the influence of the decoy varies across different levels of task difficulty and user knowledge, we ran an additional experiment on one of the three datasets, which encompasses 6 topics, 166 search sessions and 652 queries. The results indicate that when the task is less challenging, users are more likely to click on a document with a decoy. Additionally, they spend more time on the target document and assign it a higher usefulness score. Furthermore, users with lower knowledge levels about the topic tend to give higher usefulness ratings to the target document. Regarding IR system evaluation, this study provides empirical insights into measuring the vulnerability of text retrieval models to potential decoy effect. An evaluation metric, namely DEcoy Judgement and Assessment VUlnerability (DEJA-VU), is proposed to evaluate the possibility of a retrieval model ranking results in a way that could trigger decoy biases. The experiments on the Text REtrieval Conference (TREC) 19 Deep Learning (DL) passage retrieval task and the TREC 20 DL passage retrieval task demonstrate that ColBERT and SPLADE show higher relevance-oriented retrieval effectiveness while also displaying lower vulnerability to decoy effect. Overall, this work advances the understanding of decoy effect, a well-established concept in cognitive psychology and behavioral economics, in a novel application field (i.e., Information Retrieval). It contributes to modeling users’ search behavior in the context of cognitive biases, as well as assessment of the vulnerability of systems and ranking algorithms to the decoy effect.}
}


@article{DBLP:journals/tois/QuXXYLLKZ25,
	author = {Zekai Qu and
                  Ruobing Xie and
                  Chaojun Xiao and
                  Yuan Yao and
                  Zhiyuan Liu and
                  Fengzong Lian and
                  Zhanhui Kang and
                  Jie Zhou},
	title = {Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {54:1--54:28},
	year = {2025},
	url = {https://doi.org/10.1145/3708883},
	doi = {10.1145/3708883},
	timestamp = {Wed, 11 Jun 2025 21:01:33 +0200},
	biburl = {https://dblp.org/rec/journals/tois/QuXXYLLKZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the thriving of the pre-trained language model (PLM) widely verified in various NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel unified pre-trained language model enhanced sequential recommendation (UPSR) that thoroughly transfers the next item prediction task to a text generation task, aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise and ambiguity, and text length, to guide the  text → item  adaptation (selecting appropriate text to form the item textual representation) and  behavior sequence → text sequence  adaptation (transferring the sequence of item textual representations into a text sequence) differently for pre-training and fine-tuning stages, which are essential but under-explored by previous works. In experiments, we conduct extensive evaluations on seven datasets with both supervised and zero-shot settings and achieve the overall best performance. Comprehensive model analyses also provide valuable insights for behavior modeling via PLM, shedding light on large pre-trained recommendation models. The source codes will be released in the future.}
}


@article{DBLP:journals/tois/WangZYCTZCLSSZXDWW25,
	author = {Lei Wang and
                  Jingsen Zhang and
                  Hao Yang and
                  Zhiyuan Chen and
                  Jiakai Tang and
                  Zeyu Zhang and
                  Xu Chen and
                  Yankai Lin and
                  Hao Sun and
                  Ruihua Song and
                  Xin Zhao and
                  Jun Xu and
                  Zhicheng Dou and
                  Jun Wang and
                  Ji{-}Rong Wen},
	title = {User Behavior Simulation with Large Language Model-based Agents},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {2},
	pages = {55:1--55:37},
	year = {2025},
	url = {https://doi.org/10.1145/3708985},
	doi = {10.1145/3708985},
	timestamp = {Thu, 27 Nov 2025 14:26:18 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangZYCTZCLSSZXDWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Simulating high quality user behavior data has always been a fundamental yet challenging problem in human-centered applications such as recommendation systems, social networks, among many others. The major difficulty of user behavior simulation originates from the intricate mechanism of human cognitive and decision processes. Recently, substantial evidence has suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence and generalization capabilities. Inspired by such capabilities, in this article, we take an initial step to study the potential of using LLMs for user behavior simulation in the recommendation domain. To make LLMs act like humans, we design profile, memory and action modules to equip them, building LLM-based agents to simulate real users. To enable interactions between different agents and observe their behavior patterns, we design a sandbox environment, where each agent can interact with the recommendation system, and different agents can converse with their friends  via  one-to-one chatting or one-to-many social broadcasting. In the experiments, we first demonstrate the believability of the agent-generated behaviors based on both subjective and objective evaluations. Then, to show the potential applications of our method, we simulate and study two social phenomena including (1) information cocoons and (2) user conformity behaviors. We find that controlling the personalization degree of recommendation algorithms and improving the heterogeneity of user social relations can be two effective strategies for alleviating the problem of information cocoon, and the conformity behaviors can be highly influenced by the amount of user social relations. To advance this direction, we have released our project at  https://github.com/RUC-GSAI/YuLan-Rec .}
}


@article{DBLP:journals/tois/YomTovL25,
	author = {Elad Yom{-}Tov and
                  Liat Levontin},
	title = {The In-Situ Effect of Offensive Ads on Search Engine Users},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {56:1--56:22},
	year = {2025},
	url = {https://doi.org/10.1145/3704438},
	doi = {10.1145/3704438},
	timestamp = {Sat, 06 Sep 2025 20:29:40 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YomTovL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unscrupulous advertisers may try to increase attention to search ads by using offensive ads, which can increase attention and recall to the detriment of individuals and society. Here, we investigate whether offensive ads, when shown to search engine users, have such effects. We developed 12 search scenarios and created 4 versions of the search results page (SERP) for each scenario, where some of the ads were changed to be irrelevant and/or offensive. Crowdsourced judges found a strong correlation ( ≥ 0.63 ) between the reported number of annoying ads and the actual number of offensive and irrelevant ads, suggesting people conflate these attributes. Furthermore, we found that judges who assessed the SERPs for themselves reported lower positive affect and higher negative affect than judges asked to imagine the results were provided to someone else. In the latter case offensive ads also lead to slightly lower positive ( − 4 % ) and higher negative affect ( + 61 % ). Finally, in a recall test, only 6% of judges reported seeing an offensive ad when using search engines. Our work should further detract advertisers from using offensive ads since, in addition to previously documented adverse effects, such ads have a small but statistically significant negative effect on people’s emotional experience.}
}


@article{DBLP:journals/tois/LiuQZWZXT25,
	author = {Qidong Liu and
                  Zhaopeng Qiu and
                  Xiangyu Zhao and
                  Xian Wu and
                  Zijian Zhang and
                  Tong Xu and
                  Feng Tian},
	title = {A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication
                  Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {57:1--57:29},
	year = {2025},
	url = {https://doi.org/10.1145/3706631},
	doi = {10.1145/3706631},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuQZWZXT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Medication recommendation is one of the most critical health-related applications, which has attracted extensive research interest recently. Most existing works focus on a single hospital with abundant medical data. However, many small hospitals only have a few records, which hinders applying existing medication recommendation works to the real world. Thus, we seek to explore a more practical setting, i.e., multi-center medication recommendation. In this setting, most hospitals have few records, but the total number of records is large. Though small hospitals may benefit from total affluent records, it is also faced with the challenge that the data distributions between various hospitals are much different. In this work, we introduce a novel  Contrastive Pretrain Model with Prompt Tuning (TEMPT)  for multi-center medication recommendation, which includes two stages of pretraining and finetuning. We first design two self-supervised tasks for the pretraining stage to learn general medical knowledge. They are mask prediction and contrastive tasks, which extract the intra- and inter-relationships of input diagnosis and procedures. Furthermore, we devise a novel prompt tuning method to capture the specific information of each hospital rather than adopting the common finetuning. On the one hand, the proposed prompt tuning can better learn the heterogeneity of each hospital to fit various distributions. On the other hand, it can also relieve the catastrophic forgetting problem of finetuning. To validate the proposed model, we conduct extensive experiments on the public eICU, a multi-center medical dataset. The experimental results illustrate the effectiveness of our model. The implementation code is available to ease the reproducibility. 1}
}


@article{DBLP:journals/tois/YuSJ25,
	author = {Yi Yu and
                  Kazunari Sugiyama and
                  Adam Jatowt},
	title = {Domain Counterfactual Data Augmentation for Explainable Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {58:1--58:30},
	year = {2025},
	url = {https://doi.org/10.1145/3711856},
	doi = {10.1145/3711856},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YuSJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing explanations for recommendation decisions is crucial for enhancing user trust and satisfaction in recommender systems. However, existing generative methods often produce generic, repetitive explanation texts that fail to reflect the true reasons behind user interests and item attributes. Thus, it is important to address this  degeneration  issue in recommendation explanations. This work tackles a key problem in explainable recommendation: understanding how  explanation degeneration occurs  and improving explanation quality by mitigating it. We argue that examining the causal mechanism underlying the data generation process is key to addressing this problem. Along this line, we identify a neglected hidden variable, which we refer to as  textual attributes . Textual attributes encompass various aspects, such as text style, word frequency distributions, and more. Just like user persona and item attributes in traditional recommender systems, textual attributes also shape the nature of explanations. Our analysis of the causal graph reveals the underlying cause of the model’s degeneration. To address this issue, we propose a novel learning method called  Domain for Counterfactual Reasoning  (D4C). By using the auxiliary domain to generate counterfactual data and combining it with factual data, this approach helps the model focus more on the causal contributions of users and items during training. Extensive experiments on five real-world datasets from various platforms demonstrate the effectiveness of our approach.}
}


@article{DBLP:journals/tois/HuZSZHN25,
	author = {Linmei Hu and
                  Xinyu Zhang and
                  Dandan Song and
                  Changzhi Zhou and
                  Hongyu He and
                  Liqiang Nie},
	title = {Efficient and Effective Role Player: {A} Compact Knowledge-grounded
                  Persona-based Dialogue Model Enhanced by {LLM} Distillation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {59:1--59:29},
	year = {2025},
	url = {https://doi.org/10.1145/3711857},
	doi = {10.1145/3711857},
	timestamp = {Sun, 02 Nov 2025 15:09:59 +0100},
	biburl = {https://dblp.org/rec/journals/tois/HuZSZHN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incorporating explicit personas into dialogue models is critical for generating responses that fulfill specific user needs and preferences, creating a more personalized and engaging interaction. Early works on persona-based dialogue generation directly concatenate the persona descriptions and dialogue history into relatively small pre-trained language models (PLMs) for response generation, which leads to uninformative and inferior results due to the sparse persona information and the limited model generation capabilities. Recently, large language models (LLMs) have shown their surprising capabilities in language generation. Prompting the LLMs with the persona descriptions for role-playing dialogue generation has also achieved promising results. However, deploying LLMs is challenging for practical applications due to their large scale, spurring efforts to distill the generation capabilities into more concise and compact models through teacher-student learning. In this article, we propose an efficient compact  K nowledge-grounded  P ersona-based  D ialogue model enhanced by LLM  D istillation (KPDD). Specifically, first, we propose to enrich the annotated persona descriptions by integrating external knowledge graphs (KGs) with a mixed encoding network, coupled with a mixture of experts (MoE) module for both informative and diverse response generation. The mixed encoding network contains multiple layers of modality interaction operations, enabling information from both modalities propagates to the other. Second, to fully exploit the generation capabilities of LLMs, we turn to the distillation technique to improve the generation capabilities of our model, facilitated by a natural language inference (NLI)-based filtering mechanism to extract high-quality information from LLMs. In addition, we employ a curriculum learning strategy to train our model on the high-quality filtered distilled data and progressively on the relatively noisy original data, enhancing its adaptability and performance. Extensive experiments show that KPDD outperforms state-of-the-art baselines in terms of both automatic and human evaluation.}
}


@article{DBLP:journals/tois/YuHDN25,
	author = {Li Yu and
                  Jianyong Hu and
                  Qihan Du and
                  Xi Niu},
	title = {MVideoRec: Micro Video Recommendations through Modality Decomposition
                  and Contrastive Learning},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {60:1--60:27},
	year = {2025},
	url = {https://doi.org/10.1145/3711855},
	doi = {10.1145/3711855},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YuHDN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized micro video recommendation aims to recommend the micro videos tailored to user preference based on the user’s interaction history with the micro videos, which has drawn increasing attention from both the academic and industrial communities. Existing solutions primarily concentrate on video-level interactions between users and micro videos to model their preferences, and cannot distinguish the finer-grained users’ interactions with various modalities. Ignoring modality-level interactions prevents the full understanding of the user’s true and subtle preferences on micro videos. To this end, in this article, we propose a Contrastive Multimodal Interaction Graph Learning ( MVideoRec ) model to automatically and explicitly learn the modality-level interaction between users and micro videos for recommendations. Specifically, we designed a graph structure learning module with a sparsification strategy to infer modality-level interaction graph, which will be dynamically and iteratively updated based on the node representations obtained from the node representation learning module. Furthermore, to address the lack of ground truth labels, we propose to generate teacher view from video-level interaction graph and student view from modality-level interaction graph, as well as construct intra-modality and inter-modality contrastive pairwise instances to provide self-supervised signals. Extensive experiments on three real-world micro video datasets validate the effectiveness of MVideoRec.}
}


@article{DBLP:journals/tois/LiuZCJZTYC25,
	author = {Zirui Liu and
                  Hailin Zhang and
                  Boxuan Chen and
                  Zihan Jiang and
                  Yikai Zhao and
                  Yangyu Tao and
                  Tong Yang and
                  Bin Cui},
	title = {{CAFE+:} Towards Compact, Adaptive, and Fast Embedding for Large-scale
                  Online Recommendation Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {61:1--61:42},
	year = {2025},
	url = {https://doi.org/10.1145/3713072},
	doi = {10.1145/3713072},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuZCJZTYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously achieve memory efficiency, low latency, and adaptability to dynamic data distribution. This article presents CAFE+, a Compact, Adaptive, and Fast Embedding compression framework that meets the above requirements. The design philosophy of CAFE+ is to dynamically allocate more memory to important features and less to unimportant ones. We assign unique embedding to important feature and allow multiple unimportant features sharing one embedding. We propose a fast and lightweight feature monitor, to real-time capture feature importance and report important features. We theoretically analyze the accuracy of our feature monitor and prove the superiority of CAFE+ from the aspect of model convergence. Extensive experiments show CAFE+ outperforms existing embedding compression methods, yielding  3.94 %  and  3.94 %  superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of  10,000 × . Building on our conference version [ 114 ], this journal version introduces several novel designs (implicit importance attenuation, adaptive threshold adjustment, and ColdSifter) that enable CAFE+ to more effectively adapt to long-term online learning and achieve better model quality. All codes are available at GitHub [ 112 ].}
}


@article{DBLP:journals/tois/LiuPZLCR25,
	author = {Yuanxing Liu and
                  Jiahuan Pei and
                  Weinan Zhang and
                  Ming Li and
                  Wanxiang Che and
                  Maarten de Rijke},
	title = {Augmentation with Neighboring Information for Conversational Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {62:1--62:49},
	year = {2025},
	url = {https://doi.org/10.1145/3712588},
	doi = {10.1145/3712588},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuPZLCR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational recommender systems (CRSs) suggest items to users by understanding their needs and preferences from natural language conversations. While users can freely express preferences, modeling needs and preferences solely from users’ conversations is challenging due to the sparsity of the available information. Prior work introduces external resources to enrich information expressed in conversations. Obtaining such resources is challenging and not always effective. Can learning intrinsic relations among conversations and items enhance information without the use of external resources? Inspired by collaborative filtering, we propose to use so-called neighboring relations within training data, i.e., relations between conversations, items, and similar conversations and items, to enhance our algorithmic understanding of CRSs. We propose a neighboring relations enhanced conversational recommender system (NR-CRS) and study how neighboring relations improve CRSs from two angles: (i) We mine preference information from neighboring conversations to enhance the modeling of user representations and learning of user preferences. (ii) We generate negative samples based on neighboring items to extend the data available for training CRSs. Experiments on the  ReDial  dataset show that neighboring relations enhanced conversational recommender system (NR-CRS) outperforms the state-of-the-art baseline by 11.3–20.6% regarding recommendation performance while generating informative and diverse responses. We also assess the capabilities of large language models (i.e., Llama 2, Llama 3, and Chinese-Alpaca2) for CRSs. While the generated responses exhibit enhanced fluency and informativeness, recommending target items with LLMs remains challenging; we recommend that LLMs be used as a decoding base for NR-CRS to generate relevant and informative responses.}
}


@article{DBLP:journals/tois/TranCHHCY25,
	author = {Hung Vinh Tran and
                  Tong Chen and
                  Nguyen Quoc Viet Hung and
                  Zi Huang and
                  Lizhen Cui and
                  Hongzhi Yin},
	title = {A Thorough Performance Benchmarking on Lightweight Embedding-based
                  Recommender Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {63:1--63:32},
	year = {2025},
	url = {https://doi.org/10.1145/3712589},
	doi = {10.1145/3712589},
	timestamp = {Tue, 27 Jan 2026 19:58:07 +0100},
	biburl = {https://dblp.org/rec/journals/tois/TranCHHCY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the creation of the Web, recommender systems (RSs) have been an indispensable personalization mechanism in information filtering. Most state-of-the-art RSs primarily depend on categorical features such as user and item IDs, and use embedding vectors to encode their information for accurate recommendations, resulting in an excessively large embedding table owing to the immense feature corpus. To prevent the heavily parameterized embedding table from harming RSs’ scalability, both academia and industry have seen increasing efforts compressing RS embeddings, and this trend is further amplified by the recent uptake in edge computing for online services. However, despite the prosperity of existing lightweight embedding-based RSs (LERSs), a strong diversity is seen in the evaluation protocols adopted across publications, resulting in obstacles when relating the reported performance of those LERSs to their real-world usability. On the other hand, among the two fundamental recommendation tasks, namely traditional collaborative filtering and content-based recommendation, despite their common goal of achieving lightweight embeddings, the outgoing LERSs are designed and evaluated with a straightforward “either-or” choice between the two tasks. Consequently, the lack of discussions on a method’s cross-task transferability will likely hinder the development of unified, more scalable solutions for production environments. Motivated by these unresolved issues, this study aims to systematically investigate existing LERSs’ performance, efficiency, and cross-task transferability  via  a thorough benchmarking process. To create a generic, task-independent baseline, we propose an efficient embedding compression approach based on magnitude pruning, which is proven to be an easy-to-deploy yet highly competitive baseline that outperforms various complex LERSs. Our study reveals the distinct performance of different LERSs across the two recommendation tasks, shedding light on their effectiveness and generalizability under different settings. Furthermore, to account for edge-based recommendation—an increasingly popular use case of LERSs, we have also deployed and tested all LERSs on a Raspberry Pi 4, where their efficiency bottleneck is exposed compared with GPU-based deployment. Finally, we conclude this article with critical summaries on the performance comparison, suggestions on model selection based on task objectives, and underexplored challenges around the applicability of existing LERSs for future research. To encourage and support future LERS research, we publish all source codes and data, checkpoints, and documentation at  https://github.com/chenxing1999/recsys-benchmark .}
}


@article{DBLP:journals/tois/ZhangWSLSLW25,
	author = {Haonan Zhang and
                  Dongxia Wang and
                  Zhu Sun and
                  Yanhui Li and
                  Youcheng Sun and
                  Huizhi Liang and
                  Wenhai Wang},
	title = {KG4RecEval: Does Knowledge Graph Really Matter for Recommender Systems?},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {64:1--64:36},
	year = {2025},
	url = {https://doi.org/10.1145/3713071},
	doi = {10.1145/3713071},
	timestamp = {Thu, 30 Oct 2025 19:09:51 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangWSLSLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems (RSs) are designed to provide personalized recommendations to users. Recently, knowledge graphs (KGs) have been widely introduced in RSs to improve recommendation accuracy. In this study, however, we demonstrate that RSs do not necessarily perform worse even if the KG is downgraded to the user-item interaction graph only (or removed). We propose an evaluation framework  KG4RecEval  to systematically evaluate how much a KG contributes to the recommendation accuracy of a KG-based RS, using our defined metric  KG utilization efficiency in recommendation  (KGER). We consider the scenarios where knowledge in a KG gets completely removed, randomly distorted and decreased, and also where recommendations are for cold-start users. Our extensive experiments on four commonly used datasets and a number of state-of-the-art KG-based RSs reveal that: to remove, randomly distort or decrease knowledge does not necessarily decrease recommendation accuracy, even for cold-start users. These findings inspire us to rethink how to better utilize knowledge from existing KGs, whereby we discuss and provide insights into what characteristics of datasets and KG-based RSs may help improve KG utilization efficiency. The code and supplementary material of this article are available at:  https://github.com/HotBento/KG4RecEval .}
}


@article{DBLP:journals/tois/LiuHAWLLZSCT25,
	author = {Bulou Liu and
                  Yiran Hu and
                  Qingyao Ai and
                  Yueyue Wu and
                  Yiqun Liu and
                  Chenliang Li and
                  Fan Zhang and
                  Weixing Shen and
                  Chong Chen and
                  Qi Tian},
	title = {Structure-Aware Conversational Legal Case Retrieval},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {65:1--65:28},
	year = {2025},
	url = {https://doi.org/10.1145/3711854},
	doi = {10.1145/3711854},
	timestamp = {Tue, 07 Oct 2025 08:47:51 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuHAWLLZSCT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Legal case retrieval is an important task in information retrieval that aims to retrieve relevant cases for given query cases. Conversational search paradigms have been shown to improve the search experience in legal case retrieval. However, there are two challenges in applying conversational search to legal scenarios. Firstly, legal search conversations often focus on different parts of legal case documents, but existing models struggle to capture the complex structural information and extract accurate relevance signals. Secondly, collecting large-scale conversational search datasets is costly, making it difficult to build reliable conversational legal case retrieval models. To address these challenges, we propose a Structure-Aware Matching Model (SAMM) for conversational legal case retrieval. SAMM extracts matching signals between conversational utterances and segments of the legal cases to incorporate structural information. We decouple the conversational search task into three subtasks and design pre-training tasks to overcome the lack of training data. Additionally, we create ConvLegal, the largest conversational legal case retrieval dataset to the best of our knowledge, for better evaluation of different methods. We train and evaluate SAMM and baselines on both a public dataset (CLCR) and ConvLegal. Experimental results demonstrate that SAMM outperforms existing models in legal case retrieval and conversational search.}
}


@article{DBLP:journals/tois/WangWRCRR25,
	author = {Zihan Wang and
                  Hanbing Wang and
                  Pengjie Ren and
                  Zhumin Chen and
                  Maarten de Rijke and
                  Zhaochun Ren},
	title = {Graph-Enhanced Prompt Learning for Cross-Domain Contract Element Extraction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {66:1--66:24},
	year = {2025},
	url = {https://doi.org/10.1145/3715100},
	doi = {10.1145/3715100},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangWRCRR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain contract element extraction (CEE) aims to transfer knowledge from a source domain to facilitate the extraction of legally relevant elements (e.g., contract dates or payments) from contracts in a target domain. To achieve this goal, recent studies encode the domain-invariant relations between elements and legal clause types and enhance performance through bidirectional supervision between the CEE task and the clause classification task. However, two challenges remain unresolved—(i) data sparsity due to expensive annotation costs and a large number of element types, and (ii) label discrepancies among element types across domains, both of which severely impede effective knowledge transfer from the source to the target domain. Recent developments in prompt learning have shown promising performance in low-resource settings. Drawing inspiration from these advances, we propose a novel framework,  graph-enhanced prompt learning  (GEPL), for the cross-domain CEE task to address these challenges. GEPL includes two kinds of prompt: (i) instance-oriented prompts and (ii) label-oriented prompts. Given the input instances, instance-oriented prompts are automatically generated by retrieving relevant examples in the training data, providing auxiliary supervision to enhance the transfer process in low-resource scenarios. To mitigate label discrepancies across different domains, we identify relations among element types using mutual-information criteria and transform these into label-oriented prompt templates. On this basis, a multi-task training strategy is designed to simultaneously optimize the representations of the original input sentence and prompts, enabling GEPL to better understand the tasks and capture label relations in both source and target domains. Empirical results on cross-domain CEE datasets indicate that GEPL significantly outperforms state-of-the-art baselines. Moreover, extensive experiments reveal that GEPL achieves the state-of-the-art performance on cross-domain named entity recognition datasets and demonstrates a high level of generalizability. Our code is released at  https://github.com/WZH-NLP/GEPL .}
}


@article{DBLP:journals/tois/DengLLYLC25,
	author = {Yang Deng and
                  Lizi Liao and
                  Wenqiang Lei and
                  Grace Hui Yang and
                  Wai Lam and
                  Tat{-}Seng Chua},
	title = {Proactive Conversational {AI:} {A} Comprehensive Survey of Advancements
                  and Opportunities},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {67:1--67:45},
	year = {2025},
	url = {https://doi.org/10.1145/3715097},
	doi = {10.1145/3715097},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/DengLLYLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dialogue systems are designed to offer human users social support or functional services through natural language interactions. Traditional conversation research has put significant emphasis on a system’s response-ability, including its capacity to understand dialogue context and generate appropriate responses. However, the key element of proactive behavior—a crucial aspect of intelligent conversations—is often overlooked in these studies. Proactivity empowers conversational agents to lead conversations towards achieving pre-defined targets or fulfilling specific goals on the system side. Proactive dialogue systems are equipped with advanced techniques to handle complex tasks, requiring strategic and motivational interactions, thus representing a significant step towards artificial general intelligence. Motivated by the necessity and challenges of building proactive dialogue systems, we provide a comprehensive review of various prominent problems and advanced designs for implementing proactivity into different types of dialogue systems, including open-domain dialogues, task-oriented dialogues, and information-seeking dialogues. We also discuss real-world challenges that require further research attention to meet application needs in the future, such as proactivity in dialogue systems that are based on large language models, proactivity in hybrid dialogues, evaluation protocols and ethical considerations for proactive dialogue systems. By providing a quick access and overall picture of the proactive dialogue systems domain, we aim to inspire new research directions and stimulate further advancements towards achieving the next level of conversational AI capabilities, paving the way for more dynamic and intelligent interactions within various application domains.}
}


@article{DBLP:journals/tois/WangGZCDZYY25,
	author = {Hao Wang and
                  Bin Guo and
                  Yating Zeng and
                  Mengqi Chen and
                  Yasan Ding and
                  Ying Zhang and
                  Lina Yao and
                  Zhiwen Yu},
	title = {Enabling Harmonious Human-Machine Interaction with Visual-Context
                  Augmented Dialogue System: {A} Review},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {68:1--68:59},
	year = {2025},
	url = {https://doi.org/10.1145/3715098},
	doi = {10.1145/3715098},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangGZCDZYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.}
}


@article{DBLP:journals/tois/YangPFGLYM25,
	author = {Enyue Yang and
                  Weike Pan and
                  Lixin Fan and
                  Hanlin Gu and
                  Zhitao Li and
                  Qiang Yang and
                  Zhong Ming},
	title = {Ownership Verification for Federated Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {69:1--69:27},
	year = {2025},
	url = {https://doi.org/10.1145/3715320},
	doi = {10.1145/3715320},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YangPFGLYM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most federated learning-based recommender systems allow clients to access a well-trained high-quality model locally, which provides adversaries with the opportunity to infringe the legitimate copyright of the model. In response, we study an emerging and important problem, i.e., copyright protection of a federated recommendation model, which has not yet been addressed in the community of federated learning or recommender systems. We propose the first backdoor-based ownership verification scheme for federated recommendation (OVFR), which enables the server to claim its ownership for a given suspicious recommendation model. First, we propose to generate a trigger set tailored to recommendation scenarios. In particular, we generate some fake users and items, and then construct a set of fake users with fake interaction records as a trigger set. Moreover, we ensure that the distribution of the popularity of the fake items follows a long-tailed distribution for the effectiveness of the incorporated watermarking. To provide robustness assurance, we propose two different hybrid strategies to make the embeddings of the fake items similar to those of the real items. Second, we focus on effectively learning from a trigger set for recommendation scenarios. In particular, we design an MSE loss function and a contrastive loss function for incorporating the backdoor-based watermarking into the item embeddings, since the item embeddings are often more valuable and easier to be accessed than other parameters of a federated recommendation model. We then design a contrastive loss function to reduce the risk of the fake items being detected. Extensive experiments on three public datasets show the effectiveness of our OVFR in terms of ownership verification, model performance, and robustness.}
}


@article{DBLP:journals/tois/LinLCZLWLYX25,
	author = {Xixun Lin and
                  Rui Liu and
                  Yanan Cao and
                  Lixin Zou and
                  Qian Li and
                  Yongxuan Wu and
                  Yang Liu and
                  Dawei Yin and
                  Guandong Xu},
	title = {Contrastive Modality-Disentangled Learning for Multimodal Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {70:1--70:31},
	year = {2025},
	url = {https://doi.org/10.1145/3715876},
	doi = {10.1145/3715876},
	timestamp = {Tue, 11 Nov 2025 11:41:35 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LinLCZLWLYX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multimodal recommendation, which utilizes rich multimodal information to learn user preferences, has attracted significant attention. Most works focus on designing powerful encoders for extracting multimodal features, and simply aggregate the learned features together to make prediction. Consequently, they have a limited capacity to learn the inter-modality knowledge including the modality-shared and modality-unique knowledge. In fact, learning the modality-shared knowledge enables us to align cross-modality data for fusing heterogeneous modality features. Learning the modality-unique knowledge is equally important when recommendation tasks only involve a small amount of shared features and the necessary information is contained within specific modality. In this article, we propose Contrastive Modality-Disentangled Learning (CMDL) to overcome this critical limitation. CMDL exactly captures the inter-modality knowledge by achieving modality disentanglement. Specifically, CMDL first disentangles the initial representation into the modality-invariant and modality-specific representations. Afterwards, CMDL introduces a novel manner of contrastive learning to approximate the MI upper bounds for achieving disentanglement regularization. Building upon the proposed regularization, CMDL encourages the modality-invariant and modality-specific representations to capture the modality-shared and modality-unique knowledge respectively and to be statistically independent to each other. Empirically, extensive experiments are conducted on benchmark datasets, demonstrating the superior performance of CMDL compared with strong multimodal recommenders.}
}


@article{DBLP:journals/tois/ZhangZD25,
	author = {Haobo Zhang and
                  Qiannan Zhu and
                  Zhicheng Dou},
	title = {A Unified Prompt-aware Framework for Personalized Search and Explanation
                  Generation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {71:1--71:26},
	year = {2025},
	url = {https://doi.org/10.1145/3716131},
	doi = {10.1145/3716131},
	timestamp = {Sat, 06 Sep 2025 20:29:40 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Product search is crucial for users to find and purchase products they need. Personalized product search, which models users’ search intent and provides tailored results, has become a prominent research problem in industry and academia. Recent studies often leverage knowledge graphs (KGs) to improve search performance and generate explanations for search results. However, existing KG-based methods treat search and explanation tasks separately and explore paths in KGs as explanations, creating a gap between search results and generated explanations. Also, path-formed explanations in KGs are not flexible enough to build correlations with the user’s current query. To address these challenges, we propose P-PEG, a unified prompt-aware framework for personalized product search and explanation generation. P-PEG leverages a pre-trained language model (PLM) and search signal to enhance the generation of user-understandable explanations. We introduce a prompt learning technique and design prompt generators for search and explanation generation tasks based on a fixed PLM. By incorporating search results in explanation-based prompts, we bridge the gap between search results and explanations, facilitating better interaction. Additionally, we utilize the user’s current query, historical search log, and KGs to personalize the explanations and inject task knowledge into PLM. Experimental results show that P-PEG outperforms existing methods in the explanation generation task of the three datasets and the search task of the Electronics dataset, and achieves comparable performance in the search task of the Cellphones & Accessories and CD & Vinyl datasets.}
}


@article{DBLP:journals/tois/MaoWCGWH25,
	author = {Wenyu Mao and
                  Jiancan Wu and
                  Weijian Chen and
                  Chongming Gao and
                  Xiang Wang and
                  Xiangnan He},
	title = {Reinforced Prompt Personalization for Recommendation with Large Language
                  Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {72:1--72:27},
	year = {2025},
	url = {https://doi.org/10.1145/3716320},
	doi = {10.1145/3716320},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/MaoWCGWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Designing effective prompts can empower LLMs to understand user preferences and provide recommendations with intent comprehension and knowledge utilization capabilities. Nevertheless, recent studies predominantly concentrate on task-wise prompting, developing fixed prompt templates shared across all users in a given recommendation task (e.g., rating or ranking). Although convenient, task-wise prompting overlooks individual user differences, leading to inaccurate analysis of user interests. In this work, we introduce the concept of instance-wise prompting, aiming at personalizing discrete prompts for individual users. Toward this end, we propose Reinforced Prompt Personalization (RPP) to realize it automatically. To improve efficiency and quality, RPP personalizes prompts at the sentence level rather than searching in the vast vocabulary word-by-word. Specifically, RPP breaks down the prompt into four patterns, tailoring patterns based on multi-agent and combining them. Then the personalized prompts interact with LLMs (environment) iteratively, to boost LLMs’ recommending performance (reward). In addition to RPP, to improve the scalability of action space, our proposal of RPP+ dynamically refines the selected actions with LLMs throughout the iterative process. Extensive experiments on various datasets demonstrate the superiority of RPP/RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting in LLMs for recommendation. Our code is available at  https://github.com/maowenyu-11/RPP .}
}


@article{DBLP:journals/tois/LiuXCZQZ25,
	author = {Zunlong Liu and
                  Yang Xu and
                  Gao Cong and
                  Lei Zhu and
                  Qinjun Qiu and
                  Huaxiang Zhang},
	title = {{ARTS:} {A} General and Efficient Multi-Task Self-Prompt Framework
                  for Explainable Sequential Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {73:1--73:30},
	year = {2025},
	url = {https://doi.org/10.1145/3717833},
	doi = {10.1145/3717833},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuXCZQZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing sequential recommendations along with easily comprehensible natural language explanations can significantly enhance users’ trust in the recommender systems. However, this approach presents two key challenges: (1) The different objectives of the two tasks make it challenging to achieve joint optimization and mutual enhancement. (2) The simultaneous generation of accurate sequential recommendations and high-quality natural language explanations presents serious challenges to the model’s time and space efficiency. To address these challenges, we propose a general and efficient multi-task self-prompt framework for explainable sequential recommendation (ARTS), which improves collaboration performance and time and space efficiency of multi-task modules based on the generated personalized semantic prompts. Specifically, we propose a self-prompt generator that transfers the user’s global behavior features into the continuous prompt, achieving efficient information sharing among multi-task modules. Additionally, we design a personalized prompt-based short sequence inputs strategy under the pre-training and prompt-tuning paradigm, which achieves mutual enhancement among the multi-task modules and significantly improves the model’s time and space efficiency. Extensive experiments have verified that the proposed ARTS outperforms the state-of-the-art methods in both sequential recommendation and explanation generation tasks. The generality, efficiency and effectiveness of each module of the framework have also been validated through various experiments 1 .}
}


@article{DBLP:journals/tois/ZhangSHZW25,
	author = {Junjie Zhang and
                  Wenqi Sun and
                  Yupeng Hou and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
	title = {Review-Enhanced Universal Sequence Representation Learning for Recommender
                  Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {74:1--74:31},
	year = {2025},
	url = {https://doi.org/10.1145/3717832},
	doi = {10.1145/3717832},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangSHZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread deployment of recommender systems on various online platforms, researchers are striving to develop transferable recommendation algorithms that can effectively adapt to new task scenarios without requiring the re-training of new recommenders. However, there have been challenges in dealing with explicit ID modeling in this context. Recently, researchers have drawn inspiration from the achievements of pre-trained language models (PLMs), making it possible to acquire ID-agnostic representations by utilizing the corresponding texts of items. These representations have shown to be transferable across diverse domains. However, while these methods demonstrate generalization, they are less proficient in making  personalized  recommendations as they learn  universal  representation. In light of this issue, we present a review-enhanced universal sequence representation learning approach named  RUNSRec . Our goal is to not only comprehend universal user behavioral patterns across different domains but also capture their inherent preferences to make recommendations. Our approach makes three technical advancements toward this objective. Firstly, we introduce a lightweight item encoding architecture based on parametric whitening and mixture-of-experts enhanced adapter. It learns discriminative item textual representations by encoding their corresponding identity text and review text, with a discriminative keyword extraction method to enhance the representation identifiability. Secondly, we propose a universal sequence representation learning method that enables the training of transferable recommenders across diverse domains, based on two novel contrastive learning tasks. Furthermore, we introduce a personalized adapter tuning mechanism that enables the universal recommender to capture user personal preferences in a parameter-efficient way. By incorporating universal behavioral patterns learned during the pre-training stage and personalized user tastes captured through adapter tuning, our approach achieves a better balance between generalization and personalization in transferable recommender systems. Extensive experiments conducted on five real-world datasets have demonstrated the effectiveness of our proposed approach.}
}


@article{DBLP:journals/tois/CuiQZWYZ25,
	author = {Jiajun Cui and
                  Hong Qian and
                  Chanjin Zheng and
                  Lu Wang and
                  Mo Yu and
                  Wei Zhang},
	title = {Rebalancing Discriminative Responses for Knowledge Tracing},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {75:1--75:25},
	year = {2025},
	url = {https://doi.org/10.1145/3716821},
	doi = {10.1145/3716821},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/CuiQZWYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Tracing (KT) is a crucial task in computer-aided education and intelligent tutoring systems, predicting students’ performance on new questions from their responses to prior ones. An accurate KT model can capture a student’s mastery level of different knowledge topics, as reflected in their predicted performance on different questions. This helps improve the learning efficiency by suggesting appropriate new questions that complement students’ knowledge states. However, current KT models have significant drawbacks that they neglect the imbalanced discrimination of historical responses. A significant proportion of question responses provide limited information for discerning students’ knowledge mastery, such as those that demonstrate uniform performance across different students. Optimizing the prediction of these cases may increase overall KT accuracy, but also negatively impact the model’s ability to trace personalized knowledge states, especially causing a deceptive surge of performance. Towards this end, we propose a framework to reweight the contribution of different responses based on their discrimination in training. Additionally, we introduce an adaptive predictive score fusion technique to maintain accuracy on less discriminative responses, achieving proper balance between student knowledge mastery and question difficulty. Experimental results demonstrate that our framework enhances the performance of three mainstream KT methods on three widely used datasets.}
}


@article{DBLP:journals/tois/LiSGFY25,
	author = {Dongyang Li and
                  Jianshan Sun and
                  Chongming Gao and
                  Fuli Feng and
                  Kun Yuan},
	title = {Independent or Social Driven Decision? {A} Counterfactual Refinement
                  Strategy for Graph-Based Social Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {76:1--76:27},
	year = {2025},
	url = {https://doi.org/10.1145/3717830},
	doi = {10.1145/3717830},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiSGFY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social recommendation models have traditionally relied on social homophily to enhance user preference prediction by incorporating information from socially connected friends. However, this approach neglects the diverse nature of social relationships. Some individuals with independent personalities often prioritize their own interests over friends’ advice when making purchase decisions. Conversely, those who seek advice from others are more susceptible to social influence. Moreover, the existing methods tend to overlook redundant and noisy social relationships within the network, hindering their ability to achieve accurate recommendations. In response, this article proposes a novel counterfactual method to understand the causal factors driving purchase behaviors, thereby identifying the influence of users’ friends on their purchase decisions. By answering counterfactual questions about the influence of a friend’s purchase behavior on the user’s choices, we develop a causal model to represent social influence in the network. Our proposed refinement strategy, grounded in causal inference, generates counterfactual purchase behavior and guides the refinement of the social graph. Moreover, we present tailored graph refinement methods at various levels, ensuring fine-grained improvements. Experimental results on benchmark data demonstrate that the application of our strategy to different social recommendation models significantly enhances their predictive performance. The source code has been made available on  https://github.com/LDY911/CFRSSR-Code .}
}


@article{DBLP:journals/tois/ZhangZHLJXS25,
	author = {Chengde Zhang and
                  Zihan Zhang and
                  Jiaying Huang and
                  Yan Liu and
                  Dawei Jin and
                  Xia Xiao and
                  Zuwu Shen},
	title = {MKCRec: Meta-relation guided Knowledge Coupling for Paper Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {77:1--77:29},
	year = {2025},
	url = {https://doi.org/10.1145/3715101},
	doi = {10.1145/3715101},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangZHLJXS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the surge of academic papers, it has become a common practice to recommend papers based on authors’ research interests. Existing methods focus on leveraging author–paper research interactions to mine authors’ research interests with coauthorship networks. However, sparse research interactions would pose a huge challenge to distinguish research interests of authors. Fortunately, inter-dependent knowledge across papers provides rich potential heterogeneous connections for author–paper interactions, offering much insights for learning authors’ research interests. Therefore, we propose a meta-relation–guided knowledge coupling approach for paper recommendation. Specifically, we construct a meta-relation–guided heterogeneous graph architecture to depict the numerous inter-dependencies among authors and papers, thereby exploring complex author–paper interactions. First, a meta-relation–aware heterogeneous graph encoder is developed to extract relational structure which maintains the relation-specific representation of authors’ research interest and papers’ research relatedness. Then, a cross-meta-path attention network is designed to aggregate the characteristics of different meta-relations and obtain research features of authors and papers. Finally, a self-supervised data augmentation architecture is constructed to mine and preserve local and global graph structure information, acquiring papers with high relevance to author’s research interests through training loss. Numerous experiments are conducted on two real academic datasets, effectively demonstrating the superiority of our proposed model and validating its effectiveness in paper recommendation.}
}


@article{DBLP:journals/tois/ZhangGLGLCDXW25,
	author = {Gangyi Zhang and
                  Chongming Gao and
                  Wenqiang Lei and
                  Xiaojie Guo and
                  Shijun Li and
                  Hongshen Chen and
                  Zhuozhi Ding and
                  Sulong Xu and
                  Lingfei Wu},
	title = {Vague Preference Policy Learning for Conversational Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {78:1--78:27},
	year = {2025},
	url = {https://doi.org/10.1145/3717831},
	doi = {10.1145/3717831},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangGLGLCDXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational Recommendation Systems (CRS) effectively address information asymmetry by dynamically eliciting user preferences through multi-turn interactions. However, existing CRS methods commonly assume that users have clear, definite preferences for one or multiple target items. This assumption can lead to over-trusting user feedback, treating accepts/rejects as definitive signals to filter items and reduce the candidate space, potentially causing over-filtering and excluding relevant alternatives. In reality, users often exhibit vague preferences, lacking well-defined inclinations for certain attribute types (e.g., color, pattern), and their decision-making process during interactions is rarely binary. Instead, users’ choices are relative, reflecting a range of preferences rather than strict likes or dislikes. To address this issue, we introduce a novel scenario called Vague Preference Multi-Round Conversational Recommendation (VPMCR), which employs a soft estimation mechanism to assign non-zero confidence scores to all candidate items, accommodating users’ vague and dynamic preferences while mitigating over-filtering. In the VPMCR setting, we introduce a solution called Vague Preference Policy Learning (VPPL), which consists of two main components: Ambiguity-Aware Soft Estimation (ASE) and Dynamism-Aware Policy Learning (DPL). ASE aims to accommodate the ambiguity in user preferences by estimating preference scores for both directed and inferred preferences, employing a choice-based approach and a time-aware preference decay strategy. DPL implements a policy learning framework, leveraging the preference distribution from ASE, to guide the conversation and adapt to changes in users’ preferences for making recommendations or querying attributes. Extensive experiments conducted on diverse datasets demonstrate the effectiveness of VPPL within the VPMCR framework, outperforming existing methods and setting a new benchmark for CRS research. Our work represents a significant advancement in accommodating the inherent ambiguity and relative decision-making processes exhibited by users, improving the overall performance and applicability of CRS in real-world settings.}
}


@article{DBLP:journals/tois/ZhangLZ25,
	author = {Jinyu Zhang and
                  Chao Li and
                  Zhongying Zhao},
	title = {Lightweight yet Efficient: An External Attentive Graph Convolutional
                  Network with Positional Prompts for Sequential Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {79:1--79:25},
	year = {2025},
	url = {https://doi.org/10.1145/3719343},
	doi = {10.1145/3719343},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-Based Sequential Recommender Systems (GSRSs)  have gained significant research attention due to their ability to simultaneously handle user–item interactions and sequential relationships between items. Current GSRSs often utilize composite or in-depth structures for graph encoding (e.g., the Graph Transformer). Nevertheless, they have high computational complexity, hindering the deployment on resource-constrained edge devices. Moreover, the relative position encoding in Graph Transformer has difficulty in considering the complicated positional dependencies within sequence. To this end, we propose an  External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation (EA-GPS) . Specifically, we first introduce an external attentive graph convolutional network that linearly measures the global associations among nodes via two external memory units. Then, we present a positional prompt-based decoder that explicitly treats the absolute item positions as external prompts. By introducing length-adaptive sequential masking and a soft attention network, such a decoder facilitates the model to capture the long-term positional dependencies and contextual relationships within sequences. Extensive experimental results on five real-world datasets demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods. Remarkably, it achieves the superior performance while maintaining a smaller parameter size and lower training overhead. The implementation of this work is publicly available at  https://github.com/ZZY-GraphMiningLab/EA-GPS .}
}


@article{DBLP:journals/tois/ZhaoYLZGW25,
	author = {Chu Zhao and
                  Enneng Yang and
                  Yuliang Liang and
                  Jianzhe Zhao and
                  Guibing Guo and
                  Xingwei Wang},
	title = {Symmetric Graph Contrastive Learning against Noisy Views for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {80:1--80:28},
	year = {2025},
	url = {https://doi.org/10.1145/3722103},
	doi = {10.1145/3722103},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhaoYLZGW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Contrastive Learning (GCL) leverages data augmentation techniques to produce contrasting views, enhancing the accuracy of recommendation systems through learning the consistency between contrastive views. However, existing augmentation methods, such as directly perturbing interaction graph (e.g., node/edge dropout), may interfere with the original connections and generate poor contrasting views, resulting in sub-optimal performance. In this article, we define the views that share only a small amount of information with the original graph due to poor data augmentation as noisy views (i.e., the last 20% of the views with a cosine similarity value less than 0.1 to the original view). We demonstrate through detailed experiments that noisy views will significantly degrade recommendation performance. Further, we propose a model-agnostic Symmetric Graph Contrastive Learning (SGCL) method with theoretical guarantees to address this issue. Specifically, we introduce symmetry theory into graph contrastive learning, based on which we propose a symmetric form and contrast loss resistant to noisy interference. We provide theoretical proof that our proposed SGCL method has a high tolerance to noisy views. Further demonstration is given by conducting extensive experiments on three real-world datasets. The experimental results demonstrate that our approach substantially increases recommendation accuracy, with relative improvements reaching as high as 12.25% over nine other competing models. These results highlight the efficacy of our method. The code is available at  https://github.com/user683/SGCL .}
}


@article{DBLP:journals/tois/BronHFS25,
	author = {Michiel P. Bron and
                  Peter G. M. van der Heijden and
                  Ad Feelders and
                  Arno Siebes},
	title = {Using Chao's Estimator as a Stopping Criterion for Technology-Assisted
                  Review},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {81:1--81:51},
	year = {2025},
	url = {https://doi.org/10.1145/3724116},
	doi = {10.1145/3724116},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/BronHFS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technology-Assisted Review aims to reduce the human effort required for screening processes such as abstract screening for Systematic Literature Reviews. Human reviewers label documents as relevant or irrelevant during this process, while the system incrementally updates a prediction model based on the reviewers’ previous decisions. After each model update, the system proposes new documents it deems relevant, to prioritize relevant documents over irrelevant ones. A stopping criterion is necessary to guide users in stopping the review process to minimize the number of missed relevant documents and the number of read irrelevant documents. In this article, we propose and evaluate a new ensemble-based Active Learning strategy and a stopping criterion based on Chao’s Population Size Estimator that estimates the prevalence of relevant documents in the dataset. Our simulation study demonstrates that this criterion performs well on several datasets and is compared to other methods presented in the literature.}
}


@article{DBLP:journals/tois/SuJLYEGZLZ25,
	author = {Yixin Su and
                  Wei Jiang and
                  Fangquan Lin and
                  Cheng Yang and
                  Sarah M. Erfani and
                  Junhao Gan and
                  Yunxiang Zhao and
                  Ruixuan Li and
                  Rui Zhang},
	title = {Intrinsic and Extrinsic Factor Disentanglement for Recommendation
                  in Various Context Scenarios},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {82:1--82:32},
	year = {2025},
	url = {https://doi.org/10.1145/3722553},
	doi = {10.1145/3722553},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/SuJLYEGZLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommender systems, the patterns of user behaviors (e.g., purchase, click) may vary greatly in different contexts (e.g., time and location). This is because user behavior is jointly determined by two types of factors:  intrinsic factors , which reflect consistent user preference, and  extrinsic factors , which reflect external incentives that may vary in different contexts. Differentiating between intrinsic and extrinsic factors helps learn user behaviors better. However, existing studies have only considered differentiating them from a single, pre-defined context (e.g., time or location), ignoring the fact that a user’s extrinsic factors may be influenced by the interplay of various contexts at the same time. In this article, we propose the intrinsic-extrinsic disentangled recommendation (IEDR) model, a generic framework that differentiates intrinsic from extrinsic factors considering various contexts simultaneously, enabling more accurate differentiation of factors and hence the improvement of recommendation accuracy. IEDR contains a context-invariant contrastive learning component to capture intrinsic factors, and a disentanglement component to extract extrinsic factors under the interplay of various contexts. The two components work together to achieve effective factor learning. Extensive experiments on real-world datasets demonstrate IEDR’s effectiveness in learning disentangled factors and significantly improving recommendation accuracy by up to 4% in NDCG.}
}


@article{DBLP:journals/tois/LiJZZZZD25,
	author = {Xiaoxi Li and
                  Jiajie Jin and
                  Yujia Zhou and
                  Yuyao Zhang and
                  Peitian Zhang and
                  Yutao Zhu and
                  Zhicheng Dou},
	title = {From Matching to Generation: {A} Survey on Generative Information
                  Retrieval},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {3},
	pages = {83:1--83:62},
	year = {2025},
	url = {https://doi.org/10.1145/3722552},
	doi = {10.1145/3722552},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiJZZZZD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information Retrieval (IR) systems are crucial tools for users to access information, which have long been dominated by traditional methods relying on similarity matching. With the advancement of pre-trained language models, Generative Information Retrieval (GenIR) emerges as a novel paradigm, attracting increasing attention. Based on the form of information provided to users, current research in GenIR can be categorized into two aspects: (1)  Generative Retrieval  ( GR ) leverages the generative model’s parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. (2)  Reliable Response Generation  employs language models to directly generate information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching while offering flexibility, efficiency, and creativity to meet practical needs. This article aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training and structure, document identifier, incremental learning, and so on, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, and so on. We also review the evaluation, challenges, and future developments in GenIR systems. This review aims to offer a comprehensive reference for researchers, encouraging further development in the GenIR field (Github Repository:  https://github.com/RUC-NLPIR/GenIR-Survey ).}
}


@article{DBLP:journals/tois/LiuDNCTXW25,
	author = {Jiongnan Liu and
                  Zhicheng Dou and
                  Jian{-}Yun Nie and
                  Zhenlin Chen and
                  Guoyu Tang and
                  Sulong Xu and
                  Ji{-}Rong Wen},
	title = {Enhancing Sequential Personalized Product Search with External Out-of-sequence
                  Knowledge},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {84:1--84:25},
	year = {2025},
	url = {https://doi.org/10.1145/3726864},
	doi = {10.1145/3726864},
	timestamp = {Tue, 03 Feb 2026 11:25:30 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiuDNCTXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A key challenge in personalized product search is to capture user’s preferences. Recent work attempted to model sequences of user historical behaviors, i.e., product purchase histories, to build user profiles and to personalize results accordingly. Although these approaches have demonstrated promising retrieval performances, we notice that most of them focus solely on the intra-sequence interactions between items. However, as there is usually a small amount of historical behavior data, the user profiles learned by these approaches could be very sensitive to the noise included in it. To tackle this problem, we propose incorporating out-of-sequence external information to enhance user modeling. More specifically, we inject the external item–item relations (e.g., belonging to the same brand), and query–query relations (e.g., the semantic similarities between them), into the intra-sequence interaction to learn better user profiles. In addition, we devise two auxiliary decoders, with the historical item sequence reconstruction task and the global item similarity prediction task, to further improve the reliability of user modeling. Experimental results on two datasets from simulated and real user search logs respectively show that the proposed personalized product search method outperforms existing approaches.}
}


@article{DBLP:journals/tois/XuWLZG25,
	author = {Nuo Xu and
                  Pinghui Wang and
                  Zi Liang and
                  Junzhou Zhao and
                  Xiaohong Guan},
	title = {How Vital Is the Jurisprudential Relevance: Law Article-Intervened
                  Legal Case Retrieval and Matching},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {85:1--85:32},
	year = {2025},
	url = {https://doi.org/10.1145/3725729},
	doi = {10.1145/3725729},
	timestamp = {Wed, 17 Dec 2025 16:01:33 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuWLZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Legal case retrieval aims to automatically scour comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this article, we propose an end-to-end model named  LCM-LAI  to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction sub-task, without any additional assumptions in inference. In addition, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on the law distribution, which is more effective than semantic similarity. We perform a series of exhaustive experiments that include two different tasks that involving four real-world datasets. The results demonstrate that LCM-LAI achieves state-of-the-art performance.}
}


@article{DBLP:journals/tois/WangWHQ25,
	author = {Huili Wang and
                  Chuhan Wu and
                  Yongfeng Huang and
                  Tao Qi},
	title = {Learning Human Feedback from Large Language Models for Content Quality-aware
                  Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {86:1--86:28},
	year = {2025},
	url = {https://doi.org/10.1145/3727144},
	doi = {10.1145/3727144},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangWHQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems are widely employed to mitigate information overload by tailoring online content to individual preferences. Existing recommendation methods typically focus on optimizing the relevance between candidate item content and user historical behaviors. However, these methods often neglect the quality of recommended content, which can negatively affect user experience and hinder the long-term growth of platforms. In fact, addressing this issue is particularly challenging, as signal on content quality feedback is typically sparse in the user interaction data (e.g., clicks) commonly used for model training. In this article, we propose a human feedback alignment framework for recommender system (HFAR), which leverages well-aligned large language models to simulate human feedback on content quality to enhance recommendation. Specifically, we propose a multi-task learning-based knowledge transfer framework to infuse recommendation models with an awareness of fine-grained feedback on content quality from targeted perspectives. Furthermore, we develop a contrastive learning-based feedback integration mechanism to embed targeted human feedback into the ranking strategy to enable quality-aware recommendation decision-making. Besides, we propose a multi-objective joint training framework to optimize the model jointly under utility and quality objectives. Experiments show that HFAR achieves a maximum improvement of 84.78% in recommendation quality, while maintaining both recommendation accuracy and efficiency.}
}


@article{DBLP:journals/tois/RuanLCFZCCC25,
	author = {Shulan Ruan and
                  Huijie Liu and
                  Zhao Chen and
                  Bin Feng and
                  Kun Zhang and
                  Caleb Chen Cao and
                  Enhong Chen and
                  Lei Chen},
	title = {{CPWS:} Confident Programmatic Weak Supervision for High-Quality Data
                  Labeling},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {87:1--87:26},
	year = {2025},
	url = {https://doi.org/10.1145/3725730},
	doi = {10.1145/3725730},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/RuanLCFZCCC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmatic Weak Supervision (PWS) is a recent data labeling paradigm, which employs several Labeling Functions (LFs) to provide weak labels and involves a Label Model (LM) for label aggregation. Despite the significant progress, there still remain some inherent challenges in PWS. From the view of labeling, LFs may wrongly label some data points. From the view of data, some data points themselves may be low-quality (e.g., ambiguous texts or blurred images). These largely stem from the lack of an explicit evaluation mechanism for LFs or data points. To this end, inspired by confident learning focusing on label quality, we propose a Confident PWS (CPWS) approach for high-quality data labeling. Specifically, several LFs are firstly utilized to provide weak labels for unlabeled data. Then, we develop an explicit Dual Evaluation Mechanism (DEM) to evaluate the quality of both LFs and data points, which not only employs data to evaluate trained models but also leverages trained models to evaluate data. Along this line, we further design a Distribution-Guided Pruning Strategy (DPS) to prune low-quality data and aggregate weak labels under the guidance of label class distribution. Extensive experiments on various benchmark datasets demonstrate the effectiveness and generalization ability of our proposed approach.}
}


@article{DBLP:journals/tois/HuWTHWZZZL25,
	author = {Shirui Hu and
                  Weichang Wu and
                  Zuoli Tang and
                  Zhaoxin Huan and
                  Lin Wang and
                  Xiaolu Zhang and
                  Jun Zhou and
                  Lixin Zou and
                  Chenliang Li},
	title = {{HORAE:} Temporal Multi-Interest Pre-training for Sequential Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {88:1--88:29},
	year = {2025},
	url = {https://doi.org/10.1145/3727645},
	doi = {10.1145/3727645},
	timestamp = {Tue, 07 Oct 2025 08:47:51 +0200},
	biburl = {https://dblp.org/rec/journals/tois/HuWTHWZZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The data sparsity problem has been a long-standing obstacle towards achieving better recommendation performance since it is miserable to estimate the user’s interests from limited historical behaviors. The pre-training paradigm, i.e., learning universal knowledge across a wide spectrum of domains, has increasingly become a new de-facto practice in many fields, especially for adaption to new domains. The merit of this superior generalizability renders it a natural choice to tackle the data sparsity problem for various recommendation scenarios. Hence, several efforts mainly follow masked language modeling or simple data augmentation via contrastive learning to build a pre-trained recommendation model. Our recent work (namely  Miracle ) suggests that the common treatment utilizing the masked language modeling is not sufficient for pre-training a recommender system, since a user’s intent could be more complex than predicting the next word or item. The encouraging results demonstrate that the multi-interest modeling could significantly push the frontier of recommender system pre-training. Nevertheless, how to accommodate the temporal dynamics of the user interests seems to be underexplored under both single vector representation and multi-interest schemes. In this article, we aim to incorporate sophisticated temporal information modeling with the current advance in this line. More specifically, we extend  Miracle  by further considering relative position information and two kinds of relative time interval information jointly when performing multi-interest learning. Then, a sequential process for interest refinement is proposed to learn the subtle nuances of how interests change and shift along the timeline, leading to a more precise representation of user interests. Our extensive experiments on multiple real-world datasets validate the effectiveness of the proposed solution, demonstrating a significant improvement over current state-of-the-art models on these benchmarks. The code is available at  https://github.com/WHUIR/Horae .}
}


@article{DBLP:journals/tois/WangQHDHH25,
	author = {Jinguang Wang and
                  Shengsheng Qian and
                  Jun Hu and
                  Wenxiang Dong and
                  Xudong Huang and
                  Richang Hong},
	title = {End-to-End Explainable Fake News Detection Via Evidence-Claim Variational
                  Causal Inference},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {89:1--89:26},
	year = {2025},
	url = {https://doi.org/10.1145/3728462},
	doi = {10.1145/3728462},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangQHDHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explainable Fake News Detection (EFND) is a new challenge that aims to verify news authenticity and provide clear explanations for its decisions. Traditional EFND methods often treat the tasks of classification and explanation as separate, ignoring the fact that explanation content can assist in enhancing fake news detection. To overcome this gap, we present a new solution: the End-to-End Explainable Fake News Detection Network ( EExpFND ). Our model includes an evidence-claim variational causal inference component, which not only utilizes explanation content to improve fake news detection but also employs a variational approach to address the distributional bias between the ground truth explanation in the training set and the prediction explanation in the test set. Additionally, we incorporate a masked attention network to detail the nuanced relationships between evidence and claims. Our comprehensive tests across two public datasets show that  EExpFND  sets a new benchmark in performance. The code is available at  https://anonymous.4open.science/r/EExpFND-F5C6 .}
}


@article{DBLP:journals/tois/WangZWMZZYR25,
	author = {Shuliang Wang and
                  Jiabao Zhu and
                  Yi Wang and
                  Chen Ma and
                  Xin Zhao and
                  Yansen Zhang and
                  Ziqiang Yuan and
                  Sijie Ruan},
	title = {Hierarchical Gating Network for Cross-Domain Sequential Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {90:1--90:32},
	year = {2025},
	url = {https://doi.org/10.1145/3715321},
	doi = {10.1145/3715321},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangZWMZZYR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain sequential recommendation (CDSR) utilizes data from multiple domains to recommend the user’s next interaction based on his latest interaction sequence. Currently, many cross-domain sequential recommendation algorithms have been proven to achieve good recommendation performance. However, these algorithms overlook the influence of users’ long-term behavioral patterns and general interests when extracting their current preferences. In this article, we propose a Hierarchical Gating Network for Cross-Domain Sequential Recommendation (HGNCDSR). Specifically, we simultaneously train single-domain and cross-domain interaction sequences, utilizing a hierarchical gating network to capture user interest representations in single-domain and cross-domain, respectively. A feature gating and an instance gating are applied respectively to extract user interests at item feature level and instance level. While learning current preferences from behavior sequences, user representations that reflect behavioral patterns and general interests are simultaneously learned and strengthened. Additionally, we employ the item–item product to model the relationships between candidate items and those in the interaction sequence. Both current interests and item relevance are considered simultaneously, integrating single-domain and cross-domain user preferences to predict the user’s next interaction. We design extensive experiments to show that HGNCDSR has better recommendation performance than other state-of-the-art models.}
}


@article{DBLP:journals/tois/QiaoZWGLCL25,
	author = {Shutong Qiao and
                  Wei Zhou and
                  Junhao Wen and
                  Chen Gao and
                  Qun Luo and
                  Peixuan Chen and
                  Yong Li},
	title = {Multi-view Intent Learning and Alignment with Large Language Models
                  for Session-based Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {91:1--91:25},
	year = {2025},
	url = {https://doi.org/10.1145/3719344},
	doi = {10.1145/3719344},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/QiaoZWGLCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-based recommendation (SBR) methods often rely on user behavior data, which can struggle with the sparsity of session data, limiting performance. Researchers have identified that beyond behavioral signals, rich semantic information in item descriptions is crucial for capturing hidden user intent. While Large Language Models (LLMs) offer new ways to leverage this semantic data, the challenges of session anonymity, short-sequence nature, and high LLM training costs have hindered the development of a lightweight, efficient LLM framework for SBR. To address the above challenges, we propose an LLM-enhanced SBR framework that integrates semantic and behavioral signals from multiple views. This two-stage framework leverages the strengths of both LLMs and traditional SBR models while minimizing training costs. In the first stage, we use multi-view prompts to infer latent user intentions at the session semantic level, supported by an intent localization module to alleviate LLM hallucinations. In the second stage, we align and unify these semantic inferences with behavioral representations, effectively merging insights from both large and small models. Extensive experiments on two real datasets demonstrate that the LLM4SBR framework can effectively improve model performance. We release our codes along with the baselines at  https://github.com/tsinghua-fib-lab/LLM4SBR .}
}


@article{DBLP:journals/tois/SritrakoolMT25,
	author = {Nakarin Sritrakool and
                  Saranya Maneeroj and
                  Atsuhiro Takasu},
	title = {{QUADEN:} Discovering Latent Neighbors for Sparse Users and Items
                  across Interaction Quadrants in Recommender System},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {92:1--92:33},
	year = {2025},
	url = {https://doi.org/10.1145/3725886},
	doi = {10.1145/3725886},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/SritrakoolMT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many recommender systems leverage Graph Neural Networks to capture user–item relations for delivering recommendations. However, the representation of nodes heavily relies on neighbors, causing limited neighbor nodes (sparse nodes) to lack expressive representation. Existing works discovered latent neighbors for sparse nodes but ignored node sparsity, resulting in the  neighbor misallocation problem  due to overlooking quality and quantity aspects. We propose discovering high-quality latent neighbors by progressively transferring knowledge from dense nodes by categorizing user–item interactions into four quadrants (dense user–dense item, dense user–sparse item, sparse user–dense item, and sparse user–sparse item). We leverage the node sparsity to determine the optimal quantity of latent neighbors. We propose a Domain Adaptation Network for transferring knowledge from dense to sparse quadrants without encountering the  domain misalignment problem  arising from the distinct representations between dense and sparse quadrants. An Enrichment Network is proposed to address the  inexpressive representation problem  due to limited observed interactions by enriching the sparse node representation. A Heterogeneous Graph Neural Network architecture is proposed to capture multiple relations between dense/sparse users and items. Experimental results on three benchmark datasets demonstrate the superiority of the proposed method over Graph Neural Network baselines, both with and without latent neighbors.}
}


@article{DBLP:journals/tois/JianWXSHW25,
	author = {Meng Jian and
                  Tuo Wang and
                  Zhuoyang Xia and
                  Ge Shi and
                  Richang Hong and
                  Lifang Wu},
	title = {Geometric-Augmented Self-Distillation for Graph-Based Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {93:1--93:23},
	year = {2025},
	url = {https://doi.org/10.1145/3729223},
	doi = {10.1145/3729223},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/JianWXSHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prevalent recommendation techniques explore the graph structure of interactions to alleviate the interaction sparsity issue for inferring users’ interests. These graph models focus on extracting local structural signals to model users’ interests, introducing grid-like distortion and ignoring the hierarchical tree-like structure when learning from the interaction graph. The learned interests lack significant hierarchical signals, resulting in suboptimal recommendation performance. In this article, we investigate geometric-augmented graph learning with hyperbolic and Euclidean geometries to delve into local structural and hierarchical knowledge from the interaction graph. A self-teaching network called geometric-augmented self-distillation (GASD) is proposed to transfer hierarchical knowledge from hyperbolic to Euclidean space. The transfer learning enables shrinking of the network into a primary student to implement effective and efficient inference in Euclidean space, preventing computational burden in hyperbolic space. Experiments on publicly available datasets demonstrate that the proposed GASD outperforms the state-of-the-art models, verifying the effectiveness and efficiency of knowledge transfer by self-distillation to aggregate knowledge adaptively for personalized recommendation.}
}


@article{DBLP:journals/tois/QinJGQXZ25,
	author = {Yifang Qin and
                  Wei Ju and
                  Yiyang Gu and
                  Ziyue Qiao and
                  Zhiping Xiao and
                  Ming Zhang},
	title = {PolyCF: Towards Optimal Spectral Graph Filters for Collaborative Filtering},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {94:1--94:28},
	year = {2025},
	url = {https://doi.org/10.1145/3728464},
	doi = {10.1145/3728464},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/QinJGQXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative Filtering (CF) is a pivotal research area in recommender systems that capitalizes on collaborative similarities between users and items to provide personalized recommendations. With the remarkable achievements of node embedding-based Graph Neural Networks (GNNs), we explore the upper bounds of expressiveness inherent to embedding-based methodologies and tackle the challenges by reframing the CF task as a graph-signal processing problem. To this end, we propose PolyCF, a flexible graph signal filter that leverages polynomial graph filters to process interaction signals. PolyCF exhibits the capability to capture spectral features across multiple eigenspaces through a series of Generalized Gram filters and is able to approximate the optimal polynomial response function for recovering missing interactions. A graph optimization objective and a pairwise ranking objective are jointly used to optimize the parameters of the convolution kernel. Experiments on three widely adopted datasets demonstrate the superiority of PolyCF over the state-of-the-art CF methods.}
}


@article{DBLP:journals/tois/YuanZCZCWLZ25,
	author = {Meng Yuan and
                  Zhao Zhang and
                  Wei Chen and
                  Chu Zhao and
                  Tong Cai and
                  Deqing Wang and
                  Rui Liu and
                  Fuzhen Zhuang},
	title = {{HEK-CL:} Hierarchical Enhanced Knowledge-Aware Contrastive Learning
                  for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {95:1--95:27},
	year = {2025},
	url = {https://doi.org/10.1145/3728463},
	doi = {10.1145/3728463},
	timestamp = {Tue, 11 Nov 2025 11:42:43 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YuanZCZCWLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, there has been an emergence of self-supervised recommendation methods that integrate knowledge graphs. Upon conducting a comprehensive review of contrastive learning (CL) in recommender systems, we conclude that existing methods solely focus on data view generation (the first phase) while neglecting the equally pivotal data view alignment (the second phase). However, due to the complexity and variability of real-world graph data, regardless of the graph augmentation strategy employed, it may be unrealistic to expect all entities to benefit from CL. In this article, we propose a  H ierarchical  E nhanced  K nowledge-Aware  C ontrastive  L earning (HEK-CL) method for recommendation. Overall, we aim to hierarchically carry out enhancement strategies in both the first and second phases of knowledge-aware CL: (1) From the perspective of enhancing data view generation, we focus on combining non-Euclidean representation learning with graph denoising modules. Owing to the unified space’s ability to learn the ideal curvature from data distributions, the quality of embeddings for graph data has seen enhancements; (2) From the perspective of enhancing data view alignment, we propose a hyperbolic robust contrastive loss, named HRCL. Through rigorous theoretical analysis and experiments, we demonstrate that HRCL provides a more balanced and equitable training process for all entities than InfoNCE. Numerous experiments on the three real-world datasets show that our HEK-CL outperforms state-of-the-art baselines.}
}


@article{DBLP:journals/tois/HuangLLYLX25,
	author = {Xu Huang and
                  Jianxun Lian and
                  Yuxuan Lei and
                  Jing Yao and
                  Defu Lian and
                  Xing Xie},
	title = {Recommender {AI} Agent: Integrating Large Language Models for Interactive
                  Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {96:1--96:33},
	year = {2025},
	url = {https://doi.org/10.1145/3731446},
	doi = {10.1145/3731446},
	timestamp = {Mon, 19 Jan 2026 18:37:55 +0100},
	biburl = {https://dblp.org/rec/journals/tois/HuangLLYLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender models capture ever-changing user preferences by training with in-domain user behavior data. These models are typically lightweight, facilitating real-time and large-scale online services. However, these models often falter when tasked with providing more sophisticated functionalities, such as offering explanations or engaging in conversations. Recently, large language models (LLMs) have emerged as a significant advancement towards artificial general intelligence, demonstrating impressive capabilities in instruction comprehension, reasoning, and human interaction. Unfortunately, LLMs lack the understanding of domain-specific item catalogs and behavioral patterns, especially in areas that deviate from general world knowledge, such as online e-commerce. This limitation makes them unsuitable to function as recommender models directly. In this article, we bridge the gap between recommender models and LLMs, combining their respective strengths to create an interactive recommender system. We present an efficient framework, termed as  InteRecAgent , which utilizes LLMs as the brain and recommender models as instrumental tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. To overcome specific challenges associated with LLM-based agents for recommender systems, we enhance three core components, covering memory mechanism, task planning, and tool learning abilities. The InteRecAgent empowers traditional recommender systems, like ID-based matrix factorization models, to evolve into versatile and interactive systems with a natural language interface through the integration of LLMs. Experimental results derived from three public datasets demonstrate that the InteRecAgent delivers strong performance as a conversational recommender system, surpassing general LLMs such as GPT-4.}
}


@article{DBLP:journals/tois/AdaliaSMZ25,
	author = {Ramon Ad{\`{a}}lia and
                  Gemma Sanjuan and
                  Tom{\`{a}}s Margalef and
                  Ismael Zamora},
	title = {The LambdaGap Framework for Precision-Oriented Ranking},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {97:1--97:39},
	year = {2025},
	url = {https://doi.org/10.1145/3733235},
	doi = {10.1145/3733235},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/AdaliaSMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LambdaRank has proven effective for optimizing information retrieval metrics such as Normalized Discounted Cumulative Gain (NDCG). However, its application to Precision at document  k  (P@ k ) poses significant challenges because of the metric’s unique definition, which heavily restricts the number of effective training document pairs. This limitation diminishes the learning signal for relevant documents beyond the top k, potentially resulting in suboptimal performance. To overcome this, we propose LambdaGap, a ranking algorithm inspired by LambdaRank specifically tailored for optimizing P@ k . LambdaGap replaces the pairwise weighting scheme in LambdaRank by one where pairs of documents within k positions in the ranking are masked out. We establish a theoretical link between LambdaGap and P@ k  by identifying the implicit metric optimized by the model. Furthermore, we introduce a new metric, Average Relevance Position beyond document  k , which can be used in conjunction with LambdaRank to indirectly optimize for P@ k . Our extensive experiments on publicly available datasets demonstrate the effectiveness of the proposed methods, yielding statistically significant improvements in P@ k  performance and highlighting their potential for more efficient training.}
}


@article{DBLP:journals/tois/ChuSHGZZZ25,
	author = {Yuxing Chu and
                  Mingxu Sun and
                  Ke Huang and
                  Haokun Geng and
                  Yanyu Zhang and
                  Lili Zhang and
                  Menghua Zhang},
	title = {Personality Dialogue Agent Based on Personality Description and Conversation
                  History},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {98:1--98:29},
	year = {2025},
	url = {https://doi.org/10.1145/3731679},
	doi = {10.1145/3731679},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ChuSHGZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the study of dialogue system, personalized dialogue mainly focuses on the semantic matching degree between response and role. However, various factors, such as semantic style, dialogue noise, and sparse personality information, can affect the performance of the model. For this purpose, we construct a novel personalized dialogue model. Based on the personality description information and conversation history, it uses the information enhancement algorithm to cluster the personality description text into a number of fine sparse categories, and uses the feature classifier to precisely select the features highly relevant to the current dialogue situation according to the input query content. At the same time, the history information selector will fine-filter the conversation history, retaining the parts that are valuable for generating replies. We combine the processed personality description text with the filtered historical context information, and send it to the decoder through the feature cue learning strategy for deep processing to generate personalized responses. Our model integrates the proposed algorithms, classifiers, selectors, and feature cue learning strategies to build a complete dialogue system. Experiments on two datasets show that our model is superior to other models in terms of consistency and coherence.}
}


@article{DBLP:journals/tois/LiYGJZW25,
	author = {Fei Li and
                  Enneng Yang and
                  Guibing Guo and
                  Linying Jiang and
                  Jianzhe Zhao and
                  Xingwei Wang},
	title = {Preference Logical Reasoning with Preference Operators for Explainable
                  Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {99:1--99:30},
	year = {2025},
	url = {https://doi.org/10.1145/3733596},
	doi = {10.1145/3733596},
	timestamp = {Sat, 13 Sep 2025 17:49:10 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiYGJZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Preference logical reasoning utilizes user-item interactions (e.g., ratings and reviews) to infer user preferences and discover user decision paths from the knowledge graph to enhance the explainability of item recommendations. However, existing algorithms assume that the ratings and reviews of any item are always consistent, ignoring situations where items with high ratings have negative reviews or items with low ratings but positive reviews. This leads to inaccurate learning of user preferences. In fact, through experimental analysis of two real datasets, we found that on average, about 10% of the interactive data exhibited this inconsistency, that is, items with high ratings but negative reviews appear in the recommendation list. To address this issue, we propose a general preference logical reasoning method based on preference operators. Specifically, we capture the semantic information of users toward the item (its corresponding attributes) in reviews and define two preference operators ( like  and  dislike ) for the item to correct ambiguous neutral ratings or false ratings that do not reflect true preferences. In the process of preference path reasoning, the  like  preference operator increases the occurrence probability of liked items, while the  dislike  preference operator reduces the occurrence probability of disliked items. By fusing the preference operators in the preference path, we obtain consistent user preferences and enhance the explainability of item recommendations. The experimental results on four real datasets demonstrate that our method can effectively improve the performance of all comparison baselines in terms of recommendation accuracy and user decision explainability.}
}


@article{DBLP:journals/tois/WangCZGGLGZ25,
	author = {Tairan Wang and
                  Xiuying Chen and
                  Qingqing Zhu and
                  Taicheng Guo and
                  Shen Gao and
                  Zhiyong Lu and
                  Xin Gao and
                  Xiangliang Zhang},
	title = {New Paradigm for Evaluating Scholar Summaries: {A} Facet-aware Metric
                  and a Meta-evaluation Benchmark},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {100:1--100:25},
	year = {2025},
	url = {https://doi.org/10.1145/3733597},
	doi = {10.1145/3733597},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangCZGGLGZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Evaluation of summary quality is particularly crucial within the scientific domain, because it facilitates efficient knowledge dissemination and automated scientific information retrieval. This article presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods. These methods, including  n -gram overlap calculations, embedding comparisons, verification, and QA-based approaches, often fall short in providing explanations, grasping scientific concepts, or identifying key content. Correspondingly, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different facets. The  facet granularity  is tailored to the structure of scientific abstracts, offering an integrated evaluation approach that is not fragmented, while also providing fine-grained interpretability. Recognizing the absence of an evaluation benchmark in the scientific domain, we curate a Scientific abstract summary evaluation Dataset (ScholarSum) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. We hope our benchmark inspires better evaluation metrics and future enhancements to LLMs:  https://github.com/iriscxy/ScholarSum .}
}


@article{DBLP:journals/tois/YuZLZL25,
	author = {Qing Yu and
                  Lixin Zou and
                  Xiangyang Luo and
                  Xiangyu Zhao and
                  Chenliang Li},
	title = {Uniform Graph Pre-training and Prompting for Transferable Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {101:1--101:26},
	year = {2025},
	url = {https://doi.org/10.1145/3724392},
	doi = {10.1145/3724392},
	timestamp = {Tue, 03 Feb 2026 12:19:20 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YuZLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the paradigm of pre-training and fine-tuning has achieved impressive performance owing to their ability to transfer general knowledge from pre-trained domain to target domain. Meanwhile,  graph neural networks (GNNs)  have gained prominence in recommender systems. However, there is a lack of unified pre-training and fine-tuning paradigms in graph-based recommendation systems. Applying pre-training and fine-tuning in graph-based recommendation is challenging due to the unique characteristics of recommendation data, including the non-uniform representation, negative transfer effects, and skewed data distributions. To overcome these challenges, we introduce  pre-training and prompting recommendation ( ProRec ) , a novel model that synergizes uniform graph pre-training with prompt-tuning for recommendation systems. Specifically, to address the challenge of inconsistent features across different recommendation datasets,  ProRec  constructs unified input features at the subgraph level and uses a graph auto-encoder for pre-training, laying the foundation for uniform knowledge transfer from the pre-trained domain to the downstream domain. Additionally,  ProRec  employs prompt-tuning during the fine-tuning phase, which, in a parameter-efficient manner, enhances the generalization of pre-trained knowledge to downstream tasks thereby reducing negative transfer effects. Furthermore, a cross-layer contrastive learning strategy is adopted to eliminate uneven data distribution, promoting more evenly distributed and informative representations. Finally, extensive benchmark comparisons have demonstrated that  ProRec  outperforms the latest state-of-the-art methods. The source code necessary for replication is available at  https://github.com/Code2Q/ProRec .}
}


@article{DBLP:journals/tois/LiuHALWLS25,
	author = {Bulou Liu and
                  Yiran Hu and
                  Qingyao Ai and
                  Yiqun Liu and
                  Yueyue Wu and
                  Chenliang Li and
                  Weixing Shen},
	title = {Generating Clarifying Questions for Conversational Legal Case Retrieval
                  without External Knowledge},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {102:1--102:26},
	year = {2025},
	url = {https://doi.org/10.1145/3736161},
	doi = {10.1145/3736161},
	timestamp = {Tue, 07 Oct 2025 08:47:51 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LiuHALWLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In legal case retrieval, existing work has shown that human-mediated conversational search can improve users’ search experience. One of the key problems for a practical conversational search system is how to ask high-quality clarifying questions to initiate conversations with users and understand their search intents. Previous works demonstrated that human-annotated external domain knowledge (such as event schemas) can improve the legal utility of clarifying questions generated by large language models. However, these methods are restricted to specific law systems or languages and cannot be generalized to others. To this end, we propose to generate context and domain-specific questions with LLMs without external annotations or knowledge by extracting information from top-retrieved documents given the current conversation context. Specifically, we construct a conversational legal case retrieval system CARQ that iteratively selects neighbor candidate case documents from the retrieved list at each conversation step to ask clarifying questions. We pretrain CARQ to capture the differences between legal cases and employ the reward augmented maximum likelihood to optimize the system directly for retrieval metrics. Extensive automated and human evaluations on three widely adopted legal case retrieval datasets demonstrate the superior effectiveness of our approach as compared with the state-of-the-art baselines.}
}


@article{DBLP:journals/tois/LuGYCHZ25,
	author = {Ziang Lu and
                  Lei Guo and
                  Xu Yu and
                  Zhiyong Cheng and
                  Xiaohui Han and
                  Lei Zhu},
	title = {Federated Semantic Learning for Privacy-preserving Cross-domain Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {103:1--103:27},
	year = {2025},
	url = {https://doi.org/10.1145/3728359},
	doi = {10.1145/3728359},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LuGYCHZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the evolving landscape of recommender systems, the challenge of effectively conducting privacy-preserving Cross-domain Recommendation, especially under strict non-overlapping constraints, has emerged as a key focus. Despite extensive research has made significant progress, several limitations still exist: (1) Previous semantic-based methods fail to deeply exploit rich textual information, since they quantize the text into codes, losing its original rich semantics. (2) The current solution solely relies on the text-modality, while the synergistic effects with the ID-modality are ignored. (3) Existing studies do not consider the impact of irrelevant semantic features, leading to inaccurate semantic representation. To address these challenges, we introduce federated semantic learning and devise FFMSR as our solution. For Limitation 1, we locally learn items’ semantic encodings from their original texts by a multi-layer semantic encoder and then cluster them on the server to facilitate the transfer of semantic knowledge between domains. To tackle Limitation 2, we integrate both ID and Text modalities on the clients, and utilize them to learn different aspects of items. To handle Limitation 3, a Fast Fourier Transform-based filter and a gating mechanism are developed to alleviate the impact of irrelevant semantic information in the local model. We conduct extensive experiments on two real-world datasets, and the results demonstrate the superiority of our FFMSR method over other SOTA methods. Our source codes are publicly available at  https://github.com/Sapphire-star/FFMSR .}
}


@article{DBLP:journals/tois/BaiHYYHZS25,
	author = {Ting Bai and
                  Le Huang and
                  Yue Yu and
                  Cheng Yang and
                  Cheng Hou and
                  Zhe Zhao and
                  Chuan Shi},
	title = {Efficient Multi-task Prompt Tuning for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {104:1--104:21},
	year = {2025},
	url = {https://doi.org/10.1145/3736403},
	doi = {10.1145/3736403},
	timestamp = {Mon, 08 Dec 2025 16:35:44 +0100},
	biburl = {https://dblp.org/rec/journals/tois/BaiHYYHZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the expansion of business scenarios, real recommender systems are facing challenges in dealing with the constantly emerging new tasks in multi-task learning frameworks. In this article, we attempt to improve the generalization ability of multi-task recommendations when dealing with new tasks. A novel two-stage prompt-tuning MTL framework (MPT-Rec) is proposed to address task irrelevance and training efficiency problems in multi-task recommender systems. Specifically, we disentangle the task-specific and task-sharing information in the multi-task pre-training stage and then use task-aware prompts to transfer knowledge from other tasks to the new task effectively. By freezing parameters in the pre-training tasks, MPT-Rec solves the negative impacts that may be brought by the new task and greatly reduces the training costs. Extensive experiments on three real-world datasets show the effectiveness of our proposed multi-task learning framework. MPT-Rec achieves the best performance compared to the SOTA multi-task learning method on three real-world datasets. Besides, it maintains comparable model performance but vastly improves the training efficiency (i.e., with up to 10% parameters in the full-training way) in the new task learning. Our code is publicly available at  https://github.com/BAI-LAB/MPT-Rec .}
}


@article{DBLP:journals/tois/LuccheseNOPV25,
	author = {Claudio Lucchese and
                  Franco Maria Nardini and
                  Salvatore Orlando and
                  Raffaele Perego and
                  Alberto Veneri},
	title = {Explainable, Effective, and Efficient Learning-to-Rank Models Using
                  {ILMART}},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {105:1--105:37},
	year = {2025},
	url = {https://doi.org/10.1145/3733232},
	doi = {10.1145/3733232},
	timestamp = {Sat, 06 Sep 2025 20:29:39 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LuccheseNOPV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning ranking models that are both explainable and effective is an emerging topic within the research area of explainable AI. Several Learning-to-Rank (LtR) algorithms have been recently proposed that build models that are simple to explain and, at the same time, almost as effective as their state-of-the-art,  black-box  counterparts. In this work, we propose Interpretable LambdaMART (ILMART), a novel framework with different strategies to constrain the state-of-the-art LtR LambdaMART algorithm to generate interpretable models, i.e., ensembles whose trees can use either single features (main effects) or a limited number of interacting features (interaction effects). ILMART facilitates a straightforward tradeoff between model explainability and effectiveness by precisely tuning the quantity of main and interaction effects during the learning phase. We show that slightly increasing their number allows ILMART models to reach ranking performances at par with full-complexity LambdaMART ones. Furthermore, reproducible experiments conducted on publicly available LtR datasets demonstrate that ILMART can improve nDCG@10 by up to 10% compared to state-of-the-art competitors while preserving an explainable structure. Finally, we explore the relationship between model explainability and inference efficiency by introducing a novel and easy-to-implement scoring algorithm for ILMART ranking models, achieving up to a  100 ×  speedup compared to the baseline.}
}


@article{DBLP:journals/tois/MengAAAR25,
	author = {Chuan Meng and
                  Negar Arabzadeh and
                  Arian Askari and
                  Mohammad Aliannejadi and
                  Maarten de Rijke},
	title = {Query Performance Prediction Using Relevance Judgments Generated by
                  Large Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {106:1--106:35},
	year = {2025},
	url = {https://doi.org/10.1145/3736402},
	doi = {10.1145/3736402},
	timestamp = {Sat, 06 Sep 2025 20:29:39 +0200},
	biburl = {https://dblp.org/rec/journals/tois/MengAAAR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically  gen erated  re levance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of predicting the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels. This also allows us to interpret predicted IR evaluation measures, and identify, track, and rectify errors in generated relevance judgments to improve QPP quality. We predict an item’s relevance by using  open source  large language models (LLMs) to ensure scientific reproducibility. We face two main challenges: (i) excessive computational costs of judging an entire corpus for predicting a metric considering recall, and (ii) limited performance in prompting open source LLMs in a zero-/few-shot manner. To solve the challenges, we devise an approximation strategy to predict an IR measure considering recall and propose to fine-tune open source LLMs using human-labeled relevance judgments. Experiments on the TREC 2019–2022 deep learning tracks and CAsT-19–20 datasets show that QPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural rankers.}
}


@article{DBLP:journals/tois/ZhaoTZL25,
	author = {Chuang Zhao and
                  Hui Tang and
                  Hongke Zhao and
                  Xiaomeng Li},
	title = {Beyond Sequential Patterns: Rethinking Healthcare Predictions with
                  Contextual Insights},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {107:1--107:32},
	year = {2025},
	url = {https://doi.org/10.1145/3733234},
	doi = {10.1145/3733234},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhaoTZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Healthcare predictions, such as readmission prediction, stand as a cornerstone of societal well-being, exerting a profound influence on individual health outcomes and communal vitality. Existing research primarily employs advanced graph neural networks and sequential algorithms for patient modeling, with a focus on discerning the connections and sequential patterns inherent in Electronic Health Records (EHRs). However, the heterogeneity of entity interactions, the locality of EHR data, and the oversight of target relevance hinder further improvements. To address these limitations, we introduce a novel framework  B eyond  S equential  P atterns (BSP), which facilitates precise healthcare predictions by incorporating tri-contextual information. Specifically, we establish a symptom-driven hypergraph network with four semantic hyperedges tailored to the intricacies of the healthcare scenario, such as ontology. This serves as a global context, tracking the heterogeneous entity collaboration within and across patients. Moreover, we construct an extensive knowledge graph leveraging existing medical databases and large language models. By sampling and refining knowledge subgraphs as local context, we bolster the semantic associations of medical entities from closed-set EHR data to the open world. Finally, we introduce the candidate context, an explicit entity-relation loss. It enforces the neighbor consistency between the target and the representation during optimization, thus accounting for correlations among targets. Extensive experiments and rigorous robustness analysis on five tasks derived from four large medical datasets underscore the BSP’s superiority over the leading baselines, with improvements of 11%, 3%, 11%, 3.5%, and 2% across five tasks, demonstrating the efficacy of incorporating diverse contexts.}
}


@article{DBLP:journals/tois/LiangCYY25,
	author = {Xurong Liang and
                  Tong Chen and
                  Wei Yuan and
                  Hongzhi Yin},
	title = {Lightweight Embeddings with Graph Rewiring for Collaborative Filtering},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {108:1--108:29},
	year = {2025},
	url = {https://doi.org/10.1145/3742424},
	doi = {10.1145/3742424},
	timestamp = {Sat, 01 Nov 2025 21:58:19 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiangCYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GNN-based recommender systems have become increasingly popular in academia and industry due to their ability to capture high-order information from user-item interaction graphs. However, as recommendation services scale rapidly and their deployment now commonly involves resource-constrained edge devices, GNN-based models face significant challenges, including high embedding storage costs and run-time latency from graph propagations. Our previous work, LEGCF, effectively reduced embedding storage costs but struggled to maintain recommendation performance under stricter storage limits. Additionally, LEGCF did not address the extensive run-time computation costs associated with graph propagation, which involves heavy multiplication and accumulation operations (MACs). These challenges consequently hinder effective training and inference on resource-constrained edge devices. To address these limitations, we propose Lightweight Embeddings with Rewired Graph (LERG) for Graph Collaborative Filtering, an improved extension of LEGCF. LERG retains LEGCF’s compositional codebook structure but introduces quantization techniques to reduce the storage cost of embedding weights, enabling the inclusion of more meta-embeddings within the same storage constraints for improved model expressiveness. To optimize graph propagation for edge devices, we pretrain the quantized compositional embedding table using the full interaction graph on resource-rich servers, after which a fine-tuning stage is engaged to identify and prune low-contribution entities via a gradient-free binary integer programming approach, constructing a rewired graph that excludes these entities (i.e., user/item nodes) from propagating signals. The quantized compositional embedding table with selective embedding participation and sparse rewired graph is transferred to edge devices which significantly reduce computation memory and inference time. Experiments on three public benchmark datasets, including an industry-scale dataset, demonstrate that LERG achieves superior recommendation performance while dramatically reducing storage and computation costs for graph-based recommendation services.}
}


@article{DBLP:journals/tois/XiangZDYT25,
	author = {Zongyi Xiang and
                  Yan Zhang and
                  Lixin Duan and
                  Hongzhi Yin and
                  Ivor W. Tsang},
	title = {Coherence-guided Preference Disentanglement for Cross-domain Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {109:1--109:28},
	year = {2025},
	url = {https://doi.org/10.1145/3742855},
	doi = {10.1145/3742855},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/XiangZDYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discovering user preferences across different domains is pivotal in cross-domain recommendation systems, particularly when platforms lack comprehensive user-item interactive data. The limited presence of shared users often hampers the effective modeling of common preferences. While leveraging shared items’ attributes, such as category and popularity, can enhance cross-domain recommendation performance, the scarcity of shared items between domains has limited research in this area. To address this, we propose a Coherence-guided Preference Disentanglement (CoPD) method aimed at improving cross-domain recommendation by (i) explicitly extracting shared item attributes to guide the learning of shared user preferences and (ii) disentangling these preferences to identify specific user interests transferred between domains. CoPD introduces coherence constraints on item embeddings of shared and specific domains, aiding in extracting shared attributes. Moreover, it utilizes these attributes to guide the disentanglement of user preferences into separate embeddings for interest and conformity through a popularity-weighted loss. Experiments conducted on real-world datasets demonstrate the superior performance of our proposed CoPD over existing competitive baselines, highlighting its effectiveness in enhancing cross-domain recommendation performance. The code is available at  https://github.com/XiangZongyi/CoPD .}
}


@article{DBLP:journals/tois/YanCLLH25,
	author = {Mingshi Yan and
                  Zhiyong Cheng and
                  Fan Liu and
                  Yingda Lyu and
                  Yahong Han},
	title = {User Invariant Preference Learning for Multi-Behavior Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {4},
	pages = {110:1--110:24},
	year = {2025},
	url = {https://doi.org/10.1145/3728465},
	doi = {10.1145/3728465},
	timestamp = {Thu, 11 Sep 2025 20:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YanCLLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-behavior recommendation scenarios, analyzing users’ diverse behaviors, such as  click ,  purchase , and  rating , enables a more comprehensive understanding of their interests, facilitating personalized and accurate recommendations. A fundamental assumption of multi-behavior recommendation methods is the existence of shared user preferences across behaviors, representing users’ intrinsic interests. Based on this assumption, existing approaches aim to integrate information from various behaviors to enrich user representations. However, they often overlook the presence of both commonalities and individualities in users’ multi-behavior preferences. These individualities reflect distinct aspects of preferences captured by different behaviors, where certain auxiliary behaviors may introduce noise, hindering the prediction of the target behavior. To address this issue, we propose a user invariant preference learning (UIPL) for multi-behavior recommendation, aiming to capture users’ intrinsic interests (referred to as invariant preferences) from multi-behavior interactions to mitigate the introduction of noise. Specifically, UIPL leverages the paradigm of invariant risk minimization to learn invariant preferences. To implement this, we employ a variational autoencoder (VAE) to extract users’ invariant preferences, replacing the standard reconstruction loss with an invariant risk minimization constraint. Additionally, we construct distinct environments by combining multi-behavior data to enhance robustness in learning these preferences. Finally, the learned invariant preferences are used to provide recommendations for the target behavior. Extensive experiments on four real-world datasets demonstrate that UIPL significantly outperforms current state-of-the-art methods.}
}


@article{DBLP:journals/tois/ChenGDLJLW25,
	author = {Lei Chen and
                  Chen Gao and
                  Xiaoyi Du and
                  Hengliang Luo and
                  Depeng Jin and
                  Yong Li and
                  Meng Wang},
	title = {Enhancing ID-based Recommendation with Large Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {112:1--112:30},
	year = {2025},
	url = {https://doi.org/10.1145/3704263},
	doi = {10.1145/3704263},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ChenGDLJLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have recently garnered significant attention in various domains, including recommendation systems. Recent research leverages the capabilities of LLMs to improve the performance and user modeling aspects of recommender systems. These studies primarily focus on utilizing LLMs to interpret textual data in recommendation tasks. However, it's worth noting that in ID-based recommendations, textual data is absent, and only ID data is available. The untapped potential of LLMs for ID data within the ID-based recommendation paradigm remains relatively unexplored. To this end, we introduce a pioneering approach called “LLM for ID-based recommendation” (LLM4IDRec). This innovative approach integrates the capabilities of LLMs while exclusively relying on ID data, thus diverging from the previous reliance on textual data. The basic idea of LLM4IDRec is that by employing LLM to augment ID data, if augmented ID data can improve recommendation performance, it demonstrates the ability of LLM to interpret ID data effectively, exploring an innovative way for the integration of LLM in ID-based recommendation. Specifically, we first define a prompt template to enhance LLM's ability to comprehend ID data and the ID-based recommendation task. Next, during the process of generating training data using this prompt template, we develop two efficient methods to capture both the local and global structure of ID data. We feed this generated training data into the LLM and employ LoRA for fine-tuning LLM. Following the fine-tuning phase, we utilize the fine-tuned LLM to generate ID data that aligns with users’ preferences. We design two filtering strategies to eliminate invalid generated data. Thirdly, we can merge the original ID data with the generated ID data, creating augmented data. Finally, we input this augmented data into the existing ID-based recommendation models without any modifications to the recommendation model itself. We evaluate the effectiveness of our LLM4IDRec approach using three widely used datasets. Our results demonstrate a notable improvement in recommendation performance, with our approach consistently outperforming existing methods in ID-based recommendation by solely augmenting input data.}
}


@article{DBLP:journals/tois/LuoHZSQHZYLXZS25,
	author = {Sichun Luo and
                  Bowei He and
                  Haohan Zhao and
                  Wei Shao and
                  Yanlin Qi and
                  Yinya Huang and
                  Aojun Zhou and
                  Yuxuan Yao and
                  Zongpeng Li and
                  Yuanzhang Xiao and
                  Mingjie Zhan and
                  Linqi Song},
	title = {RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k
                  Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {113:1--113:31},
	year = {2025},
	url = {https://doi.org/10.1145/3705728},
	doi = {10.1145/3705728},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LuoHZSQHZYLXZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Prior research has employed specialized  prompts  to leverage the in-context learning capabilities of LLMs for recommendation purposes. More recent studies have utilized instruction tuning techniques to align LLMs with human preferences, promising more effective recommendations. However, existing methods suffer from several limitations. The full potential of LLMs is not fully elicited due to low-quality tuning data and the overlooked integration of conventional recommender signals. Furthermore, LLMs may generate inconsistent responses for different ranking tasks in the recommendation, potentially leading to unreliable results. In this article, we introduce Ranker for top- k  Recommendations (RecRanker), tailored for instruction tuning LLMs to serve as the Ranker for top- k  Recommendations. Specifically, we introduce importance-aware sampling, clustering-based sampling, and penalty for repetitive sampling for sampling high-quality, representative, and diverse training data. To enhance the prompt, we introduce a position shifting strategy to mitigate position bias and augment the prompt with auxiliary information from conventional recommendation models, thereby enriching the contextual understanding of the LLM. Subsequently, we utilize the sampled data to assemble an instruction-tuning dataset with the augmented prompts comprising three distinct ranking tasks: pointwise, pairwise, and listwise rankings. We further propose a hybrid ranking method to enhance the model performance by ensembling these ranking tasks. Our empirical evaluations demonstrate the effectiveness of our proposed  RecRanker  in both direct and sequential recommendation scenarios. 1}
}


@article{DBLP:journals/tois/ZhangXHZLW25,
	author = {Junjie Zhang and
                  Ruobing Xie and
                  Yupeng Hou and
                  Xin Zhao and
                  Leyu Lin and
                  Ji{-}Rong Wen},
	title = {Recommendation as Instruction Following: {A} Large Language Model
                  Empowered Recommendation Approach},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {114:1--114:37},
	year = {2025},
	url = {https://doi.org/10.1145/3708882},
	doi = {10.1145/3708882},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhangXHZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past few decades, recommender systems have attracted much attention in both research and industry communities. Existing recommendation models mainly learn the underlying user preference from historical behavior data (typically in the forms of item IDs), and then estimate the user–item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we develop a different recommendation paradigm, considering recommendation as  instruction following  by LLMs. The key idea is that the needs of a user can be expressed in natural language descriptions (called  instructions ), so that LLMs can understand and further execute the instruction for fulfilling the recommendation. For this purpose, we instruction tune the 3B Flan-T5-XL, to better adapt LLMs to recommender systems. We first design a general instruction format for describing the preference, intention, and task form of a user in natural language. Then we manually design 39 instruction templates and automatically generate large amounts of user-personalized instruction data with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instructions into several widely studied recommendation (or search) tasks, and conduct extensive experiments with real-world datasets. Experiment results show that our approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing user-friendly recommender systems, in which users can freely communicate with the system and obtain accurate recommendations  via  natural language instructions.}
}


@article{DBLP:journals/tois/DongHCWW25,
	author = {Zhiang Dong and
                  Liya Hu and
                  Jingyuan Chen and
                  Zhihua Wang and
                  Fei Wu},
	title = {Comprehend Then Predict: Prompting Large Language Models for Recommendation
                  with Semantic and Collaborative Data},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {115:1--115:26},
	year = {2025},
	url = {https://doi.org/10.1145/3716499},
	doi = {10.1145/3716499},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/DongHCWW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems primarily utilize user–item interactions (i.e., collaborative information) and auxiliary textual information (i.e., semantic information) to infer user preferences and provide recommendations. With the advancement in large language models (LLMs), attempts have been made to incorporate their remarkable language comprehension capabilities into recommendation tasks. However, existing LLM4Rec methods face challenges in seamlessly integrating both collaborative and semantic information, as there is an inherent gap between these two types of data. Moreover, these methods struggle to capture the fine-grained distinctions in user preferences, which are essential in recommendation tasks, due to the loss design of LLMs. To address these issues, we propose a multi-stage prompt-tuning method for leveraging pre-trained LLMs in various recommendation tasks, named SCRec. Specifically, SCRec leverages Semantic and Collaborative information as supervision signals in two distinct stages: the semantic prompt-tuning stage and the collaborative prompt-tuning stage. This method breaks down user and item representations into semantic and collaborative perspectives, enabling a pre-trained LLM to first deduce the qualitative preferences of users over items from semantic information, and then generate quantitative recommendations from collaborative information. In addition, we propose a meta-mapping approach to provide personalized mapping functions for encoding collaborative information and integrate a novel numeric-informed head based on MSE loss for LLM in the second stage, which helps to better capture fine-grained distinctions in user preferences. Experiments on three public datasets for rating prediction and top-N recommendation tasks demonstrate that our method surpasses both conventional and LLM-based techniques, showing the strength of sequentially merging semantic and collaborative information in recommendation tasks.}
}


@article{DBLP:journals/tois/YangFOHCYLN25,
	author = {Qi Yang and
                  Aleksandr Farseev and
                  Marlo Ongpin and
                  Alfred Huang and
                  Yu{-}Yi Chu{-}Farseeva and
                  Da{-}Min You and
                  Kirill Lepikhin and
                  Sergey I. Nikolenko},
	title = {Fusing Predictive and Large Language Models for Actionable Recommendations
                  in Creative Marketing},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {116:1--116:31},
	year = {2025},
	url = {https://doi.org/10.1145/3725885},
	doi = {10.1145/3725885},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/YangFOHCYLN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The opaqueness of modern digital advertising, exemplified by large platforms such as  Meta Ads , raises concerns regarding their control over audience targeting, pricing structures, and ad relevancy assessments. Locked in place by network effects, these natural monopolies attract countless advertisers who rely on subjective intuition, with billions of dollars lost on ineffective social media advertisements. The platforms’ algorithms rely on huge amounts of data unavailable to advertisers, and the algorithms themselves are opaque too, so advertisers often cannot make informed decisions. To promote transparency and help individual advertisers, we first propose novel ways to optimize advertising strategies, predicting click-through rates of novel advertising content based on the content itself. However, advertisers face both opaqueness and a vast abundance of data: a large platform has so many competitor ads that it is hard to derive meaningful insights. Drawing inspiration from the success of Large Language Models (LLM), we propose a system that merges multimodal LLMs and pretrained AI models with an emphasis on digital marketing and advertising data analysis. Leveraging the capabilities of LLMs and incorporating explainability features, including modern text-image models, we aim to improve efficiency and produce synergy between human marketers and AI systems.}
}


@article{DBLP:journals/tois/FuLWWDZZGT25,
	author = {Zichuan Fu and
                  Xiangyang Li and
                  Chuhan Wu and
                  Yichao Wang and
                  Kuicai Dong and
                  Xiangyu Zhao and
                  Mengchen Zhao and
                  Huifeng Guo and
                  Ruiming Tang},
	title = {A Unified Framework for Multi-Domain {CTR} Prediction via Large Language
                  Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {117:1--117:33},
	year = {2025},
	url = {https://doi.org/10.1145/3698878},
	doi = {10.1145/3698878},
	timestamp = {Tue, 18 Nov 2025 15:39:38 +0100},
	biburl = {https://dblp.org/rec/journals/tois/FuLWWDZZGT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-Domain Click-Through Rate (MDCTR) prediction is crucial for online recommendation platforms, which involves providing personalized recommendation services to users in different domains. However, current MDCTR models are confronted with the following limitations. Firstly, due to varying data sparsity in different domains, models can easily be dominated by some specific domains, which leads to significant performance degradation in other domains (i.e., the “seesaw phenomenon”). Secondly, when new domain emerges, the scalability of existing methods is limited, making it difficult to adapt to the dynamic growth of the domain. Traditional MDCTR models usually use one-hot encoding for semantic information such as product titles, thus losing rich semantic information and leading to insufficient generalization of the model. In this article, we propose a novel solution Uni-CTR to address these challenges. Uni-CTR leverages Large Language Model (LLM) to extract layer-wise semantic representations that capture domain commonalities, mitigating the seesaw phenomenon and enhancing generalization. Besides, it incorporates a pluggable domain-specific network to capture domain characteristics, ensuring scalability to dynamic domain growth. Experimental results on public datasets and industrial scenarios show that Uni-CTR significantly outperforms state-of-the-art (SOTA) models. In addition, Uni-CTR shows significant results in zero shot prediction. Code is available at Applied Machine Learning Lab (Pytorch), GitHub (Pytorch) and Gitee (MindSpore).}
}


@article{DBLP:journals/tois/TangHLZHFZZL25,
	author = {Zuoli Tang and
                  Zhaoxin Huan and
                  Zihao Li and
                  Xiaolu Zhang and
                  Jun Hu and
                  Chilin Fu and
                  Jun Zhou and
                  Lixin Zou and
                  Chenliang Li},
	title = {One Model for All: Large Language Models Are Domain-Agnostic Recommendation
                  Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {118:1--118:27},
	year = {2025},
	url = {https://doi.org/10.1145/3705727},
	doi = {10.1145/3705727},
	timestamp = {Wed, 08 Oct 2025 07:50:17 +0200},
	biburl = {https://dblp.org/rec/journals/tois/TangHLZHFZZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation systems aim to predict users’ next likely interaction based on their history. However, these systems face data sparsity and cold-start problems. Utilizing data from other domains, known as multi-domain methods, is useful for alleviating these problems. However, traditional multi-domain methods rely on meaningless ID-based item representation, which makes it difficult to align items with similar meanings from different domains, yielding sup-optimal knowledge transfer. This article introduces  LLM-Rec , a framework that utilizes pre-trained Large Language Models (LLMs) for domain-agnostic recommendation. Specifically, we mix user’s behaviors from multiple domains and concatenate item titles into a sentence, then use LLMs for generating user and item representations. By mixing behaviors across different domains, we can exploit the knowledge encoded in LLMs to bridge the semantic across over multi-domain behaviors, thus obtaining semantically rich representations and improving performance in all domains. Furthermore, we explore the underlying reasons why LLMs are effective and investigate whether LLMs can understand the semantic correlations as the recommendation model, and if advanced techniques like scaling laws in NLP also work in recommendations. We conduct extensive experiments with LLMs ranging from 40 M to 6.7 B to answer the above questions and to verify the effectiveness of  LLM-Rec  in multi-domain recommendation. The source code is available at  https://github.com/WHUIR/LLMRec .}
}


@article{DBLP:journals/tois/HeLHXLL25,
	author = {Xi He and
                  Yilin Liu and
                  Weikang He and
                  Xin Xing and
                  Xingyu Lu and
                  Yanbing Liu},
	title = {{TCKT:} Tree-Based Cross-domain Knowledge Transfer for Next {POI}
                  Cold-Start Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {119:1--119:31},
	year = {2025},
	url = {https://doi.org/10.1145/3709137},
	doi = {10.1145/3709137},
	timestamp = {Thu, 20 Nov 2025 14:38:53 +0100},
	biburl = {https://dblp.org/rec/journals/tois/HeLHXLL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next point of interest (POI) recommendation task recommends POIs to users that they may be interested in next time based on their historical trajectories. This task holds value for both users and businesses. However, it has consistently faced the issue of cold-start caused by sparse user check-in data. Existing research mainly focuses on knowledge transfer among cities within the same data source, but these data are very rare. The abundance of available third-party data presents opportunities to improve cold-start performance, but it is not easy. This third-party data contain numerous entities, such as POIs and users, which have different representations and distributions across different data domains, making knowledge transfer difficult. To address these challenges, we propose the Tree-Based Cross-domain Knowledge Transfer (TCKT) model. First, we construct a multi-granularity Geographical Frequency Tree (GF-Tree), transforming the POI recommendation problem into a path generation problem. Second, we design a pre-training model to mine general user behavior patterns and spatio-temporal features among POIs from large-scale third-party data. Finally, we propose a dual-channel domain adaptation model to facilitate cross-domain knowledge transfer and improve cold-start performance. Experimental results on three public datasets demonstrate that our method outperforms state-of-the-art (SOTA) baseline methods.}
}


@article{DBLP:journals/tois/XinSWX25,
	author = {Haoran Xin and
                  Ying Sun and
                  Chao Wang and
                  Hui Xiong},
	title = {{LLMCDSR:} Enhancing Cross-Domain Sequential Recommendation with Large
                  Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {120:1--120:33},
	year = {2025},
	url = {https://doi.org/10.1145/3715099},
	doi = {10.1145/3715099},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/XinSWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-Domain Sequential Recommendation (CDSR) aims to predict users’ preferences based on historical sequential interactions across multiple domains. Existing works focus on the overlapped users who interact in multiple domains to capture the cross-domain correlations. These methods often underperform in practical scenarios featuring both overlapped and non-overlapped users due to the limited cross-domain interactions and knowledge transfer misalignment for non-overlapped users. To address this, we leverage Large Language Models (LLMs) to facilitate CDSR by fully exploiting single-domain interactions. However, LLMs exhibit inherent limitations in handling extensive item repositories and sequential collaborative signals. Moreover, the generation reliability is compromised by the hallucination problem, potentially causing noisy and unstable outputs. To this end, we propose a novel  LLMCDSR  framework, which employs LLMs to predict unobserved cross-domain interactions, termed pseudo items, within single-domain interactions. Specifically, we first prompt LLMs to execute the Candidate-Free Cross-Domain Interaction Generation task. Then, we devise a Collaborative-Textual Contrastive Pre-Training strategy, learning to infuse collaborative information into textual features. Afterwards, we present a novel Relevance-Aware Meta Recall Network (RMRN) to selectively identify and retrieve high-quality pseudo items from the dataset, where the parameters are optimized in a meta-learning manner. Finally, extensive experiments on two public datasets validate the effectiveness of LLMCDSR in enhancing CDSR. The code and data are available at  https://github.com/xhran2010/LLMCDSR .}
}


@article{DBLP:journals/tois/SangWZW25,
	author = {Lei Sang and
                  Yu Wang and
                  Yiwen Zhang and
                  Xindong Wu},
	title = {Denoising Heterogeneous Graph Pre-training Framework for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {121:1--121:31},
	year = {2025},
	url = {https://doi.org/10.1145/3706632},
	doi = {10.1145/3706632},
	timestamp = {Tue, 14 Oct 2025 19:49:15 +0200},
	biburl = {https://dblp.org/rec/journals/tois/SangWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous graph neural networks (HGNN)  have exhibited significant performance gains by modeling the information propagation process in graph-structured data for recommender systems. However, existing HGNN-based Recommendation still face two challenges: (1) They overlook the rich semantics brought by the combination of different meta-paths, making it difficult to capture the importance of various meta-paths; (2) when HGNN use meta-paths to capture high-order information, they are susceptible to noise data, as noise from connected nodes can create cumulative effects on a target node in the graph. To tackle these issues, we propose a new model called the  Denoising Heterogeneous Graph Pre-training Framework (DHGPF)  to enhance recommendation tasks. This framework has two stages: pre-training and training. In the pre-training stage, we assign learnable weights to different meta-paths and use a simplified multi-layer graph convolution network to automatically aggregate semantic information from different meta-path combinations. This approach can capture the importance of these paths. The training stage focuses on reducing noise using gating mechanism and denoising structure learning methods. These methods accomplish the denoising process through information filtering. Our model was evaluated on three real-world datasets, demonstrating that DHGPF outperforms other state-of-the-art recommendation methods. We have further organized the source code of the article at  https://github.com/wangyu0627/DHGPF .}
}


@article{DBLP:journals/tois/LaiLYXIYZDDT25,
	author = {Hanyu Lai and
                  Xiao Liu and
                  Hao Yu and
                  Yifan Xu and
                  Iat Long Iong and
                  Shuntian Yao and
                  Aohan Zeng and
                  Zhengxiao Du and
                  Yuxiao Dong and
                  Jie Tang},
	title = {WebGLM: Towards an Efficient and Reliable Web-Enhanced Question-Answering
                  System},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {122:1--122:43},
	year = {2025},
	url = {https://doi.org/10.1145/3729421},
	doi = {10.1145/3729421},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LaiLYXIYZDDT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present WebGLM, an enhanced Large Language Model (LLM)-based retrieval question-answering system based on the ChatGLM3-6B, offering significant improvements over previous systems. We aim to augment a pre-trained LLM with web search and reliable retrieval capabilities while being efficient for real-world deployments. Leveraging LLM’s in-context learning ability and a robust filter strategy, we create a high-quality training dataset and address the hallucination issue with a self-check mechanism. Our base model, ChatGLM3-6B, excels in extracting critical information and generating desired responses. We tackle the decline in retrieval effectiveness for complex queries with a keywording technique and incorporate more web content for references. We align with user preferences by training a human preference-aware scorer and employing DPO training for direct alignment. Extensive experiments, including human evaluations and the Turing test, demonstrate WebGLM’s superior performance against leading web-enhanced question-answering systems, significantly enhancing performance and efficiency. The code, demo, and data are at  https://github.com/THUDM/WebGLM .}
}


@article{DBLP:journals/tois/XiaoZCZ25,
	author = {Likang Xiao and
                  Richong Zhang and
                  Junfan Chen and
                  Lei Zhang},
	title = {Including Co-Relation via Concatenate Operator for Static and Temporal
                  Knowledge Graph Embedding},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {123:1--123:26},
	year = {2025},
	url = {https://doi.org/10.1145/3733231},
	doi = {10.1145/3733231},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/XiaoZCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph Completion (KGC) aims to complete KGs by predicting missing entities. A common solution for KGC is Knowledge Graph Embedding (KGE), which assumes that semantical similar entities or relationships should possess similar representations in high-dimensional space. In KGE, a heuristic score function of the head entity and its relation with different operators is required. A typical technique is regularization for tensor factorization, such as the Nuclear-p norm and the Frobenius norm of the query/entity embedding, which significantly improve the KGE model performance on the KGC task. However, the  Co-Relation s, including the association between tail entities ( Co-Query Relation ) and the association between queries ( Co-Entity Relation ), desirable for KGC are not fully considered in existing embedding regularization techniques. In this article, we theoretically interpret the role of Co-Relation in KGE and propose a novel  ConR  regularization approach to learn embedding that takes Co-Relations into account. Extensive experiments show that our model improves static and temporal KGC tasks over decomposition-based models, ComplEx and TuckER. Further analysis of the score cumulative distribution function and embedding visualization demonstrates the effectiveness of\xa0 ConR .}
}


@article{DBLP:journals/tois/JingLCYLJW25,
	author = {Erkang Jing and
                  Yezheng Liu and
                  Yidong Chai and
                  Shuo Yu and
                  Longshun Liu and
                  Yuanchun Jiang and
                  Yang Wang},
	title = {Emotion-aware Personalized Music Recommendation with a Heterogeneity-aware
                  Deep Bayesian Network},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {124:1--124:43},
	year = {2025},
	url = {https://doi.org/10.1145/3733233},
	doi = {10.1145/3733233},
	timestamp = {Thu, 16 Oct 2025 17:11:54 +0200},
	biburl = {https://dblp.org/rec/journals/tois/JingLCYLJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Music recommender systems play a critical role in music streaming platforms by providing users with music that they are likely to enjoy. Recent studies have shown that user emotions can influence users’ preferences for music moods. However, existing emotion-aware music recommender systems (EMRSs) explicitly or implicitly assume that users’ actual emotional states expressed through identical emotional words are homogeneous. They also assume that users’ music mood preferences are homogeneous under the same emotional state. In this article, we propose four types of heterogeneity that an EMRS should account for: emotion heterogeneity across users, emotion heterogeneity within a user, music mood preference heterogeneity across users, and music mood preference heterogeneity within a user. We further propose a Heterogeneity-aware Deep Bayesian Network (HDBN) to model these assumptions. The HDBN mimics a user’s decision process of choosing music with four components: personalized prior user emotion distribution modeling, posterior user emotion distribution modeling, user grouping, and Bayesian neural network-based music mood preference prediction. We constructed two datasets, called EmoMusicLJ and EmoMusicLJ-small, to validate our method. Extensive experiments demonstrate that our method significantly outperforms baseline approaches on metrics of HR, Precision, NDCG, and MRR. Ablation studies and case studies further validate the effectiveness of our HDBN. The source code and datasets are available at  https://github.com/jingrk/HDBN .}
}


@article{DBLP:journals/tois/WuXZZZLKAX25,
	author = {Yiqing Wu and
                  Ruobing Xie and
                  Zhao Zhang and
                  Xu Zhang and
                  Fuzhen Zhuang and
                  Leyu Lin and
                  Zhanhui Kang and
                  Zhulin An and
                  Yongjun Xu},
	title = {ID-centric Pre-training for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {125:1--125:29},
	year = {2025},
	url = {https://doi.org/10.1145/3735128},
	doi = {10.1145/3735128},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WuXZZZLKAX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classical sequential recommendation models generally adopt ID embeddings to store knowledge learned from user historical behaviors and represent items. However, these unique IDs are challenging to be transferred to new domains. With the thriving of pre-trained language model (PLM), some pioneer works adopt PLM for pre-trained recommendation, where modality information is considered universal across domains via PLM. Unfortunately, the behavioral information in ID embeddings is verified to currently dominate in recommendation compared to modality information and thus limits these models’ performance. In this work, we propose a novel ID-centric recommendation pre-training paradigm (IDP), which directly transfers informative ID embeddings learned in pre-training domains to item representations in new domains. Specifically, in pre-training stage, besides the ID-based sequential recommendation model, we also build a Cross-domain ID-matcher (CDIM) learned by both behavioral and modality information. In the tuning stage, modality information of new domain items is regarded as a cross-domain bridge built by CDIM. They first adopted to retrieve behaviorally and semantically similar items from pre-training domains using CDIM. Next, these retrieved items’ pre-trained ID embeddings are directly adopted to generate downstream new items’ embeddings. Through extensive experiments on real-world datasets, we demonstrate that our proposed model significantly outperforms all baselines.}
}


@article{DBLP:journals/tois/VuongDR25,
	author = {Tung Thanh Vuong and
                  Pritom Kumar Das and
                  Tuukka Ruotsalo},
	title = {Incorporating Cognitive Abilities into Web Search Re-ranking},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {126:1--126:35},
	year = {2025},
	url = {https://doi.org/10.1145/3736401},
	doi = {10.1145/3736401},
	timestamp = {Sat, 15 Nov 2025 13:54:57 +0100},
	biburl = {https://dblp.org/rec/journals/tois/VuongDR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Web search ranking models learn from human interactions to improve retrieval performance, but they are presently limited by their use of behavioral factors, such as click-through data or dwell time, that do not account for differences in their users’ cognition. However, it is well understood that users’ behavior varies according to their abilities in processing information, making inferences, and interacting with computing systems. As a result, researchers may miss opportunities to design ranking models that are optimized for their users’ cognitive abilities. To address this, we report an approach for search result re-ranking that incorporates cognitive ability information in the ranking model. We report extensive empirical in-the-wild experiments with data from simulated tasks and real-world tasks of 20 participants to measure, predict, and use these data to train search result re-ranking models. Our results demonstrate that cognitive ability data significantly improve the effectiveness of re-ranking models in simulated-task and real-world conditions, and that cognitive abilities can be predicted from regular user interactions without requiring separate cognitive testing for each user. In particular, the models show improved performance in predicting the position of the documents the users select during search sessions. Our findings show that search engines have significant potential to improve their ranking performance by accounting for users’ cognitive ability.}
}


@article{DBLP:journals/tois/LvWCSDZLW25,
	author = {Xiangwei Lv and
                  Guifeng Wang and
                  Jingyuan Chen and
                  Hejian Su and
                  Zhiang Dong and
                  Yumeng Zhu and
                  Beishui Liao and
                  Fei Wu},
	title = {Debiased Cognition Representation Learning for Knowledge Tracing},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {127:1--127:30},
	year = {2025},
	url = {https://doi.org/10.1145/3736576},
	doi = {10.1145/3736576},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LvWCSDZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge tracing (KT) is a fundamental task in intelligent education aimed at tracking students’ knowledge status and predicting their performance on new questions. The primary challenge in KT is accurately inferring a high-quality representation of students’ knowledge state that effectively captures their understanding of questions. However, existing methods are typically developed under the assumption that students’ behaviors directly reflect their knowledge state, which may not hold true especially in online learning scenarios. Abnormal behaviors exhibited by students, such as guessing and plagiarism, can introduce biases into the data, making it difficult to accurately assess students’ true knowledge state. To address this limitation, we propose a novel DebiAsed Cognition rEpresentation (DACE) modeling approach. This approach introduces a novel adversarial training strategy based on information bottleneck theory to obtain a debiased knowledge state representation that retains only the most reliable information for accurately predicting students’ performance on new questions. Moreover, we design a novel contrastive learning module through embedding-based augmentation to further enhance the robustness and generalizability of the learned knowledge state representation. We conduct extensive experiments on three public KT datasets and the newly released dataset BaiPy to demonstrate the superiority of our model over strong baselines, particularly when confronted with biased data. Our code and datasets are available at  https://github.com/lvXiangwei/DACE.git .}
}


@article{DBLP:journals/tois/ZhuWZS25,
	author = {Jiajie Zhu and
                  Yan Wang and
                  Feng Zhu and
                  Zhu Sun},
	title = {Causal Deconfounding via Confounder Disentanglement for Dual-Target
                  Cross-Domain Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {128:1--128:33},
	year = {2025},
	url = {https://doi.org/10.1145/3737457},
	doi = {10.1145/3737457},
	timestamp = {Tue, 14 Oct 2025 19:49:15 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ZhuWZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, dual-target Cross-Domain Recommendation (CDR) has been proposed to capture comprehensive user preferences in order to ultimately enhance the recommendation accuracy in both data-richer and data-sparser domains simultaneously. However, in addition to users’ true preferences, the user–item interactions might also be affected by confounders (e.g., free shipping, sales promotion). As a result, dual-target CDR has to meet two challenges: (1) how to effectively decouple observed confounders, including single-domain confounders and cross-domain confounders, and (2) how to preserve the positive effects of observed confounders on predicted interactions, while eliminating their negative effects on capturing comprehensive user preferences. To address the above two challenges, we propose a  Causal Deconfounding Framework via Confounder Disentanglement for Dual-Target Cross-Domain Recommendation (CD2CDR) . In CD2CDR, we first propose a confounder disentanglement module to effectively decouple observed single-domain and cross-domain confounders. We then propose a causal deconfounding module to preserve the positive effects of such observed confounders and eliminate their negative effects via backdoor adjustment, thereby enhancing the recommendation accuracy in each domain. Extensive experiments conducted on seven real-world datasets demonstrate that CD2CDR significantly outperforms the state-of-the-art methods.}
}


@article{DBLP:journals/tois/SuAWXWMLWLZ25,
	author = {Weihang Su and
                  Qingyao Ai and
                  Yueyue Wu and
                  Anzhe Xie and
                  Changyue Wang and
                  Yixiao Ma and
                  Haitao Li and
                  Zhijing Wu and
                  Yiqun Liu and
                  Min Zhang},
	title = {Pre-training for Legal Case Retrieval Based on Inter-Case Distinctions},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {129:1--129:27},
	year = {2025},
	url = {https://doi.org/10.1145/3735127},
	doi = {10.1145/3735127},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/SuAWXWMLWLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Legal case retrieval aims to help legal workers find relevant cases related to their cases at hand, which is important for the guarantee of fairness and justice in legal judgments. While recent advances in neural retrieval methods have significantly improved the performance of open-domain retrieval tasks (e.g., Web search), their advantages haven’t been observed in legal case retrieval due to their thirst for annotated data. As annotating large-scale training data in legal domains is prohibitive due to the need for domain expertise, traditional search techniques based on lexical matching such as TF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval systems. While previous studies have designed several pre-training methods for IR models in open-domain tasks, these methods are usually suboptimal in legal case retrieval because they cannot understand and capture the key knowledge and data structures in the legal corpus. To this end, we propose a novel pre-training framework named Caseformer that enables the pre-trained models to learn legal knowledge and domain-specific relevance-matching patterns in legal case retrieval without any human-labeled data. This framework is designed to support both dense retrieval models and neural re-ranking models. Through three unsupervised learning tasks, Caseformer is able to capture the special language, document structure, and relevance-matching patterns of legal case documents, making it a strong backbone for downstream legal case retrieval tasks. Experimental results show that our model has achieved state-of-the-art performance in both zero-shot and fine-tuning settings. Also, experiments on both Chinese and English legal datasets demonstrate that the effectiveness of Caseformer is language-independent in legal case retrieval.}
}


@article{DBLP:journals/tois/WuWNGGWL25,
	author = {Lianwei Wu and
                  Kang Wang and
                  Kunlin Nie and
                  Sensen Guo and
                  Chao Gao and
                  Zhen Wang and
                  Shudong Li},
	title = {{TFGIN:} Tight-Fitting Graph Inference Network for Table-based Fact
                  Verification},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {130:1--130:26},
	year = {2025},
	url = {https://doi.org/10.1145/3734520},
	doi = {10.1145/3734520},
	timestamp = {Wed, 26 Nov 2025 07:41:30 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WuWNGGWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fact verification task has emerged as an essential research topic recently due to abundant fake news spreading on the Internet. The task based on unstructured data (i.e., news) has achieved great development, but the task based on structured data (i.e., table) is still in the primary development period. The existing methods usually construct complete heterogeneous graph networks around statement, table, and program subgraphs, and then infer to learn similar semantics on them for fact verification. However, they generally connect the nodes with the same content between subgraphs directly to frame a larger graph network, which has serious sparsity in connections, especially when subgraphs possess limited semantics. To this end, we propose tight-fitting graph inference network (TFGIN), which innovatively builds tight-fitting graphs (TF-graphs) to strengthen the connections of subgraphs and designs inference modeling layer (IML) to learn coherence evidence for fact verification. Specifically, different from traditional connection ways, the constructed TF-graph enhances inter-graph and intra-graph connections of subgraphs through subgraph segmentation and interaction guidance mechanisms. IML could reason the semantics with strong correlation and high consistency as explainable evidence. Experiments on three competitive datasets confirm the superiority and scalability of our TFGIN.}
}


@article{DBLP:journals/tois/ChenQZMK25,
	author = {Yankai Chen and
                  Yue Que and
                  Xinni Zhang and
                  Chen Ma and
                  Irwin King},
	title = {Learning Binarized Representations with Pseudo-positive Sample Enhancement
                  for Efficient Graph Collaborative Filtering},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {131:1--131:28},
	year = {2025},
	url = {https://doi.org/10.1145/3744239},
	doi = {10.1145/3744239},
	timestamp = {Tue, 14 Oct 2025 19:49:15 +0200},
	biburl = {https://dblp.org/rec/journals/tois/ChenQZMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning vectorized embeddings is fundamental to many recommender systems for user–item matching. To enable efficient online inference,  representation binarization , which embeds latent features into compact binary sequences, has recently shown significant promise in optimizing both memory usage and computational overhead. However, existing approaches primarily focus on  numerical quantization , neglecting the associated  information loss , which often results in noticeable performance degradation. To address these issues, we study the problem of graph representation binarization for efficient collaborative filtering. Our findings indicate that explicitly mitigating information loss at various stages of embedding binarization has a significant positive impact on performance. Building on these insights, we propose an enhanced framework, BiGeaR++, which specifically leverages supervisory signals from  pseudo-positive samples , incorporating both real item data and latent embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a fine-grained inference distillation mechanism and an effective embedding sample synthesis approach. Empirical evaluations across five real-world datasets demonstrate that the new designs in BiGeaR++ work seamlessly well with other modules, delivering substantial improvements of around 1% ∼ 10% over BiGeaR and thus achieving state-of-the-art performance compared to the competing methods. Our implementation is available at  https://github.com/QueYork/BiGeaR-SS .}
}


@article{DBLP:journals/tois/LinTCZCWY25,
	author = {Zhenghong Lin and
                  Yanchao Tan and
                  Jiamin Chen and
                  Hengyu Zhang and
                  Chaochao Chen and
                  Shiping Wang and
                  Carl Yang},
	title = {Unified Heterogeneous Hypergraph Construction for Incomplete Multimedia
                  Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {132:1--132:31},
	year = {2025},
	url = {https://doi.org/10.1145/3745020},
	doi = {10.1145/3745020},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/LinTCZCWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the dynamic environment of multimedia-sharing platforms like X (formerly known as Twitter) and TikTok, multimedia recommendation systems have been widely used to help users discover items of interest. However, traditional approaches often fall short, when the item modalities are incomplete, a common issue in real-world scenarios. To this end, we introduce the unified heterogeneous Hypergraph construction for the Incomplete multimedia REcommendation ( HIRE ), a novel framework designed to jointly learn a heterogeneous hypergraph and perform accurate recommendations under incomplete scenarios.  HIRE  first initializes the unified heterogeneous hypergraph for modality completion and employs self-supervised learning aligned with the contrastive text-centered view for multimedia recommendation. Such integration effectively handles the challenges posed by incomplete modalities, leading to improved recommendation accuracy. Furthermore, we find that the hypergraph directly learned from the  HIRE  is a dense structure which can be inaccurate and coarse. Therefore, we devise the  HIRE  framework with Sparse constraint named  HIRES , which uniquely integrates optimal transport and a  ℓ 2 , 1 -norm to refine the hypergraph structure. Our extensive experiments across various datasets demonstrate the superiority of  HIRES  in addressing incomplete modalities, establishing it as a powerful tool for personalized multimedia recommendations.}
}


@article{DBLP:journals/tois/YaoWLYX25,
	author = {Jing Yao and
                  Xiting Wang and
                  Jianxun Lian and
                  Xiaoyuan Yi and
                  Xing Xie},
	title = {Neural Recommendation Reasoning with Logic Rules},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {133:1--133:28},
	year = {2025},
	url = {https://doi.org/10.1145/3742856},
	doi = {10.1145/3742856},
	timestamp = {Mon, 19 Jan 2026 18:37:55 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YaoWLYX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Explainability is critical for recommender systems to ensure good user experience and facilitate designers to debug. However, generating explanations in recommender systems usually requires large efforts due to the dependency on additional data and case-by-case model design. One possible solution to these challenges is reasoning with logic rules, whose validity or confidence can automatically indicate high-quality explanations and formats are general. However, pioneer methods can be hardly applied in recommendation due to the high sparsity of interaction data, which raises the difficulty in accurately computing the rule validity, and the specific ranking-oriented task. To bridge this gap, we propose a general framework for  Reco mmendation with  lo gic  r ule reasoning ( Recolor ) that satisfies three desirable properties. First, we explicitly estimate the rule validity to ensure well-grounded decisions, where a fuzzy logic validity module is designed for accurate estimation on highly sparse recommendation data. Second, we ensure the generality for both the types of input data and model architectures by designing a neural logic generation module, which decouples the user–item representation learning from the rule construction. Third, we integrate the two above-mentioned modules with a ranking-oriented BPR loss and achieve a unified optimization of explainability and accuracy. For any given neural recommendation model, our proposed logic rule reasoning framework can upgrade it to a self-explainable version. Numerical experiments and user studies on four public recommendation datasets with different levels of sparsity demonstrate that our framework shows high-validity rule explanations, generality in architecture and data, and high recommendation accuracy.}
}


@article{DBLP:journals/tois/GuiYWHLZW25,
	author = {Xiaoqiang Gui and
                  Guoxian Yu and
                  Jun Wang and
                  Shuguang Han and
                  Qingzhong Li and
                  Yongqing Zheng and
                  Wei Wang},
	title = {Interaction Privacy Vulnerability in Federated Recommendation and
                  Lossless Countermeasure},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {134:1--134:31},
	year = {2025},
	url = {https://doi.org/10.1145/3745025},
	doi = {10.1145/3745025},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/GuiYWHLZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Recommendation (FedRec) systems are recognized as privacy-preserving solutions for collaboratively training recommender models without sharing users’ private data. However, recent studies have revealed that FedRec systems are vulnerable to interaction-level membership inference attacks. In such attacks, a semi-honest server can employ crafted methods to infer users’ interacted items. In this article, we identify that  user preference information is predominantly stored in the user-uploaded parameters rather than in the local parameters after local training.  Leveraging this insight, we expose a new interaction vulnerability and introduce the PubPara attack. Our experiments show that PubPara improves the inference performance by at least 40% over existing attacks, while requiring minimal inference time and remaining robust against current defense methods. To safeguard user privacy without compromising recommender performance, we propose MultiVerse, a novel countermeasure. MultiVerse utilizes untrained items outside the user’s local training data to obfuscate the server’s inference of interacted items. It includes a four-step strategy (training, optimization, refinement, and denoising) to achieve robust defense. Extensive experiments on three representative FedRec models (F-NCF, F-LightGCN, and FedRAP) across three real-world datasets validate that MultiVerse significantly degrades the attack’s inference performance to near the level of random guess while maintaining lossless recommender performance.}
}


@article{DBLP:journals/tois/WangZCHHH25,
	author = {Junmei Wang and
                  Fengjing Zhang and
                  Xiadan Chen and
                  Puyu He and
                  Ellen Anne Huang and
                  Jimmy Xiangji Huang},
	title = {Utilizing Large Language Model for Conversational Information Seeking
                  via Dual-Query Generation and Joint-Encoding},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {135:1--135:30},
	year = {2025},
	url = {https://doi.org/10.1145/3742423},
	doi = {10.1145/3742423},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/WangZCHHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational retrieval leverages multi-turn conversations to meet users’ information needs, and accurately understanding the new intent has become a significant challenge in this field. Recently, the language comprehension and reasoning capabilities of large language models (LLMs) offer a viable solution to these challenges. In this article, we propose a new Dual-Query Generation and Joint-Encoding method by utilizing LLM for Conversational Information Seeking, abbreviated as DQ-CIS. Specifically, we propose a dual-query generation approach that leverages both open source and closed source LLMs to generate two complementary queries: a full-rewrite query that preserves the context semantics of the conversation and a condensed-rewrite query that emphasizes the core intent of the current query. Additionally, to better express the semantic information of the query, we propose a dual-query joint-encoding method, which enhances the thematic expression of query vectors by treating the dual-query as semantic complementary. A query coverage fine-tuned semantic matching method is also introduced to improve result relevance and ranking by fine-tuning the original retrieval scores by ColBERT. We conducted a number of experiments on seven publicly available conversational retrieval datasets. The results show that compared with other models, DQ-CIS has strong competitiveness in both retrieval efficiency and retrieval results.}
}


@article{DBLP:journals/tois/XuMSCXH25,
	author = {Heng{-}Da Xu and
                  Xian{-}Ling Mao and
                  Fanshu Sun and
                  Tian{-}Yi Che and
                  Chun Xu and
                  Heyan Huang},
	title = {AgentTOD: {A} Task-Oriented Dialogue Agent with a Flexible and Adaptive
                  {API} Calling Paradigm},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {136:1--136:32},
	year = {2025},
	url = {https://doi.org/10.1145/3745021},
	doi = {10.1145/3745021},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/XuMSCXH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task-oriented dialogue (TOD) systems play a vital role in numerous assistance and service scenarios, significantly improving people’s daily lives. Conventionally, a TOD system adheres to a fixed paradigm, where it must first extract user goals and query external databases before it can generate the final response. However, this fixed extract-and-query paradigm is not always optimal for all dialogue turns, which is redundant for the simple turns that do not need external information, and is inadequate for the complex turns that need to interact with the external world multiple times. To address the limitations, in this article, we propose AgentTOD, a novel TOD framework that uses a large language model (LLM) as the intelligent agent to achieve a flexible dialogue paradigm. AgentTOD deprecates the traditional modular architecture (including dialogue state tracking and dialogue policy) by utilizing an LLM as the controller brain to determine when and how to call the provided APIs to obtain external information. It can choose to call APIs any number of times with various parameters until it’s enough to reply to the user. Besides, to train AgentTOD, we construct a large and comprehensive TOD dataset, called TrajsTOD (Trajectories of TODs), which consists of 66k+ user-agent dialogue trajectories converted from eight popular TOD datasets covering 60 domains. TrajsTOD is constructed with minimal dialogue annotations where only the API calling logs are needed and can empower AgentTOD with the general ability to call APIs and generate responses according to the task definition. Extensive experimental results on the MultiWOZ-series and SGD datasets demonstrate AgentTOD has superior performance on TODs as well as a superior adaptability to new task scenarios.}
}


@article{DBLP:journals/tois/GuoSGHCZ25,
	author = {Lei Guo and
                  Chenlong Song and
                  Feng Guo and
                  Xiaohui Han and
                  Xiaojun Chang and
                  Lei Zhu},
	title = {Semantic-enhanced Co-attention Prompt Learning for Non-overlapping
                  Cross-domain Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {137:1--137:27},
	year = {2025},
	url = {https://doi.org/10.1145/3742422},
	doi = {10.1145/3742422},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/GuoSGHCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-overlapping Cross-domain Sequential Recommendation (NCSR) is the task that focuses on domain knowledge transfer without overlapping entities. Compared with traditional Cross-domain Sequential Recommendation (CSR), NCSR poses several challenges: (1) NCSR methods often rely on explicit item IDs, overlooking semantic information among entities. (2) Existing CSR mainly relies on domain alignment for knowledge transfer, risking semantic loss during alignment. (3) Most previous studies do not consider the many-to-one characteristic, which is challenging because of the utilization of multiple source domains. Given the above challenges, we introduce the prompt learning technique for Many-to-one Non-overlapping Cross-domain Sequential Recommendation (MNCSR) and propose a Text-enhanced Co-attention Prompt Learning Paradigm (TCPLP). Specifically, we capture semantic meanings by representing items through text rather than IDs, leveraging natural language universality to facilitate cross-domain knowledge transfer. Unlike prior works that need to conduct domain alignment, we directly learn transferable domain information, where two types of prompts, i.e., domain-shared and domain-specific prompts, are devised, with a co-attention-based network for prompt encoding. Then, we develop a two-stage learning strategy, i.e., pre-train and prompt-tuning paradigm, for domain knowledge pre-learning and transferring, respectively. We conduct extensive experiments on three datasets and the experimental results demonstrate the superiority of our TCPLP. Our source codes have been publicly released ( https://github.com/songchenlong/TCPLP ).}
}


@article{DBLP:journals/tois/PanGCCL25,
	author = {Zhiqiang Pan and
                  Chen Gao and
                  Fei Cai and
                  Honghui Chen and
                  Yong Li},
	title = {Light Dynamic Graph Learning on Temporal Networks},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {138:1--138:27},
	year = {2025},
	url = {https://doi.org/10.1145/3745024},
	doi = {10.1145/3745024},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/PanGCCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graph learning on temporal networks aims to understand the continuous evolution pattern of networks, with an important application on forecasting the future temporal network. Existing methods mainly focus on modeling the structural and temporal features, with recent research interest shifting toward considering the structural correlations between nodes through their neighbor co-occurrences. Though satisfactory performance has been achieved, there still remain several limitations: (1) the deviation of investigated scenarios from real-world applications, since most previous researches concentrate on special cases of multigraphs with abundant repeat edges; (2) the insufficient computational efficiency of modeling the structural features, since the existing neighbor co-occurrence scheme fails to consider explicit structural correlations between nodes and suffers from a time-consuming pairwise encoding strategy; (3) the unsatisfying prediction accuracy due to inadequate modeling of temporal features, since each neighbor’s historical temporal features and the temporal domain shifting with network evolving are both neglected. To solve these issues, we first focus on the general scenarios of temporal networks without abundant repeat edges for approaching the actual applications and propose an efficient and effective dynamic graph learning method named LightDyG. Specifically, (1) on the one hand, to increase the computational efficiency, LightDyG decouples the structural correlations between nodes and their individual substructures for fast convergence based on the analysis of existing co-occurrence mechanism, and further designs an incremental strategy for efficient structural encoding; (2) on the other hand, to improve the prediction accuracy, the temporal characteristics are considered by including both the interaction and appearance timestamps of neighbors, and a time-invariant temporal encoding strategy is designed to eliminate the temporal bias introduced by the network evolution. Extensive experiments conducted on four public temporal networks demonstrate that LightDyG outperforms the best baselines by 4.54–11.39% and 6.06–16.24% in terms of AP and AUC on the temporal link prediction tasks, respectively. In addition, LightDyG reduces the time cost for training and test up to 45.91% and 63.94%, respectively, and also achieves a fast convergence speed during training. The implementation of our approach is available in  https://github.com/nudtzpan/LightDyG .}
}


@article{DBLP:journals/tois/LuLZSWLBW25,
	author = {Xiaodong Lu and
                  Mingzhe Liu and
                  Tongyu Zhu and
                  Leilei Sun and
                  Jibin Wang and
                  Weifeng Lv and
                  Yikun Ban and
                  Deqing Wang},
	title = {Adaptive Sampling-based Dynamic Graph Learning for Information Diffusion
                  Prediction},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {139:1--139:25},
	year = {2025},
	url = {https://doi.org/10.1145/3744643},
	doi = {10.1145/3744643},
	timestamp = {Tue, 11 Nov 2025 11:42:43 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LuLZSWLBW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Information diffusion prediction, aimed at estimating future interacting users for a given content, is crucial for various applications on online social platforms. Recently, methods based on dynamic graph learning have achieved superior performance. However, these methods often face scalability issues due to their full-neighbor aggregation, which requires loading the whole diffusion graph, making them impractical for large graphs. While improving model scalability through sampling is an immediate approach, it is challenging on the diffusion graph due to various user dependencies (i.e., the temporal and structural correlations of user–item interactions). To address this problem, we propose a new model named ASDIP, which performs adaptive sampling on the diffusion graph. Specifically, ASDIP employs multiple sampling strategies to extract walks from the diffusion graph, each identifying a representative user dependency by sampling walks that satisfy a specific temporal constraint. Next, the walks sampled by different strategies are first mapped into distinct strategy-specific user representations and then merged into a unified user representation, adaptively fusing the information obtained from different strategies. Finally, a cascade representation learning module is proposed to generate cascade representations based on user representations and interaction timestamps. Experimental results validate the effectiveness and scalability of ASDIP.}
}


@article{DBLP:journals/tois/MaWZZXGY25,
	author = {Jingwei Ma and
                  Jiahui Wen and
                  Lei Zhu and
                  Mingyang Zhong and
                  Yang Xu and
                  Lei Guo and
                  Hongzhi Yin},
	title = {HGDNet: De-Noised Review-Based Rating Prediction Using Hierarchical
                  Gating and Discriminative Networks},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {140:1--140:26},
	year = {2025},
	url = {https://doi.org/10.1145/3746282},
	doi = {10.1145/3746282},
	timestamp = {Wed, 15 Oct 2025 19:23:02 +0200},
	biburl = {https://dblp.org/rec/journals/tois/MaWZZXGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The expressiveness of historical reviews in capturing user preferences has garnered significant attention in recommender systems. However, this technology still has certain limitations. Firstly, irrelevant reviews can introduce noise that may adversely affect the performance of the model. Secondly, existing approaches often assume a flat structure for review features, thus failing to capture the intricate and hierarchical nature of user–item interactions. Thirdly, it is challenging for review-based recommendation models to effectively assess the usefulness of reviews due to sparse supervision signals. To address these challenges, we propose a novel Hierarchical Gating and Discriminative model for rating prediction. Specifically, we introduce a local gating module that utilizes personalized end-to-end differential thresholds to select reviews in a relatively “hard” manner, thereby minimizing the impact of noisy reviews while facilitating model training. Additionally, we incorporate a global gating module to assess the overall usefulness of review signals by estimating the uncertainties inherent in historical reviews. Moreover, we propose a hierarchical discriminative network to develop self-supervision signals at both global and local levels to guide the learning of the hierarchical gating network. Extensive experiments on public datasets have demonstrated the effectiveness of the proposed model, and further investigations provide deep insight into its superiority.}
}


@article{DBLP:journals/tois/WuWLZXNL25,
	author = {Hanrui Wu and
                  Yanxin Wu and
                  Nuosi Li and
                  Jia Zhang and
                  Yonghui Xu and
                  Michael K. Ng and
                  Jinyi Long},
	title = {Cold-start User Recommendation via Heterogeneous Domain Adaptation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {5},
	pages = {141:1--141:26},
	year = {2025},
	url = {https://doi.org/10.1145/3746637},
	doi = {10.1145/3746637},
	timestamp = {Tue, 25 Nov 2025 12:56:57 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WuWLZXNL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommendation systems, cold-start user recommendation is a challenging problem, where precise recommendations are required for users who have not appeared before. Several existing cold-start user recommendation models adopt domain adaptation to extract information from auxiliary source domains to assist the recommendations on the target domain. In this article, we propose that the cold-start user recommendation problem can be formulated by the heterogeneous domain adaption approach. We determine a transformation of user features, e.g., user social relations and historical interactions between warm users and their interested items, into a latent space so that the loss function is set by user feature reconstruction and by feature and distribution matching in the heterogeneous domains. The resulting optimization problem can be solved by matrix eigendecomposition, and the cold-start users’ preferences can thus be obtained. We also extend the proposed model using neural networks. We perform extensive experiments on several real-world datasets, and the results in terms of Precision, Recall, NDCG, and Hit Rate verify the effectiveness of the proposed model.}
}


@article{DBLP:journals/tois/XuDXHLXZQD25,
	author = {Xiaolong Xu and
                  Hongsheng Dong and
                  Haolong Xiang and
                  Xiyuan Hu and
                  Xiaoyong Li and
                  Xiaoyu Xia and
                  Xuyun Zhang and
                  Lianyong Qi and
                  Wanchun Dou},
	title = {C2lRec: Causal Contrastive Learning for User Cold-start Recommendation
                  with Social Variables},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {143:1--143:28},
	year = {2025},
	url = {https://doi.org/10.1145/3711858},
	doi = {10.1145/3711858},
	timestamp = {Fri, 26 Dec 2025 20:52:40 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuDXHLXZQD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedding-based recommender systems rely on historical interactions to model users, which poses challenges for recommending to new users, known as the user cold-start problem. Some approaches incorporate social networks to deduce preferences based on the social circles of cold-start users to solve the problem of sparse features. However, such methods have difficulty distinguishing between superficial correlations and causal relationships in social behaviors, leading to inaccuracies in predicting user preferences. To address the aforementioned issues, we propose the Causal Contrastive Learning Recommendation (C2lRec) framework. Specifically, we causally model the inference of hidden preferences from the feature and historical behavior of warm users and predict user interactions based on such preferences. The counterfactual inference is subsequently performed to intervene and extract interactions from historical behaviors of warm users that influence their preferences, designating as primary causal variables. Additionally, we utilize the primary causal variables from users within the social circle of cold-start users to substitute the missing historical interactions of cold-start users and employ a similar causal modeling approach to uncover hidden preferences as we do with warm users. Finally, we realize causal contrastive learning to enhance the distribution of cold-start users. Extensive experiments conducted on three public datasets demonstrate that the recommendation performance of C2lRec exceeds that of state-of-the-art methods.}
}


@article{DBLP:journals/tois/YuanYYCNY25,
	author = {Wei Yuan and
                  Chaoqun Yang and
                  Guanhua Ye and
                  Tong Chen and
                  Quoc Viet Hung Nguyen and
                  Hongzhi Yin},
	title = {{FELLAS:} Enhancing Federated Sequential Recommendation with {LLM}
                  as External Services},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {144:1--144:24},
	year = {2025},
	url = {https://doi.org/10.1145/3709138},
	doi = {10.1145/3709138},
	timestamp = {Fri, 26 Dec 2025 20:52:40 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YuanYYCNY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation has been widely studied in the recommendation domain since it can capture users’ temporal preferences and provide more accurate and timely recommendations. To address user privacy concerns, the combination of federated learning and sequential recommender systems (FedSeqRec) has gained growing attention. Unfortunately, the performance of FedSeqRec is still unsatisfactory because the models used in FedSeqRec have to be lightweight to accommodate communication bandwidth and clients’ on-device computational resource constraints. Recently, large language models (LLMs) have exhibited strong transferable and generalized language understanding abilities and therefore, in the NLP area, many downstream tasks now utilize LLMs as a service to achieve superior performance without constructing complex models. Inspired by this successful practice, we propose a generic FedSeqRec framework, FELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external service. Specifically, FELLAS employs an LLM server to provide both item-level and sequence-level representation assistance. The item-level representation service is queried by the central server to enrich the original ID-based item embedding with textual information, while the sequence-level representation service is accessed by each client. However, invoking the sequence-level representation service requires clients to send sequences to the external LLM server. To safeguard privacy, we implement  d X -privacy satisfied sequence perturbation, which protects clients’ sensitive data with guarantees. Additionally, a contrastive learning-based method is designed to transfer knowledge from the noisy sequence representation to clients’ sequential recommendation models. Furthermore, to empirically validate the privacy protection capability of FELLAS, we propose two interacted item inference attacks, considering the threats posed by the LLM server and the central server acting as curious-but-honest adversaries in cooperation. Extensive experiments conducted on three datasets with two widely used sequential recommendation models demonstrate the effectiveness and privacy-preserving capability of FELLAS.}
}


@article{DBLP:journals/tois/WangCZLPLG25,
	author = {Hao Wang and
                  Zhichao Chen and
                  Honglei Zhang and
                  Zhengnan Li and
                  Licheng Pan and
                  Haoxuan Li and
                  Mingming Gong},
	title = {Debiased Recommendation via Wasserstein Causal Balancing},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {145:1--145:24},
	year = {2025},
	url = {https://doi.org/10.1145/3725731},
	doi = {10.1145/3725731},
	timestamp = {Fri, 26 Dec 2025 20:52:40 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangCZLPLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems are pivotal in improving user experience on various digital platforms. However, observational training data in recommendation systems introduce selection bias, which leads to a distributional discrepancy between training data and real-world scenarios, resulting in suboptimal performance. Current causal debiasing methods such as inverse propensity score and doubly robust rely on accurately estimated propensity scores, typically optimized through negative log-likelihood (NLL) minimization. However, recent studies have highlighted the limitations of this approach, as perfect NLL minimization may not adequately correct for selection bias. To address this issue, we propose Wasserstein Balancing Metric (WBM), a novel metric that measures and enhances the balancing capacity of propensity scores in causal debiasing methods by minimizing the Wasserstein discrepancy between reweighted populations. On the basis, we introduce IPS-WBM and DR-WBM, incorporating WBM as a regularizer in standard inverse propensity score and doubly robust estimators, which enhances causal balancing capacity without introducing additional bias. Extensive experiments on three real-world recommendation datasets demonstrate that our methods improve the causal balancing capability of learned propensities and enhance debiasing performance.}
}


@article{DBLP:journals/tois/XuXLZ25,
	author = {Hangtong Xu and
                  Yuanbo Xu and
                  Chaozhuo Li and
                  Fuzhen Zhuang},
	title = {Causal Structure Representation Learning of Unobserved Confounders
                  in Latent Space for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {146:1--146:29},
	year = {2025},
	url = {https://doi.org/10.1145/3731447},
	doi = {10.1145/3731447},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/XuXLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inferring user preferences from users’ historical feedback is a valuable problem in recommender systems. Conventional approaches often rely on the assumption that user preferences in the feedback data are equivalent to the real user preferences without additional noise, which simplifies the problem modeling. However, there are various confounders during user–item interactions, such as weather and even the recommendation system itself. Therefore, neglecting the influence of confounders will result in inaccurate user preferences and suboptimal performance of the model. Furthermore, the unobservability of confounders poses a challenge in further addressing the problem. Along these lines, we refine the problem and propose a more rational solution to mitigate the influence of unobserved confounders. Specifically, we consider the influence of unobserved confounders, disentangle them from user preferences in the latent space, and employ causal graphs to model their interdependencies without specific labels. By ingeniously combining local and global causal graphs, we capture the user-specific effects of confounders on user preferences. Finally, we propose our model based on Variational Autoencoders, named  Causal Structure Aware Variational Autoencoders (CSA-VAE)  and theoretically demonstrate the identifiability of the obtained causal graph. We conducted extensive experiments on one synthetic dataset and nine real-world datasets with different scales, including three unbiased datasets and six normal datasets, where the average performance boost against several state-of-the-art baselines achieves up to 9.55%, demonstrating the superiority of our model. Furthermore, users can control their recommendation list by manipulating the learned causal representations of confounders, generating potentially more diverse recommendation results. Our code is available at Code-link ( https://github.com/MICLab-Rec/CSA ).}
}


@article{DBLP:journals/tois/HuangLYYLX25,
	author = {Sirui Huang and
                  Qian Li and
                  Haoran Yang and
                  Dianer Yu and
                  Qing Li and
                  Guandong Xu},
	title = {Causal Time-aware News Recommendations with Large Language Models},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {147:1--147:25},
	year = {2025},
	url = {https://doi.org/10.1145/3729422},
	doi = {10.1145/3729422},
	timestamp = {Thu, 25 Dec 2025 12:46:25 +0100},
	biburl = {https://dblp.org/rec/journals/tois/HuangLYYLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting user satisfaction over time is crucial in news recommendations, as users’ preferences are significantly influenced by various time-variant factors. Traditional correlation-based recommenders often suffer from redundant relationships, which can undermine their effectiveness over time. This work takes a time-aware causal approach to news recommendations, treating exposed news at a predicted time as the treatment variable and the resulting user satisfaction as the outcome variable. Capturing the evolving causal effects of exposed news items on user satisfaction poses significant challenges, particularly stemming from the need to model complex dependencies among time-variant covariates, such as news popularity and recency, as well as to effectively leverage the inherent user preferences embedded in time-invariant covariates. To these ends, we propose the  CA u S al  T ime-aware  Rec ommender, named  CAST-Rec , which accounts for the causal influences of both time-variant and time-invariant covariates. Specifically, we model the intricate causal dependencies among time-variant covariates through a series of transformer-based causal blocks. For time-invariant covariates, we utilize the semantic understanding and generative capabilities of Large Language Models (LLMs) to infer inherent user preferences while mitigating potential confounding effects. Extensive experiments demonstrate the superior performance of CAST-Rec compared to various news recommendation models and across multiple LLM implementations.}
}


@article{DBLP:journals/tois/WangQLYCX25,
	author = {Fan Wang and
                  Lianyong Qi and
                  Weiming Liu and
                  Bowen Yu and
                  Jintao Chen and
                  Yanwei Xu},
	title = {Inter- and Intra-Similarity Preserved Counterfactual Incentive Effect
                  Estimation for Recommendation Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {148:1--148:24},
	year = {2025},
	url = {https://doi.org/10.1145/3722104},
	doi = {10.1145/3722104},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangQLYCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized incentives are crucial for boosting user engagement and increasing platform revenues. Many studies have utilized uplift modeling to estimate the conditional average treatment effects (CATEs) of incentives and then allocate them under cost constraints. However, identifying which users should receive such incentives remains challenging, posing a selection bias problem. Traditional representation-based approaches mitigate bias by balancing treated and controlled distributions but overlook local similarity information. Recognizing that similar users should exhibit similar outcomes, it is vital to preserve both intra-similarity within treatment groups and inter-similarity between covariate and representation spaces. Moreover, existing methods primarily focus on CATE accuracy, neglecting the ranking ability vital for uplift modeling. We propose the Similarity Preserved Counterfactual Incentive Effect Estimation ( S-CIEE ) method, comprising three modules: (1) an Intra-Similarity Preservation Regularizer via Fused Gromov-Wasserstein Optimal Transport, (2) an Inter-Similarity Preservation Regularizer using a similarity constraint, and (3) a Rank-Aware Learning module for uplift ranking. Comprehensive experiments on one semi-synthetic and two real-world datasets show that  S-CIEE  improves both CATE accuracy and uplift modeling performance.}
}


@article{DBLP:journals/tois/YuLWTZZLX25,
	author = {Shuo Yu and
                  Yicong Li and
                  Shuo Wang and
                  Tao Tang and
                  Qiang Zhang and
                  Jingjing Zhou and
                  Ivan Lee and
                  Feng Xia},
	title = {CaGE: {A} Causality-inspired Graph Neural Network Explainer for Recommender
                  Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {149:1--149:29},
	year = {2025},
	url = {https://doi.org/10.1145/3729224},
	doi = {10.1145/3729224},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YuLWTZZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Generating  post hoc  causal explanations for graph neural network-based recommender systems is vital for enhancing the credibility and interpretability of recommendations. Existing model-agnostic explainers primarily capture statistical correlations between topological information and recommendation outcomes. However, they often fail to identify true causal relationships due to their model-agnostic design and the challenges posed by heterogeneous graph structures. To address these limitations, we propose a causality-inspired graph neural network explainer for recommender systems, namely CaGE, which generates explanations reflecting causality in recommendation scenarios without accessing the internal parameters of the recommender system. Unlike previous explainers that rely on correlation-based learning, CaGE leverages heterogeneous interventional distributions to eliminate backdoor paths of non-causal variables in the structural causal model of the recommendation task, ensuring causation is accurately captured. Specifically, CaGE incorporates backdoor adjustment based on heterogeneous interventional distributions and causal contrastive learning to optimize a set of heterogeneous soft masks that disentangle causation from non-causation. Additionally, a causality-inspired meta-path search strategy is employed to represent causation as paths between users and recommended items, further enhancing explanation readability. Extensive experiments are conducted on three recommendation datasets, and the experimental results illustrate the superior fidelity of CaGE as compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tois/ZhangYCLLZ25,
	author = {Guixian Zhang and
                  Guan Yuan and
                  Debo Cheng and
                  Lin Liu and
                  Jiuyong Li and
                  Shichao Zhang},
	title = {Mitigating Propensity Bias of Large Language Models for Recommender
                  Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {150:1--150:26},
	year = {2025},
	url = {https://doi.org/10.1145/3736404},
	doi = {10.1145/3736404},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangYCLLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Large Language Models (LLMs) creates new opportunities for recommender systems, especially by exploiting the side information (e.g., descriptions and analyses of items) generated by these models. However, aligning this side information with collaborative information from historical interactions poses significant challenges. The inherent biases within LLMs can skew recommendations, resulting in distorted and potentially unfair user experiences. On the other hand, propensity bias causes side information to be aligned in such a way that it often tends to represent all inputs in a low-dimensional subspace, leading to a phenomenon known as dimensional collapse, which severely restricts the recommender system’s ability to capture user preferences and behaviors. To address these issues, we introduce a novel framework named Counterfactual LLM Recommendation (CLLMR). Specifically, we propose a spectrum-based side information encoder that implicitly embeds structural information from historical interactions into the side information representation, thereby circumventing the risk of dimension collapse. Furthermore, our CLLMR approach explores the causal relationships inherent in LLM-based recommender systems. By leveraging counterfactual inference, we counteract the biases introduced by LLMs. Extensive experiments demonstrate that our CLLMR approach consistently enhances the performance of various recommender models.}
}


@article{DBLP:journals/tois/ChenCWWCX25,
	author = {Yuzhe Chen and
                  Jie Cao and
                  Youquan Wang and
                  Jia Wu and
                  Huanhuan Chen and
                  Guandong Xu},
	title = {Causal Variational Inference for Deconfounded Multi-Behavior Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {151:1--151:26},
	year = {2025},
	url = {https://doi.org/10.1145/3745023},
	doi = {10.1145/3745023},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ChenCWWCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-Behavior Recommendation (MBR)  aims to model personalized user preferences by integrating diverse interaction behaviors (e.g., page view, favorite, add to cart, purchase). However, latent confounders such as contextual influences and social relationships can obscure the true causal effects in real-world scenarios, thereby confounding the model’s prediction. Although existing MBR research extensively explores behavioral dependencies and heterogeneity, it frequently overlooks the impact of latent confounders, thereby limiting its ability to capture users’ genuine preferences. To address the limitations of existing methods, we identify two key challenges in MBR: (1) how to infer latent confounders, and (2) how to mitigate their influence across multi-behavior interactions. To this end, we propose  Causal Variational Inference for Deconfounded (CVID)  MBR. CVID employs a variational graph autoencoder to model latent uncertainty in multi-behavior interactions and introduces a confounder inference module to generate behavior-specific latent confounders via variational inference. In the conditional diffusion module, noise is progressively injected during the forward process to simulate the dynamic evolution of user preferences, while the reverse process leverages the inferred latent confounders to guide denoising through back-door adjustment, thereby recovering the true causal effects between multi-behavior interactions and the model’s prediction. Extensive experiments on public multi-behavior datasets demonstrate that CVID consistently outperforms state-of-the-art baselines in mitigating confounding effects and improving recommendation accuracy, validating its effectiveness and superiority.}
}


@article{DBLP:journals/tois/LuLXGZLGZZ25,
	author = {Kezhi Lu and
                  Jie Lu and
                  Hanshi Xu and
                  Kairui Guo and
                  Qian Zhang and
                  Hua Lin and
                  Mark Grosser and
                  Yi Zhang and
                  Guangquan Zhang},
	title = {Genomics-Enhanced Cancer Risk Prediction for Personalized LLM-Driven
                  Healthcare Recommender Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {152:1--152:30},
	year = {2025},
	url = {https://doi.org/10.1145/3745022},
	doi = {10.1145/3745022},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LuLXGZLGZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cancer risk prediction is a cornerstone of personalized medicine that offers opportunities for early detection and preventive interventions. However, the current models are designed to predict cancer risk face several challenges. First, most rely on traditional statistical methods, which struggle to capture the complexity of genetic, family medical history, and lifestyle factors. Hence, the accuracy of these models is limited. Additionally, the models neglect to integrate multidimensional data sources, particularly genetic information like single nucleotide polymorphisms (SNPs), which could enhance prediction accuracy. Third, while the system might effectively predict risk, it cannot translate those predictions into actionable healthcare recommendations to reduce cancer risk. In this study, we address all three of these limitations. With a focus on six prevalent cancers—we extracted SNP data from the UK Biobank and designed a novel risk prediction model for cancer and personalized healthcare recommendations based upon the mixture of experts (MoE) paradigm and large language models (LLMs), respectively. Named MoE-HRS, experts based two router networks for separate processing by the Transformer and the convolutional neural network (CNN). Experiments on UK Biobank data show that our model outperforms state-of-the-art cancer risk prediction models. To bridge the gap between risk prediction and practical healthcare applications, we devised a healthcare recommender system powered by LLMs. This approach holds promise for enhancing early detection rates and promoting preventive healthcare management (relevant coding and data are available at  https://github.com/bjtu-lucas-nlp/MoE-HRS ).}
}


@article{DBLP:journals/tois/ChenCNZ25,
	author = {Weixin Chen and
                  Li Chen and
                  Yongxin Ni and
                  Yuhan Zhao},
	title = {Causality-Inspired Fair Representation Learning for Multimodal Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {153:1--153:29},
	year = {2025},
	url = {https://doi.org/10.1145/3744240},
	doi = {10.1145/3744240},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ChenCNZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, multimodal recommendations (MMRs) have gained increasing attention for alleviating the data sparsity problem of traditional recommender systems by incorporating modality-based representations. Although MMR exhibits notable improvement in recommendation accuracy, we empirically validate that an increase in the quantity or variety of modalities leads to a higher degree of users’ sensitive information leakage due to entangled causal relationships, risking fair representation learning. On the other hand, existing fair representation learning approaches are mostly based on the assumption that sensitive information is solely leaked from users’ interaction data and do not explicitly model the causal relationships introduced by multimodal data, which limits their applicability in multimodal scenarios. To address this limitation, we propose a novel fair multimodal recommendation approach (dubbed FMMRec) through causality-inspired fairness-oriented modal disentanglement and relation-aware fairness learning. Particularly, we disentangle biased and filtered modal embeddings inspired by causal inference techniques, enabling the mining of modality-based unfair and fair user–user relations, thereby enhancing the fairness and informativeness of user representations. By addressing the causal effects of sensitive attributes on user preferences, our approach aims to achieve counterfactual fairness in MMRs. Experiments on two public datasets demonstrate the superiority of our FMMRec relative to the state-of-the-art baselines. Our source code is available at  https://github.com/WeixinChen98/FMMRec .}
}


@article{DBLP:journals/tois/ZhaoZWJMYT25,
	author = {Xuhao Zhao and
                  Yanmin Zhu and
                  Chunyang Wang and
                  Mengyuan Jing and
                  Wenze Ma and
                  Jiadi Yu and
                  Feilong Tang},
	title = {Dual-Adaptive Update Strategies-Enhanced Meta-Optimization for User
                  Cold-Start Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {154:1--154:36},
	year = {2025},
	url = {https://doi.org/10.1145/3746634},
	doi = {10.1145/3746634},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhaoZWJMYT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User cold-start recommendation presents a significant challenge for recommender systems, affecting their overall effectiveness. Meta-learning-based methods have been introduced to address this issue. These methods treat the user cold-start recommendation problem as a few-shot learning task, where each user represents a unique task. The objective is to acquire shared initialization parameters that can be effectively applied across all cold-start users. Subsequently, these shared parameters are fine-tuned into personalized parameters using individual interaction data. Recent studies argue that shared parameters are unsuitable for all users with an implicit grouping distribution of user preference. Therefore, they propose adaptive-initialization-based methods, which first differentiate tasks based on user preferences and then generate task-adaptive initialization parameters using task representations. However, both the meta-learning and adaptive-initialization-based manners ignore discovering the adaptive capability of update strategies in the process of transferring initialization parameters to personalized parameters. Instead, they rely on task-shared optimization strategies, leading the model to fall into an overfitting or underfitting situation. In response to this, we propose a dual-adaptive update strategies-enhanced meta-optimization framework (DAUS) for user cold-start recommendation. First, we integrate  dual-adaptive update strategies  to enhance the adaptive capability of transferring initialization parameters. This involves incorporating both task-adaptive optimization hyperparameters and objectives. Second, we design a  multifaceted task encoder , which can provide diverse task information to differentiate between tasks, including explicit task features (task relevance, training signals) and other implicit task information. Extensive experiments based on three real-world datasets demonstrate that our DAUS outperforms the state-of-the-art methods. The source code is available at  https://github.com/XuHao-bit/DAUS .}
}


@article{DBLP:journals/tois/ZhangDBMLCZDW25,
	author = {Zeyu Zhang and
                  Quanyu Dai and
                  Xiaohe Bo and
                  Chen Ma and
                  Rui Li and
                  Xu Chen and
                  Jieming Zhu and
                  Zhenhua Dong and
                  Ji{-}Rong Wen},
	title = {A Survey on the Memory Mechanism of Large Language Model-based Agents},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {155:1--155:47},
	year = {2025},
	url = {https://doi.org/10.1145/3748302},
	doi = {10.1145/3748302},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangDBMLCZDW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large language model (LLM)-based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this article, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss “what is” and “why do we need” the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at  https://github.com/nuster1128/LLM_Agent_Memory_Survey .}
}


@article{DBLP:journals/tois/WangZR25,
	author = {Zhenhua Wang and
                  Chen Zhang and
                  Ming Ren},
	title = {A Novel Benford's Law-Driven Approach for Detecting Machine-Generated
                  Text},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {156:1--156:25},
	year = {2025},
	url = {https://doi.org/10.1145/3748305},
	doi = {10.1145/3748305},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/WangZR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting Machine-Generated Text (MGT) is critical for the sustainable development of information systems. Existing studies often overlook the generation inconsistency between AI and human, limiting the effectiveness of detection. This article introduces a novel detection approach, BENADV, and the motivation stems from our recognition that, unlike MGT (probabilistic token prediction), Human-Written Text (HWT) is influenced by individual factors (e.g., personal experience), which constitutes a form of “manipulation” at the textual level. Specifically, BENADV is built on our new discovery that MGT adheres more closely to Benford’s law compared to HWT. We leverage the adherence patterns as detection mechanisms, and further enhance detection performance through adversarial perturbations controlled by stochastic differential equations. Extensive experiments on general-domain datasets demonstrate that BENADV is SOTA. For instance, on the HC3 dataset, BENADV achieves 99.13% accuracy and 99.18% F1, outperforming existing methods by 1.16–7.82% and 1.37–8.30%. Moreover, BENADV exhibits remarkable scalability, with its performance consistently exceeding 96% on vertical domain datasets as AI advances (from GPT-3.5 to GPT-4), far surpassing the 50–60% performance of existing methods. Notably, BENADV excels in the more challenging short MGT detection. Also, we provide practical insights and discuss implications.}
}


@article{DBLP:journals/tois/LiSJCWYZ25,
	author = {Xiaopeng Li and
                  Lixin Su and
                  Pengyue Jia and
                  Suqi Cheng and
                  Junfeng Wang and
                  Dawei Yin and
                  Xiangyu Zhao},
	title = {Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting
                  Using Multi-Agent LLMs},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {157:1--157:33},
	year = {2025},
	url = {https://doi.org/10.1145/3749099},
	doi = {10.1145/3749099},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/LiSJCWYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Search engines are crucial as they provide an efficient and easy way to access vast amounts of information on the Internet for diverse information needs. User queries, even with a specific need, can differ significantly. Prior research has explored the resilience of ranking models against typical query variations like paraphrasing, misspellings, and order changes. Yet, these works overlook how diverse demographics uniquely formulate identical queries. For instance, older individuals tend to construct queries more naturally and in varied order compared to other groups. This demographic diversity necessitates enhancing the adaptability of ranking models to diverse query formulations. To this end, in this article, we propose a framework that integrates a novel rewriting pipeline that rewrites queries from various demographic perspectives and a novel framework to enhance ranking robustness. To be specific, we use Chain of Thought (CoT) technology to utilize Large Language Models (LLMs) as agents to emulate various demographic profiles, then use them for efficient query rewriting, and we innovate a Robust Multi-gate Mixture-of-Experts (R-MMoE) architecture coupled with a hybrid loss function, collectively strengthening the ranking models’ robustness. Our extensive experiments on both public and industrial datasets assesses the efficacy of our query rewriting approach and the enhanced accuracy and robustness of the ranking model. The findings highlight the sophistication and effectiveness of our proposed model. We release our code implementation publicly ( https://github.com/Applied-Machine-Learning-Lab/ROBR ).}
}


@article{DBLP:journals/tois/GuoSFYL25,
	author = {Zhihao Guo and
                  Peng Song and
                  Chenjiao Feng and
                  Kaixuan Yao and
                  Jiye Liang},
	title = {Causal Inference for Multi-Criteria Rating Recommender Systems},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {158:1--158:30},
	year = {2025},
	url = {https://doi.org/10.1145/3757737},
	doi = {10.1145/3757737},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/GuoSFYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems are designed to assist users in discovering interesting items and bringing profits to online platforms. The existing works primarily explore the correlation between historical feedback and model predictions through the data-driven paradigm based on a single user-item rating matrix (i.e., overall rating). However, this single-criterion methods ignore the users’ multi-criteria (MC) behavioral characteristics. For example, a hotel system allows users to rate from multiple dimensions, such as environment and location (i.e., MC ratings). Moreover, selection bias is pervasive in user behavior data. Traditional data-driven methods may induce spurious association and amplified biases. To address the above challenges, we propose a debiasing framework called  Multi-Criteria Causal Recommendation  (MCCR), which encapsulates users’ diverse MC preferences and employs causal inference to construct novel training and inference strategies. Specifically, we first represent the causal relationships among variables in MC scenarios through the structural causal model. Then, we mitigate the negative impact of selection bias through the back-door adjustment. Next, a graph representation learning framework suitable for MC ratings is developed, which is used to extract higher-order information and infer the heterogeneity of users’ preferences with different criteria. Experimental results on six real datasets demonstrate that the MCCR significantly outperforms the existing baselines.}
}


@article{DBLP:journals/tois/ZhangWCLZL25,
	author = {Qian Zhang and
                  Shoujin Wang and
                  Longbing Cao and
                  Defu Lian and
                  Haibo Zhang and
                  Wenpeng Lu},
	title = {Semantic Relation Guided Dual-view Contrastive Learning for Session-based
                  Recommendations},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {159:1--159:36},
	year = {2025},
	url = {https://doi.org/10.1145/3750724},
	doi = {10.1145/3750724},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ZhangWCLZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Session-based Recommender Systems (SBRSs) aim to recommend the next item to users based on their historical interactions with items within or between sessions. A session is constituted by a sequence of interactions between the user and items within a continuous period. Existing SBRSs often focus on modeling co-occurrence-based inter-item transitions within or between sessions only. They generally overlook intrinsic inter-item semantic relations. Specifically, in practice, many items are substitutable or complementary to each other. Such relations provide significant signals to guide user interaction behaviors as well as the next-item recommendations. Moreover, existing works overlook the fact that user behaviors are driven simultaneously by both user intent and item attributes, failing to consider the implicit item characteristics embedded within. Such practice leads to entangled user intent and latent item characteristics, bringing unnecessary interference between these two aspects, impeding accurate modeling of each aspect, ultimately significantly impeding recommendation performance. To bridge these gaps, we propose a novel framework called  S emantic relation guided dual-view  C ontrastive  L earning for  S ession-based  R ecommendations (SCL-SR). SCL-SR introduces a novel semantic relation-guided contrastive learning module to capture additional supervision signals from both user intent view and item attribute view to guide the next-item prediction better. Then, we propose a novel intent-attribute disentangler to effectively mitigate the interference between user intent and latent item characteristics for further improving the recommendation performance. Extensive experiments on three real-world datasets demonstrate the significant superiority of SCL-SR over the state-of-the-art approaches, including achieving substantial improvements ranging from 7.10% to 12.82% on the Tmall dataset. Our source code and datasets are available at  https://github.com/Nishikata97/SCL-SR .}
}


@article{DBLP:journals/tois/ChenLZMCZ25,
	author = {Li Chen and
                  Rui Liu and
                  Yuxiang Zhou and
                  Xudong Ma and
                  Yong Chen and
                  Dell Zhang},
	title = {Deep Hashing with Semantic Hash Centers for Image Retrieval},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {160:1--160:38},
	year = {2025},
	url = {https://doi.org/10.1145/3749983},
	doi = {10.1145/3749983},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ChenLZMCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep hashing presents an effective strategy for large-scale image retrieval. Current hashing methods are generally categorized by their supervision types: point-wise, pairwise, and list-wise. Recent advancements in point-wise methods (e.g., CSQ, MDS) have significantly enhanced retrieval performance across diverse datasets by pre-assigning a hash center to each class, thereby improving the discriminability of the resultant hash codes. However, these methods employ purely data-independent algorithms for generating hash centers, overlooking the semantic connections between different classes, which, we argue, could degrade retrieval performance. To tackle this problem, this article expands on the newly emerged concept of “hash centers” to introduce “ semantic  hash centers,” which posits that hash centers of semantically related classes should exhibit closer Hamming distances, while those of unrelated classes should be more distant. Based on this hypothesis, we propose a three-stage framework, termed Semantic Hash Centers (SHC), to produce hash codes that preserve semantics. First, we build a classification network to detect semantic similarities between classes, and utilize a data-dependent approach to similarity calculation that can adapt to varied data distributions. Next, we develop a new optimization algorithm to generate SHC. This algorithm not only maintains semantic relatedness among hash centers but also integrates a constraint to ensure a minimum distance between them, addressing the issue of excessively proximate hash centers potentially impairing retrieval performance. Finally, we train a deep hashing network with the above generated SHC to convert each image into a binary hash code. Experiments on large-scale image retrieval across several public datasets demonstrate that SHC generates more discriminative hash codes, markedly enhancing retrieval performance. Specifically, in terms of the mAP@100, mAP@1000, and mAP@ALL metrics, SHC records average improvements of +6.24%, +6.68%, and +10.39%, respectively, over the most competitive existing methods. The code of our SHC project is available at  https://github.com/cc752424640/Deep-Hashing-with-Semantic-Hash-Centers-for-Image-Retrieval .}
}


@article{DBLP:journals/tois/SuCWLLWLZY25,
	author = {Jiajie Su and
                  Chaochao Chen and
                  Yihao Wang and
                  Weiming Liu and
                  Yuyuan Li and
                  Tao Wang and
                  Zhigang Li and
                  Xiaolin Zheng and
                  Jianwei Yin},
	title = {DuAda: Adaptive Targeted Model Poisoning Attack Framework via Dummy
                  User Simulation on Federated Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {161:1--161:37},
	year = {2025},
	url = {https://doi.org/10.1145/3757059},
	doi = {10.1145/3757059},
	timestamp = {Tue, 20 Jan 2026 14:46:51 +0100},
	biburl = {https://dblp.org/rec/journals/tois/SuCWLLWLZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Recommendation (FedRec) has been widely applied recently for realizing privacy preservation in recommender systems. However, due to direct uploads of model gradients from all clients, FedRec is vulnerable to potential poisoning attacks. In this article, we focus on the targeted model poisoning attacks in FedRec, which aims to raise the exposure ratio of specific target items by generating poisoned gradients to influence global training. Challenges emerge when implementing this kind of attack. On the one hand, simulating authentic users on the malicious clients for downstream poisoning is hard when access to prior knowledge is limited. On the other hand, distinguished item attributes and personalized user preferences require the attack to be adaptive to complex distributions. To this end, we propose a novel attack DuAda with two modules, i.e.,  dummy user simulator  and  adaptive distribution attacker . The dummy user simulator is designed to generate malicious users with characteristics similar to real users, which exploits authentic user representations and preference labels simultaneously through two-stage inversion optimization. The attacker first extracts heterogeneous distributions by a special multi-prototype clustering method, and then conducts adaptive attacks from both explicit and implicit promotion perspectives. The explicit promotion raises the prediction scores of target items based on the inherent characteristics, while the implicit promotion imbues them with the features of popular items. Targeted at our proposed attack method, we also design a merged adaptive defense mechanism to fight against DuAda and conduct defensive experiments. Empirical studies on four real-world datasets demonstrate the effectiveness and interpretability of DuAda.}
}


@article{DBLP:journals/tois/SangHWZW25,
	author = {Lei Sang and
                  Maohao Huang and
                  Yu Wang and
                  Yiwen Zhang and
                  Xindong Wu},
	title = {Bottlenecked Heterogeneous Graph Contrastive Learning for Robust Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {162:1--162:36},
	year = {2025},
	url = {https://doi.org/10.1145/3750725},
	doi = {10.1145/3750725},
	timestamp = {Sun, 07 Dec 2025 22:17:55 +0100},
	biburl = {https://dblp.org/rec/journals/tois/SangHWZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommender systems, heterogeneous graph neural networks (HGNNs) have demonstrated remarkable efficacy due to their capacity to harness rich auxiliary information within heterogeneous information networks (HINs). However, existing HGNN-based recommendation faces severe noise cascading challenge. The presence of substantial data noise can adversely affect robustness of recommender, as the graph structures are susceptible to noise and even unnoticed malicious perturbations. Moreover, these noises can propagate and accumulate through connected nodes, potentially exerting a profound impact on target nodes within the graph structure. To tackle the noise challenges, we present a Bottlenecked Heterogeneous Graph Contrastive Learning (BHGCL), aiming to enhance the robustness of recommendation systems. BHGCL can first effectively separate fine-grained latent factors from complex self-supervision signals with a disentangled-based encoder, leveraging diverse semantic information across various meta-paths. Then, by employing the information bottleneck (IB) principle, BHGCL adaptively learns to reduce noise in augmented graphs. IB can capture the minimum sufficient information from the data features, which significantly improves system performance in environments with noisy data. Experimental findings from multiple real-world datasets reveal that our approach surpasses the latest advanced recommendation systems, verifying its effectiveness and robustness. To reproduce our work, we have open-sourced our code at  https://github.com/DuellingSword/BHGCL .}
}


@article{DBLP:journals/tois/YuLHX25,
	author = {Dianer Yu and
                  Qian Li and
                  Huan Huo and
                  Guandong Xu},
	title = {Breaking the Loop: Causal Learning to Mitigate Echo Chambers in Social
                  Networks},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {163:1--163:27},
	year = {2025},
	url = {https://doi.org/10.1145/3757738},
	doi = {10.1145/3757738},
	timestamp = {Sun, 07 Dec 2025 22:17:55 +0100},
	biburl = {https://dblp.org/rec/journals/tois/YuLHX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In social networks, echo chambers form when users primarily encounter information that reinforces their existing views with limited exposure to different perspectives. This self-reinforcing isolation worsens societal issues such as division and declining public discourse. Traditional approaches attempt to mitigate echo chambers by analyzing observable interaction patterns to identify their formative mechanisms. However, they overlook unobserved implicit factors, called hidden confounders in causal inference, that significantly influence content exposure and user behaviors despite not being directly captured in the data. To address this, we propose  Causal Echo Diffusion Attenuator (CEDA) , a novel framework that integrates causal learning with sequential recommendations to detect and adjust for hidden confounders in social networks. Generally, CEDA comprises four key components: (1)  User Dual Modelling  builds comprehensive user embeddings by combining users’ attributes and structural information to fully capture behavior patterns. (2)  Causal Transformer  then estimates residual embeddings that account for hidden confounders, incorporating them into the Transformer as causal adjustments for unbiased user embeddings. (3)  Social Diffusion Predictor  uses unbiased user embeddings to jointly optimize diffusion prediction accuracy and information diversity. (4)  Targeted Interventions  strategically reshapes information flows to disrupt echo chambers based on the generated prediction and diversity insights. Extensive experiments demonstrate CEDA’s superior performance in both predicting information diffusion patterns and mitigating echo chambers.}
}


@article{DBLP:journals/tois/JiangLLLW25,
	author = {Wenjun Jiang and
                  Song Li and
                  Xueqi Li and
                  Kenli Li and
                  Jie Wu},
	title = {Uncovering Recommendation Serendipity with Objective Data-driven Factor
                  Investigation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {164:1--164:33},
	year = {2025},
	url = {https://doi.org/10.1145/3758092},
	doi = {10.1145/3758092},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/JiangLLLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The serendipity recommendation tries to burst the filter bubble while still meeting user interests. However, serendipity itself has not been well understood in the recommendation system. Thus, factor investigation in recommendation serendipity has attracted much attention, for which two challenges hinder follow-up research: (1)  Ambiguity of factors . Different works exploit different factors, and the meanings of factors are inconsistent in various works. (2)  Lack of complete impact validation . The importance of these factors in different domains is not yet fully understood. The common approach of user surveys costs much, but the results are usually less objective and limited in quantity. To this end, we strive to comprehensively identify and clarify serendipity factors and explore objective data-driven approaches to validate factor impacts in large-scale cross-domain scenarios. We first conduct a comprehensive literature review to identify all possible factors, from which we find that some factors are being used indistinguishably. To address this issue, we propose two principles of meaning coverage and factor independence to clarify and disentangle serendipity factors. Next, we propose a general experimental framework to explore the impacts of factors. Then, we implement one such framework and run experiments on nine representative datasets to study factor importance on serendipity. We also propose a quantitative method to measure the degree of disentanglement of factors and to test the effects of factor combinations. We gain several useful findings: (1)  relevance ,  diversity , and  random  are critical factors affecting serendipity; (2) domain features affect factor importance and can guide serendipity recommendation; (3) the disentanglement quantification method benefits the understanding of serendipity and the combination of factors. To our knowledge, this is the first work to comprehensively investigate serendipity factors and experimentally compare their impacts in an objective data-driven approach.}
}


@article{DBLP:journals/tois/ChenWCGLW25,
	author = {Jiajia Chen and
                  Jiancan Wu and
                  Jiawei Chen and
                  Chongming Gao and
                  Yong Li and
                  Xiang Wang},
	title = {Position-aware Graph Transformer for Recommendation},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {165:1--165:24},
	year = {2025},
	url = {https://doi.org/10.1145/3757736},
	doi = {10.1145/3757736},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/ChenWCGLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative recommendation fundamentally involves learning high-quality user and item representations from interaction data. Recently, graph convolution networks (GCNs) have advanced the field by utilizing high-order connectivity patterns in interaction graphs, as evidenced by state-of-the-art methods like PinSage and LightGCN. However, one key limitation has not been well addressed in existing solutions: capturing long-range collaborative filtering signals, which are crucial for modeling user preference. In this work, we propose a new graph transformer (GT) framework— Position-aware Graph Transformer for Recommendation  (PGTR), which combines the global modeling capability of Transformer blocks with the local neighborhood feature extraction of GCNs. The key insight is to explicitly incorporate node position and structure information from the user-item interaction graph into GT architecture via several purpose-designed positional encodings. The long-range collaborative signals from the Transformer block are then combined linearly with the local neighborhood features from the GCN backbone to enhance node embeddings for final recommendations. Empirical studies demonstrate the effectiveness of the proposed PGTR method when implemented on various GCN-based backbones across four real-world datasets and the robustness against interaction sparsity as well as noise. Our implementations are available in GitHub:  https://github.com/MEICRS/PGTR .}
}


@article{DBLP:journals/tois/BarsamianC25,
	author = {Yann Barsamian and
                  Andr{\'{e}} Chailloux},
	title = {Compressing Integer Lists with Contextual Arithmetic Trits},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {166:1--166:29},
	year = {2025},
	url = {https://doi.org/10.1145/3749098},
	doi = {10.1145/3749098},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/BarsamianC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inverted indexes allow to query large databases without needing to search in the database at each query. An important line of research is to construct inverted indexes that require a rather small space usage while still allowing low timings for compression, decompression, and queries. In this article, we show how to use trit encoding, combined with contextual methods for computing inverted indexes. We perform an extensive study of different variants of these methods and show that our method consistently outperforms the Binary Interpolative Method—which is one of the golden standards in this topic—with respect to compression size. We apply our methods to a variety of datasets and make available the source code that produced the results, together with all our datasets.}
}


@article{DBLP:journals/tois/MoMZQCCLZDN25,
	author = {Fengran Mo and
                  Kelong Mao and
                  Ziliang Zhao and
                  Hongjin Qian and
                  Haonan Chen and
                  Yiruo Cheng and
                  Xiaoxi Li and
                  Yutao Zhu and
                  Zhicheng Dou and
                  Jian{-}Yun Nie},
	title = {A Survey of Conversational Search},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {167:1--167:50},
	year = {2025},
	url = {https://doi.org/10.1145/3759453},
	doi = {10.1145/3759453},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/MoMZQCCLZDN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a cornerstone of modern information access, search engines have become indispensable in everyday life. With the rapid advancements in AI and natural language processing (NLP) technologies, particularly large language models (LLMs), search engines have evolved to support more intuitive and intelligent interactions between users and systems. Conversational search, an emerging paradigm for next-generation search engines, leverages natural language dialogue to facilitate complex and precise information retrieval, thus attracting significant attention. Unlike traditional keyword-based search engines, conversational search systems enhance user experience by supporting intricate queries, maintaining context over multi-turn interactions, and providing robust information integration and processing capabilities. Key components such as query reformulation, search clarification, conversational retrieval, and response generation work in unison to enable these sophisticated interactions. In this survey, we explore the recent advancements and potential future directions in conversational search, examining the critical modules that constitute a conversational search system. We highlight the integration of LLMs in enhancing these systems and discuss the challenges and opportunities that lie ahead in this dynamic field. Additionally, we provide insights into real-world applications and robust evaluations of current conversational search systems, aiming to guide future research and development in conversational search.}
}


@article{DBLP:journals/tois/DengRZC25,
	author = {Yang Deng and
                  Zifeng Ren and
                  An Zhang and
                  Tat{-}Seng Chua},
	title = {Towards Goal-oriented Intelligent Tutoring Systems in Online Education},
	journal = {{ACM} Trans. Inf. Syst.},
	volume = {43},
	number = {6},
	pages = {168:1--168:26},
	year = {2025},
	url = {https://doi.org/10.1145/3760401},
	doi = {10.1145/3760401},
	timestamp = {Fri, 26 Dec 2025 20:52:41 +0100},
	biburl = {https://dblp.org/rec/journals/tois/DengRZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interactive Intelligent Tutoring Systems (ITSs) enhance the learning experience in online education by fostering effective learning through interactive problem-solving. However, many current ITS models do not fully incorporate proactive engagement strategies that optimize educational resources through thoughtful planning and assessment. In this work, we propose a novel and practical task of Goal-oriented Intelligent Tutoring Systems (GITS), designed to help students achieve proficiency in specific concepts through a tailored sequence of exercises and evaluations. We introduce a novel graph-based reinforcement learning framework, named Planning-Assessment-Interaction ( PAI ), to tackle the challenges of goal-oriented policy learning within GITS. This framework utilizes cognitive structure information to refine state representation and guide the selection of subsequent actions, whether that involves presenting an exercise or conducting an assessment. Additionally,  PAI  employs a cognitive diagnosis model that dynamically updates to predict student reactions to exercises and assessments. We construct three benchmark datasets covering different subjects to facilitate offline GITS research. Experimental results validate  PAI ’s effectiveness and efficiency, and we present comprehensive analyses of its performance with different student types, highlighting the unique challenges presented by this task.}
}
