@article{DBLP:journals/compsec/Repetto25,
	author = {Matteo Repetto},
	title = {Otupy: {A} flexible, portable, and extensible framework for remote
                  control of security functions},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104597},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104597},
	doi = {10.1016/J.COSE.2025.104597},
	timestamp = {Tue, 14 Oct 2025 19:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/Repetto25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing proliferation of heterogeneous security functions ensures diversity, robustness, and adaptivity in addressing cyber-threats, but also poses management and integration challenges. OpenC2 defines a vendor- and application-agnostic abstract language for remote command and control of cyber-defense technologies. Its architecture supports multiple encoding and transfer options, but this might complicate its implementation and usage. This paper describes Otupy, a flexible and extensible implementation of the OpenC2 language specification. Otupy defines an Application Programming Interface (API) that allows programmers to focus on the control and business logic of security functions, rather than the communication syntax, protocol, and encoding. The design of Otupy leverages an abstract data notation, an inheritance model, and meta-serialization to simplify the development of extensions for specific  profiles  of security functions, as well as additional encoding and transfer protocols. We evaluate the correctness of our implementation by validating its output against both a syntax schema and external good and bad samples provided by a third party. Our analysis points out unclear and ambiguous aspects of OpenC2 that deserve further attention by its technical committee.}
}


@article{DBLP:journals/compsec/VarshneyRSAMG25,
	author = {Gaurav Varshney and
                  Akanksha Raj and
                  Divya Sangwan and
                  Sharif Abuadbba and
                  Rina Mishra and
                  Yansong Gao},
	title = {A login page transparency and visual similarity-based zero-day phishing
                  defense protocol},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104598},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104598},
	doi = {10.1016/J.COSE.2025.104598},
	timestamp = {Wed, 15 Oct 2025 19:23:20 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/VarshneyRSAMG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phishing is a prevalent cyberattack that uses look-alike websites to deceive users into revealing sensitive information. Numerous efforts have been made by the Internet community and security organizations to detect, prevent, or train users to avoid falling victim to phishing attacks. Most of this research over the years has been highly diverse and application-oriented, often serving as standalone solutions for HTTP clients, servers, or third parties. However, limited work has been done to develop a comprehensive or proactive protocol-oriented solution to effectively counter phishing attacks. Inspired by the concept of certificate transparency, which allows certificates issued by Certificate Authorities (CAs) to be publicly verified by clients, thereby enhancing transparency, we propose a concept called Page Transparency (PT) for the web. The proposed PT requires login pages that capture users’ sensitive information to be publicly logged via PLS and made available to web clients for verification. The pages are verified to be logged using cryptographic proofs. Since all pages are logged on a PLS and visually compared with existing pages through a comprehensive visual page-matching algorithm, it becomes impossible for an attacker to register a deceptive look-alike page on the PLS and receive the cryptographic proof required for client verification. All implementations occur on the client side, facilitated by the introduction of a new HTTP PT header, eliminating the need for platform-specific changes or the installation of third-party solutions for phishing prevention.}
}


@article{DBLP:journals/compsec/ShamimABPAZ25,
	author = {Nouman Shamim and
                  Muhammad Asim and
                  Thar Baker and
                  Zeeshan Pervez and
                  Ali Ismail Awad and
                  Albert Y. Zomaya},
	title = {Integrating system calls and position-specific scoring for enhanced
                  anomaly detection in Internet of Things environments},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104613},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104613},
	doi = {10.1016/J.COSE.2025.104613},
	timestamp = {Sun, 02 Nov 2025 21:29:30 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ShamimABPAZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying attacks on Internet of Things (IoT) systems through anomaly detection is an effective approach and remains a crucial area of research. The core method involves collecting system-related data during normal operation to establish a baseline of typical behavior and then continuously monitoring for deviations from this baseline. Using system call sequences for anomaly detection is a well-established and important field. System call sequences effectively capture the behavior of a target system at a low level, allowing identification of any changes in this behavior; however, these approaches face several challenges, including high false-positive rates, the need for segmentation of long sequences, and the difficulty of detecting anomalies when the system call data comes from multiple processes. This work presents a novel anomaly-detection approach that uses a position-specific scoring mechanism to analyze the content and structural properties of system call sequences. The proposed approach addresses key challenges in this field, including fixed-length segmentation of system call sequences, predetermined anomaly-detection thresholds, the detection of anomalies in both single and multiple processes, and high false-positive rates. We extensively evaluated the proposed approach using system-call-specific public datasets (ADFA-LD and UNM) of a diverse nature. The performance of the proposed content-based, structure-based, and combined content- and structure-based anomaly-detection methods was evaluated using ten-fold cross-validation. The proposed anomaly-detection approach achieves an impressive detection rate of 1.0, along with exceptionally low false-positive rates of 0.001 and 0.017 when evaluated on the UNM and ADFA-LD datasets, respectively.}
}


@article{DBLP:journals/compsec/JaniszewskiS25,
	author = {Marek Janiszewski and
                  Krzysztof Szczypiorski},
	title = {Methodological reliability evaluation of trust and reputation management
                  systems},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104620},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104620},
	doi = {10.1016/J.COSE.2025.104620},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/JaniszewskiS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trust and Reputation Management (TRM) systems are used in various environments, and their main goal is to ensure efficiency despite malicious or unreliable agents striving to maximize their usefulness or to disrupt the operations of other agents. However, TRM systems can be targeted by specific attacks, which can reduce the efficiency of the environment. The impact of such attacks on a specific system cannot be easily anticipated and evaluated. The article presents models of the environment and the TRM system operating in the environment. On that basis, measures of the reliability of TRM systems were defined to enable a comprehensive and quantitative evaluation of the resistance of such systems to attacks. The presented methodology is then used to evaluate an example TRM system (RefTRM), through the created and briefly described tool TRM-RET (Trust and Reputation Management – Reliability Evaluation Testbed). The results indicate that the system's specific properties can be indicated on the basis of the tests and metrics proposed; for example, the RefTRM system is quite vulnerable to an attack tailored to the parameters used by this system.}
}


@article{DBLP:journals/compsec/ZhaiMZZY25,
	author = {Yuqi Zhai and
                  Rui Ma and
                  Zheng Zhang and
                  Siqi Zhao and
                  Yuche Yang},
	title = {MSNFuzz: Multi-criteria state-sensitive network protocol fuzzing},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104621},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104621},
	doi = {10.1016/J.COSE.2025.104621},
	timestamp = {Tue, 14 Oct 2025 19:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/ZhaiMZZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing protocol fuzzing techniques suffer a lot from lacking state guidance on seed evaluation during seed selection and energy allocation. That reduces fuzzing efficiency and effectiveness. We thus conduct a research focusing on seed evaluation in grey-box protocol fuzzing and propose a multi-criteria state-sensitive network protocol fuzzing method named MSNFuzz. To improve seed evaluation, we firstly re-think and re-evaluate seed potential in protocol fuzzing and improve the evaluation by introducing fine-grained state-sensitive criteria. Based on the multi-criteria evaluation, a probability-based greedy algorithm is adopted to prioritize selecting promising seeds to better explore the state space of the protocol. Moreover, we also assign different mutation energies for seeds based on the occurrence frequency of its corresponding state to be selected. That allows for flexible adjustment of mutation energy. We further evaluate the performance of MSNFuzz by comparing with AFLNET, AFLNWE, StateAFL and NSFuzz, on 13 typical protocol programs from ProFuzzBench. The experimental results show that MSNFuzz discovers 17.7%, 57.7% and 30.0% more paths, 52.4%, 123.6% and 71.0% more crashes than AFLNET, AFLNWE, and StateAFL on average, and discovers 0.18% more paths and 1.8% less crashes than NSFuzz, which is the state-of-the-art but relatively heavy solution. Besides, MSNFuzz discovers 22.1% more states and 16.5% state transitions than AFLNET on average. That highlights MSNFuzz could improve the efficiency and effectiveness of fuzzing.}
}


@article{DBLP:journals/compsec/ChenYHW25,
	author = {Zhenyu Chen and
                  Lin Yao and
                  Haibo Hu and
                  Guowei Wu},
	title = {Points of the local optimal privacy utility tradeoff},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104622},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104622},
	doi = {10.1016/J.COSE.2025.104622},
	timestamp = {Thu, 06 Nov 2025 14:15:11 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ChenYHW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing prevalence of data sharing and publishing, striking a balance between data privacy and data utility, known as the privacy utility tradeoff problem, has emerged as a core challenge. Recent studies treat this tradeoff as an optimization process within the privacy protection process for certain privacy protection mechanism. However, the ability to achieve an optimal tradeoff is inherently constrained by the chosen privacy protection mechanism. In this paper, we provide a new perspective by conceptualizing the privacy utility tradeoff as a series of distinct “tradeoff points,” where the inference privacy and inference utility serve as the components to represent a tradeoff point. To identify local optimal tradeoff points, we first select those that maximize utility for a given level of privacy. Then, we discard those points that do not ensure optimal privacy for the corresponding utility. Simulations on four real-world datasets using three state-of-the-art methods demonstrate that existing tradeoff solutions are limited by their underlying privacy mechanisms, while our solution helps integrate local optimal tradeoff points into the design of privacy protection mechanisms.}
}


@article{DBLP:journals/compsec/WangGSYLF25,
	author = {Xiao Wang and
                  Yunchuan Guo and
                  Zhe Sun and
                  Mingjie Yu and
                  Fenghua Li and
                  Liang Fang},
	title = {OPMonitor: Continuously monitoring residual over-granted permissions
                  in verified access control policies},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104623},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104623},
	doi = {10.1016/J.COSE.2025.104623},
	timestamp = {Wed, 04 Feb 2026 16:41:01 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/WangGSYLF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over-permissive access control policies, which grant users permissions beyond sysadmins’ intended scope, are a primary cause of data breaches. Although policy verification serves as a critical defense mechanism by formalizing design intentions into verification goals and validating policies compliance with these goals, its effectiveness bounded by sysadmins’ expertise and the comprehensiveness of predefined intentions. Consequently, over-granted permissions which fall outside the scope of verification goals often remain undetected. This paper introduces  OPMonitor , a continuous monitoring tool that enables early detection of residual over-granted permissions overlooked by policy verification methods.  OPMonitor  operates by inferring a granting baseline from access logs, which serves as a reference model for identifying access violations in real time. To mitigate over-permissive results while ensuring correctness, we develop a two-phase framework based on approximate optimization for baseline inference. To facilitate real-time evaluation and incremental updates of the inferred baseline, we develop the locally abstract baseline tree, a tree structure that consolidates implicit authorization conditions to reduce the scale of states. Our experimental evaluation across 25 datasets, comprising both real-world and synthetic data, demonstrates the effectiveness of our approach.  OPMonitor  achieves a 1.5x higher detection rate for over-granted permissions compared to state-of-the-art solutions, while keeping the inference time under 30 s. Additionally, our locally abstract baseline tree enables microsecond-level evaluation and incremental updates that are 7x and 2x faster, respectively, than existing approaches.}
}


@article{DBLP:journals/compsec/ShuCFYHL25,
	author = {Congyan Shu and
                  Wentao Chen and
                  Guisheng Fan and
                  Huiqun Yu and
                  Zijie Huang and
                  Yuguo Liang},
	title = {Tool or Toy: Are {SCA} tools ready for challenging scenarios?},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104624},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104624},
	doi = {10.1016/J.COSE.2025.104624},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ShuCFYHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of open-source software (OSS) has introduced new security challenges to the software supply chain. While existing studies confirm the basic capabilities of Software Composition Analysis (SCA) tools, such as vulnerability detection and dependency resolution. They often focus on single ecosystems or detection aspects. This limited scope overlooks real-world complexities, including multi-language ecosystems, source and binary dependencies, and adversarial threats. Without a comprehensive evaluation, SCA tools may perform well in controlled settings but struggle in more complex scenarios. To address this gap, this study proposes a evaluation framework centered on the core functionalities of SCA tools: dependency detection, vulnerability identification, and license inspection. It covers three key dimensions including multi-language ecosystems compatibility, build forms, and attack defense. Using standardized datasets and quantitative metrics, such as precision, recall, F1-score and standard deviation, we evaluate four representative SCA tools, including both open-source and commercial options. Results reveal significant limitations in binary dependencies, language coverage, and license consistency. SCA tools also face challenges in balancing precision, coverage and robustness. The study highlights systemic shortcomings in current SCA tools, revealing that many perform like limited-use toys under real-world conditions. It offers data-driven recommendations to guide the evolution of these tools into practical, reliable solutions for supply chain security governance.}
}


@article{DBLP:journals/compsec/FernandesCA25,
	author = {Pedro Fernandes and
                  S{\'{e}}amus {\'{O}} Ciardhu{\'{a}}in and
                  M{\'{a}}rio Antunes},
	title = {Distance-based feature selection using Benford's law for malware
                  detection},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104625},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104625},
	doi = {10.1016/J.COSE.2025.104625},
	timestamp = {Tue, 14 Oct 2025 19:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/FernandesCA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting malware in computer networks and data streams from Android devices remains a critical challenge for cybersecurity researchers. While machine learning and deep learning techniques have shown promising results, these approaches often require large volumes of labelled data, offer limited interpretability, and struggle to adapt to sophisticated threats such as zero-day attacks. Moreover, their high computational requirements restrict their applicability in resource-constrained environments. This research proposes an innovative approach that advances the state of the art by offering practical solutions for dynamic and data-limited security scenarios. By integrating natural statistical laws, particularly Benford’s law, with dissimilarity functions, a lightweight, fast, and scalable model is developed that eliminates the need for extensive training and large labelled datasets while improving resilience to data imbalance and scalability for large-scale cybersecurity applications. Although Benford’s law has demonstrated potential in anomaly detection, its effectiveness is limited by the difficulty of selecting relevant features. To overcome this, the study combines Benford’s law with several distance functions, including Median Absolute Deviation, Kullback–Leibler divergence, Euclidean distance, and Pearson correlation, enabling statistically grounded feature selection. Additional metrics, such as the Kolmogorov test, Jensen–Shannon divergence, and Z statistics, were used for model validation. This approach quantifies discrepancies between expected and observed distributions, addressing classic feature selection challenges like redundancy and imbalance. Validated on both balanced and unbalanced datasets, the model achieved strong results: 88.30% accuracy and 85.08% F1-score in the balanced set, 92.75% accuracy and 95.29% F1-score in the unbalanced set. The integration of Benford’s law with distance functions significantly reduced false positives and negatives. Compared to traditional Machine Learning methods, which typically require extensive training and large datasets to achieve F1 scores between 92% and 99%, the proposed approach delivers competitive performance while enhancing computational efficiency, robustness, and interpretability. This balance makes it a practical and scalable alternative for real-time or resource-constrained cybersecurity environments.}
}


@article{DBLP:journals/compsec/Beltran25,
	author = {Marta Beltr{\'{a}}n},
	title = {{AI} algorithms under scrutiny: GDPR, DSA, {AI} Act and {CRA} as pillars
                  for algorithmic security and privacy in the European Union},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104628},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104628},
	doi = {10.1016/J.COSE.2025.104628},
	timestamp = {Tue, 14 Oct 2025 19:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/Beltran25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The General Data Protection Regulation (GDPR), Digital Services Act (DSA), Artificial Intelligence Act (AI Act) and Cyber Resilience Act (CRA) are essential pillars for algorithmic security and privacy in the European Union. Each of these regulations addresses specific aspects of technology, such as personal data protection, trustworthy online services, safe AI systems, and secure digital products while fostering trust in algorithm-based systems. Together, they can establish a robust framework for ensuring the security and privacy of AI algorithms in the EU by addressing critical concerns through a risk-based approach. This paper proposes a multi-layered approach to algorithmic security and privacy, based on these four instruments, considering organisational risk, risks to rights and freedoms, systemic risks and risks to national security. An illustrative example demonstrates how the EU can establish a global standard for trustworthy innovation and the protection of fundamental rights by leveraging the direct and indirect synergies of these laws.}
}


@article{DBLP:journals/compsec/AlmutairiKH25,
	author = {Amirah Almutairi and
                  Boojoong Kang and
                  Nawfal Al Hashimy},
	title = {Business email compromise: {A} systematic review of understanding,
                  detection, and challenges},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104630},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104630},
	doi = {10.1016/J.COSE.2025.104630},
	timestamp = {Sun, 01 Feb 2026 13:35:38 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AlmutairiKH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Business Email Compromise (BEC) is a widespread fraud targeting businesses and individuals to obtain financial benefits and gain access to highly sensitive data. BEC fraud significantly impacts almost all organizations worldwide, resulting in substantial losses. Despite its prevalence, there is a shortage of research on understanding and protecting against this fraud. Consequently, this paper aims to survey existing BEC detection techniques. It first provides an overview of the methods and strategies used by attackers in BEC schemes. It also reviews existing BEC detection and prevention techniques, including both technical and non-technical solutions. The strengths of each technique are objectively discussed, and their limitations are critically analyzed. Finally, this study offers a thorough set of current challenges in BEC detection and outlines future research directions, providing valuable guidance for improving security measures against BEC fraud.}
}


@article{DBLP:journals/compsec/LiCZYG25,
	author = {Degang Li and
                  Xi Chen and
                  Mingliang Zhu and
                  Qingjun Yuan and
                  Chunxiang Gu},
	title = {A fine-grained message clustering method based on message representation
                  and identifier fingerprints},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104631},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104631},
	doi = {10.1016/J.COSE.2025.104631},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/LiCZYG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Protocol reverse engineering is a critical technique for analyzing private protocols and unknown protocols. Message clustering is a foundational element of protocol reverse engineering, playing a key role in traffic classification and format inference. In this paper, we propose a fine-grained unknown message clustering method, termed FG-MCRF. FG-MCRF extracts deep representation vectors from the raw message data by constructing a representation network with low information loss and constructs high-purity message clusters based on representation vectors. The FG-MCRF method constructs high-precision global message fingerprints for each message cluster based on message length identifiers, operation identifiers, and counter identifiers. Subsequently, FG-MCRF constructs a message relationship graph based on these global message fingerprints and determines the final message type using the relationship graph. We also introduce the fine-grained multi-protocol dataset (FgMPD) to evaluate the clustering performance of our method. The experimental results demonstrate that the FG-MCRF methodology achieves superior clustering performance on the FgMPD dataset, outperforming other baseline methods. The clustering purity, Adjusted Rand Index (ARI), completeness, and accuracy of FG-MCRF in the fine-grained message clustering task are 0.9961, 0.9897, 0.9837, and 0.9899, respectively, representing improvements of 3.2%, 10.5%, 10.9% and 8.7% compared to state-of-the-art (SOTA) baseline methods. These results indicate that the FG-MCRF method possesses robust generalization capacity and extensibility, facilitating fine-grained message clustering.}
}


@article{DBLP:journals/compsec/WenWL25,
	author = {Zhaoyu Wen and
                  Zhiqiang Wang and
                  Biao Liu},
	title = {VERTFuzz: Version transformer-driven fuzzing for complex file parsers},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104641},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104641},
	doi = {10.1016/J.COSE.2025.104641},
	timestamp = {Mon, 22 Sep 2025 21:08:47 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/WenWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fuzzing test technology has seen significant growth in recent years and has evolved into an important tool for more thoroughly and efficiently identifying programme vulnerabilities and defects. However, fuzzing test for complex format files remains challenging. Most fuzz testers require extensive expert knowledge and heavily rely on manually constructed format models, or struggle to accurately identify complex structural relationships, resulting in numerous invalid test variants. In this paper, we propose a metadata-based mutation technique that leverages deep learning models to identify metadata location information and incorporate it into specific mutations, enabling rapid identification of file structures. We also utilise the Version Transformer model to filter out valid test cases from the queue, effectively addressing the issue of sparse defect space in input, making the mutated test cases more effective. Experimental results show that VERTFuzz has identified 32 unique errors across ten different programs, including four complex file formats. On average, VERTFuzz discovered 29% more paths and 14.54% more code blocks than AFL++.}
}


@article{DBLP:journals/compsec/AnleyCGP25,
	author = {Mulualem Bitew Anley and
                  Pasquale Coscia and
                  Angelo Genovese and
                  Vincenzo Piuri},
	title = {{FELACS:} Federated learning with adaptive client selection for IoT
                  DDoS attack detection},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104642},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104642},
	doi = {10.1016/J.COSE.2025.104642},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/AnleyCGP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed denial-of-service (DDoS) attacks pose a significant threat to network security by overwhelming systems with malicious traffic, leading to service disruptions and potential data breaches. The traditional centralized machine learning (ML) methods for detecting DDoS attacks in Internet of Things (IoT) environments raise privacy and security concerns due to their collection and distribution of data to a central entity that may not be trusted to perform model training. Federated learning (FL) offers a privacy-preserving solution that enables distributed collaboration by training a model only on local clients, without data exchanges, where the central entity only performs global model aggregation. However, the current practice of random client selection, combined with the statistical heterogeneity of client data and the device heterogeneity encountered in IoT environments, requires many training rounds to reach optimal accuracy, increasing the imposed computational overhead. To address these challenges, we propose a multiobjective optimization-based FL with adaptive client selection (FELACS) approach that maximizes client importance scores while satisfying resource, performance, and data diversity constraints. Experiments are carried out on the CIC-IDS2018, CIC-DDoS2019, BoT-IoT, and CIC-IoT2023 datasets, demonstrating that FELACS improves upon the accuracy of the existing approaches while exhibiting increased convergence speed when training a model in an FL scenario, hence reducing the number of communication rounds required to achieve the target accuracy, making it highly effective for performing IoT-based DDoS attack detection in FL scenarios.}
}


@article{DBLP:journals/compsec/YuanZC25,
	author = {Yali Yuan and
                  Weiyi Zou and
                  Guang Cheng},
	title = {Attack smarter: Attention-driven fine-grained webpage fingerprinting
                  attacks},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104643},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104643},
	doi = {10.1016/J.COSE.2025.104643},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/YuanZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Website Fingerprinting (WF) attacks aim to infer which websites a user is visiting by analyzing traffic patterns, thereby compromising user anonymity. Although this technique has been demonstrated to be effective in controlled experimental environments, it remains largely limited to small-scale scenarios, typically restricted to recognizing website homepages. In practical settings, however, users frequently access multiple subpages in rapid succession, often before previous content fully loads. WebPage Fingerprinting (WPF) generalizes the WF framework to large-scale environments by modeling subpages of the same site as distinct classes. These pages often share similar page elements, resulting in lower inter-class variance in traffic features. Furthermore, we consider multi-tab browsing scenarios, in which a single trace encompasses multiple categories of webpages. This leads to overlapping traffic segments, and similar features may appear in different positions within the traffic, thereby increasing the difficulty of classification. To address these challenges, we propose an attention-driven fine-grained WPF attack, named ADWPF. Specifically, during the training phase, we apply targeted augmentation to salient regions of the traffic based on attention maps, including attention cropping and attention masking. ADWPF then extracts low-dimensional features from both the original and augmented traffic and applies self-attention modules to capture the global contextual patterns of the trace. Finally, to handle the multi-tab scenario, we employ the residual attention to generate class-specific representations of webpages occurring at different temporal positions. Extensive experiments demonstrate that the proposed method consistently surpasses state-of-the-art baselines across datasets of different scales. Notably, under a challenging setting involving 1,000 monitored webpages, our model achieved a 50.54% mAP and 63.85% Recall@5.}
}


@article{DBLP:journals/compsec/DengXZZ25,
	author = {Qiqing Deng and
                  Zhen Xu and
                  Qihui Zhou and
                  Yan Zhang},
	title = {Cordon: Enhancing security through kernel-level control in containerized
                  computing environments},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104644},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104644},
	doi = {10.1016/J.COSE.2025.104644},
	timestamp = {Mon, 22 Sep 2025 21:08:47 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/DengXZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containers have become a foundational technology across a variety of computing environments, enabling an era of agility, efficiency, and scalability due to their inherent advantages. Simultaneously, containers confront escalating security threats, with vulnerabilities being exploited to compromise host machines and broaden attack impacts. Existing security mechanisms predominantly rely on host-based mandatory access control, which contradicts the autonomy and flexibility requirements of dynamic and scalable containerized computing environments. This paper introduces Cordon, a novel framework aimed at providing autonomous and flexible control management within the context of containerized computing, effectively addressing the limitations of existing security mechanisms. Cordon is designed to counter common attack vectors in containerized environments by implementing file access control, capability management, and system call interception, thereby enabling comprehensive container-aware security enforcement at the kernel level. Furthermore, Cordon supports multi-container management, enabling the application of security policies across various dimensions of container resources, a feature that allows for the batch security management of containers of the same type, such as multiple container instances deployed under the same Kubernetes deployment. We develop a prototype implementation of Cordon and evaluate its effectiveness, generality, and performance overhead. Our evaluation demonstrates that Cordon effectively blocks various container attacks while maintaining acceptable overhead.}
}


@article{DBLP:journals/compsec/CarneiroRMSLA25,
	author = {F{\'{a}}bio Dias Carneiro and
                  Izabela Simon Rampasso and
                  Sidney Luiz de Matos Mello and
                  Tiago F. A. C. Sigahi and
                  Hern{\'{a}}n Lespay and
                  Rosley Anholon},
	title = {A flexible {ISO} 27701-based framework for assessing cybersecurity
                  maturity: a proposition and a case application},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104645},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104645},
	doi = {10.1016/J.COSE.2025.104645},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/CarneiroRMSLA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study aims to propose a framework for assessing the cybersecurity management level of organizations, based on ISO 27701. To illustrate the proposed framework, and considering the relevance of cybersecurity for Higher Education Institutions (HEIs), an analysis of the reality of Federal HEIs in Brazil is conducted. To develop the proposed framework, the standard ISO 27701 was used to structure a questionnaire. The proposed data analysis combines Hierarchical Cluster Analysis (HCA), frequency analysis, and Fuzzy TOPSIS. The case application considered experts in information security of Federal HEIs in Brazil. The proposed framework presents eight steps: definition of the application focus, analysis of variables and scale proposed, questionnaire structuring, ethics committee submission, data gathering, HCA, frequency analysis, Fuzzy TOPSIS. Regarding the case application, aspects related to internal auditing, asset management and human resources training and analysis were the most critical. This study presents a comprehensive framework for guiding information security assessment in organizations. The proposed framework presents the necessary flexibility to be adjusted according to the requirements of practitioners and researchers. It can be used by companies and the government to assess their current reality and evaluate the impact of changes performed. Researchers can integrate the proposed framework into an Artificial Intelligence mechanism for risk prediction in organizations. The findings from the case application evidence the contributions of this framework to assess the reality of any kind of institution and highlight the insights that can be obtained from its analysis.}
}


@article{DBLP:journals/compsec/ChenZMLZ25,
	author = {Zigang Chen and
                  Hongwei Zhang and
                  Qinyu Mu and
                  Danlong Li and
                  Haihua Zhu},
	title = {In-vehicle device data tampering detection: Accurate identification
                  based on correlation calculation and data relationship},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104648},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104648},
	doi = {10.1016/J.COSE.2025.104648},
	timestamp = {Mon, 22 Sep 2025 21:08:47 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/ChenZMLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of intelligent connected vehicles (ICVs), driven by the integration of AI and 5G, has intensified the need for reliable accident forensics. We present a novel correlation analysis-based method for detecting tampered vehicle electronic data, addressing critical security vulnerabilities in current systems. Our approach establishes multivariate relationship clusters from in-vehicle data characteristics, performs dimensionality reduction, and computes anomaly scores through tail probability analysis. The experimental results demonstrate that the proposed method exhibits superior detection performance compared to existing approaches for random injection attacks, targeted tampering attacks, and outlier attacks.}
}


@article{DBLP:journals/compsec/ChenPS25,
	author = {Shengshan Chen and
                  Tun{-}Wen Pai and
                  Chin{-}Yu Sun},
	title = {EnhanceCTI: Enhanced semantic filtering and feature extraction framework
                  for industry-specific cyber threat intelligence},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104649},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104649},
	doi = {10.1016/J.COSE.2025.104649},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/ChenPS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid digitization of various industries has created an urgent need for robust cyber threat intelligence (CTI) systems. Organizations are increasingly developing cyber threat intelligence platforms (TIPs) to gather open-source intelligence (OSINT) and transform it into actionable defenses against information security breaches. However, the overwhelming volume and complexity of OSINT data, often including false or misleading information, pose significant challenges for effective CTI analysis. This study introduces EnhanceCTI, a novel system designed to improve the quality and industry-specific applicability of threat intelligence. EnhanceCTI employs an enhanced bidirectional encoder representations from transformers (DistilBERT)-based semantic filtering method to filter intelligence data and determine its alignment with industry-specific data extracted from TIPs. This filtering is applied across eight major industries: healthcare, finance, government, technology, education, telecommunications, critical infrastructure, and a miscellaneous “others” category. Additionally, EnhanceCTI leverages high-credibility CTI features, integrating them with SentenceBERT to create a merging judgment model. This model determines whether a given piece of intelligence should be merged with existing data or stored independently, thereby ensuring relevance and minimizing redundancy. Finally, a dedicated platform was developed, providing cybersecurity analysts with tools to rapidly assess both intelligence quality and the accuracy of industry-specific classification models. Experimental results demonstrate EnhanceCTI’s effectiveness, achieving an F1-score of 0.99 for intelligence identification and a 0.89 cosine Pearson correlation for SentenceBERT. A random forest algorithm, trained on 750 manually annotated samples, achieved an F1-score of 0.97 on the merging judgment model. These findings highlight EnhanceCTI’s ability to accurately identify threats, offering a valuable, industry-tailored solution for institutions facing the growing challenges of cybersecurity in the modern digital landscape.}
}


@article{DBLP:journals/compsec/KarayelS25,
	author = {Tug{\c{c}}e Karayel and
                  Metin Saygili},
	title = {Understanding smartphone security behavior through the core constructs
                  of protection motivation theory: {A} comparative study of iOS and
                  android users},
	journal = {Comput. Secur.},
	volume = {158},
	pages = {104652},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104652},
	doi = {10.1016/J.COSE.2025.104652},
	timestamp = {Tue, 14 Oct 2025 19:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/KarayelS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study investigates the factors influencing smartphone users’ security behavior intentions through the lens of Protection Motivation Theory (PMT). Utilizing structural equation modeling (PLS-SEM), the research analyzes both a general sample and subgroups based on mobile operating systems (iOS and Android) to explore potential platform-based differences. The findings reveal that threat appraisal components—particularly perceived vulnerability—have a significant impact on users’ intentions to engage in mobile security behaviors. Similarly, coping appraisal components, including response efficacy and self-efficacy, are found to be strong predictors of behavioral intention. Interestingly, response cost shows a significant effect only among iOS users, suggesting that perceived burden varies by platform. These results highlight the importance of both cognitive evaluations and contextual factors such as operating systems in shaping users’ cybersecurity motivations. The study contributes to the literature by providing nuanced insights into the predictive power of PMT and offering empirical evidence of user segmentation in mobile security behavior.}
}
