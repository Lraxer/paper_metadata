@article{DBLP:journals/compsec/OranekwuEYK26,
	author = {Ikechukwu Oranekwu and
                  Lavanya Elluri and
                  Roberto Yus and
                  Anantaa Kotal},
	title = {Scalable automation for IoT cyberSecurity compliance: Ontology-driven
                  reasoning for real-time assessment},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104711},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104711},
	doi = {10.1016/J.COSE.2025.104711},
	timestamp = {Wed, 24 Dec 2025 10:44:27 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/OranekwuEYK26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the rapid expansion of the Internet of Things (IoT) has introduced significant cybersecurity challenges, requiring manufacturers to comply with various regulatory frameworks and cybersecurity standards. Hence, to protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter-Agency Report (NISTIR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines remains a significant challenge for companies. Previously, our Automated Knowledge Framework for IoT Cybersecurity Compliance, leveraged SWRL, SPARQL queries, Web Ontology Language and Visualization (OWL Viz), Semantic Web technologies, Large Language Models (LLMs), and Retrieval Augmented Generation (RAG) pipeline to automate compliance assessment of multiple Functional requirement documents (FRDs), while systematically cross-checking Business requirement documents (BRDs) against them [Oranekwu et al., 2024]. However, these efforts primarily focused on mapping NISTIR 8259 guidelines into a structured ontology laying the foundation for us to build, expand on, and then integrate the IoT Cybersecurity Improvement Act of 2020 into the compliance framework. Furthermore, exploring its big data capability, the Knowledge Graph (KG) has been expanded and populated with more than 800 manufacturer privacy policy instances, allowing direct comparison between manufacturer-defined data properties, object properties, and regulatory compliance expectations. The primary objective is to evaluate the effectiveness of this enhanced version of the framework in identifying policy non-compliance by comparing triples extracted from privacy policies against the structured knowledge representation. Through this approach, our goal is to automate compliance verification by examining the relationships between manufacturers, security requirements, and regulatory obligations, offering a scalable solution for the security governance of IoT.}
}


@article{DBLP:journals/compsec/JohnGS26,
	author = {John C. John and
                  Arobinda Gupta and
                  Shamik Sural},
	title = {Secure multi-cloud collaboration using data leakage free attribute-based
                  access control policies},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104736},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104736},
	doi = {10.1016/J.COSE.2025.104736},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/JohnGS26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an increase in the diversity and complexity of requirements from organizations for cloud computing, there is a growing need for integrating the services of multiple cloud providers. In such multi-cloud systems, data leakage is considered to be a major security concern, which is caused by illegitimate actions of malicious users often acting in collusion. The possibility of data leakage in such environments is characterized by the number of interoperations as well as the trustworthiness of users on the collaborating clouds. In this paper, we address the problem of secure multi-cloud collaboration from an Attribute-based Access Control (ABAC) policy management perspective. In particular, we define a problem that aims to formulate ABAC policy rules for establishing a high degree of inter-cloud accesses while eliminating potential paths for data leakage. A data leakage free ABAC policy generation algorithm is proposed that first determines the likelihood of data leakage and then attempts to maximize inter-cloud collaborations. We also pose several variants of the problem by imposing additional meaningful constraints on the nature of accesses. Experimental results on several large data sets show the efficacy of the proposed approach.}
}


@article{DBLP:journals/compsec/ArenasLRRSZ26,
	author = {M{\'{o}}nica P. Arenas and
                  Gabriele Lenzini and
                  Mohammadamin Rakeei and
                  Peter Y. A. Ryan and
                  Marjan Skrobot and
                  Maria Zhekova},
	title = {Secure authentication and traceability of physical objects},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104745},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104745},
	doi = {10.1016/J.COSE.2025.104745},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ArenasLRRSZ26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study how to authenticate objects, a problem that is relevant to buyers who seek proof that a purchase is authentic. Typically, manufacturers watermark their goods or assign them IDs with a certificate of authenticity; then, buyers can check for the presence of the watermark or verify the authenticity of the certificate, matching it with the good’s ID. However, this solution falls short when manufacturers and buyers are geographically separated, such as in retail or online purchases. Since certificates can be forged and goods can be substituted with substandard clones, buyers should verify the authenticity of the goods directly. This suggests a process: honest manufacturers should provide goods with an ID and securely register it along with some unforgeable and unique data that can be (re)generated only from the original physical object. In turn, buyers can verify whether the data registered under that ID matches the data retrieved by the buyer for the good just acquired. Such enrollment and authentication processes are complex when realized as protocols because they must withstand attacks against both the physical object and the communication channel. We propose a cyber-physical solution that relies on two elements: (i) a material inseparably joined with an object from which cryptographically strong digital identities can be generated; (ii) two novel cryptographic protocols that ensure data integrity and secure authentication of agents and objects. We present a comprehensive threat model for the artifact authenticity service. We also implemented and optimized the image processing pipeline, which takes under two seconds per image set, representing a notable improvement over previous versions.}
}


@article{DBLP:journals/compsec/RaubitzekSKFMEW26,
	author = {Sebastian Raubitzek and
                  Sebastian Schrittwieser and
                  Caroline K{\"{o}}nig and
                  Patrick Felbauer and
                  Kevin Mallinger and
                  Andreas Ekelhart and
                  Edgar R. Weippl},
	title = {Obfuscation detection using matrix complexity features of binary grayscale
                  images},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104746},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104746},
	doi = {10.1016/J.COSE.2025.104746},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/RaubitzekSKFMEW26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malware that conceals its behaviour through code obfuscation remains a central challenge for automated detection. This work introduced a novel approach for detecting the presence of obfuscation and identifying specific techniques. We transform binary code into grayscale images by mapping its bytes to a pixel intensity and apply singular value decomposition (SVD) to extract 18 matrix-complexity metrics that reflect structural changes introduced by an obfuscation. Using this approach, we evaluate eight Tigress obfuscation techniques on whether they leave a distinct spectral signature that can be classified. To obtain statistically robust results, we employ an ensemble of 100 independently tuned ExtraTrees models trained on different stratified 80/20 splits. The ensemble achieves average accuracies of 0.99 for detecting obfuscation, 0.94 for obfuscation type attribution, and 0.93 for identifying specific techniques. Feature-importance rankings and per-metric distribution plots make the results interpretable and transferable. The contributions of this study are (i) a reproducible pipeline for classifying obfuscated binaries, (ii) a detailed analysis of how obfuscation alters binary structure and its image representation, and (iii) actionable insight into which SVD metrics are most indicative of each transformation.}
}


@article{DBLP:journals/compsec/GhoshDAM26,
	author = {Arka Ghosh and
                  Domenico Ditale and
                  Massimiliano Albanese and
                  Preetam Mukherjee},
	title = {Optimizing {IDS} rule placement via set covering with capacity constraints},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104748},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104748},
	doi = {10.1016/J.COSE.2025.104748},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/GhoshDAM26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intrusion Detection Systems (IDSs) are essential for identifying and mitigating cyber threats in modern network infrastructures. Although prior work has extensively explored the optimal placement of IDS sensors across networks, optimizing the deployment of detection rules across multiple IDS instances remains a mostly underexplored area. This paper addresses rule deployment by formulating it as a set covering problem with capacity constraints. We seek to minimize the number of rule deployments required to detect potential exploits of all known vulnerabilities while ensuring that no IDS exceeds its inspection capacity. Our model considers the statistical properties of network traffic, enabling the system to account for load surges and reduce the number of packets not inspected by an IDS under high-traffic conditions, such as during Distributed Denial-of-Service attacks. To solve the optimization problem, we introduce a backtracking algorithm enhanced with a priority queue, which efficiently balances rule coverage and capacity constraints. We validate our approach using the CSE-CIC-IDS2017 dataset and a simulated multi-IDS environment. Experimental results demonstrate that our method significantly reduces the number of uninspected packets, while maximizing vulnerability coverage, and outperforms typical rule deployment strategies. This work highlights the critical role of intelligent rule placement in enhancing IDS performance and paves the way for future adaptive and scalable detection systems.}
}


@article{DBLP:journals/compsec/ShaikhJK26,
	author = {Faheem Ahmed Shaikh and
                  Damien Joseph and
                  Eugene Kang},
	title = {Reassessing information security perceptions following a data breach
                  announcement: The role of post-breach management in firm-specific
                  risk},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104752},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104752},
	doi = {10.1016/J.COSE.2025.104752},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ShaikhJK26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Public announcements of data breaches often lead to short-lived negative stock price reactions, raising questions about firms’ incentives for sustained cybersecurity improvements. This study applies legitimacy theory to examine how investor perceptions of a firm’s security practices—termed information security legitimacy—shape firm-specific risk after such announcements. Analyzing media sentiment following 485 U.S. data breach announcements, we find that firms with stronger information security legitimacy experience significantly lower firm-specific risk over six months. Additionally, shorter delays in public breach announcements strengthen this risk reduction. By linking data breach announcements with post-breach management, this study offers a unified framework showing how proactive security actions and timely communication mitigate long-term financial risk. These findings provide actionable guidance for security managers to prioritize rapid disclosure and strategic legitimacy management, advancing theory on stakeholder perceptions in cybersecurity.}
}


@article{DBLP:journals/compsec/LuLP26,
	author = {Chuan Lu and
                  Senlin Luo and
                  Limin Pan},
	title = {Dynamic soft isolation and restricted eviction for cache side channel
                  attack defense},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104753},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104753},
	doi = {10.1016/J.COSE.2025.104753},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LuLP26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cache side channel attack seriously threatens microarchitectural security. A key challenge in mitigating such attack lies in analyzing behavioral characteristics and intentions at different stages. Existing methods utilize static isolation domains to prevent data eviction between attackers and victims. The number of isolation domains is constrained by cache size, making protection insufficient processes when the number of protected processes exceeds this limit. Moreover, the capacity and location of isolation domains cannot be dynamically adjusted according to the process demand, leading to either underutilized cache lines or excessive evictions, both of which degrade performance. Therefore, a  D ynamic  S oft  I solation and  R estricted  E viction for Cache Side Channel Attack Defense (DSI-RE) is proposed. DSI-RE introduces a dynamic soft isolation method with flexible isolation boundaries, which dynamically adjusts the number, capacity and location of isolation domains based on process demands by domain labels, enhancing cache utilization and operational efficiency. Additionally, a restricted eviction with intent-aware is proposed, which detects the attack behavior across different attack stage, and imposes different restrictions on the replacement algorithm to prevent sensitive evictions. Extensive experimental results show that DSI-RE outperforms the state-of-the-art methods. The proposed method novelly identifies the key behavioral intent during an attack and blocks the attack by introducing minor restrictions in attack process.}
}


@article{DBLP:journals/compsec/ZhouJLF26,
	author = {Yuhao Zhou and
                  Peng Jia and
                  Jiayong Liu and
                  Ximing Fan},
	title = {Fuzz4Cuda: Fuzzing your {NVIDIA} {GPU} libraries through debug interface},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104754},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104754},
	doi = {10.1016/J.COSE.2025.104754},
	timestamp = {Fri, 26 Dec 2025 20:52:57 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhouJLF26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The programming security of Compute Unified Device Architecture (CUDA), NVIDIA’s parallel computing platform and programming model for Graphics Processing Unit, has always been a significant concern. On the host-side, fuzzing has been remarkably successful at uncovering various software bugs and vulnerabilities, with hundreds of flaws discovered annually through different fuzzing tools. However, existing fuzzing tools typically operate on general-purpose CPU architectures and embedded systems. As an independent processing unit, the GPU does not support tools like American Fuzzy Lop for collecting instrumentation and code coverage information. Consequently, grey-box fuzzing for closed-source graphics and driver libraries has remained an unaddressed challenge. This research introduces Fuzz4Cuda, CUDA-focused GPU fuzzing framework specifically designed for GPU libraries. To enhance device-side coverage collection, Fuzz4Cuda achieved this by runtime analysis of CUDA Streaming Assembler. Furthermore, the framework could dynamically adjust the number of breakpoints to optimize test case execution speed, thereby accelerating the overall time to discover program crash inputs. The development of Fuzz4Cuda has moved GPU library fuzzing ahead, aiming to improve the security of the GPU programming environment. Over a month-long real-world fuzzing campaign aimed at vulnerability discovery, our evaluation of the CUDA Toolkit uncovered five real-world bugs, four of which have been assigned Common Vulnerabilities and Exposures (CVE) IDs.}
}


@article{DBLP:journals/compsec/LiuZHF26,
	author = {Yaqi Liu and
                  Yue Zhang and
                  Pinzhen He and
                  Shuzhen Fang},
	title = {FedGNNLDP: Federated graph neural network with locally differential
                  privacy},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104757},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104757},
	doi = {10.1016/J.COSE.2025.104757},
	timestamp = {Wed, 31 Dec 2025 22:13:24 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LiuZHF26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Graph Neural Networks (GNNs) have demonstrated remarkable performance due to their excellent graph-structure-based approach and have been widely applied in various industries. GNNs can access node features or link information as a result of their unique graph structure, enabling them to achieve more accurate machine learning model prediction. However, most graph-structure data varies across individual users or consumers in different industries, and it is inevitable that private data will be accessed during the GNN training process, which will cause serious leakage of private information. To address this problem, the author propose a privacy-preserving federal learning model for GNN with local differential privacy, named  FedGNNLDP,  which distributes a subgraph of a larger global graph to different clients for training to avoid the direct exchange of subgraph data. At the same time, the author apply the local differential privacy mechanism to the local client for GNN-subgraphs, thereby effectively avoiding the parameters being compromised by attacks during transmission. The author go on to demonstrate the solid experimental performance of our method compared to several baselines. Extensive experiments demonstrate that our model achieves a high accuracy rate even with privacy protection.}
}


@article{DBLP:journals/compsec/AbdulrahmanRBF26,
	author = {Amal Abdulrahman and
                  Debbie Richards and
                  Ayse Aysin Bilgin and
                  Paul Formosa},
	title = {Developing ethical principle awareness and reasoning in a cybersecurity
                  context: Enhancing user understanding using ripple down rules},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104761},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104761},
	doi = {10.1016/J.COSE.2025.104761},
	timestamp = {Thu, 25 Dec 2025 12:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AbdulrahmanRBF26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cybersecurity breaches are often attributed to human behaviour, where individuals fail to integrate ethical principles in their decision-making. This empirical study investigates the effectiveness of the Ripple Down Rules (RDR) method, a knowledge acquisition and representation method, in enhancing ethical awareness and reasoning in cybersecurity contexts. The proposed approach combines rule-based reasoning, case-based learning, reflection, and situated cognition to bridge the gap between ethical knowledge and action by systematically connecting scenario elements to ethical principles. Participants, recruited from a cohort of first-year psychology students, were exposed to training incorporating five ethical principles—Beneficence, Non-Maleficence, Justice, Autonomy, and Explicability—applied to realistic cybersecurity scenarios. The study employed a randomised controlled design with two treatment and one control groups, using pre- and post-study assessments to evaluate improvements in ethical principle identification and reasoning. Participants rated RDR as a clear and helpful tool for understanding ethical reasoning, with sensibility and helpfulness scores ranging from moderate to high. Results demonstrate that RDR training significantly improved participants' ability to identify ethical principles compared to learning without RDR, particularly for principles like autonomy and explicability. However, challenges persisted in distinguishing overlapping principles, such as beneficence and non-maleficence. Implications and guidance for use of RDR for ethics training are discussed.}
}


@article{DBLP:journals/compsec/MartseniukPOH26,
	author = {Yevhenii Martseniuk and
                  Andrii Partyka and
                  Ivan R. Opirskyy and
                  Oleh Harasymchuk},
	title = {Cost observability as a security control in multi-cloud environments
                  based on {SOC} 2 security standard},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104771},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104771},
	doi = {10.1016/J.COSE.2025.104771},
	timestamp = {Wed, 24 Dec 2025 10:44:27 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MartseniukPOH26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of ensuring cost transparency and proactive budget control in multi-cloud environments is becoming increasingly relevant for modern IT infrastructures. As organizations scale their use of heterogeneous cloud services, they face challenges related to fragmented billing systems, inconsistent cost metrics, and delayed anomaly detection. This study frames cost observability not merely as a financial function, but as an integral component of the organization’s security posture, aligned with the SOC 2 framework. The novelty of this research lies in the integration of cost monitoring tools — specifically Splunk, Cherwell, and JSON-based cloud Application Programming Interfaces (APIs) — with operational and security processes, enabling real-time detection of budget deviations, automated incident escalation, and cost-based policy enforcement. This study presents a proposed future architecture that implements a unified cost observability layer across heterogeneous billing systems in multi-cloud environments. The architecture transforms provider-specific formats — including AWS Cost Explorer JSON exports, Azure Cost Management APIs, and GCP Billing BigQuery exports — into standardized cost events. These normalized streams create a single temporal view of expenditures against unified budget thresholds, while generating consolidated financial telemetry that enables cross-provider anomaly detection and correlation. By reframing cost data as actionable observability signals, the approach advances beyond fragmented dashboards toward a centralized, audit-ready control layer that supports compliance, incident response, and financial governance. The system further incorporates role-based access logic, escalation thresholds, and forecasting models, creating a cost governance layer with direct implications for FinOps, DevSecOps, and compliance teams.}
}


@article{DBLP:journals/compsec/RoascioMC26,
	author = {Gianluca Roascio and
                  Nicol{\`{o}} Maunero and
                  Gabriele Costa},
	title = {Control-Flow Integrity for Resource-Constrained Devices},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104730},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104730},
	doi = {10.1016/J.COSE.2025.104730},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/RoascioMC26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Control-Flow Integrity (CFI) ensures that an attacker cannot tamper with the execution logic of a program, e.g., by reusing its code to implement malicious operations. In the past, several attacks have actively exploited CFI failures for hijacking the control logic of programs. Although enforcing the CFI of programs is a significant concern, implementing effective control mechanisms is highly complex. Although often control-flow properties are regarded as practically enforceable, as most languages include data-driven branch operators, CFI is, in fact, also a data-flow property. Furthermore, when the execution platform supports any sort of non-determinism, e.g., think of program interrupts, static models for CFI analysis, such as control-flow graphs (CFG), cannot be accurately computed. Thus, it is not surprising that CFI is often only partially guaranteed by means of weaker security models. In this paper, we present a novel CFI enforcement framework for dealing with the aforementioned issues. Like other proposals, our method relies on code instrumentation for deploying CFI checks among the instructions of a target program. However, our policy enforcement framework also monitors interrupt routines to ensure that the attacker cannot exploit them. Finally, we provide our proposal with formal semantics that we use to provide the correctness of our solution.}
}


@article{DBLP:journals/compsec/MunizMMMLAR26,
	author = {David {\'{A}}lvarez Mu{\~{n}}iz and
                  Luis Perez Miguel and
                  Miguel and
                  Alberto Mateo Mu{\~{n}}oz and
                  Xavier Larriva{-}Novo and
                  Manuel Alvarez{-}Campana and
                  Diego Rivera},
	title = {Design and generation of a dataset for training insider threat prevention
                  and detection models: The {SPEDIA} dataset},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104743},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104743},
	doi = {10.1016/J.COSE.2025.104743},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MunizMMMLAR26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing complexity of insider threats poses a critical challenge for modern cybersecurity. Existing datasets used for training detection systems often lack realism, suffer from severe class imbalance, or are outdated. This paper presents a novel methodology for the generation of insider threat datasets through the integration of three data sources: (1) real user behavior collected during a controlled cyber exercise, (2) simulated user activity modeled on realistic work roles, and (3) synthetic data derived from the CERT Insider Threat Test dataset. The result is the SPEDIA dataset, designed to support the development and evaluation of machine learning models for detecting insider threats. The dataset includes detailed event-level logs of user activity, such as file manipulation, command execution, service usage, and network behavior, with annotations mapped to MITRE ATT&CK tactics and techniques. Unlike previous datasets, SPEDIA achieves a more balanced distribution of malicious and non-malicious events, enhancing its suitability for supervised learning. This work also provides a replicable framework for generating similar datasets, contributing to the advancement of insider threat detection research and the development of robust, real-world mitigation strategies.}
}


@article{DBLP:journals/compsec/RezkHBBA26,
	author = {Marianna Rezk and
                  Hassan Harb and
                  Ismail Bennis and
                  S{\'{e}}bastien Bindel and
                  Abdelhafid Abouaissa},
	title = {Internet of cybersecurity things in the third decade of the 21st century:
                  {A} forward vision},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104747},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104747},
	doi = {10.1016/J.COSE.2025.104747},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/RezkHBBA26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, integrating Artificial Intelligence (AI) into the Internet of Things (IoT) networks had led to a revolution in cybersecurity-based applications. It secures the interconnected devices in such networks and detecting various types of attacks. Thus, we are witnessing the rise of a new network generation called the Internet of Cybersecurity Things (IoCT). Particularly, this study explores the evolution of IoCT networks in the last century, while examining their pivotal role in securing smart cities applications. Firstly, this study focuses various IoCT applications and provides a comprehensive review of recent AI-based techniques and approaches proposed to overcome challenges raised in IoCTs. Then, we propose a multi-layer architecture for safeguarding IoT devices and networks from unauthorized access, intrusions and data breaches. The architecture is mainly based on AI, edge/fog computing and knowledge distillation, and consists of five layers: perception, network, edge/fog, detection, and decision. Then, we provide an overview of various benchmark datasets used in the validation process to help realize the new architecture. For efficiency validation, we provide a performance analysis of nowadays IoCT systems; some existing techniques have been selected for testing and comparison according to a set of criteria. Finally, the results of the comparative study are anticipated to direct future research paths and assist in developing secure mechanisms tailored for the evolving landscape of the Internet of Cloud Things (IoCT).}
}


@article{DBLP:journals/compsec/HagenLPWSS26,
	author = {Martin Slind Hagen and
                  Emil Lundqvist and
                  Alex Phu and
                  Yenan Wang and
                  Kim Strandberg and
                  Elad Michael Schiller},
	title = {Towards a formal verification of secure vehicle software updates},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104751},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104751},
	doi = {10.1016/J.COSE.2025.104751},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/HagenLPWSS26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures. Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF’s architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF’s compliance with essential security requirements — confidentiality, integrity, authenticity, freshness, order, and liveness —demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.}
}


@article{DBLP:journals/compsec/MaCLC26,
	author = {Yuxiang Ma and
                  Tao Chen and
                  Jiaqi Lin and
                  Ying Cao},
	title = {ICloud: An intrusion detection and dynamic defense mechanism for cloud
                  environments},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104755},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104755},
	doi = {10.1016/J.COSE.2025.104755},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MaCLC26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of artificial intelligence (AI), cloud environments are becoming increasingly important. However, cloud environment networks are at risk of various network attacks. Therefore, it is crucial to detect abnormal traffic in cloud environment networks. With the continuous development of network technology, the diversity of cloud environment network traffic continues to increase (intra-class diversity), and the boundary between malicious and benign behaviors becomes more blurred (inter-class similarity), leading to false detection. At the same time, most game theory defense deception methods for cloud environment networks assume that the attacker and defender maintain consistent views under uncertainty. In fact, the attacker and defender have different views on the same game. To address the above issues, we propose an intrusion detection and dynamic defense mechanism for cloud environments. To address the challenges brought by intra-class diversity and inter-class similarity, we propose an intrusion detection system (IDS) based on contrastive learning, which can make correct decisions when classifying samples of different categories. To identify traffic more accurately, this paper proposes an improved lightweight ResNet-34 model (IResNet34). To address the challenge that the attacker and defender have different views on the same game, we propose a hypergame model involving multiple attackers and defenders. The attacker cannot obtain complete game information through defensive deception technology, resulting in attack failure. In addition, we propose an adaptive defense strategy selection method based on machine learning, which automatically selects the best defense strategy based on the game record. The output of dynamic defense will be fed back to the intrusion detection module to reduce the false alarm rate. Finally, experiments verified that the method based on contrastive learning proposed in this paper can achieve high detection accuracy in the real world and benchmark datasets, and the dynamic defense method can effectively reduce the false positive rate (FPR) of IDS.}
}


@article{DBLP:journals/compsec/ZengWLZ26,
	author = {Yishun Zeng and
                  Yue Wu and
                  Xicheng Lu and
                  Chao Zhang},
	title = {RTFuzz: Fuzzing browsers via efficient render tree mutation},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104756},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104756},
	doi = {10.1016/J.COSE.2025.104756},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZengWLZ26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rendering engine is a cornerstone of modern web browsers, responsible for transforming heterogeneous inputs-HTML, CSS, and JavaScript-into visual page content. This complex process involves constructing and updating the render tree, which governs layout and painting, but also introduces subtle defects that manifest as robustness and security challenges. Existing browser fuzzers largely fall short in thoroughly testing the rendering engine due to two fundamental challenges: (i) the vast, multidimensional input space makes efficient exploration difficult; (ii) the periodic, incremental rendering model of modern rendering engines merges multiple updates of the render tree within each rendering cycle, reducing activation of deep pipeline logic such as layout and painting. In this paper, we aim to enhance the testing depth of the rendering pipeline-rather than simply increasing code coverage-by focusing on updating the render tree, the central data structure linking frontend inputs to backend layout and painting modules. Our approach incorporates (i) correlation-based pruning strategies for HTML elements and CSS properties to prioritize high-yield input combinations, and (ii) a time-sliced testing scheme that intentionally distributes mutations across multiple rendering cycles within a single test case, thereby increasing the trigger frequency of backend rendering modules. We implement a prototype, RTFuzz, and evaluate it extensively. Compared to state-of-the-art fuzzers Domato, FreeDom, and Minerva, RTFuzz helps uncover 43.1 %, 28.7 %, and 75.7 % more unique crashes, 83.3 % of which occur in the rendering pipeline, and further identified 20 real-world defects during long-running experiments. Ablation studies confirm that correlation-based pruning increases unique crashes by 79.2 %, and the time-sliced scheme contributes a 16.2 % improvement.}
}


@article{DBLP:journals/compsec/HuangGKZWKF26,
	author = {Tao Huang and
                  Yansong Gao and
                  Boyu Kuang and
                  Zhi Zhang and
                  Zhanfeng Wang and
                  Hyoungshick Kim and
                  Anmin Fu},
	title = {ExMOP: Extensible protocol reverse engineering framework based on
                  Multi-objective OPtimization},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104758},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104758},
	doi = {10.1016/J.COSE.2025.104758},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/HuangGKZWKF26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Protocol Reverse Engineering (PRE) has become the foundation of numerous downstream security analyses, including vulnerability mining and intrusion detection. As the mainstream PRE technique, network trace-based PRE methods utilize various protocol features (e.g., specific features or universal features) to identify fields and their semantics. However, the inherent limitations of these features consequently constrain the performance of these PRE methods, compromising their generalizability or effectiveness. To address this, we propose an  Ex tensible Protocol Reverse Engineering Framework Based on  M ulti-objective  OP timization (ExMOP) that flexibly incorporates multiple basic feature rules while synergistically integrating their complementary advantages to enhance protocol field segmentation performance. Each basic feature rule can be easily formalized as an optimization objective function, transforming the protocol field segmentation problem into a constrained multi-objective optimization model. We employ the Differential Evolution (DE) algorithm to solve this model, deriving the optimal field segmentation strategy. Ultimately, we conduct comprehensive experiments on publicly available datasets of multiple Internet protocols and industrial protocols. ExMOP demonstrates superior performance across all evaluation metrics (including 81 % precision, 81 % recall, 86 % accuracy, 80 % F1-score, 11 % FPR, and 60 % Perfection), significantly outperforming state-of-the-art methods, including NEMESYS (Usenix Security ’2018), AWRE (Usenix Security ’2019), NetPlier (NDSS ’2021), and BinaryInferno (NDSS ’2023). Furthermore, experiments affirm that expanding higher-efficiency feature rules can significantly enhance ExMOP’s performance in terms of accuracy, convergence, and stability.}
}


@article{DBLP:journals/compsec/SayedSRAD26,
	author = {Mohammad Ali Sayed and
                  Khaled Sarieddine and
                  Rinith Reghunath and
                  Chadi Assi and
                  Mourad Debbabi},
	title = {Plug and prey: Exploiting design flaws to hijack {EV} charging stations},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104759},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104759},
	doi = {10.1016/J.COSE.2025.104759},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/SayedSRAD26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electric Vehicles (EVs) have become a major element in the global push to combat climate change, given their ability to reduce the transportation sector’s emissions. To support the increasing number of EVs on the road, EV Charging Stations (EVCSs) are being deployed and have become a core element of the transportation infrastructure. EVCSs with individual web portals have been widely studied and proven to be vulnerable to network-based attacks. On the other hand, EVCSs that do not host web portals and cannot be accessed remotely are considered more secure. These EVCSs are generally considered to be more secure and have been overlooked in previous studies. Consequently, in this work, we present the first attack framework that exploits design flaws in this type of EVCS to hijack their operation. Our tests were performed on six actual EVCSs that follow the deployment strategy commonly preferred in North America by most operators and a few operators in Europe. We demonstrate how adversaries can successfully exploit the discussed vulnerabilities to gain unauthorized access to the EVCS configuration and acquire administrator privileges. We then proceed to craft multiple attacks to affect the power grid, steal money, or deteriorate EVCS availability.}
}


@article{DBLP:journals/compsec/LeeSHWSK26,
	author = {Seunghyeon Lee and
                  Hyunmin Seo and
                  Hwanjo Heo and
                  Anduo Wang and
                  Seungwon Shin and
                  Jinwoo Kim},
	title = {SecTracer: {A} framework for uncovering the root causes of network
                  intrusions via security provenance},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104760},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104760},
	doi = {10.1016/J.COSE.2025.104760},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LeeSHWSK26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern enterprise networks comprise diverse and heterogeneous systems that support a wide range of services, making it challenging for administrators to track and analyze sophisticated attacks such as advanced persistent threats (APTs), which often exploit multiple vectors. To address this challenge, we introduce the concept of  network-level security provenance , which enables the systematic establishment of causal relationships across hosts at the network level, facilitating the accurate identification of the root causes of security incidents. Building on this concept, we present  SecTracer  as a framework for a network-wide provenance analysis.  SecTracer  offers three main contributions: (i) comprehensive and efficient forensic data collection in enterprise networks via software-defined networking (SDN), (ii) reconstruction of attack histories through provenance graphs to provide a clear and interpretable view of intrusions, and (iii) proactive attack prediction using probabilistic models. We evaluated the effectiveness and efficiency of  SecTracer  through a real-world APT simulation, demonstrating its capability to enhance threat mitigation while introducing less than 1 % network throughput overhead and negligible latency impact.}
}


@article{DBLP:journals/compsec/PatraR26,
	author = {Dwibik Patra and
                  Narendran Rajagopalan},
	title = {Integration of emerging technologies in cybersecurity for healthcare:
                  {A} systematic review},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104763},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104763},
	doi = {10.1016/J.COSE.2025.104763},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/PatraR26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integration of Internet of Medical Things (IoMT) devices into healthcare has enhanced clinical services but also widened the attack surface, exposing systems to ransomware, data exfiltration, and protocol spoofing. Conventional security mechanisms often fall short in addressing such diverse and evolving threats. This review examines the role of hybrid approaches that combine machine learning (ML) and deep learning (DL) models with metaheuristic optimization techniques in strengthening healthcare cybersecurity. Techniques such as Genetic Algorithms, Particle Swarm Optimization, and Ant Colony Optimization are assessed for their capacity to fine-tune learning models, improve detection accuracy, and enhance adaptability against complex attack patterns. Evidence from recent studies demonstrates that these hybrid solutions achieve higher resilience and better handling of imbalanced or dynamic datasets compared with traditional methods. However, challenges persist in achieving interpretability, ensuring real-time processing, and maintaining compliance with regulatory frameworks, including HIPAA and GDPR. The review highlights how explainable AI methods such as SHAP and LIME, alongside multi-objective optimization frameworks such as NSGA-II, contribute to balancing accuracy, latency, and privacy requirements. Applications discussed include intrusion detection in hospital networks, protection of IoMT infrastructures, and safeguarding of electronic health records. The paper concludes by identifying open research challenges and proposing a roadmap for developing lightweight, interpretable, and regulation-aware AI solutions tailored to the specific needs of healthcare cybersecurity.}
}


@article{DBLP:journals/compsec/IqbalHHAS26,
	author = {Nasir Iqbal and
                  Faisal Bashir Hussain and
                  Hassan Jalil Hadi and
                  Naveed Ahmad and
                  Ali Shoker},
	title = {{SETPA:} Structural evasion techniques for {PDF} malware detection
                  systems},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104775},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104775},
	doi = {10.1016/J.COSE.2025.104775},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/IqbalHHAS26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Portable Document Format (PDF) is widely used because of its cross-platform compatibility, document integrity, and security features. However, their structural vulnerabilities make them a prime target for malware attacks. Machine-learning-based detection systems often struggle with feature engineering, dataset diversity, and robustness against adversarial attacks. These limitations result in high false positives, disruption of benign processes, and false negatives, enabling adversarial malware to evade detection. To address these challenges, this study introduces Structural Evasion Techniques for PDF Augmentation (SETPA), a novel evasion framework designed to bypass PDF malware detection systems. SETPA employs eight structural techniques, such as empty object streams, fake XREF table entries, and benign metadata, to obfuscate malicious content while preserving file functionality.Comprehensive experiments conducted on two leading detection models, PDFRate v2.0 and Hidost, demonstrate that SETPA consistently outperforms a Deep Reinforcement Learning (DRL)-based evasion framework. SETPA achieves evasion success rates that are 6 % to 10 % higher, and reduces average detection accuracy by 58 % to 80 %. These results confirm SETPA’s robust and reliable evasion performance across various detection systems. The findings highlight SETPA’s capability to exploit structural vulnerabilities in PDF detectors and underscore the need for adaptive, behavior-aware defense mechanisms that can counter entropy-driven structural evasions in emerging cyber threats.}
}


@article{DBLP:journals/compsec/MullinsT26,
	author = {Antony Mullins and
                  Nik Thompson},
	title = {Technostress and information security - {A} review and research agenda
                  of security-related stress},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104776},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104776},
	doi = {10.1016/J.COSE.2025.104776},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MullinsT26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technostress is a growing concern for organisations, given the negative impacts of stress on employees' job satisfaction, productivity, and intention to comply with or violate policies. Security-related stress (SRS), a dimension of technostress, addresses how security-related activities, such as information technology compliance, can impact an individual's stress. Addressing security-related stress research is vital, given it can help identify factors that can both enhance employee well-being and strengthen an organisation's security posture. In this paper, we systematically review the literature from the past two decades addressing security-related stress and identify twenty-seven relevant studies for analysis. We make contributions in three areas. Firstly, we discover the predominant theoretical frameworks and models that address security-related stress while examining key factors and constructs that examine security-related stress. Secondly, we describe how security-related stress is measured and what interventions have proven effective in reducing it. Finally, based on our comprehensive analysis, we present a research agenda to inform future research directions of security-related stress.}
}


@article{DBLP:journals/compsec/ToryH26,
	author = {Adrita Rahman Tory and
                  Khondokar Fida Hasan},
	title = {An evaluation framework for network {IDS/IPS} datasets: Leveraging
                  {MITRE} ATT{\&}CK and industry relevance metrics},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104777},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104777},
	doi = {10.1016/J.COSE.2025.104777},
	timestamp = {Sun, 01 Feb 2026 13:35:39 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ToryH26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.}
}


@article{DBLP:journals/compsec/YinLZLLW26,
	author = {Zhihua Yin and
                  Zixuan Li and
                  Youtong Zhang and
                  Jianxi Li and
                  Dong Liu and
                  Hongqian Wei},
	title = {Temporal-spatial feature fusion based intrusion detection system for
                  in-vehicle networks},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104781},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104781},
	doi = {10.1016/J.COSE.2025.104781},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/YinLZLLW26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a typical cyber-physical system, the in-vehicle network is evolving from an information silo into a mobile interconnection terminal. The controller area networks (CAN), serving as the real-time communication medium between automotive electronic control units (ECUs), encounter significant security challenges due to the absence of essential identity authentication and encryption mechanisms. The intrusion detection systems (IDSs) for CAN provide threat alerts but struggles to effectively identifying same origin method execution (SOME) attacks due to their frame homology and high concealment characteristics. To this end, a temporal-spatial feature fusion based IDS (TSFF-IDS) is developed. First, a comprehensive analysis is conducted on the temporal and spatial characteristics of CAN bus traffic. On this basis, a hybrid model integrating bidirectional long short-term memory (BiLSTM) networks and convolutional neural networks (CNN) is proposed to automatically extract the temporal and spatial features in parallel. A two-layer attention network is introduced to measure the distinct contributions of temporal-spatial features and recognize crucial features. Finally, the features are weighted fused to detect potential anomalies. To validate the effectiveness of the proposed method, comprehensive experiments have been conducted and the results show that the proposed TSFF-IDS exhibits superior identification capability and high adaptability compared to state-of-the-art schemes.}
}


@article{DBLP:journals/compsec/ZhangZYFA26,
	author = {Shengcai Zhang and
                  Fanchang Zeng and
                  Huiju Yi and
                  Zhiying Fu and
                  Dezhi An},
	title = {TFCNet: Based on time-frequency domain and multi-channel analysis
                  for Network security situation prediction},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104782},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104782},
	doi = {10.1016/J.COSE.2025.104782},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhangZYFA26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of internet technology has given rise to various security threats for cyberspace. Network Security Situation Prediction (NSSP) is proactive defence technology that can predict future network development trends based on historical attack information. As traditional passive defence methods are inadequate against modern network traffic attacks, NSSP based on proactive defence has become crucial. The time-frequency domain features in network traffic contain a lot of useful information that can enhance the accuracy of model predictions. However, existing studies has not approached NSSP from the perspective of time-frequency domain feature fusion. Therefore, this paper presents TFCNet, an improved iTransformer model that incorporates time-frequency domain feature fusion to enhance NSSP performance. Firstly, Variational Mode Decomposition (VMD) is applied to reduce the non-stationarity of network traffic data. Subsequently, by fusing the time-frequency domain features of the network traffic data, frequency-domain features are extracted across multiple channels to further enhance feature extraction and obtain richer time-frequency information of the network traffic. Experimental results on the three network security datasets NSL-KDD, UNSW-NB15, and CSE-CIC-IDS2018 indicate that TFCNet achieves the best predictive performance compared with nine other mainstream prediction methods. Compared with the iTransformer model, TFCNet reduces the Mean Squared Error (MSE) and Mean Absolute Error (MAE) by an average of 36.5 % and 23 %, respectively. In addition, efficiency evaluation results of the model demonstrates that TFCNet an effective balance between predictive performance and computational efficiency, which verifies the feasibility of TFCNet in NSSP.}
}


@article{DBLP:journals/compsec/ZhaoHC26,
	author = {Jinxian Zhao and
                  Haidong Hou and
                  Liang Chang},
	title = {Intrusion detection algorithm based on multi-scale feature fusion},
	journal = {Comput. Secur.},
	volume = {161},
	pages = {104783},
	year = {2026},
	url = {https://doi.org/10.1016/j.cose.2025.104783},
	doi = {10.1016/J.COSE.2025.104783},
	timestamp = {Tue, 13 Jan 2026 10:56:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhaoHC26.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network intrusion detection plays a crucial role in ensuring cybersecurity by promptly mitigating network attacks. However, existing deep learning methods have limited capabilities in capture network attack features and address class imbalances, resulting in low classification accuracy. This paper proposes a deep-learning intrusion detection model named FLSPPMRXt, which is built upon ResNeXt50. It enhances feature capture by improving the backbone convolution and introducing a multi-scale feature fusion module, including the Soft Pool layer. Meanwhile, focal loss is employed as the loss function to effectively mitigate the impact of class imbalance on classification accuracy. Furthermore, this method proposes a data visualization processing algorithm to provide an image representation that is more consistent with the feature nearest neighbor distribution. Experimental results show that the FLSPPMRXt model achieves 93.3 % and 95.2 % in overall classification accuracy and F1 score on UNSW_NB15 dataset, respectively. Compared with existing algorithms, such as the 2DCNN and RNN models, the method demonstrates superior network intrusion detection performance.}
}
