@article{DBLP:journals/compsec/AdamsM25,
	author = {Matthew Adams and
                  Tyler Moore},
	title = {How informative are cybersecurity risk disclosures? Empirical analysis
                  of firms targeted by ransomware},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104626},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104626},
	doi = {10.1016/J.COSE.2025.104626},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/AdamsM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Public companies face escalating requirements to disclose cybersecurity risks and damages in regulatory filings. In theory, such disclosures should equip investors with knowledge required to make informed decisions, while also encouraging firms to adopt more robust strategies for managing cybersecurity risks. In practice, discussions are often embedded in disparate locations of long documents full of legalese, which hinders systematic examination. This paper examines the regulatory filings of 61 firms that experienced ransomware incidents between 2018 and 2021. We describe a process whereby 7681 cyber-related statements were extracted from 314 10-K filings between 2018–23, then categorized using an iterative process inspired by grounded theory. We then perform quantitative and qualitative analysis of the statements, examining how firms discuss cybersecurity before and after experiencing an incident.}
}


@article{DBLP:journals/compsec/WangCHCGS25,
	author = {Jinlong Wang and
                  Yunhe Cui and
                  Rongfei He and
                  Yi Chen and
                  Chun Guo and
                  Guowei Shen},
	title = {{FTOA-RP:} {A} 'group'-based flow entry replacement policy
                  probing and flow table overflow attack method},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104629},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104629},
	doi = {10.1016/J.COSE.2025.104629},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/WangCHCGS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a new network architecture, Software-Defined Networking (SDN) introduces potential risks, such as configuration probing of switches and flow table overflow attacks. The existing flow entry replacement probing methods may not accurately probe the replacement policies due to network jitter and packet loss. Meanwhile, existing flow table overflow attacks ignore the fact that the flow entry replacement policy will evict the malicious flow entries during the attack. As evicted flow entries are not reinstalled promptly, the attack’s effect is reduced. To address the above issues, FTOA-RP, a new two-phase flow entry replacement policy probing method, and a new flow table overflow attack method considering the flow entry replacement policy are proposed. To overcome the decrease in replacement policy probing accuracy caused by network jitter and packet loss, FTOA-RP probes the replacement policy using a group of probing packets. In particular, FTOA-RP designs TPRPP, a probing algorithm that consists of a forward/backward eviction detection phase and a forward eviction fine-grained detection phase to probe the flow entry replacement policies. TPRPP designs a specially structured packet group, which consists of multiple subgroups, each containing several probing packets. By sending probing packets in groups, TPRPP effectively mitigates the negative effects of network jitter and packet loss. To address the issue that the effectiveness of existing flow table overflow attacks is reduced by the flow entry replacement policy, FTOA-RP designs a flow table overflow attack method that considers the impact of flow entry replacement. More specifically, FTOA-RP designs two attack packet-sending algorithms. The first one is SAP-FLR, which is used to launch a flow table overflow attack under FIFO and LRU. The second one is SAP-LF, used to launch a flow table overflow attack under LFU. During the attack phase, SAP-FLR and SAP-LF adjust the sending order of attack packets based on the replacement policies to ensure the timely reinstallation of evicted flow entries. The evaluation results show that FTOA-RP outperforms the existing attack methods in terms of the probing accuracy, probing cost, and the ability to maintain malicious flow entries during the attack.}
}


@article{DBLP:journals/compsec/ZhangYLZFP25,
	author = {Dongni Zhang and
                  Xiuzhang Yang and
                  Side Liu and
                  Yilin Zhou and
                  Jianming Fu and
                  Guojun Peng},
	title = {A survey on Android dynamic evasive malware: Taxonomy, countermeasures
                  and open challenges},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104646},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104646},
	doi = {10.1016/J.COSE.2025.104646},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/ZhangYLZFP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Android dynamic evasive malware has gained increasing attention and has become a critical threat in recent years. The inherent stealth and context-aware behavior of such malware allow it to bypass dynamic analysis systems and remain undetected during execution. In this paper, we present a comprehensive review of the evolution of dynamic evasion techniques in Android malware and systematically analyze their core strategies and implementation trends. We propose a novel, context-dependent taxonomy that unifies evasion behaviors under two major categories: those leveraging analysis environment artifacts and those relying on trigger conditions. Based on this taxonomy, we further conduct a systematic and comparative review of existing countermeasure techniques, highlighting their strengths and limitations. Moreover, we identify key open challenges that hinder current detection efforts, such as native code blind spots, AI-powered evasions, and analysis-monitoring contradictions, and offer forward-looking insights into potential research directions for mitigating dynamic evasive threats in Android malware.}
}


@article{DBLP:journals/compsec/HaoZGM25,
	author = {Fengrui Hao and
                  Shiyi Zhao and
                  Tianlong Gu and
                  Yan Ma},
	title = {dK-DGDP: {A} differential privacy approach on directed social network
                  graphs},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104647},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104647},
	doi = {10.1016/J.COSE.2025.104647},
	timestamp = {Tue, 14 Oct 2025 19:41:44 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/HaoZGM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the spread of social network services, the publication of social network graphs provides useful insights into various fundamental social phenomena such as information dissemination and personalized recommendations. Social network graphs usually contain a lot of sensitive information. However, most of the existing social network graph publishing methods are designed for undirected graphs, and there are few approaches exist for publishing directed graphs securely. In this paper, we initiate the exploration of a solution for publishing directed social network graphs, and develop a novel  dK -series based differential privacy directed graph publishing approach, named  dK -DGDP. In our solution, the noise is added to the 2K entries extracted from the original directed graph by the  dK -series differential privacy mechanism, and then an adaptive strategy is designed to handle the situation in which entries with added noise cannot generate the graph directly. Moreover, a subgroup division strategy is proposed to reduce the amount of noise during perturbation and address the issue of excessive noise. Experimental results show that  dK -DGDP can achieve effective privacy protection of directed graph data while almost maintaining its utility.}
}


@article{DBLP:journals/compsec/BustamantevSRSF25,
	author = {Diegof Bustamantev and
                  Luis Enrique S{\'{a}}nchez and
                  David Garcia Rosado and
                  Antonio Santos{-}Olmo and
                  Eduardo Fern{\'{a}}ndez{-}Medina},
	title = {Towards a sustainable cybersecurity framework for Agriculture 4.0
                  based on a systematic analysis of proposals},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104650},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104650},
	doi = {10.1016/J.COSE.2025.104650},
	timestamp = {Sun, 02 Nov 2025 12:35:00 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/BustamantevSRSF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The world is currently experiencing a profound transformation driven by the convergence of disruptive technologies under the concept of Industry 4.0. These technologies have driven sectors such as agriculture to modernise and automate for greater sustainability, leading to what is now referred to as Agriculture 4.0 However, this transformation entails risks and requires new frameworks that address cybersecurity, sustainability, and knowledge reuse. In this paper, we conduct a systematic review of these new systems with the aim of identifying their main shortcomings and proposing a new framework. The review revealed a significant gap in comprehensively addressing cybersecurity, AI, and sustainability. This highlights the need for deeper exploration of how these elements interact to benefit the agricultural sector. To this end, we propose the development of the QUILLAQUA framework, oriented towards secure, intelligent, and sustainable agriculture, with a focus on fostering effective synergies among these crucial components. This framework integrates advanced technologies in cybersecurity, IoT, and AI to optimise the management of water and nutritional resources in hydroponic systems, ensuring sustainability and data security. This approach aims to enhance technological efficiency in agriculture. It also aims to foster greater awareness to tackle present and future challenges in sustainable agriculture. By doing so, it ensures a successful transition towards more digitized and secure agricultural practices.}
}


@article{DBLP:journals/compsec/QiaoGWZRL25,
	author = {Sibo Qiao and
                  Qiang Guo and
                  Min Wang and
                  Haohao Zhu and
                  Joel J. P. C. Rodrigues and
                  Zhihan Lyu},
	title = {Advances in network flow watermarking: {A} survey},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104653},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104653},
	doi = {10.1016/J.COSE.2025.104653},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/QiaoGWZRL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past three decades, with the rapid development of the Internet, network security threats have increasingly intensified. Traffic analysis (TA) has become a core method for addressing these threats, enabling real-time monitoring and analysis of network traffic to identify abnormal behaviors and potential security risks accurately. TA is usually divided into passive and active categories. Passive TA monitors network traffic and analyzes its characteristics and patterns to detect threats, with no intervention in the data flow; the observer remains a bystander. However, as network threats become more complex, passive TA has gradually revealed its limitations in terms of accuracy, real-time performance, and handling of encrypted traffic. To overcome these limitations, network watermarking, as a form of active TA, has become an important tool for tracking and identifying network traffic. In recent years, with the development of watermarking technology, its application scope is no longer limited to the areas described in previous studies. Our primary contribution lies in providing a more comprehensive exploration of the technological evolution of network watermarking. Specifically, we investigate the potential of watermarking in emerging cross-domain application scenarios; articulate the overall architecture of watermarking systems together with novel embedding carriers; discuss the threats currently faced by watermarking technology; and systematically analyze the performance of various watermarking algorithms. Moreover, to overcome practical bottlenecks such as line-rate latency, large-scale concurrency, power consumption, and portability, we propose an FPGA-based watermarking concept, examine the potential synergy between watermarking techniques and field-programmable gate arrays (FPGAs), and outline new research challenges.}
}


@article{DBLP:journals/compsec/YangGWZC25,
	author = {Yahui Yang and
                  Yangyang Geng and
                  Qiang Wei and
                  Man Zhou and
                  Xin Che},
	title = {Physical semantic inference method for industrial control proprietary
                  protocol data fields},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104654},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104654},
	doi = {10.1016/J.COSE.2025.104654},
	timestamp = {Sat, 15 Nov 2025 17:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/YangGWZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The precise interpretation of physical semantics within industrial proprietary protocol data fields, which directly govern cyber–physical interactions, plays a pivotal role in securing industrial control systems (ICS). Current protocol reverse engineering methods face three fundamental limitations that hinder ICS security efforts: (1) discerning physical significance in raw hexadecimal streams, (2) unassisted field delineation amid unknown data types, and (3) mapping concurrent physical semantics to protocol fields without contextual references. To overcome these challenges, we propose Physeinfer, a novel physical semantic inference framework that innovatively integrates visual human–machine interface (HMI) monitoring with temporal sequence analysis. Our methodology leverages camera-acquired HMI panel differentials to extract physical semantic sequences, develops adaptive step-size strategies for longitudinal multiple sequence alignment, and employs dynamic time warping (DTW) to establish cyber–physical correlations. Evaluated in six industrial scenarios, Physeinfer was able to accurately recover the physical semantics of industrial control protocol data fields and demonstrated superior performance in field segmentation (11%–27% improvement over Netzob, MSERA, and Fieldhunter) without requiring prior knowledge of the protocol. This breakthrough establishes an essential foundation for context-aware security mechanisms in industrial infrastructure, enabling physics-informed vulnerability discovery and anomaly detection for ICS.}
}


@article{DBLP:journals/compsec/AbrathCS25,
	author = {Bert Abrath and
                  Bart Coppens and
                  Bjorn De Sutter},
	title = {MVX-based mitigation of position-independent code reuse},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104655},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104655},
	doi = {10.1016/J.COSE.2025.104655},
	timestamp = {Wed, 15 Oct 2025 19:23:21 +0200},
	biburl = {https://dblp.org/rec/journals/compsec/AbrathCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In literature, position-independent return-oriented programming (PIROP) attacks have been demonstrated that exploit memory vulnerabilities even when coarse-grained address space layout randomization (ASLR) is deployed and in the complete absence of information disclosure. The demonstrated attacks involved the patching of code pointers present on massaged stacks to create useful ROP chains. To the best of our knowledge, no effective countermeasures have been researched and evaluated to this date. While multi-variant execution (MVX) has been demonstrated in literature to provide strong protection against many classes of code-reuse attacks, the structured forms of diversification used in MVX in the past, such as disjoint code layouts (DCL), fail to mitigate PIROP attacks, for the same reason that ASLR fails to do so. In this paper, we present additional forms of structural diversity that, together with DCL, enable MVX to mitigate patch-based PIROP attacks. We present these extensions to MVX with DCL, and present a qualitative and quantitative security analysis, as well as a performance analysis, revealing that strong mitigation is achieved, at performance costs that vary from negligible to significant but in many scenarios acceptable. The main result is that the number of available, chainable gadgets is reduced to nearly zero, leaving virtually zero opportunities for successful patch-based PIROP attacks.}
}


@article{DBLP:journals/compsec/BorzacchielloCMGC25,
	author = {Luca Borzacchiello and
                  Matteo Cornacchia and
                  Davide Maiorca and
                  Giorgio Giacinto and
                  Emilio Coppa},
	title = {DroidReach++: Exploring the reachability of native code in android
                  applications},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104657},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104657},
	doi = {10.1016/J.COSE.2025.104657},
	timestamp = {Sun, 02 Nov 2025 12:35:00 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/BorzacchielloCMGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern Android applications often incorporate numerous native C/C++ libraries to efficiently handle CPU-intensive tasks or interact at a low level with specific hardware, such as performing specialized GPU rendering. Recent research on Android security has revealed that these libraries are frequently adopted by third-party developers and may pose security risks if not regularly updated, as publicly disclosed vulnerabilities in outdated libraries can be exploited by malicious actors. To determine whether these known vulnerabilities represent an immediate and tangible threat, it is essential to assess whether the vulnerable functions can be executed during application runtime – a research problem commonly known as  function reachability . In this article, we introduce  DroidReach++ , a novel static analysis approach for evaluating the reachability of native function calls in Android applications. Our framework overcomes the limitations of existing state-of-the-art methods by combining heuristics with symbolic execution, enabling a more precise reconstruction of Inter-procedural Control-Flow Graphs (ICFGs). When applied to the top 500 applications from the Google Play Store,  DroidReach++  identifies a significantly higher number of execution paths compared to previous techniques. Finally, two case studies demonstrate how  DroidReach++  serves as an effective tool for vulnerability assessment.}
}


@article{DBLP:journals/compsec/LiXWJCXCX25,
	author = {Jizhe Li and
                  Haoran Xu and
                  Yongjun Wang and
                  Zhiyuan Jiang and
                  Huang Chun and
                  Peidai Xie and
                  Yongxin Chen and
                  Tian Xia},
	title = {Fuzzing JavaScript {JIT} compilers with a high-quality differential
                  test oracle},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104660},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104660},
	doi = {10.1016/J.COSE.2025.104660},
	timestamp = {Fri, 28 Nov 2025 11:40:32 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LiXWJCXCX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern JavaScript engines use Just-In-Time (JIT) compilers to convert frequently executed code into machine instructions, boosting performance for web applications and cross-platform systems. However, the optimizations in JIT compilers often introduce vulnerabilities while enhancing speed, especially optimization bugs which are difficult to detect. Despite progress in detecting these bugs by using differential testing oracle, existing methods are limited by high false positives and inefficiencies. This paper proposes AccuOracle, a test oracle for detecting JIT optimization bugs. We uses an input template-based test oracle that collects differential results from a single execution, enabling efficient fuzzing. To address the high false positive challenge, AccuOracle employs a four-layer progressive filtering architecture: the dynamism elimination and environment isolation layers address root causes, while the pre-check and differential arbitration layers assess JIT-induced divergences. Experiments on engines like V8, SpiderMonkey, and JavaScriptCore show that AccuOracle effectively eliminates false positives while maintaining high operational efficiency. It provides a high-accuracy and high-efficiency solution for JIT defect detection by integrating high-quality input templates and systematic false positive elimination. Notably, AccuOracle has uncovered eight new bugs (two of them have been assigned CVE), five of which Mozilla has confirmed and fixed.}
}


@article{DBLP:journals/compsec/MilaniSHCAMS25,
	author = {Alessandra Maciel Paz Milani and
                  Arty Starr and
                  Samantha Hill and
                  Callum Curtis and
                  Norman Anderson and
                  David Moreno{-}Lumbreras and
                  Margaret{-}Anne D. Storey},
	title = {Fuzzy to clear: Elucidating the threat hunter cognitive process and
                  cognitive support needs},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104651},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104651},
	doi = {10.1016/J.COSE.2025.104651},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MilaniSHCAMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With security threats increasing in frequency and severity, it is critical that we consider the important role of threat hunters. These highly-trained security professionals learn to see, identify, and intercept security threats. Many recent works and existing tools in cybersecurity are focused on automating the threat hunting process, often overlooking the critical human element. Our study shifts this paradigm by emphasizing a human-centered approach to understanding the lived experiences of threat hunters. By observing threat hunters during hunting sessions and analyzing the rich insights they provide, we seek to advance the understanding of their cognitive processes and the tool support they need. Through an in-depth observational study of threat hunters, we introduce a model of how they build and refine their mental models during threat hunting sessions. We also present 23 themes that provide a foundation to better understand threat hunter needs and suggest five actionable design propositions to enhance the tools that support them. Through these contributions, our work enriches the theoretical understanding of threat hunting and provides practical insights for designing more effective, human-centered cybersecurity tools.}
}


@article{DBLP:journals/compsec/GholamiAZ25,
	author = {Arash Gholami and
                  Furkan Alaca and
                  Mohammad Zulkernine},
	title = {An anomaly detection based approach for continuous authentication
                  with smartwatch inertial sensors},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104656},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104656},
	doi = {10.1016/J.COSE.2025.104656},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/GholamiAZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional authentication methods protect unattended devices when they are logged out; however, logged-in devices left unattended are vulnerable to unauthorized access. Inactivity timeouts help mitigate this threat; however, long timeouts increase susceptibility to attack, whereas short timeouts hurt usability. In contrast, continuous authentication mitigates this threat by continuously and non-intrusively verifying whether a device is being used by the user who initially logged in. If verified, the user remains logged in; otherwise, the user is logged out. We design and evaluate a comprehensive data processing pipeline for smartwatch-based continuous authentication using inertial sensor data. We use a Siamese convolutional neural network to learn and extract discriminative features, and one-class classifiers to determine if a user is the account owner. We compare our learned features with handpicked features proposed in prior work; we show that our learned features achieve better equal-error rates (EER) compared to the handpicked features, particularly for shorter-duration time-series windows. We find that learned features are a promising approach to more quickly and accurately detect unauthorized use of devices. This work thus contributes to making smartwatch-based continuous authentication more secure and usable.}
}


@article{DBLP:journals/compsec/SharmaSGAS25,
	author = {Saurabh Sharma and
                  Jaiteg Singh and
                  Ankur Gupta and
                  Farman Ali and
                  Sukhjit Singh Sehra},
	title = {{PRIVIUM:} {A} differentiated privacy-privilege model for user security
                  and safety in the metaverse},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104658},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104658},
	doi = {10.1016/J.COSE.2025.104658},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/SharmaSGAS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vision of the Metaverse enables exciting new application domains through immersive experiences. However, the immersive nature of the metaverse, enjoyed through anonymous avatars, also poses significant risks to the safety and security of individual users. Already, the early iterations of the metaverse have reported incidents around user safety and the subsequent inability of the platform to fix accountability. Metaverse adoption, without addressing genuine concerns around user safety, therefore seems far-fetched. This paper presents PRIVIUM (Privacy and Privilege Integrated User Model), a novel Differentiated Privacy-Privilege based framework, designed to address the privacy-privilege-accountability paradox within the Metaverse. The model establishes a tiered privilege structure, allowing users to actively select their desired level of anonymity in exchange for corresponding privileges, through a dynamic two-stage AI model pipeline. The proposed model is dynamic, amenable to diverse use-case scenarios for the average user within the metaverse, while delicately balancing the trade-off between the user’s need for complete privacy and the platform’s responsibility to ensure safety and enforce accountability. This approach empowers users with granular control over their privacy to meet their navigation and application-specific consumption needs within the metaverse while allowing the platform to ensure safe experiences within the metaverse. Simulation results are presented, demonstrating feasibility, potential challenges in implementing PRIVIUM in the real-world discussed, and future evolution imagined.}
}


@article{DBLP:journals/compsec/MadhushanieVA25,
	author = {Nadisha Madhushanie and
                  Sugandima Vidanagamachchi and
                  Nalin A. G. Arachchilage},
	title = {Real-time privacy vulnerability detection techniques in software development:
                  {A} Systematic Literature Review},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104659},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104659},
	doi = {10.1016/J.COSE.2025.104659},
	timestamp = {Thu, 25 Dec 2025 12:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MadhushanieVA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time privacy vulnerability detection is one of the major concerns nowadays in developing secure software systems due to the growing complexity of software development and the increased attention to data privacy. This study conducts a Systematic Literature Review (SLR) to explore existing techniques, tools, and frameworks for detecting privacy vulnerabilities in real-time during the software development. We analyze relevant studies to identify key approaches, their effectiveness, and limitations by using the Kitchenham methodology and include it into the PRISMA framework. In addition, we categorize existing approaches into IDE integrated tools, network security solutions, mobile specific techniques, and general analysis tools. Summary tables further synthesize these techniques, tools, and their comparative attributes. Our findings reveal a variety of methods, including static and dynamic analysis, machine learning based detection, and integration of privacy-by-design. We also highlight challenges such as scalability, false positives, and the need for developer friendly tools. This review provides a comprehensive overview of the state-of-the-art in real-time privacy vulnerability detection approaches and offers insights into future research directions to enhance privacy protection in software development environments.}
}


@article{DBLP:journals/compsec/ZhengSJWT25,
	author = {Weining Zheng and
                  Xiaohong Su and
                  Yuan Jiang and
                  Hongwei Wei and
                  Wenxin Tao},
	title = {VDExplainer: Sequential decision-making and probability sampling guided
                  statement-level explanation for vulnerability detection},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104670},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104670},
	doi = {10.1016/J.COSE.2025.104670},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhengSJWT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most existing deep learning (DL) based vulnerability detection methods, including pre-trained models, are coarse-grained binary classification methods that lack the interpretability for detection results. Although the explanation of deep learning has received significant attention, there is little research on the explanation of pre-trained model-based vulnerability detection methods. Therefore, we focus on providing statement-level interpretability for these vulnerability detection models to help developers understand the vulnerabilities. More specifically, given a vulnerable code detected by the model, our task is to find the set of vulnerability-related statements that lead to the prediction. Inspired by the manual code review process, this paper proposes a framework for explaining vulnerability detection called VDExplainer. VDExplainer includes an explorer that uses sequential decision-making and probability sampling to find the combination of vulnerability-related statements and a navigator that helps reduce the search space by learning the vulnerability patterns. It is worth noting that the navigator is trained in advance and then integrated with the explorer, further enhancing the efficiency and effectiveness of VDExplainer. Extensive experiments on the semi-synthetic dataset and the widely used real-world project dataset show that VDExplainer achieves superior performance, outperforming current state-of-the-art methods.}
}


@article{DBLP:journals/compsec/LiWLXZ25,
	author = {Fang Li and
                  Gang Wang and
                  Guangjun Liu and
                  Xiao Xue and
                  Deyu Zhou},
	title = {Integrity verification scheme for distributed dynamic data in service
                  ecosystems},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104671},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104671},
	doi = {10.1016/J.COSE.2025.104671},
	timestamp = {Tue, 20 Jan 2026 14:46:51 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LiWLXZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Big data distributed storage provides solid data support for various service ecosystem services. The cloud computing platform is the key infrastructure to realize the management of big data distributed storage. To cope with increasingly complex network threats and data protection requirements, distributed storage systems often require a higher level of information-theoretic security assurance. Among them, how to realize data security audit and ensure data integrity and reliability is the core key technology that must be addressed in the field of cloud computing distributed storage. Existing cloud computing outsourced dynamic data audit schemes mainly rely on the security technology of computational complexity and still have such problems as insufficient security and poor availability, so it is difficult to directly apply or effectively extend them to distributed storage systems with requirements for information-theoretic security. In order to address this challenge, this paper proposes a lightweight algebraic remote data audit methodology, which explores an orthogonal authentication technique for the linear subspace generated from cloud-stored data vectors. This approach offers a novel application for algebraic coding in the context of distributed dynamic cloud storage auditing. Different from the existing dynamic audit mechanism, the proposed scheme does not rely on any authentication data structure, which ensures the real-time update and integrity audit of outsourced dynamic storage data. Experimental analysis demonstrates that the proposed scheme is capable of resisting forgery or replay attacks and achieving the objective of distributed information-theoretic security auditing. Compared with existing similar schemes, the proposed scheme involves lower storage overhead and less computation in the process of dynamic data updating.}
}


@article{DBLP:journals/compsec/ZhanMFPSL25,
	author = {Lige Zhan and
                  Jiang Ming and
                  Jianming Fu and
                  Guojun Peng and
                  Letian Sha and
                  Lili Lan},
	title = {The hidden complexities of Android {TPL} detection: An empirical analysis
                  of techniques, challenges, and effectiveness},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104672},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104672},
	doi = {10.1016/J.COSE.2025.104672},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhanMFPSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Third-party libraries (TPLs) play a crucial role in Android application (app) development and have become an indispensable part of the Android ecosystem. However, TPLs also introduce potential security risks, as they may propagate 1-day vulnerabilities or even malicious code into apps. Moreover, certain downstream tasks, such as app clone detection, license violation identification and patch presence test, require accurate TPL detection as a prerequisite. Consequently, TPL detection has gained increasing importance over the past decade in improving maintainability and enhancing security within the software supply chain. To ensure robustness against external factors and precise vulnerability identification, modern library detection tools, in addition to recognizing TPL variety, must be resilient to code obfuscation and optimization, and must also be capable of accurately identifying library versions. Although recent studies have reported progress in addressing these issues, none have conducted a comprehensive evaluation to determine whether the proposed methods effectively overcome these challenges. Furthermore, critical aspects such as tool performance on real-world apps, as well as the generalizability of existing approaches, are frequently overlooked in current research. To gain deeper insights into TPL detection research, we conducted a comprehensive empirical analysis of state-of-the-art approaches in this domain. This study begins by summarizing the common technologies used at each stage of the TPL detection process, followed by an analysis of the prevalence of code obfuscation and optimization in real-world apps to identify key external factors that hinder effective library detection. Next, we evaluate the performance of cutting-edge tools on multiple ground-truth datasets to validate our findings. Specifically, we systematically analyze the methodologies employed by these tools, assessing their capabilities in TPL variety detection, version identification, resilience to common obfuscation and optimization techniques, and the underlying causes of their failures. Finally, we assessed the generalizability of these tools by comparing their performance across diverse datasets and validating them with real-world data. Our findings confirm that obfuscation and optimization are indeed prevalent in real-world scenarios. However, the code transformations introduced by these techniques often exceed the scope of scenarios considered in prior TPL detection studies. We also observe that even the most advanced detection features struggle to accurately differentiate between library versions. In addition to errors caused by obfuscation and optimization, overly simplistic library features can further contribute to false positives. Moreover, while most tools perform well on their own curated datasets and show reduced performance on external datasets, their effectiveness in real-world scenarios does not exhibit a substantial disparity. Overall, this paper presents a comprehensive analysis and evaluation of current TPL detection techniques, providing a solid foundation for future research in this area.}
}


@article{DBLP:journals/compsec/KampourakisKGK25,
	author = {Vyron Kampourakis and
                  Georgios Kavallieratos and
                  Vasileios Gkioulos and
                  Sokratis K. Katsikas},
	title = {Cracks in the chain: {A} technical analysis of real-life supply chain
                  security incidents},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104673},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104673},
	doi = {10.1016/J.COSE.2025.104673},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/KampourakisKGK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Industry 5.0 drives greater digitalization and interconnectivity, supply chains have become vital to global commerce, ensuring the seamless flow of goods, services, and data. However, this reliance has also swelled the attack surface, rendering supply chains a prime target for evildoers. Meanwhile, the inherent complexity of supply chain ecosystems prevents defenders from fully applying contemporary security controls promptly and effectively. Clearly, the combination of these hindering factors has led to some of the most severe cybersecurity incidents of the past years. This study is the first to our knowledge that undertakes a comprehensive technical analysis of reported supply chain security incidents. Our analysis is done both from offensive and defensive prisms, leveraging well-established cybersecurity frameworks and guidelines, namely, the ATT&CK MITRE knowledge base matrix and the NIST SP 800-161, respectively. Furthermore, to consolidate our findings and facilitate future research initiatives, we compiled a fundamental dataset that can be used as the basis for automated analysis and potential integration with cybersecurity workflows. The key observations of a 33-incident analysis through the lens of an ATT&CK MITRE- and NIST SP 800-161-based taxonomies we propose can be wrapped up into two key points. First, the attack surface continues to expand, following an upward spiral due to the mushrooming of tactics and techniques that can facilitate the early or late stages of attacks, highlighting their complexity, sophistication, and widespread impact. Second, our findings underscore the necessity of a multifaceted approach to strengthening supply chain resilience. This includes implementing robust cybersecurity controls, comprehensive risk assessment methodologies, and transparent collaboration among suppliers, customers, and vendors to ensure adherence to state-of-the-art cybersecurity best practices.}
}


@article{DBLP:journals/compsec/NoorMT25,
	author = {Ahmad Fairuz Mohamed Noor and
                  Sedigheh Moghavvemi and
                  Farzana Parveen Tajudeen},
	title = {Identifying key factors of cybersecurity readiness in organizations:
                  Insights from Malaysian critical infrastructure},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104674},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104674},
	doi = {10.1016/J.COSE.2025.104674},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/NoorMT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cybersecurity readiness is critical for safeguarding National Critical Information Infrastructure (NCII) against rapidly evolving threats. This study applies Dynamic Capabilities (DC) theory to examine how Malaysian NCII agencies develop adaptive capabilities to counter such threats. Using a qualitative design, we conducted semi-structured interviews with 16 representatives from 15 organizations spanning finance, telecommunications, transportation, and government. Thematic analysis was employed to interpret readiness factors through the DC dimensions of sensing, seizing, and transforming. Findings reveal that  sensing capabilities  - such as situational awareness, policy flexibility, and technological agility - enable proactive threat detection and adaptation.  Seizing capabilities  emphasize dynamic leadership, strategic resource allocation, and proactive risk management as critical for addressing vulnerabilities and reinforcing resilience.  Transforming capabilities , including crisis resilience planning, continuous learning, and a security-embedded organizational culture, underscore the need for ongoing adaptation and collaboration to sustain long-term cybersecurity readiness. The study reconceptualizes cybersecurity readiness as a dynamic, capability-driven process rather than a static checklist. The findings show that cybersecurity is not a one-time compliance exercise but an ongoing, evolving process requiring continuous sensing, seizing, and transforming. Leaders must prioritize adaptive governance structures that encourage strategic agility, flexible policy responses, and proactive risk management. The proposed DC-based framework offers practical guidance for high-risk organizations emphasizing leadership commitment, a security-oriented culture, and resource alignment. Although grounded in Malaysia’s NCII context, the framework has broader applicability for critical infrastructure globally.}
}


@article{DBLP:journals/compsec/AhmadA25,
	author = {Wasim Ahmad and
                  Aitizaz Ali},
	title = {A new era of advanced privacy solutions with a novel IoT framework
                  on {IFTTT}},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104675},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104675},
	doi = {10.1016/J.COSE.2025.104675},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AhmadA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoTs) is revolutionizing industries and daily life, connecting a wide range of devices, and enabling new forms of innovation. However, the surge in the number of IoT devices has bridged major new privacy and security risks, which require additional and smarter solutions. A next-generation privacy solution for IoT ecosystems: IF This Then That (IFTTT) integration? It provides a secure connection between the devices powered by IFTTT’s automation platform, so IoT devices can link through customized triggers and actions, and the data flow and access can be controlled. Not only does this approach simplify security protocols, but it also allows users to set up and automate custom privacy rules so that potential threats can be avoided, allowing for more seamless communication with devices. The paper explores how IFTTT can serve as a dynamic middleware layer that allows real-time threat detection, automated responses, and enhanced privacy enforcement in IoT networks. The case studies and implementation strategies included in this work will highlight how IFTTT can lead the charge to secure IoT environments and the next evolution of privacy solutions.}
}


@article{DBLP:journals/compsec/AllaYL25,
	author = {Ildi Alla and
                  Selma Yahia and
                  Valeria Loscr{\`{\i}}},
	title = {{TRIDENT:} Tri-modal Real-time Intrusion Detection Engine for New
                  Targets},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104676},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104676},
	doi = {10.1016/J.COSE.2025.104676},
	timestamp = {Mon, 27 Oct 2025 08:51:40 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AllaYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing availability of drones and their potential for malicious activities pose significant privacy and security risks, necessitating fast and reliable detection in real-world environments. However, existing drone detection systems often struggle in real-world settings due to environmental noise and sensor limitations. This paper introduces  TRIDENT , a tri-modal drone detection framework that integrates synchronized audio, visual, and RF data to enhance robustness and reduce dependence on individual sensors.  TRIDENT  introduces two fusion strategies—Late Fusion and GMU Fusion—to improve multi-modal integration while maintaining efficiency. The framework incorporates domain-specific feature extraction techniques alongside a specialized data augmentation pipeline that simulates real-world sensor degradation to improve generalization capabilities. A diverse multi-sensor dataset is collected in urban and non-urban environments under varying lighting conditions, ensuring comprehensive evaluation. Experimental results show that  TRIDENT  achieves 96.89% accuracy in real-world recordings and 83.26% in a more complex setting (augmented data), outperforming unimodal and dual-modal baselines. Moreover,  TRIDENT  operates in real-time, detecting drones in just 6.09 ms while consuming only 75.27 mJ per detection, making it highly efficient for resource-constrained devices. The dataset and code have been released to ensure reproducibility ( GitHub Repository ).}
}


@article{DBLP:journals/compsec/DamJKA25,
	author = {Visal Dam and
                  Fariha Tasmin Jaigirdar and
                  Kallol Krishna Karmakar and
                  Adnan Anwar},
	title = {Security-aware data provenance for multi-domain software-defined networks},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104677},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104677},
	doi = {10.1016/J.COSE.2025.104677},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/DamJKA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As interconnectivity increases, Software-defined Networking (SDN) offers a centralized, dynamic, and programmable approach to network management. However, a significant concern lies in the transparency of network devices and data propagation, which contribute to security awareness gaps in SDN domains. Documenting and aggregating network metadata is therefore crucial to detect anomalies and linked events, which is related to the concept of data provenance. However, existing provenance solutions merely collect data without validating it, focus mainly on single-domain SDNs, and overlook supposedly-benign aspects such as switch authentication states, flow rules, and network paths. This paper explores how integrating security metadata into provenance graphs with predefined security policies increases security awareness. With this goal, we propose PRISM-Prov, a security-aware provenance framework for distributed SDNs. To the best of our knowledge, this work is the first of its kind. We identify and discuss the metadata required to enable security awareness, implementing a proof-of-concept for the popular ONOS controller. Our method is tested against six attack scenarios, confirming real-time detection capabilities, and adding only 0.021 ms to 0.102 ms to average packet processing times  − <math><mo is="true">−</mo></math>  an overhead of 4.89% to 13.4% for small and large topologies, respectively  − <math><mo is="true">−</mo></math>  demonstrating low performance costs. Finally, this study promotes security awareness in SDNs to enhance data transparency, as well as risk and trust-based decision-making systems.}
}


@article{DBLP:journals/compsec/AdelAJP25,
	author = {Amr Adel and
                  Noor H. S. Alani and
                  Tony Jan and
                  Mukesh Prasad},
	title = {A review of major {ICT} failures and recovery strategies: Strengthening
                  digital resilience},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104678},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104678},
	doi = {10.1016/J.COSE.2025.104678},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AdelAJP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a comprehensive, cross-sector analysis of large-scale ICT failures to address the persistent gap in understanding how systemic digital breakdowns occur and propagate across platforms and industries. Through a comparative study of seven major global outages (2019–2024) — selected based on scale, technical transparency, and platform diversity — we identify recurring vulnerabilities in automation governance, configuration management, centralized infrastructure, and incident response. Using a custom analytical framework grounded in socio-technical and resilience engineering theory, the paper maps failure propagation patterns and derives a taxonomy of technical and organizational failure modes. We empirically validate a suite of resilience strategies — including rollback automation, configuration-as-code, SOAR-enabled response orchestration, and chaos engineering — and demonstrate how they address failure propagation pathways observed in real-world incidents. A conceptual model for decentralized system upgrade planning is introduced, incorporating microservice segmentation, dependency mapping, and AI-assisted fault containment. The paper culminates in a forward-looking digital resilience roadmap that integrates predictive analytics, secure software supply chains, and adaptive human–machine collaboration. Core contributions include: (1) a cross-case classification of failure archetypes, (2) evidence-based design patterns for resilience, and (3) actionable frameworks for infrastructure operators and researchers working towards next-generation ICT robustness.}
}


@article{DBLP:journals/compsec/YueLZLXT25,
	author = {Shudan Yue and
                  Qingbao Li and
                  Guimin Zhang and
                  Xiaonan Li and
                  Bocheng Xu and
                  Song Tian},
	title = {NPFTaint: Detecting highly exploitable vulnerabilities in Linux-based
                  IoT firmware with network parsing functions},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104679},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104679},
	doi = {10.1016/J.COSE.2025.104679},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/YueLZLXT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The security issues of IoT firmware have become increasingly prominent, particularly taint-style vulnerabilities arising from untrusted external inputs. Although existing solutions work to detect firmware vulnerabilities automatically, they still encounter limitations regarding the accuracy of taint source identification and the efficiency of vulnerability detection. Research has shown that the network parsing function call chain, a critical path for IoT firmware to process external input data, is a high-risk area for firmware vulnerabilities. Inferring the network parsing function accurately plays a crucial role in firmware vulnerability analysis. In this paper, we propose a static analysis method called NPFTaint, which extracts the structural, behavioral, and semantic features of network parsing functions and combines supervised machine learning methods to achieve the identification of network parsing functions. Additionally, unlike traditional forward/backward analysis methods that start from classical sources or sensitive sinks, NPFTaint takes network parsing functions as the entry points, first identifying sensitive sinks on their call chains, and then using value analysis and data dependency analysis of sink-to-source to achieve the detection of highly exploitable vulnerabilities. Experimental evaluations demonstrate that NPFTaint outperforms FITS in accuracy and efficiency when identifying network parsing functions. Regarding vulnerability detection, compared to Mango, NPFTaint not only identifies taint-style vulnerabilities effectively but also improves analysis efficiency, reducing sink analysis by 40.42% and decreasing alerts by 32.77%. This solution provides a more efficient and precise vulnerability detection method for IoT firmware security, contributing to the overall security of the IoT ecosystem.}
}


@article{DBLP:journals/compsec/MiaoHCLWM25,
	author = {Yang Miao and
                  Xiaoyan Hu and
                  Guang Cheng and
                  Ruidong Li and
                  Hua Wu and
                  Yang Meng},
	title = {WEDoHTool: Word embedding based early identification of DoH tunnel
                  tool traffic in dynamic network environments},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104680},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104680},
	doi = {10.1016/J.COSE.2025.104680},
	timestamp = {Thu, 20 Nov 2025 07:44:44 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MiaoHCLWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DNS over HTTPS (DoH) protocol encapsulates DNS plaintext using HTTPS to protect user privacy. However, attackers can exploit various DoH tunnel tools to hide malicious DNS activity or evade detection. Early and accurate DoH tunnel tool traffic identification is crucial to ensure network security and stability by taking targeted countermeasures. The existing research primarily relies on conventional machine learning or deep learning technologies to detect DoH or DoH tunnel traffic based on the statistical features of network flows. The feature extraction relies on expert experience and cannot be performed until network flows or time windows end, delaying the identification of DoH traffic. Besides, the existing methods primarily focus on stable network environments, and their performance likely degrades in dynamic network environments. Moreover, work has yet to be done on identifying specific DoH tunnel tool traffic for targeted defense. Early identification of specific DoH tunnel tools with similar traffic patterns in dynamic network environments is challenging. To address the above concerns, we propose WEDoHTool, an early and accurate DoH tunnel tool traffic identification method based on word embedding technology. WEDoHTool extracts the length sequence of initial TLS records with application data from several initial packets of each unidirectional flow for early identification. Then, it employs word2vec, a word embedding technology, to efficiently capture the stable and complex relationships and patterns within the sequence. Finally, it classifies the embedding vector from the word2vec with a two-stage identification module. Specifically, WEDoHTool filters out DoH traffic from heavy background traffic with a lightweight TextCNN and then identifies the specific DoH tools based on a Transformer encoder with the self-attention mechanism. Our experimental results on the combined dataset consisting of CIRA-CIC-DoHBrw-2020 and DoH-Tunnel-Traffic-HKD demonstrate the effectiveness and efficiency of our WEDoHTool in detecting DoH traffic and identifying specific DoH tunnel tools in dynamic network environments. It maintains accuracies of at least 98.82% and 98.07% in dynamic networks at the two stages, respectively.}
}


@article{DBLP:journals/compsec/AbduazizHS25,
	author = {Kayumov Abduaziz and
                  Chansu Han and
                  Ji Sun Shin},
	title = {Semi-supervised traceability analysis of investigative scanners of
                  darknet traffic},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104681},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104681},
	doi = {10.1016/J.COSE.2025.104681},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AbduazizHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Darknet, an unused IP address space on the Internet, has led to significant research advances in the analyses of global scanning activities, predictions of incoming cyber threats, and the classification of scanning patterns in unsolicited network traffic. However, most darknet traffic research has focused on classification methods that rely on supervised learning, or on unsupervised methods that require further expert effort. To study the applicability of semi-supervision for darknet traffic analysis, we propose a semi-supervised framework that efficiently clusters and classifies scanner behaviors based on existing knowledge for the traceability analysis of investigative scanners on the darknet. The framework utilizes a word embedding model to represent similarly behaving scanners in close proximity in the vector space, followed by a semi-supervised clustering step that incorporates partial labels of known scanners. We validate the framework by combining two publicly available darknet traffic datasets: CAIDA, providing labeled data for semi-supervision, and NICT, that offers a larger set of unlabeled data for analysis. Experimental results demonstrated that integrating semi-supervised learning into darknet traffic analysis improves the interpretability of diverse scanning behaviors and enhances scalability, offering a three-fold speedup in overall runtime compared to the existing sliding window approach. By reducing reliance on fully labeled datasets, the framework facilitates large-scale threat intelligence while allowing for the smooth integration of ever-growing domain knowledge pertaining to darknet traffic. Future research can further refine the model by incorporating additional classes of darknet scanners and expanding the applicability of the model to real-time darknet traffic analysis.}
}


@article{DBLP:journals/compsec/ZhengWHWCW25,
	author = {Chaoyang Zheng and
                  Yunchao Wang and
                  Huihui Huang and
                  Yunfeng Wang and
                  Haowen Chen and
                  Qiang Wei},
	title = {Survey of network protocol fuzzers: Taxonomy, techniques, and directions},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104683},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104683},
	doi = {10.1016/J.COSE.2025.104683},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/ZhengWHWCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fuzzing has become widely adopted for network protocol vulnerability detection due to its high degree of automation and minimal reliance on domain-specific knowledge. Given the distinct characteristics of network protocol programs compared to general targets, researchers have proposed numerous innovative solutions to address the technical challenges. However, there remains a lack of thorough investigation that provide in-depth technical analysis and comprehensive summarization of these advancements, as well as a clear taxonomy to guide future research directions. To bridge this gap, this study conducts a systematic review of network protocol fuzzing and proposes a novel framework with four core modules abstracted from protocol fuzzer architectures. We analyze the key technologies in each module, discussing their advantages, limitations, and application scenarios. More significantly, this work establishes a novel taxonomy defining four fundamental capability dimensions, each addressing distinct practical challenges in protocol fuzzing. Using this framework, we conduct the systematic classification and comparative analysis of existing techniques. Our work contributes theoretical insights and practical guidance for network protocol fuzzing development.}
}


@article{DBLP:journals/compsec/LinXJYLT25,
	author = {Wenfeng Lin and
                  Fangliang Xu and
                  Zhiyuan Jiang and
                  Gang Yang and
                  Zhiwei Li and
                  Chaojing Tang},
	title = {{RPFUZZ:} Efficient network service fuzzing via pruning redundant
                  mutation},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104684},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104684},
	doi = {10.1016/J.COSE.2025.104684},
	timestamp = {Sun, 02 Nov 2025 12:35:00 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LinXJYLT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coverage-guided fuzzing (CGF) has proven its outstanding performance on vulnerability detection. However, existing approaches exhibit limitations when handling network service. Restricted by network I/O duration and chronology, long packet sequences crafted by fuzzers incur a substantial execution cost. Test cases with such non-coverage-improving mutations (i.e. redundant mutation) can significantly reduce fuzzing throughput and compromise vulnerability discovery. To address this issue, we propose RPFUZZ, a novel network fuzzing framework designed to systematically reduce redundant mutations: (1) We propose redundant mutation pruning for network service fuzzing. By early terminating redundant mutations’ execution, RPFUZZ can achieve higher throughput. (2) To detect redundant mutation, we propose redundant mutation oracle. This oracle dynamically judges whether a test case is redundant according to current code coverage and value of service-related variables (SRVs). (3)To identify SRVs, we propose an integrated approach combining dynamic call stack analysis with static value-flow graph (VFG) analysis. To evaluate the performance of RPFUZZ, we implement a prototype on top of NYX-NET. We conduct thorough experiments on ProFuzzBench, a benchmark that consists of 12 real-world network services. The results indicate that RPFUZZ achieves over 185% improvement in throughput and 1.02% rise in code coverage compared with NYX-NET. Besides, RPFUZZ has successfully uncovered 1753 unique crashes across 6 network services, including an unreported vulnerability (assigned to CVE-2024-57392) in ProFTPD, which has been well tested.}
}


@article{DBLP:journals/compsec/Kilic25,
	author = {Ahmet Kilic},
	title = {A quantitative framework for physical cybersecurity in public {EVSE}
                  systems},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104685},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104685},
	doi = {10.1016/J.COSE.2025.104685},
	timestamp = {Mon, 27 Oct 2025 08:51:40 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/Kilic25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Public Electric Vehicle Supply Equipment (EVSE) is increasingly exposed to physical cyberattacks due to its open, unattended, and hardware-accessible deployment in critical infrastructure. Despite growing connectivity, there is a lack of structured and quantitative methodologies to assess the risks arising from physical manipulations targeting components such as power supplies, USB ports, and RFID readers. This study introduces HO-PHYSICS (Holistic Physical Cybersecurity Systematics), a novel framework designed to identify, model, and quantitatively evaluate physical cyber threats in public EVSE environments. The framework consists of three integrated components: (1) Hybrid Threat Structuring (HTS) for modeling attack trees with physical and logical nodes, (2) Attack Potential Evaluation (APE) for multi-criteria risk scoring, and (3) Simulative System Stress Testing (S3T) based on dynamic MATLAB/Simulink simulations. To validate the framework, three representative attack scenarios are examined: PSU manipulation, RFID spoofing, and USB-based sabotage. The corresponding APE scores range from 23 to 30 (out of 50), indicating high feasibility and low detectability. Time-based simulations confirm critical system risks and enable a structured derivation of mitigation strategies. The developed framework bridges a methodological gap between normative security standards and operational risk analysis. It offers a transferable tool for researchers, infrastructure operators, and regulators to assess and improve physical cybersecurity in EVSE systems.}
}


@article{DBLP:journals/compsec/WangZDZ25,
	author = {Weiguang Wang and
                  Xiao Zhang and
                  Jinlian Du and
                  Wenbing Zhao},
	title = {Robust continuous authentication via multi-channel photoplethysmography
                  signals: {A} wearable wristband solution for uncontrolled environments},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104686},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104686},
	doi = {10.1016/J.COSE.2025.104686},
	timestamp = {Fri, 31 Oct 2025 16:06:19 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/WangZDZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing prevalence of wearable devices has intensified security demands for robust continuous authentication (CA) systems to safeguard privacy and data integrity. Conventional one-time authentication methods fail to adapt to dynamic user behavior and environmental variations, leaving systems vulnerable to session hijacking and context-aware attacks. CA addresses these vulnerabilities by persistently monitoring biometric traits, thereby enabling adaptive security policies that balance usability and threat mitigation. Photoplethysmography (PPG) signals are uniquely suited for CA due to their non-invasive acquisition, temporal continuity, and anti-spoofing resilience. However, existing PPG-based CA systems rely on datasets collected under controlled conditions, lacking generalizability to real-world dynamics and sufficient security against multi-layer attacks. To tackle these, we propose a secure and robust CA framework leveraging multi-channel PPG signals from wearable wristbands. We first construct a multi-behavioral PPG dataset from 40 participants with 4-channel signals (dual green, red, and infrared light) under diverse activities. Then we design a multi-stage adaptive filtering pipeline that combines cascaded filters with Independent Component Analysis (ICA), effectively suppressing motion artifacts to improve signal quality. An end-to-end security scheme is integrated to ensure security and privacy of PPG data. Finally, we develop a hybrid Inception-LSTM network for authentication. Experimental results demonstrate a mean authentication accuracy of 94.89%, outperforming conventional single-channel baselines by 23.28% and exhibiting enhanced robustness against signal distortions.}
}


@article{DBLP:journals/compsec/RostamiKKG25,
	author = {Elham Rostami and
                  Fredrik Karlsson and
                  Ella Kolkowska and
                  Shang Gao},
	title = {Towards software for tailoring information security policies to organisations'
                  different target groups},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104687},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104687},
	doi = {10.1016/J.COSE.2025.104687},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/RostamiKKG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Designing accessible and relevant information security policies (ISPs) that support employees is crucial for improving organisations’ information security. When employees are required to deal with cumbersome ISPs, there is a risk of reduced motivation towards information security, and employees’ not following the rules in ISPs has been reported as a persistent issue. Existing research has suggested adopting a tailored approach to ISPs in order to enhance their relevance to employees. Tailoring is difficult and time consuming and information security managers lack information security management systems software (ISMSS) that can assist with this tailoring task. In this paper, we develop a design theory for ISMSS to support information security managers in tailoring ISPs to different employees. To achieve this, we employ design science research, drawing on prior studies concerning the tailoring of systems development methods. We evaluate the design theory through an expository instantiation, POLCO, and with information security managers, demonstrating both proof-of-concept and proof-of-value.}
}


@article{DBLP:journals/compsec/SuoCCCLX25,
	author = {Yuhan Suo and
                  Runqi Chai and
                  Kaiyuan Chen and
                  Senchun Chai and
                  Wannian Liang and
                  Yuanqing Xia},
	title = {Robust set partitioning strategy for malicious information detection
                  in large-scale Internet of Things},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104688},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104688},
	doi = {10.1016/J.COSE.2025.104688},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/SuoCCCLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the Internet of Things (IoT), the risks of data tampering and malicious information injection have intensified, making efficient threat detection in large-scale distributed sensor networks a pressing challenge. To address the decline in malicious information detection efficiency as network scale expands, this paper investigates a robust set partitioning strategy and, on this basis, develops a distributed attack detection framework with theoretical guarantees. Specifically, we introduce a gain mutual influence metric to characterize the inter-subset interference arising during gain updates, thereby revealing the fundamental reason for the performance gap between distributed and centralized algorithms. Building on this insight, the set partitioning strategy based on Grassmann distance is proposed, which significantly reduces the computational cost of gain updates while maintaining detection performance, and ensures that the distributed setting under subset partitioning preserves the same theoretical performance bound as the baseline algorithm. Unlike conventional clustering methods, the proposed set partitioning strategy leverages the intrinsic observational features of sensors for robust partitioning, thereby enhancing resilience to noise and interference. Simulation results demonstrate that the proposed method limits the performance gap between distributed and centralized detection to no more than 1.648%, while the computational cost decreases at an order of  O ( 1 / m ) <math><mrow is="true"><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">/</mo><mi is="true">m</mi><mo is="true">)</mo></mrow></mrow></math>  with the number of subsets  m <math><mi is="true">m</mi></math> . Therefore, the proposed algorithm effectively reduces computational overhead while preserving detection accuracy, offering a practical low-cost and highly reliable security detection solution for edge nodes in large-scale IoT systems.}
}


@article{DBLP:journals/compsec/RuohonenRB25,
	author = {Jukka Ruohonen and
                  Kalle Rindell and
                  Simone Busetti},
	title = {From cyber security incident management to cyber security crisis management
                  in the European Union},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104689},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104689},
	doi = {10.1016/J.COSE.2025.104689},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/RuohonenRB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incident management is a classical topic in cyber security. Recently, the European Union (EU) has started to consider also the relation between cyber security incidents and cyber security crises. These considerations and preparations, including those specified in the EU’s new cyber security laws, constitute the paper’s topic. According to an analysis of the laws and associated policy documents, (i) cyber security crises are equated in the EU to large-scale cyber security incidents that either exceed a handling capacity of a single member state or affect at least two member states. For this and other purposes, (ii) the new laws substantially increase mandatory reporting about cyber security incidents, including but not limited to the large-scale incidents. Despite the laws and new governance bodies established by them, however, (iii) the working of actual cyber security crisis management remains unclear particularly at the EU-level. With these policy research results, the paper advances the domain of cyber security incident management research by elaborating how European law perceives cyber security crises and their relation to cyber security incidents, paving the way for many relevant further research topics with practical relevance, whether theoretical, conceptual, or empirical.}
}


@article{DBLP:journals/compsec/AryalGAS25,
	author = {Kshitiz Aryal and
                  Maanak Gupta and
                  Mahmoud Abdelsalam and
                  Moustafa Saleh},
	title = {Intra-section code cave injection for adversarial evasion attacks
                  on windows {PE} malware file},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104690},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104690},
	doi = {10.1016/J.COSE.2025.104690},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AryalGAS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Windows malware is predominantly available in cyberspace and is a prime target for deliberate adversarial evasion attacks. Although researchers have investigated the adversarial malware attack problem, a multitude of important questions remain unanswered, including (a) Are the existing techniques to inject adversarial perturbations in Windows Portable Executable (PE) malware files effective enough for evasion purposes?; (b) Does the attack process preserve the original behavior of malware?; (c) Are there unexplored approaches/locations that can be used to carry out adversarial evasion attacks on Windows PE malware?; and (d) What are the optimal locations and sizes of adversarial perturbations required to evade an ML-based malware detector without significant structural change in the PE file? To answer some of these questions, this work proposes a novel approach that injects a code cave within the section (i.e., intra-section) of Windows PE malware files to make space for adversarial perturbations. Additionally, a code loader is injected into the PE file, which reverses the effects of adversarial malware during execution, preserving the malware’s functionality and executability. To understand the effectiveness of our approach, we inject adversarial perturbations inside the  .text ,  .data  and  .rdata  sections, generated using the gradient descent and Fast Gradient Sign Method (FGSM) to target the two popular CNN-based malware detectors, MalConv and MalConv2. Our experimental analysis yielded impressive results, achieving an evasion rate of 92.31% with gradient descent and 96.26% with FGSM when targeting MalConv, as compared to the evasion rate of 16.17% for append attacks. Similarly, in the case of an attack against MalConv2, our approach achieves a remarkable maximum evasion rate of 97.93% with gradient descent and 94.34% with FGSM, significantly surpassing the 4.01% and 54.75% evasion rates observed with append attacks.}
}


@article{DBLP:journals/compsec/JiangWYZZLCW25,
	author = {Hao Jiang and
                  Kang Wang and
                  Yujie Yang and
                  Shan Zhong and
                  Shuai Zhang and
                  Chengjie Liu and
                  Xiarun Chen and
                  Weiping Wen},
	title = {DynamicFuzz: Confidence-based directed greybox fuzzing for programs
                  with unreliable call graphs},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104691},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104691},
	doi = {10.1016/J.COSE.2025.104691},
	timestamp = {Thu, 20 Nov 2025 13:44:19 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/JiangWYZZLCW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Directed greybox fuzzing (DGF) is a security testing technique designed to test specific targets. Current DGF techniques face challenges due to the dynamic nature of indirect calls. The main challenges include mitigating the influence of indirect call omissions and misjudgments on seed guidance and guiding fuzzing on unreliable function call graphs. This paper introduces DynamicFuzz, a novel dynamic guidance mechanism that uses the confidence of indirect calls to update the call graph and adjust path priorities during fuzzing. Our key insight is that functions connected by indirect calls tend to form function islands in the call graph. These islands help focus fuzzing on critical areas, improving both guidance efficiency and control over complex program structures. DynamicFuzz also incorporates two depth metrics – function depth and island depth – to better estimate the importance of each path. Based on this, DynamicFuzz employs four guiding strategies: the Target Function Selection Strategy, the Function Island Prioritization Strategy, the High-Confidence Path Prioritization Strategy, and the Deep Indirect Call Prioritization Strategy. These strategies allow DynamicFuzz to guide fuzzing effectively even when the call graph is unreliable. We evaluate DynamicFuzz on 17 benchmarks from three test suites. Compared to AFLGo, AFL, and FairFuzz, it reaches target locations 5.64 × <math><mo is="true">×</mo></math>  , 3.01 × <math><mo is="true">×</mo></math>  , and 2.89 × <math><mo is="true">×</mo></math>  faster, and detects target crashes 69.8 × <math><mo is="true">×</mo></math>  , 48.37 × <math><mo is="true">×</mo></math>  , and 161.20 × <math><mo is="true">×</mo></math>  faster, respectively. Additionally, DynamicFuzz discovered 8 CVEs from the real world.}
}


@article{DBLP:journals/compsec/BarredoEFI25,
	author = {Jorge Barredo and
                  Maialen Eceiza and
                  Jose Luis Flores and
                  Mikel Iturbe},
	title = {{GJALLARHORN:} {A} framework for vulnerability detection via electromagnetic
                  side-channel analysis in embedded systems},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104692},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104692},
	doi = {10.1016/J.COSE.2025.104692},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/BarredoEFI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of embedded systems within the Internet of Things (IoT) has heightened the difficulty of detecting vulnerabilities due to their inherent resource constraints. This paper introduces GJALLARHORN, a framework extending electromagnetic side-channel analysis (EM SCA) for early-stage vulnerability detection in embedded systems. Unlike conventional methods requiring code access or imposing computational overhead, GJALLARHORN non-invasively analyses EM emissions to identify anomalous patterns indicating potential security vulnerabilities. By observing hardware-level manifestations of software execution, GJALLARHORN complements software-level analysis, revealing vulnerabilities that might otherwise remain undetected. The framework adapts to device complexity, enabling categorisation of up to 16 distinct vulnerability types, including buffer overflows, memory leaks, and arithmetic errors. Evaluations on both low-end (STM NUCLEO-144) and high-end (Raspberry Pi 3B) architectures demonstrate GJALLARHORN’s effectiveness, achieving a recall of 95.94% and  F 1 <math><msub is="true"><mrow is="true"><mi is="true">F</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>  score of 96.39% on the low-end system, and 73.33% recall with 84.61%  F 1 <math><msub is="true"><mrow is="true"><mi is="true">F</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>  score on the high-end system. Our results reveal that memory-related vulnerabilities produce more distinguishable EM signatures than arithmetic errors, offering valuable insights for externally detecting vulnerabilities. By enabling detection during development, GJALLARHORN helps mitigate risks before deployment, potentially reducing the economic impact of security incidents in IoT infrastructure.}
}


@article{DBLP:journals/compsec/GarciaPenasRM25,
	author = {Rodolfo Garc{\'{\i}}a{-}Pe{\~{n}}as and
                  Rafael A. Rodr{\'{\i}}guez{-}G{\'{o}}mez and
                  Gabriel Maci{\'{a}}{-}Fern{\'{a}}ndez},
	title = {Characterizing Internet Background Traffic from a Spain-Based Network
                  Telescope},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104693},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104693},
	doi = {10.1016/J.COSE.2025.104693},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/GarciaPenasRM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet background traffic (or Internet Background Radiation, IBR) consists of unsolicited packets. It is traffic usually generated in the preliminary phases of attacks by computers making enumerations of targets and available services, sent as responses to denial of service attacks, or sent by mistake due to incorrect configurations and commands. Capturing and analysing this traffic enables the observation of Internet activity and serves as an important tool for identifying new types of attacks and attackers. This traffic is captured by “network telescopes”, nodes that advertise blocks of unused IP addresses and store the traffic sent to them. This article studies the traffic received by a network telescope located in Spain during 2023, with more than 4.7 billion packets and 362.39 GB of information. A statistical breakdown of the packets by protocol shows that TCP accounts for 95.96%, UDP for 3.74%, and ICMP for 0.51%. In addition, the behaviour of the traffic generators targeting the telescope’s addresses is examined, and the main attacks – such as NTP and DNS reflection – are analysed. The characteristics of the traffic are compared with those of previous studies, highlighting changes in behaviour and the most common attacks.}
}


@article{DBLP:journals/compsec/XieLD25,
	author = {Yuanfeng Xie and
                  Hanqing Luo and
                  Aoxue Ding},
	title = {Design of a high-stability {QPUF} and {QRNG} circuit based on {CCNOT}
                  gate},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104694},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104694},
	doi = {10.1016/J.COSE.2025.104694},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/XieLD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum computing, with its powerful computational capabilities, is expected to become a secure paradigm for solving complex problems. However, existing cloud-based quantum computing systems are reliant on cloud service providers for scheduling, making it impossible to directly verify the results produced by quantum hardware. This introduces significant security risks, such as scenarios where a third-party provider allocates quantum computers with suboptimal hardware performance or attackers redirect execution on the hardware to steal critical keys. Current solutions face issues with limited authentication dimensions and poor stability of physical fingerprints. This study proposes a highly stable Quantum Physical Unclonable Function (QPUF) and Quantum Random Number Generator (QRNG) based on quantum superposition and entanglement. First, a circuit model is created using the Hadamard and R Y  gate to generate tunable equal-amplitude superposition states, encoding the measurement probabilities. Dynamic Majority Voting (DMV) is then applied to improve the stability of the QPUF response, which can serve as an effective ID for cloud-executed devices. Next, the CCNOT gate is used to entangle multiple qubits, producing a QRNG with high worst-case entropy, which can be utilized as a high-performance random number generator for computations. Finally, experiments conducted on IBM's quantum hardware demonstrate that the stability of the proposed QPUF in the new unified architecture is 100%, representing a 4.16% improvement over similar models. The worst-case entropy of the QRNG is 0.974, fully validating the effectiveness of the proposed architecture in countering attacks that attempt to tamper with cloud-based quantum computing hardware.}
}


@article{DBLP:journals/compsec/LiuQL25,
	author = {Xiaojian Liu and
                  Zilin Qin and
                  Kehong Liu},
	title = {{\(\mu\)}GAN: {A} mutation-based cost optimal adversarial malware
                  generation approach against evolving Android malware variants},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104695},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104695},
	doi = {10.1016/J.COSE.2025.104695},
	timestamp = {Wed, 19 Nov 2025 07:42:10 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/LiuQL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malware  detection  and  evasion  constitute a pair of opponents locked in a relentless competitive game—to bypass stringent detection mechanisms, Android malware has evolved a variety of sophisticated evasion techniques, been continuously spawning new malware variants, which poses an ongoing challenge for Android defense systems to efficiently detect these evolving threats. To tackle this problem, adversarial training offers a promising approach to improving the resilience of detection systems against newly emerging malware variants. However, in the setting of Android malware detection, adversarial training still faces a critical challenge—how to craft  valid  and  meaningful  adversarial samples. This paper proposes a mutation-based adversarial malware generation approach, which attempts to introduce proper perturbations to the seed samples in order to enable them to successfully evade detection. To seek for such perturbations, we formulate the problem of crafting adversarial malware as a constrained combinatorial optimization problem—adversarial samples should evade detection while consuming minimal crafting efforts. For this problem, we devise a solution strategy, referred to as  μ <math><mi is="true">μ</mi></math> GAN, which combines strengths of the Generative Adversarial Networks and the Simulated Annealing algorithm, to screen the optimal adversarial samples. Furthermore, we retrain an enhanced malware classifier by augmenting the dataset with the generated adversarial malware samples to improve the performance of detection against new malware variants. Extensive experimental evaluation shows that, introducing perturbations into malware can significantly promote the ability of malware to evade security detection; the enhanced malware detector retrained using our approach demonstrates superior performance over other state-of-the-art classifiers.}
}


@article{DBLP:journals/compsec/TsirakisFMP25,
	author = {Orestis Tsirakis and
                  Konstantinos Fysarakis and
                  Vasileios Mavroeidis and
                  Ioannis Papaefstathiou},
	title = {Operationalizing cybersecurity knowledge: Design, implementation {\&}
                  evaluation of a knowledge management system for {CACAO} playbooks},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104696},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104696},
	doi = {10.1016/J.COSE.2025.104696},
	timestamp = {Thu, 25 Dec 2025 12:44:12 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/TsirakisFMP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern cybersecurity threats are growing in complexity, targeting increasingly intricate and interconnected systems. To effectively defend against these evolving threats, security teams utilize automation and orchestration to enhance response efficiency and consistency. In that sense, cybersecurity playbooks and workflows are key enablers, as they provide a structured, reusable, and continuously evolving approach to incident response. They enable organizations to codify requirements, operational domain expertise, underlying organizational policies, regulatory obligations, and best practices. Moreover, playbooks enhance and standardize the decision-making process, while allowing automation in areas where reliability is sufficiently established to ensure that mission assurance is not compromised. The emerging Collaborative Automated Course of Action Operations (CACAO) technical specification and standard defines a common machine-processable schema for cybersecurity playbooks, facilitating interoperability for their exchange and ensuring the ability to orchestrate and automate cybersecurity operations. However, despite its potential and the fact that it is a relatively new standardization effort, there is a lack of tools to support its adoption, particularly in the management and lifecycle development of CACAO playbooks, which limits their practical deployment. Motivated by the above, this work presents the design, development, and evaluation of a Knowledge Management System (KMS) for managing CACAO cybersecurity playbooks throughout their lifecycle. It provides essential tools to improve cybersecurity maturity, strengthens collaboration and coordination within and across organizations, and streamlines playbook management. By utilizing open-source technologies and open standards, the proposed approach promotes interoperability and enhances the usability of state-of-the-art cybersecurity orchestration and automation primitives. To encourage adoption, the resulting implementation is released as open-source, which, to the best of our knowledge, comprises the first publicly available and documented work in this domain, supporting the broader uptake of CACAO playbooks and promoting the widespread use of interoperable automation and orchestration mechanisms in cybersecurity operations.}
}


@article{DBLP:journals/compsec/HeLLHL25,
	author = {Xianliang He and
                  Junyi Li and
                  Yaping Lin and
                  Qiao Hu and
                  Xiehua Li},
	title = {{TDSF:} Trajectory-preserving method of dual-strategy fusion with
                  differential privacy in {LBS}},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104697},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104697},
	doi = {10.1016/J.COSE.2025.104697},
	timestamp = {Thu, 25 Dec 2025 12:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/HeLLHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When the public utilizes location-based services (LBS), a large amount of trajectory data is generated, and their location information is constantly exposed. However, providing trajectories to LBS without additional protection may result in the leakage of location privacy and correlation privacy in the trajectory. Most current methods only protect the location privacy of trajectories by adjusting the allocation of privacy budgets, without combining multiple strategies to protect location privacy and correlation privacy. These methods also struggle to balance data availability and privacy for trajectories. To address the above challenges, we propose a trajectory-preserving method of dual-strategy fusion with differential privacy (TDSF). Specifically, one strategy is used to protect the correlation privacy between sensitive locations, and the other is used to protect the non-sensitive locations. We use the trained transfer correlation matrix to extract sensitive locations in a trajectory that require correlation protection. The remaining locations introduce less noise as they involve minimal privacy disclosure, thus maintaining data availability. Finally, we also designed a privacy budget allocation strategy that is suitable for this dual-strategy fusion scenario. Strict security analysis shows that the mechanism we propose can well protect the location and correlation privacy of the trajectory. The experimental results on real data sets further demonstrate the advantages of this mechanism in data availability and confidentiality.}
}


@article{DBLP:journals/compsec/DashSSDV25,
	author = {Priyabrata Dash and
                  Debasis Samanta and
                  Monalisa Sarma and
                  Ashok Kumar Das and
                  Athanasios V. Vasilakos},
	title = {Privacy preserving unique robust and revocable passcode generation
                  from fingerprint data},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104698},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104698},
	doi = {10.1016/J.COSE.2025.104698},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/DashSSDV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This research explores generating passcodes from fingerprint images. The investigation unfolds through a three-step process: (a) fixed length feature vector generation from minutia features, (b) stable binary feature vector generation from the fixed length feature vector, and (c) passcode generation from the stable binary feature vector. The main research objectives of this work are: (1) how a unique and robust binary pattern can be generated from a fingerprint image, (2) from this binary bit pattern, how a passcode can be generated satisfying non-linkable and revocable properties, and (3) how an attacker cannot guess the source biometric given a passcode thus preserving the privacy of the fingerprint data. The generated passcode can be applied in many applications, such as unique identity generation for authentication without enrollment, encryption key generation for network security, remote authentication protocol or distributed systems, data storage security, digital wallet, etc. The proposed approach has been validated with FVC2002 and FVC2004, and results show impressive genuine acceptance rates of 99.31% and 99.25%, with 0% false acceptance rates. Further, the generated passcodes pass NIST and Diehard randomness tests, substantiating the potential key generation technique with high intra-similarity and low inter-similarity.}
}


@article{DBLP:journals/compsec/MartinezSSRF25,
	author = {Ferney Mart{\'{\i}}nez and
                  Luis Enrique S{\'{a}}nchez and
                  Antonio Santos{-}Olmo and
                  David Garcia Rosado and
                  Eduardo Fern{\'{a}}ndez{-}Medina},
	title = {Integrated maritime protection: Innovation for the safeguarding of
                  maritime systems based on {MARISMA}},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104699},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104699},
	doi = {10.1016/J.COSE.2025.104699},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/MartinezSSRF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The maritime sector is becoming increasingly susceptible to sophisticated cyber-attacks, underscoring the pressing necessity for advanced research and development to establish robust safeguards for maritime assets. Although risk assessment methods for traditional IT systems are now highly developed, they are not directly applicable to risk assessment in maritime environments due to the specific characteristics and particularities of the latter. Therefore, there is an urgent need to define approaches that adequately support risk assessment in maritime environments. To contribute to this important challenge, we propose a novel risk analysis technique, specifically tailored for the maritime sector, based on MARISMA, a security management methodology, and eMARISMA, its cloud-based technological support tool. Our work contributes to the state of the art by defining the MARISMA-SHIPS maritime cybersecurity pattern, which includes a set of reusable and adaptable elements that enable risk management and control in a maritime environment, and is aligned with major international standards such as ENISA and NIST, as well as existing maritime regulations, becoming a key part of our ongoing POSEIDON maritime cybersecurity framework. A case study is presented for a ship developed in the main shipyard in Colombia, which shows how the reusability and adaptability of the proposal allows the proposed MARISMA-SHIPS pattern to be easily adapted to any maritime environment, and which allowed the identification of critical areas of cybersecurity that could be improved. The application of the process in the maritime domain has proven its value in improving the efficiency and security management of maritime assets.}
}


@article{DBLP:journals/compsec/AnandCCS25,
	author = {P. Mohan Anand and
                  P. V. Sai Charan and
                  Hrushikesh Chunduri and
                  Sandeep Kumar Shukla},
	title = {{LARM:} Linux Anti Ransomware Monitor},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104700},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104700},
	doi = {10.1016/J.COSE.2025.104700},
	timestamp = {Thu, 25 Dec 2025 12:44:11 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/AnandCCS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Linux becomes more prevalent across servers, desktops, and cloud infrastructures, ransomware groups increasingly focus on targeting Linux-based systems, particularly those running on widely deployed x86 architectures. However, research on real-time, lightweight ransomware detection for Linux systems remains limited. The existing approaches, based on file backups, trap or decoy files, and file I/O behavior monitoring, are found to be ineffective against multithreaded ransomware variants, often leading to delayed detection and false positives. In this work, we introduce LARM (Linux Anti-Ransomware Monitor), a lightweight, real-time detection tool tailored for Linux systems with  x86_64  architecture. LARM employs a file trap monitoring module that operates at the kernel level using eBPF (extended Berkeley Packet Filter) to detect real-time ransomware activity. LARM dynamically selects trap files for monitoring through a non-parametric clustering approach of Affinity Propagation, combined with the encryption order heuristics observed in ransomware behavior. Since sole reliance on trap file monitoring may result in false positives, LARM integrates a secondary profiling mechanism that analyzes pre-encryption ransomware activity in real-time. We evaluated LARM against 14 modern Linux ransomware families, including multithreaded versions of Avos Locker and Babuk. The evaluation results demonstrate an average detection delay of 1,240 ms and a file loss rate of 0.46%, highlighting the effectiveness of LARM in early detection and mitigation of ransomware in Linux systems.}
}


@article{DBLP:journals/compsec/HuYZ25,
	author = {Zhaowang Hu and
                  Jun Ye and
                  Zhengqi Zhang},
	title = {Faster secure and efficient collaborative private data cleaning based
                  on {PSI}},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104701},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104701},
	doi = {10.1016/J.COSE.2025.104701},
	timestamp = {Wed, 21 Jan 2026 14:21:50 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/HuYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mislabeled datasets are common in the detection of software malicious behaviors in the real world. When two different Security Operation Centers (SOCs) classify the same malware attack into different threat categories due to differing detection methodologies, this creates significant challenges and security risks for subsequent operations. Through collaborative, both parties aim to align their datasets by filtering out severely misclassified or erroneously labeled entries while preserving privacy. In this privacy-preserving collaborative data cleaning scenario, each party can only learn intersection contents and misclassified items within the intersection, without obtaining any private information about non-intersection data entries. To address this challenge, we propose a novel Secure and Efficient Collaborative Private Data Cleaning Scheme (SCPDC). The scheme comprises two phases: an offline phase responsible for pre-generating computationally expensive share tuples and label encoding operations, and an online phase that utilizes these pre-generated share tuples and encoded vectors to execute a variant-labeled PSI protocol for identifying misclassified items in the intersection. SCPDC achieves an exceptionally efficient online phase while fulfilling privacy requirements for both parties. Security analysis and experimental results demonstrate that SCPDC offers reasonable execution time and lower communication overhead compared to existing related works.}
}


@article{DBLP:journals/compsec/WinklerS25,
	author = {Aidan M. Winkler and
                  Prinkle Sharma},
	title = {Proactive threat detection in enterprise systems using Wazuh: {A}
                  {MITRE} ATT{\&}CK Evaluation},
	journal = {Comput. Secur.},
	volume = {159},
	pages = {104702},
	year = {2025},
	url = {https://doi.org/10.1016/j.cose.2025.104702},
	doi = {10.1016/J.COSE.2025.104702},
	timestamp = {Sat, 15 Nov 2025 13:51:28 +0100},
	biburl = {https://dblp.org/rec/journals/compsec/WinklerS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proactive detection of advanced adversarial behaviors remains a critical challenge for Security Information and Event Management (SIEM) platforms, particularly as attackers adopt stealthy, multi-phase campaigns. This paper presents a cross-platform, MITRE ATT&CK aligned evaluation framework for systematically measuring the SIEM detection coverage, responsiveness, and accuracy. The framework was demonstrated through the Wazuh SIEM platform and atomic red team testing, targeting four high-impact tactics: Collection, Command-and-Control (C2), Exfiltration, and Impact. The results show a high detection rate for C2 and Impact techniques, and partial detection for Collection and Ex-filtration tactics owing to gaps in correlation and telemetry depth. The overall detection rate was approximately 85%, with platform-specific differences driven by the endpoint logging capabilities. Quantitative performance analysis yielded a precision of 91.4%, recall of 85.2%, and false positive rate of 4.8%, confirming both detection effectiveness and operational feasibility. The main contributions of this study are as follows: (i) a reproducible, ATT&CK aligned framework adaptable to both open source and commercial SIEMs, (ii) actionable detection rule enhancements to improve Security Operations Centerwork (SOC) operations, and (iii) scalability considerations for deployment in enterprise environments. By integrating structured adversary modeling with operational SOCs flows, the proposed framework advances proactive cyber defence in complex enterprise environments.}
}
