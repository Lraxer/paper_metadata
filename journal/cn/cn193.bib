@article{DBLP:journals/cn/YuLLZ21,
	author = {Li Yu and
                  Zongpeng Li and
                  Jiangchuan Liu and
                  Ruiting Zhou},
	title = {Online and energy-efficient task-processing for distributed edge networks},
	journal = {Comput. Networks},
	volume = {193},
	pages = {107875},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.107875},
	doi = {10.1016/J.COMNET.2021.107875},
	timestamp = {Tue, 08 Jun 2021 09:20:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YuLLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User equipment produces a series of tasks that are processed locally or remotely, falling into three categories: (i) local computing only, (ii) a fraction of the task is computed locally and the remaining task unprocessed is offloaded for remote computation, and (iii) the entire task is offloaded. Each case has attracted substantial attention in recent studies, where a delay-constrained non-linear optimization problem is often formulated. The solutions employed are either based on Lagrange duality, heuristic search, or dynamic programming. To our knowledge, there is no unifying task-processing orchestrator that is an online tailored solver for learning the model-free problems, encapsulating the three cases above. We fill this gap and present the first attempt on an innovative actor-critic reinforcement learning approach in consideration of the energy-efficiency, to compute the asymptotically optimal solutions via decomposing the comprehensive optimization into sub-problems. Rigorous theoretical analyses and experience-driven simulations demonstrate significant advantages over the benchmark approaches, in terms of task-processing delay, power efficiency, and convergence time.}
}


@article{DBLP:journals/cn/TangMBAAH21,
	author = {Yuan Tang and
                  Yiming Miao and
                  Ahmed Barnawi and
                  Bander A. Alzahrani and
                  Reem Alotaibi and
                  Kai Hwang},
	title = {A joint global and local path planning optimization for {UAV} task
                  scheduling towards crowd air monitoring},
	journal = {Comput. Networks},
	volume = {193},
	pages = {107913},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.107913},
	doi = {10.1016/J.COMNET.2021.107913},
	timestamp = {Thu, 14 Oct 2021 09:09:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TangMBAAH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale crowd management systems are used to monitor and manage crowds in various industries aspects by utilizing relevant innovative technologies. In order to overcome the shortcomings of traditional CCTV equipment in shooting angle and deployment, some scholars propose to use unmanned ariel vehicle (UAV) carried appropriate optical sensory equipment to perform aerial scene surveillance. However, UAV flight missions have problems such as poor adaptability of single-mode path planning to site conditions space and complex cluster scheduling systems. Therefore, we combine the improved particle swarm optimization(PSO) algorithm, the optimized artificial potential algorithm, path exploration switching mode and energy-based task scheduling mechanism to propose a joint global and local path planning optimization for UAV task scheduling towards crowd air monitoring (JGLPP-UTS). In this model, the PSO algorithm is improved based on mutation mechanism and iterative number dependent adaptive inertia weight, we add a path smoothing mechanism. Then, we optimize the artificial potential algorithm for the problem of the target point unreachable and the local minimum. The proposed model switches the path planning mode according to the global and local obstacle environment. Finally, our model comprehensively considers the information of the site to realize the surveillance task scheduling of the UAV. Experiments show that our proposed algorithm can effectively improve the ability of global and local path planning. Compared with the standard PSO path length, the global path is reduced by 8.92%, and the adaptive value is reduced by 82.9%. After the smoothing operation, we also report that the path length can be further reduced. Moreover, the task scheduling strategy can realize the effective use of airborne resources.}
}


@article{DBLP:journals/cn/BansalCKPA21,
	author = {Gaurang Bansal and
                  Vinay Chamola and
                  Georges Kaddoum and
                  Md. Jalil Piran and
                  Mubarak Alrashoud},
	title = {Next generation stock exchange: Recurrent neural learning model for
                  distributed ledger transactions},
	journal = {Comput. Networks},
	volume = {193},
	pages = {107998},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.107998},
	doi = {10.1016/J.COMNET.2021.107998},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BansalCKPA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A distributed stock exchange system encompasses multiple network hosts that participate in the sharing and exchange of resources. In such exchanges, the mediator or stock exchange must manage and delineate all operations in a cohesive manner. Stock exchange (SE) also acts as the transaction manager to provide consistent, isolated, durable, and atomic transactions for participating entities. However, the work for the stock exchange is not so straightforward as it may sound. With multiple transactions happening per second, the global serializability and concurrency control becomes an issue resulting in multiple threats and vulnerabilities. We propose a novel stock exchange that integrates time series prediction to distributed transactions and understanding the rapid global transactions and limitations of resources at the stock exchange. We use distributed acyclic graph (DAG) based distributed ledger technology IOTA to provide security and consensus for independent users. The paper proposes a time-variant model that adjusts its predictions based on transactions, moments of observations, participating entities, and history. We show that our model outcasts other state-of-art schemes in terms of prediction accuracy. Also, the model is fair, fast, and scalable to handle millions of transactions per second.}
}


@article{DBLP:journals/cn/ShamsoshoaraARZ21,
	author = {Alireza Shamsoshoara and
                  Fatemeh Afghah and
                  Abolfazl Razi and
                  Liming Zheng and
                  Peter Z. Ful{\'{e}} and
                  Erik Blasch},
	title = {Aerial imagery pile burn detection using deep learning: The {FLAME}
                  dataset},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108001},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108001},
	doi = {10.1016/J.COMNET.2021.108001},
	timestamp = {Sat, 30 Sep 2023 10:07:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShamsoshoaraARZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wildfires are one of the costliest and deadliest natural disasters in the US, causing damage to millions of hectares of forest resources and threatening the lives of people and animals. Of particular importance are risks to firefighters and operational forces, which highlights the need for leveraging technology to minimize danger to people and property. FLAME (Fire Luminosity Airborne-based Machine learning Evaluation) offers a dataset of aerial images of fires along with methods for fire detection and segmentation which can help firefighters and researchers to develop optimal fire management strategies.}
}


@article{DBLP:journals/cn/Radoglou-Grammatikis21,
	author = {Panagiotis I. Radoglou{-}Grammatikis and
                  Panagiotis G. Sarigiannidis and
                  Eider Iturbe and
                  Erkuden Rios and
                  Saturnino Martinez and
                  Antonios Sarigiannidis and
                  Georgios Eftathopoulos and
                  Yannis Spyridis and
                  Achilleas Sesis and
                  Nikolaos Vakakis and
                  Dimitrios Tzovaras and
                  Emmanouil Kafetzakis and
                  Ioannis Giannoulakis and
                  Michalis Tzifas and
                  Alkiviadis Giannakoulias and
                  Michail K. Angelopoulos and
                  Francisco Ramos},
	title = {{SPEAR} {SIEM:} {A} Security Information and Event Management system
                  for the Smart Grid},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108008},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108008},
	doi = {10.1016/J.COMNET.2021.108008},
	timestamp = {Mon, 28 Aug 2023 21:39:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Radoglou-Grammatikis21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The technological leap of smart technologies has brought the conventional electrical grid in a new digital era called Smart Grid (SG), providing multiple benefits, such as two-way communication, pervasive control and self-healing. However, this new reality generates significant cybersecurity risks due to the heterogeneous and insecure nature of SG. In particular, SG relies on legacy communication protocols that have not been implemented having cybersecurity in mind. Moreover, the advent of the Internet of Things (IoT) creates severe cybersecurity challenges. The Security Information and Event Management (SIEM) systems constitute an emerging technology in the cybersecurity area, having the capability to detect, normalise and correlate a vast amount of security events. They can orchestrate the entire security of a smart ecosystem, such as SG. Nevertheless, the current SIEM systems do not take into account the unique SG peculiarities and characteristics like the legacy communication protocols. In this paper, we present the Secure and PrivatE smArt gRid (SPEAR) SIEM, which focuses on SG. The main contribution of our work is the design and implementation of a SIEM system capable of detecting, normalising and correlating cyberattacks and anomalies against a plethora of SG application-layer protocols. It is noteworthy that the detection performance of the SPEAR SIEM is demonstrated with real data originating from four real SG use case (a) hydropower plant, (b) substation, (c) power plant and (d) smart home.}
}


@article{DBLP:journals/cn/LiDD21,
	author = {Bo Li and
                  Xiaoheng Deng and
                  Yiqin Deng},
	title = {Mobile-edge computing-based delay minimization controller placement
                  in SDN-IoV},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108049},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108049},
	doi = {10.1016/J.COMNET.2021.108049},
	timestamp = {Tue, 08 Jun 2021 09:20:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiDD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Vehicles (IoV) plays a fundamental role in the rapid development of intelligent transportation systems and smart cities by the way of facilitating data access. However, traditional routing protocols-based data forwarding cannot guarantee reliable services for low-latency applications in IoV. Motivated by the promising technologies of Software-defined networking (SDN) and Mobile-edge computing, this paper aims at enhancing the performance of IoV with the assistance of these two. Specifically, we first propose a three-layer hierarchy control framework for SDN-IoV based on the MEC. Under this framework, we investigate a controller placement problem with the objective of minimizing the delay between the switch and the controller. Then, we propose a Louvain algorithm-based controller placement policy to obtain the optimal location of controllers subject to load balance index and buffer size. Further, we put forward a controller replacement policy to adapt to the dynamic topology of IoV. Numerical results show that the proposed methods achieve a better performance than two baselines, in terms of the delay and load balance index.}
}


@article{DBLP:journals/cn/ZhangZSL21,
	author = {Qingyang Zhang and
                  Hong Zhong and
                  Weisong Shi and
                  Lu Liu},
	title = {A trusted and collaborative framework for deep learning in IoT},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108055},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108055},
	doi = {10.1016/J.COMNET.2021.108055},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangZSL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {More and more Internet of Things (IoT) applications provide intelligent services, with the development of artificial intelligence algorithms, such as deep reinforcement learning. However, along with the trend of utilizing a large model with high accuracy in AI-enabled IoT, resource-limited IoT devices are difficult to handle these large-scale models with high response latency. By collaborating with edge nodes, the devices could respond quickly. However, IoT applications contain a large amount of user privacy, and pushing data to others might lead to privacy leakage. Inspired by the trusted execution environment technology, we propose a framework that enables trusted collaboration for future AI-enabled IoTs, in terms of computation security and transmission security, where the data could be processed in an isolated environment, and two approaches are proposed to ensure the security in data transmission. Experimental results show that our framework provides flexible and dynamic collaboration with low overhead and can effectively support collaborative edge intelligence.}
}


@article{DBLP:journals/cn/HuangDZL21,
	author = {Shan Huang and
                  Dezun Dong and
                  Zejia Zhou and
                  Xiangke Liao},
	title = {{MP-CREDIT:} Multi-path credit for high-speed data center transports},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108061},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108061},
	doi = {10.1016/J.COMNET.2021.108061},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuangDZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In high-speed interconnection networks, credit-based flow control is widely deployed at the link layer to ensure data lossless. In this case, each credit schedules data on a one-hop fixed link. When employing credit in higher layers, e.g. transport layer, credit needs to be transmitted through multiple hops to realize end-to-end scheduling. In credit-based transports, each credit is transmitted to the data sender through a credit path, in return, each data packet is sent to the data receiver through a data path. In modern high-speed data centers that deploy topologies with multiple equivalent paths, the multi-hop credit can be routed into diverse paths. For the credit-based transports that require strong path-consistency between the data path and the credit path, properly routing credits becomes significant to improve data transmission efficiency. However, existing solutions lack consideration for exploiting multiple paths, this limits them to be single-path transports and affects their performance.}
}


@article{DBLP:journals/cn/SomesulaRS21,
	author = {Manoj Kumar Somesula and
                  Rashmi Ranjan Rout and
                  Durvasula V. L. N. Somayajulu},
	title = {Contact duration-aware cooperative cache placement using genetic algorithm
                  for mobile edge networks},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108062},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108062},
	doi = {10.1016/J.COMNET.2021.108062},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SomesulaRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Caching popular content at the base stations cooperatively is an effective solution to reduce the user-perceived latency and overwhelming data traffic by bringing content close to the user in a cellular network-based Mobile Edge Computing (MEC) architecture. Most of the existing literature assumes static network models where all the users remain static throughout the data transfer time, and the user can download the requested content from the associated base station. Caching content by considering user mobility and randomness of contact duration is an important issue which has been addressed in this work. We consider the cache placement problem in a realistic scenario where users move at different speeds. The moving users that are connected to the multiple base stations intermittently may not download full content because of contact duration. This, in turn, increases the overall delay in downloading the content for mobile users. The cache placement problem is formulated as mixed-integer nonlinear programming to maximize the saved delay with capacity constraint. The user mobility and contact duration are modeled with a Markov renewal process. Further, a greedy algorithm is presented to solve the problem by adopting submodular optimization. For real scenarios that scale to large library sizes, taking into account the computational time, we have proposed a genetic algorithm-based heuristic search mechanism. Extensive simulation results show that the proposed contact duration aware caching scheme significantly improves the performance in terms of hit ratio and acceleration ratio in a real-world scenario as compared with three existing caching mechanisms.}
}


@article{DBLP:journals/cn/OmetovSKSSPFQCC21,
	author = {Aleksandr Ometov and
                  Viktoriia Shubina and
                  Lucie Klus and
                  Justyna Skibinska and
                  Salwa Saafi and
                  Pavel Pascacio and
                  Laura Flueratoru and
                  Darwin Quezada{-}Gaibor and
                  Nadezhda Chukhno and
                  Olga Chukhno and
                  Asad Ali and
                  Asma Channa and
                  Ekaterina Svertoka and
                  Waleed Bin Qaim and
                  Raul Casanova Marques and
                  Sylvia Holcer and
                  Joaqu{\'{\i}}n Torres{-}Sospedra and
                  Sven Casteleyn and
                  Giuseppe Ruggeri and
                  Giuseppe Araniti and
                  Radim Burget and
                  Jiri Hosek and
                  Elena Simona Lohan},
	title = {A Survey on Wearable Technology: History, State-of-the-Art and Current
                  Challenges},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108074},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108074},
	doi = {10.1016/J.COMNET.2021.108074},
	timestamp = {Thu, 05 Jan 2023 17:09:20 +0100},
	biburl = {https://dblp.org/rec/journals/cn/OmetovSKSSPFQCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technology is continually undergoing a constituent development caused by the appearance of billions new interconnected “things” and their entrenchment in our daily lives. One of the underlying versatile technologies, namely wearables, is able to capture rich contextual information produced by such devices and use it to deliver a legitimately personalized experience. The main aim of this paper is to shed light on the history of wearable devices and provide a state-of-the-art review on the wearable market. Moreover, the paper provides an extensive and diverse classification of wearables, based on various factors, a discussion on wireless communication technologies, architectures, data processing aspects, and market status, as well as a variety of other actual information on wearable technology. Finally, the survey highlights the critical challenges and existing/future solutions.}
}


@article{DBLP:journals/cn/ZhouYL21,
	author = {Qizhao Zhou and
                  Junqing Yu and
                  Dong Li},
	title = {A dynamic and lightweight framework to secure source addresses in
                  the SDN-based networks},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108075},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108075},
	doi = {10.1016/J.COMNET.2021.108075},
	timestamp = {Tue, 08 Jun 2021 09:20:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of source address validation implementation (SAVI) in a Software-Defined Network (SDN) environment. The integration of SAVI and SDN can further address the challenges in a typical architecture such as the complexity of SAVI's deployment and the acquisition of security data in the access layer. A key aspect of this campaign consists of filtering forged packets and verifying the authenticity of the source address. A common strategy is to create bindings between the IP address of a node and a property of the host's network attachment. However, problem still accompany with the deployment of SAVI, including the performance cost of the SDN controller caused by the redundant validation process, especially in the case of an overflow of the flow table and other anomalous conditions in the network. Our contribution in this paper is to design and implement a dynamic framework for lightweight SAVI based on SDN (D-SAVI), which is an enhancement of SDN setups to allow proper source address validation on downstream network ingress ports, without incurring a large performance overhead. Initially, we proposed a fine-grained dual-level structure to match flow entry flexibly to complete the dynamic deployment of SAVI. A priority-based validation mechanism was added for further efficiency optimization. We then designed a state partition and transition module to optimize network performance under anomalous conditions, especially the communication performance between the controllers and switches in the SDN-based networks with a global view. Consequently, under the premise of network security, D-SAVI could filter out, with better packet forwarding efficiency, packets that do not match existing binding relationships. The experimental results demonstrate that our implementation provides source address verification with less resource consumption than existing methods.}
}


@article{DBLP:journals/cn/WangCHMKK21,
	author = {Eric Ke Wang and
                  Chien{-}Ming Chen and
                  M. Shamim Hossain and
                  Ghulam Muhammad and
                  Sachin Kumar and
                  Saru Kumari},
	title = {Transfer reinforcement learning-based road object detection in next
                  generation IoT domain},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108078},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108078},
	doi = {10.1016/J.COMNET.2021.108078},
	timestamp = {Sun, 19 Jan 2025 14:22:29 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangCHMKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The landscape of fifth generation (5G) and beyond 5G (B5G)-enabled Internet of Things(IoT) is expected to seamlessly and ubiquitously connect everything, which includes 5G, cloud computing, artificial intelligence and other cutting-edge technologies to realize truly intelligent applications in smart cities. In this paper, we present an important key technology for smart city, which is a road target recognition algorithm for smart city applications and designs a set of corresponding programs to assist automatic drivers, pedestrians and visually impaired people in road safety, or to manage city infrastructure. The system can connect robots in cars, wearable devices and body area network in pedestrians or blind people. A target recognition algorithm based on scene fusion is designed to recognize the specific target in the road environment, and transfer reinforcement learning method is used to improve the accuracy and real-time performance of target recognition. The system provides them with travel assistance, identify dangerous or useful objects for them through high-performance target recognition services. It can collect the road visual scene data by road cameras and transmit it to edge devices for training model. The model is collaborated trained in the edge devices and aggregated by the cloud. Based on the transfer reinforcement learning method, the vision-based road target recognition has been implemented, and the accurate and reliable target recognition can be realized. Many details of experiments verify the effectiveness of our technology.}
}


@article{DBLP:journals/cn/VanderhallenBPM21,
	author = {Stien Vanderhallen and
                  Jo Van Bulck and
                  Frank Piessens and
                  Jan Tobias M{\"{u}}hlberg},
	title = {Robust authentication for automotive control networks through covert
                  channels},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108079},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108079},
	doi = {10.1016/J.COMNET.2021.108079},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/VanderhallenBPM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automotive control networks offer little resistance against security threats that come with the long-range connectivity in modern cars. Remote attacks that undermine the safety of vehicles have been shown to be practical. A range of security mechanisms have been proposed to harden resource-constrained embedded microcontrollers against malicious interference, including cryptographic protocols that establish the authenticity of in-vehicle message exchange. However, authenticated communication comes with repercussions on deployability and vehicle safety in terms of reliability, real-time compliance, backwards compatibility, and bandwidth and resource use.}
}


@article{DBLP:journals/cn/WangCZSHZ21,
	author = {Jiansi Wang and
                  Haopeng Chen and
                  Fuxiao Zhou and
                  Meng Sun and
                  Ziang Huang and
                  Zhengtong Zhang},
	title = {{A-DECS:} Enhanced collaborative edge-edge data storage service for
                  edge computing with adaptive prediction},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108087},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108087},
	doi = {10.1016/J.COMNET.2021.108087},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangCZSHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the rapid development of IoT and 5G technologies, end-users of the Internet are generating a sea of data, which burdens the cloud cores and data centers. The urgent traffic demand causes a bad quality of experience in the traditional reactive networks. With the improvement of hardware, the edge of the Internet can share some responsibility for cloud servers. In small-scale scenarios, offload computing and storage tasks to edge nodes can help improve the user experience and utilize resources of the edge networks. In this paper, we propose a collaborative edge–edge data storage service with adaptive prediction called A-DECS based on our previous work DECS (Zhou and Chen, 2020) in local area scenarios. It manages nodes in the edge network and fully utilizes the limited resources in the cluster. A-DECS picks the most appropriate node to offload tasks. It can also proactively replicate popular data and generate forwarding rules in advance. A-DECS can reduce the read latency of cold-start and sudden peak flow situations. We compare it to state-of-the-art research. This experiment result proves that A-DECS is more suitable for edge clusters due to its lower latency and better resource utilization.}
}


@article{DBLP:journals/cn/LuRCTWQ21,
	author = {Liangfu Lu and
                  Xiaohan Ren and
                  Chenwei Cui and
                  Zhiyuan Tan and
                  Yulei Wu and
                  Zhizhen Qin},
	title = {A novel tensor-information bottleneck method for multi-input single-output
                  applications},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108088},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108088},
	doi = {10.1016/J.COMNET.2021.108088},
	timestamp = {Tue, 09 Aug 2022 09:37:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuRCTWQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensuring timeliness and mobility for multimedia computing is a crucial task for wireless communication. Previous algorithms that utilize information channels, such as the information bottleneck method, have shown great performance and efficiency, which guarantees timeliness. However, such methods suit only in handling single variable tasks such as image processing, but are inapplicable to multivariable applications such as video processing. To address this critical shortcoming, we propose a novel tensor information channel which extends the current single-input single-output matrix information channel to a more practical multi-input single-output tensor information channel. In comparison with the classic information channel, our tensor information channel not only performs better in experiments, but also allows for a wider range of practical applications. We further build an innovative tensor-information bottleneck method upon the state-of-the-art information bottleneck method. Experiments on video shot boundary detection are conducted using benchmark data sets to demonstrate the effectiveness of our proposed approach compared with state-of-the-art methods. In specific, our approach yields a 6.2% increase compared with the information channel-based method, and when compared to other state-of-the-art methods, we achieve 0.1%–17.7% performance gains under different experimental configurations.}
}


@article{DBLP:journals/cn/SenAMJ21,
	author = {Priyangshu Sen and
                  Viduneth Ariyarathna and
                  Arjuna Madanayake and
                  Josep Miquel Jornet},
	title = {A versatile experimental testbed for ultrabroadband communication
                  networks above 100 GHz},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108092},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108092},
	doi = {10.1016/J.COMNET.2021.108092},
	timestamp = {Sun, 12 Nov 2023 02:17:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SenAMJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication at terahertz (THz) frequencies is envisioned as a near-future landmark in wireless networking and a key component of the sixth generation (6G) wireless systems and beyond. In the last decade, major progress has happened in terms of device technology development as well as THz-wave propagation and channel modeling. In order to advance THz communication and networking research, there is tremendous necessity in developing programmable software-defined hardware and architectures that operate at THz frequencies and are able to process signals with tens to hundreds of GHz of bandwidth, thus making most out of moving to THz band. This paper presents a versatile testbed for conducting wireless experimental research above 100 GHz. The platform consists of multiple sets of analog front-ends at three different frequencies between 100 GHz and 1 THz as well as three different digital signal processing back-ends, able to manipulate more than 10 GHz of bandwidth in real-time. Implementation details and early experimental results to demonstrate the platform capabilities are presented.}
}


@article{DBLP:journals/cn/ShinyPM21,
	author = {S. Suja Golden Shiny and
                  S. Sathya Priya and
                  Krishnan Murugan},
	title = {Repeated game theory-based reducer selection strategy for energy management
                  in {SDWSN}},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108094},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108094},
	doi = {10.1016/J.COMNET.2021.108094},
	timestamp = {Tue, 21 Mar 2023 21:08:29 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ShinyPM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sensor-generated data by Internet of Things are considered to be the most common source of big data. A wide range of applications are relying on these data for analytics. While a considerable amount of data is sufficient for the application users to get valuable insights, sending vast amount of data to the cloud seems inappropriate and it only increases the communication cost in the network. It is well-known that an increase in communication cost increases energy depletion in the network. Since sensor nodes have a restricted power supply, it is necessary to harness the energy of nodes to prolong the network lifetime. In this paper, a solution for energy management of sensor nodes is proposed by integrating the software defined framework with the sensor network, software defined wireless sensor networks (SDWSN), that aids in processing the data inside the network before transferring it to the sink node. To this context, a game model has been formulated for selecting the appropriate nodes as reducers which will execute the reducer function. The software defined network (SDN) controller, geographically placed outside of the wireless sensor network, is responsible for selecting the reducers and dynamically load reducing function on them. Based on the selection, a routing protocol, routing via respective reducer (RVRR), that forwards data packets via in-network processing path and control packets via common path has been proposed. This remarkably reduces the communication cost, thereby prolonging the lifetime of the deployed network. The RVRR algorithm is implemented in NS-3 simulator to evaluate the performance of proposed work in SDWSN environment.}
}


@article{DBLP:journals/cn/SharmaA21,
	author = {Abhilasha Sharma and
                  Lalit Kumar Awasthi},
	title = {Pr-CAI: Priority based-Context Aware Information scheduling for SDN-based
                  vehicular network},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108097},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108097},
	doi = {10.1016/J.COMNET.2021.108097},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SharmaA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Intelligent Transport System (ITS) services recon upon reliable information dissemination among the heterogeneous vehicular network components along the roads. The information sharing among vehicles, pedestrian units and RoadSide Units (RSUs) is paramount in dynamic vehicular network to provide real-time data services in vehicular networks. Due to distinctive vehicular network characteristics, it is extremely challenging to provide efficient data services in heterogeneous traffic scenario. Firstly, the ITS services have different Quality of Service (QoS) constraints which varies in line with application specification. Secondly, there is a need of coordination among network resources in vehicular network. In this work, a Software Defined Network (SDN) based vehicular network model has been presented to provide reliable data services in heterogeneous traffic in urban environment. The Context Aware Information Scheduling (CAIS) problem has been formulated by analysing the characteristics and QoS requirements of ITS services. A Priority based Context Aware Information (Pr-CAI) scheduling algorithm has been proposed to ensure real time data service scheduling, which assigns priority to requests based on service deadline, request selection priority and traffic dynamics. The Pr-CAI has been simulated for different heterogeneous traffic scenarios to analyse its performance. The comprehensive simulation results prove the ascendancy of proposed algorithm with respect to the existing compared algorithms. The Pr-CAI scheduling algorithm ensures real time data service scheduling for different ITS services that maximizes the service ratio and provides fairness amongst users.}
}


@article{DBLP:journals/cn/BuYGYLZ21,
	author = {Kai Bu and
                  Yutian Yang and
                  Zixuan Guo and
                  Yuanyuan Yang and
                  Xing Li and
                  Shigeng Zhang},
	title = {Securing middlebox policy enforcement in {SDN}},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108099},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108099},
	doi = {10.1016/J.COMNET.2021.108099},
	timestamp = {Sun, 04 Aug 2024 16:19:08 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BuYGYLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN) greatly simplifies middlebox policy enforcement. Middleboxes need tag packet headers to avoid forwarding ambiguity on SDN switches. In this paper, we present a new attack, called middlebox-bypass attack, to breach SDN-based middlebox policy enforcement. Such an attack manipulates a compromised switch to locally tag attacking packets without handing them over to the attached middlebox for inspection. Existing SDN security solutions, however, cannot detect the middlebox-bypass attack under practical constraints of efficiency, robustness, and applicability. We design and implement FlowCloak, the first protocol for per-packet real-time detection and prevention of middlebox-bypass attacks. FlowCloak enables middleboxes to generate tags that are probabilistically unknown to an attacker and confines it to only random guessing. We propose a multi-tag verification technique to address the tradeoff between FlowCloak robustness and TCAM usage by tag verification rules on the egress switch. Experiment results show that dozens of verification rules can confine the attacking probability under 0.1%. We further explore implementation techniques of packet looping and field swapping that can enable a flow table pipeline on a single TCAM and mitigate packet correlation, respectively. FlowCloak imposes only a 0.01 ms packet processing delay on middleboxes and no obvious delay on the egress switch.}
}


@article{DBLP:journals/cn/YangLQZZT21,
	author = {Hanyu Yang and
                  Xutao Li and
                  Wenhao Qiang and
                  Yuhan Zhao and
                  Wei Zhang and
                  Chang Tang},
	title = {A network traffic forecasting method based on {SA} optimized {ARIMA-BP}
                  neural network},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108102},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108102},
	doi = {10.1016/J.COMNET.2021.108102},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YangLQZZT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic forecasting provides key information for network management, resource allocation, traffic attack detection. However, traditional linear and non-linear network traffic forecasting models cannot achieve enough prediction accuracy for future traffic prediction. In order to resolve this problem, a network traffic prediction method based on SA (Simulated Annealing) optimized ARIMA (Autoregressive Integrated Moving Average model)-BPNN (Back Propagation Neural Network) is proposed in this paper, which makes comprehensive use of linear model ARIMA, non-linear model BPNN and optimization algorithm SA. With enhancement of the BPNN global optimization ability, it can fully realize the potential of mining linear and non-linear laws of historical network traffic data, hence improving the prediction accuracy. This paper selects the historical network traffic data of two different sampling points in the WIDE project to predict, and utilizes the MAE(Mean Absolute Error), RMSE(Root Mean Square Error), and the MAPE(Mean Absolute Percentage Error) as the evaluation index of the prediction effect. Experimental results show that our proposed method outperformed traditional network traffic prediction model, with several improvements in network traffic prediction accuracy.}
}


@article{DBLP:journals/cn/EramoLCS21,
	author = {Vincenzo Eramo and
                  Francesco Giacinto Lavacca and
                  Tiziana Catena and
                  Paul Jaime Perez Salazar},
	title = {Application of a Long Short Term Memory neural predictor with asymmetric
                  loss function for the resource allocation in {NFV} network architectures},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108104},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108104},
	doi = {10.1016/J.COMNET.2021.108104},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/EramoLCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic and cloud resource prediction methodologies have been recently used in Network Function Virtualization environment for cloud and bandwidth resource allocation purposes. Both traditional and innovative prediction methodologies have been proposed for the application of allocation procedures. For instance Long Short Term Memory-based prediction techniques have been shown to be very effectiveness to allocate the resources. All of these techniques are based on the minimization of a symmetric cost function as the Root Mean Square Error that equally weights positive and negative prediction errors. However the error sign can differently impact the cost increase due to prediction errors. For instance when the Quality of Service degradation cost due to traffic loss is prevalent with respect to the cloud resource allocation cost, an algorithm is preferable that overestimates the offered traffic; conversely the traffic underestimation is preferable in the opposite case when the cloud allocation cost is higher than the QoS degradation one. For this reason we propose an Asymmetric LSTM traffic prediction procedure in which the cost function is defined so as to take into account both the QoS degradation and cloud resource allocation costs. In a typical network and traffic scenario, we show how the proposed solution allows for cost decrease by 40% with respect to classical LSTM prediction methodology based on the Root Mean Square Error.}
}


@article{DBLP:journals/cn/SongGLL21,
	author = {Huangfei Song and
                  Songtao Guo and
                  Pan Li and
                  Guiyan Liu},
	title = {{FCNR:} Fast and Consistent Network Reconfiguration with low latency
                  for {SDN}},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108113},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108113},
	doi = {10.1016/J.COMNET.2021.108113},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SongGLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-defined network (SDN) is one of the important technologies to achieve the customization of network services. The key to realize a highly adaptive SDN, which can respond to the changing demands or recover after network failure in a short period of time, is to update the configuration effectively. However, the inconsistent configuration update may lead to transient and incorrect network behaviors and long reconfiguration time between current configuration and target configuration may degrade undesired network performance. Most of the existing works to reduce the network reconfiguration time focus on the consistent update between current configuration and target configuration, and ignore the impact of different target configurations on network reconfiguration time. Therefore, reducing the network reconfiguration time needs to consider not only the consistent update between current configuration and target configuration, but also the impact of different target configurations. In this paper, we first analyze the impact of different target configuration of the control plane on the update operation in the forwarding rules of the data plane. Then, we formulate the shortest configuration path (SCP) problem to get the path with the shortest configuration time and propose the shortest deployment time path selection (SPS) algorithm to obtain the target configuration in the light of different rules deployment time. Subsequently, we propose the consistent scheduling update (CSU) algorithm to solve the consistency conversion from the current configuration to the target configuration. Finally, experimental results demonstrate that our algorithms can reduce the network reconfiguration time up to 39% compared with previous SDN consistent update methods while keeping the similar consistency.}
}


@article{DBLP:journals/cn/YarinezhadA21,
	author = {Ramin Yarinezhad and
                  Sadoon Azizi},
	title = {An energy-efficient routing protocol for the Internet of Things networks
                  based on geographical location and link quality},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108116},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108116},
	doi = {10.1016/J.COMNET.2021.108116},
	timestamp = {Tue, 08 Jun 2021 09:20:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YarinezhadA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the design of Internet of Things (IoT), sensor nodes are used in collecting data from an environment and sending it to a base station or sink for future processing. These sensor nodes have some resource constraints such as power supply. Hence, in order to prolong the network lifetime, we need to manage the energy consumption of these sensor devices. Thus, in this paper, a tree-based routing protocol is proposed that is efficient in power consumption and reduces the end-to-end delay in energy-efficient green-IoT networks with a mobile sink. Two new different mechanisms are introduced in the proposed protocol for managing the routing in the network. The first mechanism is an improved version of the geographic routing algorithm, which is more reliable and energy-balanced. The second mechanism is a tree-based structure that can be created with the minimum control packets and updated with an efficient procedure. Simulation results indicate that the proposed routing algorithm is superior to the existing ones in terms of energy consumption, network lifetime, delay, and throughput.}
}


@article{DBLP:journals/cn/YaoLGH21,
	author = {Xiaopeng Yao and
                  Guangxian Liang and
                  Chonglin Gu and
                  Hejiao Huang},
	title = {Rumors clarification with minimum credibility in social networks},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108123},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108123},
	doi = {10.1016/J.COMNET.2021.108123},
	timestamp = {Tue, 15 Jun 2021 17:21:24 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YaoLGH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 2020, the information about Corona Virus Disease 2019 (COVID-19) is overwhelming, which is mixed with a lot of rumors. Rumor and truth can change people’s believes more than once, depending on who is more credible. Here we use credibility to measure the influence one person has on others. Considering costs, we often hope to find the people with the smallest credibility but can achieve the maximum influence. Therefore, we focus on how to use minimal credibility in a given amount of time to clarify rumors. Given the time\nt\n, the minimum credibility rumor clarifying\n(\nM\nC\nR\nC\n)\nproblem aims to find a seed set with\nk\nusers such that the total credibility can be minimized when the total number of the users influenced by positive information reaches a given number at time\nt\n. In this paper, we propose a Longest-Effective-Hops algorithm called LEH to solve this problem that supposes each user can be influenced two or more times. The theoretical analysis proves that our algorithm is universal and effective. Extensive contrast experiments show that our algorithm is more efficient in both time and performance than the state-of-the art methods.}
}


@article{DBLP:journals/cn/ValenzaRS21,
	author = {Fulvio Valenza and
                  Matteo Repetto and
                  Stavros Shiaeles},
	title = {Guest editorial: Special issue on novel cyber-security paradigms for
                  software-defined and virtualized systems},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108126},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108126},
	doi = {10.1016/J.COMNET.2021.108126},
	timestamp = {Mon, 26 Jun 2023 20:51:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ValenzaRS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/cn/LiuLYYG21,
	author = {Zhixin Liu and
                  Xiaopin Li and
                  Yazhou Yuan and
                  Yi Yang and
                  Xinping Guan},
	title = {Game based robust power allocation strategy with QoS guarantee in
                  {D2D} communication network},
	journal = {Comput. Networks},
	volume = {193},
	pages = {108130},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2021.108130},
	doi = {10.1016/J.COMNET.2021.108130},
	timestamp = {Thu, 02 Feb 2023 19:58:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiuLYYG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device to device (D2D) communication, as a proximity communication technique that leverages the spatial–temporal locality of mobile data usage to achieve one to many simultaneous transmission, provides an effective solution for unloading heavy traffic. However, due to the sharing of spectrum resources, interference management has become a major challenge for D2D communication. In this paper, a game-based robust power allocation strategy with quality of service (QoS) guarantee is designed for D2D multicast network. A Stackelberg game is proposed to characterize the actions of the base station (BS) and D2D users (DUEs). In the game, the BS is seen as the leader, whereas the D2D users are followers. On the one hand, the purpose of BS is to maximize the leader’s profit with the constraint of the maximum tolerable interference. On the other, the DUEs attempt to maximize the followers’ profit in the network system while guaranteeing the QoS requirements for DUEs, which are modeled as probability constraints. Spectrum sharing is assumed among the cellular users (CUEs) and DUEs. Because of the dynamic characteristics of wireless channel, it is difficult and expensive to acquire accurate channel state information (CSI). On this account, the uncertain channel state information is considered in the formulated problem, and the probability threshold method is utilized to convert the uncertain constraints into tractable ones. Furthermore, a distributed algorithm to determine the optimal solutions is proposed, which significantly reduces the information exchanges. Finally, numerical results validate the performances of the proposed scheme in the aspects of convergence, utility of uses, and power consumption.}
}
