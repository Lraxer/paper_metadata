@article{DBLP:journals/cn/BrennerFOSZ24,
	author = {Bernhard Brenner and
                  Joachim Fabini and
                  Magnus Offermanns and
                  Sabrina Semper and
                  Tanja Zseby},
	title = {Malware communication in smart factories: {A} network traffic data
                  set},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110804},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110804},
	doi = {10.1016/J.COMNET.2024.110804},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BrennerFOSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning-based intrusion detection requires suitable and realistic data sets for training and testing. However, data sets that originate from real networks are rare. Network data is considered privacy sensitive and the purposeful introduction of malicious traffic is usually not possible. In this paper we introduce a labeled data set captured at a smart factory located in Vienna, Austria during normal operation and during penetration tests with different attack types. The data set consists of 173 GB of Packet Capture (PCAP) files, which represent 16 days (395 h) of factory operation. It includes Message Queuing Telemetry Transport (MQTT), OPC Unified Architecture (OPC UA), and Modbus/TCP traffic. The captured malicious traffic was originated by a professional penetration tester who performed two types of attacks: (a) aggressive attacks that are easier to detect and (b) stealthy attacks that are harder to detect. Our data set includes the raw PCAP files and extracted flow data. Labels for packets and flows indicate whether packets (or flows) originated from a specific attack or from benign communication. We describe the methodology for creating the data set, conduct an analysis of the data and provide detailed information about the recorded traffic itself. The data set is freely available to support reproducible research and the comparability of results in the area of intrusion detection in industrial networks.}
}


@article{DBLP:journals/cn/AmanatidisKMLI24,
	author = {Petros Amanatidis and
                  Dimitris Karampatzakis and
                  Georgios Michailidis and
                  Thomas Lagkas and
                  George Iosifidis},
	title = {Adaptive reverse task offloading in edge computing for {AI} processes},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110844},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110844},
	doi = {10.1016/J.COMNET.2024.110844},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AmanatidisKMLI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, we witness the proliferation of edge IoT devices, ranging from smart cameras to autonomous vehicles, with increasing computing capabilities, used to implement AI-based services in users’ proximity, right at the edge. As these services are often computationally demanding, the popular paradigm of offloading their tasks to nearby cloud servers has gained much traction and been studied extensively. In this work, we propose a new paradigm that departs from the above typical edge computing offloading idea. Namely, we argue that it is possible to leverage these end nodes to assist larger nodes (e.g., cloudlets) in executing AI tasks. Indeed, as more and more end nodes are deployed, they create an abundance of idle computing capacity, which, when aggregated and exploited in a systematic fashion, can be proved beneficial. We introduce the idea of reverse offloading and study a scenario where a powerful node splits an AI task into a group of subtasks and assigns them to a set of nearby edge IoT nodes. The goal of each node is to minimize the overall execution time, which is constrained by the slowest subtask, while adhering to predetermined energy consumption and AI performance constraints. This is a challenging MINLP (Mixed Integer Non-Linear Problem) optimization problem that we tackle with a novel approach through our newly introduced EAI-ARO (Edge AI-Adaptive Reverse Offloading) algorithm. Furthermore, a demonstration of the efficacy of our reverse offloading proposal using an edge computing testbed and a representative AI service is performed. The findings suggest that our method optimizes the system’s performance significantly when compared with a greedy and a baseline task offloading algorithm.}
}


@article{DBLP:journals/cn/DuanGSGMTZ24,
	author = {Pengfei Duan and
                  Hongmin Gao and
                  Yushi Shen and
                  Zhetao Guo and
                  Zhaofeng Ma and
                  Tian Tian and
                  Yuqing Zhang},
	title = {Secure collaborative {EHR} Sharing using multi-authority attribute-based
                  proxy re-encryption in Web 3.0},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110851},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110851},
	doi = {10.1016/J.COMNET.2024.110851},
	timestamp = {Mon, 03 Mar 2025 21:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DuanGSGMTZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Web 3.0 represents a transformative shift toward a decentralized, intelligent, and user-centric Internet. Existing electronic health record (EHR) sharing systems depend on centralized cloud servers for storage and management, with hospitals serving as primary custodians. This centralization often results in patients losing control and visibility over their EHR data, including who accesses it and how it is utilized, which contradicts the decentralized principles of Web 3.0. In this context, we propose a multi-authority attribute-based proxy re-encryption scheme that facilitates collaborative EHR sharing in Web 3.0. Our design allows the updating of ciphertext policies, thereby eliminating the need for frequent re-encryption of plaintext data amid varying cross-domain access policies. Furthermore, our scheme utilizes blockchain technology to create a decentralized and transparent environment that enables traceable cross-domain EHR sharing records. Additionally, we integrate hybrid encryption with decentralized data hosting platforms, significantly reducing the on-chain storage burden. The use of smart contracts automates the cross-domain EHR sharing and guarantees a fair distribution of benefits among all participants. Security analysis confirms that our scheme is secure against chosen plaintext attacks and resistant to collusion. Performance analysis and simulation experiments validate the efficiency and robustness of our scheme.}
}


@article{DBLP:journals/cn/MfogoZNNK24,
	author = {Volviane Saphir Mfogo and
                  Alain B. Zemkoho and
                  Laurent Njilla and
                  Marcellin Nkenlifack and
                  Charles A. Kamhoua},
	title = {Adaptive learning-based hybrid recommender system for deception in
                  Internet of Thing},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110853},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110853},
	doi = {10.1016/J.COMNET.2024.110853},
	timestamp = {Mon, 03 Mar 2025 21:30:45 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MfogoZNNK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the rapidly evolving Internet of Things (IoT) security domain, device vulnerabilities pose significant risks, frequently exploited by cyberattackers. Traditional reactive security measures like patching often fall short against advanced threats. This paper introduces a proactive deception system enhanced by an innovative Adaptive Learning-based Hybrid Recommender System (AL-HRS), utilizing the vulnerability and attack repository for IoT (VARIoT) database. This advanced system identifies existing vulnerabilities and dynamically recommends additional deceptive vulnerabilities based on real-time analysis of attacker behavior and historical exploit data. These recommended vulnerabilities mislead attackers into engaging with controlled environments such as honeypots, effectively neutralizing potential threats. The AL-HRS combines the predictive strengths of content-based filtering (CBF) and collaborative filtering (CF) with an adaptive learning mechanism that adjusts recommendations based on ongoing attacker interactions, ensuring the system’s efficacy amidst changing attack patterns. Our approach innovatively combines these methodologies to provide a continuously evolving security strategy, significantly enhancing the deception capability of IoT systems. Initial evaluations demonstrate a potential reduction in device compromise, highlighting the effectiveness and strategic relevance of this adaptive deception framework in IoT cybersecurity.}
}


@article{DBLP:journals/cn/MeyerHRKS24,
	author = {Philipp Meyer and
                  Timo H{\"{a}}ckel and
                  Sandra Reider and
                  Franz Korf and
                  Thomas C. Schmidt},
	title = {Network anomaly detection in cars: {A} case for time-sensitive stream
                  filtering and policing},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110855},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110855},
	doi = {10.1016/J.COMNET.2024.110855},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MeyerHRKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connected vehicles are threatened by cyber-attacks as in-vehicle networks technologically approach (mobile) LANs with several wireless interconnects to the outside world. Malware that infiltrates a car today faces potential victims of constrained, barely shielded Electronic Control Units (ECUs). Many ECUs perform critical driving functions, which stresses the need for hardening security and resilience of in-vehicle networks in a multifaceted way. Future vehicles will comprise Ethernet backbones that differentiate services via Time-Sensitive Networking (TSN). The well-known vehicular control flows will follow predefined schedules and TSN traffic classifications. In this paper, we exploit this traffic classification to build a network anomaly detection system. We show how filters and policies of TSN can identify misbehaving traffic and thereby serve as distributed guards on the data link layer. On this lowest possible layer, our approach derives a highly efficient network protection directly from TSN. We classify link layer anomalies and micro-benchmark the detection accuracy in each class. Based on a topology derived from a real-world car and its traffic definitions we evaluate the detection system in realistic macro-benchmarks based on recorded attack traces. Our results show that the detection accuracy depends on how exact the specifications of in-vehicle communication are configured. Most notably for a fully specified communication matrix, our anomaly detection remains free of false-positive alarms, which is a significant benefit for implementing automated countermeasures in future vehicles.}
}


@article{DBLP:journals/cn/XuYYL24,
	author = {Sheng{-}wei Xu and
                  Shu{-}han Yu and
                  Zi{-}Yan Yue and
                  Yi{-}Long Liu},
	title = {{CLLS:} Efficient certificateless lattice-based signature in VANETs},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110858},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110858},
	doi = {10.1016/J.COMNET.2024.110858},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/XuYYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Vehicular Ad-hoc Network (VANETs) has improved road safety and traffic management, and brought great convenience to intelligent transportation system (ITS). However, the transmission of data over open channels caused many security issues. Certificateless cryptography solves the certificate management and key escrow problems, which makes it the primary method for message authentication in VANETs. However, with the emergence of quantum computing, traditional cryptography faces a significant challenge. Lattice-based cryptography are regarded as effective post-quantum ciphers. Nevertheless, nearly all existing lattice-based certificateless signature schemes rely on Gaussian sampling or trapdoor techniques, resulting in computational inefficiencies and large key and signature sizes that are impractical for VANETs. To address these issues, we proposed the first efficient algebraic lattice-based certificateless signature scheme in VANETs based on the Dilithium signature algorithm. The security of our certificateless lattice-based signature(CLLS) scheme is based on the MSIS and MLWE hardness assumption, which makes the scheme resistant to quantum attacks and easy to implement. Our scheme did not use Gaussian sampling or trapdoor techniques, which improve the computational and storage efficiency. As a result, the public key of our scheme is 1X smaller than the previous scheme and the size of signature is 2X smaller than the previous efficient algebraic lattice scheme. In addition, compared to the most efficient existing CLLS scheme, the signing and verification computation cost of our scheme are reduced by 20% and 55% respectively and our proposed CLLS scheme has low power consumption. Furthermore, the security of our scheme achieves strong unforgeability against chosen-message attacks(SUF-CMA) in the random oracle model(ROM), which surpasses that of existing lattice-based certificateless signature schemes.}
}


@article{DBLP:journals/cn/LiHZQ24,
	author = {Peihao Li and
                  Jie Huang and
                  Shuaishuai Zhang and
                  Chunyang Qi},
	title = {SecureEI: Proactive intellectual property protection of {AI} models
                  for edge intelligence},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110825},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110825},
	doi = {10.1016/J.COMNET.2024.110825},
	timestamp = {Thu, 14 Nov 2024 07:27:55 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiHZQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deploying AI models on edge computing platforms enhances real-time performance, reduces network dependency, and ensures data privacy on terminal devices. However, these advantages come with increased risks of model leakage and misuse due to the vulnerability of edge environments to physical and cyber attacks compared to cloud-based solutions. To mitigate these risks, we propose SecureEI, a proactive intellectual property protection method for AI models that leverages model splitting and data poisoning techniques. SecureEI divides the model into two components: DeviceNet, which processes input data into protected license data, and EdgeNet, which operates on the license data to perform the intended tasks. This method ensures that only the transformed license data yields high model accuracy, while original data remains unrecognizable, even under fine-tuning attacks. We further employ targeted training strategies and weight adjustments to enhance the model’s resistance to potential attacks that aim to restore its recognition capabilities for original data. Evaluations on MNIST, Cifar10, and FaceScrub datasets demonstrate that SecureEI not only maintains high model accuracy on license data but also significantly bolsters defense against fine-tuning attacks, outperforming existing state-of-the-art techniques in safeguarding AI intellectual property on edge platforms.}
}


@article{DBLP:journals/cn/ChenLG24,
	author = {Jie Chen and
                  Jian Luo and
                  Kai Gao},
	title = {NetBoost: Towards efficient distillation and service differentiation
                  of network information exposure},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110829},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110829},
	doi = {10.1016/J.COMNET.2024.110829},
	timestamp = {Sat, 30 Nov 2024 21:09:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChenLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exposing network information such as distances between end hosts is useful to improve the quality of experiences for network users. Network providers calculate such information based on private topology and routing data and share it with users through well-established protocols such as Application-Layer Traffic Optimization. However, it is usually not intended to expose the original model, which can face scalability, user heterogeneity, efficacy & efficiency challenges.}
}


@article{DBLP:journals/cn/ChongM24a,
	author = {Kah Meng Chong and
                  Amizah Malip},
	title = {Local Differential Privacy for correlated location data release in
                  {ITS}},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110830},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110830},
	doi = {10.1016/J.COMNET.2024.110830},
	timestamp = {Sat, 30 Nov 2024 21:09:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChongM24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ubiquity of location positioning devices has facilitated the implementation of various Intelligent Transportation System (ITS) applications that generate an enormous volume of location data. Recently, Local Differential Privacy (LDP) has been proposed as a rigorous privacy framework that permits the continuous release of aggregate location statistics without relying on a trusted data curator. However, the conventional LDP was built upon the assumption of independent data, which may not be suitable for inherently correlated location data. This paper investigates the quantification of potential privacy leakage in a correlated location data release scenario under a local setting, which has not been addressed in the literature. Our analysis shows that the privacy guarantee of LDP could be degraded in the presence of spatial–temporal and user correlations, albeit the perturbation is performed locally and independently by the users. This privacy guarantee is bounded by a privacy barrier that is affected by the intensity of correlations. We derive several important closed-form expressions and design efficient algorithms to compute such privacy leakage in a correlated location data. We subsequently propose a\nΔ\n-CLDP model that enhances the conventional LDP by incorporating the data correlations, and design a generic LDP data release framework that renders adaptive personalization of privacy preservation. Extensive theoretical analyses and simulations on scalable real datasets validate the security and performance efficiency of our work.}
}


@article{DBLP:journals/cn/XiaoLZZTJ24,
	author = {Jingyu Xiao and
                  Qing Li and
                  Dan Zhao and
                  Xudong Zuo and
                  Wenxin Tang and
                  Yong Jiang},
	title = {Themis: {A} passive-active hybrid framework with in-network intelligence
                  for lightweight failure localization},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110836},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110836},
	doi = {10.1016/J.COMNET.2024.110836},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/XiaoLZZTJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fast and efficient failure detection and localization is essential for stable network transmission. Unfortunately, existing schemes suffer from a few drawbacks such as significant resource consumption, lack of support for fast online failure localization, and limited applicable topologies. In this paper, we design Themis, a lightweight learning-based failure localization scheme for general networks. In the data plane, Themis achieves line-speed high performance failure detection using in-network classifiers and fine-grained traffic features. To reduce communication overhead, only coarse-grained traffic features are reported to the control plane for localization when a failure occurs. In the control plane, we propose a two-stage passive-active hybrid failure localization approach to accurately locate the failure without incurring excessive probing traffic. First, passive detection is conducted through the lightweight model XGBoost to infer a Potential Failure Link Set (PFLS). Then, active detection is done by only sending out probing packets to locations in the PFLS for precise failure localization. Comprehensive experiments demonstrate that Themis achieves ms-level failure localization with at least 95.63% accuracy, while saving 87.41% of bandwidth and 41.88% of hardware resource overhead on average compared with the state-of-the-art schemes.}
}


@article{DBLP:journals/cn/WangXSJ24,
	author = {Yabo Wang and
                  Ruizhi Xiao and
                  Jiakun Sun and
                  Shuyuan Jin},
	title = {MC-Det: Multi-channel representation fusion for malicious domain name
                  detection},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110847},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110847},
	doi = {10.1016/J.COMNET.2024.110847},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangXSJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the essential fundamental infrastructure of the current network, the Domain Name System is widely abused by cyber attackers, malicious domain detection has become a crucial task in combating cyber crime. Most existing methods focus on local attributes, treating each domain name individually. Alternatively, they prioritize global associations among domain names, but ignore the attributes of the domains themselves, allowing malicious domain names to survive through sophisticated evasion techniques. In this paper, we propose MC-Det, a hybrid framework for detecting malicious domain names by fusing a Multi-channel representation of domain names. MC-Det first abstracts the domain name resolution process into three spatially independent information channels: Attribute space, which contains the intrinsic information in the domain name string itself, Constraint space, which involves the potential constraints imposed on the network activity behind the domain name, Topological space, which represents the actual usage and deployment of the domain name. Subsequently, it generates proper embedding representations of domain names for each channel. This novel Multi-channel representation provides a comprehensive understanding of domain name resolution process. Finally, a Multi-channel fusion strategy employing by attention mechanism is used to generate the final representation of domain names for the classifier, making MC-Det suitable for malicious domain name detection in different application scenarios. Experimental results demonstrate that MC-Det outperforms other state-of-the-art techniques, while only utilizing the resource information revealed in the domain name resolution phase.}
}


@article{DBLP:journals/cn/ZhaoGLSRZY24,
	author = {Liushun Zhao and
                  Deke Guo and
                  Lailong Luo and
                  Yulong Shen and
                  Bangbang Ren and
                  Shi Zhu and
                  Fangliao Yang},
	title = {Concordit: {A} credit-based incentive mechanism for permissioned redactable
                  blockchain},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110848},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110848},
	doi = {10.1016/J.COMNET.2024.110848},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhaoGLSRZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malicious attacks and the introduction of illegal data put blockchains at risk, and blockchain governance is gaining increasing attention. The redactable blockchain technology has become a mainstream solution for blockchain governance. However, a low completion rate for redaction tasks limits current redactable blockchain technologies, primarily due to the absence of an effective incentive mechanism for participants. This gap underscores the urgent need for designing and implementing robust incentive mechanisms in redactable blockchains. Incentive mechanisms can motivate and guide entities to participate and perform desired behaviors through awards and punishments. This paper proposes Concordit, the first deployable credit-based incentive mechanism for redactable blockchains. Its purpose is to encourage submitters to submit legal redaction requests, modifiers to perform legal redaction operations, and verifiers to maintain the behavior consistent with the consensus algorithm. In the context of permissioned blockchains, Concordit utilizes a credit value system for awards and punishments. Additionally, we use a game theory-based mechanism to analyze and model participants’ behavior utilities in the redactable blockchain. Meanwhile, we evaluate the credibility of nodes by combining their static initial credit values and dynamic behavior-related credit values. This system prioritizes high-credibility nodes as participants, thereby enhancing the completion rate for redaction tasks. Finally, the implementation and performance evaluation of our Concordit incentive mechanism demonstrate its effectiveness and practicality.}
}


@article{DBLP:journals/cn/GurungPL24,
	author = {Dev Gurung and
                  Shiva Raj Pokhrel and
                  Gang Li},
	title = {Performance analysis and evaluation of postquantum secure blockchained
                  federated learning},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110849},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110849},
	doi = {10.1016/J.COMNET.2024.110849},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GurungPL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the field of quantum computing progresses, traditional cryptographic algorithms such as RSA and ECDSA are becoming increasingly vulnerable to quantum-based attacks, underscoring the need for robust post-quantum security in critical systems like Federated Learning (FL) and Blockchain. In light of this, we propose a novel hybrid approach for blockchain-based FL (BFL) that integrates a stateless signature scheme, such as Dilithium or Falcon, with a stateful hash-based scheme like XMSS. This combination leverages the complementary strengths of both schemes to provide enhanced security. To further optimize performance, we introduce a linear formula-based device role selection method that takes into account key factors such as computational power and stake accumulation. This selection process is reinforced by a verifiable random function (VRF), which strengthens the blockchain consensus mechanism. Our extensive experimental results demonstrate that this hybrid approach significantly enhances both the security and efficiency of BFL systems, establishing a robust framework for the integration of post-quantum cryptography as we transition into the quantum computing era.}
}


@article{DBLP:journals/cn/DengHXHC24,
	author = {Yingzhuo Deng and
                  Zicheng Hu and
                  Weihao Xu and
                  Ningning Han and
                  Haibin Cai},
	title = {Collaborative resource allocation in computing power networks: {A}
                  game-theoretic double auction perspective},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110850},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110850},
	doi = {10.1016/J.COMNET.2024.110850},
	timestamp = {Tue, 12 Nov 2024 18:39:27 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DengHXHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growth of global data is increasing exponentially, leading to a greater demand for computing power. To address this requirement, expanding computing power from the cloud to the edge is essential. However, this transformation presents two significant challenges: how to share computing resources more efficiently and how to optimize resource allocation. To tackle these challenges, we propose a three-layer Computing Power Network (CPN) framework that focuses on implementing the collaborative allocation of computing nodes and user tasks. We formulate the resource allocation problem in CPN as a double auction game and use an experience-weighted attraction algorithm that enables participants to adjust bidding strategies based on environmental interactions. We implemented a prototype of our proposed CPN framework and conducted extensive experiments to verify our algorithm’s convergence and evaluate the benefits obtained by buyers (users) and sellers (computing nodes) from the perspective of transaction prices, rewards, and average pricing. The comprehensive experimental results demonstrate the effectiveness of our proposed method. Compared with state-of-the-art pricing strategies, our approach achieves a 20% increase in convergence speed and an 88% increase in overall returns. Furthermore, it also exhibits a 2.5% increase in deal prices and a substantial 83% rise in the income of individual users. These outcomes convincingly prove the superiority of our method in achieving better convergence, improving overall returns, and benefiting both buyers and sellers in the CPN resource auction market.}
}


@article{DBLP:journals/cn/AshrafAAI24,
	author = {Usman Ashraf and
                  Adnan Ahmed and
                  Stefano Avallone and
                  Pasquale Imputato},
	title = {Traffic evolution in Software Defined Networks},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110852},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110852},
	doi = {10.1016/J.COMNET.2024.110852},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AshrafAAI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Networking (SDN) offers unprecedented traffic engineering possibilities due to optimal centralized decision making. However, network traffic evolves over time and changes the underlying optimization problem. Frequent application of the model to reflect traffic evolution causes flooding of control messages, traffic re-routing and synchronization problems. This paper addresses the problem of graceful traffic evolution in SDNs (Software Defined Networks) minimizing rule installations and modifications, optimizing the global objectives of minimization of Maximum Link Utilization (MLU) and minimization of the Maximum Switch Table Space Utilization (MSTU). The problem is formulated as multi-objective optimization using Mixed Integer Linear Programming (MILP). Proof of NP-Hardness is provided. Then, we re-formulate the problem as a single-objective problem and propose two greedy algorithms to solve the single-objective problem, namely MIRA-Im and MIRA-Im with Conflict Detection, and experiments are performed to show the effectiveness of the algorithms in comparison to previous state of the art proposals. Simulation results show significant improvements of MIRA-Im with Conflict Detection, especially in terms of number of installed rules (with a gain till 80% with the highest number of flows) and flow table space utilization (with a gain till 55% with the highest number of flows), compared to MIRA-Im and other algorithms available in the literature, while the other metrics are essentially stable.}
}


@article{DBLP:journals/cn/CasettiCDMGS24,
	author = {Claudio Casetti and
                  Carla{-}Fabiana Chiasserini and
                  Falko Dressler and
                  Agon Memedi and
                  Diego Gasco and
                  Elad Michael Schiller},
	title = {AI/ML-based services and applications for 6G-connected and autonomous
                  vehicles},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110854},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110854},
	doi = {10.1016/J.COMNET.2024.110854},
	timestamp = {Sat, 30 Nov 2024 21:09:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/CasettiCDMGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {AI and ML emerge as pivotal in overcoming the limitations of traditional network optimization techniques and conventional control loop designs, particularly in addressing the challenges of high mobility and dynamic vehicular communications inherent in the domain of connected and autonomous vehicles (CAVs). The survey explores the contributions of novel AI/ML techniques in the field of CAVs, also in the context of innovative deployment of multilevel cloud systems and edge computing as strategic solutions to meet the requirements of high traffic density and mobility in CAV networks. These technologies are instrumental in curbing latency and alleviating network congestion by facilitating proximal computing resources to CAVs, thereby enhancing operational efficiency also when AI-based applications require computationally-heavy tasks. A significant focus of this survey is the anticipated impact of 6G technology, which promises to revolutionize the mobility industry. 6G is envisaged to foster intelligent, cooperative, and sustainable mobility environments, heralding a new era in vehicular communication and network management. This survey comprehensively reviews the latest advancements and potential applications of AI/ML for CAVs, including sensory perception enhancement, real-time traffic management, and personalized navigation.}
}


@article{DBLP:journals/cn/ZhuHWLYT24,
	author = {Zaixing Zhu and
                  Tao Hu and
                  Di Wu and
                  Chengcheng Liu and
                  Siwei Yang and
                  Zhifu Tian},
	title = {Topology sensing of {FANET} under missing data},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110856},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110856},
	doi = {10.1016/J.COMNET.2024.110856},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhuHWLYT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The topological structure of a flying ad hoc network (FANET) is crucial to understand, explain, and predict the behavior of unmanned aerial vehicle (UAV) swarms. Most studies focusing on topology sensing use perfect observations and complete datasets. However, the received signal dataset, being non-cooperative, commonly encounters instances of missing data, causing the performance of the existing algorithms to degrade. We investigate the issue of topology sensing of FANET based on external observations and propose a topology sensing method for FANET with missing data while introducing link-prediction methods to correct the topology inference results. First, we employ multi-dimensional Hawkes processes to model the communication event sequence in the network. Subsequently, to solve the problem in which the binary decision threshold is difficult to determine and cannot be adapted to the application scenario, we propose an extended multi-dimensional Hawkes model suitable for FANET and use the maximum likelihood estimation method for topology inference. Finally, to solve the problem of the low accuracy of inference results owing to missing data, we perform community detection on the observation network and combine the community detection and inference results to construct a mixed connection probability matrix, based on which we perform topology correction. The results of the analysis show that the topology sensing method proposed in this study is robust against missing data, indicating that it is an effective solution for solving this problem.}
}


@article{DBLP:journals/cn/AlkhaldiDAHO24,
	author = {Tareq M. Alkhaldi and
                  Abdulbasit A. Darem and
                  Asma Alhashmi and
                  Tawfik Al Hadhrami and
                  Azza Elneil Osman},
	title = {Enhancing smart city IoT communication: {A} two-layer NOMA-based network
                  with caching mechanisms and optimized resource allocation},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110857},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110857},
	doi = {10.1016/J.COMNET.2024.110857},
	timestamp = {Sat, 30 Nov 2024 21:09:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlkhaldiDAHO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With advancements in next-generation communication systems, large-scale Internet of Things device (IoTDs) deployments in smart cities face challenges like limited bandwidth, high latency, and network congestion. To address this, we propose a two-layer network architecture utilizing non-orthogonal multiple access (NOMA) and caching to enhance IoT communications’ performance, efficiency, and reliability. Our primary objective is to optimize resource allocation and solve the association problem in a two-layer network. We formulated a joint optimization problem to maximize system utility through device association, power, and bandwidth allocation, considering constraints like channel quality and interference. We decoupled the non-linear, non-convex problem using block coordinate descent (BCD) and inner approximation techniques to maximize the aggregated data rate in high-density IoT scenarios. This approach reduced computational complexity while proving the scheme’s theoretical and numerical convergence. To evaluate the proposed scheme, we compared its performance with an ideal backhauling approach, an exhaustive search (upper bound), and a Genetic algorithm-based heuristic. Our scheme outperformed the others, achieving 98.04% of the ideal backhauling and 99.60% of the upper bound. Statistical analysis confirmed its robustness and consistent performance across various conditions. The two-layer NOMA-based network with caching and optimized resource allocation significantly enhances IoT communication efficiency and resilience, offering a solid framework for future smart city deployments.}
}


@article{DBLP:journals/cn/OueslatiHCM24,
	author = {Ibtissem Oueslati and
                  Oussama Habachi and
                  Jean{-}Pierre Cances and
                  Vahid Meghdadi},
	title = {LoCoNOMA: {A} grant-free resource allocation for massive {MTC}},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110859},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110859},
	doi = {10.1016/J.COMNET.2024.110859},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/OueslatiHCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive machine-type communications (mMTC) represent a significant challenge in the fifth generation of wireless networks (5G) and become increasingly critical in the sixth generation (6G) due to the limited frequency spectrum. Addressing the demands of mMTC requires efficient resource sharing among multiple users. Integrating Grant-Free (GF) access with Non-Orthogonal Multiple Access (NOMA) is a promising strategy to improve spectral efficiency. However, it may cause additional interference and complexity at the gNodeB (gNB) side. To mitigate these issues, we propose a novel, low-complexity GF-NOMA framework for joint power and channel allocation, where devices autonomously select their sub-carriers and power levels in a fully distributed manner. Besides, the gNB’s role is limited to sending a global feedback for device coordination. The proposed technique has been validated analytically and through simulation, demonstrating superior performance compared to existing approaches, in particular for the massive access scenario.}
}


@article{DBLP:journals/cn/BaoDLYHFZ24,
	author = {Yubing Bao and
                  Xin Du and
                  Zhihui Lu and
                  Jirui Yang and
                  Shih{-}Chia Huang and
                  Jianfeng Feng and
                  Qibao Zheng},
	title = {Mitigating critical nodes in brain simulations via edge removal},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110860},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110860},
	doi = {10.1016/J.COMNET.2024.110860},
	timestamp = {Mon, 03 Mar 2025 21:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BaoDLYHFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Brain simulation holds promise for advancing our comprehension of brain mechanisms, brain-inspired intelligence, and addressing brain-related disorders. However, during brain simulations on high-performance computing platforms, the sparse and irregular communication patterns within the brain can lead to the emergence of critical nodes in the simulated network, which in turn become bottlenecks for inter-process communication. Therefore, effective moderation of critical nodes is crucial for the smooth conducting of brain simulation. In this paper, we formulate the routing communication problem commonly encountered in brain simulation networks running on supercomputers. To address this issue, we firstly propose the Node-Edge Centrality Addressing Algorithm (NCA) for identifying critical nodes and edges, based on an enhanced closeness centrality metric. Furthermore, drawing on the homology of spikes observed in biological brains, we develop the Edge Removal Transit Algorithm (ERT) to reorganize sparse and unbalanced inter-process communication in brain simulation, thereby diminishing the information centrality of critical nodes. Through extensive simulation experiments, we evaluate the performance of the proposed communication scheme and find that the algorithm accurately identifies critical nodes with a high accuracy. Our simulation experiments on 1600 GPU cards demonstrate that our approach can reduce communication latency by up to 25.4%, significantly shortening simulation time in large-scale brain simulations.}
}


@article{DBLP:journals/cn/LiWTYH24,
	author = {Qiyuan Li and
                  Yumeng Wang and
                  Donghai Tian and
                  Chong Yuan and
                  Changzhen Hu},
	title = {Component-based modeling of cascading failure propagation in directed
                  dual-weight software networks},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110861},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110861},
	doi = {10.1016/J.COMNET.2024.110861},
	timestamp = {Sat, 30 Nov 2024 21:09:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiWTYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software vulnerabilities often lead to cascading failures, resulting in service unavailability and potential breaches of user data. However, existing models for cascading failure propagation typically focus solely on the static design’s calling relationships, disregarding dynamic runtime propagation paths. Moreover, current network topology models primarily consider function calling frequency while overlooking critical factors like internal failure probability and component failure tolerance rates. Yet, these factors significantly influence the actual propagation of software cascading failures. In this study, we address these limitations by incorporating internal failure probabilities and calling frequencies as node and edge weights, respectively. This forms the basis of our component-based directed dual-weight software network cascading failure propagation model. This model encompasses the evaluation of cascading failure propagation through intra-component and inter-component propagation probabilities, alongside the constraint of component failure tolerance rates. Through extensive experiments conducted on six real-world software applications, our model has demonstrated its effectiveness in predicting software cascading failure propagation processes. This method deepens our understanding of software failures and structures, equipping software testers with the knowledge to make well-informed judgments regarding software quality concerns.}
}


@article{DBLP:journals/cn/VermaCA24,
	author = {Himanshu Verma and
                  Naveen Chauhan and
                  Lalit Kumar Awasthi},
	title = {Enhanced Hybrid Congestion Mitigation Strategy for '6LoWPAN-RPL
                  based patient-centric IoHT'},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110862},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110862},
	doi = {10.1016/J.COMNET.2024.110862},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/VermaCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Healthcare Things (IoHT) is rapidly evolving, providing new opportunities to enhance healthcare delivery. However, the resource limitation of connected medical sensing devices leads to congestion, resulting in reduced network performance, delayed data transmission, and loss of critical medical information, which can have significant consequences in healthcare. To address this issue, this paper proposes an Enhanced Hybrid Congestion Mitigation Strategy (EHCMS) for IPv6 over low-power wireless personal area networks (6LoWPAN) and routing protocol for low-Power and lossy networks (RPL) based patient-centric IoHT (PC-IoHT). The EHCMS combines several techniques, including traffic management, network topology optimization, and load balancing, to enhance network performance and reduce congestion. The proposed framework is a hybrid strategy that utilizes resource- and traffic-control mechanisms to alleviate congestion in the 6LoWPAN-RPL-based patient-centric IoHT network. For the resource-control-based approach, a congestion-aware composite objective function is designed using a few congestion-specific routing metrics and formulated as a multi-attribute decision-making problem solved using Grey relational analysis (GRA). In addition, a non-linear multi-criteria optimization problem-based transmission rate adaptation mechanism is contrived as a traffic-control scheme for congestion mitigation. The effectiveness of the proposed EHCMS is evaluated using simulations on the Cooja simulator in the Contiki-3.0 OS and compared with existing congestion-alleviating strategies. The results demonstrate that the proposed framework can significantly reduce congestion in the IoHT network, perform better than existing works, and improve the quality of service. This research paper contributes to the field of IoHT by proposing an effective congestion mitigation strategy that enhances the reliability and performance of the PC-IoHT network, ultimately improving the quality of healthcare delivery.}
}


@article{DBLP:journals/cn/YangLXHWC24,
	author = {Guangcan Yang and
                  Peixuan Li and
                  Yang Xin and
                  Yunhua He and
                  Chao Wang and
                  Xiubo Chen},
	title = {An efficient hierarchical attribute-based encryption scheme with cross-domain
                  data sharing},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110863},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110863},
	doi = {10.1016/J.COMNET.2024.110863},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YangLXHWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid advancement of data sharing technology, an increasing amount of data is being stored on cloud servers. To enable fine-grained access control over the data stored on cloud servers, the Ciphertext-Policy Attribute-Based Encryption (CP-ABE) technology has been widely adopted. Recognizing that shared data and files often possess a hierarchical structure, hierarchical CP-ABE technology has been proposed recently. However, most existing schemes are restricted to single-domain data access, which limits their flexibility and universal applicability in practical applications. To address this limitation, an access control scheme based on hierarchical CP-ABE, named CDS-CP-ABE, is proposed to facilitate secure and efficient cross-domain data sharing. The scheme is capable of not only realizing fine-grained hierarchical access control within a single domain but also enabling cross-domain data sharing. Security analysis confirms that our scheme effectively resists chosen-plaintext attack. Furthermore, empirical results indicate that the time consumption associated with our scheme is lower compared to other existing schemes.}
}


@article{DBLP:journals/cn/LiuXWQ24,
	author = {Xueyan Liu and
                  Xin Xiong and
                  Jia Wang and
                  Yujiao Qi},
	title = {An Internet of Vehicles road traffic data sharing scheme based on
                  signcryption and editable blockchain},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110864},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110864},
	doi = {10.1016/J.COMNET.2024.110864},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiuXWQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aiming at the problems of low real-time update efficiency of traffic data and privacy leakage of vehicle users in the Internet of Vehicles (IoV) data sharing, an IoV road traffic data sharing scheme based on signcryption and editable blockchain is proposed. Road-Side Unit (RSU) implements proxy signcryption for sharing data based on the authorization of the vehicle, ensuring the reliability of data and reducing the frequent interaction between vehicles and authorized map companies when sharing data. The editable blockchain is implemented by the chameleon hash function, and no new blocks need to be generated when updating data, thereby reducing the storage overhead caused by updating data. Combined with editable blockchain and fog computing server, the storage, sharing and update of traffic data are better realized. Security analysis shows that the proposed scheme satisfies the confidentiality, integrity, verifiability and unforgeability. Simulation results show that the proposed scheme has less communication overhead and computing overhead.}
}


@article{DBLP:journals/cn/YangFZWLY24,
	author = {Lei Yang and
                  Juan A. Fraire and
                  Kanglian Zhao and
                  Ruhai Wang and
                  Wenfeng Li and
                  Hong Yang},
	title = {Optimizing deep-space {DTN} congestion control via deep reinforcement
                  learning},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110865},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110865},
	doi = {10.1016/J.COMNET.2024.110865},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YangFZWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces an innovative congestion control mechanism for delay/disruption-tolerant networking (DTN) within deep-space communication systems, leveraging the nuanced capabilities of deep reinforcement learning (DRL). This approach significantly departs from traditional methods, addressing the unique challenges of deep-space data transmissions. The proposed DRL-based strategy demonstrates a superior balance of critical factors, including transmission delay, energy efficiency, and data reception integrity. We assess our approach through meticulous simulation and comparison with established benchmark schemes. The findings underscore the mechanism’s enhanced performance metrics, positing it as an appealing solution in the evolving landscape of non-terrestrial networking.}
}


@article{DBLP:journals/cn/ZhangZWS24,
	author = {Ruidong Zhang and
                  Jiadong Zhang and
                  Xue Wang and
                  Wenxiao Shi},
	title = {Utility optimization for computation offloading and splitting in time-varying
                  {HAP} and {LEO} satellite integrated {MEC} networks},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110866},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110866},
	doi = {10.1016/J.COMNET.2024.110866},
	timestamp = {Mon, 02 Dec 2024 08:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhangZWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To provide ubiquitous and low-latency communication and computation services for remote and disaster areas, high altitude platform (HAP) and low earth orbit (LEO) satellite integrated multi-access edge computing (HLS-MEC) networks have emerged as a promising solution. However, most current studies directly assume that the number of connected satellites is fixed and neglect the modeling of the time-varying multi-satellite computing process. Motivated by this, we establish an M/G/K(t) queuing model to illustrate task computation on satellites. To evaluate the efficiency and quality of computation offloading and splitting, we develop a utility model. This model is defined as a difference between a value function that assesses the trade-offs of task offloading, considering latency reductions and energy savings, and a cost function that quantifies expenses related to latency and energy consumption. After formulating the utility maximization problem, we propose the deep reinforcement learning-based offloading and splitting (DBOS) scheme that can overcome the time-varying uncertainties and high dynamics in the HLS-MEC network. Specifically, the DBOS scheme can learn the best computation offloading and splitting policy to maximize the utility by sensing the number of connected satellites, the distance between the HAP and satellites, the available computing resources, and the task arrival rate. Finally, we evaluate and validate the computational complexity and convergence property of the DBOS scheme. Numerical results show that the DBOS scheme outperforms the other three benchmarks and maximizes the utility under time-varying dynamics.}
}


@article{DBLP:journals/cn/DelavarJ24,
	author = {Arash Ghorbannia Delavar and
                  Zahra Jormand},
	title = {{FMORT:} The Meta-Heuristic routing method by integrating index parameters
                  to optimize energy consumption and real execution time using {FANET}},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110869},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110869},
	doi = {10.1016/J.COMNET.2024.110869},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DelavarJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decreasing energy consumption in Unmanned Aerial Vehicles (UAVs) while simultaneously enhancing their reliability and processing capabilities is considered a fundamental challenge. The routing mechanisms employed in Flying Ad Hoc Networks (FANETs) are more complex compared to those in Mobile Ad Hoc Networks (MANETs) and Vehicular Ad Hoc Networks (VANETs), a challenge addressed by the FMORT method. To tackle these complex routing challenges, clustering techniques that utilize hybrid Meta-heuristic algorithms can be applied. Data analysis within the FMORT framework identified factors influencing service integration, leading to a reduction in redundant request transmissions and overall redundancy in the proposed method. The identification of food sources in the hybrid Meta-heuristic algorithm of the FMORT method is achieved through the integration of the Sparrow and Dragonfly algorithms. These algorithms work simultaneously to increase energy efficiency and increase network lifetime. This strategy optimizes information exchange by selecting an intelligent threshold detector and categorizing inputs, thereby minimizing node mobility. As a result, it improves performance metrics and decreases delivery costs, energy consumption, and delays. In the proposed method, a balanced performance is achieved by comparing existing methods in terms of transmission delay, Packet Delivery Ratio( PDR), throughput, and energy consumption. Simulation results show that the FMORT approach provides effective and stable outcomes in terms of reliability, decreased delays, and improved packet delivery rates. The FMORT framework includes principles for neighbor selection, determining suitable cluster heads, and scoring based on the average Euclidean distance. Additionally, it manages topology access, ensures proper distribution, guarantees data connectivity, and accurately categorizes inputs. By optimizing the sensitivity rate, this method minimizes the average delays and meekly values input data through effective load balancing. Key parameters considered for real time optimization of overall performance include the number of cluster heads during re-clustering, the ratio of request-to-acknowledgment packet transmission, node, and network lifetime, end-to-end delay, and energy consumption. Ultimately, the simulation results show that compared to the MWCRSF algorithm, the average optimization of index parameters,% 0.73 decrease in energy consumption,% 2.23 network lifetime, 1.35 re-cluster construction time and also% 0.11 re-cluster lifetime has increased.}
}


@article{DBLP:journals/cn/GuptaD24,
	author = {Anjali Gupta and
                  Abhishek Dixit},
	title = {Mathematical analysis of busy tone in full-duplex optical {MAC} for
                  hidden node mitigation},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110870},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110870},
	doi = {10.1016/J.COMNET.2024.110870},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuptaD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article provides the mathematical analysis of carrier sense multiple access with collision avoidance (CSMA/CA) media access control (MAC) protocol of IEEE 802.15.7 optical wireless communication (OWC). While the prior works have performed the OWC CSMA/CA mathematical analysis using the Markov models, deviation from the simulation results has been observed. We address this by improving the Markov model calculations, which display a mere 0.2% throughput deviation, nearly matching the simulation results. Furthermore, we work on the hidden node problem of the OWC networks. This problem is solved in literature by using various full-duplex communication methods, such as bi-directional data transmission and the busy tone signal; the latter is employed in our previous work on full-duplex optical MAC (FD-OMAC). These techniques increase the coverage area of the nodes by utilizing an access point (AP) as a relay node. However, the AP response is delayed by the processing time, causing an unexpected network behavior. The quantitative effect of this delay remains unexplored, which is critical for optimizing the OWC network. We bridge this gap by extending the proposed Markov analysis to model CSMA/CA and the aforementioned full-duplex techniques. This work equips readers with mathematical insights for future OWC MAC layer enhancements.}
}


@article{DBLP:journals/cn/MohammedS24,
	author = {Hesham Mohammed and
                  Dola Saha},
	title = {Encrypted-OFDM: {A} secured wireless waveform},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110871},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110871},
	doi = {10.1016/J.COMNET.2024.110871},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MohammedS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communication has been a broadcast system since its inception, which violates security and privacy issues at the physical layer between the intended transmit and receive pairs. Consequently, it is essential to secure the wireless signal such that only the intended receiver can realize the signal properties. In this paper, we propose Encrypted-OFDM, a new waveform, where the signal structure is altered to encrypt the waveform with a shared secret key. We achieve the signal level security by modifying the OFDM signal in time-domain, thus erasing the OFDM properties and obfuscating the signal properties to an eavesdropper. We present a two-stage encryption algorithm to increase the robustness of the transmitted waveform and achieve a high level of secrecy, even when low entropy keys are used. We also introduce a novel channel estimation algorithm by removing the pilots, so that only the intended receiver can estimate the channel correctly. Furthermore, we perform both secrecy and error analysis for the transmitted and received Encrypted-OFDM waveform. Extensive simulation and over-the-air experiments show that the performance of Encrypted-OFDM is comparable to legacy OFDM, and the SNR penalty due to the secured waveform varies between\n≈\n1–4 dB. In all these scenarios, Encrypted-OFDM remains unrecognized at the eavesdropper.}
}


@article{DBLP:journals/cn/PanGJLWCOH24,
	author = {Gang Pan and
                  Xin Guan and
                  Haiyang Jiang and
                  Yongnan Liu and
                  Huayang Wu and
                  Hongyang Chen and
                  Tomoaki Ohtsuki and
                  Zhu Han},
	title = {Joint intelligent optimizing economic dispatch and electric vehicles
                  charging in 5G vehicular networks},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110872},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110872},
	doi = {10.1016/J.COMNET.2024.110872},
	timestamp = {Tue, 12 Nov 2024 18:39:27 +0100},
	biburl = {https://dblp.org/rec/journals/cn/PanGJLWCOH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, with the rapid development of 5G networks, the road traffic network composed of vehicles with different energy sources has become more and more complex, and the problems of environmental pollution and road congestion have also become increasingly serious. Electric vehicles are favored by people due to their environmental protection and energy-saving characteristics. However, improper charging dispatching will cause excess energy in charging stations, affecting the power grid and road traffic, such as energy shortages and lower traffic throughput. Therefore, how to design a reasonable charging strategy that can maximize the user’s charging satisfaction and consume the energy of the charging station as much as possible becomes a challenge. Meanwhile, this strategy should consider power economic dispatch to reduce power generation costs and polluting gas emissions. With the support of 5G’s high-bandwidth and low-latency characteristics, this paper designs an intelligent charging model which indirectly reflects the charging satisfaction through the time cost, energy consumption cost, charging cost, and the user’s range anxiety, while consuming the remaining energy of the charging station as much as possible. Due to the uncertainty of wind and photovoltaic power generation, this paper proposes a two-stage economic dispatch model to improve the accuracy of power dispatch and reduce power generation costs and carbon emissions. Due to the highly variable traffic environment and energy demand, we employ proximal policy optimization-based deep reinforcement learning algorithms to realize electric vehicle charging dispatching and charging station power dispatching. Numerical results show the efficiency of our proposed strategy for electric vehicle charging in terms of the convergence speed.}
}


@article{DBLP:journals/cn/FraireHSOHWBRMSR24,
	author = {Juan A. Fraire and
                  Santiago Henn and
                  Gregory Stock and
                  Robin Ohs and
                  Holger Hermanns and
                  Felix Walter and
                  Lynn Van Broock and
                  Gabriel Ruffini and
                  Federico Machado and
                  Pablo Serratti and
                  Jose Relloso},
	title = {Quantitative analysis of segmented satellite network architectures:
                  {A} maritime surveillance case study},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110874},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110874},
	doi = {10.1016/J.COMNET.2024.110874},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FraireHSOHWBRMSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents an in-depth trade-off analysis of a Swarm Satellite Constellation (SSC) Mission for Earth observation that leverages Segmented Architecture (SA), a concept designed by the Argentinian Space Agency (CONAE) within the New Space philosophy. This architecture consists of a scenario featuring a networked constellation of small, cooperative satellites to enhance mission flexibility, reliability, coverage, and cost-effectiveness. Despite its promising prospects, SA features challenges in its mission design and definition phases due to the complex interplay between distributed space systems, technological innovation, and geographical landscapes. Our study analyzes an innovative quantitative analysis framework integrated with Ansys’ Systems Toolkit (STK). The resulting software tool models critical components, including ground and space segments, orbital dynamics, coverage, onboard processing, and communication links. We focus on a hypothetical SARE mission to detect illicit maritime activity near Argentina’s Exclusive Economic Zone (EEZ). This case study constitutes an archetypal mission elucidating the architecture’s benefits and complexities, addressing swarm coverage, contact dynamics, and data handling strategies. Results contribute to discussions on the practical trade-off in current and future Segmented Satellite Architectures with multiple mission objectives.}
}


@article{DBLP:journals/cn/AygulTCA24,
	author = {Mehmet Ali Ayg{\"{u}}l and
                  Halise T{\"{u}}rkmen and
                  Hakan A. {\c{C}}irpan and
                  Huseyin Arslan},
	title = {Machine learning-driven integration of terrestrial and non-terrestrial
                  networks for enhanced 6G connectivity},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110875},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110875},
	doi = {10.1016/J.COMNET.2024.110875},
	timestamp = {Sat, 30 Nov 2024 21:09:15 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AygulTCA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-terrestrial networks (NTN)s are essential for achieving the persistent connectivity goal of sixth-generation networks, especially in areas lacking terrestrial infrastructure. However, integrating NTNs with terrestrial networks presents several challenges. The dynamic and complex nature of NTN communication scenarios makes traditional model-based approaches for resource allocation and parameter optimization computationally intensive and often impractical. Machine learning (ML)-based solutions are critical here because they can efficiently identify patterns in dynamic, multi-dimensional data, offering enhanced performance with reduced complexity. ML algorithms are categorized based on learning style—supervised, unsupervised, and reinforcement learning—and architecture, including centralized, decentralized, and distributed ML. Each approach has advantages and limitations in different contexts, making it crucial to select the most suitable ML strategy for each specific scenario in the integration of terrestrial and non-terrestrial networks (TNTN)s. This paper reviews the integration architectures of TNTNs as outlined in the 3rd Generation Partnership Project, examines ML-based existing work, and discusses suitable ML learning styles and architectures for various TNTN scenarios. Subsequently, it delves into the capabilities and challenges of different ML approaches through a case study in a specific scenario.}
}


@article{DBLP:journals/cn/BlancoRomeroLASGCM24,
	author = {Javier Blanco{-}Romero and
                  Vicente Lorenzo and
                  Florina Almen{\'{a}}rez and
                  Daniel D{\'{\i}}az S{\'{a}}nchez and
                  Carlos Garc{\'{\i}}a{-}Rubio and
                  Celeste Campo and
                  Andr{\'{e}}s Mar{\'{\i}}n},
	title = {Evaluating integration methods of a quantum random number generator
                  in OpenSSL for {TLS}},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110877},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110877},
	doi = {10.1016/J.COMNET.2024.110877},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BlancoRomeroLASGCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid advancement of quantum computing poses a significant threat to conventional cryptography. Whilst post-quantum cryptography (PQC) stands as the prevailing trend for fortifying the security of cryptographic systems, the coexistence of quantum and classical computing paradigms presents an opportunity to leverage the strengths of both technologies, for instance, nowadays the use of Quantum Random Number Generators (QRNGs) – considered as True Random Number Generators (TRNGs) – opens up the possibility of discussing hybrid systems. In this paper, we evaluate both aspects, on the one hand, we use hybrid TLS (Transport Layer Security) protocol that leverages the widely used secure protocol on the Internet and integrates PQC algorithms, and, on the other hand, we evaluate two approaches to integrate a QRNG, i.e., Quantis PCIe-240M, in OpenSSL 3.0 to be used by TLS. Both approaches are compared through a Nginx Web server, that uses OpenSSL’s implementation of TLS 1.3 for secure web communication. Our findings highlight the importance of optimizing such integration method, because while direct integration can lead to performance penalties specific to the method and hardware used, alternative methods demonstrate the potential for efficient QRNG deployment in cryptographic systems.}
}


@article{DBLP:journals/cn/OjijoRO24,
	author = {Mourice Otieno Ojijo and
                  Daniel T. Ramotsoela and
                  Ruth Oginga},
	title = {Slice admission control in 5G wireless communication with multi-dimensional
                  state space and distributed action space: {A} sequential twin actor-critic
                  approach},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110878},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110878},
	doi = {10.1016/J.COMNET.2024.110878},
	timestamp = {Mon, 23 Dec 2024 20:34:59 +0100},
	biburl = {https://dblp.org/rec/journals/cn/OjijoRO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing represents a paradigm shift in the way resources are allocated for different 5G network functions through network function virtualization. This innovation aims to facilitate logical resource allocation, accommodating the anticipated surge in network resource requirements. This will harness automatic processing, scheduling, and orchestration for efficient management. To overcome the challenge of managing network resources under heavy demand, slice providers need to leverage both artificial intelligence and slice admission control strategies. While 5G network resources can be allocated to maintain a slice, the logical allocation and real-time network evaluation must be continuously examined and adjusted if network resilience is to be maintained. The complex task of leveraging slice admission control to maintain 5G network resilience has not been fully investigated. To tackle this problem, we propose a machine learning approach for slice admission control and resource allocation optimization so as to maintain network resilience. Machine learning algorithms offer a powerful tool for making robust and autonomous decisions, which are crucial for effective slice admission control. By intelligently allocating resources based on real-time demand and network conditions, these algorithms can help ensure long-term network resilience and achieve key objectives. While various machine learning algorithms hold promise for 5G resource management and admission control, reinforcement learning (RL) has emerged as a particularly exciting solution. Its ability to mimic human learning processes makes it a versatile solution, well-suited to tackle the complex challenges of network control. To fill this gap, we propose a new technique known as sequential twin actor critic (STAC). Simulations show that the STAC improves network resilience through enhanced admission probability and overall utility.}
}


@article{DBLP:journals/cn/YangGHJWY24,
	author = {Duo Yang and
                  Yunqi Gao and
                  Bing Hu and
                  A{-}Long Jin and
                  Wei Wang and
                  Yang You},
	title = {{GWPF:} Communication-efficient federated learning with Gradient-Wise
                  Parameter Freezing},
	journal = {Comput. Networks},
	volume = {255},
	pages = {110886},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110886},
	doi = {10.1016/J.COMNET.2024.110886},
	timestamp = {Mon, 02 Dec 2024 08:13:58 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YangGHJWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication bottleneck is a critical challenge in federated learning. While parameter freezing has emerged as a popular approach, utilizing fine-grained parameters as aggregation objects, existing methods suffer from issues such as a lack of thawing strategy, lag and inflexibility in the thawing process, and underutilization of frozen parameters’ updates. To address these challenges, we propose Gradient-Wise Parameter Freezing (GWPF), a mechanism that wisely controls frozen periods for different parameters through parameter freezing and thawing strategies. GWPF globally freezes parameters with insignificant gradients and excludes frozen parameters from global updates during the frozen period, reducing communication overhead and accelerating training. The thawing strategy, based on global decisions by the server and collaboration with clients, leverages real-time feedback on the locally accumulated gradients of frozen parameters in each round, achieving a balanced approach between mitigating communication and enhancing model accuracy. We provide theoretical analysis and a convergence guarantee for non-convex objectives. Extensive experiments confirm that our mechanism achieves a speedup of up to 4.52 times in time-to-accuracy performance and reduces communication overhead by up to 48.73%. It also improves final model accuracy by up to 2.01% compared to the existing fastest method APF. The code for GWPF is available at https://github.com/Dora233/GWPF.}
}
