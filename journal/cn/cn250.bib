@article{DBLP:journals/cn/CharpentierSLCVM24,
	author = {Vincent Charpentier and
                  Nina Slamnik{-}Krijestorac and
                  Giada Landi and
                  Matthias Caenepeel and
                  Olivier Vasseur and
                  Johann M. M{\'{a}}rquez{-}Barja},
	title = {Paving the way towards safer and more efficient maritime industry
                  with 5G and Beyond edge computing systems},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110499},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110499},
	doi = {10.1016/J.COMNET.2024.110499},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CharpentierSLCVM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Maritime and Road Transport and Logistics (T&L) vertical industries account for 92.5% of the European total freight transport. Therefore, efficient T&L systems in busy port environments are critical for Europe’s global competitiveness. The emergence of 5G and Beyond, including Standalone (SA) with data rates up to 20 Gbps, latencies as low as 5 milliseconds (ms), and exceptional reliability (99.999%), offers a significant opportunity for innovation in the Maritime sector, in terms of smarter and safer operations. In this paper, we investigate the enhancements brought forth by Edge Network-aware Applications (EdgeApps) strategically positioned at the network edge, focusing on a practical maritime use case denominated as “Assisted Vessel Navigation”, all within the overarching framework of the edge-cloud/core continuum paradigm. We utilize the so-called edge network softwarization and intelligence that allows us to create EdgeApps, capable of running on 5G and Beyond-enabled edge and cloud infrastructure to increase the safety and efficiency of maritime operations. The EdgeApp framework simplifies the creation of complex 5G and Beyond vertical services and fosters collaboration between industry stakeholders, network experts, and EdgeApp developers. The contributions of this paper are threefold: i) this paper focuses on the significance of edge computing systems within the 5G ecosystem and their role in enhancing vertical industries such as transport and logistics (T&L), with a specific focus on maritime applications, ii) the core contribution of this paper lies in the design and implementation of EdgeApps tailored to vertical industries. We explore their deployment in a real-life maritime environment, and iii) we share valuable insights derived from our real-life experiments conducted in the Port of Antwerp-Bruges, analyzing network and service performance results based on the use case requirements. Therefore, we examine the deployment options of EdgeApps, comparing edge versus cloud environments to assess where they need to run in the edge-cloud continuum. The outcomes and lessons learned from this article hold important implications for vertical industries.}
}


@article{DBLP:journals/cn/ZhuZJXZ24,
	author = {Konglin Zhu and
                  Fuchun Zhang and
                  Lei Jiao and
                  Bowei Xue and
                  Lin Zhang},
	title = {Client selection for federated learning using combinatorial multi-armed
                  bandit under long-term energy constraint},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110512},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110512},
	doi = {10.1016/J.COMNET.2024.110512},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuZJXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a federated learning system, it is often the case that the more clients it involves, the less increment of the outcome it achieves. It is thus essential to design a client selection strategy to choose an appropriate subset of the clients to participate in federated learning. However, client selection is not easy due to the heterogeneity of clients and the long-term energy budget of each client. Moreover, long-term energy budgets intertwined with the short-term client selection often make the problem NP-hard. In this paper, we propose an online strategy Energy-Aware Client Selection for Federated Learning (EACS-FL) to address this problem. The problem is formulated with a joint energy and delay optimization objective, and the Combinatorial Multi-Armed Bandit (CMAB) is introduced to solve the problem in an online manner. We take advantage of Lyapunov optimization to manage energy consumption of clients, which enables us to deal with independent energy budgets through minimizing virtual energy deficit queues. Theoretical analysis shows that EACS-FL achieves sublinear regret and keeps all queues stable. Experiment results exhibit that the proposed approach outperforms the existing works and achieves close-to-optimal delay and energy consumption performance.}
}


@article{DBLP:journals/cn/FarreaBDL24,
	author = {Kerry Anne Farrea and
                  Zubair A. Baig and
                  Robin Ram Mohan Doss and
                  Dongxi Liu},
	title = {Provably secure optimal homomorphic signcryption for satellite-based
                  internet of things},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110516},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110516},
	doi = {10.1016/J.COMNET.2024.110516},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FarreaBDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellites have long been reliable and widely used mediums for communication. With recent advancements in the Internet of Things (IoT), satellite-based IoT has emerged, enhancing global connectivity by enabling real-time communication between remote sensing devices and central hubs through satellite links. However, secure and authenticated communication between Low Earth Orbit (LEO) satellites and resource-constrained ground devices remains underexplored. This paper introduces a novel homomorphic signcryption scheme designed for satellite IoT environments. Our scheme employs hyperelliptic curve cryptography and homomorphic encryption to facilitate the privacy-preserving transmission of aggregated data from multiple IoT devices to LEO satellites. We benchmark our homomorphic signcryption scheme against several established schemes tailored for different network environments, including for satellite networks, Identity-based Signcryption for Smart Grids, 3-Factor Authentication schemes and heterogeneous Vehicular Ad-hoc Networks (VANETs) encryption schemes. Our scheme demonstrates substantial improvements in computational efficiency, communication overhead, and storage capacity. It reduces computational costs by 90% to 99%, communication overhead by 41% to 92%, and storage demands by 62% to 93% compared to existing methods. These quantitative findings demonstrate the practical viability of our approach for satellite IoT deployments with limited resources. Additionally, the robustness of our approach is confirmed through formal security validation using BAN Logic and Scyther, which verifies its strong security, optimal performance, and privacy preservation capabilities, making it ideally suited for real-world satellite IoT systems.}
}


@article{DBLP:journals/cn/ElBouananiBDT24,
	author = {Houssam ElBouanani and
                  Chadi Barakat and
                  Walid Dabbous and
                  Thierry Turletti},
	title = {Fidelity-aware large-scale distributed network emulation},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110531},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110531},
	doi = {10.1016/J.COMNET.2024.110531},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ElBouananiBDT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The design and development of new network protocols, architectures, and technologies requires an evaluation phase where the researcher must provide empirical evidence for the performance of their contributions, potentially in comparison to existing solutions. In this context, network emulation has proven to be an attractive approach as it offers more flexibility compared to traditional testing platforms, and more realism compared to simulation. Network emulators provide contained, customisable, and scalable testing environments both for researchers to evaluate their contributions and for the community to reproduce their results. However, two limitations to network emulation have been identified and well documented in the literature: its scalability limits and its accuracy issues. This paper\n1\ndocuments our attempts to address these concerns. Our findings are distilled into Hifinet: a lightweight scalable and fidelity-aware distributed network emulator. We particularly show how Hifinet outperforms its state-of-the-art counterparts in terms of scalability and efficiency by working around the flaws of their design principles and the technological limitations of the tools they rely on. Hifinet is also fidelity-enhanced, in that it implements a well-theorised fidelity monitoring framework, which passively monitors emulated packet delays to evaluate realism of network emulation and accuracy of results. Another asset of Hifinet is its ability to infer underlying causes in case of erroneous emulation and guide the user through fixing them. This is achieved by using delay tomography algorithms and heuristics.}
}


@article{DBLP:journals/cn/JiaoSCMZY24,
	author = {Jingsen Jiao and
                  Ranran Sun and
                  Yizhi Cao and
                  Qifeng Miao and
                  Yanchun Zuo and
                  Weidong Yang},
	title = {Study on covert rate in the {D2D} networks with multiple non-colluding
                  wardens},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110532},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110532},
	doi = {10.1016/J.COMNET.2024.110532},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JiaoSCMZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the covert communication of cellular link in the device-to-device (D2D)-enabled underlaying cellular network, including a base station, a cellular user, a D2D pair, and multiple non-colluding wardens. To conduct covert communication between the base station and the cellular user without being detected by the wardens, the underlaid D2D pair transmits with a random power to add uncertainty to wardens. We first model the detection of multiple non-colluding wardens, and the average minimum detection error probability of the worst case is derived. To measure the covert performance of the cellular link, the average covert rate of the cellular link and the outage probability of the D2D link are explored. Then, we formulate an optimization problem of maximizing the average covert rate under the constraint of the average minimum detection error probability, and an optimization algorithm is designed to solve the optimization problem to identify the optimal transmit power of the D2D pair. Last, the extensive numerical results are presented to show the impacts of network parameters on the covert performance, which indicates that although D2D communication causes interference to the cellular link, it can also help achieve covert communication by carefully designing its maximum transmit power.}
}


@article{DBLP:journals/cn/ChenZZ24,
	author = {Huiling Chen and
                  Chunmei Zhang and
                  Zeyan Zhang},
	title = {Adaptive pinning control to topology identification of output stochastic
                  complex dynamical network},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110536},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110536},
	doi = {10.1016/J.COMNET.2024.110536},
	timestamp = {Wed, 12 Jun 2024 10:58:43 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper concerns partial topology identification problem of stochastic delayed complex dynamical network. Compared with many network models, the novel output coupling is introduced to the network model of this paper, which does not require all state variables of the vertex to be observable. By utilizing the graph-theoretic method and adaptive pinning control technology, the identification criteria of multi-weighted stochastic complex dynamical networks are obtained. Ultimately, the Lorenz system and small-world network are chosen to do numerical simulations. The simulation results indicate that the successful identification time (SIT) of stochastic delayed complex dynamical network with output coupling is reduced by about 56% on average by comparing with the SIT of the network without output coupling, which implies the superiority of output coupling.}
}


@article{DBLP:journals/cn/RazaKSAWWW24,
	author = {Khuhawar Arif Raza and
                  Salabat Khan and
                  Shivanshu Shrivastava and
                  M. Wasim Abbas Ashraf and
                  Ting Wang and
                  Kaishun Wu and
                  Lu Wang},
	title = {A lightweight group-based SDN-driven encryption protocol for smart
                  home IoT devices},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110537},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110537},
	doi = {10.1016/J.COMNET.2024.110537},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RazaKSAWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing adoption rate of Internet of Things (IoT) devices in smart home applications, it is vital to safeguard the privacy and security of information, communications among IoT devices and the underlying infrastructure. In open communication scenarios, an adversary can easily tamper with data transmitted by IoT communication devices. This paper proposes a softwarized lightweight authentication key agreement scheme for smart home applications in Software-Defined IoT (SDIoT) environment. The proposed scheme comprises registration, authentication, and key selection phases. Devices are registered in the registration phase after the successful completion of validation checks, while sessions’ keys are granted to the registered devices in the authentication phase. In the final phase, controllers classify IoT devices based on pre-defined parameters and impose class-specific access control based on classification. The security of the proposed scheme is validated using the widely accepted AVISPAs verification tool against a strong adversary. Simulation results demonstrate that our proposed scheme outperforms existing schemes in terms of running time, computation complexity, and energy consumption. It is found that the proposed scheme has significant cost-effectiveness.}
}


@article{DBLP:journals/cn/VoutsasVL24,
	author = {Fotios Voutsas and
                  John Violos and
                  Aris Leivadeas},
	title = {Mitigating Alert Fatigue in Cloud Monitoring Systems: {A} Machine
                  Learning Perspective},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110543},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110543},
	doi = {10.1016/J.COMNET.2024.110543},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/VoutsasVL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next generation networks will be largely based on monitoring and telemetry tools that are essential for maintaining optimal performance, ensuring security, managing costs, and performing fault detection and resolution. An integral part of the overall monitoring strategy is alerting, which provides administrators with the necessary information to proactively or reactively manage and optimize network services. However, when monitoring systems generate an excessive number of alerts, many of which may not be actionable or may not represent critical issues, the phenomenon of alert fatigue occurs. Alert fatigue refers to a situation where the volume and the speed of the continuous influx of alerts becomes so overwhelming that the network administrators become desensitized and do not respond to them. To this end, and inspired by recent trends in network automation, where human intervention tends to be minimized, we introduce an alert fatigue mitigation mechanism in monitoring focusing on cloud computing infrastructures. In particular, a composite machine learning methodology is proposed in order to select which alerts will be hidden and which ones will be presented to the administrators. Additionally, to personalize the results, the proposed approach considers the level of users’ experience along with the alert features to further optimize the accuracy of the alert filtering mechanism. The research has been conducted in a realistic environment of a leading monitoring enterprise, Netdata, which provided two datasets for testing our approach. Furthermore, the attained results of the filtering mechanism were evaluated by expert engineers of the company that verified the output of the proposed framework. Specifically, the outcomes confirm that our proposed methodology mitigates the alert fatigue problem with an accuracy that surpass 90% in most cases.}
}


@article{DBLP:journals/cn/GiarreA24,
	author = {Federico Giarr{\'{e}} and
                  Yassine Hadjadj Aoul},
	title = {Exploring the effectiveness of service migration strategies for virtual
                  network embedding},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110553},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110553},
	doi = {10.1016/J.COMNET.2024.110553},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GiarreA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing, a key component of post-5G networks, has brought virtual network embedding (VNE) to the forefront of networking research. However, existing VNE approaches are limited by their consideration of static network topologies, failing to account for the dynamic nature of real-world networks. This limitation becomes particularly problematic in the face of link failures or topology modifications, rendering these approaches ineffective. To address this, we propose a new service placement strategy that remains effective even when the network topology changes. Furthermore, we introduce several service migration strategies and thoroughly investigate their effectiveness. Our results demonstrate the adaptability of our proposed strategy, which leverages graph neural networks, in handling link failures without necessitating relearning. This adaptability underlines the potential of our approach to significantly enhance the robustness and flexibility of service migration in VNE, thereby contributing to the evolution of network slicing in post-5G networks.}
}


@article{DBLP:journals/cn/BakarMCP24,
	author = {Rana Abu Bakar and
                  Lorenzo De Marinis and
                  Filippo Cugini and
                  Francesco Paolucci},
	title = {FTG-Net-E: {A} hierarchical ensemble graph neural network for DDoS
                  attack detection},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110508},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110508},
	doi = {10.1016/J.COMNET.2024.110508},
	timestamp = {Mon, 08 Jul 2024 23:02:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BakarMCP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Denial-of-Service (DDoS) attacks are a major threat to computer networks. These attacks can be carried out by flooding a network with malicious traffic, overwhelming its resources, and/or making it unavailable to legitimate users. Existing machine learning methods for DDoS attack detection typically use statistical features of network traffic, such as packet sizes and inter-arrival times. However, these methods often fail to capture the complex relationships between different traffic flows. This paper proposes a new DDoS attack detection approach that uses Graph Neural Networks (GNN) ensemble learning. GNN ensemble learning is a type of machine learning that combines multiple GNN models to improve the detection accuracy. We evaluated our approach on the Canadian Institute for Cybersecurity Intrusion Detection Evaluation Dataset (CICIDS2018) and CICIDS2017 datasets, a benchmark dataset for DDoS attack detection. Our work provides two main contributions. First, we extend our DDoS attack detection approach using GNN ensemble learning. Second, we explore the evaluation and fine-tuning of hyperparameter metrics through ensemble learning, significantly enhancing accuracy compared to a single GNN model and achieving an average 3.2% higher F1-score. Additionally, our approach effectively reduces overfitting by incorporating regularization techniques, such as dropout and early stopping. Specifically, we use a hierarchical ensemble of GNN, where each GNN learns the relationships between traffic flows at a different granularity level. We then use bagging and boosting to combine the predictions of the individual GNN, further improving detection accuracy. Results show that our system can achieve 99.67% accuracy, with a F1-score of 99.29%, which is better than state-of-the-art methods, even using single traffic architecture.}
}


@article{DBLP:journals/cn/WangBZ24,
	author = {Junjie Wang and
                  Michael Bewong and
                  Lihong Zheng},
	title = {{SD-WAN:} Hybrid Edge Cloud Network between Multi-site {SDDC}},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110509},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110509},
	doi = {10.1016/J.COMNET.2024.110509},
	timestamp = {Mon, 08 Jul 2024 23:02:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangBZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This research presents the On-POP-Overlay architecture, a novel contribution to Multi-site Software-Defined Data Centers (SDDC). Our work contrasts this new architecture with the established On-Premises-Overlay, focusing on their deployment in SD-WAN settings. Central to our investigation is the On-POP-Overlay’s effectiveness in managing latency, jitter, and enhancing security. Our findings reveal that this architecture markedly boosts network performance, particularly in reducing latency. This attribute is especially beneficial for sectors like finance where latency sensitivity is paramount. This study optimizes network paths and reduces latency in intercontinental SD-DCs through Hybrid Edge Cloud Network Acceleration, laying the foundation for future AI-driven route selection in SD-WAN frameworks. This approach offers promising opportunities to enhance network path optimization and reinforce security measures. Such integration signifies a practical and valuable progression in the application of SD-WAN within the realm of global network infrastructures.}
}


@article{DBLP:journals/cn/WangLM24,
	author = {Qianxing Wang and
                  Wei Li and
                  Amin Mohajer},
	title = {Load-aware continuous-time optimization for multi-agent systems: toward
                  dynamic resource allocation and real-time adaptability},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110526},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110526},
	doi = {10.1016/J.COMNET.2024.110526},
	timestamp = {Mon, 08 Jul 2024 23:02:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of next-generation mobile communication networks, characterized by dynamic and evolving workloads, the efficient resource allocation becomes paramount for achieving optimal performance. This paper addresses the intricate challenges of multiagent resource management through the integration of stochastic learning automata and continuous-time optimization techniques, presenting an innovative solution for real-time adaptability and dynamic load balancing. The central aim is to achieve optimal values for local time-varying cost functions while taking into account resource constraints and the feasibility of local allocation. In the context of multi-agent resource allocation systems, where conditions vary over time, the algorithm exhibits a dynamic adaptability, continuously adjusting to the evolving optimal solutions tailored for each agent. Utilizing stochastic learning automata, the algorithm effectively addresses the time-varying load-balancing function in response to dynamic user demands. Furthermore, we introduced an scalable solution for mobility robustness optimization based on deep reinforcement learning (DRL-MRO). This method dynamically identifies network-wide optimal parameter values across the entire network to accommodate diverse mobility patterns, ensuring the effectiveness of load balancing parameters that seamlessly adapt to the dynamic configuration of the network. The overarching goal is to maintain a consistent quality of service level for each agent, thereby enhancing the overall system performance. Simulation outcomes unequivocally affirm the superior performance of the proposed load balancing approach, rooted in continuous-time optimization and stochastic learning automata, surpassing existing schemes across diverse system configurations.}
}


@article{DBLP:journals/cn/BhattacharyyaRFK24,
	author = {Abhishek Bhattacharyya and
                  Shunmugapriya Ramanathan and
                  Andrea Fumagalli and
                  Koteswararao Kondepu},
	title = {An end-to-end DPDK-integrated open-source 5G standalone Radio Access
                  Network: {A} proof of concept},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110533},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110533},
	doi = {10.1016/J.COMNET.2024.110533},
	timestamp = {Mon, 08 Jul 2024 23:02:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BhattacharyyaRFK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fifth-generation mobile networks are designed to accommodate billions of users worldwide with numerous applications and different service level requirements by providing significantly high data rates and availability with reduced latency. These requirements can be achieved by exploiting Next Generation — Radio Access Network (NG-RAN) architectures. One such famous architecture is Cloud Radio Access Network, whose Next Generation NodeB functions are physically decoupled into different entities — Radio Unit (RU), Distributed Unit (DU), and Central Unit (CU). The Central Units are connected to the 5G Core Network (CN), and all the components are likely to be virtualized and run on the commercial off-the-shelf hardware in the micro/macro data centers. However, decoupling the functionalities in the 5G RAN could not fully help in achieving higher network performance when these functions are run over in-kernel networking stack of the operating system due to overheads involved in the traditional in-kernel approaches during packet processing at the Network Interface Cards (NICs). For example, in-kernel packet processing overheads are context switches, interrupt handling, and memory copies.}
}


@article{DBLP:journals/cn/RashidP24,
	author = {Adnan Rashid and
                  Tommaso Pecorella},
	title = {Is 6LoWPAN-ND necessary? (Spoiler alert: Yes)},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110535},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110535},
	doi = {10.1016/J.COMNET.2024.110535},
	timestamp = {Mon, 08 Jul 2024 23:02:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RashidP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Power and Lossy Networks (LLNs) are based on constrained devices. Energy conservation is one of the main constraints, and the traditional IPv6 Neighbor Discovery Protocol (IPv6-NDP) was neither designed nor suitable to cope with it. This inefficiency arises from non-transitive wireless links and heavy multicast transmission, sometimes rendering it impractical in LLNs. Substantial work has been done by the Internet Engineering Task Force (IETF) to optimize the IPv6-ND protocol, known as IPv6 over Low power Wireless Personal Area Network - Neighbor Discovery Protocol (6LoWPAN-NDP). Despite these improvements, full implementation is yet to be achieved in commercial, open-source, or proprietary sectors. In this article, we debate both Neighbor Discovery Protocols (NDPs), examining various aspects. We implemented 6LoWPAN-NDP in a well-known ns3 simulator. We discuss the complexity of 6LoWPAN-NDP and see why open-source, commercial, or proprietary sectors have not widely adopted it. We present how both protocols function optimally in meshunder and non-meshunder scenarios. We present results and analysis of both NDPs control messages’ behavior. At the same time, data traffic is turned on and off, and we demonstrate the operational behavior of Link-local Unicast Address (LUA) and Global Unicast Address (GUA) in meshunder and non-meshunder scenarios. The presented implementation can be helpful in enabling large-scale simulations and evaluating scenario-specific protocol parameters, along with protocol extensions.}
}


@article{DBLP:journals/cn/SimionatoC24,
	author = {Giada Simionato and
                  Mario G. C. A. Cimino},
	title = {Swarm intelligence for hole detection and healing in wireless sensor
                  networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110538},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110538},
	doi = {10.1016/J.COMNET.2024.110538},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SimionatoC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for wireless sensor networks to monitor specific regions has prompted extensive research on sustaining coverage over time. The main threat to this goal arises from coverage holes caused by random node deployment or failures. This study proposes a swarm intelligence-based algorithm to detect and heal coverage holes. The swarm of agents relies on local and relative information, activating in response to detected holes and navigating a potential field toward the closest hole. The agents quantize their perceptions to disperse efficiently, approaching holes from different directions to accelerate healing. Based on geometric criteria, the swarm deploys at locally optimal positions along hole borders while preventing redundant deployments. Agents deployment update the potential field, guiding the rest of the swarm toward unhealed areas and ensuring dynamic detection and tracking of new holes, even near the region frontier. Experimental studies demonstrate superior coverage restoration compared to state-of-the-art solutions, showing good scalability and flexibility to different hole sizes, shapes, and multiplicity. Moreover, it exhibits high robustness to the corruption of agents’ perceptions and to their failure, while efficiently managing the battery level.}
}


@article{DBLP:journals/cn/StepanovSPBZPY24,
	author = {E. P. Stepanov and
                  Ruslan L. Smelyanskiy and
                  A. V. Plakunov and
                  A. V. Borisov and
                  Xia Zhu and
                  Jianing Pei and
                  Zhen Yao},
	title = {On fair traffic allocation and efficient utilization of network resources
                  based on {MARL}},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110540},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110540},
	doi = {10.1016/J.COMNET.2024.110540},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/StepanovSPBZPY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of traffic balancing in a data network environment is considered in relation to Network Powered by Computing (NPC) - a new generation of computational infrastructure, where computational and data transmission resources form a network. The key contribution of this work can be formulated as follows: the combination of multi-agent optimization technique with reinforcement learning and consistent hashing method ensures efficient use of bandwidth in the overlay network. This combination, named Multi-Agent Routing with Hashing (MAROH), demonstrates high performance and provides fast and fair bandwidth distribution inside an overlay network. The paper provides a detailed description of MAROH and experimental results that show that MAROH exhibits better consistent flow routing balancing than ECMP and is only slightly affected by changes in network topology.}
}


@article{DBLP:journals/cn/GuoCZQH24,
	author = {Xingyu Guo and
                  Guo Chen and
                  Xiaoning Zhan and
                  Ting Qu and
                  Zhaojiao Han},
	title = {{SECM:} Securely and efficiently connections setup using {RDMA-CM}},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110541},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110541},
	doi = {10.1016/J.COMNET.2024.110541},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GuoCZQH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote Direct Memory Access (RDMA) releases the potential of the data center as it bypasses the kernel. However, establishing and managing RDMA connections is a significant challenge due to the complex topology of RDMA networks. We present SECM, a secure and efficient RDMA Communication Management (CM) library, which can be used for distributed or resilient RDMA network connection setup. The main idea is to split the RDMA-CM connection setup phase, reuse resources between different connections, and perform multiple connection setups simultaneously in a pipelined manner. At the same time, adding message authentication codes to the CM protocol enables authentication of connection requests and prevents malicious connection requests. SECM establishes 16 connections in 1.42 times the time of 1 connection, tens or hundreds of times faster than verbs. SECM is compatible with existing commercial RNICs and provides services in a user-state-driven manner, with low CPU overhead, low memory usage, and no impact on the RDMA data level.}
}


@article{DBLP:journals/cn/WuL24,
	author = {Caihong Wu and
                  Jihua Liu},
	title = {Smart contract assisted secure aggregation scheme for model update
                  in federated learning},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110542},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110542},
	doi = {10.1016/J.COMNET.2024.110542},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WuL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a cutting-edge machine learning architecture that effectively meets the data and model training requirements of numerous businesses, while ensuring privacy protection, data security, and adherence to legal constraints. However, recent studies have demonstrated that attackers can deduce users’ personal data from their shared model parameters, thus underscoring a substantial security risk for federated learning. Despite considerable efforts by academics to address this issue, current approaches suffer from drawbacks such as high communication and processing costs, insufficient robustness, and heavy reliance on trusted third parties. In this paper, we present the smart contract assisted secure aggregation scheme (SCSA) to overcome the aforementioned problems. First, we creatively offer a three-layer secure aggregation architecture that is particularly suitable for scenarios involving multiple sensors used in model training. Secondly, to ensure the credibility of the identities between participating entities in the model parameters aggregation process of FL, a novel certificateless authentication mechanism relying on certificateless encryption is provided. Moreover, for the sake of enhancing the security of the aggregation process in FL, we employ a combination of smart contracts and secret sharing techniques to distribute security masks to users in a decentralized and highly secure manner. It also combines with secure communication to create a double fault tolerance mechanism that significantly boosts the whole system’s robustness. The effectiveness of our approach in achieving secure aggregation is substantiated through theoretical analysis and simulated trials, while simultaneously ensuring strong security and robustness.}
}


@article{DBLP:journals/cn/Haddad24,
	author = {Zaher Haddad},
	title = {Enhancing privacy and security in 5G networks with an anonymous handover
                  protocol based on Blockchain and Zero Knowledge Proof},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110544},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110544},
	doi = {10.1016/J.COMNET.2024.110544},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Haddad24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The latest advancement in cellular technology, 5G, has the ability to facilitate various applications such as Device-to-Device (D2D) communication, vehicular communications, and the Internet of Everything (IoE). However, the technology is faced with challenges related to user privacy, resource management, handover authentication, and security of the air interface and internet channels. Moreover, the use of specialized base stations such as wireless Access Points (AP) and Road-Side Units (RSU) in vehicular communications may be owned by subscribers outside formal network operators. The conventional handover authentication schemes have a lengthy latency period that conflicts with the 5G quality of service. To tackle these challenges, this paper presents an anonymous scheme that utilizes Blockchain and Zero Knowledge Proof (ZKP) to ensure efficient privacy in the handover protocol of 5G Networks. The proposed scheme has been thoroughly evaluated for security and privacy and has shown to be resistant to identity catching, location area tracing, and replay attacks while achieving Forward/Backward secrecy. Furthermore, the performance evaluation of the proposed scheme indicates that it is efficient and speedy as it involves a limited number of messages, low computation, and does not necessitate the involvement of the Home Network (HN) in the authentication or handover protocols. A diligently crafted BAHO authentication protocol, following cryptographic principles, guarantees robust security against the Real-or-Random model. This results in a seamless and secure transition for User Equipment, upholding confidentiality, integrity, and authenticity during handovers between Access Points.}
}


@article{DBLP:journals/cn/KumarAS24,
	author = {Vipin Kumar and
                  Rifaqat Ali and
                  Pawan Kumar Sharma},
	title = {IoEPM+: {A} secured and lightweight 6G-enabled pollution monitoring
                  authentication framework using IoT and blockchain technology},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110554},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110554},
	doi = {10.1016/J.COMNET.2024.110554},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KumarAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, the health of individuals, particularly those residing in industrial areas, faces escalating risks from pollution in air, soil, water, and waste residue. Understanding and monitoring such environmental dynamics is both challenging and essential for safeguarding public health. Therefore, the implementation of an Internet of Things (IoT) enabled Environmental Pollution Monitoring (EPM) framework becomes imperative. This framework involves strategically positioning IoT devices, which can collect data through mobile devices such as Unmanned Aerial Vehicles (UAVs) or Drones. However, the communication channels among these IoT devices are publicly accessible, which exposes them to potential security threats and device capture attacks. Additionally, continuous data collection raises concerns regarding data storage and latency, as well as communication and computational costs, along with energy consumption. Overcoming these challenges necessitates the development of a lightweight authentication and key-agreement framework tailored for resource-constrained drones, leveraging blockchain-enabled cloud servers. To address this, we propose IoEPM+, a secured and lightweight framework for environmental pollution monitoring, leveraging IoT and computing devices within a 6G-enabled network. The security of the proposed framework is validated through both informal security analysis and formal analysis using the “Real-or-Random (RoR) model”, supplemented by simulation techniques such as the “Scyther tool”. Additionally, a performance analysis comparison, encompassing security features, computation and communication costs, energy consumption, and execution time against other related frameworks, highlights the superiority of our approach.}
}


@article{DBLP:journals/cn/PangWGHH24,
	author = {Shanchen Pang and
                  Teng Wang and
                  Haiyuan Gui and
                  Xiao He and
                  Lili Hou},
	title = {An intelligent task offloading method based on multi-agent deep reinforcement
                  learning in ultra-dense heterogeneous network with mobile edge computing},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110555},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110555},
	doi = {10.1016/J.COMNET.2024.110555},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PangWGHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of IoT technology, various computation-intensive and latency-sensitive tasks have emerged in large numbers, which impose higher requirements on the processing efficiency of the tasks. In this paper, by fusing mobile edge computing (MEC) and ultra-dense heterogeneous network (UD-HetNet) technologies, we design the scenario of UD-HetNet with MEC to improve the efficiency of computation and communication during task processing. However, how to design a effective task offloading and resource management strategy is still a major challenge in this scenario. Therefore, in this paper, we propose a distributed task offloading and wireless resource management framework that optimizes task offloading, local computation frequency scaling, subchannel allocation, and transmit power regulation strategies to reduce system overhead (weighted sum of delay and energy consumption of task processing) effectively. First, we design a task offloading for multi-base station (BS) collaboration base on priority algorithm and optimize the local computation frequency using convex optimization theory. Following by, we introduce a multi-agent deep deterministic policy gradient (MADDPG) technology to optimize subchannel allocation and transmit power regulation during task offloading to accommodate the dynamic and variable nature of the channel. Finally, user equipment (UE)-edge server (ES), UE-subchannel, and subchannel-power matching are achieved. Simulation results show that our algorithm has significant advantages in balancing the ES load, improving channel utilization, and reducing system overhead.}
}


@article{DBLP:journals/cn/CaoXYXSHC24,
	author = {Jie Cao and
                  Yuwei Xu and
                  Enze Yu and
                  Qiao Xiang and
                  Kehui Song and
                  Liang He and
                  Guang Cheng},
	title = {GateKeeper: An UltraLite malicious traffic identification method with
                  dual-aspect optimization strategies on IoT gateways},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110556},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110556},
	doi = {10.1016/J.COMNET.2024.110556},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CaoXYXSHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) landscape is booming, and a wide range of IoT endpoints has been integrated into various aspects of life, opening opportunities for malicious attacks that compromise IoT security. Consequently, effective malicious traffic identification methods play a crucial role in ensuring IoT security. However, the current lightweight methods face two main challenges. Firstly, they often generate feature sets that are time-consuming to construct and less adaptable to various scenarios. Secondly, classification models built upon inefficient neural networks incur a significant computational overhead. To address these drawbacks, in this work, we propose GateKeeper, which is an UltraLite method for malicious traffic identification on the IoT gateway. We first propose a base model as the cornerstone and target for subsequent optimization. Then we propose dual-aspect optimization strategies for reducing the input dimension and simplifying the model structure, i.e., Key Bytes Selection (KBS) and Attention Module Simplification (AMS) strategy. Finally, we obtain the optimized UltraLite model-GateKeeper, specifically tailored for integration into IoT gateway deployments, where high-speed real-time identification is paramount. Our extensive experiments demonstrate that GateKeeper performs remarkably in identifying malicious IoT traffic, achieving over 97% accuracy in all three IoT malicious traffic classification benchmark tasks, outperforming six state-of-the-art methods. Besides, GateKeeper’s parameters and FLOPs are 65% smaller than those of existing methods. Moreover, in the case of an IoT gateway platform, GateKeeper demonstrates remarkably low time overhead and hardware resource utilization, surpassing state-of-the-art methods.}
}


@article{DBLP:journals/cn/BusaccaGPP24,
	author = {Fabio Busacca and
                  Laura Galluccio and
                  Sergio Palazzo and
                  Andrea Panebianco},
	title = {A comparative analysis of predictive channel models for real shallow
                  water environments},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110557},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110557},
	doi = {10.1016/J.COMNET.2024.110557},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BusaccaGPP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unlike traditional terrestrial scenarios, communication channels in underwater environments face severe limitations in bandwidth and experience long propagation delay. In order to address these issues, reliable techniques capable to dynamically adapt transmission parameters to time-varying channel conditions are necessary. Actually, their effectiveness primarily relies on an accurate characterization of the underwater communication channels, which is often obtained through predictive models. In this paper, we compare the performance of different types of SNR-based predictive models (i.e., Markov models, Hidden Markov models) in terms of balance between accuracy and complexity. We also provide a Kalman filter-based prediction of SNR values and compare this prediction with the performance achieved with the Markov models above mentioned. The models we have considered to carry out the comparison analysis have been developed based on real shallow water traces taken over the Tyrrhenian Sea, Italy.}
}


@article{DBLP:journals/cn/ShenLYCZ24,
	author = {Xiajiong Shen and
                  Xiaoran Li and
                  Hongjian Yin and
                  Chaoyang Cao and
                  Lei Zhang},
	title = {Lattice-based multi-authority ciphertext-policy attribute-based searchable
                  encryption with attribute revocation for cloud storage},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110559},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110559},
	doi = {10.1016/J.COMNET.2024.110559},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShenLYCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-authority attribute-based searchable encryption (MABSE) is an flexible and efficient way to securely share and search encrypted data. Compared with single-authority systems, MABSE has more complex access control policies and key management mechanism. However, most existing MABSE schemes rely on traditional number-theoretic assumptions, which maybe vulnerable to attack in the era of quantum-computers. Besides, the effective revocation of user attributes is also crucial in searchable encryption. To overcome these challenges, this paper proposes a new multi-authority ciphertext-policy attribute-based searchable encryption scheme for securely sharing encrypted data in the cloud. By calling Shamir’s threshold secret-sharing technology twice, we achieve co-management of the master key by attribute authorities and interactive generation of user private keys. Furthermore, the KUNodes algorithm is employed for attribute revocation, offering a mechanism to update private keys for non-revoked users. Compared to other schemes, MCP-ABSE-AR introduces multiple attribute authorities responsible for managing user attributes collectively. Additionally, it provides functionalities for keyword searching and attribute revocation. Finally, the proposed scheme is proved to be semantically secure under the decision learning with errors problem in the standard model.}
}


@article{DBLP:journals/cn/ChenCZ24,
	author = {Rui Chen and
                  Xiaoyu Chen and
                  Jing Zhao},
	title = {Private and utility enhanced intrusion detection based on attack behavior
                  analysis with local differential privacy on IoV},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110560},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110560},
	doi = {10.1016/J.COMNET.2024.110560},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, with the escalating security demands of the Internet of Vehicles (IoV), concerns over safety have intensified. To prevent security incidents and privacy breaches, IoV must address various threats promptly and effectively. The use of deep learning methods for intrusion detection in IoV has garnered widespread attention. Compared to traditional security defenses, deep learning can learn from heterogeneous data sources, enhancing the accuracy of detecting various security threats. However, current research based on deep learning primarily focuses on constructing intrusion detection models and overlooks the analysis and processing of extensive behavioral data. Moreover, model training requires access to and transmission of sensitive data, which may lead to high communication costs and potential privacy leaks. To ensure the network security of IoV, we propose FDL-IDM, an innovative behavior-analysis-based intrusion detection model leveraging differential privacy within federated learning. It extracts driving behavior spatiotemporally and employs noise perturbation pre-aggregation, reducing communication costs and ensuring privacy without compromising accuracy. Specifically, we process data from both temporal and spatial dimensions. Data are grouped based on sender identity and then sliced according to the time sequence to create state matrices that vary over time, enhancing the performance and robustness of the detection model. Next, we incorporate an attention mechanism to merge outputs from each time step and hidden layer, strengthening the time series model and reducing information loss. Lastly, in federated learning, we add noise perturbation to the uploaded parameters, reducing the risk of privacy breaches. Additionally, we employ a random scheduling strategy during training to select clients and assign an adjusted learning rate that decreases with iterations, enhancing the stability of model training. Therefore, FDL-IDM helps prevent security attacks and protect IoV privacy. Through experiments and privacy analysis, as well as tests on vehicle-level devices, FDL-IDM achieved F1-scores of 0.9751, 0.9851, and 0.9789 on three public datasets, demonstrating not only high accuracy but also robust privacy protection capabilities.}
}


@article{DBLP:journals/cn/DongQSLCX24,
	author = {Yongpeng Dong and
                  Shiyou Qian and
                  Wanghua Shi and
                  Junshen Li and
                  Jian Cao and
                  Guangtao Xue},
	title = {{OEM:} An operation-aware event matching algorithm for content-based
                  Pub/Sub systems},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110561},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110561},
	doi = {10.1016/J.COMNET.2024.110561},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DongQSLCX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the continuous expansion of the user base and the heightened demands for quality of service (QoS), real-time data distribution has encountered significant challenges in event matching within large-scale content-based publish/subscribe (CPS) systems. Many efficient algorithms have been proposed to enhance the matching performance. However, most existing algorithms scarcely consider operation characteristics, which often leads to performance degradation due to a multitude of repetitive operations, such as comparisons, additions, and assignments. In this paper, we propose an Operation-aware Event Matching algorithm called OEM, which takes into account the efficiency of CPU operations, SIMD instructions, and multi-thread architecture. First, we introduce a bitset-based subscription cache mechanism (SCM), which enables the efficient execution of bitwise OR operations during the matching process. Furthermore, we propose two optimizations, namely an optimal skewness-aware cell grouping (OSCG) strategy and an attribute-based clustering mechanism (ACM), to enhance the efficiency of OEM. These optimizations effectively address the challenges posed by data skewness, low cache efficiency, and high dimensionality. In addition, we establish a performance analysis model that characterizes the trade-off between memory consumption and performance improvement. We conducted extensive experiments to evaluate the performance of OEM. Compared to six state-of-the-art algorithms, namely REIN, Ada-REIN, TAMA, OpIndex, PEM and fgPEM, OEM achieves an average reduction in matching time by\n40\n.\n4\n×\n,\n38\n.\n5\n×\n,\n12\n.\n0\n×\n,\n33\n.\n1\n×\n,\n27\n.\n8\n×\n, and\n25\n.\n5\n×\n, respectively.}
}


@article{DBLP:journals/cn/XingLLHWMZ24,
	author = {Ling Xing and
                  Bing Li and
                  Lulu Liu and
                  Yuanhao Huang and
                  Honghai Wu and
                  Huahong Ma and
                  Xiaohui Zhang},
	title = {Trajectory privacy protection method based on sensitive semantic location
                  replacement},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110562},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110562},
	doi = {10.1016/J.COMNET.2024.110562},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XingLLHWMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Location-Based Services (LBSs) proliferate within the Social Internet of Vehicles (SIoVs), the collection and utilization of vehicle trajectories, which are rich in spatio-temporal and semantic informations, have intensified. Publishing these trajectories without adequate privacy safeguards significantly threatens user semantic trajectory privacy. Current methodologies for protecting semantic trajectory privacy do not adequately consider the similarity in query probability between sensitive and alternative semantic locations, thus undermining the efficacy of privacy protection. To address this critical gap, we introduces a novel trajectory privacy protection method based on sensitive semantic location replacement (SSLR), which strategically replaces sensitive semantic locations. This method consists of three key steps: first, semantically annotating trajectory sampling locations to identify sensitive semantic points; second, employing a unique double semicircular area to define replacement regions and selecting replacement Points of Interests (PoIs) that align in semantic attributes and query probabilities; third, reconstructing and publishing the modified trajectories. Our simulation results confirm that SSLR advances the state of the art by delivering superior performance in average recognition rate, geographic similarity, and semantic similarity compared to existing methods. Crucially, by incorporating considerations of query probability similarities, this paper not only enhances the effect of trajectory privacy protection but also preserves its utility. This dual enhancement represents a groundbreaking approach to trajectory privacy, with significant implications for advancing privacy strategies in LBSs within SIoVs.}
}


@article{DBLP:journals/cn/LiZZLLZLX24,
	author = {Zhaoxuan Li and
                  Ziming Zhao and
                  Rui Zhang and
                  Haoyang Lu and
                  Wenhao Li and
                  Fan Zhang and
                  Siqi Lu and
                  Rui Xue},
	title = {metaNet: Interpretable unknown mobile malware identification with
                  a novel meta-features mining algorithm},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110563},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110563},
	doi = {10.1016/J.COMNET.2024.110563},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiZZLLZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The continuous emergence of malware has threatened to the Android platform and user privacy. With the evolution of the Android system and malware, it is challenging to design a method that can accurately identify the categories of sophisticated malware, including known and unknown families, as well as their obfuscated variants, given that they may be newly emerging and lack available detection knowledge. Although some methods try to use anomaly detection and zero-shot technology to identify unseen applications, they are limited to binary classification or lack the robustness, stability, universality, and interpretability in multi-class identification. To this end, we first propose a generic meta-features mining algorithm, which can discover the potential relationships between samples belonging to the same category. Then we present metaNet, a novel method leveraging meta-features to identify sophisticated Android malware. Specifically, metaNet is mainly powered by four components: (i) mExtractor is a feature collector to obtain the static and dynamic features. (ii) mProcessor is taking unique meta-features of each category from extracted features. (iii) mLearner is a machine learning suite that leverages features and meta-features to design and train a classifier called HSU-Net. (iv) mEnforcer is a flexible deployer that identifies categories of malware families in the real world. We implement a prototype of metaNet with 15K lines of Python code and compare it with state-of-the-art (SOTA) methods. The results show that it can not only achieve superior performance in terms of known families (99.52% of accuracy) and unknown families (99.31% of accuracy trained with 80% known families) for binary classification, but also perform well in multi-class identification, i.e., 99.05% and 93.45% of accuracy for known and unknown families, respectively. Furthermore, we deploy and evaluate metaNet in the real world. It can identify applications over an acceptable time and memory overheads, i.e., average of 11.8 s and 56 MB per sample with a size of 8 MB. Also, the few-shot detection and feature perturbation experiments reflect its robustness and stability benefiting from meta-features. Finally, we collect the traffic of 112 decentralized applications (DApps) belonging to 16 categories, such as finance and health, and evaluated metaNet in DApp identification. The results illustrate its applicability across various tasks. That is, it can accurately classify 94.6% and 81.36% of DApp flows in all-known and 80%-known DApp scenarios, respectively, outperforming the SOTA methods.}
}


@article{DBLP:journals/cn/XieYZN24,
	author = {Mande Xie and
                  Jiefeng Ye and
                  Guoping Zhang and
                  Xueping Ni},
	title = {Deep Reinforcement Learning-based computation offloading and distributed
                  edge service caching for Mobile Edge Computing},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110564},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110564},
	doi = {10.1016/J.COMNET.2024.110564},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XieYZN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Mobile Edge Computing (MEC), numerous mobile terminals offload their computational tasks to edge networks to support applications that are highly sensitive to latency or energy consumption. However, the computation and storage resources of edge servers are also limited. By caching computation results to assist in computation offloading, the overhead caused by redundant task requests is effectively reduced. To minimize the long-term average cost for all users in the region, we propose an offloading algorithm based on Deep Reinforcement Learning (DRL). This algorithm integrates the Long Short-Term Memory (LSTM) network with the Deep Deterministic Policy Gradient (DDPG) framework. It leverages long-term environmental state characteristics and memory information to enable rational offloading decisions for mobile terminals. To maximize the collaborative advantages of distributed edge regions, we have developed a Distributed Edge Caching Algorithm (DECA) and implemented an efficient cache replacement strategy to improve the overall cache hit rate for target tasks. Finally, we integrate the LSTM-DDPG algorithm with DECA to enable efficient offloading decisions and caching strategies across the entire system. Simulation results indicate that our proposed method reduces the average system overhead for users by 7.6% to 11.3%, compared to the DQN (Deep Q Network) algorithm with equivalent caching support.}
}


@article{DBLP:journals/cn/AdanzaGAFDMV24,
	author = {Daniel Adanza and
                  Lluis Gifre and
                  Pol Alemany and
                  Juan Pedro Fern{\'{a}}ndez{-}Palacios and
                  {\'{O}}scar Gonz{\'{a}}lez de Dios and
                  Raul Mu{\~{n}}oz and
                  Ricard Vilalta},
	title = {Enabling traffic forecasting with cloud-native {SDN} controller in
                  transport networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110565},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110565},
	doi = {10.1016/J.COMNET.2024.110565},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AdanzaGAFDMV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network bandwidth is a scarce resource that network operators monitor to cope with future traffic demands and plan more transceiver and fibre deployments. The inclusion of Machine Learning permits the usage of traffic forecasting methods to predict future link usage. Typically, traffic analysis is performed offline due to the high computational load and difficulty of obtaining real-time data directly from the underlying network devices. To overcome these limitations, this paper presents and evaluates an architecture for SDN-controlled packetoptical transport networks to allow real-time traffic monitoring in the transport SDN controller. The presented SDN controller is based on a micro-service-based architecture, which facilitates the ease of deployment of the proposed solution. Four forecasting methods are proposed and evaluated against two topologies to select the most precise and the fastest among them.The algorithm random forest seems to be the most accurate forecasting future link usage with 79.98\u202f% and 95.88\u202f% accuracy and a reasonable fast speed when implemented it into two different topologies}
}


@article{DBLP:journals/cn/PanCZWW24,
	author = {Chengsheng Pan and
                  Xiaosong Cui and
                  Chen Zhao and
                  Yingzhi Wang and
                  Yuyue Wang},
	title = {An adaptive network congestion control strategy based on the change
                  trend of average queue length},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110566},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110566},
	doi = {10.1016/J.COMNET.2024.110566},
	timestamp = {Mon, 15 Jul 2024 16:11:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PanCZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth in the amount of data transmissions over Internet of Things (IoT) networks, a large amount of bursty traffic is more prone to cause serious network congestions, which would degrade network performance by reducing throughput, increasing end-to-end delay and packet loss, etc. Random early detection (RED) has been the most widely known method for network congestion control. However, RED and its most variant algorithms control network congestion only based on the queue length. More importantly, these algorithms ignore the direction and speed of queue changes, thus limiting their capability to adapt to fluctuations in network traffic. In this paper, a new algorithm called queue change trend based adaptive RED (QCT-ARED) is proposed. First, a novel average queue length evaluation model is introduced. This model can adjust the queue weight values according to the queue state adaptively. Besides, the proposed algorithm brings in a new parameter representing the second-order change rate of the average queue length. It helps to update the intermediate threshold according to the information on the average queue length and its first-order change rate. Finally, the QCT-ARED algorithm adopts a combination of cubic function and linear function for the dropping function settings as a way to predict and avoid early congestion. The method is compared with some existing algorithms, the results show that the proposed algorithm has a better system performance in terms of queue length, delay jitter, and throughput.}
}


@article{DBLP:journals/cn/ChangaziBYMAAA24,
	author = {Sabir Ali Changazi and
                  Asim Dilawar Bakhshi and
                  Muhammad Yousaf and
                  Syed Muhammad Mohsin and
                  Syed Muhammad Abrar Akber and
                  Mohammed B. Abazeed and
                  Mohammed Ali},
	title = {Optimization of network topology robustness in IoTs: {A} systematic
                  review},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110568},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110568},
	doi = {10.1016/J.COMNET.2024.110568},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChangaziBYMAAA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) has emerged as one of the most promising technologies of the modern era, enabling connectivity between devices and systems for efficient data exchange and enhanced user experience. However, the optimization of IoT networks remains a significant challenge due to the complexity of the interconnected devices and systems. To overcome this challenge, various techniques to optimize networks and algorithms for enhancing the robustness of the network topology have been proposed. Artificial intelligence is also used to achieve the robustness of network topologies in IoT through AI-driven predictive analytic. This article reviews the state-of-the-art optimization techniques and algorithms for topology robustness in IoT networks, highlighting the open issues and challenges in this field. The article begins by providing an overview of IoT network optimization and the need for topology robustness. It then discusses various optimization algorithms proposed in recent years, such as particle swarm optimization, memetic algorithms and genetic algorithms, among others. The article also highlights the key challenges in optimizing IoT networks, such as scalability, resource constraints, and security issues. Moreover, the article explores the open issues in this field, such as the lack of standardization and interoperability between devices and systems, which hinder the development of optimal solutions. Finally, the article identifies specific research directions in the area of topology robustness, emphasizing the vital factors to consider enhanced robustness for IoT networks. Additionally, the article discusses the enhancements for intelligent solutions, like smart cities and modern healthcare systems, which can further improve the optimization of IoT networks.}
}


@article{DBLP:journals/cn/ShaabanzadehS24,
	author = {Seyedeh Soheila Shaabanzadeh and
                  Juan S{\'{a}}nchez{-}Gonz{\'{a}}lez},
	title = {A spatio-temporal prediction methodology based on deep learning and
                  real Wi-Fi measurements},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110569},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110569},
	doi = {10.1016/J.COMNET.2024.110569},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShaabanzadehS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Wi-Fi technologies in recent years has caused a significant increase in the traffic usage. Hence, knowledge obtained from Wi-Fi network measurements can be helpful for a more efficient network management. In this paper, we propose a methodology to predict future values of some specific network metrics (e.g. traffic load, transmission failures, etc.). These predictions may be useful for improving the network performance. After data collection and preprocessing, the correlation between each target access point (AP) and its neighbouring APs is estimated. According to these correlations, either an only-temporal or a spatio-temporal based prediction is done. To evaluate the proposed methodology, real measurements are collected from 100 APs deployed in different university buildings for 3 months. Deep Learning (DL) methods (i.e. Convolutional Neural Network (CNN), Simple Recurrent Neural Network (SRNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), Transformer) are evaluated and compared for both temporal and spatio-temporal based predictions. Moreover, a hybrid prediction methodology is proposed using a spatial processing based on CNN and a temporal prediction based on RNN. The proposed hybrid methodology provides an improvement in the prediction accuracy at expenses of a slight increase in the Training Computational Time (TCT) and negligible in Prediction Computational Time (PCT).}
}


@article{DBLP:journals/cn/GarciaPenasRM24,
	author = {Rodolfo Garc{\'{\i}}a{-}Pe{\~{n}}as and
                  Rafael A. Rodr{\'{\i}}guez{-}G{\'{o}}mez and
                  Gabriel Maci{\'{a}}{-}Fern{\'{a}}ndez},
	title = {HoDiNT: Distributed architecture for collection and analysis of Internet
                  Background Radiation},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110570},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110570},
	doi = {10.1016/J.COMNET.2024.110570},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GarciaPenasRM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attacks on the Internet are constant, with different typologies and processes. The initial stages usually involve an enumeration of targets and available services, generating what is known as Internet Background Radiation (IBR). Capturing and analysing this traffic has proven to be crucial for the early identification and detection of attacks.}
}


@article{DBLP:journals/cn/VieiraMBNPMOD24,
	author = {Juan Lucas Vieira and
                  Evandro L. C. Macedo and
                  Anselmo Luiz {\'{E}}den Battisti and
                  Julia Noce and
                  Paulo F. Pires and
                  D{\'{e}}bora C. Muchaluat{-}Saade and
                  Ana Cristina Bernardo de Oliveira and
                  Fl{\'{a}}via Coimbra Delicato},
	title = {Mobility-aware {SFC} migration in dynamic 5G-Edge networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110571},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110571},
	doi = {10.1016/J.COMNET.2024.110571},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/VieiraMBNPMOD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fifth-generation mobile networks bring an evolution in the development of novel classes of applications, increasing the demand for performance and flexibility to support new classes of services. Edge Computing and Network Function Virtualization are two key technologies that bring data processing closer to the user and allow the replacement of traditional hardware-based network functions with virtualized counterparts, which reduces costs and permits the provision of low-latency services. In this scenario, services are often implemented as a Service Function Chain (SFC), consisting of a set of ordered Virtual Network Functions (VNFs) that provide the required service functionality. During service execution, users can move from one location to another, which impacts service performance. An approach to reduce the impact of user mobility is to migrate services as users move to different places. However, the SFC migration problem imposes several challenges that require complex decision-making. VNFs should be migrated to nodes that have the necessary resources to execute the VNF instances. Also, the migration should be fast and effective, using as few resources as possible to reduce migration costs. In this paper, we propose a proactive and iterative SFC migration strategy to address the performance impact caused by user mobility. Our proposed solution anticipates the movement of users and estimates the impact of their mobility on service delay, proactively triggering service migrations when needed. The proposed algorithm decides which VNFs of the SFC should be migrated and which compute nodes will host the migrating VNFs, fulfilling service, resource, and migration constraints while maintaining a low migration overhead. Our algorithm also considers the dynamic nature of the edge environment in which other SFC placement and migrations might be occurring when an SFC is being migrated, which also introduces new challenges related to the availability and concurrency of resources required for migration.}
}


@article{DBLP:journals/cn/ZhengLKYZZLZY24,
	author = {Weichang Zheng and
                  Jinke Li and
                  Ming Ke and
                  Mingcong Yang and
                  Yu Zheng and
                  Chenxiao Zhang and
                  Qiang Liu and
                  Yongbing Zhang and
                  Kun Yang},
	title = {Optimizing Spatial Channel Networks (SCNs) in Hierarchical Optical
                  Cross-Connect {(HOXC)} architectures: Impact of wavelength switching
                  granularity on performance},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110572},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110572},
	doi = {10.1016/J.COMNET.2024.110572},
	timestamp = {Mon, 08 Jul 2024 23:02:32 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhengLKYZZLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for network bandwidth highlights the critical need to enhance optical transmission systems. Utilizing the entire C-band for a single optical channel(OCh) eliminates the requirement for wavelength switching, prompting the emergence of spatial channel networks (SCNs). SCNs transform the optical layer into hierarchical spatial division multiplexing (SDM) and wavelength division multiplexing (WDM) layers through the implementation of hierarchical optical cross-connects (HOXCs). A HOXC comprises a spatial channel cross-connect (SXC) for spatial bypass switching and multiple wavelength cross-connects (WXCs) for wavelength channel switching, ensuring efficient optical transmission. Therefore, the design of the HOXC plays a pivotal role in determining both the device cost and network performance in SCNs. This study investigates the impact of wavelength switching granularity on core-selective switch (CSS)-based HOXC architectures. We propose an adaptive dynamic Routing, Spatial Channel, and Spectrum Assignment (RSCSA) algorithm to solve routing and resource allocation problems in SCNs. By examining SCN designs with varying wavelength switching granularities, we assess the overall network deployment cost, considering both network throughput and spectrum resource utilization. Our findings indicate that adjusting the granularity of wavelength switching in SCNs can significantly affect device costs and performance, highlighting the importance of identifying an optimal SCN design that strikes a balance between these factors. These insights offer valuable guidance for the practical planning and management of future SCN deployments.}
}


@article{DBLP:journals/cn/ZazhiginaYLK24,
	author = {Elizaveta Zazhigina and
                  Ruslan Yusupov and
                  Andrey I. Lyakhov and
                  Evgeny M. Khorov},
	title = {Analytical study of Restricted Access Window with short slots for
                  fast and reliable data delivery from energy-harvesting sensors},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110573},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110573},
	doi = {10.1016/J.COMNET.2024.110573},
	timestamp = {Wed, 10 Jul 2024 11:01:50 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZazhiginaYLK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To address the peculiarities of various Internet of Things applications and improve the performance of Wi-Fi-based sensor stations, a recent Wi-Fi HaLow standard introduces a mechanism called Periodic Restricted Access Window (PRAW). PRAW allows mapping groups of stations to periodic time intervals during which only these stations can transmit while the others cannot. This approach reduces contention and saves energy. However, the standard does not provide any recommendations on how to select the PRAW parameters according to scenario-specific requirements, leaving room for the research. A typical IoT scenario considers low-intensity traffic and requires fast and reliable data delivery from energy-harvesting devices while using the channel efficiently: there shall be still time for transmissions of other stations in the network. This paper develops an analytical model of PRAW that evaluates the probability of delivering data in time from every sensor station that harvests energy, accumulates it in small capacitors, and, thus, may not have enough energy for multiple retries. The model allows selecting such parameters of PRAW that minimize the channel resource consumption of a Wi-Fi HaLow network, provided that the delay and energy requirements are satisfied.}
}


@article{DBLP:journals/cn/DasBN24,
	author = {Manoj Das and
                  Madhurima Buragohain and
                  Sukumar Nandi},
	title = {Distributed neighbor discovery with mobility adaptive probing for
                  beyond 5G {D2D} communications},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110574},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110574},
	doi = {10.1016/J.COMNET.2024.110574},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DasBN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Representing a paradigm shift in cellular communication, Device-to-Device (D2D) Communication is an emerging technology that enables direct communication between two User Equipments(UEs) without going through the Base Station. It enables decentralization of cellular networks, enhances network capacity, increases spectral efficiency, and offloads network traffic. In order to proceed with direct D2D communication, the devices must have knowledge of the proximal devices and should work in all coverage scenarios such as in-coverage, partial-coverage and out-of-coverage. Most of the existing works on Device Discovery are based on 3GPP Model A and Model B. However, these schemes suffer from overhead of discovery message exchanges with the increase in the number of devices in a mobile environment. To address these issues, we propose a distributed and adaptive device discovery mechanism that can discover the proximal UEs in all possible scenarios. To achieve this, we propose a Hybrid Discovery Model ‘H’, which uses the concept of ‘Who are there?’. In Model ‘H’, the discoverer sends a single request message to discover all proximal devices. The responses of the proximal UEs are staggered based on the estimated number of neighbors to avoid collisions. In addition to that, we adapt the discovery mechanism to the relative mobility of the neighborhood using our proposed Adaptive Probing Algorithm named ‘ReMAP’. The relative mobility is determined based on the current and previous neighbor lists of each UE. The key advantage of our proposed mechanism is that we do not require the knowledge of position and mobility pattern of the UEs. We implement and evaluate our proposed work in D2D Module of the NS3 simulator. From the simulation results, we observe that our proposed model shows better performance in terms of the number of UEs discovered and reduced number of discovery message exchanges compared to the state-of-the-art. We also compare the performance of ReMAP with a constant probing mechanism and show the improvement in terms of adaptability to the mobility of the neighborhood.}
}


@article{DBLP:journals/cn/RenXLDRZXRLW24,
	author = {Wenhao Ren and
                  Zichuan Xu and
                  Weifa Liang and
                  Haipeng Dai and
                  Omer F. Rana and
                  Pan Zhou and
                  Qiufen Xia and
                  Haozhe Ren and
                  Mingchu Li and
                  Guowei Wu},
	title = {Learning-driven service caching in {MEC} networks with bursty data
                  traffic and uncertain delays},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110575},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110575},
	doi = {10.1016/J.COMNET.2024.110575},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RenXLDRZXRLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) provides extremely low-latency services for mobile users, by attaching computing resources to 5G base stations in an MEC network. Network service providers can cache their services from remote data centers to base stations to serve mobile users within their proximity, thereby reducing service latencies. However, mobile users of network services usually have bursty requests that require immediate processing in the MEC network. The data traffic of such requests is bursty, since mobile users have various hidden features including locations, group tags, and mobility patterns. Furthermore, such bursty data traffic causes uncertain congestion at base stations and thus leads to uncertain processing delays. Considering the limited resources of base stations, network services may not be able to be placed into base stations permanently to handle the bursty data traffic. As such, network services need to be dynamically cached in the MEC network to fully address the bursty data traffic and uncertain processing delays.}
}


@article{DBLP:journals/cn/ChenSKBD24,
	author = {Zhiyan Chen and
                  Murat Simsek and
                  Burak Kantarci and
                  Mehran Bagheri and
                  Petar Djukic},
	title = {Machine learning-enabled hybrid intrusion detection system with host
                  data transformation and an advanced two-stage classifier},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110576},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110576},
	doi = {10.1016/J.COMNET.2024.110576},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenSKBD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Intrusion Detection Systems (NIDS) have been extensively investigated by monitoring real network traffic and analyzing suspicious activities. However, there are limitations in detecting specific types of attacks with NIDS, such as Advanced Persistent Threats (APT). Additionally, NIDS is restricted in observing complete traffic information due to encrypted traffic or a lack of authority. To address these limitations, a Host-based Intrusion Detection system (HIDS) evaluates resources in the host, including logs, files, and folders, to identify APT attacks that routinely inject malicious files into victimized nodes. In this study, a hybrid network intrusion detection system that combines NIDS and HIDS is proposed to improve intrusion detection performance. The host data undergoes a Language Processing (NLP)-based Bidirectional Encoder Representations from Transformers (BERT) model from textual representation to a numerical one in order to process host data in a similar way to the network flow data through machine learning models. The feature flattening technique is applied to flatten two-dimensional host-based features that is provided by BERT into one-dimensional vectors so that host-based and network flow-based features can be processed by advanced Machine Learning (ML) models. In order to enhance HIDS effectiveness, a two-stage collaborative classifier is utilized, which applies two tiers of machine learning algorithms, binary and multi-class classifiers, to detect network intrusions. Once a binary classifier is used to detect benign samples to reduce the complexity of the original problem, the attack data are classified by a multi-class supervised learner to identify attack types. Hence, the overall performance of the two-stage collaborative model outperforms the baseline classifier, XGBoost. The proposed method is shown to generalize across two well-known datasets, CICIDS 2018 and NDSec-1. The performance of XGBoost, which represents conventional ML, is evaluated. Combining host and network features enhances attack detection performance (macro average F1 score) by 8.1% under the CICIDS 2018 dataset and 3.7% under the NDSec-1 dataset. Meanwhile, the two-stage collaborative classifier improves detection performance for most single classes, especially for DoS-LOIC-UDP and DoS-SlowHTTPTest, with improvements of 30.7% and 84.3%, respectively, when compared with the traditional ML models.}
}


@article{DBLP:journals/cn/WuJZLL24,
	author = {Xiaodong Wu and
                  Zhigang Jin and
                  Junyi Zhou and
                  Kai Liu and
                  Zepei Liu},
	title = {Against network attacks in renewable power plants: Malicious behavior
                  defense for federated learning},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110577},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110577},
	doi = {10.1016/J.COMNET.2024.110577},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WuJZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As reducing carbon emissions can relieve environmental concerns, networks-supported renewable power plants are being built more and more. Inevitable network attacks have become a serious threat in increasing and distributed power plants. Leveraging federated learning for training the joint model to detect network attacks in distributed power plants is efficient, but two malicious behaviors of cheating and free-riding are unavoidable. To this end, we design a new SDN based federated security architecture and propose a carbon-credit-rewarded consensus verification mechanism in this architecture to deal with malicious behaviors. For this architecture, on the one hand, considering geographical conditions of renewable power plants, multi-controller SDN is adopted in network to solve some security problems at root and to avoid single point of failure. On the other hand, the segmentation of collaborative zones reduces communication cost effectively. The proposed mechanism establishes consensus bearer and realizes the election of consensus bearer by cross-validation of client detection models. Only the excellent models are aggregated to mitigate cheating of malicious clients. Carbon emissions credit is introduced as an incentive in the proposed mechanism. The redistribution of carbon emissions credit improves the performance of global detection model and avoids free-riding. Moreover, the economic nature of carbon emissions credit enhances the spillover effect of carbon emissions trading market on the reduction of carbon emissions. The experimental results revealed that the proposed architecture has excellent performance, and can handle malicious behaviors effectively.}
}


@article{DBLP:journals/cn/KuswiradyoKS24,
	author = {Primatar Kuswiradyo and
                  Binayak Kar and
                  Shan{-}Hsiang Shen},
	title = {Optimizing the energy consumption in three-tier cloud-edge-fog federated
                  systems with omnidirectional offloading},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110578},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110578},
	doi = {10.1016/J.COMNET.2024.110578},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KuswiradyoKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most of the recent studies regarding a collaborative federation architecture that integrates multiple computing systems to support Internet of Things (IoT) networks only consider two-tier cloud–edge federation systems, which only support vertical and horizontal offloading. This paper proposes a generic architecture of cloud–edge–fog systems, which provides omnidirectional offloading between service nodes. To investigate the effectiveness of the proposed architecture, it is formulated as a workload offloading and capacity optimization problem, to minimize energy consumption. The essence of our approach lies in harmonizing the unified computing resources across the cloud, edge, and fog layers, thus optimizing the system’s overall energy efficiency and responsiveness. This architectural innovation is grounded in the philosophy that a more flexible workload offloading mechanism can significantly enhance the system’s adaptability to dynamic workload demands, leading to substantial energy savings and improved service latency. Modified simulated annealing (MSA) is implemented to develop the offloading algorithm, which leads to a reduction of energy consumption of 11%–27% when compared to other heuristic methods. Simulation results also show that three-tier cloud–edge–fog federated architecture with omnidirectional offloading can reduce the total energy consumption by 12%–21% when compared to other existing architectures in the literature. We also get better results for average response time when compared with existing architectures with the same number of input workloads.}
}


@article{DBLP:journals/cn/UnalFEDBM24,
	author = {Deniz {\"{U}}nal and
                  Sara Falleni and
                  Kerem Enhos and
                  Emrecan Demirors and
                  Stefano Basagni and
                  Tommaso Melodia},
	title = {Design and performance evaluation of SEANet, a software-defined networking
                  platform for the Internet of Underwater Things},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110579},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110579},
	doi = {10.1016/J.COMNET.2024.110579},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/UnalFEDBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the design and performance evaluation of the SEANet platform, a software-defined acoustic modem designed for enhancing underwater networking and Internet of Underwater Things (IoUT) applications. Addressing the limitations of traditional acoustic modems, which suffer from low data rates and rigid architectures, SEANet introduces a versatile, adaptive framework capable of reconfiguring all layers of the protocol stack in real-time to accommodate diverse marine applications. The platform integrates high-performance, wideband data converters with modular hardware and software components, enabling real-time adaptation to changing environmental and operational conditions. Experimental evaluations conducted in oceanic settings demonstrate the SEANet capability to significantly exceed the performance of existing commercial underwater modems, supporting data rates up to 150 kbit/s and effectively doubling the performance metrics of conventional systems. Our robust testing also highlights the SEANet proficiency in channel estimation, Orthogonal Frequency-Division Multiplexing (OFDM) link establishment, and interoperability through the JANUS communication standard. Our results underscore the SEANet potential to transform underwater communication technologies, providing a scalable and efficient solution that supports high data rate applications and fosters the expansion of IoUT deployments.}
}


@article{DBLP:journals/cn/Szabo24,
	author = {G{\'{e}}za Szab{\'{o}}},
	title = {Towards the automatic network resource management of {OPC} {UA} in
                  5G private networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110581},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110581},
	doi = {10.1016/J.COMNET.2024.110581},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Szabo24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the realm of Network Function Virtualization (NFV) and Software Defined Networking (SDN), the shift of network functions to the edge presents both challenges and opportunities across various industries. This paper investigates the utilization of Network Resource Management (NRM) within the Third Generation Partnership Project (3GPP) Service Enabler Architecture Layer (SEAL) framework for network operations in different sectors. Leveraging the Open Platform Communications (OPC) Unified Architecture (UA) Publish Subscribe (PubSub) pattern, we enhance edge networking by automating NRM within SEAL, introducing custom Message Queuing Telemetry Transport (MQTT) brokers and a mapping node. To validate our approach, we establish a SEAL NRM test server and implement a translator mechanism for interoperability. We address the integration of non-adaptive applications into the NRM domain and demonstrate our proposal with a practical Robot Operating System (ROS) Digital Twin (DT) application, showcasing dynamic NRM’s potential for optimizing network resources in scenarios like 3D printing.}
}


@article{DBLP:journals/cn/CaiHCWXG24,
	author = {Wei Cai and
                  Chengshang Hou and
                  Mingxin Cui and
                  Bingxu Wang and
                  Gang Xiong and
                  Gaopeng Gou},
	title = {Incremental encrypted traffic classification via contrastive prototype
                  networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110591},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110591},
	doi = {10.1016/J.COMNET.2024.110591},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CaiHCWXG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classifying encrypted network traffic into distinct categories, also known as encrypted traffic classification (ETC), is a crucial step in maintaining network security. There are numerous applications in the area of network security, such as quality of service. The rapid development of network applications makes tackling the classification of encrypted traffic in an incremental learning environment attractive. However, the ambiguity and imbalance of traffic restrict existing incremental approaches from achieving satisfactory results. To overcome these obstacles, we provide (1) a unique clustering-based exemplar selection approach for storing the most representative data relevant to each learned traffic scenario, as well as filtering ambiguous traffic for knowledge replay. Furthermore, (2) we ensure consistent model performance across all trained classes while addressing traffic imbalance issues via an incremental contrastive distillation approach. Using real encrypted network datasets, we validate the efficacy of our technique. The experimental findings prove the efficacy of the suggested strategy, our method surpasses the state-of-the-art model with micro F1 score improvements of 5.0%, 8.6%, and 4.8% on the ISCX-VPN, 18 Apps, and TLS1.3 benchmarks, respectively.}
}


@article{DBLP:journals/cn/LiCZ24,
	author = {Wenzhi Li and
                  Lin Cui and
                  Xiaoquan Zhang},
	title = {Enabling locality-sensitive machine learning towards low predictive
                  overhead in flow classification},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110592},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110592},
	doi = {10.1016/J.COMNET.2024.110592},
	timestamp = {Wed, 10 Jul 2024 10:43:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The expansion of computer networks and the proliferation of their applications have heightened the necessity for accurate network traffic identification to enhance resource allocation and mitigate congestion, emphasizing flow classification’s pivotal role. Traditional methods, notably deep packet inspection, have encountered challenges due to the rise in encrypted traffic, diminishing their efficacy. In response, machine learning approaches, including decision trees, have been adopted for their ability to classify flows without necessitating access to the payload, thereby enhancing classification efficiency. Nonetheless, these machine learning-based models introduce considerable overhead in category determination. To address this issue, this paper introduces a locality-sensitive flow classification (LSFC) method, leveraging locality-sensitive hashing to efficiently partition and group traffic, thereby expediting the classification process and minimizing overhead. LSFC adeptly balances classification accuracy and processing delay, catering to the specific demands of various scenarios. Comprehensive evaluations demonstrate that LSFC significantly outperforms traditional machine learning models in inference speed by over 50% and improves upon deep packet inspection techniques in handling encrypted traffic, achieving a reduction in classification delay exceeding 15% without compromising classification accuracy.}
}


@article{DBLP:journals/cn/LiZMZL24,
	author = {Nianxin Li and
                  Linbo Zhai and
                  Zeyao Ma and
                  Xiumin Zhu and
                  Yumei Li},
	title = {Lyapunov-guided Deep Reinforcement Learning for service caching and
                  task offloading in Mobile Edge Computing},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110593},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110593},
	doi = {10.1016/J.COMNET.2024.110593},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiZMZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of Internet of Things (IoT) networks and Mobile Edge Computing (MEC), many computing-intensive applications have been developed in large quantities. Due to the heterogeneity of tasks, different application services are required to perform each task. Caching application services and related data in edge servers is challenging. Hence, we study the service cache placement and task offloading problem in IoT networks. Since IoT devices and edge servers with limited storage resources can only cache a few services at the same time, we formulate the service cache placement and task offloading of IoT devices problem to minimize task service delay with long-term energy constraint of IoT devices, which is a mixed integer nonlinear programming problem. To solve this problem, an online Deep Reinforcement Learning guided by the Lyapunov optimization framework algorithm (LYADRL) is proposed. We first build a virtual queue model to decouple the problem by Lyapunov optimization technique to transform the problem into a single time slot optimization problem. Then, we use Deep Reinforcement Learning techniques to find the optimal edge service caching and task offloading policies for each time slot. Simulation results show that our algorithm can reduce the service delay compared with other benchmark algorithms.}
}


@article{DBLP:journals/cn/WangW24,
	author = {You{-}Chiun Wang and
                  Cheng{-}Yan Wu},
	title = {{EC-NTD:} Efficient countermeasure against DrDoS attacks with {NAPT}
                  and two-stage detection in SDN-based networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110594},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110594},
	doi = {10.1016/J.COMNET.2024.110594},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed reflection denial of service (DrDoS) attacks are a prevalent and troublesome form of DDoS attack. Fake service requests trigger a flood of services responses, typically in large packet sizes, that are sent to targeted hosts via public servers. As public servers are legitimate, and their services are necessary for targeted hosts, DrDoS attacks cannot be effectively blocked through firewalls or most solutions to DDoS floods. This paper leverages the software-defined networking (SDN) technique and proposes an efficient countermeasure with NAPT and two-stage detection (EC-NTD) scheme to safeguard against DrDoS attacks, where NAPT refers to network address port translation. We consider that attack sources (i.e., botnet members that send fake requests to public servers) may be outside or inside an SDN-based network where DrDoS targeted hosts reside. To guard against external attacks, filtering rules and NAPT are used in gateways to distinguish normal responses from those caused by attacks. To recognize internal attacks, the controller detects anomalies in the quantity of requests and their source IP addresses. Additionally, the adaptive adjustment of the attack detection period length helps alleviate the controller’s load while maintaining effective defense. Simulation results reveal that the EC-NTD scheme can efficiently safeguard against DrDoS attacks with different services.}
}


@article{DBLP:journals/cn/PalamaLMSBGBB24,
	author = {Ivan Palam{\`{a}} and
                  Yago Lizarribar and
                  Lorenzo Maria Monteforte and
                  Giuseppe Santaromita and
                  Stefania Bartoletti and
                  Domenico Giustiniano and
                  Giuseppe Bianchi and
                  Nicola Blefari{-}Melazzi},
	title = {5G positioning with software-defined radios},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110595},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110595},
	doi = {10.1016/J.COMNET.2024.110595},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PalamaLMSBGBB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Positioning is a key focus in 5G standardization, starting with 3GPP Release 16. However, most of the effort from the research community and work presented in the technical standardization has been limited mainly to simulation studies. This paper explores the use of software-defined radios (SDRs) platforms for 5G positioning, presenting an overview of the current state-of-the-art and available open-source platforms. Utilizing an advanced SDR-based multi-gNodeBs (gNBs) synchronized testbed, the paper conducts a series of time-based, over-the-air measurements. Results offer insights into the impact of various real-world system parameters, such as the number of gNBs, transmission bandwidth, signal processing techniques, and localization algorithms on positioning accuracy and Time to First Fix (TTFF). These findings provide a pathway for the cost-effective and efficient implementation of high-precision 5G localization systems. The paper contributes to advancing both theoretical understanding and practical applications, serving as a guide for the development of 5G positioning technology.}
}


@article{DBLP:journals/cn/ZangWZGGZ24,
	author = {Xiaodong Zang and
                  Tongliang Wang and
                  Xinchang Zhang and
                  Jian Gong and
                  Peng Gao and
                  Guowei Zhang},
	title = {Encrypted malicious traffic detection based on natural language processing
                  and deep learning},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110598},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110598},
	doi = {10.1016/J.COMNET.2024.110598},
	timestamp = {Mon, 08 Jul 2024 23:02:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZangWZGGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The focus on privacy protection has brought much-encrypted network traffic. However, attackers always abuse traffic encryption to conceal malicious behaviors. Although researchers have proposed several enlightening detection methods, they must enhance the generalization ability or improve detection performance. Our inspiration is that the packet header fields, as do the underlying grammatical rules for constructing sentences, have a strict order. We consider the original packet as text and devise a robust approach with natural language processing and a deep learning model to improve the generalization ability and detection performance. We capture the critical keywords as characteristic representations of the traffic and design an adaptive domain generalization algorithm with a new loss function. It is robust against various datasets by generating more malicious samples to augment the minority of malicious samples. Simultaneously, we design an efficient feature selection algorithm, which obtains an optimal feature subset and reduces feature dimensions by 75.3\n%\n. To evaluate our work, we conducted extensive experiments with open-source datasets (CICIDS 2017, CICDDoS 2019, and USTC-TFC 2016), the synthetic dataset from IoT-23, and Internet backbone traffic (CERNET). Experimental results demonstrate that our proposal improves detection accuracy by up to 22.8\n%\ncompared to others not using domain generalization algorithms and achieves an average detection latency of 0.67 s in the backbone. Besides, our work applies to the Industrial Internet of Things (IIoT) environment. It can be deployed at edge nodes to provide network security support for IIoT devices.}
}


@article{DBLP:journals/cn/IsmailUSAP24,
	author = {Layal Ismail and
                  Domenico Uomo and
                  Andrea Sgambelluri and
                  Faris Alhamed and
                  Francesco Paolucci},
	title = {{P4} {FANET} In-band Telemetry {(FINT)} for AI-assisted wireless link
                  failure forecasting and recovery},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110599},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110599},
	doi = {10.1016/J.COMNET.2024.110599},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/IsmailUSAP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper introduces a novel framework for enhancing quality of service predictability in Flying ad hoc Networks (FANET) by leveraging P4 data-plane programmability. The proposed solution, P4 FANET In-Band Telemetry (FINT), is specifically designed to tailor the limited resources in wireless networks and is extended to collect not only the standard INT metadata but also novel essential Unmanned Aerial Vehicles (UAV) real-time metrics, including Received Signal Strength Indicator (RSSI), geolocation information and CPU load. These parameters are then fed into an artificial intelligence (AI) system, enabling proactive prediction of FANET link failures. By integrating P4 FINT and AI, our framework aims to improve the availability and overall performance of UAV-based networks through advanced link failure forecasting.}
}


@article{DBLP:journals/cn/MaCLQZL24,
	author = {Yurui Ma and
                  Jue Chen and
                  Wenjing Lv and
                  Xihe Qiu and
                  Yue Zhang and
                  Wanxiao Liu},
	title = {An improved artificial bee colony algorithm to minimum propagation
                  latency and balanced load for controller placement in Software Defined
                  Network},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110600},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110600},
	doi = {10.1016/J.COMNET.2024.110600},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MaCLQZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Network (SDN) has become an increasingly popular network architecture as it provides flexible global network management by decoupling the control plane from the data plane, and deploys multiple controllers in a logically centralized and physically distributed manner in the network. In this architecture, controllers serve as the brains and are responsible for directing traffic and optimizing performance. However, deploying SDN controllers can be a complex process that requires careful planning and consideration, resulting in the emergence of the Controller Placement Problem (CPP). Considering that this problem is proven to be NP-hard, this paper proposes an algorithm named improved Artificial Bee Colony with K-means (ABCK) algorithm to calculate the optimal or near-optimal solution with high efficiency. On the basis of the heuristic algorithm, i.e., Artificial Bee Colony (ABC) algorithm, we adjust the initial food source search equation by using approach of K-means, and optimize related parameters to minimize the propagation latency between controllers and switches (including average latency and worst case latency) as well as maintaining load balancing among controllers. In order to prove the effectiveness of our proposed method, we conduct experiments on five real topologies from the Internet Topology Zoo and Internet2 OS3E to compare the performance between ABCK and other state-of-the-art methods including ABC, cross entropy, K-means, K-means++, Pareto Simulated Annealing (PSA) and Pareto Bacterial Foraging Optimization (ABFO). The results verify that our proposed approach can realize the minimum propagation latency (including the average and worst case latencies) for different network scales with different numbers of controllers among all the methods, with no more than 4.37 % margin from the optimal solution. At the same time, by using our approach, the load of controllers is the most balanced and the computation time is the least among all the methods. In addition, the simulation results show that our proposed method performs better in large scale network topologies.}
}


@article{DBLP:journals/cn/ChenYLH24,
	author = {Wen Chen and
                  Yuxiao Yang and
                  Sibin Liu and
                  Wenjing Hu},
	title = {Optimizing resource allocation for cluster D2D-assisted fog computing
                  networks: {A} three-layer Stackelberg game approach},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110601},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110601},
	doi = {10.1016/J.COMNET.2024.110601},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenYLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource management in fog computing is a highly challenging issue. Resource constraints of fog devices, the dynamic and diverse nature of workloads such as data-intensive, computationally intensive, and latency-sensitive applications, and the unpredictability of fog computing environments make it more challenging. How to collaboratively schedule edge, and end multi-layer ubiquitous computing resources to break through single node scale and resource bottlenecks to further improve computing performance also needs to be addressed. In this paper, we leverage idle resources on the user side, and study the resource allocation problems in the D2D-assisted fog computing networks. Firstly, we use an improved Louvain algorithm to cluster users and select the D2D device with the highest priority in each cluster as the cluster head to assist fog computing, where cluster heads can act as both servers and relays. Secondly, considering that fog nodes and D2D devices have limited resources and are not obligated to share their resources for free, we propose a price incentive method to construct a three-layer Stackelberg game model that maximizes the benefits of fog nodes and cluster heads while minimizing user costs. Then, we use the Lagrange multiplier method and the Karush–Kuhn–Tucker (KKT) conditions to solve the equilibrium point of the model to achieve our goal. Finally, the simulation results show that our proposed scheme markedly decreases the overall system cost in comparison to the four benchmark schemes. Moreover, in various scenarios the total cost of our scheme consistently remains lower than the other four schemes, affirming the efficiency of the scheme proposed in this paper.}
}


@article{DBLP:journals/cn/HanWS24,
	author = {Pengbin Han and
                  Xinfeng Wu and
                  Aina Sui},
	title = {{DTPBFT:A} dynamic and highly trusted blockchain consensus algorithm
                  for {UAV} swarm},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110602},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110602},
	doi = {10.1016/J.COMNET.2024.110602},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HanWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous development of UAV technology, the application of UAV swarm in the military field is gradually becoming the focus of research around the world. Although it can bring a series of benefits in autonomous cooperation, the traditional UAV management technology is prone to hacker attacks due to many security issues such as a single point of failure brought by centralized management. Because of the advantages of distributed, tamper-proof, and traceability, blockchain is applied to UAV swarm to solve some of the security problems caused by centralized management. However, due to the limitations of its consensus algorithm Practical Byzantine Fault Tolerance (PBFT), its communication complexity will increase rapidly with the increase of the number of nodes, which also leads to the poor scalability of the algorithm and can only be applied to small-scale networks. To use the PBFT algorithm in large-scale networks such as UAV swarm, a dynamic and highly trusted PBFT (DTPBFT) algorithm is proposed in this paper. Firstly, a new consensus algorithm model including consensus layer and verification layer is designed. Then, a consensus node election scheme based on trust mechanism is proposed under this model. The trust degree of nodes in the blockchain network is comprehensively evaluated by selecting several representative indicators, and the weight factor of each indicator is calculated by entropy weight method. It not only reduces the communication complexity, but also ensures the reliability and dynamic update of consensus nodes. Experiments show that when the number of UAVs is 200, the consensus time of DTPBFT is 0.24 s, which indicates that this algorithm can support large-scale UAV swarm without causing communication congestion, so it has good scalability. In addition, experiments also show that DTPBFT can tolerate more than\n1\n3\nmalicious nodes, which improves the fault tolerance rate of PBFT.}
}


@article{DBLP:journals/cn/AnLCL24,
	author = {Xiaobei An and
                  Yanjun Li and
                  Yuzhe Chen and
                  Tingting Li},
	title = {Joint task offloading and resource allocation for multi-user collaborative
                  mobile edge computing},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110604},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110604},
	doi = {10.1016/J.COMNET.2024.110604},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AnLCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The exponential growth of Internet of Things (IoT) terminal devices in recent years has caused overload when offloading their computational tasks solely to edge servers. Partial offloading and collaborative mobile edge computing (MEC) are thus proposed to segment the tasks and offload a portion of the tasks to neighboring devices via device-to-device (D2D) communication, which alleviates the server load and fully explores the idle resources among end-devices. Consider that the end-devices face the limitation in energy provision, this paper aims to minimize the energy consumption of the entire end-devices while ensuring the task completion delay, by optimizing the devices’ offloading decisions, corresponding offloading ratios and transmit powers. This problem is formulated as a mixed-integer nonlinear programming (MINLP) problem. To solve the problem, a two-level alternating optimization framework is proposed. At the upper level, ant colony optimization (ACO) is used to obtain a population of offloading decisions, while deep deterministic policy gradient (DDPG) algorithm is employed at the lower level to obtain the optimal offloading decision, corresponding offloading ratios and transmit powers. Simulation results demonstrate that our proposed ACO-DDPG algorithm converges well. Further more, it consumes the lowest energy compared with all the baseline algorithms and can achieve 100% task completion ratio under various simulation settings.}
}


@article{DBLP:journals/cn/HeinoJHV24,
	author = {Jenny Heino and
                  Christian Jalio and
                  Antti Hakkala and
                  Seppo Virtanen},
	title = {{JAPPI:} An unsupervised endpoint application identification methodology
                  for improved Zero Trust models, risk score calculations and threat
                  detection},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110606},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110606},
	doi = {10.1016/J.COMNET.2024.110606},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HeinoJHV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in global digitalization triggered by COVID-19 has led to a significant increase in internet traffic and has precipitated a rapid transformation of the network security landscape. Despite being increasingly difficult, accurate traffic inspection is vital for ensuring productivity while reliably protecting internal assets. Endpoint application identification enables high accuracy inspection and detection by providing network security solutions with specific context on individual connections. However, achieving it in real-time with standard fingerprinting methods based only on client-side traffic has proven to be a challenging problem with no comprehensive solution thus far. In this article, we present a new methodology for identifying endpoint applications from network traffic, utilizing machine learning. Our methodology leverages similarities in the pre-hash string of the JA3 algorithm for fingerprinting application specific TLS Client Hello messages. By utilizing well-known clustering algorithms it is possible to identify the underlying TLS libraries and the application from the traffic remarkably better than with simple string-based matching. Our model can categorize 99,5% of the traffic in a controlled network, and 93,8% in an uncontrolled network, compared to 0,1% and 0,2% using simple string matching. Our methodology is especially effective for enhancing Zero Trust models, calculating a risk score for network events, and improving threat detection accuracy in network security solutions.}
}


@article{DBLP:journals/cn/CaoWXW24,
	author = {Chengliang Cao and
                  Fenghe Wang and
                  Han Xiao and
                  Ye Wang},
	title = {Secure and efficient vehicle data downloading scheme with privacy-preserving
                  in VANETs},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110610},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110610},
	doi = {10.1016/J.COMNET.2024.110610},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CaoWXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data downloading is a basic data transfer requirement in VANETs. Cooperation data downloading can use all V2C, V2I and V2V communications to improve the data downloading efficiency. Security and identity privacy preserving of data downloading should be paid more attention to protect the required vehicle. Otherwise, the data and identity information leakages would lead to immeasurable loss. To address these challenges, we propose a lattice-based secure vehicle data downloading scheme with privacy-preserving for VANETs. The vehicle request message authentication and identity privacy of the vehicle are achieved by a designed lattice-based ring signature. We prove that the proposed scheme is strong unforgeable and unconditional privacy preserving in the random oracle model. The request data also is encrypted by a symmetric encryption algorithm to achieve the data confidentiality. Moreover, thanks to the post-quantum security of the lattice-based cryptographic tools, the proposed data downloading also can defend against the quantum attacks. At the same time, cooperation data downloading also can be used in this paper to improve the effects of data downloading. Performance comparison and analysis shows that the proposed scheme shares some advantages with respect to the space and computation efficiency. The main character of the proposed scheme is that the impact of the vehicle ring group number on both the computation and space efficiency is effective controlled, such as the fixed signature length for any ring group size. As a result, the bandwidth and computation resources of VANETs can be saved by using the proposed scheme. We also simulate the proposed scheme by NS-3 which shows the average download success ratio is always within the interval\n[\n95\n%\n−\n100\n%\n]\nand cooperative downloading can improve the downloading efficiency by 86%.}
}


@article{DBLP:journals/cn/DaiLW24,
	author = {Bin Dai and
                  Hetao Li and
                  Yifan Wang},
	title = {{SP-DG:} {A} programmable packet-level scheduling for queuing delay
                  guarantees in time-critical networks},
	journal = {Comput. Networks},
	volume = {250},
	pages = {110614},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110614},
	doi = {10.1016/J.COMNET.2024.110614},
	timestamp = {Mon, 08 Jul 2024 23:02:34 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DaiLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-critical applications, such as remote surgery, virtual reality, and automatic driving, impose stringent requirements for minimizing delay and jitter. The queuing delay at forwarding nodes significantly affects application traffic. To tackle this challenge, Time-Sensitive Networking (TSN) techniques have been proposed to ensure deterministic queuing delay. However, existing deterministic solutions, including the Time-Aware Shaper (TAS) and the Cyclic Queuing and Forwarding (CQF) model, encounter limitations due to their reliance on precise synchronization and dedicated forwarding plane devices. In this paper, we present a novel solution: the Strict Priority queuing model with a queuing Delay Guarantees (SP-DG), specifically designed for programmable forwarding planes. This model introduces an upper bound on the queuing delay for packets at each switch, effectively characterizing their distinct queuing delay requirements. By utilizing exponential smoothing to predict traffic intensity, packets are intelligently scheduled to appropriate queues based on their upper-bound queuing delay requirements. Our Netbench simulation and P4 software switch experiments demonstrate that the proposed SP-DG algorithm, compared to the state-of-the-art SP-based queue scheduling algorithms, achieves superior performance in delay-bounded queue scheduling.}
}
