@article{DBLP:journals/cn/JmilaK22,
	author = {Houda Jmila and
                  Mohamed Ibn Khedher},
	title = {Adversarial machine learning for network intrusion detection: {A}
                  comparative study},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109073},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109073},
	doi = {10.1016/J.COMNET.2022.109073},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JmilaK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intrusion detection is a key topic in cybersecurity. It aims to protect computer systems and networks from intruders and malicious attacks. Traditional intrusion detection systems (IDS) follow a signature-based approach, but in the last two decades, various machine learning (ML) techniques have been strongly proposed and proven to be effective. However, ML faces several challenges, one of the most interesting being the emergence of adversarial attacks to fool the classifiers. Addressing this vulnerability is critical to prevent cybercriminals from exploiting ML flaws to bypass IDS and damage data and systems.}
}


@article{DBLP:journals/cn/RibeiroJZPSFGS22,
	author = {Rafael Hengen Ribeiro and
                  Arthur Selle Jacobs and
                  Luciano Zembruzki and
                  Ricardo Parizotto and
                  Eder John Scheid and
                  Alberto Egon Schaeffer Filho and
                  Lisandro Zambenedetti Granville and
                  Burkhard Stiller},
	title = {A deterministic approach for extracting network security intents},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109109},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109109},
	doi = {10.1016/J.COMNET.2022.109109},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RibeiroJZPSFGS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intents brought significant improvements in network management by the use of intent-level languages. Despite these improvements, intents are not yet fully integrated and deployed in most large-scale networks. As a result, network operators may still experience problems when deploying new intents, for instance, learning a vendor-specific language to understand previously deployed configurations of a network device. Additionally, traditional configurations are distributed across multiple devices, each configured using low-level, vendor-specific languages. As a result, inferring intents from these low-level configurations is a time-consuming process. Furthermore, current solutions for deriving high-level representations from bottom-up configuration analysis do not provide results as intents or have a very limited scope, missing essential details that enhance the representation.}
}


@article{DBLP:journals/cn/LaslaAAY22,
	author = {Noureddine Lasla and
                  Lina Al{-}Sahan and
                  Mohamed Abdallah and
                  Mohamed F. Younis},
	title = {Green-PoW: An energy-efficient blockchain Proof-of-Work consensus
                  algorithm},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109118},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109118},
	doi = {10.1016/J.COMNET.2022.109118},
	timestamp = {Mon, 28 Aug 2023 21:39:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LaslaAAY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper opts to mitigate the energy-inefficiency of the Blockchain Proof-of-Work (PoW) consensus algorithm by rationally repurposing the power spent during the mining process. The original PoW mining scheme is designed to consider one block at a time and assign a reward to the first place winner of a computation race. To reduce the mining-related energy consumption, we propose to compensate the computation effort of the runner(s)-up of a mining round, by granting them exclusivity of solving the upcoming block in the next round. This will considerably reduce the number of competing nodes in the next round and consequently, the consumed energy. Our proposed scheme divides time into epochs, where each comprises two mining rounds; in the first one, all network nodes can participate in the mining process, whereas in the second round only runners-up can take part. Thus, the overall mining energy consumption can be reduced to nearly 50%. To the best of our knowledge, our proposed scheme is the first to considerably decrease the energy consumption of the original PoW algorithm. Our analysis demonstrates the effectiveness of our scheme in reducing energy consumption, the probability of fork occurrences, the level of mining centralization presented in the original PoW algorithm, and the effect of transaction censorship attack.}
}


@article{DBLP:journals/cn/ChenCLY22,
	author = {Kuan{-}Wei Chen and
                  Chih{-}Min Chao and
                  Chih{-}Yu Lin and
                  Chun{-}Chao Yeh},
	title = {Anti-jamming channel hopping protocol design based on channel occupancy
                  probability for Cognitive Radio Networks},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109125},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109125},
	doi = {10.1016/J.COMNET.2022.109125},
	timestamp = {Fri, 02 Sep 2022 19:16:38 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenCLY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a Cognitive Radio Network (CRN), a secondary user may suffer from jamming attacks if a fixed channel hopping sequence is utilized. Existing anti-jamming channel hopping protocols have at least one of the shortcomings such as sharing confidential information beforehand, no rendezvous guarantee between any pair of SUs, or long time to rendezvous. Another issue in existing anti-jamming channel hopping protocols is that the channels with different PU occupancy probabilities are being used equally. To improve the existing anti-jamming channel hopping protocols in CRNs, we propose rendezvous-guaranteed anti-jamming protocols OLAA_T and OLAA_R in this paper. Both protocols are capable of adjusting the channel usage ratio according to PU occupancy probability. Simulation results verify that OLAA_T and OLAA_R enhance the existing most practical anti-jamming channel hopping protocol, Load Awareness Anti-jamming (LAA), by up to 41% in terms of network throughput.}
}


@article{DBLP:journals/cn/HuangDZSYL22,
	author = {Shan Huang and
                  Dezun Dong and
                  Zejia Zhou and
                  Hanyi Shi and
                  Wenxiang Yang and
                  Xiangke Liao},
	title = {FastCredit: Expediting credit-based congestion control in datacenters},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109126},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109126},
	doi = {10.1016/J.COMNET.2022.109126},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuangDZSYL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Receiver-driven transports that leverage hop-by-hop credits have the merits of fast convergence, short queueing, and strong congestion avoidability, making them high-efficiency in managing congestion in current high-speed data centers. However, these transport protocols treat long and latency-sensitive short flows equally, increasing flow completion times and affecting user experiences. Although flow scheduling mechanisms have been studied extensively, they are hard to be deployed into hop-by-hop credit-based transports. The root cause is that most traditional flow scheduling mechanisms mainly work in the long queue containing flows of various sizes, while credit-based proactive transports maintain the extremely short bounded queue.}
}


@article{DBLP:journals/cn/PengNSRWSX22,
	author = {Songtao Peng and
                  Jiaqi Nie and
                  Xincheng Shu and
                  Zhongyuan Ruan and
                  Lei Wang and
                  Yunxuan Sheng and
                  Qi Xuan},
	title = {A multi-view framework for {BGP} anomaly detection via graph attention
                  network},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109129},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109129},
	doi = {10.1016/J.COMNET.2022.109129},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PengNSRWSX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the default protocol for exchanging routing reachability information on the Internet, the abnormal behavior in traffic of Border Gateway Protocols (BGP) is closely related to Internet anomaly events. The BGP anomalous detection model ensures stable routing services on the Internet through its real-time monitoring and alerting capabilities. Previous studies either focused on the feature selection problem or the memory characteristic in data, while ignoring the relationship between features and the precise time correlation in feature (whether it is long or short term dependence). In this paper, we propose a multi-view model for capturing anomalous behaviors from BGP update traffic, in which Seasonal and Trend decomposition using Loess (STL) method is used to reduce the noise in the original time-series data, and Graph Attention Network (GAT) is used to discover feature relationships and time correlations in feature, respectively. Our results outperform the state-of-the-art methods at the anomaly detection task, with the average F1 score up to 96.3% and 93.2% on the balanced and imbalanced datasets respectively. Meanwhile, our model can be extended to classify multiple anomalous and to detect unknown events.}
}


@article{DBLP:journals/cn/YangLJTLXZ22,
	author = {Donghui Yang and
                  Zhenyu Li and
                  Haiyang Jiang and
                  Gareth Tyson and
                  Hongtao Li and
                  Gaogang Xie and
                  Yu Zeng},
	title = {A deep dive into {DNS} behavior and query failures},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109131},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109131},
	doi = {10.1016/J.COMNET.2022.109131},
	timestamp = {Wed, 02 Nov 2022 13:10:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YangLJTLXZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Domain Name System (DNS) is fundamental to the operation of the Internet. Providing an up-to-date view of DNS behavior in-the-wild is thus important for various Internet stakeholders. Among the behavioral characteristics, failure is one of the most import aspects, because failures within DNS can have a dramatic impact on the wider Internet, most notably preventing access to any services dependent on domain names (e.g. web, mobile apps). In this paper, we perform a large study into DNS activity, covering 3B queries in total. We first examine DNS behavior from different perspectives including DNS query types, use of recursive resolvers, TTLs and hosting infrastructures, aiming at providing an up-to-date view of DNS. We then focus on DNS query failures. We find that 13.5% of DNS queries fail, and this leads us to explore the root causes. We observe significant differences between IPv4 and IPv6 lookups, biased failure distribution across domains, the great impact of recursive resolvers and malicious domains on query failures. We also discuss the implications of our findings and provide a handful of recommendations.}
}


@article{DBLP:journals/cn/BarakabitzeW22,
	author = {Alcardo Alex Barakabitze and
                  Ray Walshe},
	title = {{SDN} and {NFV} for QoE-driven multimedia services delivery: The road
                  towards 6G and beyond networks},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109133},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109133},
	doi = {10.1016/J.COMNET.2022.109133},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BarakabitzeW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The concept for developing the future mobile communication systems has been explored in recent years with a vision towards 5G and 6G systems. Meeting performance targets such as higher data rate transmission, lower end-to-end latency, higher capacity, lower cost, and satisfied Quality of Experience (QoE) for delivered services is the key for the success of 5G and 6G networks. Within these performance targets and associated enabling technologies, QoE and the overall management of multimedia services is the one which has been less well addressed for 5G in the past. This paper provides a comprehensive and ground-breaking discussion regarding QoE management in the context of future softwarized 5G/6G networks. We introduce a generalized QoE provisioning ecosystem for emerging multimedia streaming services in softwarized 5G/6G and beyond networks. We explore potential network softwarization and communication technologies that will enable the end-user’s QoE in 5G/6G. We further provide promising use case scenarios regarding (a) QoE-driven management of multimedia services over multi-access cloud/edge softwarized and virtualized 5G networks, (b) QoE monitoring for large traffic variations in business, residential and public areas in 5G networks, along with a Machine Learning (ML)-based softwarized controller for QoE provisioning over 5G/6G networks. We build on the achievements of 5G networks to provide the roadmap towards the development of 6G and beyond networks in terms of requirements, use case scenarios and service classes. We also present potential technologies that will be dominant through 2025–2030 in shaping the vision of 6G and beyond networks including (a) self-driving 3D network architecture, (b) network management, automation and orchestration, and (c) pervasive artificial intelligence. Moreover, we provide challenges and research directions in 6G and beyond wireless communication systems. The paper provides the first roadmap towards the management, orchestration and monitoring of multimedia 3D services in 6G and beyond networks. The comprehensive goal of this paper is to encourage researchers from the industry and academia to work together towards the realization of softwarized 6G and beyond networks while tackling the critical research challenges regarding the management of emerging 3D multimedia services and applications. The comments made by the authors in this paper are expectations on 6G, and the standardization of 6G where these aspects will be decided in 6G networks and beyond, will be started soon by different standardization bodies and fora.}
}


@article{DBLP:journals/cn/LuongAXNK22,
	author = {Nguyen Cong Luong and
                  Tran The Anh and
                  Zehui Xiong and
                  Dusit Niyato and
                  Dong In Kim},
	title = {Joint time scheduling and transaction fee selection in blockchain-based
                  RF-powered backscatter cognitive radio network},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109135},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109135},
	doi = {10.1016/J.COMNET.2022.109135},
	timestamp = {Mon, 07 Oct 2024 21:54:13 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuongAXNK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we develop a new framework called blockchain-and backscatter-aided Internet of Things (IoT) system. In the framework, IoT devices as secondary transmitters transmit their sensing data to a secondary gateway by using an RF-powered backscatter cognitive radio technology. The data collected at the gateway is then sent to a blockchain network for further verification, storage and processing. As such, the framework enables an IoT system to simultaneously optimize the spectrum usage and maximize the energy efficiency. Moreover, the framework ensures that the data collected from the IoT devices is verified, stored and processed in a decentralized but in a trusted manner. To achieve the goal, we formulate a stochastic optimization problem for the gateway under dynamics of the primary channel, uncertainty of the IoT devices, and unpredictability of the blockchain environment. In the problem, the gateway jointly decides (i) time scheduling, i.e., the energy harvesting time, backscatter time, and transmission time, among the IoT devices, (ii) the blockchain network, and (iii) a transaction fee rate to maximize the network throughput while minimizing the cost. To solve the stochastic optimization problem, we then propose to employ, evaluate, and assess a Deep Reinforcement Learning (DRL) algorithm with Dueling Double Deep Q-Networks (D3QN) to derive the optimal policy for the gateway. Simulation results clearly show that the proposed solution outperforms the conventional baseline approaches such as the conventional Q-Learning algorithm and non-learning algorithms in terms of network throughput and convergence speed. Furthermore, the proposed solution guarantees that the data is stored in the blockchain network at a reasonable cost.}
}


@article{DBLP:journals/cn/KumariYJ22,
	author = {Nidhi Kumari and
                  Anirudh Yadav and
                  Prasanta K. Jana},
	title = {Task offloading in fog computing: {A} survey of algorithms and optimization
                  techniques},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109137},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109137},
	doi = {10.1016/J.COMNET.2022.109137},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KumariYJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The exponential growth in Internet of Things (IoT) devices and the limitations of cloud computing in terms of latency and quality of service for time-sensitive applications have led to the unfolding of the efficient middleware technology called fog. Fog computing circumvents the limitations of the cloud by creating a seamless continuum between the things/IoT/end-user devices and the cloud and reducing the effective distance. However, fog computing faces challenges for offloading tasks for their remote computation at some level. Hence, the optimality of task offloading is the primary research area in fog computing. Several contemporary papers exist on this important subject. The research gap in reviewing all these task offloading algorithms has motivated us for their presentation in the form of a detailed survey in this paper. There exist some survey papers which deal with the task offloading. However, none of them has covered the basics of optimization techniques and their solution approaches. The primary objective of this paper is to provide the readers with a complete overview of the journey from a task offloading idea to its mathematical problem formulation and finally to its solution with all details of optimization techniques. We begin by introducing fog computing, and task offloading process followed by several task offloading factors governing decision-making process and their surveys. A section is fully dedicated to the survey of offloading objectives with examples. We also present several optimization approaches used in task offloading. Finally, the last section dedicates to the challenges and future direction in fog computing. The outcomes of the survey will benefit readers in learning the optimization used in task offloading, and it will also provide them a systematic design of offloading scheme with specific objectives.}
}


@article{DBLP:journals/cn/FouladiEA22,
	author = {Ramin Fadaei Fouladi and
                  Orhan Ermis and
                  Emin Anarim},
	title = {A DDoS attack detection and countermeasure scheme based on {DWT} and
                  auto-encoder neural network for {SDN}},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109140},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109140},
	doi = {10.1016/J.COMNET.2022.109140},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FouladiEA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Networking provides new functionalities to easily manage, configure, and optimize network resources by introducing a clear separation between the control entity and the forwarding devices. Such functionalities also help network operators detect and mitigate the security attacks to the network and provide better security level when compared to the traditional networks. However, some security threats, particularly distributed denial of service (DDoS) attacks, keep their effectiveness in degrading the availability of the networks even if the networking paradigm have changed. Existing DDoS attack detection approaches for SDN are mainly based on statistical (threshold-based) and Machine Learning-based (ML) approaches. Considering the dynamic characteristics of the network traffic, finding a dynamic threshold is somehow problematic. On the other hand, finding an appropriate feature that can discriminate DDoS attack from normal traffic is challenging for ML-based approaches. Therefore, to address the aforementioned issues, in this work, we propose a DDoS attack detection and countermeasure scheme based on discrete wavelet transform (DWT) and auto-encoder neural network for SDN. The proposed scheme extracts statistical features from the wavelet transform to be processed by an auto-encoder neural network to detect samples of DDoS attack traffic. Later, to reduce the computational burden imposed by the neural network model, the average hit rate in the flow table of the switches is used to activate the DDoS detection of the scheme. We also provide a detailed performance analysis by considering the computational cost complexity of the algorithms proposed in scheme and the evaluation of the successful detection rate with simulations. Our experimental results show that the proposed scheme achieves high detection rate against DNS amplification, Network Time Protocol and TCP SYN flood attacks with a remarkably low false alarm rate.}
}


@article{DBLP:journals/cn/SasabeK22,
	author = {Masahiro Sasabe and
                  Masaki Kiyomitsu},
	title = {Analysis of minimum distribution time of two-class tit-for-tat-based
                  {P2P} file distribution},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109142},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109142},
	doi = {10.1016/J.COMNET.2022.109142},
	timestamp = {Mon, 28 Aug 2023 21:39:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SasabeK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for large-scale file distribution over the Internet has been increasing, such as software and its update data distribution. When a new and/or popular file is released, many users tend to simultaneously access distribution server(s), making them bottleneck. Several systems (e.g., Ubuntu and Windows Update) have started applying the peer-to-peer (P2P) file distribution paradigm where users’ devices (i.e., peers) assist the file distribution. Since peers will consume their access link capacities to upload fragments of the file (i.e., pieces) to others, an appropriate incentive mechanism should be designed to motivate peers to behave cooperatively. In this paper, we focus on the tit-for-tat (TFT) based P2P file distribution with two-class peers where equivalent amount of piece exchange between each pair of peers is guaranteed and peers are classified into two classes: peers with high upload capacity or those with low one. In particular, given the system parameters (e.g., the number of peers and the upload capacity distribution), we specify the bottleneck of the system and derive the corresponding minimum file distribution time, with the help of a fluid model. Through numerical results, we verify the validity of analytical results and compare the system performance of TFT based P2P file distribution with that of existing architectures: client–server file distribution and cooperative P2P file distribution.}
}


@article{DBLP:journals/cn/MertensGM22,
	author = {Joannes Sam Mertens and
                  Laura Galluccio and
                  Giacomo Morabito},
	title = {{MGM-4-FL:} Combining federated learning and model gossiping in WSNs},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109144},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109144},
	doi = {10.1016/J.COMNET.2022.109144},
	timestamp = {Wed, 20 Sep 2023 16:18:58 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MertensGM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Application of Federated learning (FL) as it is, in Wireless Sensor Networks (WSNs) in impractical. In fact, it requires the transmission of a large number of model parameters. Also, it implies the consumption of a massive amount of energy and communication resources at nodes located closer to the network elements that aggregate the models calculated by the federated learners. This is a well known problem in WSN called funneling effect. Solutions have been recently proposed that decentralize FL operations further by exploiting multihop communications. Nevertheless, the proposed approaches do not focus on the networking operations needed to support such decentralization in an efficient manner. Objective of this paper is to fill this gap by proposing an integrated networking/learning scheme, named Model Gossiping Method for FL, that supports distributed FL efficiently in wireless sensor networks.}
}


@article{DBLP:journals/cn/ZafarLZIH22,
	author = {Samra Zafar and
                  Zefeng Lv and
                  Nizam Hussain Zaydi and
                  Muhammad Ibrar and
                  Xiaopeng Hu},
	title = {{DSMLB:} Dynamic switch-migration based load balancing for software-defined
                  IoT network},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109145},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109145},
	doi = {10.1016/J.COMNET.2022.109145},
	timestamp = {Tue, 12 Dec 2023 15:18:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZafarLZIH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of multiple controllers in Software-Defined Internet of Things (SD-IoT) networks enhance the distributed control plane scalability and reliability. With the rapid advancement of IoT, millions of smart devices generate enormous amounts of IoT traffic, causing significant challenges for the network to acquire the better Quality-of-Service (QoS) to the user. The control plane load management is crucial due to the dynamic nature of IoT networks and the heterogeneity of IoT devices in terms of flow generation rate. The load distribution among SD-IoT controllers relies on their available capacity to ensure optimal load balancing performance. The dynamic switch migration method was proposed in prior findings to handle load balancing distribution among controllers. However, the migration efficiency of conventional approaches is limited due to inefficient load balancing performance of the control plane after the migration of switch, leading to additional migration overhead on the network. Taking this into account, we propose a dynamic switch migration-based load balancing approach (DSMLB) to prevent control plane overhead and distribute traffic efficiently while considering heterogeneous IoT devices. The proposed DSMLB scheme measures the real-time load ratio to the mean load ratio of each controller by describing various load metrics and selecting the target controller by optimizing the migration efficiency with residual controller resource utilization. When choosing the candidate switches from the associated overloaded controller, DSMLB considers the migration efficiency and load balancing degree simultaneously for the fastest load reduction and enhances the load balancing performance. The simulation results show that DSMLB outperforms conventional approaches for efficient load balancing performance on a distributed control plane by reducing the controller response time up to 38.6% and migration cost about 45.5% on average. Moreover, DSMLB also improves controller load balancing rate and reduces communication overhead.}
}


@article{DBLP:journals/cn/HabibiB22,
	author = {Mehdi Habibi and
                  Hamzeh Beyranvand},
	title = {Reducing blocking probability and QoT violation in dynamic elastic
                  optical networks via load-aware margin selection},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109146},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109146},
	doi = {10.1016/J.COMNET.2022.109146},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HabibiB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the effect of load-aware margin selection on the performance of elastic optical networks (EONs) in dynamic service provisioning. First, a mixed-integer linear programming (MILP) formulation is proposed as a benchmark for dynamic routing, modulation level, spectrum, and power assignment (RMSPA). In this formulation, nonlinear interferences (NLIs) in optical fibers and the noise of amplifiers are considered. In addition, a margin which is calculated based on the network load is considered to estimate the additional noise which may degrade the signal quality of transmission (QoT) during its life time. The objective of this formulation is to minimize the spectrum usage and impairments of/from the signal which is optimized by choosing the optimum channel among a number of pre-calculated channels based on the QoT constraints. In order to reduce the RMSPA run time, heuristic algorithms are also proposed. The algorithms employ K-least weighted paths for routing. The defined weights are distance, the amount of traffic load, and a mixture scheme. Furthermore, the algorithms try to minimize the spectrum usage by choosing the highest possible modulation level while considering a specific criterion. The assumed criteria are: i) choosing the lowest possible power spectral density (PSD) in a discrete set, ii) selecting the optimum power among the set, iii) obtaining maximum SNR. We simulated the proposed algorithms and the MILP, and compared their performances on NSFNET and US Backbone topologies. The results reveal that the constrained best SNR scheme has the best performance among the heuristics and its performance is close to MILP, especially in higher loads. Moreover, results show that considering the links congestion leads to blocking reduction. We also simulated the first-fit based algorithms without considering NLIs which assign the modulation based on: i) considering only amplifier noise and self-channel interference (SCI), ii) experimental distance thresholds, and iii) overestimating the NLI. The results imply that the proposed MILP and heuristic algorithms outperform the other algorithms in terms of the summation of outage (probability of QoT violation) and blocking probability.}
}


@article{DBLP:journals/cn/CaoXCHJ22,
	author = {Yibo Cao and
                  Shiyuan Xu and
                  Xue Chen and
                  Yunhua He and
                  Shuo Jiang},
	title = {A forward-secure and efficient authentication protocol through lattice-based
                  group signature in VANETs scenarios},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109149},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109149},
	doi = {10.1016/J.COMNET.2022.109149},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CaoXCHJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Message authentication has been a research hotspot in current vehicular ad hoc networks (VANETs). Many researchers adopt group signatures based on number-theoretic assumptions to authenticate the vehicular users’ identities. Nevertheless, the classical group signature is vulnerable to quantum computing attacks and without considering the negative consequences of secret key disclosure. In this paper, to address these problems, we propose a novel group signature protocol for authentication in VANETs, which is based on lattice cryptography to achieve quantum-resistance and Bonsai-tree signature architecture to achieve forward security. Our scheme is proven secure in terms of traceability, anonymity, and forward-security under the Short Integer Solution (SIS) and Learning With Errors (LWE) hardness. Through comprehensive performance evaluation, we demonstrate that the storage overhead of our scheme is relatively diminutive and the computation cost of the sign and verify algorithms are efficient and practical compared with other existing schemes.}
}


@article{DBLP:journals/cn/LiIZ22,
	author = {Nan Li and
                  Alexandros Iosifidis and
                  Qi Zhang},
	title = {Collaborative edge computing for distributed {CNN} inference acceleration
                  using receptive field-based segmentation},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109150},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109150},
	doi = {10.1016/J.COMNET.2022.109150},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiIZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies inference acceleration using distributed CNNs in collaborative edge computing network. To ensure no inference accuracy loss in task partitioning, we propose receptive field-based segmentation. To reduce the computation time and communication overhead, we propose a novel collaborative edge computing using fused-layer parallelization to partition a CNN model into multiple blocks. To find the optimal partition of a CNN model, we use dynamic programming, named as DPFP. To address computation heterogeneity of edge servers (ESs), we design a low-complexity search algorithm which can select the optimal subset of collaborative ESs for inference. The experimental results show that DPFP can accelerate inference up to 71% for ResNet-50 and 73% for VGG-16 compared to running the pre-trained models, which outperforms the existing works MoDNN and DeepSlicing. Moreover, we propose an analytical method to estimate the speedup ratio of different GPU platforms by using FLOPs and effective computing capacity. Furthermore, we evaluate the service failure probability under time-variant channel and variation of image sizes, which shows that DPFP is effective to ensure high service reliability with strict service deadline.}
}


@article{DBLP:journals/cn/PicanoF22,
	author = {Benedetta Picano and
                  Romano Fantacci},
	title = {Human-in-the-loop virtual reality offloading scheme in wireless 6G
                  Terahertz networks},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109152},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109152},
	doi = {10.1016/J.COMNET.2022.109152},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PicanoF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Upcoming sixth-generation networks have to be carefully designed to offer support to a wide range of human-centric applications, typically computationally intensive and heavily dependent on human perception. This paper proposes a human-in-the-loop-based resource allocation approach found on the joint use of matching theory and stochastic network calculus principles to efficiently address tasks offloading issues within a sixth-generation network environment. The considered matching game approach is characterized by externalities, whose stability is properly discussed here. In particular, by focusing on virtual reality services, the paper aims at maximizing the average service reliability in terms of ability in guaranteeing a proper quality of experience target, i.e., keeping the end-to-end delay below a suitable threshold, by the proper exploitation of the human cognitive limitations. Finally, the effectiveness of the proposed human-in-the-loop-based resource allocation approach is verified by proving comparisons with simulation results and conventional brain-agnostic strategies.}
}


@article{DBLP:journals/cn/SouzaWMLWG22,
	author = {Cristiano Antonio de Souza and
                  Carlos Becker Westphall and
                  Renato Bobsin Machado and
                  Leandro Loffi and
                  Carla Merkle Westphall and
                  Guilherme Arthur Geronimo},
	title = {Intrusion detection and prevention in fog based IoT environments:
                  {A} systematic literature review},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109154},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109154},
	doi = {10.1016/J.COMNET.2022.109154},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SouzaWMLWG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, the Internet of Things is spreading in all areas that apply computing resources. An important ally of the IoT is fog computing. It extends cloud computing and services to the edge of the network. Smart environments are becoming real and possible through IoT and fog computing. However, they are not free from security threats and vulnerabilities. This makes special security techniques indispensable. Security is one of the biggest challenges to ensuring an optimal IoT and Fog environment. Combined with the significant damage generated by application attacks, this fact creates the need to focus efforts in this area. This need can be proven through existing reviews of the state-of-the-art that pointed out several open aspects that need greater research effort. In this way, this article presents a Systematic Literature Review (SLR) considering the context of intrusion detection and prevention in environments based on fog computing and IoT. This review addresses more than 100 studies that were included after undergoing an extensive inclusion/exclusion process with well-defined criteria. From these studies, information was extracted to build a view of the current state-of-the-art and answer the research questions of this study. In this way, we identify the state-of-the-art, open questions and possibilities for future research.}
}


@article{DBLP:journals/cn/DuanLZL22,
	author = {Jian{-}Hui Duan and
                  Wenzhong Li and
                  Xiao Zhang and
                  Sanglu Lu},
	title = {Forecasting fine-grained city-scale cellular traffic with sparse crowdsourced
                  measurements},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109156},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109156},
	doi = {10.1016/J.COMNET.2022.109156},
	timestamp = {Wed, 14 Dec 2022 11:41:06 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DuanLZL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless technology and the edge computing applications, an increasing number of 4G/5G infrastructure are densely deployed to meet the booming cellular traffic demands. Monitoring and forecasting urban cellular traffic is fundamental for urban planning, network resources allocation, traffic engineering, etc. In this paper, we address the crowdsourcing-based urban cellular traffic prediction problem, i.e., to predict the city-scale fine-grained cellular traffic patterns based on partial user-generated measurements. We propose a novel deep generative adversarial network (GAN) model called CrowdGAN to solve the problem. Specifically, CrowdGAN employs a convolutional Long Short-Term Memory (LSTM) network to extract spatio-temporal features from sparse traffic maps, and adopts a novel design of co-training a generator and a discriminator under the supervision of an accuracy assurance network to generate a high-resolution cellular traffic map for prediction. We implement the proposed CrowdGAN in TensorFlow and evaluate its performance using two real-world cellular traffic datasets. Extensive experiments show that CrowdGAN significantly outperforms the baselines on a variety of performance metrics, and achieves at least 47% reduction in root-mean-squared error compared to the state-of-the-art.}
}


@article{DBLP:journals/cn/ChenSCZC22,
	author = {Wenchao Chen and
                  Guanqun Shen and
                  Kaikai Chi and
                  Shubin Zhang and
                  Xiaolong Chen},
	title = {{DRL} based partial offloading for maximizing sum computation rate
                  of FDMA-based wireless powered mobile edge computing},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109158},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109158},
	doi = {10.1016/J.COMNET.2022.109158},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenSCZC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since most Internet of Things (IoT) nodes are powered by small-capacity batteries and equipped with small computing units, it is difficult for them to process computing-intensive and delay-sensitive tasks. To address these two limitations, a feasible approach is to use both wireless power transmission (WPT) and mobile edge computing (MEC) technologies. Wireless edge devices (EDs) have stable energy supply by capturing radio frequency (RF) energy transmitted by the energy source and offloading computation workload to edge computing servers (ECSs) to enhance the computation rate. In this paper, an FDMA-based wireless-powered MEC network is considered, which consists of an ECS, an RF energy source and multiple EDs. The research goal is to maximize the sum computation rate (SCR) by jointly optimizing the WPT duration, energy allocation for local computation, and computation offloading, and bandwidth allocation among EDs. We first formulate this as a non-convex optimization problem that is hard to address. Then, we decompose it into a top-problem for optimizing the WPT duration and a sub-problem for optimizing the energy allocation and bandwidth allocation under a given WPT duration. An online offloading algorithm based on deep reinforcement learning (DRL) is proposed to quickly obtain the near-optimal WPT duration. For the sub-problem, we design an efficient two-stage algorithm to obtain the optimal solution. The simulation results show that the proposed algorithm can obtain the near-optimal solutions with low computation complexity.}
}


@article{DBLP:journals/cn/MarkovitzS22,
	author = {Oren Markovitz and
                  Michael Segal},
	title = {{LEO} satellite beam management algorithms},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109160},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109160},
	doi = {10.1016/J.COMNET.2022.109160},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MarkovitzS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A stepping (or tracking) beam LEO constellation provides service to predefined areas instead of full coverage. As a result, satellites using stepping beams are more efficient, as power is used only for populated areas. When the constellation shifts, each satellite adjusts its beams locations and sizes, such that each area is serviced by one of the satellites that has the area in its FoV. Current algorithms do not scale well and require considerable satellite compute resources.}
}


@article{DBLP:journals/cn/BomgniSTVM22,
	author = {Alain Bertrand Bomgni and
                  Miguel Landry Foko Sindjoung and
                  Dhalil Kamdem Tchibonsou and
                  Mthulisi Velempini and
                  Jean Fr{\'{e}}d{\'{e}}ric Myoupo},
	title = {{NESEPRIN:} {A} new scheme for energy-efficient permutation routing
                  in IoT networks},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109162},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109162},
	doi = {10.1016/J.COMNET.2022.109162},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BomgniSTVM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) consists of a variety of heterogeneous interconnected devices called objects or things. These objects are generally equipped with sensing, processing and wireless communication capabilities. Unfortunately, these capabilities are not enough compared to those of devices in traditional networks. For example, objects have low battery power, limited memory storage, and less processing power. There are some cases where objects have to communicate with each other. In an Autonomous Vehicular Network for example, when a vehicle needs to change its direction, it has to alert other vehicles in the same network. This is known as the permutation routing problem. More precisely, the permutation routing problem in IoT occurs when some things of the network possess items that belong to others. The goal is to send items to their respective owners. A number of solutions to the problem have been proposed in literature which focus mainly on Wireless Sensor Networks (where the memory size of objects are the same). In this paper, we propose an efficient permutation routing scheme for a single-hop IoT network (the memory size differs from one object to another). The proposed NESEPRIN protocol consists of two phases: In the first phase, we solve the permutation routing problem in a single-hop environment with a single channel, and secondly we generalize the previous solution to a network with multiple channels. Our solution makes use of the wake and sleep technique to improve the energy conservation of objects. The simulation results show that our protocol outperforms the existing protocols designed to solve the permutation routing problem in terms of energy saving when the volume of data to route is large. NESEPRIN is the better candidate to solve the permutation routing problem in a single-hop multi-channels IoT environment where the volume of data to route is huge.}
}


@article{DBLP:journals/cn/WangZLZWG22,
	author = {Jiacheng Wang and
                  Jianhui Zhang and
                  Liming Liu and
                  Xuzhao Zheng and
                  Hanxiang Wang and
                  Zhigang Gao},
	title = {Utility Maximization for Splittable Task Offloading in IoT Edge Network},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109164},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109164},
	doi = {10.1016/J.COMNET.2022.109164},
	timestamp = {Wed, 07 Dec 2022 16:51:05 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangZLZWG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper comprehensively investigates spatio-temporal dynamics for task offloading in the Internet of Things (IoT) Edge Network (iTEN) in order to maximize utility. Different from the previous works in the literature that only consider partially dynamic factors, this paper takes into account the time-varying wireless link quality, communication power, wireless interference on task offloading, and the spatiotemporal dynamics of energy harvested by terminals and their charging efficiency. Our goal is to maximize utility during the task offloading by considering the above-mentioned factors, which are relatively complex but closer to reality. This paper designs the Time-Expanded Graph (TEG) to transfer network dynamics and wireless interference into some static weight in the graph so as to devise the algorithm easily. This paper firstly devises the Single Terminal (ST) utility maximization algorithm on the basis of TEG when there is only one terminal. In the case of multiple terminals, it is very complicated to directly solve the utility maximization of the task offloading. This paper adopts the framework of Garg and Könemann and devises a multi-terminal algorithm (MT) to maximize the total utility of all terminals. MT is a fast approximate algorithm and its approximate ratio is 1-3\nς\n, where\n0\n<\nς\n<\n1\n/\n3\nis a positive small constant. The comprehensive experiments are conducted to illustrate that our algorithm significantly improves the overall utility compared to the three basic algorithms.}
}


@article{DBLP:journals/cn/AlmasanXCSBC22,
	author = {Paul Almasan and
                  Shihan Xiao and
                  Xiangle Cheng and
                  Xiang Shi and
                  Pere Barlet{-}Ros and
                  Albert Cabellos{-}Aparicio},
	title = {{ENERO:} Efficient real-time {WAN} routing optimization with Deep
                  Reinforcement Learning},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109166},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109166},
	doi = {10.1016/J.COMNET.2022.109166},
	timestamp = {Tue, 28 Feb 2023 10:48:05 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlmasanXCSBC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wide Area Networks (WAN) are a key infrastructure in today’s society. During the last years, WANs have seen a considerable increase in network’s traffic and network applications, imposing new requirements on existing network technologies (e.g., low latency and high throughput). Consequently, Internet Service Providers (ISP) are under pressure to ensure the customer’s Quality of Service and fulfill Service Level Agreements. Network operators leverage Traffic Engineering (TE) techniques to efficiently manage the network’s resources. However, WAN’s traffic can drastically change during time and the connectivity can be affected due to external factors (e.g., link failures). Therefore, TE solutions must be able to adapt to dynamic scenarios in real-time.}
}


@article{DBLP:journals/cn/TsiropoulouDFLR22,
	author = {Eirini{-}Eleni Tsiropoulou and
                  Christos Douligeris and
                  Luca Foschini and
                  Gang Li and
                  Theofanis P. Raptis},
	title = {Guest Editorial: 26th {IEEE} symposium on computers and communications
                  {(ISCC} 2021) selected papers},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109167},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109167},
	doi = {10.1016/J.COMNET.2022.109167},
	timestamp = {Wed, 08 Nov 2023 07:52:01 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TsiropoulouDFLR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/cn/QianCZTD22,
	author = {Mimi Qian and
                  Lin Cui and
                  Xiaoquan Zhang and
                  Fung Po Tso and
                  Yuhui Deng},
	title = {\emph{dDrops}: Detecting silent packet drops on programmable data
                  plane},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109171},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109171},
	doi = {10.1016/J.COMNET.2022.109171},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/QianCZTD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Silent packet drops are common in data center networks, and are a major cause of network performance anomalies (NPAs) that have significant impacts on application performance and network management. However, existing solutions using coarse-grained statistics and flow-level telemetry either fail to provide precise location of packet drops or incur large overhead. This paper presents dDrops, a packet-level telemetry based on programmable data plane to detect and retrieve details of silent packet drops immediately when they happen. dDrops can dynamically adapt to varying ratios of silent packet drops for different ports on a switch to improve performance of silent packet drops detection. Moreover, a dynamic memory management scheme is also designed to efficiently use the limited memory on the data plane of switch. dDrops has been implemented on both P4 hardware programmable switches (based on Intel Tofino ASIC) and BMv2. Extensive experiment results show that dDrops is able to detect and locate the silent packet drops within 5 ms (including detailed information of dropped packets), and reduce the memory consumption by up to 50%.}
}


@article{DBLP:journals/cn/JurdakCPBD22,
	author = {Raja Jurdak and
                  Juan M. Corchado and
                  Jong Hyuk Park and
                  Chintan M. Bhatt and
                  Kapal Dev},
	title = {Editorial: Blockchain based sustainable, secure healthcare systems},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109175},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109175},
	doi = {10.1016/J.COMNET.2022.109175},
	timestamp = {Wed, 14 Jun 2023 17:58:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JurdakCPBD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/cn/WangWLJZ22,
	author = {Zhifei Wang and
                  Xiangming Wen and
                  Zhaoming Lu and
                  Wenpeng Jing and
                  Yujing Zhang},
	title = {Random access optimization for initial access and seamless handover
                  for 5G-satellite network},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109176},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109176},
	doi = {10.1016/J.COMNET.2022.109176},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangWLJZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite communication is becoming an indispensable component of the 5G global networks. Considering the unique characteristics of the satellite communications such as long round trip delay, large cell radius and high satellite speed, initial access and handover are necessary to be improved to guarantee the stable communication for 5G-satellite network, which usually use the random access procedure in traditional terrestrial communication and is not suitable for satellite scenario. Therefore, a 2-step random access for 5G-satellite network (TRAS) and a seamless MC-controlled handover for 5G-satellite network (SMHOS) are proposed in this paper. In particular, the access delay can be reduced and the success rate of access can be improved since the access procedure is simplified and the contention occurs only when the UEs select the same Physical Random Access Channel (PRACH) occasion and Physical Uplink Shared Channel (PUSCH) occasion in TRAS. Moreover, in SMHOS, the management center (MC) can predict the handover of the UE and inform the next satellite to send a broadcast message with embedded information about the reserved PUSCH time–frequency resource. The UE can access to the target satellite immediately when the handover occurs, therefore, seamless handover is realized. Simulation results demonstrate the effectiveness of our TRAS and SMHOS.}
}


@article{DBLP:journals/cn/DongC22,
	author = {Xiaodong Dong and
                  Binlei Cai},
	title = {Slardar: Scheduling information incomplete inter-datacenter deadline-aware
                  coflows with a decentralized framework},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109178},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109178},
	doi = {10.1016/J.COMNET.2022.109178},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DongC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, geographically distributed edge clusters and datacenters generate large volumes of wide-area network (WAN) traffic. Therefore, it is critical to schedule inter-datacenter flows, especially coflows that imply application-level semantics, to improve the performance of geo-distributed applications. However, coflows often have a mixture of soft and hard deadline requirements, which cannot be efficiently guaranteed by existing methods. In this paper, a decentralized deadline-ware coflow scheduling framework, called Slardar, is proposed. It schedules inter-datacenter coflows while considering both hard and soft deadline requirements. However, it is challenging to design such a framework without complete coflow information. Hence, we first model the intra-coflow bandwidth allocation problem as a mean-field game where each datacenter aims to minimize its own cost with only local coflow information. We further prove its mean-field equilibrium is also the optimal bandwidth allocation strategy and propose a deep reinforcement learning based algorithm to find an approximate optimal numerical solution. Then, we investigate the offline inter-coflow bandwidth allocation problem by modeling it as a linear programming problem, and an online greedy heuristic algorithm is proposed to find an approximate optimal solution. Finally, we compare Slardar with state-of-art methods by large-scale simulation experiments. Experimental results show that our proposed method can greatly improve the transmission revenue and increase the soft and hard deadline guarantee ratios by 7.5% and 10% at least.}
}


@article{DBLP:journals/cn/Eltaief22,
	author = {Hamdi Eltaief},
	title = {Flex-CC: {A} flexible connected chains scheme for multicast source
                  authentication in dynamic {SDN} environment},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109179},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109179},
	doi = {10.1016/J.COMNET.2022.109179},
	timestamp = {Sat, 10 Sep 2022 21:00:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Eltaief22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multicast service has the advantage of saving the resources of senders. It ensures the efficiency of the network by lowering network share, but it is necessary to address security issues. Source authentication is one of the basic requirements for multicast applications such as distribution data of stock quotes, video-streaming, online news, IPTV, etc. Despite many efforts in providing source authentication for multicast applications, no efficient multicast source authentication mechanism that can deal with the requirements of dynamic environments has been proposed. In this paper, we analyze source authentication mechanisms with non-repudiation from 20 years of research. Most of them do not provide an efficient mechanism that takes into account the fact that users and networks are dynamic. Taking advantage of the centralized view of the control plane and the contextual data from the data plane in Software Defined Network (SDN), we propose a new flexible multicast source authentication scheme called Flexible Connected Chains (Flex-CC). It is an efficient mechanism that can deal with the requirements of dynamic environments, ensures non-repudiation, and tolerates packet loss. The simulation results show that the proposed mechanism works efficiently in resource utilization with reduced communication and delay overhead in a dynamic scenario.}
}


@article{DBLP:journals/cn/FuZ22,
	author = {Juan{-}juan Fu and
                  Xing{-}Lan Zhang},
	title = {Gradient importance enhancement based feature fusion intrusion detection
                  technique},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109180},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109180},
	doi = {10.1016/J.COMNET.2022.109180},
	timestamp = {Fri, 17 Feb 2023 18:27:30 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FuZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to better deal with the current unstable network security situation and improve the accuracy and generalization ability of the intrusion detection model, this paper proposes a feature fusion technique based on gradient importance enhancement, which combines feature fusion and feature enhancement to increase the diversity of sample features, so that the model can focus more on the sample features related to classification, making the model more generalizable and improving the accuracy of the model. The final model was experimented on two datasets, NSL-KDD and CICIDS2017, and its accuracy reached 99.84% and 99.78%, respectively.}
}


@article{DBLP:journals/cn/WangYBT22,
	author = {Cen Wang and
                  Noboru Yoshikane and
                  Filippos Balasis and
                  Takehiro Tsuritani},
	title = {{OSDL:} Dedicated optical slice provisioning in support of distributed
                  deep learning},
	journal = {Comput. Networks},
	volume = {214},
	pages = {109191},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2022.109191},
	doi = {10.1016/J.COMNET.2022.109191},
	timestamp = {Fri, 02 Sep 2022 19:16:38 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangYBT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Networks are the well-known bottlenecks for distributed deep learning (DDL) jobs. The DDL jobs require topologies matching their communication patterns as well as high and stable bandwidth in network transmission, which makes the traditional optimization methods in the field of electrical packet switching (EPS) perform poorly when adapting to the DDL jobs. In contrast, optical circuit switching (OCS) has the natural advantages of topology reconstruction and high bandwidth, which can well fulfill the network requirements and reduce the in-network delay of the DDL jobs. Consequently, in the present work, we propose Optical Slicing provisioning in support of DDL jobs (OSDL). Practically, over an EPS + OCS hybrid network, to match the network topology and the traffic patterns of DDL jobs, we comprehensively design and propose corresponding algorithms for job placement, and scheduling of both OCS and EPS. We first evaluate OSDL with large-scale networks and high-density DDL jobs, by simulations. The simulation results show that OSDL outperforms multiple well-known scheduling methods. More specifically, the speedup of OSDL achieves up to 3.31 times and 10.48 times, over a homogeneous network and a heterogeneous network, respectively. Additionally, we conduct experiments with a relatively small-scale network and relatively low-density DDL jobs, we still observe up to 1.74 times of the speedup. The simulations and the experiments verify the effectiveness of OSDL in accelerating DDL jobs.}
}
