@article{DBLP:journals/cn/SiasiJG24,
	author = {Nazli Siasi and
                  Mohammed Jasim and
                  Nasir Ghani},
	title = {Post-fault restoration of service function chains in fog networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110580},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110580},
	doi = {10.1016/J.COMNET.2024.110580},
	timestamp = {Fri, 19 Jul 2024 23:18:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SiasiJG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The network function virtualization (NFV) paradigm is rapidly being adopted by enterprises and service providers as an alternative to proprietary hardware. It enables virtual network functions (VNFs) to be embedded on commercial edge servers for the delivery of highly-specialized service function chains (SFCs). Here embedding SFC requests on fog computing supports clients with stringent delay requirements and contextual service requirements. Now one of the major challenges is the survivability of embedded service chains in fog computing, which may be compromised due to node, link, and application failures. These failures can disrupt service continuity and degrade the quality-of-service. Therefore, there is a demand for SFC provisioning to be more resilient, incorporating rapid restoration techniques that oppose proactive solutions that reserve backup paths, subsequently compromising network capacity. Hence, this paper presents heuristic methods for post-fault SFC restorations due to multiple failures in fog computing. These methods establish post-provisioning restoration for failed requests, known as end-to-end (E2E) and intermediate (IM) restoration, that aim to restore paths with the least delay and cost.}
}


@article{DBLP:journals/cn/RustamAEJ24,
	author = {Furqan Rustam and
                  Wajdi Aljedaani and
                  Mahmoud Said Elsayed and
                  Anca Delia Jurcut},
	title = {{FAMTDS:} {A} novel MFO-based fully automated malicious traffic detection
                  system for multi-environment networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110603},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110603},
	doi = {10.1016/J.COMNET.2024.110603},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/RustamAEJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-environment networks, such as those in smart homes, handle both IoT and traditional IP-based traffic. Weak security protocols in IoT devices and the diverse traffic flow make these networks vulnerable to security breaches. This study delves into this pressing challenge and presents a pioneering solution—FAMTDS (Fully Automated Malicious Traffic Detection System)—designed explicitly for multi-environment networks. FAMTDS addresses the critical need for robust security measures by intelligently analyzing the amalgamation of IoT and IP-based traffic. FAMTDS comprises three pivotal stages: innovative multi-environment dataset creation, optimization of machine learning model hyperparameters, and holistic system optimization. For the multi-environment dataset, we amalgamate two prominent open-source datasets, UNSW-NB-15 and IoTID-20. Initially, crucial features are extracted from both datasets using an extra trees classifier. Subsequently, an equal number of features is generated by employing a neural network to harmonize these datasets. The resulting multi-environment dataset encompasses 19 distinct attack types, a comprehensive inclusion unprecedented in prior research on malicious traffic detection. This dataset exhibits diversity owing to its varied traffic samples, addressing a crucial gap in existing studies. To accommodate this diversity, machine learning models are deployed with fine-tuned hyperparameters. The Mouth Flame Optimizer streamlines feature extraction, feature generation, and hyperparameter tuning, automating the optimization process. FAMTDS demonstrates exceptional performance, achieving an accuracy score of 0.85 for the multi-environment dataset. We also integrated the CICDDOS2019 dataset with IoTID-20 in our multi-environment dataset, achieving a notable accuracy of 0.82 against recent attacks, thus enhancing our approach’s validation. To further validate the generalizability of our proposed approach, we applied it to zero-day attack prediction. Our method demonstrated an accuracy of 0.84 for zero-day attacks, indicating its effectiveness in detecting newly emerging threats in multi-environment networks.}
}


@article{DBLP:journals/cn/GuoGCZTL24,
	author = {Jiaxing Guo and
                  Chunxiang Gu and
                  Xi Chen and
                  Xieli Zhang and
                  Kai Tian and
                  Ji Li},
	title = {Stateful black-box fuzzing for encryption protocols and its application
                  in IPsec},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110605},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110605},
	doi = {10.1016/J.COMNET.2024.110605},
	timestamp = {Mon, 29 Jul 2024 07:57:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GuoGCZTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to the rapid development of information security technology, the security analysis of encryption protocols has received widespread attention. In this paper, we propose a stateful black-box encryption protocol fuzzing method to analyze the security of real-world black-box encryption protocol programs and devices. This method does not rely on the source code but uses captured packets as input, performs state selection based on a Monte Carlo tree search algorithm, and processes the encryption/decryption conversion of mutant test cases based on the intermediate mapper. It sends test cases and collects responses in interactive communication with the tested program and dynamically optimizes the corpus based on the collected state information. Based on this method, we develop SBEPFuzz and primarily analyze IPsec. We evaluate SBEPFuzz on six widely used IPsec implementations. The experimental results show that SBEPFuzz achieves higher code coverage, and can discover more protocol state sequences and vulnerabilities. Furthermore, we discover four anomalies, including malformed packets triggering service crashes and abnormal interactions leading to plaintext ID leakage, which also reflect the differences in details among different implementations.}
}


@article{DBLP:journals/cn/ZhengCTLJ24,
	author = {Yuxin Zheng and
                  Lin Cui and
                  Fung Po Tso and
                  Zhetao Li and
                  Weijia Jia},
	title = {{DNN} acceleration in vehicle edge computing with mobility-awareness:
                  {A} synergistic vehicle-edge and edge-edge framework},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110607},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110607},
	doi = {10.1016/J.COMNET.2024.110607},
	timestamp = {Fri, 19 Jul 2024 23:18:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhengCTLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, vehicular networks have seen a proliferation of applications and services such as image tagging, lane detection, and speech recognition. Many of these applications rely on Deep Neural Networks (DNNs) and demand low-latency computation. To meet these requirements, Vehicular Edge Computing (VEC) has been introduced to augment the abundant computation capacity of vehicular networks to complement limited computation resources on vehicles. Nevertheless, offloading DNN tasks to MEC (Multi-access Edge Computing) servers effectively and efficiently remains a challenging topic due to the dynamic nature of vehicular mobility and varying loads on the servers. In this paper, we propose a novel and efficient distributed DNN Partitioning And Offloading (DPAO), leveraging the mobility of vehicles and the synergy between vehicle–edge and edge–edge computing. We exploit the variations in both computation time and output data size across different layers of DNN to make optimized decisions for accelerating DNN computations while reducing the transmission time of intermediate data. In the meantime, we dynamically partition and offload tasks between MEC servers based on their load differences. We have conducted extensive simulations and testbed experiments to demonstrate the effectiveness of DPAO. The evaluation results show that, compared to offloaded all tasks to MEC server, DPAO reduces the latency of DNN tasks by 2.4x. DPAO with queue reservation can further reduce the task average completion time by 10%.}
}


@article{DBLP:journals/cn/LuYG24,
	author = {Xinyue Lu and
                  Yifan Yang and
                  Wei Gong},
	title = {Challenges of ambient WiFi backscatter systems in healthcare applications},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110608},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110608},
	doi = {10.1016/J.COMNET.2024.110608},
	timestamp = {Fri, 19 Jul 2024 23:18:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuYG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In healthcare, patients commonly use medical devices that require batteries and wired connections, and do not have timely access to monitoring data. Ambient WiFi backscatter systems are attracting attention in healthcare owing to their ultra-low power consumption, wireless communication method, battery-free mode, and the ability for continuous monitoring. However, there is a lack of investigations on it. In this paper, we focus on discovering ambient WiFi backscatter systems’ applications in healthcare. Therefore, in this paper, we provide a comprehensive survey of ambient WiFi backscatter in healthcare. Firstly, we present some existing ambient WiFi backscatter systems to help us better understand the operating principle, and then classify them based on the use of commercial WiFi receivers. After that, we survey their latest applications and discuss the challenges in healthcare, starting from two general directions: smart hospitals and medical devices. Finally, we foresee the development of ambient WiFi backscatter technology and expand it in other fields. The research aims to receive a comprehensive understanding of the applications and challenges of ambient WiFi backscatter systems in healthcare, and thus promoting their further development.}
}


@article{DBLP:journals/cn/XiongXWXS24,
	author = {Runqun Xiong and
                  Qing Xiao and
                  Zhoujie Wang and
                  Zhuqing Xu and
                  Feng Shan},
	title = {Leveraging lightweight blockchain for secure collaborative computing
                  in {UAV} Ad-Hoc Networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110612},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110612},
	doi = {10.1016/J.COMNET.2024.110612},
	timestamp = {Fri, 19 Jul 2024 23:18:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XiongXWXS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicle (UAV) Ad-Hoc Networks (UANET) enable collaborative work among UAVs for versatile task execution, but they face security challenges due to physical vulnerabilities, software issues, and dynamic wireless networks. This paper proposes a secure collaborative computing framework based on blockchain technology. Specifically, we first design a lightweight blockchain scheme suitable for UANET and present an improved Practical Byzantine Fault Tolerance (PBFT) consensus algorithm based on trust evaluation, aiming to reduce consensus overhead and establish trust relationships among UAVs. Furthermore, we devise a smart contract-based UAV task allocation strategy that considers both task execution efficiency and offloading security. This strategy enables UAVs to make optimal task-offloading decisions and facilitates collaborative computing according to smart contract rules. Simulation results demonstrate that our proposed consensus algorithm reduces average consensus latency by 47% and message count by 76% compared to PBFT-based algorithms. Additionally, the task allocation strategy decreases task costs by 48% compared to local computing strategies, achieving efficient and secure collaboration among UAVs in UANET. The real-world experiments with a UAV swarm further validate the efficiency and security of our framework, confirming its practical applicability in UANET.}
}


@article{DBLP:journals/cn/ChenXS24,
	author = {Zhenping Chen and
                  Miaomiao Xu and
                  Chunxia Su},
	title = {Online quality-based privacy-preserving task allocation in mobile
                  crowdsensing},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110613},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110613},
	doi = {10.1016/J.COMNET.2024.110613},
	timestamp = {Fri, 19 Jul 2024 23:18:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenXS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aimed at the existing problems, such as the improving requirements on privacy, and the low quality of collected data in mobile crowd sensing, one online Quality-based Privacy-preserving Task Allocation (QPTA) mechanism is considered in this paper. First, a travel budget-related polynomial is designed to measure the influence of candidate tasks on the whole task quality, and an online task quality-based allocation model is designed to maximize the total task quality when both the budget and time constraints can be satisfied simultaneously. Second, when the perceived data of task participants and requesters are encrypted values, with the introduction of typical Paillier homomorphic encryption technology, the division, the square root, and the Euclidean distance calculations of the encrypted values are then designed. In this way, the encrypt calculations of the task allocation algorithm are solved, and the data privacy of the task allocation system is thus preserved. Finally, the bilateral privacy analysis of the proposed method is illustrated, and some simulations are carried out to show the effectiveness of the proposed method. Simulation results show that the proposed method can allocate tasks effectively while guaranteeing the privacy of both the participants and requesters. It outperforms the most related methods in terms of average perception quality.}
}


@article{DBLP:journals/cn/ZhaoYGZL24,
	author = {Liang Zhao and
                  Yujun Yao and
                  Jianmeng Guo and
                  Qingjun Zuo and
                  Victor C. M. Leung},
	title = {Collaborative computation offloading and wireless charging scheduling
                  in multi-UAV-assisted {MEC} networks: {A} TD3-based approach},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110615},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110615},
	doi = {10.1016/J.COMNET.2024.110615},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhaoYGZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Computation offloading, resource allocation, and endurance issues in unmanned aerial vehicle (UAV)-aided mobile edge computing (MEC) networks have always been a research focus. UAV-aided MEC allows mobile users’ (MUs’) tasks to be offloaded to drones for processing in special scenarios, such as natural disasters or military attacks. However, as the number and size of offloaded tasks continuously increase, it is difficult for a single UAV to meet all computational demands, result in the decline of QoS. In order to address this issue, this paper presents a collaborative computation offloading scheme where multiple UAVs can cooperate to handle massive tasks. Firstly, considering that battery-limited UAVs cannot complete all tasks and sustain flight without recharging, we incorporate charging stations (CS) into multi-UAV-assisted MEC networks. Subsequently, we design a price-based incentive mechanism to encourage the adjacent UAVs with unused computation resources to cooperate with busy UAVs, so as to maximize the total revenue obtained from UAVs’ collaborative computation. Then, we formulate the joint optimization problem of computation offloading, resource allocation and charging scheduling as a Markov Decision Process (MDP), and propose a Twin Delayed Deep Deterministic policy gradient (TD3) algorithm to find optimal strategies. Finally, extensive simulations demonstrate that the proposed TD3 algorithm outperforms other benchmark methods, achieving the maximum overall system utility under different scenarios.}
}


@article{DBLP:journals/cn/PekarJ24,
	author = {Adri{\'{a}}n Pek{\'{a}}r and
                  Richard Jozsa},
	title = {Evaluating ML-based anomaly detection across datasets of varied integrity:
                  {A} case study},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110617},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110617},
	doi = {10.1016/J.COMNET.2024.110617},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/PekarJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impact of data integrity on ML efficacy. Our study underscores the importance of continual refinement and methodological rigor in dataset generation for network security research. As the landscape of network threats evolves, so must the tools and techniques used to detect and analyze them.}
}


@article{DBLP:journals/cn/ShlingbaumYKZ24,
	author = {Erez Shlingbaum and
                  Raz Ben Yehuda and
                  Michael Kiperberg and
                  Nezer Jacob Zaidenberg},
	title = {Virtualized network packet inspection},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110619},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110619},
	doi = {10.1016/J.COMNET.2024.110619},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShlingbaumYKZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network-based cyber attacks differ in their objectives, techniques, and levels of sophistication, yet they all maintain communication with their controllers. Current approaches to block unauthorized communication fall short or are susceptible to attacks at the kernel level. Our work showcases the feasibility of clandestine network transmissions across different network interface cards, utilizing solely data writes to physical pages. For certain cards, we employ a code-reuse attack to execute IO instructions. This paper presents Virtualized Packet Inspection (VPI), a virtualization-based solution for preventing malicious communication. VPI is embedded in QEMU-KVM, making it particularly suitable for private clouds. Being integrated into QEMU-KVM, VPI is not vulnerable to kernel-mode attacks. In addition, VPI’s ability to monitor the activity of user-mode applications and the network card, allows it to block malicious communications initiated by kernel-mode malware. Our evaluation shows that VPI’s performance overhead is\n≈\n20\n%\nfor monitored system calls, and is negligible in other cases.}
}


@article{DBLP:journals/cn/WangYX24,
	author = {Wenbo Wang and
                  Peng Yi and
                  Huikai Xu},
	title = {DoubleR: Effective {XSS} attacking reality detection},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110567},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110567},
	doi = {10.1016/J.COMNET.2024.110567},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-site scripting (XSS) attack has been one of the most dangerous attacks in cyberspace security. Traditional methods essentially discover XSS attack by detecting malicious payloads in requests, which is unable to distinguish blind&random scanning with the attacking reality. Moreover, it also brings tens of thousands of worthless security alerts to administrators, as well as unfriendly experience to users. In this paper, we propose DoubleR, a bi-directional framework which detects both Requests and Responses to discover XSS attacking reality. On the basis of conventional detection of malicious requests, DoubleR collects responses from web server and trains a bagging based PU learning model to determine whether the vulnerability is truly triggered. To validate our proposed framework, experiments are performed on 5 popular Web applications with 11 specified CVE recorded vulnerabilities. Results show that DoubleR effectively distinguishes attacking reality from attacking attempts, reduce the worthless security alarms, and at the same time works well on other web attacks of the same type.}
}


@article{DBLP:journals/cn/XuJLWZX24,
	author = {Ke Xu and
                  Donghong Jiang and
                  Yanbiao Li and
                  Xin Wang and
                  Dafang Zhang and
                  Gaogang Xie},
	title = {MaP: Increasing node capacity of programmable cloud gateways},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110590},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110590},
	doi = {10.1016/J.COMNET.2024.110590},
	timestamp = {Wed, 18 Sep 2024 14:53:45 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuJLWZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual routing table lookup is a crucial operation in multi-tenant cloud gateways, the essential devices to enable the elastic management of Virtual Private Clouds (VPCs). However, software-based cloud gateways face performance bottlenecks when handling ever-increasing traffic. To address this challenge, a recent trend involves accelerating virtual routing table lookups using programmable switches, with Sailfish being the state-of-the-art solution. Nonetheless, Sailfish’s table scale on a single programmable switch is limited by the ASIC memory sizes. We introduce a novel scheme, named MaP (Merge and Partition), to significantly increase the memory efficiency on a single Tofino 1.0 programmable switch. MaP exploits the similarities in routing table entries across various VPCs for table compression and leverages the hardware architecture of programmable chips to minimize the memory occupancy. Performance evaluations using real-world virtual routing tables demonstrate that our approach reduces the memory cost from 25% to 47.7% compared to Sailfish, without negatively impacting the throughput and latency. With MaP, a single programmable switch can manage a massive table containing over 2 million entries for nearly 1 million VPCs, while only occupying 59.0% of memory and 63.0% of stages.}
}


@article{DBLP:journals/cn/LiZLCGW24,
	author = {Jianbin Li and
                  Hengyang Zhang and
                  Shike Li and
                  Long Cheng and
                  Yiguo Guo and
                  Sixing Wu},
	title = {{BD-TTS:} {A} blockchain and DRL-based framework for trusted task
                  scheduling in edge computing},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110609},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110609},
	doi = {10.1016/J.COMNET.2024.110609},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiZLCGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing migrates tasks to the edge of the network for execution, which can provide users with lower latency and better quality of service (QoS). However, due to the complexity and dynamism of edge environments, many task scheduling algorithms in edge computing face challenges in achieving real-time scheduling, while there are also trust issues in heterogeneous and dynamic edge environments. To tackle these issues, we introduce a novel framework for trusted task scheduling in edge computing, leveraging blockchain and deep reinforcement learning (DRL) technologies, named BD-TTS. Specifically, we design a blockchain-based trust management scheme tailored for task scheduling in edge computing. The scheme uses blockchain to store, propagate, and update trust information in a decentralized manner to evaluate the trustworthiness of edge servers. In addition, to assign tasks to edge servers with higher trust values for execution, we introduce a DRL-driven task scheduling algorithm. The algorithm dynamically schedules tasks in real-time based on fluctuations in the trust values of edge servers. The experimental results show that compared to other baseline approaches, BD-TTS effectively reduces the number of tasks assigned to malicious edge servers by over 64.4%, reduces the average task response time by at least 13.9%, and improves the success rate by more than 14.7%.}
}


@article{DBLP:journals/cn/SagarMSZZP24,
	author = {Subhash Sagar and
                  Adnan Mahmood and
                  Quan Z. Sheng and
                  Wei Emma Zhang and
                  Yang Zhang and
                  Jitander Kumar Pabani},
	title = {Understanding the trustworthiness management in the social Internet
                  of Things: {A} survey},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110611},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110611},
	doi = {10.1016/J.COMNET.2024.110611},
	timestamp = {Fri, 07 Feb 2025 20:42:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SagarMSZZP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of the Internet of Things (IoT) facilitates the integration of the notion of social networking into smart objects, i.e., things, to establish a social network of interconnected objects. This integration has led to the evolution of a promising and emerging paradigm of the Social Internet of Things (SIoT), wherein smart objects act as social objects and intelligently impersonate social behaviour similar to humans. These social objects are capable of establishing social relationships with the other objects in a network and can utilize these relationships for service discovery. Trust plays an indispensable role in establishing and maintaining such social relationships to achieve the common goal of trustworthy collaboration and cooperation among the objects and guarantee systems’ credibility and reliability. In SIoT, an untrustworthy object can disrupt the basic functionality of a service by delivering malicious messages and adversely affecting the quality and reliability of the service. In this survey, we present a holistic review of trustworthiness management in SIoT. The essence of trust in various disciplines and the trust in SIoT have been discussed, followed by a detailed study on trust management components in SIoT. Furthermore, we analyse and compare the trust management schemes by primarily categorizing them into four groups in terms of their strengths, limitations, trust management components (employed in each of the referred trust management schemes), and the performance of these schemes vis-à-vis a number of trust evaluation dimensions. Finally, we discuss the future research directions of the emerging paradigm of SIoT, in particular, in the context of trustworthiness management in SIoT.}
}


@article{DBLP:journals/cn/SivaroopanBMJJT24,
	author = {Nirhoshan Sivaroopan and
                  Dumindu Bandara and
                  Chamara Madarasingha and
                  Guillaume Jourjon and
                  Anura P. Jayasumana and
                  Kanchana Thilakarathna},
	title = {NetDiffus: Network traffic generation by diffusion models through
                  time-series imaging},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110616},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110616},
	doi = {10.1016/J.COMNET.2024.110616},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SivaroopanBMJJT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While Machine-Learning based network data analytics are now commonplace for many networking solutions, nonetheless, limited access to appropriate networking data has been an enduring challenge for many networking problems. Causes for lack of such data include complexity of data gathering, commercial sensitivity, as well as privacy and regulatory constraints. To overcome these challenges, we present a Diffusion-Model (DM) based end-to-end framework, NetDiffus, for synthetic network traffic generation which is one of the emerging topics in networking and computing system. NetDiffus first converts one-dimensional time-series network traffic into two-dimensional images, and then synthesizes representative images for the original data. We demonstrate that NetDiffus outperforms the state-of-the-art traffic generation methods based on Generative Adversarial Networks (GANs) by providing 66.4% increase in the fidelity of the generated data and an 18.1% increase in downstream machine learning tasks. We evaluate NetDiffus on seven diverse traffic traces and show that utilizing synthetic data significantly improves several downstream ML tasks including traffic fingerprinting, anomaly detection and traffic classification. The code has been included at https://github.com/Nirhoshan/NetDiffus.}
}


@article{DBLP:journals/cn/MaXWDXZZM24,
	author = {Yunxiao Ma and
                  Changqiao Xu and
                  Zhonghui Wu and
                  Renjie Ding and
                  Han Xiao and
                  Lujie Zhong and
                  Yirong Zhuang and
                  Gabriel{-}Miro Muntean},
	title = {CA-Live360: Crowd-assisted transcoding and delivery for live 360-degree
                  video streaming},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110618},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110618},
	doi = {10.1016/J.COMNET.2024.110618},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MaXWDXZZM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the burgeoning field of digital media, the popularity of live 360-degree video has witnessed an upward trajectory, underpinned by its immersive experience. In live 360-degree video streaming services, transcoding enormous video tiles and providing low-latency services for global viewers require significantly more computing and communication resources than traditional live services. Inspired by the huge crowdsourcing computational resources available at crowd, this paper designs CA-Live360, a Crowd-Assisted Live 360-degree video streaming solution, in which transcoding and delivery are supported by crowdsourcing devices. Due to the dynamic availability of resources, it is challenging to achieve low-latency live services, while guaranteeing fair transcoding. Therefore, firstly an innovative fairness-guaranteed transcoding task assignment scheme is proposed. This scheme employs a novel Fair Bandit algorithm based on the combinatorial multi-armed bandit approach to achieve low-latency transcoding while considering time-varying available resources and fairness constraints. Secondly, to reduce delivery latency, we design a transcoding-aware delivery scheme, in which a provider–requester matching algorithm is introduced to realize effective requester scheduling. Real-world trace-driven experiments demonstrate the improved performance of the proposed CA-Live360 solution in terms of system delay and fairness.}
}


@article{DBLP:journals/cn/LiuJWHZLS24,
	author = {Yanxiu Liu and
                  Linpeng Jia and
                  Xin Wang and
                  Huawei Huang and
                  Qinglin Zhao and
                  Zhongcheng Li and
                  Yi Sun},
	title = {Coral: {A} blockchain protocol for handling transactions with deadline
                  constraints},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110620},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110620},
	doi = {10.1016/J.COMNET.2024.110620},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuJWHZLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transactions with deadline constraints are prevalent in blockchain-based scenarios like auctions, elections, and supply chain management, where only transactions completed within an exact deadline are valid. This paper introduces effective throughput as a performance metric for blockchain systems, representing the average number of valid transactions processed per second. Two critical issues limiting the effective throughput of current permissioned blockchains are identified: (1) Current blockchain consensus mechanisms exclusively rely on block height, rather than exact time, to determine whether a transaction meets the deadline. This inaccurate criterion leads to numerous transactions failing to meet the deadline. (2) Traditional transaction packing mechanisms, without considering the transaction deadline constraints, are unsuitable for processing such transactions. To address these issues, this paper proposes a blockchain protocol called Coral, crafted to improve effective throughput through a dual approach. Firstly, the protocol achieves consistent consensus using exact time, ensuring accurate determination of the suitable block to meet transaction deadlines. Secondly, it employs an adaptive transaction packing algorithm (ADPA) to enhance performance under dynamic workload and resource constraints. Furthermore, we give the security analysis and implement a prototype of Coral. Comprehensive evaluation results demonstrate that Coral improves effective throughput by 2.25\n×\nand transaction success ratio by 2.01\n×\ncompared to existing blockchain systems.}
}


@article{DBLP:journals/cn/McEachernMHB24,
	author = {Jordin McEachern and
                  Ehsan Malekshahi and
                  Mojgan Mirzaei Hotkani and
                  Jean{-}Fran{\c{c}}ois Bousquet},
	title = {Real-time passive underwater localization using a compact acoustic
                  sensor array},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110621},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110621},
	doi = {10.1016/J.COMNET.2024.110621},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/McEachernMHB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an underwater passive localization system that can be installed on remote platforms and can locate in real-time sound sources using a compact 4-element array. The localization system relies on both the Time Difference of Arrival (TDoA) algorithm to localize sources that are impulsive, and a low-complexity implementation of the matched field processing (MFP) algorithm for broadband continuous waves, such as propeller noise. The array is intended to be mounted on small platforms and compact geometries are investigated to optimize the localization accuracy. Figures of merit are defined to assess the accuracy and reliability of the algorithm as a function of the distance between the elements. The proposed algorithm is implemented on a System-on-Chip (SoC) which executes the algorithm on an embedded system for real-time operation. Finally, the performance is assessed in realistic ocean conditions in different environments. It is shown that the MFP provides an excellent angle estimate for broadband continuous sources, but TDoA is more reliable, particularly when the deployment is sensitive to the environment conditions.}
}


@article{DBLP:journals/cn/ZhuBCY24,
	author = {Rongxin Zhu and
                  Azzedine Boukerche and
                  Yanxia Chen and
                  Qiuling Yang},
	title = {A reliable cluster-based opportunistic routing protocol for underwater
                  wireless sensor networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110622},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110622},
	doi = {10.1016/J.COMNET.2024.110622},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuBCY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater Acoustic Sensor Networks (UASNs) have gained significant popularity and application in various marine engineering exploration scenarios, driven by the global evolution of the ocean ecosystem. However, UASNs face numerous challenges arising from the unique characteristics of the underwater environment and acoustic channels, such as limited energy resources, unreliable communication, and dynamic network topology in uncertain environments, all of which impact the network lifetime and transmission reliability. To address these challenges and enable efficient underwater data packet transmissions, this paper proposes a reliable cluster-based routing method called RCRP. The RCRP organizes sensor nodes into clusters, where cluster heads transmit the fused data to the sink node through multi-hop forwarding. To overcome routing voids caused by node failures during packet transmission, a prediction model based on the Markov chain is introduced to identify void nodes and inform neighboring nodes about the routing holes. Furthermore, a connection prediction mechanism is developed to tackle changes in the uncertain network topology, taking into account node mobility and the received SNR. For packet transmissions, a waiting mechanism inspired by the opportunistic routing (OR) paradigm is proposed, which determines the optimal forwarding route based on the calculated probability of successful connection between nodes. Simulation results demonstrate that RCRP outperforms existing routing protocols in terms of packet delivery ratio, surviving time, and energy consumption, highlighting its effectiveness in enhancing the performance of UASNs.}
}


@article{DBLP:journals/cn/ZhangYXC24,
	author = {Qianhui Zhang and
                  Lingyun Yuan and
                  Tianyu Xie and
                  Han Chen},
	title = {Auditable and dynamic access control scheme with behavior and identity
                  tracing},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110623},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110623},
	doi = {10.1016/J.COMNET.2024.110623},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhangYXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ciphertext policy attribute-based encryption (CP-ABE) is a potential solution to security sharing issues since it offers one-to-many encryption and fine-grained access control. To prevent users from disclosing their access permissions, traceable CP-ABE schemes are emerging. However, the current traceable CP-ABE schemes still have some security problems, such as impromptu attribute retracting, unreliable tracking, and easy disclosure of access policy privacy, making it essential to implement dynamic data security sharing and malicious user auditing. To address these issues, we propose an auditable and dynamic access control scheme with behavior and identity tracing. The scheme first uses an access tree with hidden leaf nodes to express the access policy, improving the flexibility and security of access control. Secondly, the Blockchain and InterPlanetary File System (IPFS) frameworks perform the audit authentication and store the ciphertext, policy, and public keys to avoid tampering with shared data. Furthermore, to prevent users from leaking their decryption keys, a tracing algorithm based on white-box traceability is developed. On this basis, an authority update algorithm is proposed to revoke the malicious user’s permission immediately and update the decrypted key for unrevoked users. An auditable smart contract based on blockchain is proposed to prevent user denial. Finally, theoretical analyses prove that the proposed scheme not only achieves traceability, revocation, authority updating, policy hiding, and audit, but also guarantees security under a chosen plaintext attack model. Experiments demonstrate that our proposal reduces the key generation time by 22.6%, the encryption and decryption time by 39.2% and 59.8%, and the storage costs of ciphertext and private key are reduced by 12.1% and 5.6% over existing schemes.}
}


@article{DBLP:journals/cn/ChenBZXL24,
	author = {Tao Chen and
                  Yanling Bu and
                  Yue Zeng and
                  Lei Xie and
                  Sanglu Lu},
	title = {RegionFilter: Region-aware video filtering mechanism on resource-constrained
                  edge nodes},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110624},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110624},
	doi = {10.1016/J.COMNET.2024.110624},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenBZXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The video analytics pipeline consumes substantial network and computing resources, and in many scenarios videos are sent from cameras to servers. Due to limited camera-side resources, most existing systems are frame-level filtering approaches that filter out frames that are unchanged or irrelevant to applications before transmitting data. However, video applications usually care about the regions of interest (RoIs) of frames rather than background regions, and frame-level filtering fails to consider the spatial redundancy of intra-frame regions. We argue that video analytics should adopt a region-level filtering approach. Unfortunately, how to perform region-aware filtering on cameras without GPU is a significant challenge. To overcome this, we built RegionFilter, which achieves region awareness on resource-constrained edge nodes through feedback results from server-side DNN models. RegionFilter splits each video segment into multiple sub-segments in spatial distribution through region awareness. Then different configuration parameters (e.g., resolution, quantization parameter (QP)) are selected according to the video application, and the sub-segments are encoded into sub-streamings with different qualities that are uploaded to servers for further processing. Experiments on various video tasks and datasets show that RegionFilter achieves considerable filtering benefits (bandwidth savings of 11.7–41.2% when transmitting HD video) while always meeting the desired accuracy.}
}


@article{DBLP:journals/cn/ValenteLFE24,
	author = {Francesco Valente and
                  Francesco Giacinto Lavacca and
                  Tiziana Fiori and
                  Vincenzo Eramo},
	title = {Proposal and investigation of a distributed learning strategy in Orbital
                  Edge Computing-endowed satellite networks for Earth Observation applications},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110625},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110625},
	doi = {10.1016/J.COMNET.2024.110625},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ValenteLFE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the key enabling solutions to in-orbit extract information from Earth Observation images is given by deep learning techniques. However, the accuracy of these algorithms is strictly related to the availability of large datasets of satellite images for training purposes. Limitations on the available transmission bandwidth in the orbital context may prevent the possibility to downlink all acquired images to a node where centralized training happens. Instead, Federated Learning (FL) could be fruitfully leveraged in this scenario, since it provides for each satellite to train a local model only with its own dataset, and then to share its trained model with a central server, which receives models trained by the different satellites and aggregates them into a new global model being eventually shared with all the satellites, and this repeats until convergence is reached. However, because communication with a node acting as a central parameter server may be still limited by short visibility time, the described process may need a long time because of limited communication windows, negatively impacting the time needed to reach model convergence. For this reason, we propose a communication strategy to support a completely distributed learning technique to train a deep learning model in-orbit, by leveraging the fact that satellites may form a network thanks to the potential availability of Inter-Satellite Links (ISLs) within and between orbital planes. Our proposal is different from a FL approach since we provide for each satellite to receive all the information needed to calculate an updated global model by itself, without leaning on a central parameter server. Numerical results show that distributed learning outperforms FL in number of learning rounds completed in the unit time, allowing for reaching validation accuracy convergence in a shorter time, as it has been verified on a land coverage classification task based on the EuroSAT dataset.}
}


@article{DBLP:journals/cn/RajabW24,
	author = {Abubakarsidiq Makame Rajab and
                  Bang Wang},
	title = {Autonomous and reliable fingerprint map maintenance for indoor positioning
                  system},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110626},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110626},
	doi = {10.1016/J.COMNET.2024.110626},
	timestamp = {Mon, 29 Jul 2024 22:06:52 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RajabW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In WiFi-based crowdsourced fingerprinting indoor positioning systems (IPS), the signal space of WiFi APs changes over time. To autonomously maintain a highly reliable localization accuracy, the fingerprint database needs regular maintenance and updates. However, this process is labor-intensive and time-consuming due to the appearance of faulty access points (APs) in crowdsourced samples received from target clients, which are mainly used for autonomous maintenance and updates. Additionally, if some APs are relocated, removed, or replaced in the indoor environment after fingerprint map construction, it can severely impact IPS accuracy. In this paper, we present Dynamic Online Fingerprint Learning with Altered APs status-aware and updating (DOFLAAU) for IPS. Our systematic methodology involves several key steps. First, we employ the Modified Group Method of Data Handling (GMDH) neural network regression for autonomous AP status monitoring and alteration detection. Next, we perform autonomous APs subset sample selection using a modified Thomson model, followed by outlier feature detection and removal based on the weighted sample density rate. Thirdly, we propose a dynamic online fingerprint map updating model using the cosine similarity distance metric. Lastly, we adopted an effective signal distance-based weighted k-nearest neighbor algorithm to improve IPS accuracy. The experiment results validate the efficiency and effectiveness of our proposed algorithms in terms of better localization accuracy compared to peer algorithms.}
}


@article{DBLP:journals/cn/DinarteACAB24,
	author = {Henrique A. Dinarte and
                  Karcius D. R. Assis and
                  Daniel A. R. Chaves and
                  Raul C. Almeida and
                  Raouf Boutaba},
	title = {Multi-objective optimization of asymmetric bit rate partitioning for
                  multipath protection in elastic optical networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110627},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110627},
	doi = {10.1016/J.COMNET.2024.110627},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DinarteACAB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In survivable elastic optical networks, multi-path protection combined with traffic squeezing has gained attention. Link-disjoint multipath routing (LD-MPR) and bandwidth squeezing protection (BSP) when applied to the routing, modulation level and spectrum assignment problem are efficient strategies to address the excessive bandwidth demanded by protected services, spectral fragmentation and link-load balancing. LD-MPR enables the division of service transmission bit rate into independent flows, whereas BSP tolerates traffic reduction under link failure. An open issue in the literature is how to efficiently divide demanded traffic among the link-disjoint routes under dynamic-traffic and BSP, while complying the service level agreement (SLA). We propose in this paper a multi-objective-optimization genetic algorithm that defines how service transmission bit rate should be partitioned among the candidate link-disjoint routes. A customized partitioning definition is made for each source–destination node pair in the network, aiming the simultaneous minimization of network blocking probability and average squeezing transmission bit rate experienced by the services during single-link failure. We also propose, a fixed-alternate routing using groups of disjoint paths (FARgdp). Complex dynamic-traffic network scenarios that simultaneously consider BSP, LD-MPR and FARgdp are addressed.}
}


@article{DBLP:journals/cn/WuWWZH24,
	author = {Dongkuo Wu and
                  Xing{-}Wei Wang and
                  Xueyi Wang and
                  Rongfei Zeng and
                  Min Huang},
	title = {Differentially private and truthful auction-based resource procurement
                  for budget-constrained {DAG} applications in clouds},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110628},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110628},
	doi = {10.1016/J.COMNET.2024.110628},
	timestamp = {Mon, 29 Jul 2024 22:06:52 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WuWWZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid proliferation of emerging scientific computing, more and more cloud users (CUs) with large-scale applications are seeking efficient executing paradigms in clouds. Following this trend, a cloud market with multiple cloud service providers (CSPs) is emerging, which aims to provide efficient cross-cloud services to CUs through the cooperation among CSPs. In the market, CUs can procure different virtual machine resources and their combinations from one or more CSP(s) in a pay-as-you-go way. In such a procurement process, it is challenging to achieve privacy protection and truthfulness of CSPs while enabling flexible resource combinations. As such, we design a differentially private and truthful auction-based resource procurement mechanism (DTARP) in clouds. Specifically, we first propose a budget-aware resource profile determination algorithm (BRPD) to obtain a set of resource profiles, in order to bridge the difference between application tasks and resources under the budget constraint. Next, we design a differential privacy-based winning CSP selection algorithm to choose a set of winning CSPs, and the payment calculation algorithm to calculate the payment for each winning CSP. Strict theoretical analysis proves that the proposed DTARP achieves differential privacy, truthfulness, individual rationality, computational efficiency, and approximate social cost minimization while guaranteeing the efficient completion of application tasks under the budget. Extensive simulation results also demonstrate the effectiveness of the proposed DTARP.}
}


@article{DBLP:journals/cn/DiegoRM24,
	author = {Santiago de Diego and
                  Cristina Regueiro and
                  Gabriel Maci{\'{a}}{-}Fern{\'{a}}ndez},
	title = {Collaborative credentials for the Internet of Things},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110629},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110629},
	doi = {10.1016/J.COMNET.2024.110629},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DiegoRM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Self-Sovereign Identity (SSI) paradigm has emerged as a decentralized Identity Management model with interesting capabilities for the Internet of Things (IoT). However, current standard SSI protocols and procedures only consider that individuals store their own identity, failing to provide an accurate solution for the identity management of groups where participants might use credentials from different identities and collaborate to meet a set of verifier´s requirements. The present work introduces the concept of Collaborative Credentials (CCs) to formalize identity management procedures that model the collaboration within a group of participants. CCs allow to leverage use cases requiring collaboration that cannot be solved with standard SSI verifiable credentials, increase the privacy of group participants and enable the development of a software framework that any verifier/holder could use to generate a generic application. In the paper, a generic model for CCs is presented, together with an implementation example that is subsequently evaluated in an experimental testbed.}
}


@article{DBLP:journals/cn/ChenLWRGZ24,
	author = {Rui Chen and
                  Lailong Luo and
                  Xiaodong Wang and
                  Bangbang Ren and
                  Deke Guo and
                  Shi Zhu},
	title = {Knowing the unknowns: Network traffic detection with open-set semi-supervised
                  learning},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110630},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110630},
	doi = {10.1016/J.COMNET.2024.110630},
	timestamp = {Wed, 14 Aug 2024 08:21:23 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenLWRGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic classification plays a crucial role in network security and network quality of service. The new emerging traffic data in the real world is unlabeled and may contain unknown classes, which introduces the problem of identifying unknown-class traffic in unlabeled data. However, existing deep learning-based methods either rely on large amounts of labeled data or assume the labeled and unlabeled data share the common label space. Therefore, they are unable to identify unknown traffic, resulting in decreased detection accuracy. This paper introduces DivinEye, a novel method for unknown network traffic detection based on open-set semi-supervised learning. DivinEye utilizes a small quantity of labeled data and a large amount of unlabeled data to train the model, achieving the ability to identify known classes accurately while also detecting unknown ones. To leverage incoming unlabeled traffic, DivinEye employs open-set semi-supervised techniques to select known-class data from unlabeled data to optimize the known traffic detection model. To detect unknown traffic, DivinEye combines the known traffic classifier and multi-binary classifier to construct an unknown traffic detection model, generating auxiliary targets for all unlabeled data as supervision to train the model. Trace-driven experiments demonstrate that DivinEye outperforms state-of-the-art methods by a large margin.}
}


@article{DBLP:journals/cn/GirauAPCRFLG24,
	author = {Roberto Girau and
                  Matteo Anedda and
                  Roberta Presta and
                  Silvia Corpino and
                  Pietro Ruiu and
                  Mauro Fadda and
                  Chan{-}Tong Lam and
                  Daniele D. Giusto},
	title = {Definition and implementation of the Cloud Infrastructure for the
                  integration of the Human Digital Twin in the Social Internet of Things},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110632},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110632},
	doi = {10.1016/J.COMNET.2024.110632},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GirauAPCRFLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the integration of virtualization technologies, the Internet of Things (IoT) is expanding its capabilities and quickly becoming a complex ecosystem of networked devices. The Social Internet of Things (SIoT), where intelligent things include social properties that improve functioning and user engagement, is the result of this progress. The SIoT still has issues with scalability, data management, and user-centric operations, despite tremendous progress. In order to overcome these obstacles, a strong architecture is needed that can handle the enormous number of IoT devices while simultaneously streamlining the user interface.}
}


@article{DBLP:journals/cn/LiXLGJL24,
	author = {Long Li and
                  Gaochao Xu and
                  Ziqi Liu and
                  Jiaqi Ge and
                  Wenchao Jiang and
                  Jiahui Li},
	title = {Task execution latency minimization for energy-sensitive IoTs in wireless
                  powered mobile edge computing: {A} DRL-based method},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110633},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110633},
	doi = {10.1016/J.COMNET.2024.110633},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiXLGJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless powered mobile edge computing (MEC) has become a vital component of future 6G networks, offering efficient computational capabilities to internet of things (IoT) devices and ensuring a stable energy supply. In this paper, we propose an innovative design for a wireless powered MEC-assisted energy-sensitive IoT systems. The system integrates wireless power technology (WPT), allowing IoT devices to efficiently perform tasks under wireless power supply without consuming their battery energy. Therefore, we formulate a dynamic computational task execution latency minimization (DCTELM) problem to tackle the impacts of unpredictable energy requirements and potential communication latency on resource allocation. We also analyze the differences between time division multiple access and non-orthogonal multiple access communication protocols. Given the complexity and dynamic nature of this problem, we define it as a mixed integer nonlinear programming problem. To address this problem, we introduced an enhanced deep reinforcement learning (DRL) algorithm. First, the DCTELM problem is transformed into a Markov decision process (MDP), which simplifies the structure of the problem. Then, after determining the offloading decision, we reveal the convex relationship between resource allocation and task latency. Furthermore, the action space of MDP is further reduced, and a multi-head proximal policy optimization (PPO) algorithm is proposed to deal with the simplified MDP problem. This process reduces the complexity of problem solving, decreases the consumption of computational resources. Simulation results demonstrate that our method outperforms other baselines in terms of computation latency, particularly with the introduction of an exploration strategy during the training phase, which significantly reduces task latency.}
}


@article{DBLP:journals/cn/QiPP24,
	author = {Zhuoran Qi and
                  Roberto Petroccia and
                  Dario Pompili},
	title = {RD-ASVTuw: Receiver-Driven Adaptive Scalable Video Transmission in
                  underwater acoustic networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110634},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110634},
	doi = {10.1016/J.COMNET.2024.110634},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/QiPP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Achieving reliable underwater acoustic communications is a challenging task due to the time-varying channels in the underwater acoustic environment. Scalable Video Coding (SVC) has been widely used in video transmissions. However, inappropriate SVC structures can lead to poorer received video quality than user requirements or resource waste, especially in underwater time-varying channels. In this work, an adaptive cross-layering solution is proposed and validated for video transmissions in underwater acoustic multicast networks, namely Receiver-Driven Adaptive Scalable Video Transmission (RD-ASVTuw). In RD-ASVTuw, the decision-making about transmission schemes takes place at the receiver through Machine Learning (ML). The transmitter collects over time the selected transmission scheme indexes and the user’s video quality requirements to transmit the SVC video adaptively. At-sea experiments were conducted to collect the required acoustic data. The data collected were then used in MATLAB simulations to validate RD-ASVTuw.}
}


@article{DBLP:journals/cn/YeHLYK24,
	author = {Jin Ye and
                  Huilin Hu and
                  Jiahua Liang and
                  Linfei Yin and
                  Jiawen Kang},
	title = {Lightweight adaptive Byzantine fault tolerant consensus algorithm
                  for distributed energy trading},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110635},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110635},
	doi = {10.1016/J.COMNET.2024.110635},
	timestamp = {Mon, 05 Aug 2024 15:14:25 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YeHLYK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of smart grid, constructing distributed energy trading market (DETM) based on blockchain to coordinate distributed energy resources (DER) has become a future direction. However, existing consensus algorithms of blockchain face many challenges in large-scale energy trading scenarios, such as high resource overhead, slow transaction procedure. To solve the above crucial problems for wide deployment of distributed energy trading, this study proposes a novel consensus algorithm named Lightweight adaptive Byzantine fault tolerant consensus (LA-BFT), and a reputation calculation method based on behavioral characteristics for selection of consensus nodes. The LA-BFT consists of two parts: (i) weak consensus for normal cases. By introducing threshold signature mechanism, weak consensus simplifies the consensus process to achieve linear communication complexity\nO\n(\nn\n)\n. (ii) byzantine node detection scheme is enable for malicious cases. With consensus committee, the detection scheme can detect the potential byzantine nodes by cross-validation, which ensures transaction safety. The reputation calculation method is presented to cooperate with LA-BFT to elect leaders and candidates for consensus procedure. Once a round of consensus is completed, the reputation of each node needs to be updated, only nodes with high reputation are eligible to become leaders or committee nodes in the next round. With the reputation calculation method, honest nodes and byzantine nodes can be effectively identified, ensuring the security of the consensus process. Numerical results indicate that LA-BFT exhibits superior performance on communication overhead and bandwidth occupancy in large-scale concurrent energy trading scenarios. When the number of nodes is 50, under normal scenarios, LA-BFT’s communication overhead is notably lower, constituting a mere 6.02% of PBFT and 24.26% of SHBFT, while bandwidth occupancy amounts to merely 5.55% of PBFT and 9.76% of SHBFT.}
}


@article{DBLP:journals/cn/XuSH24,
	author = {Zhilin Xu and
                  Hao Sun and
                  Weibin Han},
	title = {Boosting task completion rate for time-sensitive {MCS} system},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110636},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110636},
	doi = {10.1016/J.COMNET.2024.110636},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuSH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Mobile Crowdsensing system, many sensing tasks are time-sensitive, hence, time validity is essential cause data outside the time frame is useless which will not only increase requesters’ costs but also damage requesters’ efficiency and utilities. Therefore, it is crucial to design an incentive mechanism to increase system’s task completion rate within time limitations. A dilemma arises between requesters and the MCS system due to the trad-off between requester’s goal to maximize its own utilities and the MCS system’s purpose to increase the whole system’s task completion rate. To solve the dilemma, we come up with a dynamic coopetition (competition and cooperation) incentive mechanism where in every stage there are competition to assure requesters’ utilities and cooperation to reallocate data to advance the system’s task completion rate. During competition, a matching algorithm with budget constraints is presented to derive the optimal matching strategies for requesters and participants without exceeding requesters’ budgets while maximizing utilities. After competing, the centralized MCS platform would reallocate requesters’ excessive data to those who failed to get enough data on time through a data reallocation algorithm. By simulations, we compare the performance of the coopetition incentive mechanism with different orders and parameters. Regardless of parameters or orders, our algorithm can promise a minimum task completion rate of 94% compared with less than 32% task completion rate with only competition. Among all three orders, emergency degree has the best performance with task completion rate up to 99%.}
}


@article{DBLP:journals/cn/TaktakEB24,
	author = {Wiem Taktak and
                  Mohamed Escheikh and
                  Kamel Barkaoui},
	title = {{DRL} approach for online user-centric QoS-Aware {SFC} embedding with
                  dynamic {VNF} placement},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110637},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110637},
	doi = {10.1016/J.COMNET.2024.110637},
	timestamp = {Fri, 02 Aug 2024 21:41:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TaktakEB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we tackle online Service Function Chaining (SFC) embedding issue in Software Defined Network (SDN)/Network Function Virtualization (NFV) enabled networks using Deep Reinforcement Learning (DRL) approach. This approach investigates the problem of Quality of Experience (QoE)/Quality of Service (QoS) driven SFC embedding with dynamic Virtual Network Functions (VNF) placement. We particularly focus on video streaming traffic requiring a given Mean Opinion Score (MOS) quantified in terms of QoE. In this regard, we propose a DRL algorithm based on Deep-Q-Network (DQN) (named DQN_QoE/QoS_SFC algorithm) implementing an optimization problem maximizing QoE while meeting QoS requirements. Our algorithm named DQN_QoE/QoS_SFC improves previous work given in Chen et al. (2018) dealing with the same issue through bringing performance evaluation of the training phase by testing the learning quality. This is achieved through using a QoE Threshold Score (\nQ\no\nE\nT\nh\n−\nS\nc\n) to be attained on average by the DQN_QoE/QoS_SFC agent along the last 100 runs of the training phase while maximizing the expected cumulative reward. We conduct simulation experiments through two performance metrics related to the SFC request namely QoE (\nQ\no\nE\n) and Rejection Ratio (\nR\nR\n). We investigate also how these metrics evolve for the proposed algorithm when compared with two standard baseline algorithms (Violent search, Random search) during learning phase to achieve satisfactory training quality. We highlight how DQN_QoE/QoS_SFC algorithm behaves along the training process and how it attempts reaching a near-optimal solution as closer as possible to the optimal solution provided by Violent search algorithm.}
}


@article{DBLP:journals/cn/DuttaAANDDKN24,
	author = {Ashit Kumar Dutta and
                  Nuha Alruwais and
                  Eatedal Alabdulkreem and
                  Noha Negm and
                  Abdulbasit A. Darem and
                  Mesfer Al Duhayyim and
                  Wali Ullah Khan and
                  Ali Nauman},
	title = {Fair resource optimization for cooperative non-terrestrial vehicular
                  networks},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110639},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110639},
	doi = {10.1016/J.COMNET.2024.110639},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DuttaAANDDKN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-terrestrial networks aim to extend wireless communications capabilities beyond traditional terrestrial infrastructure by utilizing various platforms such as satellites, high-altitude platform stations (HAPS), and unmanned aerial vehicles to provide global or localized coverage. However, improving data rates in non-terrestrial cooperative communications (NTCC) networks presents unique challenges, necessitating innovative solutions. This paper presents a novel approach to enhance the request–response ratio (RRR) of NTCC in time asynchrony. By leveraging the advantages of HAPS, such as wide coverage, high altitude, and flexible deployment, we aim to optimize the performance of NTCC networks. We formulate an optimization problem considering the simultaneous connection between the HAPS and ground users, power distribution, and decoding order. The joint optimization problem is formulated as non-convex-non-linear and is also NP-hard, making it very challenging to obtain a globally optimal solution. To reduce the complexity of the problem and make it more tractable, we decouple it into subproblems and achieve an efficient solution in two steps. In the first step, we use the Gale–Shapley algorithm to solve the many-to-many two-sided matching problem of HAPS user terminal association, given the fixed decoding and power allocation. Then, we solve the decoding order and power allocation problem using the Dinkeback-like algorithm, given the optimal association in the second step. Our proposed method iteratively updates the preference lists, mitigating interference among HAPS and enhancing overall system performance. Through extensive simulations, we demonstrate that implementing our proposed schemes achieves high RRR compared to benchmark schemes. Additionally, when temporal asynchrony is employed alongside our approach, there is an increase in the overall performance obtained from the process.}
}


@article{DBLP:journals/cn/DaiZWLZ24,
	author = {Yuqi Dai and
                  Hua Zhang and
                  Jingyu Wang and
                  Jianxin Liao and
                  Penghui Zhang},
	title = {{INCS:} Intent-driven network-wide configuration synthesis based on
                  deep reinforcement learning},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110640},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110640},
	doi = {10.1016/J.COMNET.2024.110640},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DaiZWLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Configuring modern networks is a complex task due to the intricate low-level configuration languages and diverse routing protocols used across numerous interconnected devices. Network configuration synthesis has been proposed to solve these challenges. However, existing synthesizers encounter several limitations: (i) They cannot scale to large topologies with more than a few tens of routers. (ii) They only support specific routing protocols. (iii) They require prolonged synthesis times. (iv) They lack configuration optimization.}
}


@article{DBLP:journals/cn/LiXWZW24,
	author = {Pengyong Li and
                  Jiaqi Xia and
                  Qian Wang and
                  Yujie Zhang and
                  Meng Wu},
	title = {Secure architecture for Industrial Edge of Things(IEoT): {A} hierarchical
                  perspective},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110641},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110641},
	doi = {10.1016/J.COMNET.2024.110641},
	timestamp = {Fri, 06 Sep 2024 20:43:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiXWZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Industrial Internet of Things (IIoT) is an application of the IoT specifically tailored for industrial manufacturing, characterized by its heightened requirements for intelligent industrial production and the creation of a safer and more manageable industrial production environment. Edge Computing (EC) is emerging as an innovative strategy to establish end-to-end connectivity with lower transmission latency and higher processing efficiency among large-scale industrial equipment. The fusion of IIoT and EC has given rise to a new paradigm known as the Industrial Edge of Things (IEoT), leveraging edge intelligence to enhance the overall performance of industrial networks. However, the limited computing resources of edge devices and the complex heterogeneous distribution environment pose significant challenges for IEoT in monitoring maintenance, countering network attacks, and ensuring data security. In this paper, we underscore the importance of constructing a comprehensive security framework for IEoT across all levels. Initially, we devise a novel layered framework tailored to the unique characteristics of IEoT. Subsequently, we conduct a layered analysis of the threats and challenges encountered by each layer of IEoT. Concurrently, we provide a systematic review of the latest advancements in security solutions, layer by layer. To facilitate comprehension and implementation, we present a case study of IEoT security architecture aimed at ensuring device security, control security, network security, and data security. Lastly, we outline key research challenges and promising research directions to expedite the rapid advancement of IEoT technology applications.}
}


@article{DBLP:journals/cn/WuZLLH24,
	author = {Zuodong Wu and
                  Dawei Zhang and
                  Yong Li and
                  Chaoyang Li and
                  Xu Han},
	title = {{PRSD:} Efficient protocol for privacy-preserving retrieval of sensitive
                  data based on labeled {PSI}},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110649},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110649},
	doi = {10.1016/J.COMNET.2024.110649},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WuZLLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for privacy-preserving retrieval of sensitive data arises in various practical scenarios, from national security to personal data privacy. The typical scenario involves two parties: the client, who seeks sensitive data from the other party without revealing their interests, and the server, who is willing to send only the requested sensitive data. However, the current design process for mainstream solutions relying on Public-Key (PK) or Oblivious Transfer (OT) requires complex modular exponentiation operations or a significant number of matrix transmissions, seriously reducing the efficiency of these solutions in terms of computational and communication costs.}
}


@article{DBLP:journals/cn/YamansavascilarOE24,
	author = {Baris Yamansavascilar and
                  Atay Ozgovde and
                  Cem Ersoy},
	title = {Air computing: {A} survey on a new generation computation paradigm},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110653},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110653},
	doi = {10.1016/J.COMNET.2024.110653},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YamansavascilarOE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an ever-growing race between what novel applications demand from the infrastructure and what the continuous technological breakthroughs bring in. Especially after the proliferation of smart devices and diverse IoT requirements, we observe the dominance of cutting-edge applications with ever-increased user expectations in terms of mobility, pervasiveness, and real-time response. Over the years, to meet the requirements of those applications, cloud computing provides the necessary capacity for computation, while edge computing ensures low latency. However, these two essential solutions would be insufficient for next-generation applications since computational and communicational bottlenecks are inevitable due to the highly dynamic load. On the other hand, inadequate infrastructure considering rural areas and disaster sites makes the utilization of those solutions difficult. Therefore, a 3D networking structure using different air layers including Low Altitude Platforms, High Altitude Platforms, and Low Earth Orbits in a harmonized manner for both urban and rural areas should be applied to satisfy the requirements of the dynamic environment. In this perspective, we put forward a novel, next-generation paradigm called Air Computing that presents a dynamic, responsive, and high-resolution computation environment for all spectrum of applications. In this survey, we define the components of air computing, investigate its architecture in detail, and discuss its essential use cases and the advantages it brings for next-generation application scenarios. We provide a detailed and technical overview of the benefits and challenges of air computing as a novel paradigm and spot the important future research directions.}
}


@article{DBLP:journals/cn/BaiXWRPRYMLZ24,
	author = {Yang Bai and
                  Gaojie Xing and
                  Hongyan Wu and
                  Zhihong Rao and
                  Chengzong Peng and
                  Yutang Rao and
                  Wentao Yang and
                  Chuan Ma and
                  Jiani Li and
                  Yimin Zhou},
	title = {{ISPPFL:} An incentive scheme based privacy-preserving federated learning
                  for avatar in metaverse},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110654},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110654},
	doi = {10.1016/J.COMNET.2024.110654},
	timestamp = {Thu, 14 Nov 2024 14:45:54 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BaiXWRPRYMLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The metaverse, seen as the internet’s successor, is a digital world with its own values and economy, linked to the physical world. Accessed via digital avatars using VR equipment, it involves extensive personal data, raising concerns about data security and privacy. Federated learning is a distributed machine learning technique that ensures privacy by allowing metaverse avatars to share knowledge without revealing user data. However, recent research shows that federated learning still faces privacy threats, such as Source Inference Attacks (SIAs) in Horizontal Federated Learning (HFL) and Label Inference Attacks (LIAs) in Vertical Federated Learning (VFL). To solve these problems, in this paper, we propose the first Incentive Scheme based Privacy-Preserving Federated Learning for avatar in metaverse (ISPPFL). The framework is composed of a privacy risks auditor, perturbation generation mechanism and adaptive incentive mechanisms, which effectively defends against privacy risks. We conducted comprehensive experiments on two distinct datasets in various scenarios. The result demonstrated that, the ISPPFL can effectively defend against privacy attacks while maintain model accuracy. For SIAs under HFL, compared to the baseline, the introduction of perturbation and incentive mechanisms could minimize privacy risk indicator (PRI) to around 20%, while maintain the model performance. For LIAs under VFL, the PRI of the model with perturbation generation mechanisms could decrease by approximately 10% compared to the model trained without defenses. Simultaneously, with the introduction of adaptive incentive mechanisms, the PRI could drop from 90% to 69%. Ultimately, the paper also summarizes the completed work and proposes directions for future research.}
}


@article{DBLP:journals/cn/QinJQGLD24,
	author = {Xingsheng Qin and
                  Frank Jiang and
                  Xingguo Qin and
                  Lina Ge and
                  Meiqu Lu and
                  Robin Doss},
	title = {CGAN-based cyber deception framework against reconnaissance attacks
                  in {ICS}},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110655},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110655},
	doi = {10.1016/J.COMNET.2024.110655},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/QinJQGLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Industrial Control Systems (ICSs) have faced increasing vulnerability to cyber attacks due to their integration with the Internet. Despite efforts to enhance cybersecurity, reconnaissance attacks remain a significant threat, prompting the need for innovative defensive strategies. This paper introduces a novel approach to strengthen the defensive capabilities of ICS networks against reconnaissance attacks using machine learning-driven cyber deception techniques. Leveraging Conditional Generative Adversarial Networks (CGANs), the proposed framework dynamically generates defensive network topologies to network shuffling and implement deception strategies, prioritizing system availability. Extensive simulations demonstrate the superior efficacy of the proposed framework in enhancing cybersecurity while minimizing computational overhead. By effectively mitigating reconnaissance attacks, this solution reinforces the resilience of ICS networks, safeguarding critical industrial infrastructure from evolving cyber threats. These findings underscore the significance of adopting machine learning-based cyber deception as a pragmatic security measure for protecting ICS networks in real-world industrial contexts.}
}


@article{DBLP:journals/cn/BertalanicHF24,
	author = {Blaz Bertalanic and
                  Vid Hanzel and
                  Carolina Fortuna},
	title = {Explainable semantic wireless anomaly characterization for digital
                  twins},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110660},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110660},
	doi = {10.1016/J.COMNET.2024.110660},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BertalanicHF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The shift towards software-centric network infrastructures is driven by the increasing need for networks to be responsive, flexible, and scalable in the face of an expanding set of connected devices. The digital twin (DT) approach, mirroring physical entities in a digital format, has emerged as a key enabler of network reliability and availability. Incorporating artificial intelligence (AI) into DTs enhances the resilience of networks by providing in-depth analysis and increasingly automated mitigation strategies against operational disruptions. In this work, we propose a new AI-based information extraction module that is part of the DT Monitoring component able to process RSS data, extract and characterize abrupt anomalies. The output of this component is used to maintain an anomaly history in the Link Abstraction within the DT and subsequently inform possible automatic mitigation actions. We design the AI-based information extraction module to identify and characterize three types of RSS based anomalies. Our extensive performance analysis on 10 versions of the “You Only Look Once” architecture reveals that YOLOv8n produces a good tradeoff between performance and computational complexity. We show that our approach performs on par with the state of the art for anomaly detection, while enabling anomaly characterization by location, duration, and severity. By employing two SotA explainability algorithms, we also provide insights into the important regions of the input that trigger the selected model’s classification and characterization decisions.}
}


@article{DBLP:journals/cn/LiCT24,
	author = {Jian Li and
                  Tongbao Chen and
                  Shaohua Teng},
	title = {A comprehensive survey on client selection strategies in federated
                  learning},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110663},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110663},
	doi = {10.1016/J.COMNET.2024.110663},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged as a promising paradigm for collaborative model training while preserving data privacy. Client selection plays a crucial role in determining the overall performance and efficiency of the FL training process. However, the heterogeneity of clients in terms of computational resources, data quality, and network conditions poses significant challenges in selecting the optimal set of clients for each training round. This survey provides a comprehensive review of client selection methodologies in FL, addressing the challenges and opportunities in this field. We present a systematic categorization of existing client selection techniques based on their underlying principles and objectives, and discuss the key challenges, including resource constraints, data heterogeneity, and security concerns. Additionally, we provide a comparative analysis of the surveyed techniques, highlighting their strengths, limitations, and suitability for various FL scenarios. Furthermore, we identify open research problems and propose future research directions, emphasizing the need for more efficient, adaptive, and secure selection strategies. This survey serves as a comprehensive framework for understanding client selection in FL, bridging gaps in the existing literature and providing guidance for future research, including ethical considerations and domain-specific applications. Our work aims to support practitioners in making informed decisions and to stimulate further research in this critical area of FL.}
}


@article{DBLP:journals/cn/XuY24,
	author = {Jun Xu and
                  Dejun Yang},
	title = {Energy-efficient resource allocation for {D2D} communication underlaying
                  cellular networks with incomplete {CSI}},
	journal = {Comput. Networks},
	volume = {251},
	pages = {110664},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110664},
	doi = {10.1016/J.COMNET.2024.110664},
	timestamp = {Thu, 08 Aug 2024 09:22:50 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-to-device (D2D) communication supports direct communications between nearby devices, which has potential to improve network capacity, spectrum efficiency and energy efficiency. Considering the high overhead to obtain the complete channel state information (CSI), we investigate resource allocation problems of D2D communication underlaying cellular networks with incomplete CSI to minimize the total power consumption. To deal with the challenge brought by incomplete CSI in estimating the instantaneous rates of D2D pairs (DPs), we consider two QoS metrics in terms of the expected rate and outage probability using statistical CSI. Based on that, we first investigate the energy-efficient power control problem for single cellular user (CU) and single DP sharing the same spectrum. With rigorous theoretical analysis of the intrinsic properties of CU rate and two QoS metrics, we design an optimal energy-efficient power control (EPO) algorithm for single CU and single DP. Using EPO as a building block, we then propose an energy-efficient resource allocation algorithm for multiple CUs and multiple DPs with incomplete CSI. Simulation results show that our algorithms consume the lowest powers compared with two baseline algorithms.}
}
