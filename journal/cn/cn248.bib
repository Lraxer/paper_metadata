@article{DBLP:journals/cn/KonduruN24,
	author = {Prasanthi Konduru and
                  N. P. Nethravathi},
	title = {{SECURE} and energy-efficient routing protocol based on micro-segmentation
                  and batch authentication},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110293},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110293},
	doi = {10.1016/J.COMNET.2024.110293},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KonduruN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An effective technique in designing routing algorithms for Wireless Sensor Networks (WSNs) is named clustering nodes, which augments the network lifespan. More tasks are performed by the Cluster Heads (CHs) of the clustered WSNs, thereby consuming more energy. Hence, a secure energy-efficient Routing Protocol (RP) centered on micro-segmentation and batch authentication is proposed here. The proposed method begins by initializing the WSN nodes. After that, a random key is engendered for every node centered on their Node ID. The nodes undergo a Micro-segmentation process; in addition, they are securely grouped utilizing the Supremum Distance based K-Prototype (SD-KP) algorithm. Now, a fog layer with a number of fog nodes is initialized; also, every single micro-segmented group is assigned with an optimal fog node. Then, by using the Quasi Deterministic Sequence-Black Widow Optimization (QDS-BWO) algorithm, the optimal fog nodes are selected. By using the Root Squared- Diffie Hellman (R2-DH) technique, a key agreement is created for each group centered on their Node IDs. By using the Bitwise Cyclic Shift –BLAKE 512 (BCS-BLAKE-512) algorithm, the created keys are converted into hashcode. Now, for the purpose of performing batch authentication, the hashcode of each group is sent to the trusted authority. Subsequent to verification, data sensing occurs. Lastly, by using the Elliptic Curve Cryptography (ECC) algorithm, the data is securely encrypted and it is transmitted via the routes selected utilizing the Learned Gradient Weight Initialization-based Geography and Energy Aware Routing (LGWI-GEAR) algorithm. The proposed framework's efficacy is proved by the experimental outcomes.}
}


@article{DBLP:journals/cn/SegataCLPTSKL24,
	author = {Michele Segata and
                  Paolo Casari and
                  Marios Lestas and
                  Alexandros Papadopoulos and
                  Dimitrios Tyrovolas and
                  Taqwa Saeed and
                  George K. Karagiannidis and
                  Christos Liaskos},
	title = {CoopeRIS: {A} framework for the simulation of reconfigurable intelligent
                  surfaces in cooperative driving environments},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110443},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110443},
	doi = {10.1016/J.COMNET.2024.110443},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SegataCLPTSKL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future connected vehicles will require high-performance communication technologies for advanced cooperative driving applications such as maneuvering and cooperative perception. mmWave communications can meet the bandwidth requirements of such applications, but the typically harsh propagation conditions of vehicular environments hinder the broad adoption of mmWave devices on cars. Reconfigurable intelligent surfaces (RISs) can help mitigate this problem by enabling the reflection of signals in a configurable direction. In turn, this can result in more stable non-line of sight (NLoS) links whenever a LoS path is not available. RISs have recently gained attention in the vehicular domain but, while providing benefits, they also introduce a lot of research challenges. To measure their effectiveness at scale, it is necessary to develop simulation tools that can reproduce their characteristics with high fidelity and federate them with existing cooperative driving simulation frameworks. In this work we present CoopeRIS, an open-source simulation framework federated within the Plexe/Veins/SUMO ecosystem, capable of modeling and simulating RIS-based mmWave communications in a vehicular environment. We exploit CoopeRIS to perform an initial feasibility study, highlighting the challenges ahead and the performance RISs need to deliver in order to enable this type of communication. In addition we propose a method to combine multiple RIS configurations into a single one to enable multi-user service delivery, showing its performance via CoopeRIS. The insights we presents within this work show the potential of such simulation framework and thus how the community can build further research work on top if it.}
}


@article{DBLP:journals/cn/RahmatovB24,
	author = {Nematullo Rahmatov and
                  Hoki Baek},
	title = {Exploring scale-free networks: Survey on autonomous system dynamics
                  and connectivity analysis},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110454},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110454},
	doi = {10.1016/J.COMNET.2024.110454},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RahmatovB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The global Internet is continuously developing; however, it is becoming more complex with time owing to the introduction of new nodes and links among autonomous systems (ASes). The network links among ASes appear and disappear, especially when the shortest path is required or a low-cost upstream Internet service provider (ISP) is a prerequisite. Understanding the Internet dynamics is critical for improving Internet characteristic analysis. In this work, we reviewed the existing studies from the last two decades on AS network topology as well as relations and routing among ASes. Similarly, we measured the link value differences within Russia and Central Asian ASes using the Barabasi–Albert (BA) model. Our analysis results show that in the last two years, the number of newly added links among 5377 ASes is 65,782. Further, the results show that the ASes with a high network degree in terms of high connectivity or preferential attachment approach are those used by the largest telecom companies in Russia and Central Asian regions. This survey paper aimed to determine whether the Internet AS graphs in these regions adhere to scale-free network properties, and we further examined the growth patterns of node connectivity within these regions.}
}


@article{DBLP:journals/cn/FilhoOM24,
	author = {Jo{\~{a}}o da Mata Lib{\'{o}}rio Filho and
                  Jhonathan Araujo Oliveira and
                  Cesar A. V. Melo},
	title = {Super-resolution with perceptual quality for improved live streaming
                  delivery on edge computing},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110463},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110463},
	doi = {10.1016/J.COMNET.2024.110463},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FilhoOM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {According to Cisco projections, mobile connection throughput is anticipated to more than triple by the year 2023, with the expected typical 5G connection throughput reaching 575 Mbps. Additionally, smartphones and TV sets equipped with computational capabilities for decoding 4K videos have significantly redefined the landscape of Internet video traffic, accounting for 82% of the total Internet traffic. This surge in connection throughput, coupled with the escalating demand for video-centric content, presents an unprecedented challenge in effectively managing traffic across the backhaul infrastructure of 5G networks. In this article, we introduce and evaluate the On-edge enhanced live streaming with super-resolution (ELiveSR) framework for live video delivery, which takes advantage of throughput improvements, in the access networks, and makes use of real-time super-resolution procedure to reduce transmission rate demands in the backhaul segment of the network. This SR procedure is carried by a deep neural network based model and is capable of coping with video content of different subjects. Experimental assessments conducted revealed that the proposed framework can reduce video-related traffic in the backhaul segment by up to 88.37%. Additionally, it simultaneously enhances the quality of experience metrics observed during live video streaming sessions.}
}


@article{DBLP:journals/cn/PengZOL24,
	author = {Jiaxin Peng and
                  Siwang Zhou and
                  Liubo Ouyang and
                  Xingting Liu},
	title = {Volatility-based diversity awareness for distributed data storage
                  of Mobile Crowd Sensing},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110466},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110466},
	doi = {10.1016/J.COMNET.2024.110466},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PengZOL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the widespread utilization of sensors and the rapid development of Mobile Crowd Sensing (MCS), the framework of distributed data storage, which makes use of the limited storage capacity of sensing devices, is providing an alternative to cloud-based data storage. Since the construction of the distributed storage framework for MCS has to be completed by Compressed Sensing (CS) theory, a reasonable measurement allocation becomes a no-brainer if we want to obtain more recovery precision of data without wasting measurement resources. To more efficiently address the above problems, we present a novel strategy of Volatility-Based Diversity Awareness for CS measurement allocation called VBDA, which collectively achieves lower computational complexity, adaptive sampling rate allocation for non-visual data, and high-quality reconstruction. We first process the target monitoring region in blocks. Then, we calculate the degree of data diversity of each area using the volatility, which is used to assess the importance of the different areas. Worthy note, based on the volatility calculation, we can obtain the magnitude of the variation in diversity that is only present in the real data, even when using very ambiguous recovered data. This is primarily due to the volatility unique design concept, which attempts to offset partially identical errors in adjacent recovered data using volatility calculations. Finally, to rationally allocate measurement resources, a volatility-based sampling rate allocation scheme is proposed. We further provide an implementation for practical deployments of VBDA in various related contexts. Numerous experimental show that the VBDA performs excellently. In comparison to the current state-of-the-art strategy, it improves data recovery accuracy by 12.2% without the use of any prior knowledge, while being adaptable to many different distribution types of data.}
}


@article{DBLP:journals/cn/TianXDLZ24,
	author = {Shujuan Tian and
                  Ke{-}Ke Xu and
                  Wen{-}Jian Ding and
                  Yanchun Li and
                  Deze Zeng},
	title = {An offloading and pricing mechanism based on virtualization in edge-cloud
                  computing},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110468},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110468},
	doi = {10.1016/J.COMNET.2024.110468},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TianXDLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrate users, edge servers and cloud into a system can improve resource utilization and quality of service (QoS). The users can send the same request to multiple Edge Servers. However, edge servers have limited scope, which may result in repeated offloading tasks for the same user. In this paper, we consider the cloud resource migration in the form of virtual machines (VMs) to achieve high computing efficiency and low latency requirements. Obviously, the migration of VMs causes changes in the number and price of edge resources, which further affects the task offloading strategy. Therefore, it is necessary to optimize the VMs migration strategy and resource pricing strategy successively to achieve the optimal offloading strategy suitable for users. First, based on the competition between edge servers, a static VMs migration algorithm (SVMMA) is designed to formulate the strategy and price of VMs migration. Then, according to the users’ resource needs and the resource possession of different edge servers, a resource partitioning approach (RPA) have been proposed. Finally, two different QoS functions are evaluated, representing the QoS matching degree between tasks and multiple resource combination blocks (MRCBs). Wherein, a dynamic matching and pricing of tasks algorithm (DMPT) is conducted to obtain the optimal offloading strategy and price of the task. Experiments show that our algorithms can improve the offloading rate of tasks and achieve the relative balance among user, cloud server, and edge server benefits.}
}


@article{DBLP:journals/cn/FanXW24,
	author = {Chenchen Fan and
                  Hongyu Xu and
                  Qingling Wang},
	title = {Multi-agent deep reinforcement learning for trajectory planning in
                  UAVs-assisted mobile edge computing with heterogeneous requirements},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110469},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110469},
	doi = {10.1016/J.COMNET.2024.110469},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FanXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In heterogeneous wireless networks, massive user equipments (UEs) generate computing tasks with time-varying heterogeneous requirements. To improve the service quality, this paper formulates a unmanned aerial vehicles (UAVs)-assisted mobile edge computing (MEC) framework for time-varying heterogeneous task requirements. In the framework, the task delay and the number of successfully executed tasks are optimized by jointly controlling the trajectories of multiple UAVs. To address the considered trajectory planning optimization problem, a collaborative multi-agent deep reinforcement learning (MADRL) algorithm is proposed, where each UAV is regarded as a learning agent. First, a counterfactual inference based personalized policy update mechanism is proposed to evaluate the independent policy of agents by comparing the policy with a designed counterfactual policy. Based on this idea, each agent updates a personalized policy from both group and individual interests to improve its cooperation ability in dynamic and complex environments. Then, a diversified experience sampling mechanism is proposed to enhance the efficiency of policy evaluation and update with rich experiences provided by the environment interaction and the modified whale optimization algorithm. Finally, evaluation results demonstrate the superiority and effectiveness of the proposed MADRL algorithm.}
}


@article{DBLP:journals/cn/KazemiGN24,
	author = {Nader Kazemi and
                  Reza Ghaderi and
                  Soheila Nazari},
	title = {An IoT-based packet aggregation mechanism for the SDN-based wide area
                  networks},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110474},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110474},
	doi = {10.1016/J.COMNET.2024.110474},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KazemiGN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is destined to play a crucial role in our daily lives. Numerous short packets are sent over the networks due to these IoT devices' communication with one another or with the relevant servers via the Internet, which causes an increase in the bps and pps in the network nodes, high overheads, and inefficient performance of the networks. Many packet aggregation mechanisms have been developed and studied to solve these issues. Among them, some works are about the aggregation of the IoT packets in the WAN area; however, they have few studies and discussions about selecting the aggregator nodes and the possible options. In this work, a packet aggregation mechanism is proposed for near-the-edge SDN-based AS networks in the WAN area, which can select the aggregation type and the aggregator nodes among the possible options. The simulation results of the proposed mechanism are presented for different options, and the related discussions and comparisons are made. The results show improvements in the network efficiency for different modes of the proposed packet aggregation mechanism related to the non-aggregation mode and simple/app-level aggregation in all network nodes.}
}


@article{DBLP:journals/cn/ToureISDG24,
	author = {Almamy Tour{\'{e}} and
                  Youcef Imine and
                  Alexis Semnont and
                  Thierry Delot and
                  Antoine Gallais},
	title = {A framework for detecting zero-day exploits in network flows},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110476},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110476},
	doi = {10.1016/J.COMNET.2024.110476},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ToureISDG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Zero-day attack detection solutions aim to proactively identify unknown threats targeting valuable assets within a given system. While many Intrusion Detection System (IDS) solutions leverage learning techniques to build novel attack detection systems, they often focus on enhancing accuracy for specific attack types, overlooking the potential for multiple attack scenarios. Therefore, we introduce a novel framework for detecting zero-day attacks that evade current detection systems. Our framework enhances attack identification and qualification through a hybrid learning approach, where supervised learning ensures detection of known attacks and unsupervised learning. It encompasses intrusion detection phases from data collection to new attack class detection by identifying anomalies in real-time network flow data. Unsupervised learning, which involves grouping similar data points into clusters, establishes minimum distances within these clusters. This process triggers cluster division when certain thresholds are reached. Finally, an online supervised learning process validates our approach’s effectiveness in identifying anomalies associated with zero-day attack flows.}
}


@article{DBLP:journals/cn/WangLL24,
	author = {Jiaqin Wang and
                  Kai Liu and
                  Hantao Li},
	title = {LSTM-based graph attention network for vehicle trajectory prediction},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110477},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110477},
	doi = {10.1016/J.COMNET.2024.110477},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle Trajectory Prediction (VTP) is one of the key technologies for autonomous driving, which can improve the safety and collaboration of the autonomous driving system. The interaction behavior among vehicles in reality has an impact on VTP. However, many methods ignore the interaction among vehicles, which results in limited accuracy of prediction results. Therefore, we propose a Long Short-Term Memory (LSTM)-based Graph Attention Network (GAT) method for VTP, which encodes vehicle trajectory information with LSTM networks and represents vehicle interactions with GAT. Firstly, in order to capture the temporal relationship between positions and consider their influence, we use LSTM model to encode the position data. Meanwhile, to comprehensively model vehicle motion and use multidimensional feature representation, we employ another LSTM model to encode the motion data, including position, velocity and acceleration. Secondly, to learn distinct feature representation, we use one GAT module to process the LSTM position encoding features for capturing spatial relationships of position information. Another GAT module is employed to process the LSTM motion encoding features for fully considering multidimensional motion dynamics and spatial–temporal dependencies. Finally, the LSTM decoder receives all features and predicts the vehicle trajectory. The experimental results show that the proposed method demonstrates superior predictive performance by using the Next Generation Simulation (NGSIM) dataset.}
}


@article{DBLP:journals/cn/WeiPXLYW24,
	author = {Xingchen Wei and
                  Laixian Peng and
                  Renhui Xu and
                  Aijing Li and
                  Xingyue Yu and
                  Hai Wang},
	title = {3D position deployment and performance optimization of mmWave UAV-assisted
                  HetIoT under jamming condition},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110478},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110478},
	doi = {10.1016/J.COMNET.2024.110478},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WeiPXLYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous Internet of Things (HetIoT) has received widespread attention due to its provision of various convenient services in fields such as smart cities, intelligent transportation, environmental monitoring and security systems. Due to HetIoT inherently demands high data rates, bandwidth, and low latency, the application of millimeter-wave (mmWave) unmanned aerial vehicle (UAV) as emergency aerial base station (ABS) providing services to HetIoT users has become a low-cost and efficient means. However, due to the sensitivity of mmWave to obstacles and jamming, guaranteeing network performance has become a pressing issue. This paper considers a mmWave UAV-assisted HetIoT under jamming conditions, where auxiliary ABSs serve multiple ground users (GUs) who generate a large amount of sensor data. We establish a coverage maximization problem under the constraints of signal-to-interference ratio (SIR) threshold, maximum power of ABSs and maximum number of GUs that the base station can serve, and propose a novel ABS hovering deployment algorithm M-HiAPSO that combines the artificial potential field (APF) method and the improved particle swarm optimization (PSO) algorithm in a hierarchical manner. Specifically, the multi-element APF method is used to characterize the horizontal force between nodes, combined with the improved hierarchical adaptive PSO algorithm to adjust the horizontal position of the ABS to obtain the optimal UAV hovering position and power allocation strategy. Numerical results show that the coverage rate reached 96.2% when the number of iterations was 283, and it can reach up to 99.6%.}
}


@article{DBLP:journals/cn/MartinVV24,
	author = {Raul Martin and
                  Iv{\'{a}}n Vidal and
                  Francisco Valera},
	title = {A software-defined connectivity service for multi-cluster cloud native
                  applications},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110479},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110479},
	doi = {10.1016/J.COMNET.2024.110479},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MartinVV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containerization technologies have risen in popularity for deploying microservices applications in cloud-native environments, offering the benefits of traditional virtualization with reduced overhead. However, existing container networking solutions lack support for applications requiring isolated link-layer communications among containers in different clusters. These communications are fundamental to enable the seamless integration of cloud-native solutions in 5G and beyond networks. Accordingly, we present an SDN-enabled networking solution that supports the creation of isolated link-layer virtual networks between containers across different Kubernetes clusters by building virtual circuits that dynamically adapt to changes in the topology. In this article, we introduce our solution, highlighting its advantages over existing alternatives, and provide a comprehensive design overview. Additionally, we validate it through an experiment, offering a deeper understanding of its functionality. Our work fills an existing gap for applications with inter-cluster link-layer networking access requirements in the cloud-native ecosystem.}
}


@article{DBLP:journals/cn/AlexandriD24,
	author = {Talmon Alexandri and
                  Roee Diamant},
	title = {Detection and characterization of ship underwater radiated narrowband
                  noise},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110480},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110480},
	doi = {10.1016/J.COMNET.2024.110480},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AlexandriD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater radiated noise (URN) emitted by ships is a high-intensity noise that interferes with acoustic transmissions. It is composed of broadband cavitation noise, tonal noise and its harmonics that can be observed up to several tens of kHz. The in-situ detection of ship noise and the identification of its tonal components are therefore useful for adaptive underwater communication techniques and for channel sensing. To this end, we present a detection scheme to identify the presence of a vessel and identifying its narrowband components. For presence detection, we develop a Convolution Neural Network (CNN) model whose input is a Detection Envelope Modulation On Noise (DEMON) analysis for a given observation window. The model is trained with our collected recordings of ship data and ambient noise. After identifying the ship’s URN, our tonal detector relies on the stability and stationarity of the ship’s tonal lines, as opposed to the randomness of the ambient noise. Cross-correlation and spectral entropy are used as detection metrics. To reduce the sensitivity to the tested environment, the detection thresholds are set adaptively. The results show a favorable trade-off between precision and recall compared to benchmark methods. We share our database of URN of labeled ships.}
}


@article{DBLP:journals/cn/NithyaMMGMDC24,
	author = {D. Nithya and
                  V. Madhusudanan and
                  B. S. N. Murthy and
                  R. Geetha and
                  Nguyen Xuan Mung and
                  Nhu{-}Ngoc Dao and
                  Sungrae Cho},
	title = {Delayed dynamics analysis of {SEI2RS} malware propagation models in
                  cyber-Physical systems},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110481},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110481},
	doi = {10.1016/J.COMNET.2024.110481},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NithyaMMGMDC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber-physical systems facilitate seamless interaction between the physical and digital elements for improved efficiency, automation, and real-time monitoring across domains. This study analyzes a novel virus-spreading model called the delayed SEI2RS model, which is specifically designed for cyber-physical systems. This model incorporates a saturated incidence rate and treatment. An emphasis of this research is to explore the impact of time delay on the transient immunity interval of restored nodes. By using the time delay associated with the transitory immunity interval of recovered nodes as the bifurcation parameter, we derive a comprehensive set of appropriate conditions to assess the local stability of the malware-existence equilibrium and determine Hopf bifurcation. The center manifold theorem and normal form theory are employed to investigate the path and stability of Hopf bifurcation. Numerical calculations were used to validate the results, providing empirical evidence for the proposed model and its implications.}
}


@article{DBLP:journals/cn/Osman24,
	author = {Radwa Ahmed Osman},
	title = {Internet of Medical Things (IoMT) optimization for healthcare: {A}
                  deep learning-based interference avoidance model},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110491},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110491},
	doi = {10.1016/J.COMNET.2024.110491},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Osman24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Medical Things (IoMT) is a significant component of the broader Internet of Things (IoT), IoMT is responsible for monitoring, collecting, and transmitting critical medical data to central medical computer centers. IoMT faces significant challenges, including energy efficiency, interference from other devices in the spectrum, latency, and security and privacy concerns. As a result, in hospital settings, the development of a precise and dependable IoMT system is crucial. Consequently, this investigation offered an innovative, accurate, and dependable IoMT method. The suggested model intends to reduce or eliminate interference caused by other transmitting devices sharing the same IoMT spectrum within hospitals or medical institutions. It also presents an interference-avoidance distributed deep learning model for IoMT to medical receptionists. This model uses data from the Lagrange optimization technique to determine the ideal distance between interfering devices and medical receptions. As a result, the required system signal-to-interference-plus-noise\n(\nS\nI\nN\nR\nt\nh\n)\nis met while maintaining the highest energy efficiency (EE) and system achievable data rate (R). The proposed analytical model and deep learning model demonstrate the efficacy of the proposed approach by achieving the required system signal-to-interference-plus-noise\n(\nS\nI\nN\nR\nt\nh\n)\nwith the best energy efficiency (EE) and system achievable data rate (R) while suppressing interference to any medical receptions.}
}


@article{DBLP:journals/cn/ChenLWQWP24,
	author = {Zhuoxing Chen and
                  Yiqin Lu and
                  Haihan Wang and
                  Jiancheng Qin and
                  Meng Wang and
                  Weiqiang Pan},
	title = {Dynamic stream partitioning for time-triggered traffic in Time-Sensitive
                  Networking},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110492},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110492},
	doi = {10.1016/J.COMNET.2024.110492},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenLWQWP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-Sensitive Networking (TSN) is a promising network technology that can ensure bounded latency and jitter for industrial real-time scenarios. It establishes the IEEE 802.1 Qbv standard to precisely control the periodic transmission time of the time-triggered (TT) traffic. However, it is still a challenge to efficiently schedule the TT traffic in large-scale networks with high transmission performance. In this paper, we propose a novel method called dynamic stream partitioning (DSP) to solve this problem. It satisfies the IEEE 802.1 Qbv standard and can significantly reduce the scheduling time by reducing the number of constraints. Based on the DSP method, we analyze the impact of stream partitioning on the transmission performance of the TT traffic, and propose an indicator called Normalized Degradation of Performance (NDOP) to quantify it dynamically. Furthermore, we design a partitioning-aware dynamic routing (PADR) to expand the scheduling space. Integrated with NDOP and PADR, we propose a joint routing and dynamic stream partitioning (JR/DSP) algorithm. Extensive simulation experiments verify that the proposed JR/DSP algorithm has higher scalability, transmission performance, and schedulability compared to the static stream partitioning algorithms.}
}


@article{DBLP:journals/cn/ZhengFCZC24,
	author = {Danyang Zheng and
                  He Fang and
                  Shaohua Cao and
                  Yihan Zhong and
                  Xiaojun Cao},
	title = {Towards resources optimization in deploying service function chains
                  with shared protection},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110494},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110494},
	doi = {10.1016/J.COMNET.2024.110494},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhengFCZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging artificial intelligence techniques enable the Internet-connected devices automatically generate massive SFC requests in scenarios with ultra-reliability demands such as unmanned vehicle systems, smart factories. In such scenarios, it is imperative to deploy these huge amounts of SFCs in a fault-tolerant manner. The existing works on fault-tolerant SFC deployment employs the dedicated SFC backup protection with the resources redundancy of at least 100%, which addresses the ratio between the backup and primary resource consumptions. Due to this, the dedicated SFC protection likely prevents these reliable SFCs from being massively deployed. To tackle this challenge, for the first time, this work studies how to enhance the resource utilization by deploying SFCs with shared protection. First, we introduce new concepts of SFC protection set and sharing-risk SFC graph (SCG). Next, we define a new problem called SFCs embedding with SCG-based shared protection. Then, we propose a heuristic algorithm called SCG-based SFC embedding and protecting (SCG-SEP), which is proved to be logarithm-approximate. Extensive simulations show that SCG-SEP outperforms the benchmarks by an average of 12.25% and 43.67%.}
}


@article{DBLP:journals/cn/XuWWGHZ24,
	author = {Renjie Xu and
                  Guangwei Wu and
                  Weiping Wang and
                  Xing Gao and
                  An He and
                  Zhengpeng Zhang},
	title = {Applying self-supervised learning to network intrusion detection for
                  network flows with graph neural network},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110495},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110495},
	doi = {10.1016/J.COMNET.2024.110495},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuWWGHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows. However, most present GNN-based methods for NIDS are supervised or semi-supervised. Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. The existing GNN-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. This paper studies the application of GNNs to identify the specific types of network flows in an unsupervised manner. We first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor. Then, a self-supervised method based on graph contrastive learning is proposed. The method samples center nodes, and for each center node, generates subgraph by it and its direct neighbor nodes, and corresponding contrastive subgraph from the interpolated graph, and finally constructs positive and negative samples from subgraphs. Furthermore, a structured contrastive loss function based on edge features and graph local topology is introduced. To the best of our knowledge, it is the first GNN-based self-supervised method for the multiclass classification of network flows in NIDS. Detailed experiments conducted on four real-world datasets (NF-Bot-IoT, NF-Bot-IoT-v2, NF-CSE-CIC-IDS2018, and NF-CSE-CIC-IDS2018-v2) systematically compare our model with the state-of-the-art supervised and self-supervised models, illustrating the considerable potential of our method. Our code is accessible through https://github.com/renj-xu/NEGSC.}
}


@article{DBLP:journals/cn/DursunAD24,
	author = {Yunus Dursun and
                  Suhaib M. Al{-}Basit and
                  Zhiguo Ding},
	title = {Wireless powered NOMA-based cognitive radio for 6G networks},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110497},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110497},
	doi = {10.1016/J.COMNET.2024.110497},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DursunAD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need for physically charging mobile devices is anticipated to become a thing of the past in the not-too-distant future. In this paper, a novel approach for wireless-powered mobile devices is presented, which is based on non-orthogonal multiple access (NOMA) and multi-user multiple-input multiple-output (MU-MIMO) antenna systems. The proposed method is specifically designed for use in a cognitive underlay radio (CR) scenario, where the primary network requires a certain quality of service (QoS). Meanwhile, secondary network users can download their data using the same spectrum. The main objective of this paper is to maximize the sum rate of the secondary network. We have optimized a joint beamforming vector for both primary and secondary networks and a time-switching coefficient for energy harvesting and information transfer to achieve this objective. The initial problem formulation was non-convex, requiring the implementation of various techniques to ensure an efficient and low-complexity algorithm. To this end, we employed semi-definite programming, successive convex approximation, and alternating optimization. The proposed NOMA-based solution outperforms the TDMA-based benchmark scheme regarding sum data rate, specifically in the low transmit power region.}
}


@article{DBLP:journals/cn/RodriguezKBS24,
	author = {Jes{\'{u}}s Garc{\'{\i}}a Rodr{\'{\i}}guez and
                  Stephan Krenn and
                  Jorge Bernal Bernab{\'{e}} and
                  Antonio F. Skarmeta},
	title = {Beyond selective disclosure: Extending distributed p-ABC implementations
                  by commit-and-prove techniques},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110498},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110498},
	doi = {10.1016/J.COMNET.2024.110498},
	timestamp = {Mon, 03 Mar 2025 21:30:45 +0100},
	biburl = {https://dblp.org/rec/journals/cn/RodriguezKBS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing user awareness and regulatory framework (e.g., GDPR, eIDAS2) have contributed to considering data minimization and privacy-by-design as central guiding principles for new systems. Among others, this has led to a paradigm shift towards Self-Sovereign Identity solutions to put the user in full control over their data. Despite the promising landscape, privacy-preserving Attribute-Based Credentials (p-ABC) have not been widely adopted, mainly due to the lack of secure, flexible and efficient implementations that cover the basic and advanced needs in p-ABC systems. In this work, we tackle this gap by developing an improved zero-knowledge showing protocol of a distributed p-ABC scheme based on Pointcheval–Sanders Multi-Signatures to allow for modular extensions through commit-and-prove techniques. We use it to implement a flexible p-ABC system with decentralized issuance that, apart from the basic notions of p-ABCs, covers range proofs, pseudonyms, inspection and revocation. Lastly, we thoroughly evaluate the performance of the system under different testbed conditions, showing a significant efficiency improvement over previous implementations.}
}


@article{DBLP:journals/cn/ZhaoGLXSR24,
	author = {Liushun Zhao and
                  Deke Guo and
                  Lailong Luo and
                  Junjie Xie and
                  Yulong Shen and
                  Bangbang Ren},
	title = {Tiger Tally: {A} secure IoT data management approach based on redactable
                  blockchain},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110500},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110500},
	doi = {10.1016/J.COMNET.2024.110500},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhaoGLXSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain-based Internet of Things (IoT) data management is increasingly ubiquitous across smart cities, supply chains, e-health and other domains. The immutability of blockchain is crucial to securing these IoT data management systems. In actual application scenarios, redacting the on-chain IoT data is still desired or even legally required. Various redactable blockchain technologies have been suggested for breaking immutability and redacting on-chain data. However, conventional redactable blockchain technologies incur security and performance degradations like redact privileges abuse and key exposure, prior art merely reactively corrects rather than fundamentally forestalling such misuse. In this paper, we rethink the conflict between the immutability and redaction of blockchain-based IoT data systems and propose Tiger Tally as a secure redactable architecture to fundamentally forestall these vulnerabilities. Tiger Tally introduces a novel Targeted Policy-Based Chameleon Hash, along with tokenized redact privileges, to form integrated cryptography and access control mechanisms. We further propose a full lifecycle redactable blockchain framework with rigorous security proofs to instantiate Tiger Tally. Furthermore, to meet its practical needs, we introduce the proposed Tiger Tally to the comprehensive IoT data workflows. At last, proof-of-concept implementation and performance evaluation demonstrate that our Tiger Tally is practical for IoT data management systems and greatly reduces the time overhead caused by malicious modification by at least 73.0% or even 3.26 times at the cost of 5.4% incremental space overhead.}
}


@article{DBLP:journals/cn/RatheeKKC24,
	author = {Geetanjali Rathee and
                  Akshay Kumar and
                  Chaker Abdelaziz Kerrache and
                  Carlos T. Calafate},
	title = {A trust management solution for 5G-based future generation Internet
                  of Vehicles},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110501},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110501},
	doi = {10.1016/J.COMNET.2024.110501},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RatheeKKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, 5G Internet-of-Vehicles (IoV) related standards have drawn attention in both academia and industry for supporting an efficient and reliable communication among vehicles in the network. However, notice that 5G-IoV related issues are defined as critical in both real and cyber space environments, as malfunctioning in such systems could cause traffic paralysis, serious accidents, and privacy leaks. Therefore, in order to overcome these challenges, a trusted and ideal framework is proposed using Dempster–Shafer Theory (DST), which is combined with a recommender mechanism in order to identify several DDoS threats. The major trust elements in order to identify the legitimacy of a node include frequency statistics, trust factor, residual energy, and trust policy. The proposed trusted mechanism is designed to provide an improved and efficient security solution by avoiding intruders which attempt to disrupt a 5G-IoV network. The proposed mechanism is further analyzed against several security measures, and validated using traditional security schemes in IoV environments.}
}


@article{DBLP:journals/cn/LiLLZLL24,
	author = {Zhuo Li and
                  Shuaijun Liu and
                  Jindian Liu and
                  Yu Zhang and
                  Teng Liang and
                  Kaihua Liu},
	title = {{SIM:} {A} fast real-time graph stream summarization with improved
                  memory efficiency and accuracy},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110502},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110502},
	doi = {10.1016/J.COMNET.2024.110502},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiLLZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A graph stream composed of sequentially approaching arriving edges is commonly utilized to represent complicated structured data in interactive application systems. Since graph streams are extraordinarily vast and high velocity, efficient storage and analysis of graph streams face serious challenges. Current graph stream summarization schemes have effectively achieved the storage and management of graph streams. Unfortunately, they either cannot accomplish real-time queries or have low overall query accuracy. To this end, a novel summary structure, named Shared Interaction Matrix (SIM), is proposed for real-time queries rapidly and accurately with smaller memory. SIM is designed as a two-layer adjacency matrix with different structures to improve memory efficiency while preserving the key of heavy edges to support real-time measurement. Moreover, SIM leverages shared hash technology and an integral replacement strategy to boost insertion query speed and query accuracy. The performance of SIM is evaluated by conducting extensive experiments on the CPU and OVS platform. The experimental results show that SIM significantly enhances measurement accuracy and reduces insertion and query processing latency by 39.21%–93.50% while achieving real-time queries, compared with the state-of-the-art schemes.}
}


@article{DBLP:journals/cn/RolichTVB24,
	author = {Alexey Rolich and
                  Ion Turcanu and
                  Alexey V. Vinel and
                  Andrea Baiocchi},
	title = {Understanding the impact of persistence and propagation on the Age
                  of Information of broadcast traffic in 5G {NR-V2X} sidelink communications},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110503},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110503},
	doi = {10.1016/J.COMNET.2024.110503},
	timestamp = {Mon, 03 Mar 2025 21:30:45 +0100},
	biburl = {https://dblp.org/rec/journals/cn/RolichTVB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The current Cellular V2X (C-V2X) standard for direct communication between vehicles is based on the so called Semi-Persistent Scheduling (SPS) algorithm. It is based on the multiple access structure of 5G New Radio (NR) and exploits sensing and persistence to realize a randomized multiple access. We consider the application of SPS to support periodic broadcast traffic where no acknowledgments are provided. Persistence is a key feature, which consists of a node using the same resource for its transmissions for multiple frames. The specific resource used is randomly selected from among those that are perceived to be idle, and reselected anew after a randomized number of frames. We define a model to gain insight into the interplay between persistence and key performance metrics for the type of traffic considered, namely probability of successful delivery and Age of Information (AoI). A core version of the model is validated against simulations and used to show that there exists an optimal level that minimizes AoI. The model is then extended to account for a distance-dependent propagation model, allowing further insight into the effects of the interplay between sender–receiver distance and persistence. Finally, an even more detailed model is investigated, using ns-3-based simulations. This further analysis confirms the qualitative behaviors revealed by the analytical model and provides more insight into the complex interactions of system parameters and channel characteristics. The obtained results help to identify the limits of SPS, opening the way to system-principled parameter optimization and design of more powerful variants of the multiple access scheme.}
}


@article{DBLP:journals/cn/ImanakaKCO24,
	author = {Shoya Imanaka and
                  Akio Kawabata and
                  Bijoy Chand Chatterjee and
                  Eiji Oki},
	title = {Polynomial-time server allocation algorithm in delay-sensitive internet-of-things
                  monitoring services},
	journal = {Comput. Networks},
	volume = {248},
	pages = {110504},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110504},
	doi = {10.1016/J.COMNET.2024.110504},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ImanakaKCO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a polynomial-time algorithm for a server allocation problem in delay-sensitive Internet-of-Things (IoT) monitoring services. The server allocation problem determines the appropriate servers to which the database and application are allocated to minimize the maximum delay between the latest update of reference data and the start of application processing for monitoring data. The server allocation problem was previously handled by expressing it as an integer linear programming (ILP) problem. Nevertheless, it fails to meet the computational time complexity needed to solve the problem, and it does not offer a more efficient technique than the ILP approach. The proposed algorithm consists of two components. The first step entails choosing utilization servers for both the database and the application. Next, the second phase entails matching each usage server and its corresponding IoT device. We prove that the proposed algorithm obtains an optimal solution in polynomial time. We compare computation times between the ILP approach and the proposed algorithm. Numerical results show that the proposed algorithm obtains the optimal solution faster than the ILP approach.}
}
