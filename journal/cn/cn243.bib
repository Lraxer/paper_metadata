@article{DBLP:journals/cn/MaierACCGGSS24,
	author = {Guido Maier and
                  Antonino Albanese and
                  Michele Ciavotta and
                  Nicola Ciulli and
                  Stefano Giordano and
                  Elisa Giusti and
                  Alfredo Salvatore and
                  Giovanni Schembra},
	title = {WatchEDGE: Smart networking for distributed AI-based environmental
                  control},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110248},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110248},
	doi = {10.1016/J.COMNET.2024.110248},
	timestamp = {Wed, 22 May 2024 12:46:41 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MaierACCGGSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern applications that make extensive use of AI and ML require the support of an agile network environment that allows raw data to be processed at the edge, where they are collected, while computing functions both for the training and inference phases need to be efficiently moved from site to site. In this paper we present the WatchEDGE project. WatchEDGE studies an advanced edge-computing architecture supporting AI-based image processing applications distributed on geographically-distant sites, which can be adopted for environmental surveillance in the context of smart agriculture and wildlife protection. WatchEDGE is orchestrated to maximize data processing at the edge: each site (or “island”) is equipped with an edge-computing infrastructure that can be provided by fleets of flying drones (FANET); moreover, in the islands there are smart cameras and smart radars; cameras can be either grounded or mounted on board the UAVs. The islands are interconnected by SD-WAN technology. The project is part of the RESTART program, funded by NextGenerationEU.}
}


@article{DBLP:journals/cn/GuerraVSGG24,
	author = {Renata Rojas Guerra and
                  Anna Vizziello and
                  Pietro Savazzi and
                  Emanuele Goldoni and
                  Paolo Gamba},
	title = {Forecasting LoRaWAN {RSSI} using weather parameters: {A} comparative
                  study of ARIMA, artificial intelligence and hybrid approaches},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110258},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110258},
	doi = {10.1016/J.COMNET.2024.110258},
	timestamp = {Sat, 08 Jun 2024 13:14:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GuerraVSGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRaWAN technology’s reliability is challenged by weather parameters, which can influence the communication channel design, especially when dealing with outdoor devices. We propose to analyze this effect by evaluating the relationship between the received signal strength indicator (RSSI) and different weather parameters, as well as its temporal changes. A rigorous statistical analysis of the RSSI sequences is conducted to assess if they could be represented by a specific statistical model. For this purpose, several models are investigated. The Artificial Intelligence (AI) algorithms cover machine learning (ML) and deep learning methods are appealing when dealing with time series forecasting. Nevertheless, the classical autoregressive integrated moving average (ARIMA) model can be an attractive alternative due to its simplicity. Therefore, this work proposes a comparative study of ARIMA, AI, and hybrid approaches to forecast the RSSI using weather parameters as regressors. The considered AI algorithms are the artificial neural network (ANN), support vector machine (SVM), random forest (RF), and Long Short-Term Memory (LSTM). Also, hybrid models are constructed, coupling the ARIMA with them. The models are evaluated in time series of RSSI, measured by eight different LoRaWAN transmitter nodes and considering the temperature, pressure, relative humidity, and rain as weather parameters. Our analysis reveals that temperature is the dominant factor among weather parameters, and negatively affects RSSI. The ARIMA model that uses only the temperature as a regressor provides consistently better fits than the ARIMA without regressors. Moreover, coupling the ARIMA with the temperature as a regressor and the ANN (ARIMA-ANN) is the best option among the pure AI and hybrid approaches. However, it provided accuracy measures very close to those obtained from the ARIMA model fitted in the first stage, with similar performance. Therefore, the ARIMA model considering the temperature is the most competitive alternative when analyzing RSSI measurements, with the advantage of being the most straightforward method. These results suggest that the RSSI from the analyzed LoRaWAN receiver nodes may not present nonlinear patterns and, considering several weather parameters, they are affected mainly by the outdoor temperature.}
}


@article{DBLP:journals/cn/GuGLYZLX24,
	author = {Zheyuan Gu and
                  Gaopeng Gou and
                  Chang Liu and
                  Chen Yang and
                  Xiyuan Zhang and
                  Zhen Li and
                  Gang Xiong},
	title = {Let gambling hide nowhere: Detecting illegal mobile gambling apps
                  via heterogeneous graph-based encrypted traffic analysis},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110278},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110278},
	doi = {10.1016/J.COMNET.2024.110278},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuGLYZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile gambling apps, as a new type of online gambling service, have not only enriched people’s online entertainment activities but also brought about negative impacts for both individuals and society. Existing gambling app detection methods mainly extract static content-based characteristics, such as vision and textual features, to mine the key information from the user interface to detect gambling apps. In this paper, we are the first to introduce a dynamic communication characteristic-based approach by utilizing encrypted traffic analysis. Firstly, we conduct an analysis of the communication characteristics of 175 popular gambling apps in China, and unveil their important server domain name randomization and inter-app level familial characteristics, which cannot be well-leveraged by previous encrypted traffic analysis methods. Based on the analysis results, we design HeCGamb, a Heterogeneous Communication Graph-based method to enhance the Gambling app detection performance. HeCGamb models the inter-flow relations from various traffic flows to mine the flow-level communication patterns of gambling apps. Based on the inter-flow relations, HeCGamb further constructs inter-app relations to utilize the app-level familial characteristics. Finally, the multi-view semantic information from both servers and apps with inter-app relations are fused to generate the app node representations to comprehensively leverage the characteristics from server domains and familial apps. Extensive experiments not only demonstrate the superior performance(94.1% F1-score and 96.8% AUC score in open world) of HeCGamb, but also highlight its potential in gambling industry chain tracking.}
}


@article{DBLP:journals/cn/Kilic24,
	author = {Ahmet Kilic},
	title = {TLS-handshake for Plug and Charge in vehicular communications},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110281},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110281},
	doi = {10.1016/J.COMNET.2024.110281},
	timestamp = {Wed, 22 May 2024 12:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Kilic24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Electric vehicle technology has brought some important issues to order. Securing the communication between electric vehicle and charging station is an important one. The International Organization for Standardization released ISO 15118 standard to specify the communication between electric vehicles and electric vehicle supply equipment. The ISO 15118 standard requires Transport Layer Security function for secure charging communication between electric vehicle and charging station as mandatory. The Transport Layer Security requirements have been completely changed with the latest version of the standard (ISO 15118-20) and have not been clearly described and implemented yet. The implementation of the function causes an inevitable vulnerability due to non-functioning authentication resulting with the entire vehicle function and security system can suddenly stop working. For this reason, new Transport Layer Security protocol must urgently be analyzed, developed, integrated, and implemented.}
}


@article{DBLP:journals/cn/BathamT24,
	author = {Deepak Batham and
                  Vandana Vikas Thakare},
	title = {An improved cost function-based class of service provisioning scheme
                  for elastic optical networks},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110283},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110283},
	doi = {10.1016/J.COMNET.2024.110283},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BathamT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Elastic optical network (EON) is a trusted communication network which supports the advance services like 5G/6G, IoT, cloud computing, store-and-forward big-data, high definition video-on-demand, artificial intelligence, and machine learning. Due to the enormous use of these emerging services, the Internet traffic increases manifold time. These services require high and flexible bandwidth with heterogeneous data rate where each service has different characteristics such as prime or fixed, deadline-driven, or delay-tolerant. Based on these characteristics, the static traffic can be categorized as three class of services (CoSs) such as CoS1 (prime and fixed or hard time bound traffic), CoS2 (deadline-driven traffic), and CoS3 (delay-tolerant traffic). In this paper, the improved cost function-based CoS provisioning (ICFCoSP) schemes using two spectrum allocation (SA) methods (i.e., Method-1(M1) and Method-2 (M2)) are proposed called as ICFCoSP-M1 and ICFCoSP-M2, respectively. The proposed ICFCoSP-M1 and ICFCoSP-M2 schemes prioritize the connection requests first according to the cost function value then select an appropriate SA method in which first fit (FF) and last fit (LF) technique are used as per (i) the requested even or odd number of frequency slots (FSs) in M1, and (ii) the value of the cost function in M2, respectively. The cost function includes CoS, requested FS, and pathlength of the selected route. The performance of the proposed schemes is tested on the standard INDIAN and ARPANET network topologies using MATLAB. The rigorous simulation experiments have been performed, and the average results are evaluated on the metric of connection established, bandwidth blocking probability, network spectrum fragmentation, spectrum utilization ratio, network capacity utilization, revenue index, and fairness index. The proposed schemes show significantly improved performances on each evaluating metric as compared to the existing CoS provisioning (CoSP), and cost function-based CoS provisioning (CFCoSP) scheme.}
}


@article{DBLP:journals/cn/MalandrinoNC24,
	author = {Francesco Malandrino and
                  Alessandro Nordio and
                  Carla{-}Fabiana Chiasserini},
	title = {Eavesdropping with intelligent reflective surfaces: Near-optimal configuration
                  cycling},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110284},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110284},
	doi = {10.1016/J.COMNET.2024.110284},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MalandrinoNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surfaces (IRSs) have several prominent advantages, including improving the level of wireless communication security and privacy. In this work, we focus on the latter aspect and introduce a strategy to counteract the presence of passive eavesdroppers overhearing transmissions from a base station towards legitimate users that are facilitated by the presence of IRSs. Specifically, we envision a transmission scheme that cycles across a number of IRS-to-user assignments, and we select them in a near-optimal fashion, thus guaranteeing both a high data rate and a good secrecy rate. Unlike most of the existing works addressing passive eavesdropping, the strategy we envision has low complexity and is suitable for scenarios where nodes are equipped with a limited number of antennas. Through our performance evaluation, we highlight the trade-off between the legitimate users’ data rate and secrecy rate, and how the system parameters affect such a trade-off.}
}


@article{DBLP:journals/cn/ZhaoQPLZG24,
	author = {Zhe Zhao and
                  Zhiliang Qiu and
                  Weitao Pan and
                  Hui Li and
                  Ling Zheng and
                  Ya Gao},
	title = {Design and implementation of a frame preemption model without guard
                  bands for time-sensitive networking},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110285},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110285},
	doi = {10.1016/J.COMNET.2024.110285},
	timestamp = {Sat, 08 Jun 2024 13:14:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhaoQPLZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In time-sensitive networks, the frames of time-triggered (TT) flows need to be transmitted in scheduled slots. To avoid the interference of frames from other flows to the frames of the TT flows, guard bands are generally reserved prior to scheduled slots. The time-sensitive networking (TSN) standard, IEEE 802.1Qbu, specifies a frame preemption model that enables frames of TT flows (preemption frames) to preempt frames of other flows (preempted frames). According to this model, preempted frames are divided into fragments that are transmitted in the gaps of preemption frames. Consequently, the IEEE 802.1Qbu model reduces the guard band from the longest Ethernet frame (typically 1518 bytes) to 123 bytes, which improves the bandwidth utilization and delay performance of preempted frames without causing frame disorder. However, in scenarios in which the preempted frame load is heavy, and the length is short, numerous guard bands smaller than 123 bytes are generated. These guard bands prevent the IEEE 802.1Qbu model from transmitting preempted frame fragments, resulting in a considerable decrease in bandwidth utilization. To solve this problem, we propose a novel frame preemption model without guard bands, based on a padding and splicing mechanism. While ensuring that preemption frames are transmitted according to scheduled slots, this model can transmit preempted frames within arbitrary byte gaps based on the schedule without obtaining the length of the preempted frames in advance. We compared the transmission-delay performance of the proposed model with that of the IEEE 802.1Qbu model using a theoretical analysis. The evaluation results obtained from heavily loaded preemption frame scenarios revealed that the proposed model improved link utilization by 32.8% relative to the IEEE 802.1Qbu model and reduced the transmission delay by more than one order of magnitude. Moreover, when the IEEE 802.1Qbu model fails, the proposed model still transmits 60% of preempted frames.}
}


@article{DBLP:journals/cn/MauroGPSL24,
	author = {Mario Di Mauro and
                  Giovanni Galatro and
                  Fabio Postiglione and
                  Wei Song and
                  Antonio Liotta},
	title = {Hybrid learning strategies for multivariate time series forecasting
                  of network quality metrics},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110286},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110286},
	doi = {10.1016/J.COMNET.2024.110286},
	timestamp = {Sat, 08 Jun 2024 13:14:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MauroGPSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work addresses the challenge of forecasting temporal metrics that characterize cellular traffic behavior. The ultimate goal is to provide network operators with a valuable tool for modeling mobile network traffic and optimizing connected resources. The idea is to estimate beforehand the temporal evolution of some Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics, which is helpful for accurately tuning the allocation of network resources. Remarkably, these metrics (expressed as time series) are typically correlated, and changes in one time series can affect others in a variety of ways and to different extents. For example, high network delay (a QoS-related metric) is associated with degradation in voice quality over time (a QoE-related metric). Accordingly, we address the problem of cellular traffic forecasting with correlated time series, proposing three innovative hybrid learning strategies designed by combining the advantages of two approaches:\n(\ni\n)\na statistical approach, implemented through the Vector Autoregressive (VAR) model, which encodes each metric as a combination of past values of the same metric along with a combination of values of other related metrics, resulting in a multivariate structure; and\n(\ni\ni\n)\nan approach based on deep learning techniques (specifically, CNN, LSTM, and GRU) which operate on such a multivariate structure to perform the forecasting. The resulting performance demonstrates the benefits of the proposed hybrid schemes (VAR-CNN, VAR-LSTM, VAR-GRU) over their pure counterparts, with a significant reduction in forecasting errors. The network metrics were gathered in a real urban cellular environment, where the presence of exogenous factors (e.g., interferences, weather conditions, etc.) makes the forecasting assessment particularly challenging.}
}


@article{DBLP:journals/cn/MunozSP24,
	author = {Irene Vil{\`{a}} Mu{\~{n}}oz and
                  Oriol Sallent and
                  Jordi P{\'{e}}rez{-}Romero},
	title = {Relay-empowered beyond 5G radio access networks with edge computing
                  capabilities},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110287},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110287},
	doi = {10.1016/J.COMNET.2024.110287},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MunozSP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relevant services envisaged for beyond 5G (B5G) systems, such as extended reality and holographic communications, have extremely demanding user experience requirements with significant computational and communication demands. While edge computing aims to address the computation requirements by offloading the computational tasks to edge servers near the user, the communication will take advantage of the technologies developed for 5G New Radio jointly with an never-before-seen degree of network densification. This paper proposes the use of relays with edge computing capabilities. The approach's potential for B5G are identified, and a system model is defined to characterize both computational and communications viewpoints. Based on this, results are provided to highlight the gains and limitations of the proposed approach from a system-level perspective. Finally, the main challenges for enabling relays with computing capabilities in B5G deployments are discussed.}
}


@article{DBLP:journals/cn/LiW24,
	author = {Xia Li and
                  Yuping Wang},
	title = {TABDeep: {A} two-level action branch architecture-based deep reinforcement
                  learning for distributed sub-tree scheduling of online multicast sessions
                  in {EON}},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110288},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110288},
	doi = {10.1016/J.COMNET.2024.110288},
	timestamp = {Wed, 22 May 2024 12:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The surge in demand for multimedia applications has brought a great pressure on network bandwidth resources. In order to improve the efficiency of bandwidth resource usage and ensure the transmission quality of multicast sessions, it is very urgent to effectively schedule online multicast sessions. To do so, in this paper, we study the distributed sub-tree scheduling problem of online multicast sessions in elastic optical network (EON). First, we model the problem as a Markov Decision Process (MDP) by defining state, action and reward. Second, a two-level action branch architecture-based deep reinforcement learning (TABDeep) algorithm is proposed to realize the destination node grouping, source node selection, route establishment and spectrum assignment of multicast sessions. Finally, to test the superiority of TABDeep, we construct a series of simulation experiments to compare TABDeep with a link-aware distributed steiner sub-tree benchmark algorithm (LA-DSST). The experimental results confirm that TABDeep performs better than LA-DSST in reducing the blocking probability of the session.}
}


@article{DBLP:journals/cn/ChintapalliPTM24,
	author = {Venkatarami Reddy Chintapalli and
                  Rajat Partani and
                  Bheemarjuna Reddy Tamma and
                  C. Siva Ram Murthy},
	title = {Energy efficient and delay aware deployment of parallelized service
                  function chains in NFV-based networks},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110289},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110289},
	doi = {10.1016/J.COMNET.2024.110289},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChintapalliPTM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Functions Virtualization (NFV) replaces traditional hardware-based network equipment and middleboxes with flexible Virtualized Network Functions (VNFs) in order to reduce costs and improve agility and scalability. The VNFs are logically arranged in a specific sequence to form a Service Function Chain (SFC) which ensures that the traffic is processed according to the desired service requirements. However, the inherent length of SFCs leads to an undesirable increase in end-to-end delay experienced by the packets. Parallelized SFC (PSFC) addresses this problem by trying to allow multiple VNFs of the SFC to process packets in parallel by co-locating parallelizable VNFs on the same server. The energy-efficient deployment of PSFCs while considering the impact of contention for the shared resources on the server is unexplored in the literature. Hence, in this work, we formulate the PSFC deployment problem as an Integer Linear Program (ILP) that minimizes energy consumption while considering the impact of shared resource contentions without violating end-to-delay constraints. Since the ILP is NP-hard, we also propose a heuristic scheme named EPSFC, which provides flexible resource allocation-based deployment that minimizes the total energy consumption and ensures end-to-end delay requirements while considering the effects of shared resource contentions on the end-to-end delay. The effectiveness of EPSFC is evaluated through extensive simulations, and the results show a significant reduction in energy consumption while improving the PSFC acceptance ratio as compared to state-of-the-art schemes.}
}


@article{DBLP:journals/cn/GudepuCCVTK24,
	author = {Venkateswarlu Gudepu and
                  Venkatarami Reddy Chintapalli and
                  Piero Castoldi and
                  Luca Valcarenghi and
                  Bheemarjuna Reddy Tamma and
                  Koteswararao Kondepu},
	title = {The drift handling framework for open radio access networks: An experimental
                  evaluation},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110290},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110290},
	doi = {10.1016/J.COMNET.2024.110290},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GudepuCCVTK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Open Radio Access Network (i.e., Open RAN) aims to transform the inflexible, proprietary RAN into one that is flexible, programmable, and disaggregated to provide intelligent solutions at the network edge for delivering the highly dynamic service demands of Beyond 5G (B5G) networks. Open RAN Working Group-2 (WG2) focuses on the architecture and standards of Artificial Intelligence/Machine Learning (AI/ML) to meet the Service Level Agreement (SLA) requirements for emerging use cases. The inherent dynamic nature of B5G networks and their applications often leads to AI/ML model performance degradation (i.e., drift), resulting in violations of SLAs, over- or under-provisioning of resources, etc.}
}


@article{DBLP:journals/cn/GhafourEHA24,
	author = {Mohamed G. Abd El Ghafour and
                  Ahmed H. Abd El{-}Malek and
                  Ola E. Hassan and
                  Mohammed Abo{-}Zahhad},
	title = {Securing cooperative vehicular networks amid obstructing vehicles
                  and mixed fading channels},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110291},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110291},
	doi = {10.1016/J.COMNET.2024.110291},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GhafourEHA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart cities (SCs) were founded on the basis of the Internet of Vehicles paradigm, aiming to enhance the quality of life. Despite not having a unique composition, the intelligent transportation services (ITS) is a key component in most of the SCs. The ITS integrates the new communication systems along with the traditional transportation systems, forming a universal network of static and vehicular entities communicating over wireless broadcast channels. In addition to its vulnerability to co-channel interference (CCI), eavesdropping, and fading, the vehicle-to-vehicle communication channels could be subjected to shadowed fading. That is due to the probabilistic obstruction by big vehicles. Hence, degrading the signal-to-noise ratio at the receiving vehicle severely. This work evaluates the secrecy performance of a practical cooperative vehicular relaying network in terms of its secrecy outage probability (SOP). Due to the probabilistic existence of a vehicular obstacle, three different communication scenarios are considered and a novel closed-form analytical expression for the SOP is obtained over mixed Nakagami-m fading and Nakagami-N-Gamma shadowed fading channels. The system comprises legitimate source and destination operating in full-duplex mode, thus imposing CCI on both the relay and the passive eavesdropper. The result demonstrate the impacts of the secrecy data rate threshold, the strength of shadowing, the density of the obstructing vehicles, and several parameters on the SOP. High levels of shadowing strength and density of obstructing vehicles significantly degrades the secrecy performance by increasing the SOP. Accordingly, adopting the mixed fading channel model is of great importance for practical network modeling.}
}


@article{DBLP:journals/cn/LarsenCRB24,
	author = {Line M. P. Larsen and
                  Henrik Lehrmann Christiansen and
                  Sarah Ruepp and
                  Michael S. Berger},
	title = {The Evolution of Mobile Network Operations: {A} Comprehensive Analysis
                  of Open {RAN} Adoption},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110292},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110292},
	doi = {10.1016/J.COMNET.2024.110292},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LarsenCRB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Mobile Network Operators (MNOs) seek to revolutionise their infrastructures and meet the growing demands of a data-centric world, the adoption of open RAN (Radio Access Network) technology has emerged as a compelling solution. This paper offers a comprehensive exploration of open RAN’s transformative potential within the MNO landscape and investigates the concrete implementation opportunity of Open-RAN (O-RAN). Starting out by providing an exhaustive overview of the state of the art in open RAN technology, research and deployments, leads to an in-depth analysis of the decision roadmap of open RAN adoption. This roadmap considers the network design, vendor criteria and implementation strategy. This paper contributes with a comprehensive examination of components and an overview of increasingly complex design opportunities including functional split and accelerator options. Thus, it serves as an enlightening and pragmatic guide for MNOs navigating the transition to opening the interfaces in their RAN. Finally the paper proposes a pragmatic exploration of implementation strategies, providing insights to the drawbacks MNOs will face and proposed solutions. Hence, this paper highlights how the O-RAN system is built in modules, which adds additional importance to the increasingly complex design phase when multiple aspects must be evaluated to become the (r)evolution of mobile networks.}
}


@article{DBLP:journals/cn/RajabYS24,
	author = {Mirna El Rajab and
                  Li Yang and
                  Abdallah Shami},
	title = {Zero-touch networks: Towards next-generation network automation},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110294},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110294},
	doi = {10.1016/J.COMNET.2024.110294},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RajabYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Zero-touch network and Service Management (ZSM) framework represents an emerging paradigm in the management of the fifth-generation (5G) and Beyond (5G+) networks, offering automated self-management and self-healing capabilities to address the escalating complexity and the growing data volume of modern networks. ZSM frameworks leverage advanced technologies such as Machine Learning (ML) to enable intelligent decision-making and reduce human intervention. This paper presents a comprehensive survey of the applications of the ZSM framework, covering network optimization, traffic monitoring, energy efficiency, and security aspects of next-generation networks. The paper explores the challenges associated with ZSM, particularly those related to ML, which necessitate the need to explore diverse network automation solutions. In this context, the study investigates the application of Automated ML (AutoML) within a ZSM framework, to reduce network management costs and enhance performance. AutoML automates the selection and tuning process of a ML model for a given task. Specifically, the focus is on AutoML’s ability to predict application throughput and autonomously adapt to data drift. Experimental results demonstrate the superiority of the proposed AutoML pipeline over traditional ML in terms of prediction accuracy. Integrating AutoML and ZSM concepts significantly reduces network configuration and management efforts, allowing operators to allocate more time and resources to other important tasks. The paper also provides a high-level 5G system architecture incorporating AutoML and ZSM concepts. This research highlights the potential of ZSM and AutoML to revolutionize the management of 5G+ networks, enabling automated decision-making and empowering network operators to achieve higher efficiency, improved performance, and enhanced user experience.}
}


@article{DBLP:journals/cn/GallegoMadridBRSS24,
	author = {Jorge Gallego{-}Madrid and
                  Irene Bru{-}Santa and
                  Alvaro Ruiz{-}Rodenas and
                  Ramon Sanchez{-}Iborra and
                  Antonio F. Skarmeta},
	title = {Machine learning-powered traffic processing in commodity hardware
                  with eBPF},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110295},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110295},
	doi = {10.1016/J.COMNET.2024.110295},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GallegoMadridBRSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network softwarization is paving the way for the design and development of Next-Generation Networks (NGNs), which are demanding profound improvements to existing communication infrastructures. Two of the fundamental pillars of NGNs are flexibility and intelligence to create elastic network functions capable of managing complex communication systems in an efficient and cost-effective way. In this sense, the extended Berkeley Packet Filter (eBPF) is a state-of-the-art solution that enables low-latency traffic processing within the Linux kernel in commodity hardware. When combined with Machine Learning (ML) algorithms, it becomes a promising enabler to perform smart monitoring and networking tasks at any required place of the fog-edge-cloud continuum. In this work, we present a solution that leverages eBPF to integrate ML-based intelligence with fast packet processing within the Linux kernel, enabling the execution of complex computational tasks in a flexible way, saving resources and reducing processing latencies. A real implementation and a series of experiments have been carried out in an Internet of Things (IoT) scenario to evaluate the performance of the solution to detect attacks in a 6LowPAN system. The performance of the in-kernel implementation shows a considerable reduction in the execution time (-97%) and CPU usage (-6%) of a Multi-Layer Perceptron (MLP) model in comparison with a user space development approach; thus positioning our proposal as a promising solution to embed ML-powered fast packet processing within the Linux kernel.}
}


@article{DBLP:journals/cn/PengGHX24,
	author = {Yufei Peng and
                  Yingya Guo and
                  Run Hao and
                  Chengzhe Xu},
	title = {Network traffic prediction with Attention-based Spatial-Temporal Graph
                  Network},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110296},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110296},
	doi = {10.1016/J.COMNET.2024.110296},
	timestamp = {Wed, 22 May 2024 12:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PengGHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic prediction plays a significant role in network management. Previous network traffic prediction methods mainly focus on the temporal relationship between network traffic, and used time series models to predict network traffic, ignoring the spatial information contained in traffic data. Therefore, the prediction accuracy is limited, especially in long-term prediction. To improve the prediction accuracy of the dynamic network traffic in the long term, we propose an Attention-based Spatial–Temporal Graph Network (ASTGN) model for network traffic prediction to better capture both the temporal and spatial relations between the network traffic. Specifically, in ASTGN, we exploit an encoder–decoder architecture, where the encoder encodes the input network traffic and the decoder outputs the predicted network traffic sequences, integrating the temporal and spatial information of the network traffic data through the Spatio-Temporal Embedding module. The experimental results demonstrate the superiority of our proposed method ASTGN in long-term prediction.}
}


@article{DBLP:journals/cn/ValkoK24,
	author = {Danila Valko and
                  Daniel Kudenko},
	title = {Reducing {CO2} emissions in a peer-to-peer distributed payment network:
                  Does geography matter in the lightning network?},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110297},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110297},
	doi = {10.1016/J.COMNET.2024.110297},
	timestamp = {Wed, 22 May 2024 12:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ValkoK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bitcoin, Ethereum and other blockchain-related globally distributed peer-to-peer networks have become an important payment infrastructure. Because they are naturally decentralized and distributed, they can consume resources unevenly, raising some sustainability concerns. The Lightning Network (LN) is a growing but understudied second layer network built on blockchain infrastructure and designed to make fast and anonymous global multi-pass payments. At a conservative estimate, this infrastructure produces about 1.4 million tons of carbon dioxide (\nCO\n2\n) per year. Thus, in this paper, we aim to justify a general approach to consider topology and the geographical distribution of this type of network in the design and investigate ways to prevent excessive\nCO\n2\nemissions. While the LN itself shows great promise as a scalable and widely adopted solution, there has been limited research exploring its structure, distribution, and performance from a sustainability perspective. This study contributes to analyzing the LN’s topology, geospatial distribution, and pathfinding algorithms. By examining real-world data snapshots of the LN, we investigate the relationship between payment routes that are produced by native pathfinding algorithms, geographical distribution of the network and the carbon intensity of electricity in the countries involved in the final payment paths. Our analysis highlights the important structural and geospatial characteristics of the LN and reveals a significant correlation between the length of payment paths, geographical distance, carbon intensity of electricity and other features. To tackle sustainability concerns, we propose an original pathfinding heuristic that effectively prevents excessive carbon dioxide emissions in LN infrastructure. Our computational experiments have shown that, under optimal parameters, such a heuristic can prevent the associated\nCO\n2\nemissions both directly – by limiting path lengths, number of intercountry and intercontinental hops – and indirectly — by giving more weight to channels covering places with lower carbon intensity of electricity. Technically, the highest result it achieves is as follows: the average path length reduced by 28.7%, the average number of intercontinental hops by 28.7%, the average number of intercountry hops by 21.3%, and the average carbon intensity by 9.4%. This solution also maintains a compromise between the average locktimes and fee ratios. In conclusion, we discuss that geographic distribution is a rather important characteristic of decentralized peer-to-peer payment networks, which is usually underestimated at the network design stage.}
}


@article{DBLP:journals/cn/LiX24,
	author = {Zhiyuan Li and
                  Xiaoping Xu},
	title = {L2-BiTCN-CNN: Spatio-temporal features fusion-based multi-classification
                  model for various internet applications identification},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110298},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110298},
	doi = {10.1016/J.COMNET.2024.110298},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying various types of Internet applications is a key issue for network management. Traffic classification is one of the important technologies for identifying Internet applications traffic. However, some secure technologies such as protocol encryption, information hiding, and traffic obfuscation pose new challenges to traffic identification and classification, especially in feature extraction. In addition, multi-classification has always been a difficult problem in traffic classification research. This paper proposes a spatio-temporal features fusion-based multi-classification model, named as L2-BiTCN-CNN. Our approach combines bidirectional temporal convolutional network (TCN) and convolutional neural network (CNN) to create an enhanced model. The L2-BiTCN model utilizes a bidirectional approach to learn both forward and backward sequence features of traffic, with a focus on the hierarchical structure of network traffic known as “byte-packet-session flow”. To fully extract timing characteristics between adjacent bytes and packets, this model incorporates attention mechanisms for both “byte hierarchy” and “packet hierarchy”. Filters of various sizes are utilizes in the CNN stage to capture packet features of different ranges. The spatial features of the packets are then obtained through two convolution and average pooling processes. The results of the experiments provide evidence of the effectiveness and superiority of our approach. Our model achieved an impressive overall accuracy of 99.839% and 97.186% in two public available datasets, namely USTC-TFC2016 and ISCX VPN-nonVPN2016, respectively. It outperforms the state-of-the-art models in terms of classification performance, which suggests that the model has the potential to be a valuable tool in various applications that require high accuracy.}
}


@article{DBLP:journals/cn/JinZG24,
	author = {Yanliang Jin and
                  Jiahao Zhou and
                  Yuan Gao},
	title = {HSGAN-IoT: {A} hierarchical semi-supervised generative adversarial
                  networks for IoT device classification},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110299},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110299},
	doi = {10.1016/J.COMNET.2024.110299},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JinZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, IoT device classification has become a highly focused issue, because it can achieve network performance optimization, security threat detection, application scenario analysis and device performance evaluation. Existing researches mainly focus on two types of methods: machine learning (ML) and deep learning (DL). However, with the continuous emergence of new devices, ML methods require experts to constantly update and design high-quality features, while DL methods require capturing new data from raw traffic and manually labeling it. These processes are time-consuming and error prone. To solve the problems above, this paper proposes a hierarchical semi-supervised generative adversarial networks for IoT device classification called HSGAN-IoT. HSGAN-IoT proposes a new structure and joint loss to make adversarial training focus on classification tasks instead of generating tasks. Compared to traditional models, HSGAN-IoT classifies a device by three attributes: type, function, and manufacturer. This enables it to classify unknown devices which have not appeared during training. Moreover, HSGAN-IoT avoids redundant calculations through its hierarchical classification structure, which filters out IoT devices from raw traffic data in the first layer and performs further classification in the second layer. This paper also establishes a fusion dataset to test the classification performance of HSGAN-IoT. The results show that HSGAN-IoT is superior to the state-of-the-art researches, with accuracy rates of 99.56%, 96.97%, and 97.86% on three classification tasks, respectively.}
}


@article{DBLP:journals/cn/LiaoDHT24,
	author = {Zhuofan Liao and
                  Wenqiang Deng and
                  Shiming He and
                  Qiang Tang},
	title = {Collaborative Filtering-based Fast Delay-aware algorithm for joint
                  {VNF} deployment and migration in edge networks},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110300},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110300},
	doi = {10.1016/J.COMNET.2024.110300},
	timestamp = {Wed, 22 May 2024 12:46:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiaoDHT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Network Function Virtualization (NFV) continues to advance, Virtual Network Functions (VNFs) such as firewalls are increasingly used. Service Function Chains (SFCs) are formed by combining specific VNFs in a particular order, which are then deployed on the physical network to provide dedicated services to end users. Occasionally, partial VNF migration is employed to maintain service and network stability. However, decision-making times and system delays may become unacceptable due to the heterogeneous resource requirements of VNFs and the massive state migration of VNFs, especially in Multi-access Edge Computing (MEC) networks where resources are scarce and demand fluctuations are frequent. To solve these challenges, we first formulate the problem of Minimizing decision Time and system Latency for joint VNF Deployment and Migration (MTLDM) as a multi-objective optimization problem. Then, we propose a Collaborative Filtering-based Fast Delay-aware algorithm (CFFD) to solve this problem. In this algorithm, we introduce an innovative approach, referred to as the collaborative filtering-based method, which utilizes the preference information of deployed/migrated VNFs to assist the current VNF deployment/migration in reducing decision-making time. Additionally, we design a similarity-based method in CFFD to search for suitable hosts for the current VNF, thereby reducing the complexity caused by heterogeneity and minimizing system latency. Furthermore, we implement a heuristic method in CFFD to increase the number of accepted requests. In the end, extensive simulations are conducted to evaluate the performance of CFFD in comparison with baseline algorithms and to select the suitable similarity algorithm. The results of the selection simulation show that Manhattan distance and cosine similarity are superior to Pearson’s correlation. Moreover, the comparison simulation results indicate that CFFD outperforms the baseline algorithms in terms of delay optimization and decision time by up to 22.08% and 99%, respectively.}
}


@article{DBLP:journals/cn/BuccafurriAIL24,
	author = {Francesco Buccafurri and
                  Vincenzo De Angelis and
                  Maria Francesca Idone and
                  Cecilia Labrini},
	title = {A hierarchical distributed trusted location service achieving location
                  k-anonymity against the global observer},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110301},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110301},
	doi = {10.1016/J.COMNET.2024.110301},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BuccafurriAIL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As widely known in the literature, location-based services can seriously threaten users’ privacy. Privacy-aware location-based services can be obtained by protecting the user’s identity, so that queries cannot be linked with users. A way to do this is to place a trusted third party, called Location Trusted Service, between the user and the service provider, with the role of mediating the queries coming from the users and proxying them to the provider. Before proxying the query, the Location Trusted Service builds a cloaking area that includes a sufficient number of users such that it can represent an anonymity set. This way, the identity of the user is protected against an untrusted service provider. Unfortunately, in wide-area scenarios, a centralized location-trusted service might represent a serious threat to security and privacy because the service represents a single point of failure that manages very critical and massive information. Moreover, privacy protection also against a global adversary capable to monitor the whole traffic, would result in an excessive amount of cover traffic in the network (being cover traffic necessary in this threat model).}
}


@article{DBLP:journals/cn/ChenRGYQG24,
	author = {Shiyou Chen and
                  Lanlan Rui and
                  Zhipeng Gao and
                  Yang Yang and
                  Xuesong Qiu and
                  Shaoyong Guo},
	title = {Resource sharing for collaborative edge learning: {A} privacy-aware
                  incentive mechanism combined with demand prediction},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110302},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110302},
	doi = {10.1016/J.COMNET.2024.110302},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenRGYQG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge intelligence benefits from ubiquitous resource capabilities by numerous edge devices. However, interest concerns and privacy issues caused by the openness of the edge network make smart devices reluctant to participate in collaborative edge learning. This work proposes a privacy-aware incentive mechanism based on combinatorial double auction (PIMCDA) to facilitate idle resource sharing of diversified edge devices. First, we establish an auction model based on blockchain to form a trusted sharing market in a multi-resource binding manner. Then, we propose a two-stage auction solution combined with differential privacy: The privacy-aware winner selection stage matches the optimal winner candidate to the task that is the closest in probability to maximize the revenue of resource providers with the obfuscated location information. The probabilistic pricing decision stage is designed based on a uniform pricing method and exponential mechanism, which can ensure bid privacy from inference attacks. Furthermore, to ensure the effectiveness of PIMCDA for long-term participation, we design a supply and demand balance mechanism with a learning-based resource prediction method. Theoretical and simulation analysis demonstrate that the proposed mechanism achieves privacy-preserving in location and bid information while ensuring effective incentive properties. Our approach effectively motivates the sharing of multiple resources in edge computing.}
}


@article{DBLP:journals/cn/JovanovicHBM24,
	author = {Zorka Jovanovic and
                  Zhe Hou and
                  Kamanashis Biswas and
                  Vallipuram Muthukkumarasamy},
	title = {Robust integration of blockchain and explainable federated learning
                  for automated credit scoring},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110303},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110303},
	doi = {10.1016/J.COMNET.2024.110303},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JovanovicHBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article examines the integration of blockchain, eXplainable Artificial Intelligence (XAI), especially in the context of federated learning, for credit scoring in financial sectors to improve the credit assessment process. Research shows that integration of these cutting-edge technologies is in its infancy, specifically in the areas of embracing broader data, model verification, behavioural reliability and model explainability for intelligent credit assessment. The conventional credit risk assessment process utilises historical application data. However, reliable and dynamic transactional customer data are necessary for robust credit risk evaluation in practice. Therefore, this research proposes a framework for integrating blockchain and XAI to enable automated credit decisions. The main focus is on effectively integrating multi-party, privacy-preserving decentralised learning models with blockchain technology to provide reliability, transparency, and explainability. The proposed framework can be a foundation for integrating technological solutions while ensuring model verification, behavioural reliability, and model explainability for intelligent credit assessment.}
}


@article{DBLP:journals/cn/HeWZXXW24,
	author = {Yunhua He and
                  Shuang Wang and
                  Zhihao Zhou and
                  Ke Xiao and
                  Anke Xie and
                  Bin Wu},
	title = {A Blockchain-based carbon emission security accounting scheme},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110304},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110304},
	doi = {10.1016/J.COMNET.2024.110304},
	timestamp = {Tue, 24 Dec 2024 22:38:41 +0100},
	biburl = {https://dblp.org/rec/journals/cn/HeWZXXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To solve the problem of climate warming, countries around the world have paid special attention to the construction of carbon governance. Carbon emission accounting is an important policy tool to control the vented CO\n2\n. But at present, there are third-party agencies in carbon emission accounting that cannot ensure the fairness and impartiality of accounting, and there may be risks such as illegal use and leakage of sensitive information in the process of carbon emission data transmission. Therefore, We design the blockchain-based carbon emission security accounting scheme (BCESAS) and propose cross-chain verification contract to ensure the efficiency of cross-chain information accounting. In addition, bilinear pairing is used to ensure data integrity, and we encrypt private data using an improved and more secure homomorphic encryption algorithm to ensure that privacy is not leaked during the transfer of carbon emission data, which is more efficient than other homomorphic encryption algorithms. We also use reputation mechanism to regulate the behavior of carbon emission auditors. The theoretical and experimental analysis demonstrates that BCESAS can verify the integrity, correctness and privacy of cross-chain data calculation result effectively, realizing secure and reliable expansion of blockchain.}
}


@article{DBLP:journals/cn/SongZW24,
	author = {Yu Song and
                  Shilong Zhang and
                  Shubin Wang},
	title = {An energy efficient fusing data gathering protocol in wireless sensor
                  networks},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110305},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110305},
	doi = {10.1016/J.COMNET.2024.110305},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SongZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is rapidly developing as a promising technology for today’s digital world. Wireless sensor networks (WSN) are a crucial component of IoT, and one of the major problems faced is energy constraints. The clustering protocol is an effective energy saving solution. In this paper, we propose an energy efficient fusing data gathering protocol via distributed second level clustering and deep learning-based data fusion (EEFDG) for WSN. EEFDG divides the WSN into several second-level clusters by a novel distributed cluster algorithm. The transmission distance and the number of neighbors are jointly considered to select an energy-efficient parent node in each second-level cluster. A new cluster head weight function is proposed for intra-cluster parent node rotation. To minimize redundant data transmission, a multivariate convolutional neural network and long short-term memory network (MVCLNet) are designed for data fusion. MVCLNet is deployed hierarchically into the second-level clustered structure to extract features of the multi-sensor data. The EEFDG is an effective solution to the energy limitation problem of WSN. The proposed second-level clustering and data fusion algorithm based on MVCLNet typically decreases communication distances and the amount of data, thereby minimizing communication overhead and energy consumption. Simulation results show that EEFDG significantly prolongs the network lifetime in terms of the first node dead, compared to existing protocols including MR-LEACH, BPDA, CNNDA, FDEAM, DCNN, and LPLL-LEACH, achieving improvements of 181.3%, 42.2%, 48.2%, 19.6%, 62.0%, and 30.2%, respectively. The EEFDG enables highly accurate data fusion and has superior performance in terms of remaining energy, network lifetime, and data collection.}
}


@article{DBLP:journals/cn/HassanIGQ24,
	author = {Ali Haj Hassan and
                  Youcef Imine and
                  Antoine Gallais and
                  Bruno Quoitin},
	title = {Detecting malicious proxy nodes during IoT network joining phase},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110308},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110308},
	doi = {10.1016/J.COMNET.2024.110308},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HassanIGQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {During the joining phase of an IoT network, when a node plays the role of a proxy node, it becomes responsible for forwarding Join Requests and Join Responses between the joining node and the network coordinator. If a proxy node is malicious, it has the potential to prevent new nodes from joining the network or direct them towards another entity impersonating the coordinator. Therefore, the joining phase is a critical stage for ensuring network security.}
}


@article{DBLP:journals/cn/BiYLHL24,
	author = {Meihua Bi and
                  Chenghao Yan and
                  Rui Lin and
                  Yuanyuan Hong and
                  Jun Li},
	title = {Client scheduling and bandwidth slicing for multiple federated learning
                  tasks over multiple passive optical networks},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110309},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110309},
	doi = {10.1016/J.COMNET.2024.110309},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BiYLHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has attracted extensive attention in facilitating emerging edge intelligence applications for its inherent advantages of ensuring data security and privacy. Especially in the edge computing networks connected by Passive Optical Network (PON) system, FL is introduced to enable applications like autonomous driving, intelligent manufacturing, and precision medicine. However, in this system, the FL deployment over PON inevitably faces challenges induced by the conflict between a large volume of model data with the restrict latency limitation and the confined bandwidth resource of PON, especially for the multiple FL tasks. To address these issues, a novel scheme is proposed for tackling the client distribution and bandwidth allocation problems under the scenario of multiple simultaneous FL tasks supported by the multiple interconnected PON systems, which is consisted of the client scheduling and bandwidth slicing processes. To be specific, an easy-to-implement heuristic algorithm is first performed to assign the client numbers to PONs based on iterative method, with which certain operations are repetitively executed to achieve optimal solutions. And then, the serial bandwidth slicing which adapts the traditional policy, i.e., one-task-per-cycle, to the situation with multiple FL tasks, and parallel slicing with the multi-task-per-cycle, are designed for the investigated system. Furthermore, the simulation system is constructed to verify our method. The corresponding results exhibit that, the largest 43.2 % round time reduction is achieved by client scheduling compared to benchmark without the scheduling. Compared to the benchmark with serial slicing, our scheme can achieve a maximum 50.11 % of the round time decrease. It's also validated that, our proposed client scheduling and parallel bandwidth slicing method can improve the learning efficiency by reducing communication delay, especially for the situation with less client number and smaller FL threshold.}
}


@article{DBLP:journals/cn/BottaCNSV24,
	author = {Alessio Botta and
                  Roberto Canonico and
                  Annalisa Navarro and
                  Giovanni Stanco and
                  Giorgio Ventre},
	title = {Adaptive overlay selection at the {SD-WAN} edges: {A} reinforcement
                  learning approach with networked agents},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110310},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110310},
	doi = {10.1016/J.COMNET.2024.110310},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BottaCNSV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Wide Area Networks (SD-WANs) have emerged as a promising solution to address the connectivity demands of modern distributed enterprises. However, the effective application of the Software Defined Networking (SDN) paradigm in such broad and dynamic environments remains a significant challenge. In this paper, we present two novel contributions. First, we design a decentralized control plane for SD-WANs that leverages edge-based network monitoring and overlays’ configuration. Then we present a Reinforcement Learning-based orchestration plane that leverages local information for the enforcement of SD-WAN policies. Since traditional approaches suffer either a lack of scalability due to the problem’s complexity or suboptimal performance due to isolated decision-making, the proposed approach leverages a cooperative Multi-Agent Reinforcement Learning framework. Our novel cooperative approach is based on per-site agents that exchange a small amount of information to enhance performance while preserving scalability. To validate the efficacy of our proposed approach, we conducted an extensive experimental evaluation considering diverse SD-WAN scenarios. Results show that our framework is able to satisfy global network policies for a multi-site SD-WAN with different QoS requirements and cost constraints.}
}


@article{DBLP:journals/cn/ArdizzonCT24,
	author = {Francesco Ardizzon and
                  Paolo Casari and
                  Stefano Tomasin},
	title = {A RNN-based approach to physical layer authentication in underwater
                  acoustic networks with mobile devices},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110311},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110311},
	doi = {10.1016/J.COMNET.2024.110311},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ArdizzonCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater acoustic communications are becoming a popular solution for underwater data communications and telemetry, making the authentication of transmitted data a necessity. In this paper, we propose a physical-layer authentication strategy for underwater acoustic networks (UWANs) with mobile devices. Such a scenario is more challenging than classical authentication scenarios in static networks, because the mobility of the receiver and/or transmitter implies that channel conditions slowly change over time. Thus, we cannot rely on the statistics of channel features to be stationary. In our proposed strategy, we assume that the receiver can rely on a set of sensors. We first extract a set of channel features, to be used to track the channel evolution over time. We then develop a long short-term memory (LSTM)-based approach, where at each step the sensors predict future feature values based on a learned model and on previously observed feature values. Next, each sensor computes the prediction error and passes it on to the actual receiver, which makes a decision on the signal authenticity through a generalized likelihood ratio test (GLRT). We model different classes of attacks and test them using simulation data obtained via the Bellhop ray tracing software. Numerical results show that our authentication mechanism successfully distinguishes between legitimate and impersonating transmitters, even when considering challenging attacking scenarios where the attacker can successfully mimic the channels between the legitimate transmitter and the sensors.}
}


@article{DBLP:journals/cn/ZhouZJ24,
	author = {Hongliang Zhou and
                  Yifeng Zheng and
                  Xiaohua Jia},
	title = {Towards robust and privacy-preserving federated learning in edge computing},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110321},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110321},
	doi = {10.1016/J.COMNET.2024.110321},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has recently emerged as an attractive distributed machine learning paradigm for harnessing the distributed data in edge computing. Its salient feature is that the individual datasets can stay local all the time during the training process and only model updates need to be exchanged for aggregation. Despite being intriguing, FL is also known to be confronted with critical security and privacy concerns. Firstly, even sharing the model updates/gradients can incur privacy leakages of the local datasets. Secondly, there could be malicious clients who may attempt to launch poisoning attacks so as to compromise the utility of trained models. Driven by such challenges, various research efforts have been proposed to secure FL. However, most existing works have just considered either privacy preservation or robustness against poisoning attacks. In this paper, we propose a new robust and privacy-preserving FL framework RoPPFL for edge computing applications, which supports hierarchical federated learning with privacy preservation as well as robust aggregation against poisoning attacks. RoPPFL delicately bridges local differential privacy for privacy protection and similarity-based robust aggregation for resistance to malicious clients. We formally analyze the convergence and privacy guarantees of RoPPFL. Extensive experiments demonstrate the superior performance of RoPPFL.}
}


@article{DBLP:journals/cn/WuZZ24,
	author = {Yunyi Wu and
                  Kai Zhang and
                  Yongbing Zhang},
	title = {An accelerated Benders decomposition approach for virtual base station
                  formation in stochastic Cloud-RANs},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110328},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110328},
	doi = {10.1016/J.COMNET.2024.110328},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WuZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Cloud-RANs, the baseband unit (BBU) is separated from the remote radio head (RRH) at the cell site and migrated to a server cloud to decrease deployment and maintenance costs. A base station (BS) can be formed as a network slice in which the virtual network functions (VNFs) of its BBU can be placed on any server by network virtualization (NFV) technology and logically connected by software-defined networking (SDN). However, the uncertain demands of users make it very difficult to determine a network slice with appropriate VNF placement and flow routing between VNFs. We aim to minimize the cost of providing BS services while meeting their time-varying demands. We consider the network slice scheme and fronthaul establishment jointly and formulate this as an integer linear programming (ILP) problem. We propose scenario classification Benders decomposition (CBD) based on Benders decomposition (BD) but with fewer Benders cuts, accelerating the solution convergence. Moreover, we propose an efficient heuristic approach to obtain good solutions for the ILP problem based on the solutions of its relaxed problems obtained by BD or CBD. The numerical experiments show that when combining our heuristic approach with CBD, the results are close to the optimal solutions of the ILP problem, while the computation time is significantly shorter than those of previous approaches.}
}


@article{DBLP:journals/cn/RuizSPSV24,
	author = {Olga Ruiz and
                  Juan S{\'{a}}nchez{-}Gonz{\'{a}}lez and
                  Jordi P{\'{e}}rez{-}Romero and
                  Oriol Sallent and
                  Irene Vil{\`{a}}},
	title = {Space and time user distribution measurements dataset in a university
                  campus},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110329},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110329},
	doi = {10.1016/J.COMNET.2024.110329},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RuizSPSV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Resource Management (RRM) strategies are essential components in mobile wireless networks, such as Long Term Evolution (LTE) or 5G New Radio (NR)[1]. The design of RRM algorithmic solutions is an area of research that has received a lot of attention for decades (see e.g. surveys [2–4] and references therein). Given that RRM strategies need to handle traffic dynamicity in the Radio Access Network (RAN), the availability of realistic user distributions in space and time can be very useful to support a more realistic performance assessment of RRM solutions. For this purpose, this article introduces a dataset containing real measurements of the number of users connected to the different Wifi Access Points (APs) at the Campus Nord facilities of the Universitat Politècnica de Catalunya (UPC) in Barcelona. The Wifi network is composed of 247 APs. To characterize the temporal variations, the data was collected for each AP every 1000 s (approximately) during 62 days. Besides the number of users connected to each AP (including users connected to both 2.4 GHz and 5 GHz bands), the dataset also contains information about the theoretical coverage area of each AP, so that the number of users connected to each AP can be associated to a specific geographical area. In this way, the dataset captures the spatio-temporal variations of users in the Campus at different times of the day, different days of the week and different periods of the academic year.}
}


@article{DBLP:journals/cn/LiuMLDLG24,
	author = {Liang Liu and
                  Wuping Mao and
                  Wenwei Li and
                  Jie Duan and
                  Guanyu Liu and
                  Bingchuan Guo},
	title = {Edge computing offloading strategy for space-air-ground integrated
                  network based on game theory},
	journal = {Comput. Networks},
	volume = {243},
	pages = {110331},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110331},
	doi = {10.1016/J.COMNET.2024.110331},
	timestamp = {Fri, 31 May 2024 21:06:37 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuMLDLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The limited coverage of terrestrial networks makes it difficult to provide low-latency and high-reliable computing services for users and Internet of Thing (IoT) devices in remote areas such as mountainous regions and oceans. Space-Air-Ground Integrated Network (SAGIN) combined with Mobile Edge Computing (MEC) can provide seamless three-dimensional services for users by deploying edge servers on satellites, Unmanned Aerial Vehicles (UAVs) and ground infrastructures. However, due to the limited heterogeneous computing resources in satellites-UAV clusters network and the energy resources of IoT devices, it brings a significant challenge in determining how to offload the computing tasks generated by ground user devices to satellite edge nodes, UAV edge nodes or locally for processing. In this paper, we first propose a satellites-UAV clusters-ground three-layer edge computing network architecture consisting of a global controller, inter-domain controllers and MEC servers. We then model the task offloading problem as a Binary Integer Linear Programming (BILP) aiming at minimizing the offloading cost composed of delay and energy consumption and prove it is NP-hard. Next, the original offloading problem is transformed to a noncooperative strategic game and the existence of Nash equilibrium is proven using potential games. Finally, we propose a Nash Equilibrium Iteration Offloading algorithm based on Game theory (NEIO-G) to find the optimal offloading strategy. Compared with other baseline algorithms, simulation results demonstrate that the NEIO-G can significantly reduce the system offloading overhead in terms of delay and energy consumption.}
}
