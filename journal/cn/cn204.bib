@article{DBLP:journals/cn/Saltini22,
	author = {Roberto Saltini},
	title = {BigFooT: {A} robust optimal-latency {BFT} blockchain consensus protocol
                  with dynamic validator membership},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108632},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108632},
	doi = {10.1016/J.COMNET.2021.108632},
	timestamp = {Wed, 23 Feb 2022 11:16:09 +0100},
	biburl = {https://dblp.org/rec/journals/cn/Saltini22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Permissioned blockchains are blockchains where only a finite and known subset of all the nodes, that we call validator set, is allowed to propose the next block to be added to the blockchain maintained by each node.}
}


@article{DBLP:journals/cn/GrinerSA22,
	author = {Chen Griner and
                  Stefan Schmid and
                  Chen Avin},
	title = {CacheNet: Leveraging the principle of locality in reconfigurable network
                  design},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108648},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108648},
	doi = {10.1016/J.COMNET.2021.108648},
	timestamp = {Wed, 23 Feb 2022 11:16:10 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GrinerSA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging optical communication technologies support the dynamic reconfiguration of datacenter network topologies depending on the traffic they serve. However, to reap the benefits of such demand-aware networks, control logic that quickly learns and adapts to traffic patterns is required. This paper presents CacheNet, a novel approach to efficiently control demand-aware networks. CacheNet consists of two components, a demand-aware links-cache, and a demand-oblivious topology. CacheNet leverages temporal and spatial locality in the traffic by managing the reconfigurable links of the optical switches as a links-cache. Network traffic, in turn, can be served either by a link from the links-cache component or by a demand-oblivious topology component. We study several classic caching algorithms like online LFU and LRU as our caching algorithms, as well as offline optimal caching as a benchmark, and provide an analytical model which captures their performance benefits compared to an all demand-oblivious topology. Our analytical results show that based on the hit ratios and the links-cache size, when considering the average packet delay, our hybrid design outperforms a design that is based only on demand-oblivious topology. We also evaluate CacheNet empirically, using both synthetic and real-world traffic traces, confirming the potential of our approach to consider reconfigurable links as a network of links-cache.}
}


@article{DBLP:journals/cn/WanQGX22,
	author = {Yichen Wan and
                  Youyang Qu and
                  Longxiang Gao and
                  Yong Xiang},
	title = {Privacy-preserving blockchain-enabled federated learning for B5G-Driven
                  edge computing},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108671},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108671},
	doi = {10.1016/J.COMNET.2021.108671},
	timestamp = {Sat, 30 Sep 2023 10:07:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WanQGX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The arrival of the fifth-generation technology standard for broadband cellular networks (5G) and beyond 5G networks (B5G) rises the speed and robustness ceiling of communicating networks and thereby empowers the rapid popularization of edge computing. Consequently, B5G-Driven edge computing allows a growing volume of data to be collected from and transmitted among pervasive edge devices for big data analytics. The collected big data becomes the driving force of artificial intelligence (AI) by training high-quality machine learning (ML) models, which is followed by severe individual privacy leakage. Federated learning(FL) is then proposed to achieve privacy-preserving machine learning by avoiding the exchange of raw data. Unfortunately, several major issues remain outstanding. Centralized processing costs significant communication resources between cloud and edge while data falsification problems persist. In addition, the private data may be reconstructed by malicious participants by exploiting the context of model parameters in FL. To solve the identified problems, we propose to integrate blockchain-enabled FL with Wasserstein generative adversarial network (WGAN) enabled differential privacy (DP) to protect the model parameters of edge devices in B5G networks. Blockchain enables decentralized FL to reduce communication costs between cloud and edge while alleviating the data falsification issues, and it also provides an incentive mechanism to alleviate the data island issue in B5G-Driven edge computing. WGAN is used to generate controllable random noise complying with DP requirements, which is then injected to model parameters. WGAN-enabled DP is able to achieve an optimized trade-off between differential privacy protection and improved data utility of model parameters. Time delay analysis is conducted to show the efficiency of the proposed model. Extensive evaluation results from simulations demonstrate superior performances from aspects of convergence efficiency, accuracy, and data utility.}
}


@article{DBLP:journals/cn/GuoLPLC22,
	author = {Mian Guo and
                  Qirui Li and
                  Zhiping Peng and
                  Xiushan Liu and
                  Delong Cui},
	title = {Energy harvesting computation offloading game towards minimizing delay
                  for mobile edge computing},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108678},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108678},
	doi = {10.1016/J.COMNET.2021.108678},
	timestamp = {Wed, 23 Feb 2022 11:16:10 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuoLPLC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has emerged for meeting the ever-increasing computation demands from mobile applications. Mobile users endowing with computation and energy harvesting (EH) capabilities, called EH devices, are desired in MEC systems. With computation capability, EH devices can switch between local computing and mobile-edge computing for better QoEs. With EH technologies, mobile users could survive perpetually for supporting long-term task processing. However, the heterogeneous computation resource among EH devices and edge servers, limited wireless resource as well as time-constrained harvestable energy challenge the computation offloading. This paper investigates a green MEC with EH devices and develops an effective EH computation offloading scheme towards minimizing delay for green MEC. We formulate and analyze the EH computation offloading problem with game theory. Then, an EH computation offloading game scheme, including the Lyapunov drift-based energy harvesting and computation offloading best response algorithms, is designed to find out optimal energy harvesting and computation offloading solutions to address the problem. The simulation results have been conducted to demonstrate the efficiency of the proposal.}
}


@article{DBLP:journals/cn/SalmanECK22,
	author = {Ola Salman and
                  Imad H. Elhajj and
                  Ali Chehab and
                  Ayman I. Kayssi},
	title = {Towards efficient real-time traffic classifier: {A} confidence measure
                  with ensemble Deep Learning},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108684},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108684},
	doi = {10.1016/J.COMNET.2021.108684},
	timestamp = {Wed, 23 Feb 2022 11:16:09 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SalmanECK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic classification is a key network function for managing both Quality of Service (QoS) and security. While some traffic classification applications (e.g. QoS based path allocation) can tolerate delays, other applications (e.g. attack detection) are time critical. In this context, early traffic classification has been proposed based on the first few packets of flows. However, the choice of the number of packets to inspect is method dependent and based on empirical assessment without considering the information carried by these packets (features). In this paper, we aim at identifying the sufficient number of packets,\nN\n, that guarantees high classification accuracy while optimizing the response time, based on both empirical classification results and information theory. We propose a confidence measure based on the variations in the model training accuracy and the average mutual information among the packetsâ€™ features and the label vector. This measure is then used to define the value of\nN\n, which optimizes the trade-off between the time overhead and the classification accuracy. In addition, we propose an ensemble Deep Learning (DL)-based classifier model to enhance the classification accuracy by training successive DL models based on the traffic stream. The proposed ensemble method output is based on the average of the individual classifiers predictions. The experimental results show that when using the proposed confidence measure, we can achieve good classification accuracy at early phase of the flow. In addition, using the proposed ensemble method presents enhancement in the early classification accuracy. Consequently, combining the ensemble method with the confidence measure criteria allows for striking a good balance between high accuracy and fast response time.}
}


@article{DBLP:journals/cn/AskiDKVR22,
	author = {Vidyadhar Jinnappa Aski and
                  Vijaypal Singh Dhaka and
                  Sunil Kumar and
                  Sahil Verma and
                  Danda B. Rawat},
	title = {Advances on networked ehealth information access and sharing: Status,
                  challenges and prospects},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108687},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108687},
	doi = {10.1016/J.COMNET.2021.108687},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AskiDKVR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Medical Things (IoMT) is no longer a futuristic technology and has become a day-to-day reality with the increasing ease of availing modern internet services. IoMT services interconnect numerous healthcare ecosystem stakeholders, such as doctors, patients, pharmacists, etc., seamlessly under the light of Advanced Internet Communication Tools (AICT). Wireless Body Area Network (WBAN) is a primary constituent of any IoMT device, offering a data acquisition environment through various bio-sensors deployed in edge devices. The data generated through such edge devices should be stored securely in Electronic Health Server (EHS) for further analysis and knowledge inference by medical professionals. Thus designing access control techniques that prevent unauthorised access at the cloud and device level is crucial. Interoperability concern in the IoMT development cycle is becoming a focus of interest for many researchers because most devices and underlying protocols are highly heterogeneous. Lack of worldwide acceptable protocol standards is another major issue in dealing with platform interoperability. In this study, the authors describe the various existing device and data access control strategies such as RBAC (Role Based Access Controlling), CaPBAC (Capability-Based Access Controlling), and ABAC (Attribute-Based Access Controlling), etc. This article majorly surveys the literature from 2000 to 2020 on various access controlling and Interoperability aspects as they are potential players of modern IoT applications. The study also comprehensively discusses the state-of-the-art strategies that provide platform interoperability followed by crucial implementation challenges.}
}


@article{DBLP:journals/cn/WangKLW22,
	author = {Jian Wang and
                  Hongchang Ke and
                  Xuejie Liu and
                  Hui Wang},
	title = {Optimization for computational offloading in multi-access edge computing:
                  {A} deep reinforcement learning scheme},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108690},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108690},
	doi = {10.1016/J.COMNET.2021.108690},
	timestamp = {Sat, 09 Apr 2022 12:25:52 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangKLW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Owing to their limited computing power and battery level, wireless users (WUs) can hardly handle compute-intensive workflows by the local processor. Multi-access edge computing (MEC) servers attached to base stations have ample computing power and communication resources, which can be used to address the computation tasks or workloads of WUs. In this study, we design a framework with multiple static and vehicle-assisted MEC servers to handle the workloads offloaded by WUs. For obtaining the optimal computation offloading scheme to minimize the weighted sum cost, including transmission and execution cost, energy consumption cost, and communication bandwidth cost, we model the offloading decision optimization problem as a Markov decision process (MDP). Then, we propose a partial computation offloading scheme based on reinforcement learning (RL) to address the absence of priori knowledge. The proposed scheme can learn the optimal offloading decision based on stochastic workload arrival, the changing channel state, and the dynamic distance between WUs and the edge servers. Moreover, to avoid the curse of dimensionality caused by the complex state and action spaces, we present an improved computation offloading method based on deep RL (DRL) to learn the optimal offloading policy using deep neural networks. Extensive numerical results illustrate that the proposed algorithms based on RL and DRL can autonomously learn the optimal computation offloading policy with no priori knowledge, and their performance are better than that of four baselines algorithms.}
}


@article{DBLP:journals/cn/AbubakerJAAZB22,
	author = {Zain Abubaker and
                  Nadeem Javaid and
                  Ahmad Almogren and
                  Mariam Akbar and
                  Mansour Zuair and
                  Jalel Ben{-}Othman},
	title = {Blockchained service provisioning and malicious node detection via
                  federated learning in scalable Internet of Sensor Things networks},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108691},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108691},
	doi = {10.1016/J.COMNET.2021.108691},
	timestamp = {Wed, 23 Feb 2022 11:16:10 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AbubakerJAAZB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a blockchained Beyond Fifth Generation (B5G) enabled malicious node detection model is proposed for the Internet of Sensor Things (IoSTs). Moreover, a secure service provisioning scheme using cascading encryption and feature evaluation process is also proposed for the IoSTs. The presence of malicious nodes causes severe issues in the localization and service provisioning, which discourages new entities to join the network. Therefore, it is very important to establish trust between all entities by detecting and removing such nodes. The proposed B5G enabled malicious node detection model uses federated learning for the detection of malicious nodes. The federated learning uses Support Vector Machine (SVM) and Random Forest (RF) classifiers to detect the malicious nodes. The malicious nodes are classified on the bases of their honesty and end-to-end delay. Moreover, the service provider nodes provide services to each other and get the reward. However, the service provisioning in the IoSTs has many issues like a repudiation of service providers as well as the clients. The feature evaluation and cascading encryption mechanisms are used to solve these issues. The digital signature in cascading encryption ensures the non-repudiation of the service provider. On the other hand, feature evaluation of service ensures that the client cannot repudiate about actually demanded services. Moreover, the conformance of services is also ensured by the feature evaluation process. The simulation results show the effectiveness of our proposed non-repudiation model. The SVM and RF classifiers are compared in terms of accuracy, precision, F1 score and recall. The accuracy, precision, F1 score and recall of SVM are 79%, 1, 0.8795 and 0.78, respectively. On the other hand, the accuracy, precision, F1 score and recall of RF classifier are 95%, 0.92, 0.96 and 1, respectively. The results show that RF has better accuracy than RF in malicious nodes detection.}
}


@article{DBLP:journals/cn/ReySCB22,
	author = {Valerian Rey and
                  Pedro Miguel S{\'{a}}nchez S{\'{a}}nchez and
                  Alberto Huertas Celdr{\'{a}}n and
                  G{\'{e}}r{\^{o}}me Bovet},
	title = {Federated learning for malware detection in IoT devices},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108693},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108693},
	doi = {10.1016/J.COMNET.2021.108693},
	timestamp = {Fri, 13 May 2022 19:52:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ReySCB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Billions of IoT devices lacking proper security mechanisms have been manufactured and deployed for the last years, and more will come with the development of Beyond 5G technologies. Their vulnerability to malware has motivated the need for efficient techniques to detect infected IoT devices inside networks. With data privacy and integrity becoming a major concern in recent years, increasing with the arrival of 5G and Beyond networks, new technologies such as federated learning and blockchain emerged. They allow training machine learning models with decentralized data while preserving its privacy by design. This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware affecting IoT devices is presented. N-BaIoT, a dataset modeling network traffic of several real IoT devices while affected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware affecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The first one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participantâ€™s privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered. The baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to different attacks, even with a single adversary. The performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios. These functions provide a significant improvement against malicious participants, but more efforts are still needed to make federated approaches robust.}
}


@article{DBLP:journals/cn/Sengupta22,
	author = {Binanda Sengupta},
	title = {{VALNET:} Privacy-preserving multi-path validation},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108695},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108695},
	doi = {10.1016/J.COMNET.2021.108695},
	timestamp = {Wed, 07 Dec 2022 23:01:40 +0100},
	biburl = {https://dblp.org/rec/journals/cn/Sengupta22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network path validation (or simply, path validation) provides network end-hosts with the ability to enforce the network paths they want their packets to traverse. Path validation also enables each on-path node to validate whether a packet has followed the same path specified by the respective source node. Traditional networking uses single-path routing. Multi-path routing provides on-path nodes (including the source node) with the flexibility to choose a path, among a set of options available to them, in order to forward a packet to the same destination node. Multi-path validation combines these two notions: a source node, instead of choosing a single path, can choose a set of paths to send its packets to a destination node; and each of the other on-path nodes can verify whether a packet has indeed traversed one of the legitimate paths (the paths designated by the source) and can further select any of these legitimate paths downstream to forward packets to the destination node. The source node typically embeds the set of paths in a packet-header to enable path validation at each on-path node. However, this results in several privacy issues and makes the underlying network prone to certain attacks. In this work, we introduce privacy-preserving multi-path validation with two privacy notions: path privacy and index privacy. We design VALNET, a network that achieves privacy-preserving multi-path validation. We analyze the security as well as the performance of VALNET. VALNET finds numerous use cases (e.g., in secure and private military communications between base stations and on-field devices such as sensor nodes and drones), where data communications need to take place over multiple paths (enforced by the end-hosts) for enhanced reliability, and privacy of any of these paths must not be compromised even if some of the on-path nodes are compromised. To the best of our knowledge, VALNET is the first work that addresses privacy concerns in the context of multi-path validation.}
}


@article{DBLP:journals/cn/DienTLHPQH22,
	author = {Nguyen Van Dien and
                  Nguyen V. Tuan and
                  Mai T. Phuong Le and
                  Nguyen V. Hieu and
                  Vuong Q. Phuoc and
                  Nguyen Q. N. Quynh and
                  Nguyen Tan Hung},
	title = {Tolerance of {SCM} Nyquist and {OFDM} signals for heterogeneous fiber-optic
                  and millimeter-wave mobile backhaul links under the effect of power
                  amplifier saturation induced clipping},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108697},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108697},
	doi = {10.1016/J.COMNET.2021.108697},
	timestamp = {Tue, 15 Feb 2022 09:56:16 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DienTLHPQH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extremely high frequency carrier communications such as millimeter-wave (mmW) is considered a promising solution for the future mobile networks. To compensate for the large path loss of the millimeter-wave links, saturated power amplifiers are inevitably used during amplify-and-forward relaying, which potentially causes system degradation due to power saturation induced clipping. In this paper, the tolerance of OFDM and Nyquist subcarrier multiplexing (SCM) signals against power amplifier saturation induced clipping is studied in the context of fiber-optic and millimeter-wave integrated communications for heterogeneous mobile backhaul networks. In the system, 10 Gbaud OFDM and Nyquist SCM signals are converted to an intermediate frequency passband before converted to the optical domain for a radio-over-fiber transmission to a remote antenna unit (RAU). At RAU, the optical radio signals are converted back to the electrical domain at millimeter-wave range. Those are then amplified by an power amplifier prior to a millimeter-wave communication to a remote radio head (RRH) for detection. Since Nyquist SCM signal exhibits a level of lower peak-to-average power ratio (PAPR) than that of OFDM, Nyquist SCM signals are more tolerant against the clipping caused by the power amplifier saturation. For 16 QAM, at clipping ratio (CR) of 4.5 dB, one-band Nyquist SCM signal shows a signal-to-noise ratio (SNR) performance over 10 times higher than that of OFDM. While the increase in the number of Nyquist SCM bands reduces the tolerance, Nyquist SCM signal maintains a better tolerance than OFDM one with respect to 16 QAM, 32 QAM and 64 QAM formats, which suffer severer impact with larger SNR penalty for higher modulation levels.}
}


@article{DBLP:journals/cn/HameedGAK22,
	author = {Khizar Hameed and
                  Saurabh Garg and
                  Muhammad Bilal Amin and
                  Byeong Kang},
	title = {Towards a formal modelling, analysis and verification of a clone node
                  attack detection scheme in the internet of things},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108702},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108702},
	doi = {10.1016/J.COMNET.2021.108702},
	timestamp = {Thu, 23 Jun 2022 20:03:26 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HameedGAK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A clone node attack is one of the severe attacks on the Internet of Things (IoT) network. In a clone node attack, an adversary aims to physically capture the secret credentials of the deployed IoT devices to make the clone of devices look similar to the original devices. In recent years, several solutions for detecting clone node attacks have been designed; however, the focus of such designed solutions is on system design discussion, specific feature sets, high-level abstraction of underlying system architecture and fulfilling the performance analysis requirements. While the designed solutions efficiently detect clone attacks, critical features such as modelling, analysis, and verification of such solutions are frequently overlooked, necessitating that users verify their internal behaviour and claimed properties to ensure that no problematic scenarios or anomalies exist. Taking into account the rationale for formal verification and the limitations of previous works, this paper aims to perform formal modelling, analysis, and verification of our existing proposed clone node attack detection scheme for IoT. We used the High-Level Petri Nets (HLPNs) and Z-specification language to model our proposed scheme and analyse the logic correctness of our scheme using incidence markings and confidence interval matrices. To validate the functional correctness, we utilised the Satisfiability Modulo Theories Library (SMT-Lib) and the Z3 Solver and verified the functional satisfiable properties by analysing sat/unsat verification results and their execution time. Finally, we extend our modelling work by utilising the Coloured Petri Nets (CPNs), taking into consideration their both timed and untimed models with the aim of validating functional correctness and logical correctness, respectively. To analyse both correctnesses of CPN models, we used logging facility statistics such as count, sum, average, minimum, and maximum as data packet observations.}
}


@article{DBLP:journals/cn/CheemaAAHQBP22,
	author = {Muhammad Asaad Cheema and
                  Rafay Iqbal Ansari and
                  Nouman Ashraf and
                  Syed Ali Hassan and
                  Hassaan Khaliq Qureshi and
                  Ali Kashif Bashir and
                  Christos Politis},
	title = {Blockchain-based secure delivery of medical supplies using drones},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108706},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108706},
	doi = {10.1016/J.COMNET.2021.108706},
	timestamp = {Sat, 09 Apr 2022 12:25:51 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CheemaAAHQBP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advantages provided by the drones with regards to three dimensional mobility and ease of deployment makes them a viable candidate for 5G and beyond (B5G) networks. Significant amount of research has been conducted on the aspect of networking for using drones as base stations to provide different services. In this work, we deviate from the traditional use of drones to provide connectivity and explore the delivery of products through drones in the context of maintaining social distancing. However, drone delivery process for critical applications such as delivering medical supplies is vulnerable to attacks such as impersonation attacks. The security of drone operation is important to save the users from any breaches that can lead to financial and physical losses. To cope with these security issues and to make the delivery process transparent, we propose a blockchain-based drone delivery system that registers and authenticates the participating entities including products (medical supplies), warehouse (medical centers) and drones. To this end, we utilize Ethereum platform for implementation of blockchain and smart contract and we present an analysis of different factors that influence the authentication process in terms of time and the number of transactions. Furthermore, to make the communication of a drone with command and control center more secure and robust, we use machine learning (ML)-based intrusion detection system.}
}


@article{DBLP:journals/cn/ZhouYL22,
	author = {Anan Zhou and
                  Benshun Yi and
                  Laigan Luo},
	title = {Tree-structured data placement scheme with cluster-aided top-down
                  transmission in erasure-coded distributed storage systems},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108714},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108714},
	doi = {10.1016/J.COMNET.2021.108714},
	timestamp = {Mon, 28 Aug 2023 21:39:20 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouYL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In erasure-coded distributed storage systems, the rapid completion of data placement process is very critical to maintain system performance, where the process is defined as to insert coded blocks into a set of redundant storage nodes. However, the existing data placement schemes have some shortcomings, such as load imbalance and long transmission latency. To solve these problems, this paper proposes a tree-structured data placement scheme with cluster-aided top-down transmission, which improves the practicality and the efficiency of the data insertion. Specifically, the storage nodes are clustered by a location-based K-mean method at first, and then selecting a master node as the source node to encode file and distribute coded blocks in each cluster by designing a master nodeâ€™s selection algorithm based on evolutionary game theory. Since multiple master nodes are used to receive data files from data core, the load imbalance problem caused by single node receiving data files is mitigated. In addition, to improve the data transmission efficiency, we construct a tree-structured network to organize the connections of the storage nodes in cluster, where the master node is the root node. Meanwhile, a data placement scheme with top-down transmission by combining an improved metaheuristic algorithm is proposed to manage the pipelined data transmission along the tree-structured network. Experimental results show that the proposed scheme can effectively improve the survive life of storage network and reduce the data insertion time. Furthermore, the proposed scheme has better performance than the optimal tree-structured scheme based on other optimization algorithms.}
}


@article{DBLP:journals/cn/WuDQJ22,
	author = {Zheng Wu and
                  Yu{-}ning Dong and
                  Xiaohui Qiu and
                  Jiong Jin},
	title = {Online multimedia traffic classification from the QoS perspective
                  using deep learning},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108716},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108716},
	doi = {10.1016/J.COMNET.2021.108716},
	timestamp = {Wed, 23 Feb 2022 11:16:10 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WuDQJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the next generation of communication systems, data traffic is expected to increase dramatically and continuously. Particularly for multimedia traffic, it has a dominant share in the increasing traffic. Therefore, there is an urgent need to develop an effective and accurate scheme to achieve online and automatic traffic management. To this end, this paper proposes an online multimedia traffic classification framework based on a Convolutional Neural Network (CNN), capable of conducting fast and early classification as well as class incremental learning. First, the sliding window technique is applied to capture the flow slices for further feature extraction. Then, the 3-dimensional flow representation is extracted based on the probability distribution function. After that, according to the specific structure of features, a deeply adapted structure of CNN is devised to better learn the knowledge from the representation. Besides, to better support the addition of new services, a class incremental learning model is developed with the techniques of knowledge distillation and bias correction to achieve continuous learning without retraining from scratch. Our experimental results reveal that the proposed method achieves faster and more accurate traffic classification compared with the state-of-the-art. Additionally, the deployed scheme using incremental learning achieves drops by about 50% in both time and memory consumptions compared with existing methods, while guaranteeing the accurate classification after adding new classes.}
}


@article{DBLP:journals/cn/AouiniP22,
	author = {Zied Aouini and
                  Adri{\'{a}}n Pek{\'{a}}r},
	title = {NFStream: {A} flexible network data analysis framework},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108719},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108719},
	doi = {10.1016/J.COMNET.2021.108719},
	timestamp = {Fri, 13 May 2022 19:52:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AouiniP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic analytics have increased in relevance as researchers promoted machine learning techniques to tackle several traffic management challenges. Over the past decade, the research community and the networking industry have investigated, proposed, and developed a growing number of solutions. However, a large subset of proposed approaches is based on unreliable measurement tools and methodologies. Additionally, some findings are reported on private datasets, which results in a lack of applicability and reproducibility. This paper covers the design and implementation of NFStream, a flexible network data analysis framework. Its key features are flexibility, real-time statistical analysis, and the ability to provide reliable ground truth for modern network usage. NFStream provides the community with a common research framework that can help stimulate research in this field and develop more efficient, reproducible solutions.}
}


@article{DBLP:journals/cn/ZhengGCL22,
	author = {Kechen Zheng and
                  Haijiang Ge and
                  Kaikai Chi and
                  Xiaoying Liu},
	title = {Energy provision minimization of energy-harvesting cognitive radio
                  networks with minimal throughput demands},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108721},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108721},
	doi = {10.1016/J.COMNET.2021.108721},
	timestamp = {Wed, 23 Feb 2022 11:16:09 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhengGCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The newly emerging energy-harvesting cognitive radio networks (EH-CRNs) have drawn significant research interests in the fields of spectrum reuse and lifetime prolongation. This paper focuses on the EH-CRN with a pair of primary users (PUs) and multiple pairs of secondary users (SUs), and aims to minimize the primary transmitter (PT)â€™s energy provision (EP) while satisfying the networkâ€™s or usersâ€™ minimal throughput demands. Specifically, under the networkâ€™s minimal throughput demand, we first formulate the EP minimization (EPM) problems as non-linear optimization problems, then transform them into convex problems, and finally propose an algorithm that jointly uses the golden-section search and bisection search method to obtain the optimal time allocation of PTâ€™s energy transfer and each SUâ€™s packet transmission. Under the usersâ€™ minimal throughput demands, we jointly use the golden-section search, bisection search methods, and the dual sub-gradient algorithm to derive the optimal time allocation of PUs and SUs. Simulation results demonstrate the effectiveness of our algorithms from the perspective of reducing EP.}
}


@article{DBLP:journals/cn/MeiGCCL22,
	author = {Lifan Mei and
                  Jinrui Gou and
                  Yujin Cai and
                  Houwei Cao and
                  Yong Liu},
	title = {Realtime mobile bandwidth and handoff predictions in 4G/5G networks},
	journal = {Comput. Networks},
	volume = {204},
	pages = {108736},
	year = {2022},
	url = {https://doi.org/10.1016/j.comnet.2021.108736},
	doi = {10.1016/J.COMNET.2021.108736},
	timestamp = {Wed, 23 Feb 2022 11:16:09 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MeiGCCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile apps are increasingly relying on high-throughput and low-latency content delivery, while the available bandwidth on wireless access links is inherently time-varying. The handoffs between base stations and access modes due to user mobility present additional challenges to deliver a high level of user Quality-of-Experience (QoE). The ability to predict the available bandwidth and the upcoming handoffs will give applications valuable leeway to make proactive adjustments to avoid significant QoE degradation. In this paper, we explore the possibility and accuracy of realtime mobile bandwidth and handoff predictions in 4G/LTE and 5G networks. Towards this goal, we collect long consecutive traces with rich bandwidth, channel, and context information from public transportation systems. We develop Recurrent Neural Network models to mine the temporal patterns of bandwidth evolution in fixed-route mobility scenarios. Our models consistently outperform the conventional univariate and multivariate bandwidth prediction models. For the next second bandwidth prediction, in terms of Mean Absolute Error (MAE), our model is on average 15.28% better than the other methods in 4G traces and 15.37% better than the other methods in 5G traces. For 4G & 5G co-existing networks, we propose a new problem of handoff prediction between 4G and 5G, which is important to achieve good application performance in realistic 5G scenarios. We develop classification and regression based prediction models, which achieve more than 80% accuracy in predicting handoffs between 4G and 5G in a recent 5G dataset.}
}
