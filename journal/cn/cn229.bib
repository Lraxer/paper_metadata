@article{DBLP:journals/cn/JavanmardiSMAC23,
	author = {Saeed Javanmardi and
                  Mohammad Shojafar and
                  Reza Mohammadi and
                  Mamoun Alazab and
                  Antonio Caruso},
	title = {An {SDN} perspective IoT-Fog security: {A} survey},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109732},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109732},
	doi = {10.1016/J.COMNET.2023.109732},
	timestamp = {Mon, 26 Jun 2023 20:51:10 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JavanmardiSMAC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The utilization of the Internet of Things (IoT) has burst in recent years. Fog computing is a notion that solves cloud computing’s limitations by offering low latency to IoT network user applications. However, the significant number of networked IoT devices, the large scale of the IoT, security concerns, users’ critical data, and heterogeneity in this extensive network significantly complicate the implementation. The IoT-Fog architecture consists of fog devices (servers) at the fog layer, which decreases network utilization and response time due to their closeness to IoT devices. However, as the number of IoT and fog devices under the IoT-Fog architecture grows, new security concerns and requirements emerge. Because incorporating fog computing into IoT networks introduces some vulnerabilities to IoT-Fog networks, the nodes in the fog layer are the target of security threats. Software-Defined Networking (SDN) is a novel paradigm that decouples the data plane from control plane, resulting in better programmability and manageability. Attack defense mechanisms can be implemented in the IoT-Fog network without SDN. But SDN paradigm provides the IoT-Fog with some characteristics that facilitate counterattacks. This survey briefly explains some works that utilized the SDN features in the IoT-Fog network for security threats in the IoT-Oriented fog layer. To this end, we examine IoT-Fog, SDN, and SDN-based IoT-Fog networks. We describe security threats in IoT-Fog networks and briefly explain the vulnerabilities and attacks in the fog layer. Then, we describe the fog layer’s most common IoT-Fog security defense mechanisms. Following that, we present the SDN features, explore how SDN can help defensive mechanisms in IoT-Fog networks, and categorize the works based on the SDN features they use. We explain their features and present a comparison between them. Finally, we discuss the disadvantages of SDN in IoT-Fog networks.}
}


@article{DBLP:journals/cn/ChkirbeneHEKA23,
	author = {Zina Chkirbene and
                  Ridha Hamila and
                  Aiman Erbad and
                  Serkan Kiranyaz and
                  Nasser Al{-}Emadi},
	title = {D2DLive: Iterative live video streaming algorithm for {D2D} networks},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109734},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109734},
	doi = {10.1016/J.COMNET.2023.109734},
	timestamp = {Fri, 02 Jun 2023 21:23:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChkirbeneHEKA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart healthcare system incorporates new technologies making healthcare more efficient and more convenient. Virtual consultations provide interactions with healthcare professionals via video technology, which lessens the number of patients visiting health facilities and consequently reduces the risk of infections. Recently, device to device (D2D) communication showed extraordinary abilities to save the available resources and improve the network quality of service (QoS). Some of the existing algorithms for multimedia services over D2D networks consider only the signal to noise ratio (SNR) and ignore temporal requirements, which do not provide optimum performances. So, more efficient transmission mechanisms that improve the users’ QoS are needed to respond quickly to the ever-changing application demands. In response to these challenges, the research community began exploring novel solutions for video streaming, namely: Quantile-based Carrier-sense multiple access (CSMA), Flexible video transmission (Flexi) and Distributed Random approach (DR). However, these solutions introduce higher throughput, and jitter, especially in the case of live streaming. Other solutions are also available in the literature, but most of them cannot efficiently scale to large multi videos, multi-hops multi-user live video streaming. In particular, in most of these techniques, the chunk number, node transmission capacity or specific video delay, and time requirements are not taken into consideration in the path optimization.To overcome these shortcomings, we propose a novel D2DLive video streaming algorithm as a new solution for a higher QoS to improve medical data delivery. For each requested video, the proposed algorithm divides its content into small playable units called chunks that are transmitted using a new network selection algorithm. The proposed algorithm computes iteratively a suboptimal set of paths to be used to forward all the video chunks cooperatively to their destinations. Furthermore, we develop a new weight algorithm to evaluate the performance of the proposed algorithm in adhering to the upload and download capacities for each device while minimizing the transmission delay. Simulation results show that the proposed algorithm outperforms other established methods in terms of full delay, average upload, and download capacities.}
}


@article{DBLP:journals/cn/JiangJLWZFC23,
	author = {Jianguo Jiang and
                  Shang Jiang and
                  Yi Liu and
                  Siye Wang and
                  Yanfang Zhang and
                  Yue Feng and
                  Ziwen Cao},
	title = {Wi-Gait: Pushing the limits of robust passive personnel identification
                  using Wi-Fi signals},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109751},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109751},
	doi = {10.1016/J.COMNET.2023.109751},
	timestamp = {Wed, 22 Jan 2025 14:53:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/JiangJLWZFC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personnel identification plays a crucial role in many security applications, where the knowledge factor, such as a personnel identification number (PIN), constitutes the most popular personnel identification element. Meanwhile, thanks to the pervasive Wi-Fi infrastructure, personnel identification enabled by wireless sensing is gaining increasing attention with the advantages of non-intrusiveness, privacy-preserving, and anti-counterfeiting. In particular, the popularity of the fine-grained Wi-Fi channel state information (CSI) allows us to identify people via gait recognition. However, existing systems still have multiple limitations: (1) heavily rely on the strong assumptions of walking conditions; (2) require the environment to remain unchanged, especially the device placement; (3) extract low-level gait features for personnel identification. To address the above issues, our paper proposes Wi-Gait, a gait-based personnel identification system, and the contribution is threefold. First, we customized a novel deep learning model to extract unique gait features and achieve high accuracy in personnel identification. Second, thanks to our designed model, we can remove the dependency on walking cofactors and device placement and make Wi-Fi gait-based identification more realistic. Third, we evaluated the performance using the most popular Wi-Fi gait dataset, i.e., Widar 3.0. Extensive experiments show an average identification accuracy of 92.9% for ten users under various complex conditions.}
}


@article{DBLP:journals/cn/LauLCHO23,
	author = {Wei Jian Lau and
                  Joanne Mun{-}Yee Lim and
                  Chun Yong Chong and
                  Nee Shen Ho and
                  Thomas Wei Min Ooi},
	title = {General Outage Probability Model for UAV-to-UAV links in Multi-UAV
                  Networks},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109752},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109752},
	doi = {10.1016/J.COMNET.2023.109752},
	timestamp = {Mon, 17 Jun 2024 22:12:40 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LauLCHO23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicles (UAVs) swarms have become increasingly ubiquitous in the civilian domains due to their maneuverability and scalability in collaborative flying missions. However, there is a lack of characterization of wireless communication performance in multi-UAV system without being constrained by the node spatial distributions. In this paper, the performance of a UAV swarm network is analyzed by characterizing the effects of the UAV-to-UAV (U2U) interference in terms of the Signal-to-Interference-plus-Noise Ratio (SINR). To this end, a closed-form analytical model for the Cumulative Distribution Function (CDF) of the SINR is derived by considering both deterministic and stochastic channel processes, as well as deterministic UAV mobilities. The derived model can be used to analyze U2U interference in a UAV swarm network for any arbitrary node locations. Using this model, the effects of the network parameters on various network Key Performance Indicators (KPIs) such as the outage probabilities and channel capacity rate are studied. The model is extended to incorporate hybrid LoS channels, and the general model is used to evaluate the temporal evolutions of network outage probability given the UAV trajectories. With this, the main advantage of the model in analyzing the baseline outage probabilities of multi-UAV deployments is showcased.}
}


@article{DBLP:journals/cn/LiLJLGHRZ23,
	author = {Junfeng Li and
                  Dan Li and
                  Huiyou Jiang and
                  Du Lin and
                  Jinkun Geng and
                  Yukai Huang and
                  K. K. Ramakrishnan and
                  Kai Zheng},
	title = {Light: {A} Compatible, high-performance and scalable user-level network
                  stack},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109756},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109756},
	doi = {10.1016/J.COMNET.2023.109756},
	timestamp = {Mon, 26 Jun 2023 20:51:09 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiLJLGHRZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the number of CPU cores and the speed of Ethernet NICs keep increasing on server machines, the network stack in the kernel has become the bottleneck for applications demanding very high throughput and ultra-low latency. Recently there is a trend towards moving the network stack out of the kernel. However, most kernel-bypass network stacks discard POSIX APIs that legacy applications have been built on, and the intricate work of transplanting applications brings the barrier to real-world deployment of kernel-bypass stacks. In this work, we propose Light, a novel user-level network stack, which not only gains highly scalable performance on multi-core server, but also achieves compatibility with legacy applications. For compatibility, Light realizes efficient blocking APIs in the user space, intercepts network-related APIs in a non-intrusive manner, and uses the FD space separation technique for proper API redirection. For high performance and scalability, Light adopts lock-free shared queue based inter-process communication and full connection affinity to reduce overheads of system call, cache miss, etc. Experiments demonstrate that many types of legacy applications could run on Light without modifying their source code. Compared with the latest kernel stack, Nginx on Light achieves up to\n2\n.\n86\n×\nthroughput and 78.2 % lower tail latency (99.9th percentile) with 14 CPU cores.}
}


@article{DBLP:journals/cn/SeyoumSCK23,
	author = {Yemane Teklay Seyoum and
                  Syed Maaz Shahid and
                  Eun Seon Cho and
                  Sungoh Kwon},
	title = {Distributed load balancing algorithm considering QoS for next generation
                  multi-RAT HetNets},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109758},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109758},
	doi = {10.1016/J.COMNET.2023.109758},
	timestamp = {Fri, 02 Jun 2023 21:23:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SeyoumSCK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a distributed load balancing algorithm for multi-radio access technology (multi-RAT) heterogeneous networks (HetNets). The random deployment of small cells and the user mobility make the network load distribution uneven, which degrades overall network capacity and the QoS for users. Furthermore, the disparate capabilities of multiple RATs such as the different propagation delays in terrestrial and non-terrestrial RATs affect the latency requirements. To balance the network load in multi-RAT HetNets, we propose a distributed game-theoretic algorithm considering QoS for users. To that end, a cost function is defined to reflect the cell load status, required data rates, and delay constraints. In the proposed algorithm, overloaded cells iteratively minimize user cost in a distributed manner by associating a user with a less-loaded cell based on its cost. We provide analysis to show that minimizing each user cost in the game theory setting achieves a balanced load distribution among cells. We show that the proposed algorithm brings each cell load to a balanced state in a finite number of iterations. Via simulation, we show that the proposed algorithm achieves superior performance in terms of even load distribution, network throughput, and the number of users with satisfactory QoS.}
}


@article{DBLP:journals/cn/KressSSHHWHB23,
	author = {Fabian Kre{\ss} and
                  Vladimir Sidorenko and
                  Patrick Schmidt and
                  Julian H{\"{o}}fer and
                  Tim Hotfilter and
                  Iris Walter and
                  Tanja Harbaum and
                  J{\"{u}}rgen Becker},
	title = {CNNParted: An open source framework for efficient Convolutional Neural
                  Network inference partitioning in embedded systems},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109759},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109759},
	doi = {10.1016/J.COMNET.2023.109759},
	timestamp = {Sun, 12 Nov 2023 02:17:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/KressSSHHWHB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applications such as autonomous driving or assistive robotics heavily rely on the usage of Deep Neural Networks. In particular, Convolutional Neural Networks (CNNs) provide precise and reliable results in image processing tasks like camera-based object detection or semantic segmentation. However, to achieve even better results, CNNs are becoming more and more complex. Deploying these networks in distributed embedded systems thereby imposes new challenges, due to additional constraints regarding performance and energy consumption in the near-sensor compute platforms, i.e. the sensor nodes. Processing all data in the central node, however, is disadvantageous since raw data of camera consumes large bandwidth and running CNN inference of multiple tasks requires certain performance. Moreover, sending raw data over the interconnect is not advisable for privacy reasons. Hence, offloading CNN workload to the sensor nodes in the system can lead to reduced traffic on the link and a higher level of data security.}
}


@article{DBLP:journals/cn/AlmstedtBMJKW23,
	author = {Lennart Almstedt and
                  Kai Bleeke and
                  Mohammad Mahhouk and
                  Leander Jehl and
                  R{\"{u}}diger Kapitza and
                  Lars C. Wolf},
	title = {ContractBox: Realizing accountable data sharing on the edge using
                  a small scale blockchain},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109768},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109768},
	doi = {10.1016/J.COMNET.2023.109768},
	timestamp = {Sun, 19 Jan 2025 14:22:24 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlmstedtBMJKW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The utilization of IoT devices is becoming omnipresent in industrial settings. However, adoption in more rural areas still poses challenges. Especially when processing data on the edge, privacy, data integrity, accountability and data ownership pose challenges since the devices may be easily accessed and manipulated.}
}


@article{DBLP:journals/cn/ChengJYZG23,
	author = {Zhimo Cheng and
                  Xinsheng Ji and
                  Wei You and
                  Yu Zhao and
                  Zhongfu Guo},
	title = {{SECHO:} {A} deep reinforcement learning-based scheme for secure handover
                  in mobile edge computing},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109769},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109769},
	doi = {10.1016/J.COMNET.2023.109769},
	timestamp = {Fri, 02 Jun 2023 21:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChengJYZG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The experience of users in Mobile Edge Computing (MEC) highly depends on data offloading whose major bottle neck is the design of handover scheme. Existing research advocates that offloading security and quality of service (QoS) should be both considered for an optimal handover. However, recent studies concentrate on improving QoS while neglect security. Due to the complexity of MEC scenarios, generating an optimal handover scheme improving both QoS and security remains a challenge. To solve this problem, we propose SECHO, a secure handover scheme for blockchain-based single-user mobile edge computing system. Then, we design a security policy selection algorithm of offloading tasks to guarantees security performance and controls overhead. Finally, we customize the neural network for a deep reinforcement learning algorithm to realize optimal handover, where a security policy selection method is engaged to collaboratively enhance offloading QoS while decreasing underlying risks. Simulation results show that SECHO leads a 29.6%–302% advantage in overall performance than existing approaches. It proves that our proposal is effective to achieve secure handover while not degrading offloading QoS.}
}


@article{DBLP:journals/cn/NandePSUBFB23,
	author = {Swaraj Shekhar Nande and
                  Marius Paul and
                  Stefan Senk and
                  Marian Ulbricht and
                  Riccardo Bassoli and
                  Frank H. P. Fitzek and
                  Holger Boche},
	title = {Quantum enhanced time synchronisation for communication network},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109772},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109772},
	doi = {10.1016/J.COMNET.2023.109772},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NandePSUBFB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is essential to establish precise times in future communication networks. Any real-time task’s ability to function depends on the system’s ability to synchronise time. In the current communication network, time synchronisation is critical and must be maintained to transmit data packets. The functionality of 6G, the Tactile Internet, Time-Sensitive Networking, and ultra-reliable low-latency communications is highly susceptible to time synchronisation. We investigated the idea of employing time synchronisation across different communication network nodes. The current state-of-the-art employs network protocols like Precision Time Protocol for clock synchronisation across different nodes. These network protocols are not very robust and can generate jitters in data transmission. In this paper, we suggested synchronising the time of the node clocks at three different places using quantum technology. Notably, the oscillation frequencies of each qubit (or oscillator) located at these nodes can be synchronised using quantum synchronisation technique. This set of three oscillators will work as a single clock and will be the master clock of the network. We propose distributing precise time and frequency standards using quantum synchronisation on node clocks. We can synchronise the three qubits (each placed at one node) to oscillate at an identical frequency by applying an external field of a wavelength of\n813\n.\n32\nnm\n. We analysed our model for different coupling constants and dissipation rates to provide an analysis of the behaviour of the amount of synchronisation in different experimental configurations. The optimal accuracy for our system is\n1\n.\n6\n×\n1\n0\n15\nsignals per second. Further, we used the Allan deviation to examine the stability of our system for various noise strengths.}
}


@article{DBLP:journals/cn/RivasA23,
	author = {Angel Esteban Labrador Rivas and
                  Taufik Abr{\~{a}}o},
	title = {Raptor-IRSA Grant-free Random Access protocol for smart grids applications},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109775},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109775},
	doi = {10.1016/J.COMNET.2023.109775},
	timestamp = {Fri, 02 Jun 2023 21:23:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RivasA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper deals with the reliability of random access (RA) protocols for massive wireless smart grid communication (m-SGC). We propose and analyze an improved grant-free RA (GF-RA) protocol for critical SG applications under strict QoS m-SGC requirements. At first, we discuss the main features of the SG neighborhood area network (NAN) architecture. We explore the main features of low-rate machine-type wireless networks, and also we describe a technology characterization of wireless neighborhood area networks (WNAN) in medium-range coverage applications. We propose a new-improved irregular repetition slotted ALOHA, combining Raptor codes and irregular ALOHA, namely RapIRSA random access protocol, to better respond to critical high-reliability QoS requirements under a 5G network perspective. Then, we compare and comprehensively analyze the proposed RapIRSA protocol with two existing RA protocols, the IRSA protocol, and the classical Slotted Aloha. Finally, We summarize the potential challenges in implementing the proposed RA protocol for SG critical applications considering a massive number of smart sensors (SS).}
}


@article{DBLP:journals/cn/YangZZX23,
	author = {Cheng Yang and
                  Yangming Zhao and
                  Gongming Zhao and
                  Hongli Xu},
	title = {{DFS:} Joint data formatting and sparsification for efficient communication
                  in Distributed Machine Learning},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109777},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109777},
	doi = {10.1016/J.COMNET.2023.109777},
	timestamp = {Wed, 17 May 2023 10:03:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YangZZX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient communication is crucial to Distributed Machine Learning (DML). In this work, we propose an approach jointing Data Formatting and Sparsification (DFS) to optimize the communication in DML systems based on the parameter server framework. By doing so, we can reduce the time to transmit (aggregated) gradients between the parameter server and workers, and consequently the time to complete training jobs. More specifically, in DFS, every worker first tries to derive as many blocks with all-zero gradients as possible via sparsification, and transmits gradients block by block in a streaming fashion. By skipping blocks with all-zero gradients, we can reduce the communication cost for gradient transmission. Different from previous works on optimizing the communication in DML systems, DFS has three distinct features: (i). it dynamically determines the gradient block size; (ii). it takes into consideration both the data transfer from workers to the parameter server and that from the parameter server to workers; and (iii). it jointly optimizes the data formatting and sparsification. In other words, it performs sparsification in the way that helps form more (or larger) all-zero blocks and save more communication cost. By implementing DFS on a real testbed, we find that it can reduce the time to train a ResNet-18 model by 74.12%. Through extensive simulations, we demonstrate that DFS outperforms the state-of-the-art technique, i.e., OmniReduce (Fei et al., 2021), by up to 87.17% in terms of reducing communication cost in DML systems.}
}


@article{DBLP:journals/cn/HuCC23,
	author = {Bo Hu and
                  Liangyu Chen and
                  Shanzhi Chen},
	title = {Joint trajectory-resource optimization for UAV-enabled uplink communication
                  networks with wireless backhaul},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109779},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109779},
	doi = {10.1016/J.COMNET.2023.109779},
	timestamp = {Fri, 02 Jun 2023 21:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies trajectory optimization and resource allocation for unmanned aerial vehicle (UAV) enabled uplink communication networks with wireless backhaul (WB), in which a UAV provides non-orthogonal multiple access (NOMA) based uplink relaying services for multiple mobile users with stochastic movement and forwards the decoded multi-user signals to a remote BS by means of a WB link. To assure that the UAV are able to forward the decoded multi-user signals to the remote BS, we formulate a problem of maximizing the uplink average achievable sum-rate by jointly optimizing UAV trajectory, each mobile user’s transmit power, and bandwidth allocation coefficients of both wireless access and backhaul links, subject to information-causality constraint, system available bandwidth limitation and power budget of each mobile user. The challenge lies in that the time-vary successive interference cancellation (SIC) decoding order results in a strong-coupled joint NOMA decoding order design and UAV trajectory optimization problem, which makes this problem intractable. To tackle this challenge, we firstly transform the original problem into a tractable form. Then, a penalized iteration approach is developed to handle the reformulated problem. Simulation results verify the proposed scheme can achieve higher uplink average achievable sum rate than benchmarks.}
}


@article{DBLP:journals/cn/ZhouWZL23,
	author = {Qiang Zhou and
                  Liangmin Wang and
                  Huijuan Zhu and
                  Tong Lu},
	title = {Few-shot website fingerprinting attack with cluster adaptation},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109780},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109780},
	doi = {10.1016/J.COMNET.2023.109780},
	timestamp = {Fri, 02 Jun 2023 21:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouWZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Website fingerprinting attacks can destroy users’ privacy on the Internet, even when the communication is through an anonymous system, such as Tor. Deep learning-based website fingerprinting attacks have made great progress in anonymous traffic classification, and obtained high classification accuracy. However, deep website fingerprinting attacks require a large amount of training data with annotations. With the version update of anonymous network systems or network condition changing, previously collected traffic traces cannot be used for the classification of the newly collected traces, and recollecting traffic traces is time-consuming and labor-intensive. To address the above problems, this paper proposes Cluster Website Fingerprinting Attack (CWFA) for the few-shot website fingerprinting attack, which is based on the clustering assumption: samples belonging to the same cluster have the same category. CWFA utilizes deep neural networks to extract the trace features, and aligns the labeled category centers with unlabeled target cluster centers in the feature space. CWFA can preserve the category-level structure and facilitates the classification of the newly collected traffic traces. We conduct extensive experiments on public datasets in both closed-world and open-world scenarios, and we test the performance of the defended traces. Compared with the state-of-the-art WF methods, experiment results demonstrate the effectiveness and superiority of our proposed method.}
}


@article{DBLP:journals/cn/MohammadiS23,
	author = {Razieh Mohammadi and
                  Zahra Shirmohammadi},
	title = {RLS\({}^{\mbox{2}}\): An energy efficient reinforcement learning-
                  based sleep scheduling for energy harvesting WBANs},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109781},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109781},
	doi = {10.1016/J.COMNET.2023.109781},
	timestamp = {Wed, 17 May 2023 10:03:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MohammadiS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Energy-Harvesting Wireless Body Area Networks (EH-WBAN), one of the fundamental challenges is preserving the self-sustainability of sensors without compromising network reliability and connectivity. Determining the sleep/wake schedule of body nodes (BNs) is an efficient way to achieve self-sustainability. Sleeping nodes should be connected to at least one active node to reduce delay and keep the network connected. There are two fundamental problems with previous methods for determining BN's sleep/wake schedule: (1) BN suffers from emergency packet loss and unnecessary frequent sleeping and waking up, and (2) They do not guarantee network connectivity. Studies that have only examined connectivity in EH-WBAN also have two main issues: (1) BNs are considered homogenous in terms of energy harvesting and its consumption, (2) These methods cannot adapt to the time-varying behavior of energy-harvesting resources. This study proposes a new method for sleep/wake scheduling called Reinforcement Learning-based Sleep Scheduling (RLS2). RLS2 has the following innovative points: (1) To avoid emergency packet loss or unnecessary frequent sleeping and waking up, each BN has its own sleep/wake schedule based on its energy level and sensed data changes, (2) Lowest possible number of BNs are determined as relay nodes in each round to increase network reliability and connectivity; these BNs remain active in each round, while the others operate according to the determined schedule. In this part of the proposed method: (1) Heterogeneous BNs are considered, (2) As a first step in solving adaptability, the problem of finding the optimal active groups is formulated as a Markov decision process (MDP), followed by a Q-learning algorithm capable of learning time-varying behavior of energy harvesting resources, (3) The unavailable action space is removed to reduce the problem's complexity, (4) To achieve good Q-learning performance, a reward function based on residual energy level and neighborhood degree of BNs is defined. It can find an active group with the lowest cardinality in the current round, which is maximum in terms of the residual energy of its sensors. The performed simulations indicate the appropriate convergence of the proposed method. The results show that, on average, the proposed method improves network connectivity and energy efficiency by 50% and 31%, respectively, and reduces network delay by 27%.}
}


@article{DBLP:journals/cn/LastovickaHVJC23,
	author = {Martin Lastovicka and
                  Martin Hus{\'{a}}k and
                  Petr Velan and
                  Tom{\'{a}}s Jirs{\'{\i}}k and
                  Pavel Celeda},
	title = {Passive operating system fingerprinting revisited: Evaluation and
                  current challenges},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109782},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109782},
	doi = {10.1016/J.COMNET.2023.109782},
	timestamp = {Sat, 30 Sep 2023 10:07:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LastovickaHVJC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fingerprinting a host's operating system is a very common yet precarious task in network, asset, and vulnerability management. Estimating the operating system via network traffic analysis may leverage TCP/IP header parameters or complex analysis of hosts' behavior using machine learning. However, the existing approaches are becoming obsolete as network traffic evolves which makes the problem still open. This paper discusses various approaches to passive OS fingerprinting and their evolution in the past twenty years. We illustrate their usage, compare their results in an experiment, and list challenges faced by the current fingerprinting approaches. The hosts' differences in network stack settings were initially the most important information source for OS fingerprinting, which is now complemented by hosts' behavioral analysis and combined approaches backed by machine learning. The most impactful reasons for this evolution were the Internet-wide network traffic encryption and the general adoption of privacy-preserving concepts in application protocols. Other changes, such as the increasing proliferation of web applications on handheld devices, raised the need to identify these devices in the networks, for which we may use the techniques of OS fingerprinting.}
}


@article{DBLP:journals/cn/KhalilSR23,
	author = {Alvi Ataur Khalil and
                  Mohamed Y. Selim and
                  Mohammad Ashiqur Rahman},
	title = {Deep learning-based energy harvesting with intelligent deployment
                  of RIS-assisted UAV-CFmMIMOs},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109784},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109784},
	doi = {10.1016/J.COMNET.2023.109784},
	timestamp = {Fri, 02 Jun 2023 21:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KhalilSR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-evolving internet of things (IoT) has spawned hundreds of wireless sensors that communicate via the internet infrastructure. The lifetime and self-sustainability of these sensors are pivotal factors dictating the performance of respective application infrastructure. Radio frequency energy harvesting (RFEH) technology has exhibited the capability of effectively augmenting the battery lifetime of these sensors. In this work, we introduce a novel framework called CURe, which combines the advantages of cell-free massive multiple-input multiple-output (CFmMIMO) and reconfigurable intelligent surfaces (RISs) to provide uninterrupted energy harvesting for IoT devices through RFEH. CFmMIMO integrates the advantages of distributed systems and massive MIMO, while RIS improves the signal strength of the information transfer and RFEH via its passive reflection capabilities. Moreover, we consider unmanned aerial vehicles (UAVs) equipped with CFmMIMO as mobile access points (APs) to better serve the moving sensory devices. To further enhance RFEH, we propose DeNCE, a channel estimation technique based on deep learning (DL) that eliminates the need for traditional closed-form equation-based channel estimation methods. Through evaluation, we first validate the performance of CURe by comparing it with the modified bisection search for max–min fairness (MBS-MMF) algorithm and later corroborate that DeNCE significantly improves the performances of both models. Finally, to optimize the UAV deployment and ensure continuous RFEH coverage, we propose dARL, a deep reinforcement learning (DRL)-based scheduling framework that enables UAV-CFmMIMO swarms to perform continuous energy harvesting in the coverage area collaboratively.}
}


@article{DBLP:journals/cn/AliSEA23,
	author = {Zainab Hassan Ali and
                  Noha A. Sakr and
                  Nora El{-}Rashidy and
                  Hesham A. Ali},
	title = {A reliable position-based routing scheme for controlling excessive
                  data dissemination in vehicular ad-hoc networks},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109785},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109785},
	doi = {10.1016/J.COMNET.2023.109785},
	timestamp = {Wed, 27 Sep 2023 21:14:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AliSEA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past two decades, the automotive industry has undergone tremendous changes, as this field has become one of the fastest developing and growing fields, especially with the progress of global digitization and massive research related to networks. So, it comes as no surprise that self-driving vehicles are ubiquitous mainly relying on Vehicular ad-hoc networks (VANETs). This transformation ensures improved road navigation and traffic congestion avoidance by relying on Rapid Data Deployment (DD). On the other hand, although DD achieves high connection reliability, it may affect network bandwidth and performance. In addition, excessive DD causes frequent link outages resulting in reduced data delivery, massive packet loss, and premature end of network life. This paper presents an integrated architecture proposal for a vehicle dynamic assistance architecture, based on reliable methods to ensure steering accuracy while minimizing the energy expended for controlling the DD rate via VANETs. The proposed architecture integrates between Software-Defined Networks (SDN) and fog computing based on dealing with the mobility factors that exploit vehicle communication behaviors. Such integration will aid in improving network performance in terms of packet delivery and DD. The study also discusses how to take into consideration the Euclidean distance, geographical routing information, residual power ratio, and latency time to maximize network stability and avoid possible link disruption. The simulation results prove that there is a 62% to 70% enhancement of the whole power consumption and network throughput, depending on the implementation of the proposed position-based routing approach. Interestingly, the proposed routing protocol is a dual-phase routing protocol with a 90% of SDN data packet delivery ratio and an 82% of SDN data loss reduction. So, when the SDN fails to deliver packets, the proposed position-based routing handles them as a parallel mechanism of SDN.}
}


@article{DBLP:journals/cn/WangYZ23,
	author = {Jian Wang and
                  Yuping Yan and
                  Guosheng Zhao},
	title = {Task Recommendation Method Combining Multimodal Cognition and Collaboration
                  in Mobile Crowdsensing Systems},
	journal = {Comput. Networks},
	volume = {229},
	pages = {109796},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109796},
	doi = {10.1016/J.COMNET.2023.109796},
	timestamp = {Mon, 03 Jun 2024 08:09:47 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A multimodal data fusion optimization and task recommendation method is proposed in mobile crowd sensing for the complex data redundancy generated by sensing users, which leads to low task recommendation accuracy and high sensing cost. The method extracts the modal features using BERT and Faster-RCNN. It obtains self-attention and cross-guided features by the attention mechanism to achieve intra-modal and inter-modal information sharing and reduce the risk of fusing unrelated modal features. These obtained modal features are then used for hierarchical fusion. Extracting fusion features of different granularity by capturing implicit features within a single modality and complementary features between multiple modalities, then jointly optimizing them, thus making the fusion results more focused on information of interest to users in multimodal historical task data. The cross-guided self-attention mechanism is designed to improve the accuracy of multimodal data fusion by fully fusing modal data and the joint optimization of their different fusion features. Thus it increases the sensing user's interest in completing the task, which in turn increases the motivation of the sensing user to participate in the sensing task and improves the sensing quality. Finally, the similarity between the new task and the historical task is calculated to decide whether to recommend the new task to the sensing user or not. Experiments based on the Flicker8k and Pascal Sentence datasets show that our proposed method can effectively fuse multimodal data and improve the accuracy of task recommendations.}
}
