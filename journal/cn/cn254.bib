@article{DBLP:journals/cn/NiuCCQ24,
	author = {Dandan Niu and
                  Guang Cheng and
                  Zihan Chen and
                  Xing Qiu},
	title = {Video stalling identification for web live streaming under {HTTP-FLV}},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110714},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110714},
	doi = {10.1016/J.COMNET.2024.110714},
	timestamp = {Wed, 12 Feb 2025 08:32:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/NiuCCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Live broadcasts have become one of the most popular forms of entertainment. Quality of user Experience (QoE) is a vital quantitative criterion for evaluating user satisfaction while watching live broadcasts, and it is positively correlated with the increase in the income of Internet Service Providers (ISPs). Video stalling identification plays a crucial role in the evaluation of QoE. However, encrypted live streaming hides video content, which makes identifying video stalling challenging. Existing studies primarily detect video stalling in a fixed time interval and focus on high-dimensional features. However, the capacity of the client byte buffer is dynamic, resulting in the stalling and non-stalling existing in a certain and fixed stalling time. In addition, the handling time of abundant features causes further latency. We propose Truncation of Dynamic Bytes and non-linear Integrated Modification based on Double Buffers (\nD\nB\n2\n) to identify video stalling under HTTP-FLV protocol in various network conditions and live types. We pull real-time video to get client buffer parameters and build a dynamic mapping based on the double buffer between network packets and the video playing states. This allows a more objective and precise evaluation of video stalling. We design a new network feature by creating a non-linear relationship between network packets and the client buffer. This is achieved by combining non-linear convergent distribution with basic traffic features. The feature is fed into a lightweight machine learning model to train the classifier, achieving low processing latency and high identification accuracy. The experimental results show that\nD\nB\n2\ncan achieve 98.91% stalling identification accuracy with 1.256 ms operation time in a mixture of live video types, outperforming state-of-the-art techniques.}
}


@article{DBLP:journals/cn/RagoGPCVMVBTSPMSPG24,
	author = {Arcangela Rago and
                  Alessandro Guidotti and
                  Giuseppe Piro and
                  Ernestina Cianca and
                  Alessandro Vanelli{-}Coralli and
                  Simone Morosi and
                  Giuseppe Virone and
                  Fabrizio Brasca and
                  Martina Troscia and
                  Marina Settembre and
                  Laura Pierucci and
                  Francesco Matera and
                  Mauro De Sanctis and
                  Sara Pizzi and
                  Luigi Alfredo Grieco},
	title = {Multi-layer {NTN} architectures toward 6G: The {ITA-NTN} view},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110725},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110725},
	doi = {10.1016/J.COMNET.2024.110725},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/RagoGPCVMVBTSPMSPG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper describes the integration of Terrestrial and Non-Terrestrial Networks, wherein space-based network entities collaborate with traditional and emerging terrestrial communication frameworks to furnish pervasive, resilient, and three-dimensional wireless connectivity worldwide toward the 6th Generation of communication networks. This integration supports heterogeneous services, such as enhancing coverage, user experience, system capacity, service reliability, and availability, while also providing high-speed connectivity in remote or disaster-affected areas, improving existing 5th Generation technologies. Various Use Cases are detailed, highlighting the pivotal roles that Non-Terrestrial Networks play in distinguishing between urban/suburban and rural environments, with particular emphasis on transportation ecosystems. Through this analysis, Key Performance Indicators and requirements are delineated to characterize the requisite service quality for these diverse Use Cases. The paper further presents an overview of potential and standards-compliant integrated Terrestrial/Non-Terrestrial architectures, delineating their roles both in backhauling and access across different layers of Non-Terrestrial systems and elements. These insights are derived from studies conducted within the Integrated Terrestrial And Non-Terrestrial Networks (ITA-NTN) project, part of the European Union initiative defined as the Italian National Recovery and Resilience Plan (NRRP) RESTART Research Program.}
}


@article{DBLP:journals/cn/AmiriDELMNS24,
	author = {Zuhra Amiri and
                  Shahram Dehdashti and
                  Kareem H. El{-}Safty and
                  Igor Litvin and
                  Pere Munar{-}Vallespir and
                  Janis N{\"{o}}tzel and
                  Simon Sekavcnik},
	title = {Quantum advantages for data transmission in future networks: An overview},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110727},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110727},
	doi = {10.1016/J.COMNET.2024.110727},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AmiriDELMNS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We review recent advancements in the domain of Joint Detection Receivers and Entanglement-Assisted Data transmission links with an emphasis on their potential use in future networks. Both data transmission techniques can surpass the Shannon limit by significant amounts in situations where either the number of photons per received information carrier or the number of transmitted photons per information carrier is extremely small. To obtain an advantage from shared entanglement, significant noise levels are needed as well. These fundamental constraints dictate that only certain application scenarios are of relevance to the new technology. We discuss these constraints in detail in the context of the current network architecture and stress the relation to optical computation. Based on our discussion, we go on to propose potential domains of application for the new technology.}
}


@article{DBLP:journals/cn/ZhuGYTDWH24,
	author = {Tengteng Zhu and
                  Zehua Guo and
                  Chao Yao and
                  Jiaxin Tan and
                  Songshi Dou and
                  Wenrun Wang and
                  Zhenzhen Han},
	title = {Byzantine-robust Federated Learning via Cosine Similarity Aggregation},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110730},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110730},
	doi = {10.1016/J.COMNET.2024.110730},
	timestamp = {Fri, 20 Sep 2024 14:02:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuGYTDWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is proposed to train a machine learning model for clients with different training data. During the training of FL, a centralized server is usually employed to aggregate local models from clients iteratively. The aggregation process suffers from Byzantine attacks, where clients’ models could be maliciously modified by attackers to degrade the training performance. Existing defense aggregation solutions use distances or angles between different gradients to identify and eliminate malicious models from clients. However, they do not work well due to the high dimensional property of the machine learning model. Distance-based solutions cannot effectively identify attackers when the gradient direction of the model is maliciously tampered with. Angle-based solutions face the issue of low model accuracy for large models. In this paper, we propose Convolutional Kernel Angle-based Defense Aggregation (CKADA) to improve defense performance under various Byzantine attacks. The key of CKADA is to use the angle between convolutional kernels as the attack detection metric because the obtuse angle indicates the wrong training direction. CKADA calculates the angle between a client’s convolutional kernel gradients and the server’s convolutional kernel gradients as the attacker detection metric and eliminates convolutional kernel gradients of clients that create an obtuse angle to mitigate the impact of attackers on the model. We evaluate the performance of CKADA using AlexNet, ResNet-50, and GoogLeNet under two typical attacks. Simulation results show that CKADA mitigates the impact of Byzantine attacks and outperforms existing angle-based solutions and distance-based solutions by improving inference accuracy up to 67% and 89% respectively.}
}


@article{DBLP:journals/cn/AkbarAZJ24,
	author = {Aamina Akbar and
                  Ashfaq Ahmed and
                  Adnan Zafar and
                  Sobia Jangsher},
	title = {An energy-efficient JT-CoMP enabled framework with adaptive {OMA/NOMA}
                  in HetNets},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110733},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110733},
	doi = {10.1016/J.COMNET.2024.110733},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AkbarAZJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing demand for higher data rates and improved energy efficiency (EE) in next-generation wireless networks necessitates the optimized selection of multiple-access and coordination techniques. A hybrid joint transmission (JT)-coordinated multi-point (CoMP) enabled orthogonal multiple access (OMA)/non-orthogonal multiple access (NOMA) technique, combining the spectral efficiency (SE) and capacity benefits of CoMP NOMA with the interference mitigation of CoMP OMA, offers a highly adaptable solution for future wireless networks. This paper studies the joint optimization of CoMP/non-CoMP selection, OMA/NOMA selection, power allocation, and user pairing, with the objective of maximizing the EE in the network. A Dynamic CoMP user selection with energy-efficient adaptive multiple access (DCEAMA) algorithm to solve the formulated problem is proposed. Our Monte Carlo simulations show that the DCEAMA surpasses both the pure CoMP OMA and CoMP NOMA schemes in terms of EE, with an average increase of 38% and 26% respectively. We compare our heuristic technique to an exhaustive search strategy to evaluate its efficiency. The findings indicate that our strategy produces comparable EE across various power levels with reduced computational complexity.}
}


@article{DBLP:journals/cn/ZhangYLNLG24,
	author = {Jingyu Zhang and
                  Jin Yang and
                  Liyao Li and
                  Qifeng Nian and
                  Lailong Luo and
                  Deke Guo},
	title = {{DBUP:} Dynamic blockchain {UTXO} processing for storage efficiency
                  optimization},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110744},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110744},
	doi = {10.1016/J.COMNET.2024.110744},
	timestamp = {Fri, 20 Sep 2024 14:02:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangYLNLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In traditional blockchain systems, the Unsent Transaction Output (UTXO) dataset stores all generated UTXOs, which typically represent digital assets of the users. UTXOs can be used to track the output status of cryptocurrency transactions. However, with the development of blockchain networks, the UTXO dataset continuously increasing in scale. In the UTXO dataset, a large number of low-value UTXOs occupied huge storage space, making the blockchain dataset expanded. This impacts the running performance of the whole blockchain system. To address this issue, this paper proposes the Dynamic Blockchain UTXO Processing strategy (DBUP) for UTXO storage efficiency optimization. This strategy aims to rapidly consume the low-value UTXOs, effectively alleviating UTXO dataset expansion. Furthermore, Through improved simulated annealing (ISA) algorithm and a UTXO dynamic adjustment mechanism in DBUP, the strategy ensures that the transaction fee in the transaction process remain at a low level. The experimental evaluation indicates that our strategy can achieve the better performance against the Low Value First (LVF) and the First-In-First-Out (FIFO) strategies, enhancing the storage efficiency of blockchain systems.}
}


@article{DBLP:journals/cn/LiZXLLZLLZZ24,
	author = {Zhuo Li and
                  Nan Zhang and
                  Hao Xun and
                  Jindian Liu and
                  Peng Luo and
                  Yu Zhang and
                  Teng Liang and
                  Kaihua Liu and
                  Wang Zhang and
                  Wanli Zhao},
	title = {LearningTuple: {A} packet classification scheme with high classification
                  and high update},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110745},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110745},
	doi = {10.1016/J.COMNET.2024.110745},
	timestamp = {Fri, 20 Sep 2024 14:02:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiZXLLZLLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet classification is widely used in network infrastructures and is the key technique that supports security and other functions. The real-time nature of network services naturally demands high classification speed, while the emerging SDN makes rule changes more flexible, thus placing higher demands on the performance of rule update in classification schemes. In this paper, Learning Tuple(LT) is proposed to achieve high classification performance for packets while maintaining the high update characteristics of tuple space-based schemes. Specifically, to solve the issue of excessive tuples and rule overlap due to merging tuples, LT iteratively divides the space by using rule overlap and hash collisions as negative feedback and applies a reinforcement learning algorithm, SARSA, at each level to ensure its reasonableness. Efficient space partitioning guides the construction of tuples, and an excellent rule mapping method called PLR is designed, which improves classification performance. Experimental results demonstrate that compared with classic and advanced classification schemes TSS, TupleMerge, MultilayerTuple, PartitionSort, HybridTSS, and TupleTree, LT achieves average classification performance improvements of 9.23x, 1.74x, 1.45x, 2.85x, 1.37x and 1.25x, as well as average update performance improvements of 1.83x, 6.75x, 1.22x, 6.16x, 1.21x, 10.66x, respectively.}
}


@article{DBLP:journals/cn/YuCKM24,
	author = {Jaehak Yu and
                  Yangseo Choi and
                  Ki{-}Jong Koo and
                  Daesung Moon},
	title = {A novel approach for application classification with encrypted traffic
                  using {BERT} and packet headers},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110747},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110747},
	doi = {10.1016/J.COMNET.2024.110747},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YuCKM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have seen substantial advancements in Internet technology along with environmental changes, which have led to the emergence of various security issues. There is also a trend of explosive growth in applications that encrypt network traffic for various types of services. Therefore, the classification of applications within encrypted traffic represents an important research issue for both secure network management and efficient bandwidth management. In such encrypted traffic, the payload itself is encrypted, and it is no longer viable to classify applications based on signatures extracted from plaintext. Most applications in public datasets for encrypted traffic classification are collected with the same IP address and port number, which makes the 5-tuple information a strong identifier. However, this 5-tuple contains many characteristics related to both the traffic collection environment and user-specific traits, rather than intrinsic features of the applications themselves. Therefore, when addressing the problem of encrypted traffic application classification, it is advisable to utilize header information excluding the 5-tuple and payload. Therefore, this paper proposes a novel service type and application classification system based on the Bidirectional Encoding Representation Transformer (BERT), which utilizes packet header information from encrypted traffic. The proposed system ensures the accuracy and generalization performance of the classification model by using only the header information from traffic packets, excluding the 5-tuple and payload. Further, to preserve the characteristics and semantic meaning of an encrypted traffic packet, sentences embedded with 2-byte tokens were used as input for BERT. The proposed system was designed to exclude labeling information from all sentences during the pre-training phase before proceeding with training. Fine-tuning was then conducted to align the system with the objectives of the service type and application classification. This experiment utilized the publicly available ISCX VPN-nonVPN dataset, and the proposed model achieved remarkable accuracy in the key performance measure, i.e., F1-scores, with values of 99.24 % in service type classification and 98.74 % in application classification. This capability can be used in maintaining the confidentiality of encrypted traffic, network security monitoring, Quality of Service (QoS), and traffic management in complex IT environments.}
}


@article{DBLP:journals/cn/ZhouJYQZX24,
	author = {Deqiang Zhou and
                  Xinsheng Ji and
                  Wei You and
                  Hang Qiu and
                  Yu Zhao and
                  Mingyan Xu},
	title = {{DDQN-SFCAG:} {A} service function chain recovery method against network
                  attacks in 6G networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110748},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110748},
	doi = {10.1016/J.COMNET.2024.110748},
	timestamp = {Fri, 20 Sep 2024 14:02:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouJYQZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service Function Chain (SFC) as an effective solution can satisfy the diverse service requirements of six application scenarios in Network Function Virtualization (NFV)-enabled 6G networks. Resilience as a new key capability indicator in 6G networks puts forward higher requirements for the Quality of Service (QoS) of SFCs. In this article, we study the resilience recovery method in network attack scenarios. We make full use of the monitoring and early warning capability of the monitoring function and propose a proactive recovery method called Double Deep Q-Network based on SFC Attack Graph (DDQN-SFCAG). We fully consider the characteristics of network attacks to generate SFC attack graphs and determine the recovery strategy of SFC according to the service security requirements to provide guidance for recovery. Among them, we design three recovery modes for the recovered Virtual Network Functions (VNFs) to determine the optimal recovery strategy and avoid resource waste. We formalize the SFC recovery problem, which aims to minimize the recovery cost while meeting the recovery strategy. In order to shorten the interruption time, we use DDQN to quickly solve the recovery solution to ensure optimal recovery performance. Our extensive evaluation shows DDQN-SFCAG has excellent recovery performance in network attack scenarios and can reduce the recovery cost by at least 31% compared to the state-of-the-art methods.}
}


@article{DBLP:journals/cn/ArzoSBDFF24,
	author = {Sisay Tadesse Arzo and
                  Domenico Scotece and
                  Riccardo Bassoli and
                  Michael Devetsikiotis and
                  Luca Foschini and
                  Frank H. P. Fitzek},
	title = {Softwarized and containerized microservices-based network management
                  analysis with {MSN}},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110750},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110750},
	doi = {10.1016/J.COMNET.2024.110750},
	timestamp = {Mon, 03 Mar 2025 21:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ArzoSBDFF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Microservice architecture is a service-oriented paradigm that enables the decomposition of cumbersome monolithic-based software systems. Using microservice design principles, it is possible to develop flexible, scalable, reusable, and loosely coupled software that could be containerized and deployed in a distributed edge/cloud environment. The flexible deployment of microservices in an edge environment increases system performance in terms due to dynamic service function placement and chaining possibly resulting in latency reduction, fault tolerance, scalability, efficient resource utilization, cost reduction, and energy consumption reduction. On the other hand, virtualization and containerization of microservices add processing and communication overheads. Therefore, to evaluate end-to-end microservices-based system performance, we need to have an end-to-end mathematical formulation of the overall microservice-based network system. Incorporating the virtualization overhead, here we provide end-to-end mathematical formulation considering system parameters: latency, throughput, computational resource usage, and energy consumption. We then evaluate the formulation in a testbed environment with the Microservice-based SDN (MSN) framework that decomposes the Software-defined Networking (SDN) controller in microservices with Docker Container. The final result validates the presented mathematical modeling of the system’s dynamic behavior which can be used to design a microservice-based system.}
}


@article{DBLP:journals/cn/UllahYJ24,
	author = {Waheed Ullah and
                  Fengfan Yang and
                  Dushantha Nalin K. Jayakody},
	title = {{OTFS} modulated massive {MIMO} with 5G {NR} {LDPC} coding: Trends,
                  challenges and future directions},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110751},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110751},
	doi = {10.1016/J.COMNET.2024.110751},
	timestamp = {Fri, 20 Sep 2024 14:02:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/UllahYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the performance of coded massive multiple-input multiple-output (MIMO) systems utilizing Orthogonal Time Frequency and Space modulation (OTFS). Our innovative approach harnesses the power of OTFS modulation, a cutting-edge modulation technique renowned for its capacity to mitigate the detrimental effects of time-varying channels. Additionally, we introduce a comprehensive system model that incorporates the pivotal elements of channel coding and decoding The system model incorporates channel coding and decoding to improve the bit error rate and enhance the overall performance. The numerical results show that the proposed scheme outperforms existing techniques in terms of BER and spectral efficiency, especially in high-mobility scenarios. The proposed system demonstrates significant robustness against channel estimation errors and Doppler spread. This indicates that coded massive MIMO employing OTFS modulation offers a highly effective solution for future wireless communication systems. The findings highlight the potential of this approach to enhance the reliability and performance of next-generation networks.}
}


@article{DBLP:journals/cn/DeduA24,
	author = {Eugen Dedu and
                  Masoud Asghari},
	title = {Ray tracing routing using packet reception timing in dense nanonetworks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110753},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110753},
	doi = {10.1016/J.COMNET.2024.110753},
	timestamp = {Fri, 20 Sep 2024 14:02:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DeduA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a novel routing method called ray tracing. The method targets wireless nanonetworks, which are networks of nanometer-sized nodes with applications in various domains such as future medicine and metamaterial. Due to their tiny size, nanonodes have quite limited resources, in particular energy, and it is crucial to use as few resources as possible. In multi-hop nanonetworks, resource consumption for routing depends on several factors, such as the number of nodes that forward packets, and the number of packets sent and received. To reduce the energy used, in the proposed ray tracing routing, only the nodes that receive duplicate packets at the same time forward them. This leads to the selection of the forwarding nodes over a quasistraight line between the source and the destination, which can bend at the edges of the network. The proposed method is a major improvement over its previous version, aiming to fix several crucial issues such as double propagation, large forwarding angle, and die-out. Moreover, the applicability of ray tracing in heterogeneous and 3D nanonetworks is also analyzed and a practical application for the protocol is proposed. Extensive simulation results demonstrate the fixing of these issues, and the superiority of the proposed improved ray tracing algorithm over other routing methods in the long and narrow nanonetworks, such as blood vessels and branches.}
}


@article{DBLP:journals/cn/LiuJLZSL24,
	author = {Gaosai Liu and
                  Xinglong Jiang and
                  Huawang Li and
                  Zhenhua Zhang and
                  Siyue Sun and
                  Guang Liang},
	title = {Collaborative last-hop scheduling strategy for large-scale {LEO} constellation
                  routing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110760},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110760},
	doi = {10.1016/J.COMNET.2024.110760},
	timestamp = {Fri, 20 Sep 2024 14:02:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuJLZSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The transmission of data packets from satellites to Earth Stations (ESs) is the last hop of satellite network routing. As the final stage of packet transmission, the use of different scheduling strategies directly affects the throughput of the satellite network. Traditionally, researchers have attempted to enhance network performance by investigating inter-satellite routing protocols or inter-satellite data offloading strategies. However, these approaches have failed to address scheduling issues in the last hop of large-scale Low Earth Orbit (LEO) constellations. In this paper, we present the first Cooperative Last-Hop Scheduling (CLHS) strategy for routing in large-scale LEO constellations based on a bidirectional communication domain. In this strategy, we first utilize the bidirectional communication domain to determine the communication ranges of satellites and ESs. Subsequently, an information flow is established to interact with ESs and satellites. The ESs receive and reconstruct the Information Matrix (IM) from the information flow. Moreover, we propose the Maximum Decision Value Priority (MDVP) algorithm, which takes the reconstructed IM as input and computes the scheduling commands for satellites within the communication range of the ESs. To address the issue of multiple ESs simultaneously scheduling the same satellite, we introduce the Collision Avoidance Algorithm (CAA). Finally, to enhance the data packet transmission efficiency of the scheduled satellites, we propose the Weighted First-In-First-Out (WFIFO) algorithm, which is specifically designed for satellite packet dequeuing. We validate the CLHS through simulations on two satellite constellations: the first-generation Starlink constellation with 4409 satellites and the GW-2 constellation with 6912 satellites. The results show that CLHS can achieve better network throughput than traditional strategies. CLHS provides a novel method of scheduling the last hop in large-scale satellite constellations.}
}


@article{DBLP:journals/cn/NguyenHTP24,
	author = {Ba Cao Nguyen and
                  Pham Thanh Hiep and
                  Xuan Nam Tran and
                  Nguyen Thu Phuong},
	title = {Performance enhancement of {NOMA} multi-user networks with aerial
                  reconfigurable intelligent surfaces and aerial relay},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110767},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110767},
	doi = {10.1016/J.COMNET.2024.110767},
	timestamp = {Fri, 20 Sep 2024 14:02:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NguyenHTP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a combination of multiple aerial reconfigurable intelligent surfaces (ARISs) with an aerial relay (AR) for improving the performance of multiple users adopting the non-orthogonal multiple access (NOMA) scheme. In particular, two groups of ARIS are utilized, one for aiding ground-to-air (G2A) communication and another one for aiding air-to-ground (A2G) communication. Mathematical formulas of outage probability (OP), throughput, and achievable capacity (AC) of every user in the proposed ARISs-AR-NOMA network are derived over the Nakagami-\nm\nchannel. The propagation environment recommended for the fifth generation and beyond (5G/B5G) is applied to enhance the practical behavior of the proposed network. To confirm the performance enhancement, we compare the OP, throughput, and AC of the proposed system with the conventional AR-NOMA network (i.e., without ARISs). Numerical illustrations demonstrate the great benefits of the proposed ARISs-AR-NOMA network, its transmit power is dramatically lower than that of the conventional network. Moreover, the throughput and AC of the proposed network are considerably higher than the conventional network’s. Based on the effect of key parameters and network behaviors, various effective solutions are recommended to enhance the performance of the proposed ARISs-AR-NOMA network.}
}


@article{DBLP:journals/cn/CaleffiAFIMC24,
	author = {Marcello Caleffi and
                  Michele Amoretti and
                  Davide Ferrari and
                  Jessica Illiano and
                  Antonio Manzalini and
                  Angela Sara Cacciapuoti},
	title = {Distributed quantum computing: {A} survey},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110672},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110672},
	doi = {10.1016/J.COMNET.2024.110672},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/CaleffiAFIMC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, quantum computing has reached the engineering phase, with fully-functional quantum processors integrating hundreds of noisy qubits. Yet – to fully unveil the potential of quantum computing out of the labs into the business reality – the challenge ahead is to substantially scale the qubit number, reaching orders of magnitude exceeding thousands of fault-tolerant qubits. To this aim, the distributed quantum computing paradigm is recognized as the key solution for scaling the number of qubits. Indeed, accordingly to such a paradigm, multiple small-to-moderate-scale quantum processors communicate and cooperate for executing computational tasks exceeding the computational power of single processing devices. The aim of this survey is to provide the reader with an overview about the main challenges and open problems arising with distributed quantum computing from a computer and communications engineering perspective. Furthermore, this survey provides an easy access and guide towards the relevant literature and the prominent results in the field.}
}


@article{DBLP:journals/cn/ShiRSWYHZHD24,
	author = {Jianzhi Shi and
                  Rou Rao and
                  Yang Song and
                  Xingwei Wang and
                  Bo Yi and
                  Qiang He and
                  Chao Zeng and
                  Min Huang and
                  Sajal K. Das},
	title = {Service recommendation in JointCloud environments: An efficient regret
                  theory-based Qos-aware approach},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110716},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110716},
	doi = {10.1016/J.COMNET.2024.110716},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShiRSWYHZHD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of data-intensive applications, there arises an urgent demand for a substantial amount of cloud services to meet their requirements for data analysis. This globalized yet cooperative business landscape necessitates new cooperative models across the world. JointCloud, as a novel cross-cloud cooperation computing model, takes the first step towards establishing an evolving cloud ecosystem where all cloud service providers could collaboratively serve globalized computation needs. The collaboration among various cloud service providers enhances both the availability and Quality of Services(QoS) of cloud services, enabling a cloud service provider to concurrently serve users with differentiated QoS requirements. This unique characteristic further complicates the problems of QoS-aware service recommendations, rendering conventional approaches obsolete and inefficient. Thus, there is an urgent need to improve the efficiency and effectiveness of the service recommendation method, which is of vital importance for the JointCloud environment. In this paper, we present a two-stage efficient regret theory-based service recommendation method for the JointCloud environment. In the first stage of our proposed method, we cluster the cloud service providers to reduce the choice space to improve the efficiency of cloud service recommendations. In the second stage, we meticulously identify the most appropriate services within one cluster. To enhance the overall rationality of service recommendation, we introduce a subjective and objective combined weighting method and a regret theory based ranking method. Extensive experimental results demonstrate that our approach can facilitate fast and accurate service recommendations.}
}


@article{DBLP:journals/cn/NandeRHBPMRBCFS24,
	author = {Swaraj Shekhar Nande and
                  Tommaso Rossi and
                  Muhammad Idham Habibie and
                  Mohamed Barhoumi and
                  Krishna Palaparthy and
                  Wassim Mansouri and
                  Ashwin Raju and
                  Riccardo Bassoli and
                  Ernestina Cianca and
                  Frank H. P. Fitzek and
                  Mauro De Sanctis},
	title = {Satellite-based positioning enhanced by quantum synchronization},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110734},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110734},
	doi = {10.1016/J.COMNET.2024.110734},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/NandeRHBPMRBCFS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study focuses on the innovative field of quantum synchronization for satellite-based navigation systems including Global Navigation Satellite Systems (GNSSs) and the Non-Terrestrial Network (NTN) component of future 6G networks integrating both communication and navigation services.}
}


@article{DBLP:journals/cn/JeonRL24,
	author = {Yu{-}Ran Jeon and
                  Jung{-}Hwa Ryu and
                  Il{-}Gu Lee},
	title = {{ART:} Adaptive relay transmission for highly reliable communications
                  in next-generation wireless LANs},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110752},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110752},
	doi = {10.1016/J.COMNET.2024.110752},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/JeonRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ongoing evolution of Wi-Fi technology has spurred the development of high-performance and highly efficient wireless networks. The recently introduced Institute of Electrical and Electronics Engineers (IEEE) 802.11bn standard aims to foster next-generation Wi-Fi technology, thereby ensuring ultra-high reliability by significantly enhancing data throughput, reducing latency, and minimizing power consumption. However, despite the expanding market for information technology products equipped with Wi-Fi and the escalating security risks associated with wireless local area networks, current standardization efforts fall short in bolstering security. This study proposes a transmission scheme in which a station discerns the security and latency requirements of packets and executes Adaptive Relay Transmission (ART). When low-latency transmission is imperative, the ART scheme resorts to a direct transmission approach using single-hop transmission. Conversely, when stringent security measures are warranted, it adopts a relay transmission scheme to minimize the risk of eavesdropping by transmitting packets with short hops. In scenarios necessitating both low latency and a high level of security, a transmission scheme is selected based on a comprehensive evaluation of data throughput, energy efficiency, and security, thereby enhancing the network reliability. Experimental results indicate that ART enhances the network energy efficiency by 13.1 times compared to the direct transmission scheme, which consistently employs direct packet transmission in a delay-tolerant network environment. Furthermore, it limits information leakage to the level of the relay packet transmission scheme, which utilizes multiple hops in a high-security threat environment. ART also boosts the network throughput environment by 212.4 % compared to the direct transmission scheme in a densely populated network and maintains link throughput even in scenarios affected by significant interference, thereby demonstrating effective interference mitigation. The experimental results underscore the efficacy of the ART model in mitigating strong interference and addressing security threats in high-density wireless sensor networks.}
}


@article{DBLP:journals/cn/ZhuYHL24,
	author = {Qiang Zhu and
                  Lin You and
                  Gengran Hu and
                  Wei{-}Nan Liu},
	title = {{BIAS:} {A} novel secure and efficient biometric-based anonymous authentication
                  scheme},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110754},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110754},
	doi = {10.1016/J.COMNET.2024.110754},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuYHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, biometric-based authentication schemes are widely adopted in the field of online payments. Consequently, this has led to an increasing number of people becoming concerned about the privacy-preservation of their biometric data. Gunasinghe et al. presented PrivBioMTAuth, a solution for mobile phone biometric-based authentication designed to protect users’ privacy. However, the solution has drawbacks, such as its impact on the execution efficiency of the authentication protocol or its vulnerability to man-in-the-middle attacks during the authentication phase. Moreover, the user’s biometric image and the password must be revealed to the identity provider, which may raise security concerns. In this work, we present a novel secure and efficient biometric-based anonymous authentication solution with fully succinct verification and significantly lower storage and communication overhead. Different from PrivBioMTAuth, we rely on the NIZK argument given in Groth’s work to reduce the size of the anonymous identity and simplify the verification complexity. In addition, we design a high-performance protocol for conducting large-scale verification of the user’s anonymous identities. We propose an optimized multi-exponentiation argument based on Bayer et al.’s work and utilize it to ensure that a semi-trusted identity provider who seeks to access the users’ sensitive biometric information can faithfully execute the users’ identity registration protocol. The experiment results show that our proposed scheme is efficient and has privacy-preserving capabilities, and it can be applied in the resource-constrained devices.}
}


@article{DBLP:journals/cn/ByunYRKCC24,
	author = {Gyurin Byun and
                  Huigyu Yang and
                  Syed M. Raza and
                  Moonseong Kim and
                  Min Young Chung and
                  Hyunseung Choo},
	title = {Generative spatiotemporal image exploitation for datacenter traffic
                  prediction},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110755},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110755},
	doi = {10.1016/J.COMNET.2024.110755},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ByunYRKCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The tremendous growth rate of global internet traffic in past years increases the importance of traffic prediction for network operators to ensure seamless Quality of Service (QoS) with proactive traffic engineering. Even minor anomalies in traffic management can lead to service disruptions that affect a vast user base, necessitating highly accurate traffic predictions. While recent studies have exploited Deep Learning for accurate traffic predictions, most of them have targeted mobile network traffic and they often fall short in delivering precise long-range predictions and effective spatiotemporal feature extraction from single-stream time-series data. This research addresses these limitations by proposing a Convolutional Recurrent Generative Adversarial Network (CoRe-GAN) consisting of generator and discriminator neural networks for high-accuracy traffic prediction. The generator with Convolutional Long Short-Term Memory (ConvLSTM) model effectively captures intricate features, whereas the discriminator utilizes a Convolutional Neural Network (CNN) to train the generator through feedback. Moreover, advanced training techniques like fact forcing and feature matching increase the learning convergence rate, avoid mode collapse, and amplify prediction accuracy of CoRe-GAN. The evaluation with Pangyo Network Dataset (PND) and synthetic Intrusion Detection Dataset (IDD) confirms CoRe-GAN superiority. The results show that it outperforms ConvLSTM models with an average 20% and 16% lower Mean Square Error (MSE) with PND and IDD traffic data, respectively.}
}


@article{DBLP:journals/cn/GongCLJDL24,
	author = {Yiru Gong and
                  Susu Cui and
                  Song Liu and
                  Bo Jiang and
                  Cong Dong and
                  Zhigang Lu},
	title = {Graph-based insider threat detection: {A} survey},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110757},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110757},
	doi = {10.1016/J.COMNET.2024.110757},
	timestamp = {Sat, 14 Dec 2024 21:39:21 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GongCLJDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Insider threat detection has been a significant topic in recent years. However, as network technology develops, the intranet becomes more complex. Therefore, simply matching attack patterns or using traditional machine learning methods (Logistic Regression, Gaussian-NB, Random Forest, etc.) does not work well. On the other hand, the graph structure can better adapt to intranet data, thus graph-based insider threat detection methods have become mainstream. In order to study the design and effectiveness of graph-based insider threat detection, in this paper, we conduct a systematic and comprehensive survey of existing related research. Specifically, we provide a framework and a taxonomy based on the detection process, classifying existing work from three aspects: data collection, graph construction, and graph anomaly detection. We conduct a quantitative analysis of existing representative graph methods and find that the models with more information have better performance. In particular, we discuss the scalability of existing methods to large-scale networks and their feasibility in real environments. Based on the survey results, we propose 7 pain points in this field and provide specific future research directions. Our survey will provide future researchers with a complete solution.}
}


@article{DBLP:journals/cn/GePLWL24,
	author = {Huanhuan Ge and
                  Shiva Raj Pokhrel and
                  Zhenyu Liu and
                  Jinlong Wang and
                  Gang Li},
	title = {{PFL-DKD:} Modeling decoupled knowledge fusion with distillation for
                  improving personalized federated learning},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110758},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110758},
	doi = {10.1016/J.COMNET.2024.110758},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GePLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We develop a novel framework for personalized federated learning (PFL) utilizing a decoupled version of knowledge distillation (DKD). Unlike traditional PFL methods, the proposed PFL-DKD creates a dynamically connected network among local clients and categorizes them according to their knowledge, storage, and computational capabilities. The developed decoupling of knowledge distillation into target class (TC) and latent class (LC) enables knowledge-rich clients to efficiently transfer their expertise to knowledge-poor clients. To further enhance our innovative PFL-DKD approach, we extend it to PFL-FDKD by introducing a ”logit fusion” that seamlessly aggregates knowledge and experiences from neighboring clients. Both our theoretical analyses and extensive experiments reveal that PFL-DKD outperforms existing centralized and decentralized PFL approaches, making significant strides in mitigating the challenges associated with heterogeneous data and system configurations. The details of our implementation with the codebase are in PFL-DKD.}
}


@article{DBLP:journals/cn/ChenYWLW24,
	author = {Yishan Chen and
                  Shumei Ye and
                  Jie Wu and
                  Wei Li and
                  Jiyuan Wang},
	title = {Low-latency intelligent service combination caching strategy with
                  density peak clustering algorithm in vehicle edge computing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110761},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110761},
	doi = {10.1016/J.COMNET.2024.110761},
	timestamp = {Mon, 07 Oct 2024 08:28:28 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenYWLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the dynamic field of Vehicle Edge Computing (VEC), the demand for intelligent vehicular systems to process vast amounts of data is escalating, driven by advancements in autonomous driving and real-time navigation technologies. Optimizing service latency and minimizing transmission costs are crucial for enhancing the performance of vehicular networks. Traditional service caching strategies, which largely rely on the popularity of individual services, often fail to account for the intricate interdependencies between services. The oversight can result in redundant data transfers and inefficient use of storage resources. In response, our paper introduces a novel approach to service combination caching within a heterogeneous computational framework comprising vehicles, edge servers, and the cloud. Our strategy focuses on minimizing user wait times and data transmission costs during task execution, while adhering to the caching budget constraints of service providers. Key contributions include the development of an Improved Density Peak Clustering (IDPC) algorithm to facilitate cooperative clustering among edge servers and the design of a Service Combination Caching Strategy (SCCS). The SCCS approach reduces caching costs by categorizing servers, forming efficient clusters, and strategically allocating storage. Simulation results demonstrate that the method outperforms existing strategies by significantly decreasing task execution delays and transmission costs, thereby greatly enhancing the quality of service in vehicular applications.}
}


@article{DBLP:journals/cn/FrankGCVS24,
	author = {Lucas Rodrigues Frank and
                  Antonino Galletta and
                  Lorenzo Carnevale and
                  Alex Borges Vieira and
                  Edelberto Franco Silva},
	title = {Intelligent resource allocation in wireless networks: Predictive models
                  for efficient access point management},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110762},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110762},
	doi = {10.1016/J.COMNET.2024.110762},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FrankGCVS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the significant increase in mobile users connected to the wireless network, coupled with the escalating energy consumption and the risk of network saturation, the search for resource management has become paramount. Managing several access points throughout a whole region is hugely relevant in this context. Moreover, a wireless network must keep its Service Level Agreement, regardless of the number of connected users. With that in mind, in this work, we propose four prediction models that allow one to predict the number of connected users on a wireless network. Once the number of users has been predicted, the network resources can be properly allocated, minimizing the number of active access points. We investigate the use of Particle Swarm Optimization and Genetic Algorithms to hyper-parameterize a Multilayer Perceptron neural network and a Decision Tree. We evaluate our proposal using a campus-based wireless network dataset with more than 20,000 connected users. As a result, our model can considerably improve network performance by intelligently allocating the number of access points, thereby addressing concerns related to energy consumption and network saturation. The results have shown an average accuracy of 95.18%, managing to save network resources effectively.}
}


@article{DBLP:journals/cn/TodiscoCMBSB24,
	author = {Vittorio Todisco and
                  Claudia Campolo and
                  Antonella Molinaro and
                  Antoine O. Berthet and
                  Richard A. Stirling{-}Gallacher and
                  Alessandro Bazzi},
	title = {Full duplex based collision detection to enhance the {V2X} sidelink
                  autonomous mode},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110763},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110763},
	doi = {10.1016/J.COMNET.2024.110763},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TodiscoCMBSB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Third Generation Partnership Project (3GPP) recently introduced the fifth-generation (5G) new radio (NR) sidelink to enable vehicle-to-everything (V2X) communications supporting advanced safety services. Nevertheless, improvements over the previous generation still pose challenges to meet the reliability and latency requirements of V2X communications, particularly in the allocation of distributed resources, i.e., Mode 2. In Mode 2, vehicles autonomously select radio resources for their message transmissions and can maintain the selected resources for a given reservation period to efficiently handle periodic data traffic. However, potential collisions during this period may remain undetected due to half-duplex communications and unacknowledged broadcast transmissions, resulting in persistent message losses and posing a threat to road safety. This paper aims to improve the 5G NR-V2X sidelink for systems beyond 5G-Advanced by exploiting full-duplex transceivers. We propose a novel medium access control (MAC) scheme where vehicles can detect collisions while transmitting, dynamically adapt the collision detection threshold according to the measured channel load, and react to detected collisions through appropriate resource reselection and retransmission procedures. Extensive simulations conducted under various settings show that this MAC scheme brings substantial performance gains in terms of reliability and latency, compared to the current legacy Mode 2 procedure and a benchmark full-duplex scheme from the literature.}
}


@article{DBLP:journals/cn/HuangZWXYTWZYADC24,
	author = {Chengyuan Huang and
                  Tianfan Zhang and
                  Li Wang and
                  Yibo Xiao and
                  Chao Yang and
                  Chen Tian and
                  Xiaoliang Wang and
                  Dong Zhang and
                  Bingheng Yan and
                  Ahmed M. Abdelmoniem and
                  Wanchun Dou and
                  Guihai Chen},
	title = {MpScope: Enabling multi-pipeline monitoring inside a switch},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110764},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110764},
	doi = {10.1016/J.COMNET.2024.110764},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuangZWXYTWZYADC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The core of programmable switches is the flexible data plane, composed of multiple programmable pipelines in existing programmable switches. These pipelines are isolated from each other and cannot share state and data. However, most of network monitoring systems ignore this condition and implicitly assume that the switch has only a single pipeline. This results in an inaccurate measurement and high communication overhead with the practical switch. To tackle this problem, we propose MpScope, a general multi-pipeline monitoring framework, which centers around the control plane, supporting accurate and efficient network monitoring. Specifically, MpScope combines the switch’s data plane and control plane to achieve comprehensive network monitoring of the whole switch scope. The control plane aggregates the statistical results from multiple pipelines and tunes the monitoring module residing in the different pipelines in the data plane dynamically. The data plane is responsible for real-time traffic measurement and statistic reports. Its behaviors can be adjusted periodically with the instructions from the control plane. Two typical monitoring applications, i.e., heavy hitter detection and distinct counting, are developed with MpScope to validate the effectiveness of multi-pipeline monitoring. Experiments show that MpScope significantly reduces communication overhead compared to the static threshold scheme while maintaining high detection accuracy over time.}
}


@article{DBLP:journals/cn/BaigCVF24,
	author = {Roger Baig and
                  Lloren{\c{c}} Cerd{\`{a}}{-}Alabern and
                  Pedro V{\'{\i}}lchez and
                  Efra{\'{\i}}n Foglia},
	title = {eXO: {A} commons-based community network Internet Service Provider,
                  description and subscribers usage patterns},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110765},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110765},
	doi = {10.1016/J.COMNET.2024.110765},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BaigCVF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of Wi-Fi in the 1990s stimulated the creation of Community networks (CNs), a new networking paradigm deployed and maintained by their users. The non-profit and grassroots self-organization of CNs has shown significant potential in enhancing communication technologies grounded in ethical values, transcending mere Internet expansion to contribute to broader social impacts. Since its conception, hundreds of CNs have been deployed worldwide. However, many have faltered, primarily due to a lack of solid economic models and legal setup. This paper presents Expansió Xarxa Oberta (eXO) association, a commons Internet Service Provider (ISP). In contrast to commercial ISPs, eXO follows the same grassroots principles of CNs. Establishing a commons-based ISP is challenging, yet it is a key component for the sustainability and success of CNs. Therefore, our paper aims to dissect the complexity of a commons ISP by explaining eXO, a pioneering example that has already been in production for more than 14 years. To the best of our knowledge, this is the first time that a commons ISP is described in all its components.}
}


@article{DBLP:journals/cn/LiuWLC24,
	author = {Xiao Liu and
                  Zhenyang Wei and
                  Gaoxiang Li and
                  Jining Chen},
	title = {An enhanced traceable access control scheme based on multi-authority
                  {CP-ABE} for cloud-assisted e-health system},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110766},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110766},
	doi = {10.1016/J.COMNET.2024.110766},
	timestamp = {Mon, 04 Nov 2024 22:21:28 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiuWLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the inherent openness of the Internet of Things (IoT), e-health systems that utilize this technology on a broad scale are susceptible to network attacks. The propose black and white box traceable multi-authority ciphertext policy attribute-based encryption scheme (BWMABE) that combines black and white box accountability with ciphertext policy attribute-based encryption (CP-ABE) to improve data security in e-health systems. The proposed BWMABE scheme enhances security by enabling traceable permissions leakage and employs distributed attribute and key management across multiple authorities. By outsourcing the decryption process to cloud service providers, the proposed BWMABE significantly reduces the computational burden on end-users, making it suitable for lightweight IoT devices with limited computing power. Compared to previous traceable CP-ABE methods, the proposed BWMABE requires less computational effort while supporting both black box and white box traceability. By means of thorough security analysis and performance assessment, the proposed BWMABE has been validated to provide robust security and high efficiency, demonstrating its effectiveness in protecting e-health systems.}
}


@article{DBLP:journals/cn/SameeraPAC24,
	author = {K. M. Sameera and
                  Vinod P. and
                  Rafidha Rehiman K. A. and
                  Mauro Conti},
	title = {LFGurad: {A} Defense against Label Flipping Attack in Federated Learning
                  for Vehicular Network},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110768},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110768},
	doi = {10.1016/J.COMNET.2024.110768},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SameeraPAC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosive growth of the interconnected vehicle network creates vast amounts of data within individual vehicles, offering exciting opportunities to develop advanced applications. FL (Federated Learning) is a game-changer for vehicular networks, enabling powerful distributed data processing across vehicles to build intelligent applications while promoting collaborative training and safeguarding data privacy. However, recent research has exposed a critical vulnerability in FL: poisoning attacks, where malicious actors can manipulate data, labels, or models to subvert the system. Despite its advantages, deploying FL in dynamic vehicular environments with a multitude of distributed vehicles presents unique challenges. One such challenge is the potential for a significant number of malicious actors to tamper with data. We propose a hierarchical FL framework for vehicular networks to address these challenges, promising lower latency and coverage. We also present a defense mechanism, LFGuard, which employs a detection system to pinpoint malicious vehicles. It then excludes their local models from the aggregation stage, significantly reducing their influence on the final outcome. We evaluate LFGuard against state-of-the-art techniques using the three popular benchmark datasets in a heterogeneous environment. Results illustrate LFGuard outperforms prior studies in thwarting targeted label-flipping attacks with more than 5% improvement in the global model accuracy, 12% in the source class recall, and a 6% reduction in the attack success rate while maintaining high model utility.}
}


@article{DBLP:journals/cn/GuoHGG24,
	author = {Wenjing Guo and
                  Yingao Hou and
                  Yanglan Gan and
                  Wenli Guo},
	title = {Efficient data transmission mechanisms in energy harvesting wireless
                  body area networks: {A} survey},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110769},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110769},
	doi = {10.1016/J.COMNET.2024.110769},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuoHGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Limited energy and reliable data transmission are two key issues in Wireless body area networks (WBANs). The utilization of energy harvesting technology has alleviated the energy problem in WBANs, making continuous operation possible. However, Energy Harvesting WBANs (EH-WBANs) face new challenges. How to design efficient data transmission mechanisms taking into account the unstable energy harvesting conditions and dynamic network topology has become crucial. The efficiency of data transmission mainly depends on the network layer and media access control (MAC) layer. Therefore, this paper surveys the routing and MAC protocols proposed for EH-WBANs. There are some surveys on routing and MAC protocols for traditional battery-powered WBANs. However, these mechanisms cannot be directly applied to EH-WBANs due to the randomness and time-varying nature of the energy obtained by energy harvesting, which differs from the energy characteristics of nodes powered solely by batteries. In addition, due to the dynamic network topology and heterogeneous nodes in WBANs, the research results on routing and MAC protocols for Energy Harvesting Wireless Sensor Networks (EH-WSNs) cannot be directly applied to EH-WBANs. Thus, unlike previous surveys, this paper focuses on protocols specifically designed for EH-WBANs. It introduces and analyzes these protocols, summarizes the comprehensive performance metrics and efficient measures for data transmission mechanisms in EH-WBANs, and conducts a comprehensive performance analysis on the protocols proposed for EH-WBANs based on these metrics. This paper intends to provide assistance in addressing the energy and reliable data transmission issues in WBANs, thereby enhancing the applicability of EH-WBANs.}
}


@article{DBLP:journals/cn/HuLLCLW24,
	author = {Xiaoyan Hu and
                  Di Li and
                  Miao Li and
                  Guang Cheng and
                  Ruidong Li and
                  Hua Wu},
	title = {AHDom: Algorithmically generated domain detection using attribute
                  heterogeneous graph neural network},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110770},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110770},
	doi = {10.1016/J.COMNET.2024.110770},
	timestamp = {Mon, 07 Oct 2024 08:28:29 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuLLCLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many cyber-attacks use Algorithmically Generated Domain (AGD) names to establish connections with command and control servers for subsequent attack behaviors. Identifying and blocking such AGDs helps detect and prevent attacks quickly. Traditional machine or deep learning detection methods rely only on individual domain features and face challenges in accurately distinguishing AGDs that attackers have crafted to evade detection. Thus, researchers leverage the inherent associated features among domains, clients, and resolved IP addresses to detect AGDs. In such research, heterogeneous graph neural networks are extensively employed. However, most existing methods rely on associated features, leading to inaccurate detection of isolated domain nodes. Besides, most existing detection methods employ transductive learning and are time-consuming. This paper proposes an AGD detection method, AHDom, to address these challenges. AHDom models DNS traffic as a Heterogeneous Information Network (HIN) to capture the intricate relationships between domains, clients, and resolved IP addresses. Besides, it extracts character and behavior features as initial attributes of domains to obtain an Attribute HIN (AHIN), enhancing the detection accuracy of isolated domain nodes. Based on the AHIN, it combines meta-path random walk, the inductive learning algorithm GraphSAGE, and the attention mechanism to obtain effective embedding representations of domain nodes. Ultimately, it achieves domain classification based on embedding representations of domain nodes. Our experimental results demonstrate that AHDom is superior to state-of-the-art methods in the performance and efficiency of detecting AGDs. AHDom achieves an average accuracy of 98.74% on our constructed dataset and costs only about 30.23% of the existing best graph neural network approach in the testing time.}
}


@article{DBLP:journals/cn/LiWSLZF24,
	author = {Yafei Li and
                  Huiqiang Wang and
                  Jiayu Sun and
                  Hongwu Lv and
                  Wenqi Zheng and
                  Guangsheng Feng},
	title = {Two-timescale joint service caching and resource allocation for task
                  offloading with edge-cloud cooperation},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110771},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110771},
	doi = {10.1016/J.COMNET.2024.110771},
	timestamp = {Wed, 25 Sep 2024 21:46:07 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiWSLZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task offloading with edge–cloud cooperation has emerged as a pivotal solution for meeting the intricate array of application coupled with dynamically evolving business demand in 6G business scenarios, such as traffic sensing, environmental monitoring, and video surveillance in smart cities. Nonetheless, effectively leveraging heterogeneous edge–cloud network resources for effective task offloading presents substantial challenges. Additionally, the inherent differences in system decision cycles escalate the complexity of the task offloading problem to a new dimension. In this study, we delve into a two-timescale joint service caching and resource allocation optimization for task offloading within edge–cloud cooperation aiming to maximize long-term network performance while adhering to energy constraints. We propose a novel edge–cloud cooperation task offloading scheme that supports both edge–cloud and edge–edge cooperation to effectively balance the edge–cloud and edge–edge loads, promoting the efficient co-utilization of all edge–cloud system resources. Furthermore, we devise an online two-timescale Lyapunov-based joint optimization framework for service caching, task offloading, and computing resource allocation. Our two-timescale decision-making framework can flexibly accommodate the inherent differences in the sensitive decision optimization periods, thereby mitigating the degradation of task offloading performance caused by frequent service caching updates. Finally, theoretical analysis confirms that our proposed algorithm can converge to an approximate optimal solution in polynomial time, and the superiority of our scheme is validated by extensive simulation experiments.}
}


@article{DBLP:journals/cn/MehjabinYTESK24,
	author = {Suhee Sanjana Mehjabin and
                  Mohamed F. Younis and
                  Ali Tekeoglu and
                  Mohammad Ebrahimabadi and
                  Tamim Sookoor and
                  Naghmeh Karimi},
	title = {{PETIT:} PUF-enabled trust evaluation framework for IoT networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110772},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110772},
	doi = {10.1016/J.COMNET.2024.110772},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MehjabinYTESK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet-of-Things (IoT) is characterized by the incorporation of resource constrained devices that are inter-networked in an ad-hoc manner. Given the diversity of the devices and the operating conditions, it is important to assess the trustworthiness of IoT nodes and factor it in the network management. Contemporary trust evaluation and management schemes found in the literature mostly consider observable network-level behavior parameters and initially assume that all nodes are equally trustworthy owing to the absence of historical data or background. Such an equal trust initialization approach raises concerns in terms of accuracy, fairness, and adaptability. This paper aims to mitigate these shortcomings by proposing a novel trust evaluation and aggregation framework. Our framework leverages hardware primitives such as Physical Unclonable Functions (PUFs) to assign trust scores at the network bootstrapping phase. The paper explores the establishment of both direct and recommendation based indirect trust score evaluation and detection of irregularities to ensure the dynamic, safe, and reliable operation of the network. Simulation outcomes demonstrate that the trust value computed through this mechanism effectively and precisely mirrors the node’s credibility.}
}


@article{DBLP:journals/cn/TianJTL24,
	author = {Junfeng Tian and
                  Caishi Jing and
                  Jin Tian and
                  Yaze Li},
	title = {PartChain: Scaling blockchain through account-based partitioned sharding},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110773},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110773},
	doi = {10.1016/J.COMNET.2024.110773},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TianJTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sharding technology, as one of the mainstream methods for overcoming the scalability bottlenecks in blockchain, divides the nodes of the entire blockchain network into several subsets to process multiple sets of transactions in parallel. However, as the number of shards continues to increase, the number of cross-shard transactions also increases. The proportion of cross-shard transactions may become very high, leading to increased communication costs, higher time expenses, and reduced transaction throughput in the sharded system.}
}


@article{DBLP:journals/cn/LiWFFL24,
	author = {Ying Li and
                  Jiuqi Wei and
                  Ziyu Fei and
                  Yufan Fu and
                  Xiaodong Lee},
	title = {DiSAuth: {A} DNS-based secure authorization framework for protecting
                  data decoupled from applications},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110774},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110774},
	doi = {10.1016/J.COMNET.2024.110774},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiWFFL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, centralized applications are called for returning data ownership to owners, due to their frequent data breaches. Decoupling data from applications is a popular way to give back owners’ control over data. To control data only accessed by authorized parties, authorization is critical. When faced with the new and complex relationships between applications and data, traditional authorizations cannot meet ownership and usage protection needs, and security requirements. This paper proposes DiSAuth, a secure authorization framework based on Domain Name System (DNS) and blockchain, to provide tamper-resistant, verifiable, and privacy-preserving authorization to protect data which are decoupled from applications. Our novel tree-based data structure for authorization is backward compatible with DNS, which brings high utility. Besides, our design of a hybrid encryption schema and anonymous identities provides privacy-preserving authorization. To our knowledge, DiSAuth is the first authorization framework that utilizes a robust Internet infrastructure DNS as the basis and proposes a new Internet authorization protocol for protecting data decoupled from applications. Our evaluation demonstrates the utility of DiSAuth and shows the superior efficiency of DiSAuth in authorization verification. Compared to traditional blockchain-based solutions, our combination of DNS and blockchain achieves higher efficiency while ensuring security; especially, the read time is\n∼\n3 orders of magnitude better.}
}


@article{DBLP:journals/cn/ForuhandehYCSWY24,
	author = {Mahsa Foruhandeh and
                  Hanchao Yang and
                  Xiang Cheng and
                  Angelos Stavrou and
                  Haining Wang and
                  Yaling Yang},
	title = {All in one: Improving {GPS} accuracy and security via crowdsourcing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110775},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110775},
	doi = {10.1016/J.COMNET.2024.110775},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ForuhandehYCSWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {GPS is an integral part of billions of devices that serve a wide range of applications. This reliance upon GPS renders the users vulnerable to GPS spoofing attacks, especially when in need of precise or real-time location information. To protect commodity devices, we first propose a crowdsourcing-based method for detecting GPS spoofing. In this method, called method I, we leverage the orientation diversity of different users to expose spoofing attacks and, in many cases, the location of the attacker. In all scenarios, our method not only recovers the correct location but also significantly improves the location accuracy. This is an important incentive that can drive the adoption of our approach along with the use of privacy-preserving location sharing. Additionally, we leverage the users’ distances produced by GPS and Bluetooth measurements to detect discrepancies and account for errors, called Method II. Method II is robust even in the presence of multiple coordinate adversaries. The experimental results based on our prototype implementation and large-scale simulations demonstrate a detection rate as high as 98.72 % and latency of 62 ms with average localization error of 2.43 m.}
}


@article{DBLP:journals/cn/LuLPWNL24,
	author = {Qiuyu Lu and
                  Jun'e Li and
                  Zhao Peng and
                  Libing Wu and
                  Ming Ni and
                  Jianbo Luo},
	title = {Detecting the cyber-physical-social cooperated APTs in high-DER-penetrated
                  smart grids: Threats, current work and challenges},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110776},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110776},
	doi = {10.1016/J.COMNET.2024.110776},
	timestamp = {Wed, 25 Sep 2024 21:46:07 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuLPWNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale renewable distributed energy sources (DERs) penetrating into smart grids (SGs) is an inevitable trend. Such high-DER-penetrated SGs entail heavy reliance on information and communication technologies and increasing impact of social behaviors on system operation and management. In this sense, the SGs become cyber-physical-social systems. However, the deeply coupling of cyber networks, physical grids, and societies leads SGs more complex and openness, and therefore a higher possibility of facing to various threats, especially advanced persistent threats (APTs) that disrupt system operations at a large scale. To better study the threats, current APTs detection work and challenges of the SGs, we first analyze the key features of high-DER-penetrated SGs, and the vulnerabilities of devices, networks, and applications in the SGs introduced by system design, limitation of deployed security measures, and social behaviors. On this basis, we analyze APTs faced by the SGs and deem that the APTs are in the form of cyber-physical-social cooperated and multi-stage APTs. The possible attacking methods for each stage of the APTs, typically stealthy attacks at the early stages and coordinated attacks at the action stage, are also summarized. Thereafter, a review of current work on security architectures for APT detection and intelligent intrusion detection methods is provided. Finally, we discuss the key challenges, research needs, and potential solutions of future work for the SGs against the APTs from the aspects of threat modeling, threat detection, threat hunting, and implementation technology.}
}


@article{DBLP:journals/cn/FuLWLP24,
	author = {Yufan Fu and
                  Xiaodong Lee and
                  Jiuqi Wei and
                  Ying Li and
                  Botao Peng},
	title = {Securing the internet's backbone: {A} blockchain-based and incentive-driven
                  architecture for {DNS} cache poisoning defense},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110777},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110777},
	doi = {10.1016/J.COMNET.2024.110777},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FuLWLP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain Name System (DNS) is the backbone of the Internet infrastructure, converting human-friendly domain names into machine-processable IP addresses. However, DNS remains vulnerable to various security threats, such as cache poisoning attacks, where malicious attackers inject false information into DNS resolvers’ caches. Although efforts have been made to enhance DNS against such vulnerabilities, existing countermeasures often fall short in one or more areas: they may offer limited resistance to the collusion attack, introduce significant overhead, or require complex implementation that hinders widespread adoption. To address these challenges, this paper introduces TI-DNS+, a trusted and incentivized blockchain-based DNS resolution architecture for cache poisoning defense. TI-DNS+ introduces a Verification Cache exploiting blockchain ledger’s immutable nature to detect and correct forged DNS responses. The architecture also incorporates a multi-resolver Query Vote mechanism, enhancing the ledger’s credibility by validating each record modification through a stake-weighted algorithm. This algorithm selects resolvers as validators based on their stake proportion. To promote well-behaved participation, TI-DNS+ also implements a novel stake-based incentive mechanism that optimizes the generation and distribution of stake rewards. This ensures that incentives align with participants’ contributions, achieving incentive compatibility, fairness, and efficiency. Moreover, TI-DNS+ possesses high practicability as it requires only resolver-side modifications to current DNS. Finally, through comprehensive prototyping and experimental evaluations, the results demonstrate that our solution effectively mitigates DNS cache poisoning. Compared to competitors, our solution improves attack resistance by 1-3 orders of magnitude, while also reducing resolution latency by 5% to 68%.}
}


@article{DBLP:journals/cn/PanLFS24,
	author = {Guoqing Pan and
                  Luyu Li and
                  Rong Fan and
                  Yishan Su},
	title = {A joint helper selection and power allocation scheme for secure underwater
                  acoustic transmission},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110779},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110779},
	doi = {10.1016/J.COMNET.2024.110779},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PanLFS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater acoustic sensor networks (UASNs) are usually deployed in open environments. Due to the broadcast characteristic of acoustic signals, data transmission in the network is vulnerable to eavesdropping risks from potential eavesdroppers. In this paper, we propose a joint helper selection and power allocation scheme (HSPAS) for acoustic cooperative jamming. We construct a joint multi-node jamming model and decompose the original optimization problem into two sub-problems of helper selection and jamming power allocation. On the basis of exploring the influence of node position on jamming effect, we implement the selection of helpers and propose a distributed algorithm to achieve the efficient allocation of jamming power, obtaining a feasible optimal solution with low complexity. Both simulation results and field experiments prove that the proposed scheme can effectively reduce the eavesdropping risk and improve the security of data transmission in the network.}
}


@article{DBLP:journals/cn/SotoCGMGKSVNFBLCPACGF24,
	author = {Paola Soto and
                  Miguel Camelo and
                  Gines Garcia{-}Aviles and
                  Esteban Municio and
                  Marco Gramaglia and
                  Evangelos A. Kosmatos and
                  Nina Slamnik{-}Krijestorac and
                  Danny De Vleeschauwer and
                  Antonio Bazco Nogueras and
                  Lidia Fuentes and
                  Joaqu{\'{\i}}n Ballesteros and
                  Andra Lutu and
                  Luca Cominardi and
                  Ivan Paez and
                  Sergi Alcal{\'{a}}{-}Mar{\'{\i}}n and
                  Livia Elena Chatzieleftheriou and
                  Andres Garcia{-}Saavedra and
                  Marco Fiore},
	title = {Designing the Network Intelligence Stratum for 6G networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110780},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110780},
	doi = {10.1016/J.COMNET.2024.110780},
	timestamp = {Mon, 03 Mar 2025 21:30:46 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SotoCGMGKSVNFBLCPACGF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security. A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response. These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance. They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI). Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach. However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective. This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum). This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains. The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management. We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments. The paper also outlines major challenges and open issues in deploying and managing NI.}
}


@article{DBLP:journals/cn/DasKGPBF24,
	author = {Siddharth Das and
                  Stefan Krause and
                  Kay{-}Uwe Giering and
                  Ricardo J. B. Pousa and
                  Riccardo Bassoli and
                  Frank H. P. Fitzek},
	title = {Leveraging quantum uncertainty: Quantum randomness through the lens
                  of classical communication networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110781},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110781},
	doi = {10.1016/J.COMNET.2024.110781},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DasKGPBF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The generation of random numbers and the study of its properties have been an elusive field for a fair portion of the century. The application of random numbers is employed in many use cases such as cryptography, neural networks, numerical simulation, and gambling. The performance of each of these use cases is profoundly impacted by the employed random numbers; henceforth, the quality of randomness is of critical importance when it comes to their usage. A poorly generated random number can make a security system vulnerable, or any numerical or statistical evaluation misleading. Although various modes of classical random number generators exist and still function to provide strong cryptographic properties, emergence of quantum mechanical randomness has shed light on a novel path of generating certified randomness which supersedes the classical counterpart in terms of security. Harnessing quantum mechanical phenomena enables generation of true random numbers which can be certified and further implemented to elevate the net quality of the specific use cases. In this work, we generate and analyze random numbers from three different sources — 50: 50 beam splitter (BS), quantum key distribution (QKD) setup with classical post-processing scheme, and a commercially available quantum random number generator (QRNG) (ID Quantique (IDQ)). The quality of the generated random numbers from the various sources is checked in statistical tests and compared. Further on, we have developed a system which implements the QRNG-based random numbers to facilitate message authentication code (MAC) and one time password (OTP) protocols, demonstrating a communication network application. In this manner we discuss about a network which integrates quantum mechanics to the current classical networking approaches to enhance certain aspects of the networking protocol — in this case, the security.}
}


@article{DBLP:journals/cn/ShamsoshoaraLMAG24,
	author = {Alireza Shamsoshoara and
                  Fatemeh Lotfi and
                  Sajad Mousavi and
                  Fatemeh Afghah and
                  Ismail G{\"{u}}ven{\c{c}}},
	title = {Joint path planning and power allocation of a cellular-connected {UAV}
                  using apprenticeship learning via deep inverse reinforcement learning},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110789},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110789},
	doi = {10.1016/J.COMNET.2024.110789},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ShamsoshoaraLMAG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates an interference-aware joint path planning and power allocation mechanism for a cellular-connected unmanned aerial vehicle (UAV) in a sparse suburban environment. The UAV’s goal is to fly from an initial point and reach a destination point by moving along the cells to guarantee the required quality of service (QoS). In particular, the UAV aims to maximize its uplink throughput and minimize interference to the ground user equipment (UEs) connected to neighboring cellular base stations (BSs), considering both the shortest path and limitations on flight resources. Expert knowledge is used to experience the scenario and define the desired behavior for the sake of the agent (i.e., UAV) training. To solve the problem, an apprenticeship learning method is utilized via inverse reinforcement learning (IRL) based on both Q-learning and deep reinforcement learning (DRL). The performance of this method is compared to learning from a demonstration technique called behavioral cloning (BC) using a supervised learning approach. Simulation and numerical results show that the proposed approach can achieve expert-level performance. We also demonstrate that, unlike the BC technique, the performance of our proposed approach does not degrade in unseen situations.}
}


@article{DBLP:journals/cn/DongTAHKRB24,
	author = {Shi Dong and
                  Junxiao Tang and
                  Khushnood Abbas and
                  Ruizhe Hou and
                  Joarder Kamruzzaman and
                  Leszek Rutkowski and
                  Rajkumar Buyya},
	title = {Task offloading strategies for mobile edge computing: {A} survey},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110791},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110791},
	doi = {10.1016/J.COMNET.2024.110791},
	timestamp = {Wed, 25 Sep 2024 21:46:07 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DongTAHKRB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide adoption of 5G technology and the rapid development of 6G technology, a variety of new applications have emerged. A multitude of compute-intensive and time-sensitive applications deployed on terminal equipment have placed increased demands on Internet delay and bandwidth. Mobile Edge Computing (MEC) can effectively mitigate the issues of long transmission times, high energy consumption, and data insecurity. Task offloading, as a key technology within MEC, has become a prominent research focus in this field. This paper presents a comprehensive review of the current research progress in MEC task offloading. Firstly, it introduces the fundamental concepts, application scenarios, and related technologies of MEC. Secondly, it categorizes offloading decisions into five aspects: reducing delay, minimizing energy consumption, balancing energy consumption and delay, enabling high-computing offloading, and addressing different application scenarios. It then critically analyzes and compares existing research efforts in these areas.}
}


@article{DBLP:journals/cn/WangWCHR24,
	author = {Suyue Wang and
                  Hua Wu and
                  Guang Cheng and
                  Xiaoyan Hu and
                  Jing Ren},
	title = {{SD-MDN-TM:} {A} traceback and mitigation integrated mechanism against
                  DDoS attacks with {IP} spoofing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110793},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110793},
	doi = {10.1016/J.COMNET.2024.110793},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangWCHR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traceback has been very attractive against DDoS attacks with IP spoofing instead of traditional mitigation methods because attacks require removal near attackers, resulting in affecting legitimate traffic as little as possible. There have been some approaches dedicated to achieving effective traceback. However, existing approaches often modify protocols to apply to multi-domain scenarios, and the implemented mitigation usually lags behind traceback. Therefore, this paper proposes the Software-Defined Multi-Domain Network Tracer and Mitigator (SD-MDN-TM) to traceback and mitigate DDoS attacks with IP spoofing in multi-domain SDN scenarios. We apply systematic sampling and flow feature extraction based on the Count-Min Sketch data structure for the lightweight statistics collection of massive DDoS attack traffic. We also design the TracebackTree data structure to construct the traceback paths of attackers of distributed attack sources. The Border Switch Trigger Mechanism is proposed to overcome the drawbacks of the commonly-used packet marking in cross-domain traceback information transfer, achieving no modification of existing network protocols and independent traceback among multiple domains. Mitigation is integrated with traceback for faster removal of attacks from the network. The proposed scheme can traceback DDoS attack sources both inside and outside domains accurately and effectively by constructing traceback path. It can be implemented without modifying the existing protocols, therefore achieving direct application to the existing network architecture. Furthermore, the traceback of attack sources outside domains can maintain independence in multi-domain scenarios. Mitigation integrated with traceback can achieve less impact on legitimate traffic and faster removal of attacks from the network.}
}


@article{DBLP:journals/cn/NakamuraK24,
	author = {Ryo Nakamura and
                  Noriaki Kamiyama},
	title = {On the aggregation of FIBs at {ICN} routers using routing strategy},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110794},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110794},
	doi = {10.1016/J.COMNET.2024.110794},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NakamuraK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Utilizing in-network caching is essential for the current communication network. In the last decade, ICN (Information-Centric Networking) has been under the spotlight as a network that mainly focuses on transmitted and received data rather than on hosts that transmit and receive data. In ICNs, to appropriately forward request packets, a router maintains a routing table called FIB (Forwarding Information Base). However, it is unsuitable for us to assume that FIB can store entries of all contents within a network. This is mainly because the FIB memory is not large enough to store the prefix of all contents. Thus, for realizing global-scale ICNs, it is crucial to develop an effective technique to reduce the size of FIB. In this paper, to tackle the reduction in the FIB size with the aggregation, we propose a routing strategy called constrained shortest-path tree (CSPT) routing. The fundamental idea of our CSPT routing is to combine shortest-paths on the network and that on a shortest-path tree of the network, which is intended to enhance the effect of FIB aggregation. Furthermore, we extensively investigate the relationship, i.e., trade-off, between the FIB aggregation and the communication performance of ICN using CSPT routing. Consequently, we reveal that our CSPT routing can dramatically reduce the number of FIB entries while suppressing the increase in the number of hops required to deliver request packets.}
}


@article{DBLP:journals/cn/SripotchanartSCCQ24,
	author = {Romtham Sripotchanart and
                  Weisheng Si and
                  Rodrigo N. Calheiros and
                  Qing Cao and
                  Tie Qiu},
	title = {A two-step linear programming approach for repeater placement in large-scale
                  quantum networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110795},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110795},
	doi = {10.1016/J.COMNET.2024.110795},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SripotchanartSCCQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Thanks to the applications such as Quantum Key Distribution and Distributed Quantum Computing, the deployment of quantum networks is gaining great momentum. A major component in quantum networks is repeaters, which are essential for reducing the error rate of qubit transmission for long-distance links. However, repeaters are expensive devices, so minimizing the number of repeaters placed in a quantum network while satisfying performance requirements becomes an important problem. Existing solutions typically solve this problem optimally by formulating an Integer Linear Program (ILP). However, the number of variables in their ILPs is\nO\n(\nn\n2\n)\n, where\nn\nis the number of nodes in a network. This incurs infeasible running time when the network scale is large. To overcome this drawback, this paper proposes to solve the repeater placement problem by two steps, with each step using a linear program of a much smaller scale with\nO\n(\nn\n)\nvariables. Although this solution is not optimal, it dramatically reduces the time complexity, making it practical for large-scale networks. Moreover, it constructs networks that have higher node connectivity than those by existing solutions, since it deploys slightly more number of repeaters into networks. Our extensive experiments on both synthetic and real-world network topologies verified our claims.}
}


@article{DBLP:journals/cn/LiuLL24,
	author = {Xi Liu and
                  Jun Liu and
                  Weidong Li},
	title = {Truthful mechanism for joint resource allocation and task offloading
                  in mobile edge computing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110796},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110796},
	doi = {10.1016/J.COMNET.2024.110796},
	timestamp = {Mon, 07 Oct 2024 08:28:30 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of mobile edge computing (MEC), the delay-sensitive tasks can achieve real-time data processing and analysis by offloading to the MEC servers. The objective is maximizing social welfare in an auction-based model. However, the distances between mobile devices and access points lead to differences in energy consumption. Unfortunately, existing works have not considered both maximizing social welfare and minimizing energy consumption. Motivated by this, we address the problem of joint resource allocation and task offloading in MEC, with heterogeneous MEC servers providing multiple types of resources for mobile devices (MDs) to perform tasks remotely. We split the problem into two sub-problems: winner determination and offloading decision. The first sub-problem determines winners granted the ability to offload tasks to maximize social welfare. The second sub-problem determines how to offload tasks among the MEC servers to minimize energy consumption. In the winner determination problem, we propose a truthful algorithm that drives the system into equilibrium. We then show the approximate ratios for single and multiple MEC servers. In the offloading decision problem, we propose an approximation algorithm. We then show it is a polynomial-time approximation scheme for a single MEC server. Experiment results show that our proposed mechanism finds high-quality solutions in changing mobile environments.}
}


@article{DBLP:journals/cn/LiSWLWJX24,
	author = {Tun Li and
                  Peng Shou and
                  Xin Wan and
                  Qian Li and
                  Rong Wang and
                  Chaolong Jia and
                  Yunpeng Xiao},
	title = {A fast malware detection model based on heterogeneous graph similarity
                  search},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110799},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110799},
	doi = {10.1016/J.COMNET.2024.110799},
	timestamp = {Wed, 09 Oct 2024 23:03:46 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiSWLWJX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Android operating system has long been vulnerable to malicious software. Existing malware detection methods often fail to identify ever-evolving malware and are slow in detection. To address this, we propose a new model for rapid Android malware detection, which constructs various Android entities and relationships into a heterogeneous graph. Firstly, to address the semantic fusion problem in high-order heterogeneous graphs that arises with the increase in the depth of the heterogeneous graph model, we introduce adaptive weights during node aggregation to absorb the local semantics of nodes. This allows more attention to be paid to the feature information of the node itself during the semantic aggregation stage, thereby avoiding semantic confusion. Secondly, to mitigate the high time costs associated with detecting unknown applications, we employ an incremental similarity search model. This model quickly measures the similarity between unknown applications and those within the sample, aggregating the weights of nodes based on similarity scores and semantic attention coefficients, thereby enabling rapid detection. Lastly, considering the high time and space complexity of calculating node similarity scores on large graphs, we design a NeuSim model based on an encoder–decoder structure. The encoder module embeds each path instance as a vector, while the decoder converts the vector into a scalar similarity score, significantly reducing the complexity of the calculation. Experiments demonstrate that this model can not only rapidly detect malware but also capture high-level semantic relationships of application software in complex malware networks by hierarchically aggregating information from neighbors and meta-paths of different orders. Moreover, this model achieved an AUC of 0.9356 and an F1 score of 0.9355, surpassing existing malware detection algorithms. Particularly in the detection of unknown application software, the NeuSim model can double the detection speed, with an average detection time of 105 ms.}
}


@article{DBLP:journals/cn/BodetHMTMRJ24,
	author = {Duschia Bodet and
                  Jacob Hall and
                  Ahmad Masihi and
                  Ngwe Thawdar and
                  Tommaso Melodia and
                  Francesco Restuccia and
                  Josep Miquel Jornet},
	title = {Data signals for deep learning applications in Terahertz communications},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110800},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110800},
	doi = {10.1016/J.COMNET.2024.110800},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BodetHMTMRJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Terahertz (THz) band (0.1–10 THz) is projected to enable broadband wireless communications of the future, and many envision deep learning as a solution to improve the performance of THz communication systems and networks. However, there are few available datasets of true THz signals that could enable testing and training of deep learning algorithms for the research community. In this paper, we provide an extensive dataset of 120,000 data frames for the research community. All signals were transmitted at 165 GHz but with varying bandwidths (5 GHz, 10 GHz, and 20 GHz), modulations (4PSK, 8PSK, 16QAM, and 64QAM), and transmit amplitudes (75 mV and 600 mV), resulting in twenty-four distinct bandwidth-modulation-power combinations each with 5,000 unique captures. The signals were captured after down conversion at an intermediate frequency of 10 GHz. This dataset enables the research community to experimentally explore solutions relating to ultrabroadband deep and machine learning applications.}
}


@article{DBLP:journals/cn/ShaikhJ24,
	author = {Shahzaib Shaikh and
                  Manar Jammal},
	title = {Survey of fault management techniques for edge-enabled distributed
                  metaverse applications},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110803},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110803},
	doi = {10.1016/J.COMNET.2024.110803},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShaikhJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The metaverse, envisioned as a vast, distributed virtual world, relies on edge computing for low-latency data processing. However, ensuring fault tolerance – the system’s ability to handle failures – is critical for a seamless user experience. This paper analyzes existing research on fault tolerance in edge computing over the past six years, specifically focusing on its applicability to the metaverse. We identify common fault types like node failures, communication disruptions, and security issues. The analysis then explores various fault management techniques including proactive monitoring, resource optimization, task scheduling, workload migration, redundancy for service continuity, machine learning for predictive maintenance, and consensus algorithms to guarantee data integrity. While these techniques hold promise, adaptations are necessary to address the metaverse’s real-time interaction requirements and low-latency constraints. This paper analyzes existing research and identifies key areas for improvement, providing valuable research guidelines and insights to pave the way for the development of fault management techniques specifically tailored to the metaverse, ultimately contributing to a robust and secure virtual world.}
}


@article{DBLP:journals/cn/MarweinSK24,
	author = {Phibadeity S. Marwein and
                  Samarendra Nath Sur and
                  Debdatta Kandar},
	title = {Efficient load distribution in heterogeneous vehicular networks using
                  hierarchical controllers},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110805},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110805},
	doi = {10.1016/J.COMNET.2024.110805},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MarweinSK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle movement poses significant challenges in vehicular networks, often resulting in uneven traffic distribution. Fog computing (FC) addresses this by operating at the network edge, handling specific tasks locally instead of relying solely on cloud computing (CC) facilities. There are instances where FC may need additional resources and must delegate tasks to CC, leading to increased delay and response time. This work conducts a thorough examination of previous load balancing (LB) strategies, with a specific focus on software-defined networking (SDN) and machine learning (ML) based LB within the internet of vehicles (IoV). The insights derived from this research expedite the development of SDN controller-based LB solutions in the IoV network. The authors proposes the integration of a local SDN controller (LSDNC) within the FC tier to enable localized LB, addressing delay concerns. However, the information will be available to the main SDN controller (MSDNC) too. The authors explore the concept mathematically and simulates the formulated model and subjecting it to a comprehensive performance analysis. The simulation results demonstrate a significant reduction in delay, with a 125 ms difference when 200 onboard units (OBUs) are used, compared to conventional software-defined vehicular networks (SDVN). This improvement continues to increase as the number of OBUs grows. Our model achieves the same maximum throughput as the previous model but delivers faster response times, as decisions are made locally without the need to wait for the main controller.}
}


@article{DBLP:journals/cn/LiuLL24a,
	author = {Xi Liu and
                  Jun Liu and
                  Weidong Li},
	title = {A truthful double auction mechanism for resource provisioning and
                  elastic service in vehicle computing},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110806},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110806},
	doi = {10.1016/J.COMNET.2024.110806},
	timestamp = {Mon, 07 Oct 2024 08:28:30 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuLL24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent vehicles, equipped with powerful computing and sensing resources, serve as versatile mobile computing platforms, offering many resources to users. This study focuses on resource provisioning and elastic service to address the paramount issue of resource provisioning for in-vehicle computing. It introduces an elastic sensing service, enabling users to declare multiple requested areas to obtain sensing data. It allows various vehicles to collaborate in providing services to a single user when individual vehicles cannot complete the task alone. The approach formulated as a double auction-based setting involves a market with multiple self-interested users and vehicles. The main objective is to design a mechanism that maximizes social welfare. First, a greedy mechanism provides different task allocation strategies while ensuring truthfulness. The proposed mechanism is truthful and equilibrium-driven, achieving individual rationality, consumer sovereignty, and budget balance. It demonstrates the approximation ratio. Simulation results indicate that the proposed mechanism can increase social welfare and the number of served users by at least 29% and 9%, respectively, compared with baseline methods. This research paves the way for more efficient resource provisioning in intelligent vehicles, ultimately enhancing these mobile computing platforms’ overall user experience and capabilities.}
}


@article{DBLP:journals/cn/WangWWCE24,
	author = {Jun Wang and
                  Ning Wang and
                  Haoju Wang and
                  Kerang Cao and
                  Ahmed M. El{-}Sherbeeny},
	title = {{GCP:} {A} multi-strategy improved wireless sensor network model for
                  environmental monitoring},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110807},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110807},
	doi = {10.1016/J.COMNET.2024.110807},
	timestamp = {Wed, 25 Sep 2024 21:46:07 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangWWCE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, smart environmental monitoring devices are widely used in various fields, and one of the most representative tools is the wireless sensor network (WSN). WSNs are easy to deploy and provide real-time information feedback, which is very suitable for environmental monitoring. As we all know, the environmental monitoring network because of the special nature of its work requirements, the need for uninterrupted and real-time transmission of monitoring data, which leads to energy consumption is extremely large, and this cannot meet the needs of its long-term work. Existing traditional routing has problems such as unscientific cluster head election and high redundancy in data transmission, which usually lead to a large amount of energy consumption, which is not conducive to the long-term stable operation of sensor networks. In this paper, we improve the traditional routing protocol and design a cluster head election method based on the genetic algorithm, which proposes a new fitness function in terms of energy, distance, and the number of nodes in the cluster, and performs the selection of cluster head nodes based on this method. In addition, we propose a new grey prediction model, which can realize the real-time update of data queues, and optimize the data transmission process of traditional WSNs based on this prediction model to reduce the amount of intra-cluster data transmission. Combining these improvements, a grey cluster prediction (GCP) model is proposed, and the performance of the model is tested based on real mine and soil data sets. The simulation results show that the model significantly reduces energy loss and extends the network life cycle while ensuring the integrity of data transmission. It can also meet the requirements of long-term stable operation of environmental monitoring equipment.}
}


@article{DBLP:journals/cn/LiNLW24,
	author = {Feng Li and
                  Wei Nie and
                  Kwok{-}Yan Lam and
                  Li Wang},
	title = {Network traffic prediction based on PSO-LightGBM-TM},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110810},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110810},
	doi = {10.1016/J.COMNET.2024.110810},
	timestamp = {Thu, 03 Oct 2024 00:46:19 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiNLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic prediction is critical in wireless network management by allowing a good estimate of the traffic trend, which is also an important approach for detecting traffic anomalies in order to enhance network security. Deep-learning-based method has been widely adopted to predict network traffic matrix (TM) though with the main drawbacks in high complexity and low efficiency. In this paper, we propose a traffic prediction model based on Particle Swarm Optimization (PSO) and LightGBM (PSO-LightGBM-TM), which optimizes the LightGBM parameters for each network flow by PSO so that LightGBM can adapt to each of the network traffic flow. Compared with existing commonly used deep learning models, our model has a more straightforward structure and yet outperforms existing deep learning models. Sufficient comparison tests on three real network traffic datasets, Abilene, GÉANT, and CERNET have been conducted, and the results show that our model provides more accurate results and higher prediction efficiency.}
}


@article{DBLP:journals/cn/LiuXFW24,
	author = {Jianxin Liu and
                  Zhiguo Xu and
                  Rui Fan and
                  Zhigang Wen},
	title = {Learning efficiency maximization in UAV-and-RIS-aided mobile edge
                  learning system},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110756},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110756},
	doi = {10.1016/J.COMNET.2024.110756},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuXFW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the ever-increasing number of Internet of Things devices (IoTDs) and the rapid development of artificial intelligence (AI) technologies, mobile edge learning (MEL) has emerged as a new communication network paradigm that can deploy machine learning on mobile edge computing (MEC) platforms with abundant computational resources. Then how to fully exploit the potential of MEL and enhance its performance is an important issue. To address this issue, the paper proposes an unmanned aerial vehicle (UAV)-and-reconfigurable intelligent surface (RIS)-aided MEL system. In the system, the UAV equipped with a MEL server can fly close to the IoTDs to collect their data for training a deep learning model. And the RIS mounted on the building can improve the wireless channel environment between the UAV and IoTDs to assist the UAV in collecting data. In order to maximize the MEL learning performance while minimizing the system energy consumption, this paper also proposes a new optimization metric called learning efficiency. Then, a learning efficiency maximization problem based on the proposed system is formulated by jointly optimizing the minority class sample size, the transmit resource of the IoTDs, the phase shift of the RIS, and the trajectory of the UAV. Considering the intractability of the problem, we solve it using the alternating optimization (AO) algorithms based on the two types of UAV trajectory design, i.e., a time-division-multiple-access (TDMA) design with higher performance and a Flight-Hover design with lower complexity. The simulation results demonstrate that the proposed optimization metric and algorithms are effective and perform excellently compared to other baselines.}
}


@article{DBLP:journals/cn/ZhangHHH24,
	author = {Yan Zhang and
                  Haopeng Huang and
                  Qingqing Huang and
                  Yan Han},
	title = {6TiSCH IIoT network: {A} review},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110759},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110759},
	doi = {10.1016/J.COMNET.2024.110759},
	timestamp = {Wed, 09 Oct 2024 20:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangHHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-power and Lossy Networks (LLN) constitute an interconnected network of numerous resource-constrained nodes, forming a wireless mesh network. The Time slotted Channel Hopping (TSCH) mode, introduced as a revision of the Medium Access Control (MAC) section within the IEEE 802.15.4 standard, stands as an emerging standard for industrial automation and process control. In 2013, the Internet Engineering Task Force (IETF) established the IPv6 over the TSCH mode of IEEE 802.15.4e (6TiSCH) working group (WG), defining the IPv6 deterministic wireless network—6TiSCH. This development is pivotal for advancing the broader adoption of IPv6 in industrial standards and facilitating the convergence of operational technology (OT) and information technology (IT). As of July 2023, the primary documents encompassing architecture, configuration and parameters, and Minimum Scheduling Function for the 6TiSCH protocol stack have been completed, and the status of the WG has transitioned from active to concluded. Over the past decade, the academic community has extensively researched protocol stacks related to 6TiSCH. This paper furnishes a comprehensive survey of the architecture and developmental processes underlying the 6TiSCH network, encapsulating research achievements since its inception, and delineating the challenges and prospective directions for its future development.}
}


@article{DBLP:journals/cn/TekinDG24,
	author = {Nazli Tekin and
                  Bilge Kagan Dedeturk and
                  Vehbi Cagri Gungor},
	title = {Lifetime maximization of IoT-enabled smart grid applications using
                  error control strategies},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110778},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110778},
	doi = {10.1016/J.COMNET.2024.110778},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TekinDG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, with the advancement of Internet of Things (IoT) technology, IoT-enabled Smart Grid (SG) applications have gained tremendous popularity. Ensuring reliable communication in IoT-based SG applications is challenging due to the harsh channel environment often encountered in the power grid. Error Control (EC) techniques have emerged as a promising solution to enhance reliability. Nevertheless, ensuring network reliability requires a substantial amount of energy consumption. In this paper, we formulate a Mixed Integer Programming (MIP) model which considers the energy dissipation of EC techniques to maximize IoT network lifetime while ensuring the desired level of IoT network reliability. We develop meta-heuristic approaches such as Artificial Bee Colony (ABC) and Particle Swarm Optimization (PSO) to address the high computation complexity of large-scale IoT networks. Performance evaluations indicate that the EC-Node strategy, where each IoT node employs the most energy-efficient EC technique, yields a minimum of 8.9% extended lifetimes compared to the EC-Net strategies, where all IoT nodes employ the same EC method for a communication. Moreover, the PSO algorithm reduces the computational time by 77% while exhibiting a 2.69% network lifetime decrease compared to the optimal solution.}
}


@article{DBLP:journals/cn/DingSZDDX24,
	author = {Ruiyang Ding and
                  Lei Sun and
                  Weifei Zang and
                  Leyu Dai and
                  Zhiyi Ding and
                  Bayi Xu},
	title = {Towards universal and transferable adversarial attacks against network
                  traffic classification},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110790},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110790},
	doi = {10.1016/J.COMNET.2024.110790},
	timestamp = {Wed, 09 Oct 2024 20:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DingSZDDX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, deep learning technology has shown astonishing potential in many fields, but at the same time, it also hides serious vulnerabilities. In the field of network traffic classification, attackers exploit this vulnerability to add designed perturbations to normal traffic, causing incorrect network traffic classification to implement adversarial attacks. The existing network traffic adversarial attack methods mainly target specific models or sample application scenarios, which have many problems such as poor transferability, high time cost, and low practicality. Therefore, this article proposes a method towards universal and transferable adversarial attacks against network traffic classification, which can not only perform universal adversarial attacks on all samples in the network traffic dataset, but also achieve cross data and cross model transferable adversarial attacks, that is, it has transferable attack effects at both the network traffic data and classification model levels. This method utilizes the geometric characteristics of the network model to design the target loss function and optimize the generation of universal perturbations, resulting in biased learning of features at each layer of the network model, leading to incorrect classification results. Meanwhile, this article conducted universality and transferability adversarial attack verification experiments on standard network traffic datasets of three different classification applications, USTC-TFC2016, ISCX2016, and CICIoT2023, as well as five common network models such as LeNet5. The results show that the proposed method performs universal adversarial attacks on five network models on three datasets, USTC-TFC2016, ISCX2016, and CICIoT2023, with an average attack success rate of over 80 %, 85 %, and 88 %, respectively, and an average time cost of about 0–0.3 ms; And the method proposed in this article has shown good transferable attack performance between five network models and on three network traffic datasets, with transferable attack rates approaching 100 % across different models and datasets, which is more closely related to practical applications.}
}


@article{DBLP:journals/cn/BeschastnyiOMGKS24,
	author = {Vitalii Beschastnyi and
                  Darya Y. Ostrikova and
                  Dmitri Moltchanov and
                  Yuliya Gaidamaka and
                  Yevgeni Koucheryavy and
                  Konstantin E. Samouylov},
	title = {Comparison of energy conservation strategies for 5G {NR} RedCap service
                  in industrial environment},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110792},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110792},
	doi = {10.1016/J.COMNET.2024.110792},
	timestamp = {Mon, 03 Mar 2025 21:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BeschastnyiOMGKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recently standardized reduced capability (RedCap) type of user equipment (UE) for 5G New Radio (NR) systems offers a useful option for energy-constrained devices. By utilizing a combination of discontinuous reception (DRX), wake-up signal (WUS), and radio resource management (RRM) relaxation functions, RedCap UE may provide excellent power efficiency for a large set of applications. In this study, we investigate power efficiency and battery lifetime for RedCap UEs for different types of applications and various combinations of energy conservation mechanisms. We utilize extended virtual reality (X-VR) and web browsing as applications of interest. To this aim, we develop a versatile mathematical framework representing the sought metrics as a function of time, accounting for specifics of millimeter wave (mmWave) propagation, micro- and macro-mobility of UEs, human body blockage, and type of application. Numerically, we show that higher micro- and macro-mobility speeds lead to worse power efficiency and the loss can be quite substantial amounting up to 30%. Antenna arrays with worse directivity show better performance in the presence of micro- and macro-mobilities when RRM Relaxation is utilized, with a difference between\n15\n×\n15\nand\n4\n×\n4\narrays reaching 1.5 bit/J/KHz, which is approximately 40%. The energy conservation mechanisms produce no noticeable impact on the power efficiency and battery lifetime for rate-greedy applications such as X-VR. Low-data-rate applications with long pauses between transmission cycles, such as web browsing, may benefit from utilizing WUS and RRM Relaxation in addition to conventional DRX. However, their impact is rather small at a scale of 5%–10%.}
}


@article{DBLP:journals/cn/UmarHJGKH24,
	author = {Amara Umar and
                  Syed Ali Hassan and
                  Haejoon Jung and
                  Sahil Garg and
                  Georges Kaddoum and
                  M. Shamim Hossain},
	title = {{HAC-SAGIN:} High-altitude computing enabled space-air-ground integrated
                  networks for 6G},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110797},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110797},
	doi = {10.1016/J.COMNET.2024.110797},
	timestamp = {Mon, 03 Mar 2025 21:30:46 +0100},
	biburl = {https://dblp.org/rec/journals/cn/UmarHJGKH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The space–air–ground integrated networks (SAGINs) provide a new paradigm for the evolution of the Internet of Things (IoT) networks by enhancing coverage and deploying computing resources near IoT devices, especially in emergency situations and disaster-hit regions. In the context of the IoT networks, aerial platforms such as unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) present in the air layer of SAGINs with access and aerial computing (AC) capabilities have the potential to significantly expand coverage, enhance performance, reduce delay and handle complex computation tasks for IoT devices. Seeking the stated prospect, we propose a high-altitude computing (HAC)-enabled SAGIN leveraging millimeter waves (mmWave) frequency range in which the IoT devices are provided access services by low-earth orbit satellites (LEO-SATs) and HAPs while the HAPs offer AC facility as well. Non-orthogonal multiple access (NOMA) is used as a multiple access scheme with different clustering mechanisms in uplink (UL) and downlink (DL) communication. We aim to establish high-rate data transmission in DL along with minimizing the execution time of IoT devices offloading their data to the HAPs in UL communication. The mmWaves range is targeted to have high-rate data transmissions and NOMA implementation further enhances the bandwidth available for an individual IoT device. For efficient offloading in UL communication, we formulate an optimization problem aiming to minimize the execution time by using the Lagrangian function-based approach. Execution time is minimized by reducing the transmission and computation time, which is attained by the optimization of allocated power and computation resources. Simulation results demonstrate that the proposed HAC-SAGIN is able to establish high-rate transmissions in DL and exhibits a significant decrease in execution time in UL in contrast to the no optimization case. Optimum power assignment improves the achievable rate, leading to reduced transmission time, while optimum core assignment efficiently reduces the computation time. In addition, the offloaded data size-driven NOMA implementation in UL prominently improves the system effective throughput.}
}


@article{DBLP:journals/cn/WahidaCKA24,
	author = {Farah Wahida and
                  M. A. P. Chamikara and
                  Ibrahim Khalil and
                  Mohammed Atiquzzaman},
	title = {An Adversarial Machine Learning Based Approach for Privacy Preserving
                  Face Recognition in Distributed Smart City Surveillance},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110798},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110798},
	doi = {10.1016/J.COMNET.2024.110798},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WahidaCKA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart cities rely heavily on surveillance cameras for urban management and security. However, the extensive use of these cameras also raises significant concerns regarding data privacy. Unauthorized access to facial data captured by these cameras and the potential for misuse of this data poses serious threats to individuals’ privacy. Current privacy preservation solutions often compromise data usability with noise application-based approaches and vulnerable centralized data handling settings. To address these privacy challenges, we propose a novel approach that combines Adversarial Machine Learning (AML) with Federated Learning (FL). Our approach involves the use of a noise generator that perturbs surveillance data right from the source before they leave the surveillance cameras. By exclusively training the Federated Learning model on these perturbed samples, we ensure that sensitive biometric features are not shared with centralized servers. Instead, such data remains on local devices (e.g., cameras), thereby ensuring that data privacy is maintained. We performed a thorough real-world evaluation of the proposed method and achieved an accuracy of around 99.95% in standard machine learning settings. In distributed settings, we achieved an accuracy of around 96.24% using federated learning, demonstrating the practicality and effectiveness of the proposed solution.1}
}


@article{DBLP:journals/cn/WangCLYGG24,
	author = {Houpeng Wang and
                  Suzhi Cao and
                  Huanjing Li and
                  Lei Yan and
                  Zhonglin Guo and
                  Yu'e Gao},
	title = {Multi-objective joint optimization of task offloading based on {MADRL}
                  in internet of things assisted by satellite networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110801},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110801},
	doi = {10.1016/J.COMNET.2024.110801},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangCLYGG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) integrates a large number of heterogeneous terminals and systems, possessing ubiquitous sensing and computing capabilities. Satellite networks are the crucial supplement to terrestrial networks, particularly in remote areas where network infrastructures are sparingly distributed or unavailable. Combining edge computing with satellite networks provides on-orbit computing capabilities for IoT applications, reducing service delay and enhancing service quality. Due to the resource constraints of satellites, achieving collaborative services through task offloading among multiple satellites becomes essential. Both the privacy leakage risk arising from frequent data interactions and the load imbalance resulting from offloading preferences cannot be overlooked. The key challenge of task offloading is to safeguard the privacy of offloaded data and ensure the system’s load balance while minimizing the delay and energy consumption. In this paper, the task offloading problem is formulated as a Partially Observable Markov Decision Process (POMDP), and a task offloading algorithm based on multi-objective joint optimization using multi-agent deep reinforcement learning in a distributed architecture is proposed. The simulation results validate the efficacy of our model and algorithm, demonstrating that our proposed algorithm achieves better performance in minimizing comprehensive offloading costs.}
}


@article{DBLP:journals/cn/TruongN24,
	author = {Truong Van Truong and
                  Anand Nayyar},
	title = {Enhancing security offloading performance in {NOMA} heterogeneous
                  {MEC} networks using access point selection and meta-heuristic algorithm},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110802},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110802},
	doi = {10.1016/J.COMNET.2024.110802},
	timestamp = {Tue, 15 Oct 2024 20:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TruongN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The research delves into the intricate domain of security offloading within the context of non-orthogonal multiple access (NOMA) heterogeneous mobile edge computing (het-MEC) networks operating over Rayleigh fading channels. The investigation centers on a system model comprising a single antenna-equipped edge user, denoted as\nU\n, which strategically offloads computational tasks to two distinct heterogeneous wireless access points (APs): the far AP (\nA\nP\n1\n) and the near one (\nA\nP\n2\n), employing NOMA techniques. Notably, the research accounts for a passive eavesdropper (\nE\n) intending to intercept the\nU\n−\nA\nP\n2\ntransmission. A four-phase protocol is proposed to ensure the security offloading process, namely SAPS, which leverages wireless access point selection (APS) and physical layer security (PLS) techniques. The focus extends to derive a closed-form expression for a novel critical system performance metric: the secrecy successful computation probability (SSCP). Furthermore, an algorithm based on Ant Colony Optimization (ACO) within the continuous domain is introduced, which aims to enhance the SSCP by intelligently determining system parameters. The impact of critical factors such as transmit power, power allocation coefficient, bandwidth, CPU frequency, and task division ratio under the SAPS scheme is explored and compared to the conventional approach using pure NOMA. Remarkably, the algorithm in the proposed scheme demonstrates up to a 3% performance improvement. The validity and accuracy of the study findings are verified through Monte-Carlo simulations. The work contributes significantly to advancing secure offloading strategies in NOMA-based MEC networks, offering valuable insights for practical deployment and optimization.}
}


@article{DBLP:journals/cn/JavedJKRMAD24,
	author = {Danish Javed and
                  N. Z. Jhanjhi and
                  Navid Ali Khan and
                  Sayan Kumar Ray and
                  Alanoud Al Mazroa and
                  Farzeen Ashfaq and
                  Shampa Rani Das},
	title = {Towards the future of bot detection: {A} comprehensive taxonomical
                  review and challenges on Twitter/X},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110808},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110808},
	doi = {10.1016/J.COMNET.2024.110808},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/JavedJKRMAD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Harmful Twitter Bots (HTBs) are widespread and adaptable to a wide range of social network platforms. The use of social network bots on numerous social network platforms is increasing. As the popularity and utility of social networking bots grow, the attacks using social network-based automated accounts are getting more coordinated, resulting in crimes that might endanger democracy, the financial market, and public health. HTB designers develop their bots to elude detection while academics create several algorithms to identify social media bot accounts. This field is active and necessitates ongoing improvement due to the never-ending cat-and-mouse game. X, previously known as Twitter, is among the biggest social network platforms that has been plagued by automated accounts. Even though new research is being conducted to tackle this issue, the number of bots on Twitter keeps on increasing. In this research, we establish a robust theoretical foundation in the continuously evolving domain of Harmful Twitter Bot (HTB) detection by analyzing the existing HTB detection techniques. Our research provides an extensive literature review and introduces an enhanced taxonomy that has the potential to help the scientific community form better generalizations for HTB detection. Furthermore, we discuss this domain's obstacles and open challenges to direct and improve future research. As far as we are aware, this study marks the first comprehensive examination of HTB detection that includes articles published between June 2013 and August 2023. The review's findings include a more thorough classification of detection approaches, a spotlight on ways to spot Twitter bots, and a comparison of recent HTB detection methods. Moreover, we provide a comprehensive list of publicly available datasets for HTB detection. As bots evolve, efforts must be made to raise awareness, equip legitimate users with information, and help future researchers in the field of social network bot detection.}
}


@article{DBLP:journals/cn/PatettaST24,
	author = {Mario Patetta and
                  Stefano Secci and
                  Sami Taktak},
	title = {Line rate botnet detection with SmartNIC-embedded feature extraction},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110809},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110809},
	doi = {10.1016/J.COMNET.2024.110809},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/PatettaST24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Botnets pose a significant threat in network security, exacerbated by the massive adoption of vulnerable Internet-of-Things (IoT) devices. In response to that, great research effort has taken place to propose intrusion detection solutions to the botnet menace. As most techniques focus on either packet or flow granularity, port-based analysis can help detecting newly developed botnets, especially during their early propagation phase. In this paper, we introduce a line rate distributed anomaly detection system that employs NetFPGA Smart-Network Interface Cards (SmartNIC) as programmable switches. Per-port feature extraction modules are deployed directly on the data plane, enabling a centralized controller to periodically retrieve collected metrics, and feed them to a botnet detection algorithm we refine from the state of the art. We evaluate our system using real world traces spanning several months from 2016 and 2023. We show how our solutions allow keeping low the number of anomalies detected, retaining only the most relevant ones, thanks to the distributed monitoring approach that helps discriminating systemic changes from local phenomena. Furthermore, we provide an analysis of the most significant alerts, accounting for the limited ground-truth on the dataset.}
}


@article{DBLP:journals/cn/RastogiRRJS24,
	author = {Eshita Rastogi and
                  Abhishek Roy and
                  Ayush Rastogi and
                  Jaehoon Jeong and
                  Navrati Saxena},
	title = {An efficient beam sweeping scheme with backup paging occasions in
                  NR-Unlicensed Spectrum},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110811},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110811},
	doi = {10.1016/J.COMNET.2024.110811},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RastogiRRJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing demand of user applications and the shortage of cellular spectrum, New Radio (NR) access to Unlicensed Spectrum (NR-U) is getting significant attention. Listen Before Talk (LBT) procedure is used to sense unlicensed channel availability prior to transmission. The transmissions are subjected to LBT procedure failure, which brings extra delay as they are postponed to the next available transmission opportunity. This excess delay also affects time-critical procedures, such as paging. The directional communication in NR requires beamforming, which further complicates paging procedure as additional paging message transmissions are needed to cover all the beams. In this work, we propose an NR-U paging mechanism (called\nF\nF\n_\nB\nP\n) consisting of Full and Fast Paging Cycles with Backup Paging Occasions (POs). Full Paging uses a normal NR-U paging cycle and broadcasts a paging message on all the beams. On the other hand, Fast Paging, which targets delay-sensitive mobile users, uses a shorter paging cycle and broadcasts a paging message on selected beams only. The Backup POs deal with LBT procedure failure impairments while paging broadcast on selective beams reduces paging resource usage. We analyze\nF\nF\n_\nB\nP\nusing a discrete-time semi-Markov chain model and validate the model by extensive simulations. The simulation results show that\nF\nF\n_\nB\nP\noutperforms several baseline mechanisms in terms of average paging delay and paging resource usage.}
}


@article{DBLP:journals/cn/AhsanAM24,
	author = {Muhammad Ahtazaz Ahsan and
                  Amna Arshad and
                  Adnan Noor Mian},
	title = {Leveraging tabular GANs for malicious address classification in ethereum
                  network},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110813},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110813},
	doi = {10.1016/J.COMNET.2024.110813},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AhsanAM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The popularity of ethereum for cryptocurrency transactions attracts malicious actors to engage in illegal activities like phishing, ponzi, and gambling. Previous studies have focused mainly on phishing due to the large number of phishing addresses. However, there is no work done on ponzi or gambling classification due to the limited availability of these addresses, which makes their classification more challenging. In this paper, we propose a machine learning (ML) based method for classifying malicious addresses in ethereum, with a specific focus on phishing, ponzi, and gambling addresses. We use a selective upsampling technique through the tabular generative adversarial network (GAN) to solve limited data problems. We perform not only binary but also multiclass classification on various feature extraction methods, including Trans2Vec and Node2Vec, using Ethereum transactional data. We evaluate our method on\nF\n1\nscore, precision, recall, and accuracy. Our results show that the proposed method is effective in ponzi and gambling detection when compared with the state-of-the-art.}
}


@article{DBLP:journals/cn/YuCXW24,
	author = {Chuan Yu and
                  Shuhui Chen and
                  Qianqian Xing and
                  Ziling Wei},
	title = {Protecting unauthenticated messages in {LTE/5G} mobile networks: {A}
                  two-level Hierarchical Identity-Based Signature {(HIBS)} solution},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110814},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110814},
	doi = {10.1016/J.COMNET.2024.110814},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YuCXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an essential public infrastructure, the security and reliability of mobile networks have a profound impact on people’s production and life. Although the security of LTE/5G networks has been improved a lot with the evolution of standards, there are still some unprotected messages being transmitted between the cellular network and device due to the symmetric key-based security architecture and the trade-off between security and other criteria like network availability. By exploiting these messages, various security attacks have been proposed and demonstrated against commercial mobile networks and devices in existing literature, such as user location tracking, bidding-down, and DoS attacks. To address this security issue, in this paper, we aim to protect these unauthenticated messages in mobile networks using digital signatures. Based on the idea of Hierarchical Identity-Based Signature (HIBS) in existing work, we analyse and design a two-level HIBS solution in detail in terms of different aspects such as keys generation and provisioning procedures, replay mitigation, and cell selection. Unlike previous work, our proposed solution also supports the protection of individual vulnerable RRC and NAS layer signalling in addition to authenticating the base station. We evaluated the efficiency and feasibility of several existing HIBS schemes and implemented the most efficient one in the 5G standalone network setup using open-source software. The implementation results further proved the feasibility of the solution in practice.}
}


@article{DBLP:journals/cn/MadureiraS24,
	author = {Andr{\'{e}} Luiz R. Madureira and
                  Leobino N. Sampaio},
	title = {{NDN} multicast over wireless networks: {A} survey on fundamentals,
                  challenges, and open issues},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110815},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110815},
	doi = {10.1016/J.COMNET.2024.110815},
	timestamp = {Tue, 15 Oct 2024 20:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MadureiraS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless devices have shown remarkable growth in data demand in the last decade due to the proliferation of smart devices and the emergence of bandwidth-hungry applications. This increasing data traffic demand is overcrowding the radio frequency spectrum, leading wireless multicast schemes to become a popular research topic again, as they provide efficient data dissemination. NDN supports multicast communication by design. It is a networked system formed by named entities that adopt a communication model focusing on the content rather than its location. The architecture follows a receiver-driven communication model through which content consumers retrieve data through semantically meaningful names instead of specific destinations. Its properties are essential for multicast communication on ad hoc networks, as its features provide enhanced support for dynamic topologies, decentralized control, and the self-organization of participant nodes that communicate without needing a pre-existing network infrastructure. However, despite the wireless medium being broadcast by nature, NDN multicast is still challenging in wireless scenarios, especially in ad hoc environments, due to the node’s high mobility, link instability, constant handovers, and data transmission over a shared medium. Hence, this survey discusses the benefits of NDN for mobile scenarios through an in-depth analysis of NDN multicast features, focusing on fundamentals, challenges, and open issues when applied to wireless networking.}
}


@article{DBLP:journals/cn/LuoXXLH24,
	author = {Xijian Luo and
                  Jun Xie and
                  Liqin Xiong and
                  Yaqun Liu and
                  Yuan He},
	title = {Fault-tolerant topology construction and down-link rate maximization
                  in air-ground integrated networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110817},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110817},
	doi = {10.1016/J.COMNET.2024.110817},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuoXXLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advantages in flexible deployment, fast communication recovery and high quality of service (QoS), unmanned aerial vehicles (UAVs) have been widely used in wireless communications and networking. However, existing works have not sufficiently solved the problem of reliable and survivable connectivity of UAV networks. In this paper, we investigate the deployment of multiple UAVs, cooperating with a ground mobile station (GMS), to provide wireless coverage for ground users. Under the constraints of satisfying different QoS of users and the resource limitation of UAVs, our aim is to minimize the number of deployed UAVs and maximize the sum down-link rate of all users. In order to ensure the survivability of the deployed UAV networks, we also constraint that each UAV has at least two disjoint paths to GMS to form a fault-tolerant topology. Unfortunately, the formulate problem is intractable and cannot be solved directly. To solve this problem, we decouple the original problem into two subproblems: (1) minimized fault-tolerant topology construction concerning user demands; (2) maximized down-link rate with access policy optimization. Then, we propose a heuristic link-cost minimization method and a potential-game-based user rate maximization method to solve the two subproblems, respectively. The effectiveness of the solution is validated through simulations. Meanwhile, ours also outperforms some baselines in minimized UAVs, down-link transmission rate as well as running time.}
}


@article{DBLP:journals/cn/SethRY24,
	author = {Sayanta Seth and
                  Debashri Roy and
                  Murat Yuksel},
	title = {Cognisseum: Cognitive radios on Colosseum facing adversaries},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110818},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110818},
	doi = {10.1016/J.COMNET.2024.110818},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SethRY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cognitive radio technology brings a lot of interesting features which affect the transmission and reception properties of modern communication devices. Dynamic spectrum sensing, channel hopping and allocation, and software-based control are among the many. The new features allow strategic defense mechanisms while also enabling more capable adversarial attacks. In this work, we study coalitions of secondary users (SUs) against adversaries. In the presence of primary users (PUs), we inspect the behavior of SU pairs in cognitive radio networks, before and after adversarial attacks. We propose algorithms for forming coalitions among SU pairs. We consider two attack strategies for the adversaries: smart or naïve. We study how the channels are allocated if there is an attack and how the payoffs of those SU pairs vary with varying number of channels. We also show the effects of attack from the attackers’ point-of-view and how the attack strategy changes if the adversaries act smart vs. naïve. Using Colosseum, a large-scale wireless channel emulator, we construct a functional cognitive radio network and use its software-defined radio (SDR) hardware as SU and adversarial nodes. Using this setup, we run experiments and record data by running network performance measurement tool iPerf3 for various coalitional setups.}
}


@article{DBLP:journals/cn/AhmedSM24,
	author = {Waqas Ahmed and
                  Nadir Shah and
                  Gabriel{-}Miro Muntean},
	title = {An innovative NSGA-II-based Byzantine Fault Tolerant solution for
                  software defined network environments},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110819},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110819},
	doi = {10.1016/J.COMNET.2024.110819},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AhmedSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Byzantine fault tolerance (BFT) of the control plane in Software Defined Networking (SDN) is achieved by mapping each switch to\n3\nf\n+\n1\nnumber of controllers, where\nf\nrepresents the number of faulty controllers that can be tolerated at a time. A BFT approach protects the data plane from any potential malicious activity at the control plane by detecting the inconsistency among the response messages from multiple controllers. To compute the optimal mapping of switches to the controller, the existing literature does not consider some important parameters. This paper proposes a novel approach, named NBFT-SDN, that extends an artificial intelligence algorithm (i.e. NSGA-II) to solve a new formulated multi-objective optimization problem associated with this mapping. NBFT-SDN considers the very important parameters link reliability and link load along with switch-to-controller minimum delay, switch-to-controllers maximum reliability, controller-to-controller minimum delay, minimum link load, minimum hop count, and controller load balancing when mapping the switches to the controllers in optimum manner. The performance of our proposed approach is evaluated in comparison to a state-of-art approach using real network traces with network topologies of diverse sizes. Our proposed approach NBFT-SDN show improved network performance in terms of reliability, delay, hop count, load balancing and link load.}
}


@article{DBLP:journals/cn/WeiPXW24,
	author = {Xingchen Wei and
                  Laixian Peng and
                  Renhui Xu and
                  Hai Wang},
	title = {Jamming avoidance trajectory planning and load balancing user association
                  in mmWave UAV-assisted HetECN},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110820},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110820},
	doi = {10.1016/J.COMNET.2024.110820},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WeiPXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emergency communication network (ECN) can provide fast, efficient and high-capacity communication services for specific areas by using mmWave transmission and unmanned aerial vehicles (UAVs) serving as aerial base stations (ABSs) or relay nodes. Now, in order to satisfy diverse demands, ECN should support different types of nodes, access methods, and traffic distributions, which is referred to as heterogeneous ECN (HetECN). Therefore, inappropriate trajectory planning and unbalanced traffic loading can lead to UAV flight collisions and network congestion. In this article, we jointly optimize UAV jamming avoidance trajectory and user association strategy aimed to load balancing, to maximize the utilization of HetECN. Specifically, an improved artificial potential field (APF) method along with mmWave beam forming technology is used to obtain the jamming avoidance trajectory of UAVs, and the optimal deployment location of UAVs are determined based on the distribution of ground users (GUs). Subsequently, the matching game and alliance game are comprehensively used to determine the load balancing based GU-UAV associated strategy under various GU demands, thereby ensuring traffic load balancing and resource optimization allocation. In addition, altitude fine-tuning have been made to further power consumption, thereby improving overall network efficiency. Simulation results demonstrate that the proposed method can achieve the expected performance in network utilities such as coverage rate, network capacity, load balancing effect of mmWave UAV-assisted HetECNs.}
}


@article{DBLP:journals/cn/WangZTL24,
	author = {Jie Wang and
                  Zhiju Zhang and
                  Jing Tian and
                  Hongtao Li},
	title = {Local differential privacy federated learning based on heterogeneous
                  data multi-privacy mechanism},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110822},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110822},
	doi = {10.1016/J.COMNET.2024.110822},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangZTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning enables the development of robust models without accessing users data directly. However, recent studies indicate that federated learning remains vulnerable to privacy leakage. To address this issue, local differential privacy mechanisms have been incorporated into federated learning. Nevertheless, local differential privacy will reduce the availability of data. To explore the balance between privacy budgets and data availability in federated learning, we propose federated learning for clustering hierarchical aggregation with adaptive piecewise mechanisms under multiple privacy-FedAPCA as a way to balance the relationship between privacy preservation and model accuracy. First, we introduce an adaptive piecewise mechanism that dynamically adjusts perturbation intervals based on the data ranges across different layers of the model, ensuring minimized perturbation variance while maintaining the same level of privacy. Second, we propose two dynamic privacy budget allocation methods, which are allocating the privacy budget based on global accuracy and global loss, and allocating the privacy budget based on local accuracy and loss, to ensure that better model accuracy can be achieved under the same privacy budget. Finally, we propose a clustering hierarchical aggregation method in the model aggregation stage, and the model is updated and aggregated after the unbiased estimation of the disturbance in each cluster according to the variance of each layer. FedAPCA improves the balance between privacy preservation and model accuracy. Our experimental results, comparing FedAPCA with the SOTA multi-privacy local differential privacy federated learning frameworks on the MNIST and CIFAR-10 datasets, demonstrate that FedAPCA improves model accuracy by 1%–2%.}
}


@article{DBLP:journals/cn/ZhaoZLC24,
	author = {Xingwen Zhao and
                  Han Zhang and
                  Hui Li and
                  Xuangui Chen},
	title = {Open set identification of malicious encrypted traffic based on multi-feature
                  fusion},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110824},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110824},
	doi = {10.1016/J.COMNET.2024.110824},
	timestamp = {Wed, 09 Oct 2024 20:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhaoZLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the current network environment, an increasing amount of malicious traffic is transmitted through encrypted channels, carrying control commands and data. With the continuous development of communication protocols and applications, new types of malicious encrypted traffic are emerging, posing significant challenges for network management (e.g., traffic engineering). Therefore, accurately identifying malicious traffic in complex open network spaces has become a hot research topic in network security. In this study, we draw inspiration from channel theory in image science and innovatively convert traffic data into Red-Green-Blue (RGB) image format to achieve the fusion of multiple features. Inspired by image recognition technologies, we have designed a multi-granularity network model that integrates both global and local features, serving as our core network architecture. At the top of the model, we have equipped each known category with a unique autoencoder, using its generated manifold to replace traditional prototypes for model construction. Classification is accomplished through a scoring mechanism that evaluates category membership and by setting thresholds to achieve open set recognition of unknown categories. Relying on our self-created dataset,Malicious and Encrypted Traffic 2024 (MNET2024), we conduct a series of extensive experiments. The results demonstrate that our proposed method exhibits outstanding performance in both closed-set and open-set recognition tasks.}
}


@article{DBLP:journals/cn/RuiZGWQG24,
	author = {Lanlan Rui and
                  Liangchen Zhao and
                  Zilong Guo and
                  Zihan Wang and
                  Xue{-}Song Qiu and
                  Shaoyong Guo},
	title = {Mobile ad hoc network access authentication mechanism based on rotation
                  election and two-factor aggregation},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110826},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110826},
	doi = {10.1016/J.COMNET.2024.110826},
	timestamp = {Tue, 15 Oct 2024 20:57:49 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RuiZGWQG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous development and progress of computer network communication technology, mobile ad hoc networks (MANET) with strong indestructibility and no fixed infrastructure have become one of the new hot research areas in the academic community. However, due to the open wireless channel and unpredictable changes of network topology, mobile ad hoc networks are susceptible to security threats such as identity spoofing, identity tracing, and DoS attacks, and face security problems that cannot be ignored. Therefore, it is necessary to design a trusted access authentication scheme to authenticate the identity of unknown nodes and ensure the security of connected nodes. In this paper, the Certificate Authority rotation election algorithm based on improved DPoS and access authentication mechanism based on dual identity factor aggregation are proposed, which improve the authentication efficiency and the anti-attack ability of the authentication system, and ensure the security and stability of the network. At the same time, the authentication information is uploaded to the blockchain to ensure the security and immutability of the data. The safety analysis and performance simulation experiments show that the proposed scheme has high safety and good performance.}
}


@article{DBLP:journals/cn/BayazeedKA24,
	author = {Adnan Bayazeed and
                  Khaldoun Khorzom and
                  Mohamad Aljnidi},
	title = {Towards coordinating self-healing instances: Policy-based and non-cooperative
                  game theory-based approaches with location awareness},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110827},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110827},
	doi = {10.1016/J.COMNET.2024.110827},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BayazeedKA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-healing is an essential functionality in Self-Organizing cellular Network (SON). It aims to ensure service continuity by detecting, diagnosing, and compensating network outage after triggering appropriate Cell Outage Compensation (COC) functions according to the outage case. The role of self-healing is increasingly become indispensable, especially in high density network as in 5 G and beyond, in order to maintain the availability and retainability of mobile service and save revenues. However, the essence of COC entails modifications of different Network Control Parameters (NCP) which are under control of SON Functions (SONF) that are related to other self-x functionalities. In addition, it can request different types of compensation functions according to the outage case. Moreover, it requires to harmonize the objectives of the requested stand-alone SONFs over the involved Candidate Cells (CC) in order to re-build a consistent coverage model. Thus, the need for self-coordination is mandatory to guarantee conflict-free outage compensation procedure. In this paper, we propose a novel integrated coordination scheme in order to coordinate Antenna Tilt (AT)-based and Transmission Power (TXP)-based COC functions. Beside their primary targets of conflict avoidance and resolution, we prove that our location-aware coordination mechanisms, which are based on policies and Non-Cooperative Zero-Sum Game (NCZSG) theory, can collaboratively guide greedy stand-alone single-objective COC functions to reach a stable coverage model. The coordination is done even when there is no prior knowledge of the environment or the compensatory algorithms themselves. The choice of optimal Network Control Parameters Values (NCV) is determined according to the function after the coordinator's verification.}
}


@article{DBLP:journals/cn/MorenoRSC24,
	author = {Jes{\'{u}}s Fernando Cevallos Moreno and
                  Alessandra Rizzardi and
                  Sabrina Sicari and
                  Alberto Coen{-}Porisini},
	title = {{ASAP:} Automatic Synthesis of Attack Prototypes, an online-learning,
                  end-to-end approach},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110828},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110828},
	doi = {10.1016/J.COMNET.2024.110828},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MorenoRSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Zero-day attack detection and categorization is an open-research field where four main context factors need to be taken into account: novel or zero-day attacks (i) are unlabeled by definition, (ii) may correspond to out-of-distribution data, (iii) can arise concurrently, and (iv) distribution shifts in the feature space need online-learning. Given such constraints, the online detection and categorization of new cyber threats can be modeled as a heterogeneous collective anomaly detection problem, for which no online-learning solutions exist purely based on back-propagation. To this respect, this paper presents an online-learning, end-to-end back-propagation strategy for Automatically Synthesizing the potential signatures or Attack Prototypes of novel cyber threats (asap). The presented framework incorporates automatic feature engineering, operating over raw data from the OpenFlow monitoring API and raw bytes of traffic captures. In asap, specialized inductive biases enhance the training data efficiency and accommodate the inference machinery to resource-constrained scenarios such as the Internet of Things. Finally, the validity of this framework is demonstrated in a live training experiment comprising IoT traffic emulation 3.}
}


@article{DBLP:journals/cn/DaiLW24a,
	author = {Bin Dai and
                  Hetao Li and
                  Yifan Wang},
	title = {aCroSS: AI-driven cross-layer adaptive streaming for short video applications},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110832},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110832},
	doi = {10.1016/J.COMNET.2024.110832},
	timestamp = {Thu, 24 Oct 2024 08:17:55 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DaiLW24a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As short video applications gain popularity, researchers are exploring ways to enhance Quality of Experience (QoE) for short videos while maximizing network bandwidth efficiency. Despite the growing interest, existing Adaptive Bitrate (ABR) algorithms primarily concentrate on content prefetching strategies and often overlook the dynamic interaction between network congestion control and ABR. This interaction is especially critical for short video streaming, where network conditions can fluctuate rapidly, and user expectations for seamless playback are high. To address these challenges, we propose aCroSS, an AI-driven framework for adaptive short video streaming that jointly optimizes both the application and transport layers to enhance QoE and bandwidth utilization. The aCroSS algorithm leverages advanced machine learning techniques to adapt in real time to fluctuating network conditions and dynamic user behaviors, delivering a more robust and responsive streaming experience. Our simulation results demonstrate that aCroSS consistently outperforms existing baseline algorithms, achieving more than a 10% improvement in utility scores across various network trace datasets. This highlights the effectiveness of aCroSS in delivering superior performance in diverse streaming environments.}
}


@article{DBLP:journals/cn/Almasaeid24,
	author = {Hisham M. Almasaeid},
	title = {Efficient multichannel energy harvesting with dedicated energy transmitters
                  in CR-IoT networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110834},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110834},
	doi = {10.1016/J.COMNET.2024.110834},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Almasaeid24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio Frequency (RF) energy harvesting is strongly believed to be a sustainable solution to the power depletion problem in battery powered IoT devices. In addition to harvesting energy from ambient RF signals, the use of dedicated energy transmitters (ETs) that transmit energy to nearby IoT devices via RF signals has recently been proposed. In this paper, we study the problem of designing an energy harvesting policy for a group of cognitive radio-enabled IoT (CR-IoT) devices served by a number of ETs to maximize the minimum of their charging rates. With the help of cognitive radios, a CR-IoT node is capable of changing its frequency channel of operation allowing for multi-channel energy harvesting. Frequency channels are assumed to be opportunistically accessible depending on the activity of wireless users that are licensed to use those channels. The problem entails the design of the ET’s transmission policy (to what CR-IoT device, and over what channel) and the design of an ambient harvesting policy for every CR-IoT device (when it is not served by ETs). The problem is formulated as a mixed integer linear program (MILP). The objective is to maximize a lower bound on the total harvested energy in a given time frame per CR-IoT node. This optimization is subject to scheduling, total energy budget, and maximum transmit power constraints. Given the intractability of MILP formulations, a sub-optimal algorithm is proposed. Extensive experimentation is carried out to assess the effectiveness of the proposed sub-optimal algorithm by comparing it to the MILP’s solution obtained using IBM CPLEX solver with a limit on the execution time. We also combine our sub-optimal algorithm withe the CPLEX solver to produce a new two-stages algorithm that improves the original one by around 47%. Finally, we investigate the effect of multiple parameters including number of ETs, number of channels, and channel availability probability on the minimum charging rate.}
}


@article{DBLP:journals/cn/TemeneNSGV24,
	author = {Natalie Temene and
                  Andreas Naoum and
                  Charalampos Sergiou and
                  Chryssis Georgiou and
                  Vasos Vassiliou},
	title = {A fault tolerant node placement algorithm for WSNs and IoT networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110835},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110835},
	doi = {10.1016/J.COMNET.2024.110835},
	timestamp = {Mon, 03 Mar 2025 21:30:46 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TemeneNSGV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The operation of Internet of Things (IoT) Network and Wireless Sensor Networks (WSNs) can be often disrupted by a number of factors, such as node faults, security attacks, as well as path disconnections. Despite any disruptions or any failures of network components the functionality and performance of the network should remain unaltered. An approach that can assist in resolving the above issues is the use of mobile nodes. In this work, we present a Fault Tolerant Node Placement Algorithm (FTNPA) that utilizes mobile nodes for addressing failures in the network. We initially propose a Mobile Fault Tolerant (MobileFT) Framework, that supplies the two main functionalities of the Fault Tolerant Node Placement Algorithm (FTNPA), which are the detection and the recovery. Then, we present two variations of the FTNPA algorithm, the Decentralized FTNPA and the Centralized FTNPA. The first variation uses a decentralized detection method, where the detection is performed by the neighboring nodes in the network, as well as a local recovery method, where a mobile node is placed in a certain position to assist the affected area. Whereas, the second variation employs a centralized detection method, where the sink is responsible for the detection, and a recovery method that creates alternatives paths of mobile nodes towards a destination node. Simulation results show that the proposed algorithms can significantly contribute to the detection and recovery of faults in IoT Networks and WSNs.}
}


@article{DBLP:journals/cn/TangLLZZ24,
	author = {Hailiang Tang and
                  Dawei Lin and
                  Wanyu Li and
                  Wenxiao Zhang and
                  Jun Zhao},
	title = {Cyber threat indicators extraction based on contextual knowledge prompt},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110839},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110839},
	doi = {10.1016/J.COMNET.2024.110839},
	timestamp = {Wed, 09 Oct 2024 20:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TangLLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extracting Indicators of Compromise (IOC) from security-related social data (e.g., security blogs, hacker forums) is crucial for predicting cyber risks and mitigating cyber attacks proactively. However, existing IOC extraction approaches suffer from two serious limitations. First, they fail to learn the multiculti-granular and fine-grained IOC features, resulting in high false positives. Second, current methods cannot incorporate symbolic rules and contextual knowledge, resulting in poor interpretability. In this paper, we propose AIIOC, an Accurate and Interpretable I O C extraction model based on contextual knowledge prompts. Particularly, AIIOC first proposes a multi-granularity attention mechanism to learn fine-grained IOC features and boost the accuracy of IOC identification. Additionally, AIIOC designs a novel sequence labeling method that integrates symbolic rules and contextual knowledge prompts, which can encode symbolic rules and contextual semantics of IOC in trainable recurrent neural networks to improve both accuracy and interpretability. Experimental results on two real-world datasets verify that AIIOC outperforms state-of-the-art methods and showcases promising interpretability by incorporating symbolic rules and contextual knowledge prompts.}
}


@article{DBLP:journals/cn/HongCFWZ24,
	author = {Hsiang{-}Jen Hong and
                  Sang{-}Yoon Chang and
                  Wenjun Fan and
                  Simeon Wuthier and
                  Xiaobo Zhou},
	title = {Secure and Efficient Authentication using Linkage for permissionless
                  Bitcoin network},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110840},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110840},
	doi = {10.1016/J.COMNET.2024.110840},
	timestamp = {Tue, 22 Oct 2024 21:11:44 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HongCFWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cryptocurrency’s permissionless and large-scale broadcasting requirements prohibit the traditional authentication implementation on the blockchain’s underlying peer-to-peer (P2P) networking. Thus, blockchain networking implementations remain vulnerable to networking integrity threats such as spoofing or hijacking. We design Secure and Efficient Authentication using Linkage (SEAL) to build connection security for permissionless Bitcoin networking. SEAL uses the linkage between the packets for a symmetric operation, in contrast to the traditional authentication approach relying on identity-credential-based trust. To make it appropriate for cryptocurrency networking, SEAL utilizes the packet header, protects the end-to-end connection, and separates the online process and the offline process so that the real-time overhead is minimal for greater efficiency and practicality. We implement SEAL on a functioning Bitcoin node and demonstrate that SEAL operates efficiently with minimal overhead. Specifically, it reduces the hash rate by only 1.3% compared to an unsecured node. Additionally, we use a network simulator to emulate the Bitcoin Mainnet and analyze SEAL’s impact on block propagation delay. SEAL yields 2.04 times smaller delay and 1.25 times smaller delay in block propagation than HMAC and ChaCha20-Poly1305, respectively. The key advantage of SEAL is that it requires fewer hash computations and simpler mixing operations, resulting in significantly lower computational overhead compared to traditional authentication schemes based on message authentication codes (MACs).}
}


@article{DBLP:journals/cn/ZhengLYXDQ24,
	author = {Songsong Zheng and
                  Jinyao Liu and
                  Xu Yan and
                  Ziyang Xing and
                  Xiaoqiang Di and
                  Hui Qi},
	title = {{BBR-R:} Improving {BBR} performance in multi-flow competition scenarios},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110816},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110816},
	doi = {10.1016/J.COMNET.2024.110816},
	timestamp = {Thu, 24 Oct 2024 15:33:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhengLYXDQ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of network infrastructures and the evolving demands of internet services impose higher requirements on congestion control algorithms. Although Google’s BBR algorithm achieves lower latency and higher goodput compared to traditional congestion control algorithms, it still has many issues. BBR sets the congestion window larger than the calculated ideal value to prevent transmission stalling in the presence of delayed and aggregated ACKs. However, in scenarios with multi-flow competition, this compromise on the congestion window leads to large amounts of queued data, causing increased latency and decreased fairness. Additionally, the ProbeRTT mechanism deviates from its original intent. In this study, we analyze the existing issues of the BBR algorithm from a theoretical standpoint and propose the BBR-R algorithm, which incorporates an adaptive sending rate adjustment mechanism and a new ProbeRTT triggering mechanism. While maintaining the ability for dynamic bandwidth exploration, the sending rate is adjusted based on a latency-related factor called Adaptive_RTprop to control the over-injected data. Coupled with the new ProbeRTT triggering mechanism, BBR-R reduces the frequency of entering the ProbeRTT phase and thereby improves transmission stability. In conducted experiments, BBR-R decreases the frequency of entering the ProbeRTT phase in many scenarios, achieves a 41.86% reduction in latency in the dual-flow competition scenario, and improves fairness by 22.79% in the five-flow competition scenario.}
}


@article{DBLP:journals/cn/BenamorHCM24,
	author = {Amani Benamor and
                  Oussama Habachi and
                  Jean{-}Pierre Cances and
                  Vahid Meghdadi},
	title = {Physical layer security for confidential transmissions in frequency
                  hopping-based downlink {NOMA} networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110821},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110821},
	doi = {10.1016/J.COMNET.2024.110821},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BenamorHCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Facing the exponential number of Internet of Things (IoT) devices and the scarcity of available resources, next-generation wireless networks have to meet very challenging performance targets in terms of providing massive access and ensuring higher spectral efficiency. In this vein, Non-Orthogonal Multiple Access (NOMA) has been widely recognized as one of the advantageous techniques to handle the proliferation of the IoT. Nevertheless, from a security standpoint, enabling a user to decode the signals of the other users, while using Successive Interference Cancellation (SIC), raises serious concerns regarding confidentiality and vulnerability to malicious attacks. Meanwhile, conventional security paradigms, such as upper-layer encryption and sophisticated authentication mechanisms, require high computational complexity and additional processing, which impose an overwhelming burden on energy-efficient IoT devices. Alternatively, Physical layer Security (PLS) has sparked a significant interest as a promising complement to cryptographic techniques. The key idea of PLS is to avail wireless communication properties to secure communications without adding complex encryption mechanisms at higher layers. In this paper, we propose a PLS approach based on a network coding technique to prevent eavesdroppers from decoding users’ information transmitted through a downlink-based NOMA system. This results in correlating the packets to be transmitted with each other, making the interception of a single packet useless. We demonstrate that the eavesdropper’s decoding complexity increases exponentially with the sequence length, making the task intractable for relatively long ones.}
}


@article{DBLP:journals/cn/PalenaCC24,
	author = {Marco Palena and
                  Tania Cerquitelli and
                  Carla{-}Fabiana Chiasserini},
	title = {Edge-device collaborative computing for multi-view classification},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110823},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110823},
	doi = {10.1016/J.COMNET.2024.110823},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/PalenaCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the proliferation of Internet-of-Thing (IoT) devices and the rapid advances in the field of deep learning, there is a growing interest in pushing deep learning computations, conventionally handled by the cloud, to the edge of the network to deliver faster responses to end users, reduce bandwidth consumption to the cloud, and address privacy concerns. However, to fully realize deep learning at the edge, two main challenges still need to be addressed: (i) how to meet the high resource requirements of deep learning on resource-constrained devices, and (ii) how to leverage the availability of multiple streams of spatially correlated data, to increase the effectiveness of deep learning and improve application-level performance. To address the above challenges, we explore collaborative inference at the edge, in which edge nodes and end devices share correlated data and the inference computational burden by leveraging different ways to split computation and fuse data. Besides traditional centralized and distributed schemes for edge-end device collaborative inference, we introduce selective schemes that decrease bandwidth resource consumption by effectively reducing data redundancy. As a reference scenario, we focus on multi-view classification in a networked system in which sensing nodes can capture overlapping fields of view. The proposed schemes are compared in terms of accuracy, computational expenditure at the nodes, communication overhead, inference latency, robustness, and noise sensitivity. Experimental results highlight that selective collaborative schemes can achieve different trade-offs between the above performance metrics, with some of them bringing substantial communication savings (from 18% to 74% of the transmitted data with respect to centralized inference) while still keeping the inference accuracy well above 90%.}
}


@article{DBLP:journals/cn/GuanLGLXDH24,
	author = {Zhong Guan and
                  Chang Liu and
                  Gaopeng Gou and
                  Zhen Li and
                  Gang Xiong and
                  Yangyang Ding and
                  Chengshang Hou},
	title = {A blind flow fingerprinting and correlation method against disturbed
                  anonymous traffic based on pattern reconstruction},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110831},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110831},
	doi = {10.1016/J.COMNET.2024.110831},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuanLGLXDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tor is the most widely used anonymous communication system at present which can anonymize users’ network behavior. At the same time, many illegal network activities also appear more frequently with the help of Tor, posing serious challenges for cyberspace security. Therefore, flow fingerprinting and flow correlation analysis methods are put forward to de-anonymize the malicious anonymous behaviors, which utilize external traffic features as the side-channel information. However, the adversary often reduces the ability of above two methods by adding the disturbance to the anonymous traffic. As a countermeasure against the interference, disturbance-resistant analysis methods can effectively identify those adversarial behaviors while knowing how the traffic is modified. However, in real scenarios, it is unrealistic to distinguish between disturbed and non-disturbed anonymous traffic, let alone to have a clear grasp of the disturbing strategy. In this paper, we propose a blind anonymous traffic analysis method called Blind Analyzer based on pattern reconstruction skills in a “masking-generation” manner. Specifically, Blind Analyzer extracts the pattern knowledge from non-disturbed traffic samples by masking and reconstructing them. During the method application, disturbed anonymous traces are processed following the same way, aiming at removing the incremental noise at the masking stage and restoring the original shape at the reconstruction stage. Besides, a conditional discriminator is designed to determine whether the generated sample has obvious class characteristics. Benefited from the proposed method, we can improve the effectiveness of the anonymous network behavior analysis since the disturbed traffic can be restored as normal ones accurately enough. Experiment results on three datasets show that reconstructed traffic samples output by Blind Analyzer are more useful for base analysis models, which improve the corresponding metric values by 11.23% and 6.61% in max for flow fingerprinting and correlation tasks, respectively.}
}


@article{DBLP:journals/cn/GuptaDK24,
	author = {Jit Gupta and
                  Sourav Das and
                  Krishna Kant},
	title = {NeSt: {A} QoS differentiating end-to-end networked storage simulator},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110833},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110833},
	doi = {10.1016/J.COMNET.2024.110833},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuptaDK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging high-speed storage technologies increasingly use Nonvolatile Memory Express (NVMe) protocol to meet their high throughput and low latency needs. In a datacenter environment, applications accessing multiple such devices over the fabric (i.e. the network) tend to have Quality of Service (QoS) requirements pertaining to offered throughput and experienced latency. In this paper we describe a networked storage system simulator called NeSt that supports end-to-end (E2E) QoS differentiation across multiple classes of service. This is done by conveying the class designation end to end and using it to consistently but independently apply the differentiation in each segment of the path. We demonstrate the ability of NeSt to provide end-to-end QoS differentiation under a variety of situations. To the best of our knowledge, NeSt is the first simulator of networked storage (consisting of multiple NVMe SSDs) that supports E2E QoS differentiation.}
}


@article{DBLP:journals/cn/AlbishariLAAT24,
	author = {Mohammed Albishari and
                  Mingchu Li and
                  Majid Ayoubi and
                  Ala Alsanabani and
                  Jiyu Tian},
	title = {Federated deep learning models for detecting {RPL} attacks on large-scale
                  hybrid IoT networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110837},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110837},
	doi = {10.1016/J.COMNET.2024.110837},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlbishariLAAT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid spread of the Internet of Things (IoT), smart applications and services become increasingly crucial, making them an easily accessible source of personally identifiable information. Over the last few years, the use of machine learning in securing routing layers, particularly routing protocol for low-power and lossy networks (RPL), has become fundamental in ensuring successful routing and privacy preservation as a crucial consideration among edge nodes. In recent works, training of collected data on a central server has increased concerns regarding data privacy. Consequently, decentralized learning is currently a solution for privacy preservation. It has gained popularity in IoT networks in which the models are trained on hybrid data located in edge nodes and enable global decision-making without sharing global data, causing high communication costs during weight updates. We propose a federated learning of routing protocol (Fed-RPL)-based gated recurrent unit (GRU) model for decentralized training rounds and quantization method (Q-8bit) to decrease the number of weight updates that can significantly mitigate the communication overhead and maintain the local model with high accuracy. Meanwhile, the ensemble unit aggregates the updates and selects the best local model to enhance the global model accuracy. Our experiments show that Fed-RPL outperforms classical machine learning (ML) methods in privacy-preserving edge data, significantly reduces the communication cost in non-IID scenarios, and achieves higher detection accuracy than recent FL approaches.}
}


@article{DBLP:journals/cn/ChenYHQL24,
	author = {Honghong Chen and
                  Jie Yang and
                  Zhanjun Hao and
                  Tian Qi and
                  Tingting Liu},
	title = {Research on indoor multi-floor positioning method based on LoRa},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110838},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110838},
	doi = {10.1016/J.COMNET.2024.110838},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChenYHQL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing floor localization methods are plagued by low accuracy, high algorithmic complexity, dense node deployment, susceptibility to environmental factors, and the inability to track trajectories. This paper introduces a localization method designed to address the challenges of multi-floor environments, leveraging LoRa technology. The approach involves deploying LoRa vertical positioning devices and establishing offline and threshold fingerprint databases. To enhance localization accuracy, it combines Time-of-Flight (TOF) ranging values (referred to as "RANGE" in this paper) with Received Signal Strength Indicator (RSSI) values, referred to as "RSSI-RANGE". Subsequently, a multi-floor determination is achieved using the RSSI-RANGE floor determination algorithm and a range-based signal source autonomous switching mechanism. The fingerprinting technique is then employed for trajectory recognition. Comprehensive vertical information is obtained by combining floor determination and trajectory award. Gaussian filtering is utilized for fingerprint preprocessing to eliminate gross errors. The particle swarm optimization algorithm is employed to fine-tune the hyperparameters of the random forest algorithm following noise reduction. Using the random forest algorithm, optimal RSSI-RANGE values are derived, and the offline fingerprint database is established by applying Kriging interpolation. Localization is then achieved in the concluding online recognition phase. Empirical findings illustrate the system\'s high floor accuracy rate of 97.8%, achieving high determination accuracy and comprehensive floor localization when combined with trajectory recognition.}
}


@article{DBLP:journals/cn/AbdulqadderAZ24,
	author = {Ihsan H. Abdulqadder and
                  Israa T. Aziz and
                  Deqing Zou},
	title = {DT-Block: Adaptive vertical federated reinforcement learning scheme
                  for secure and efficient communication in 6G},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110841},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110841},
	doi = {10.1016/J.COMNET.2024.110841},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AbdulqadderAZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The necessities of security and data sharing have focused on federated learning because of using decentralized data sources. The existing works used federated learning for security, however, it still faces many challenges such as poor security and privacy, computational complexity, etc. In this research, we propose adaptive vertical federated learning using a reinforcement learning approach and blockchain. The proposed work includes three phases: user registration and authentication, machine learning-based client selection, and adaptive secure federated learning. Initially, all the users register their credentials to the cognitive agent, which generates a private key, public key, and random number using a Chaotic Isogenic Post Quantum Cryptography (CIPQC) algorithm. Second, optimal clients are selected for participating in federated learning which improves learning rate and reduces complexity. Here, optimal clients are selected by the Enhanced Multilayer Feed Forward Neural Network (EMFFN) algorithm by considering CSI, RSSI, bandwidth, energy, communication efficiency, and statistical efficiency. Finally, adaptive secure federated learning is performed by the Distributed Distributional Deep Deterministic Policy Gradient (D\n4\nPG) algorithm, where the local models are adaptively used by the private strategy based on its sensitivity. The aggregated global models are stored in DT-block (dendrimer tree-based blockchain) which stores the data in a dendrimer tree structure for increasing scalability and reducing search time during data retrieval. The simulation of this research is conducted by NS-3.26 network simulator and the performance of the proposed DT-Block model is estimated based on various performance metrics such as accuracy, delay, loss, f1-score, and security strength this demonstrated that the suggested effort produced better results both in terms of quantitative and qualitative aspects.}
}


@article{DBLP:journals/cn/LiDYLL24,
	author = {Yuchen Li and
                  Hongwei Ding and
                  Zhijun Yang and
                  Bo Li and
                  Zhuguan Liang},
	title = {Integrated trajectory optimization for UAV-enabled wireless powered
                  {MEC} system with joint energy consumption and AoI minimization},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110842},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110842},
	doi = {10.1016/J.COMNET.2024.110842},
	timestamp = {Thu, 24 Oct 2024 15:33:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiDYLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies an unmanned aerial vehicle (UAV)-enabled wireless powered mobile edge computing (MEC) system, where a UAV, equipped with RF Chains and MEC servers, can sustainably provide wireless energy for charging Internet of Things (IoT) devices and executing computing tasks from these devices while hovering at designated hover points. Our goal is to minimize the weighted sum of energy consumption and Age of information (AoI) in this system, which depended on the UAV’s hovering time at designated points and its flying time. To achieve this, we jointly optimize the deployment of hover points and the visiting order of these points by the UAV. It is NP hard and mixed-integer non-convex which is difficult to solve by traditional methods. To tackle this problem, we present a trajectory optimization algorithm for joint energy consumption and AoI (TOJEA), which consists of two phases. In the first phase, an Equilibrium Optimizer (EO) algorithm with a variable individual size via its coding and updating strategies, in which each particle (individual) with its concentration (position) represents a target solution i.e. the whole deployment of hover points, is proposed to optimize the number and locations of hover points. Based on the deployment of hover points, a low-complexity greedy algorithm is adopted in the second stage to generate the optimal visiting order for the UAV. Experimental results demonstrate that TOJEA outperforms other algorithms on ten instances with up to 400 IoT devices.}
}


@article{DBLP:journals/cn/ZhengXFC24,
	author = {Danyang Zheng and
                  Huanlai Xing and
                  Li Feng and
                  Xiaojun Cao},
	title = {Provably efficient security-aware service function tree composing
                  and embedding in multi-vendor networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110843},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110843},
	doi = {10.1016/J.COMNET.2024.110843},
	timestamp = {Wed, 06 Nov 2024 22:18:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhengXFC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multicast greatly benefits many emerging applications such as federated learning, metaverse, and data warehouse. Recently, due to frequent cyber-attacks, multicast services have tended to request rigorous security agreements, which likely differ among the destinations. To meet such agreements, one can employ security-aware service functions (SFs) to construct the security-aware SF tree (S-SFT) for multicast services. A security-aware SF can be provided by various vendors with diverse configurations and implementation costs. The multi-configured SFs and the various security agreements will add significant complexity to the deployment process of the security-aware multicast request. In this work, for the first time, we study how to effectively compose and embed an S-SFT over the network with multiple vendors. We formulate the problem of security-aware SFT composing and embedding. We develop a new technique called cost-security-centrality (CSC) based on the pigeonhole’ s principle and propose a heuristic algorithm called CSC-based S-SFT deployment (CSC-SD). Via thorough mathematical proofs, we show that CSC-SD is logarithm approximate. Extensive simulations show that CSC-SD significantly outperforms the benchmarks and reveal that more function sharing facilitates saving implementation cost, but more routing sharing does not indicate saving routing cost.}
}


@article{DBLP:journals/cn/KumarSL24,
	author = {Navneet Kumar and
                  Karan Singh and
                  Jaime Lloret},
	title = {{WAOA:} {A} hybrid whale-ant optimization algorithm for energy-efficient
                  routing in wireless sensor networks},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110845},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110845},
	doi = {10.1016/J.COMNET.2024.110845},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/KumarSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Sensor Networks (WSNs) are vital for collecting data from remote environments. Nevertheless, the limited energy resources of sensor nodes render energy-efficient routing a critical concern for the successful operation of WSNs. To address these concerns, clustering, and routing are essential tasks in WSNs; clustering aims to organize sensor nodes into groups or clusters to minimize energy usage and prolong the network's lifespan. On the other hand, routing involves determining the optimum paths for transmitting data from the source nodes to the destination nodes. Nonetheless, it has been established that the current energy-efficient routing problem is an NP-hard, requiring a trade-off between energy and overall network performance. In this paper, we proposed a Hybrid Whale-Ant Optimization Algorithm (WAOA) for energy-efficient routing in WSNs. The proposed WAOA utilizes the Whale Optimization Algorithm (WOA) to find the suitable cluster head in the predefined search space, while the Ant Colony Optimization (ACO) searches the optimal route from the source cluster sensors to the cluster head within its predefined space. Linear programming construction is employed to formulate optimization problems for cluster head selection and search for the optimal route. The performance analysis demonstrates that the proposed WAOA performs better than MOORP, MMABC, and AZEBR by 5.78 %,16.11 %, and 18.52 %, respectively, in terms of network lifetime.}
}


@article{DBLP:journals/cn/HadiG24,
	author = {Majid Hadi and
                  Reza Ghazizadeh},
	title = {UAV-mounted {IRS} assisted wireless powered mobile edge computing
                  systems: Joint beamforming design, resource allocation and position
                  optimization},
	journal = {Comput. Networks},
	volume = {254},
	pages = {110846},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110846},
	doi = {10.1016/J.COMNET.2024.110846},
	timestamp = {Thu, 24 Oct 2024 15:33:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HadiG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS) and unmanned aerial vehicle (UAV) have been recently used in wireless-powered mobile edge computing (MEC) systems to enhance the computation bits and energy harvesting performance. However, in the conventional IRS- and UAV-aided MEC systems, the IRS is installed at fixed locations on a building, which restricts the computation performance. UAV-mounted IRS (UAV-IRS), as a promising technology, combines the advantages of UAV and IRS. Hence, in this work, we study a UAV-IRS wireless-powered MEC system, where multiple UAV-IRSs are considered between Internet of Things (IoT) devices and the base station to improve the computation bits and energy harvesting. The multi-antenna base station first charges the IoT devices via radio frequency signals, and then IoT devices offload their computation tasks to the base station via UAV-IRSs. We formulate a computation bits maximization problem for all IoT devices by jointly determining detection beamforming at IoT devices, active energy beamforming at the base station, power allocation, time slot assignment, CPU frequency, the phase shifts design in the wireless energy transfer (WET) and task offloading, and UAV-IRSs positions. A block coordinate descent (BCD) algorithm by decomposing the introduced problem into four blocks is proposed, while the detection beamforming, active energy beamforming, transmit power, time slot assignment, CPU frequency, and the phase shifts design in the task offloading are derived in closed-form results. Also, the successive convex approximation and semidefinite relaxation (SDR) are adopted to obtain the UAV-IRS positions and the phase shifts in the WET, respectively. The simulation results verify the effectiveness of the presented BCD method compared with the different benchmark schemes.}
}
