@article{DBLP:journals/cn/LiHTL23,
	author = {Ziyong Li and
                  Yuxiang Hu and
                  Le Tian and
                  Zhao Lv},
	title = {Packet rank-aware active queue management for programmable flow scheduling},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109632},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109632},
	doi = {10.1016/J.COMNET.2023.109632},
	timestamp = {Sat, 01 Feb 2025 00:24:36 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiHTL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flow scheduling is crucial in improving network Quality of Service (QoS). According to different optimization goals, many scheduling algorithms (e.g., STFQ, pFabric, SRPT, WFQ) have emerged in network. However, these scheduling algorithms are bound with hardware, which are only applicable to fixed network scenarios. Push-In First-Out (PIFO) queues are proposed as a queue abstraction to support programmable packet scheduling, but implementing them in hardware is difficult. We present a general framework named PR-AQM, which uses limited number of FIFO queues to approximate PIFO behavior. The core idea of PR-AQM is to perform Active Queue Management (AQM) on programmable data plane by capturing packet rank distribution and queue congestion level in real time. Specifically, PR-AQM can dynamically adjust the mapping between packet ranks and queues according to the delay status of different priority queues, thus efficiently achieving priority scheduling of packets. We carry out extensive simulations and implement PR-AQM prototype in Tofino switches to evaluate the performance of PR-AQM. Experimental results show that compared with state-of-the-art algorithms, PR-AQM can adapt well to different scheduling algorithms, and achieve better performance by reducing the average flow completion time for short flows up to 45%.}
}


@article{DBLP:journals/cn/HamnacheKB23,
	author = {Mohamed Hamnache and
                  Rahim Kacimi and
                  Andr{\'{e}}{-}Luc Beylot},
	title = {Joint load-balancing and power control strategy to maximize the data
                  extraction rate of LoRaWAN networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109633},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109633},
	doi = {10.1016/J.COMNET.2023.109633},
	timestamp = {Sun, 16 Apr 2023 20:31:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HamnacheKB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LPWAN enabled networks have a dizzying growth and continue to meet an essential need in the Internet of Things market due to their ability to provide low-cost wireless access to geographically spread-out devices. Consequently, an efficient allocation of wireless resources in order to support numerous devices becomes a major concern. In this paper, we propose an SF assignment approach in LoRa networks, paying attention on the traffic load both per Spreading Factor and over the channels. Indeed, our strategy consists in finding a better distribution of the end-devices on the SF by orchestrating an effective load balancing. Moreover, the performance of our solution is evaluated under diverse network configurations, taking into account the capture effect and the non-orthogonality of SFs. Furthermore, we extended the solution by a transmission power control strategy for overall system energy-efficiency. In addition, we validated some assumptions by full-scale experiments like for the 3GPP path loss model, which is used for the first time in LoRa simulations. Our results suggest that Load Shifting leads to better performance in terms of Date Extraction Rate (DER) while guaranteeing good scalability on the network size and density.}
}


@article{DBLP:journals/cn/IqbalGKKKC23,
	author = {Faheem Iqbal and
                  Moneeb Gohar and
                  Hanen Karamti and
                  Walid Karamti and
                  Seok{-}Joo Koh and
                  Jin{-}Ghoo Choi},
	title = {Use of {QUIC} for {AMQP} in IoT networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109640},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109640},
	doi = {10.1016/J.COMNET.2023.109640},
	timestamp = {Mon, 26 Jun 2023 20:51:09 +0200},
	biburl = {https://dblp.org/rec/journals/cn/IqbalGKKKC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of IoT devices is expanding every day in today’s environment. An interoperable protocol like AMQP is essential for supporting multiple IoT use cases and interconnecting IoT devices from different vendors. Many IoT applications are sensitive to delays, which researchers are working to avoid as much as possible. One of the main sources of the delay is the underlying transport layer protocol, such as TCP or UDP. TCP is more reliable than UDP, although it is slower due to the three-way handshake and the use of TLS for security. QUIC, a new transport layer protocol standardized by the Internet Engineering Task Force, combines the finest aspects of UDP and TCP to provide quick and reliable communication. We use the Go programming language to integrate AMQP 1.0 with QUIC to reduce latency and improve battery life. The Docker tool is used to containerize the AMQP 1.0 Broker, Sender, and Receiver implementations, and various scenarios are tested in the NS3 simulator. The findings demonstrated that even though Round Trip Time was 71% higher for QUIC, using QUIC at the transport level improved Startup Latency and Total Communication Time by 62% and 22%, respectively. The proposed scheme (AMQP 1.0 over QUIC) transported 3.5 times more data than the existing scheme (AMQP 1.0 over TCP), but QUIC’s throughput was 7 times higher, which shorten the communication time and 31% less energy was consumed. Furthermore, metrics including Packet Loss, Delay, and Channel Bandwidth were used to compare the two schemes. The results showed that, with the exception of the low channel bandwidth scenario, the proposed scheme consistently outperformed the existing scheme.}
}


@article{DBLP:journals/cn/WangADMZHY23,
	author = {Hua Wang and
                  Adri{\'{a}}n P{\'{e}}rez Aguilar and
                  Almudena D{\'{\i}}az{-}Zayas and
                  Germ{\'{a}}n Corrales Madue{\~{n}}o and
                  Changming Zhang and
                  Nan Hao and
                  Xianbin Yu},
	title = {A general QoE assessment framework for applications and services},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109641},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109641},
	doi = {10.1016/J.COMNET.2023.109641},
	timestamp = {Mon, 26 Jun 2023 20:51:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangADMZHY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we have designed a testbed to evaluate end-to-end Quality of Experience (QoE) performance for mobile applications and devices in a controllable and repeatable manner. Various network scenarios have been defined which allow replication of real-life conditions in a controlled environment. The general testbed framework is presented, which is comprised of a deconstruction process and an aggregation process. A convergence study of the testbed has been carried out to identify the required number of iterations for achieving stable results, and human panel experiments have been introduced to calibrate the performance of the testbed. A case study of two use cases, i.e. content streaming and high-speed internet, is presented with focus on the user experience domain. The measurement results have demonstrated the measurement capability of the proposed testing methodology, which is flexible to generalize the QoE computations for many of the use cases. It is shown that with the help of the testbed, we can better understand how radio resources and propagation channel models will influence the performance of the application/service under test.}
}


@article{DBLP:journals/cn/ZhouZJS23,
	author = {Hongliang Zhou and
                  Yifeng Zheng and
                  Xiaohua Jia and
                  Jiangang Shu},
	title = {Collaborative prediction and detection of DDoS attacks in edge computing:
                  {A} deep learning-based approach with distributed {SDN}},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109642},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109642},
	doi = {10.1016/J.COMNET.2023.109642},
	timestamp = {Mon, 28 Aug 2023 21:39:20 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhouZJS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing (EC) has greatly facilitated the deployment of networked services with fast responses and low bandwidth, by deploying computing and storage at the network edge which is closer to the data sources. However, it is challenging to have the EC servers gain security protections like those in centralized data centers, making them more vulnerable to security attacks, especially distributed denial-of-service (DDoS) attacks. Existing detection approaches relying on the feedback from EC servers under the attacks can incur high bandwidth costs and service performance degradation. In this paper, we propose a new framework CoWatch for collaborative prediction and detection of DDoS Attacks in EC scenarios. Based on the distributed software-defined networking (SDN) architecture, CoWatch can collaboratively predict the DDoS attacks towards the EC servers and detect the attack flows in time. To efficiently filter the suspicious flows in distributed SDN, we devise an optimal threshold model by balancing the trade-off between collaboration efficiency and prediction effectiveness. We also explore and build on the LSTM model to design an algorithm for collaborative prediction and detection of DDoS Attacks. Experiment results on a number of datasets demonstrate the promising performance of CoWatch in effectiveness and efficiency.}
}


@article{DBLP:journals/cn/JinGZ23,
	author = {Nansen Jin and
                  Jinsong Gui and
                  Xinran Zhou},
	title = {Equalizing service probability in UAV-assisted wireless powered mmWave
                  networks for post-disaster rescue},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109644},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109644},
	doi = {10.1016/J.COMNET.2023.109644},
	timestamp = {Sun, 02 Apr 2023 17:25:25 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JinGZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are widely used in the areas where ground network infrastructure is inadequate (or crippled by accidents and disasters) to provide services for ground user equipments (UEs). However, the existing related works cannot serve ground UEs fairly. Due to the original intention of post-disaster emergency network construction with the highest goal of saving lives, fair access service for ground UEs is particularly important in this scenario. Therefore, we address this problem by jointly optimizing the number of service grids, the flight trajectory and the hovering position of a UAV. Firstly, an adaptive area division and consolidation (AADC) method is proposed to optimize the number of service grids of a UAV. Then, a new fair service policy is designed to plan the UAV flight trajectory. Finally, the hovering position of UAV is optimized in each service grid to improve service efficiency. The simulation results show that the proposed scheme both ensures fairness of services to ground UEs and improves system throughput.}
}


@article{DBLP:journals/cn/ChenQCLWHYP23,
	author = {Hao Chen and
                  Hua Qin and
                  Weimin Chen and
                  Ni Li and
                  Tao Wang and
                  Jianxin He and
                  Gelan Yang and
                  Yang Peng},
	title = {{BMS:} Bandwidth-aware Multi-interface Scheduling for energy-efficient
                  and delay-constrained gateway-to-device communications in IoT},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109645},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109645},
	doi = {10.1016/J.COMNET.2023.109645},
	timestamp = {Sun, 16 Apr 2023 20:31:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenQCLWHYP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid growth of Internet of Things (IoT), gateways are being widely deployed and constantly deliver data traffic to their surrounding IoT devices, referred to as gateway-to-device or G2D communications. As the most commonly-accepted wireless technology, WiFi has been recommended for G2D communications due to its high data rate, high reliability, native IP compatibility, and good reusability of existing infrastructures. However, WiFi is inherently energy-hungry and thus could potentially shorten the lifetime of battery-powered IoT devices. Although numerous schemes have been designed to reduce the energy consumption of WiFi devices, they often suffer from long latency due to untimely wake-ups of the WiFi interface, especially under dynamic traffic. To address this, multi-interface protocols have been proposed to leverage the coexisting low-power ZigBee to enable timely wake-ups of the high-power WiFi interface. Despite high energy efficiency and low delay, these protocols have the silent assumption of unlimited ZigBee bandwidth, thus overlooking the optimization of ZigBee bandwidth efficiency. Hence, their performance is limited, when device density is high and/or ZigBee concurrently performs some other data transmission tasks. In this paper, we propose a Bandwidth-aware Multi-interface Scheduling (BMS) scheme, aiming to make efficient use of the limited ZigBee bandwidth to minimize IoT device’s WiFi energy consumption with constrained transmission delay in G2D communications. With BMS, the gateway utilizes the limited ZigBee bandwidth to dynamically alternate each device between two WiFi transmission modes for minimized energy consumption and bounded delay. A prototype of the proposed system is implemented and evaluated, and the testbed results show that under moderate traffic and delay bound, the energy consumption of BMS is 95.1% and 44.8% lower than those of the standard 802.11 power saving management and a state-of-the-art multi-interface scheme, respectively.}
}


@article{DBLP:journals/cn/ValsamasSMC23,
	author = {Polychronis Valsamas and
                  Sotiris Skaperas and
                  Lefteris Mamatas and
                  Luis M. Contreras},
	title = {Virtualization Technology Blending for resource-efficient edge clouds},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109646},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109646},
	doi = {10.1016/J.COMNET.2023.109646},
	timestamp = {Sun, 16 Apr 2023 20:31:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ValsamasSMC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing brings virtualized services closer to users to improve their operation (e.g., in terms of communication latency, reliability, or data privacy) and is considered as a main technological enabler for 5G and beyond ecosystems. Edge cloud orchestration approaches are typically adapting server and network resources to rapid changes in the workload, mitigate resource exhaustion, or address service performance requirements. In this context, we introduce and investigate a new cloud orchestration strategy, called Virtualization Technology Blending (VTB). VTB considers edge clouds supporting alternative virtualization options, including multiple unikernel flavors and container builds of the same or a similar service, which can be effectively blended to handle cases with challenging resource requirements, since they exhibit diverse resource requirements and performance capabilities. Here, we provide an elaboration of VTB, backed by a relevant optimization framework, in order to validate its feasibility and assess its benefits. According to our results, the proposed strategy augments the number of users that can be served up to 74% and improves the server and network resource utilization up to 50.9% and 34.4%, respectively.}
}


@article{DBLP:journals/cn/MalekghainiASLBMMT23,
	author = {Navid Malekghaini and
                  Elham Akbari and
                  Mohammad A. Salahuddin and
                  Noura Limam and
                  Raouf Boutaba and
                  Bertrand Mathieu and
                  Stephanie Moteau and
                  St{\'{e}}phane Tuffin},
	title = {Deep learning for encrypted traffic classification in the face of
                  data drift: An empirical study},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109648},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109648},
	doi = {10.1016/J.COMNET.2023.109648},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MalekghainiASLBMMT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning models have shown to achieve high performance in encrypted traffic classification. However, when it comes to production use, multiple factors challenge the performance of these models. The emergence of new protocols, especially at the application layer, as well as updates to previous protocols affect the patterns in input data, making the model’s previously learned patterns obsolete. Furthermore, proposed model architectures for encrypted traffic classification are usually tested on datasets collected in controlled settings, which makes the reported performances unreliable for production use. In this paper, we study how the performances of two high-performing state-of-the-art encrypted traffic classifiers change on multiple real-world datasets collected over the course of two years from a major ISP’s network. We investigate the changes in traffic data patterns highlighting the extent to which these changes, also known as data drift, impact the performance of the two models in service-level as well as application-level classification. We propose best practices for architecture adaptations to improve the accuracy of the model in the face of data drift. We show that our best practices are generalizable to other encryption protocols and different levels of labeling granularity.}
}


@article{DBLP:journals/cn/MabroukN23,
	author = {Abdelfettah Mabrouk and
                  Assia Naja},
	title = {Intrusion detection game for ubiquitous security in vehicular networks:
                  {A} signaling game based approach},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109649},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109649},
	doi = {10.1016/J.COMNET.2023.109649},
	timestamp = {Sun, 02 Apr 2023 17:25:26 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MabroukN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communications between smart vehicles can make travel quicker and more secure in vehicular networks. However, hackers can perform malicious activities against the communication system, which may cause accidents. To overcome this issue and insure universal public safety on roads, motoring organization needs to reinforce the security of the embedded systems to shield them from different vindictive attacks. In this research work, we present an intrusion detection approach to recognize noxious nodes. When an accident occurs, it gets stored in the vehicles data. Be that as it may, because of the presence of malicious nodes, this data can be erased from the network. Concerning this matter, we have developed a numerical model based on both the coalition game and the signaling game to design an Intrusion Detection Game (IDG). On the one hand, this approach aims to model the interactions between malicious nodes and the Coalition Head which is equipped with an Intrusion Detection System (CH-IDS). On the other hand, it intends to seek the Bayesian Nash Equilibrium (BNE) for an efficient intrusion detection. The simulation results have shown the effectiveness of the proposed approach, thus, the CH-IDS agents are able to select their optimal strategies to detect malicious nodes.}
}


@article{DBLP:journals/cn/GuptaG23,
	author = {Rajni Gupta and
                  Juhi Gupta},
	title = {Federated learning using game strategies: State-of-the-art and future
                  trends},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109650},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109650},
	doi = {10.1016/J.COMNET.2023.109650},
	timestamp = {Sun, 16 Apr 2023 20:31:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GuptaG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a new and promising paradigm that allows devices to learn without sharing data with the centralized server. It is often built on decentralized data where edge nodes use the internet of everything to mitigate the malicious attacks. The server gives incentive to all the participants according to their individual contributions. For profit maximization, each participating node balances between training rewards and costs. Game theory (GT) is a mathematical optimization technique that can be used to solve problems in wireless communication including security, resource allocation, power management, node rewards and punishments, and balancing numerous trade-offs. The complicated interactions between the server and the edge devices are interpreted using GT to maximize their utility. In this review article, we present an overview of the latest research on GT-based FL models for profit maximization, authentication, privacy management, trust management, and threat detection. This study also investigates the bibliometric analysis covering the period from 2019 to 2022 with an emphasis on various mechanisms of FL for GT applications. This article seeks to fill the gap by exploring the significant works highlighting the authors, citations, algorithms used, findings, and applications in this field. Based on the findings, we conclude this article with several key challenges and future approaches for researchers to implement an efficient GT-based FL model.}
}


@article{DBLP:journals/cn/JungKMK23,
	author = {Soyi Jung and
                  Jae{-}Hyun Kim and
                  David Mohaisen and
                  Joongheon Kim},
	title = {Truthful and performance-optimal computation outsourcing for aerial
                  surveillance platforms via learning-based auction},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109651},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109651},
	doi = {10.1016/J.COMNET.2023.109651},
	timestamp = {Sun, 12 Nov 2023 02:17:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/JungKMK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel truthful computing algorithm for learning task outsourcing decision-making strategies in edge-enabled unmanned aerial vehicle (UAV) networks. In our considered scenario, a single UAV performs face identification in a monitored target area. The execution of the identification requires a certain computing power, and its complexity and time are dependent on the number of faces in the recorded images. As a consequence, the task cannot be fully executed by a single UAV under high image arrivals or with images that have a high density of faces. In those conditions, UAV can outsource the task to one of the nearby edges. Importantly, the computing task distribution should be energy-efficient and delay-minimal due to the constraints imposed by the UAV platform characteristics and applications. Based on those fundamental requirements, our proposed algorithm conducts sequential decision-making for image sharing with one selected edge. The edge is selected based on a second price auction for truthfulness. Besides the truthfulness guarantees, deep learning based approximation for the auction solution is used for revenue-optimality. Our evaluation demonstrates that the proposed algorithm achieves the desired performance.}
}


@article{DBLP:journals/cn/DaiXX23,
	author = {Jianbang Dai and
                  Xiaolong Xu and
                  Fu Xiao},
	title = {{GLADS:} {A} global-local attention data selection model for multimodal
                  multitask encrypted traffic classification of IoT},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109652},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109652},
	doi = {10.1016/J.COMNET.2023.109652},
	timestamp = {Sun, 22 Oct 2023 11:14:48 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DaiXX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the Internet of Things (IoT), numerous of IoT devices and different characteristics in IoT traffic patterns need traffic classification to enable many important applications. Deep-learning-based (DL-based) traffic methods have gained increasing attention due to their high accuracy and because manual feature extraction is not needed. Furthermore, seek a lightweight, multitask methods that supports a “performance-speed” trade-off. Thus, we proposed the 0.11 M global-local attention data selection (GLADS) model. The core of the GLADS model includes an “indicator” mechanism and a “local + global” framework. The “indicator” mechanism is a completely different method for handling multimodal input that allows the model to efficiently extract features from multimodal input with a single-modal-like approach. The “local + global” framework for the “performance-speed” trade-off includes a “local” part to obtain the features of each patch in the model input and a Global-Local Attention mechanism in the “global” part outputs the classification results under all possible lengths. Tests on the ISCX-VPN-2016, ISCX-Tor-2016, USTC-TFC-2016, and TON_IoT datasets show that GLADS achieves better performance than several state-of-the-art baselines, ranging from 2.42% to 7.76%. Furthermore, we also propose the “indicator,” which allows the model to simply cope with multimodal input. Based on global-local attention, we analyze the relation of the input section and model performance in detail.}
}


@article{DBLP:journals/cn/UllahBY23,
	author = {Farman Ullah and
                  Muhammad Bilal and
                  Su{-}Kyung Yoon},
	title = {Intelligent time-series forecasting framework for non-linear dynamic
                  workload and resource prediction in cloud},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109653},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109653},
	doi = {10.1016/J.COMNET.2023.109653},
	timestamp = {Sun, 12 Nov 2023 02:17:56 +0100},
	biburl = {https://dblp.org/rec/journals/cn/UllahBY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The industrial revolution 4.0 (I4.0), internet of things developments, and the expansion of online web services have caused exponential growth and deployment in the number of cloud data centers(CDC). Cloud computing is a paradigm that enables tenants to use storage and computing resources in the pay-per-use model. Cloud service providers maximize their profits by distributing the tenant’s demands to the reserved storage and computing servers, minimizing the reservation cost with the satisfaction of the tenant’s quality of service level agreement. Workload prediction and resource management are fundamental and critical problems due to cloud-distributed infrastructure and nonlinear dynamic workload conditions. Conventional prediction techniques in a cloud environment provide the one-dimensional output. Existing solutions mostly forecast resources, such as CPU and memory usage, each as a single output. However, the one-dimensional output in the form of resource provision and usage is not able to capture the relationship of application requirements of multiple resources such as CPU, memory, CPU cores, Disk, and network, which result in inaccurate prediction results and limited information. Efficient resource management requires predicting multiple resource parameters using multivariate state variables for efficient resource allocation. This study proposes an intelligent computing framework based on multivariate time-series bidirectional long short-term memory (BiLSTM) forecasting for predicting cloud virtual machine resources. We consider multi-dimensional resources such as CPU provisioned and usage, memory provisioned and usage, CPU cores, Disk write and read throughput, and network receives and transmit throughput. We investigate several deep learning techniques the proposed multivariate BiLSTM, LSTM, stacked BiLSTM, stacked BiLSTM with LSTM, and BiLSTM auto-encoder. Furthermore, we evaluate the effectiveness of the proposed framework on two real workload traces: Bitbrains traces fastStorage and Rnd. The performance metrics used to evaluate forecasting accuracy are the root mean square error, mean absolute error and mean absolute percentage error. Furthermore, we observe the training-testing data size and the historical window size variation effects on these models.}
}


@article{DBLP:journals/cn/MarkovitzS23,
	author = {Oren Markovitz and
                  Michael Segal},
	title = {Demand Island Routing for {LEO} satellite constellations},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109655},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109655},
	doi = {10.1016/J.COMNET.2023.109655},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MarkovitzS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Earth Orbit (LEO) constellations create a network that includes the satellites (as routing nodes) connected by Inter-Satellite Links (ISLs) and the terminals dynamically connected to one or more satellites. The combination of transient high rate changes in the network topology due to terminals handover, and the end-to-end high propagation time between the routing nodes presents a unique challenge for designing a guaranteed-bandwidth routing protocol that can support the frequent changes. In addition, terminals connected to multiple satellites can balance their traffic between multiple paths in a frame granularity based on the links conditions, and require real-time multipath guaranteed-bandwidth. Prior work focuses on end-to-end routing between multiple gateways and terminals and does not provide a multipath guaranteed-bandwidth service.}
}


@article{DBLP:journals/cn/SunWNFZL23,
	author = {Jiayu Sun and
                  Huiqiang Wang and
                  Lili Nie and
                  Guangsheng Feng and
                  Zhibo Zhang and
                  Jingyao Liu},
	title = {A joint strategy for service deployment and task offloading in satellite-terrestrial
                  IoT},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109656},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109656},
	doi = {10.1016/J.COMNET.2023.109656},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SunWNFZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, low earth orbit satellite constellations, which are an important component of 6G, have been considered as a potential solution to achieve seamless network services for remote areas. Service deployment based on network function virtualization (NFV) has become an essential trend among satellite networks to enable flexible network services. However, current satellite–terrestrial IoT task offloading schemes rarely consider NFV-based satellite service deployment, which limits the performance of satellite networks. In this study, we address this problem by proposing an optimization problem that jointly considers service deployment and task offloading. To solve such a problem with many coupling decision variables, we decouple the problem using a two-stage approach. We propose a deep reinforcement learning-based service deployment policy to solve the service deployment subproblem and an alternating direction multiplier method-based distributed approach to solve the task offloading subproblem, which with the aim of minimizing the task latency and energy consumption of IoT devices. Simulation experiments demonstrate that our scheme can obtain near-optimal solution and can be adapted to large-scale satellite–terrestrial IoT network scenarios.}
}


@article{DBLP:journals/cn/DrainakisPKSAK23,
	author = {Georgios Drainakis and
                  Panagiotis Pantazopoulos and
                  Konstantinos V. Katsaros and
                  Vasilis Sourlas and
                  Angelos Amditis and
                  Dimitra I. Kaklamani},
	title = {From centralized to Federated Learning: Exploring performance and
                  end-to-end resource consumption},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109657},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109657},
	doi = {10.1016/J.COMNET.2023.109657},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DrainakisPKSAK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning (ML) is increasingly implemented in a distributed fashion to harness the data abundance generated in the mobile client devices. Contrary to cloud-based centralized learning (CL), distributed schemes like Federated Learning (FL) shift the computational load to the user-equipment. Trying to compare and characterize the performance of the two schemes, existing literature focuses solely on model accuracy, neglecting the impact of training tasks on the underlying network resources. In this work, we introduce a realistic measurement-based model to thoroughly capture the cloud-to-client bandwidth and energy footprint of ML training on the network, while assessing the achieved accuracy, in the light of different data/model ratios. Our model is implemented in an FL simulation framework that allows trace-driven mobile users to conduct image classification tasks, under a varying exposure to training data heterogeneity. Having hyper-parameters tuned inline with network resources, our experiments unveil how the data to model ratio regulates both bandwidth expenditure and the client/infrastructure energy consumption, in cellular and wireless environments. Further simulation results confirm CL’s superiority over FL under data heterogeneity and capture how FL trades-off accuracy and convergence time; thus, shedding light into so-far open questions that shape (distributed) learning solutions in networked environments.}
}


@article{DBLP:journals/cn/JiangHL23,
	author = {Jinfang Jiang and
                  Guangjie Han and
                  Chuan Lin},
	title = {A survey on opportunistic routing protocols in the Internet of Underwater
                  Things},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109658},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109658},
	doi = {10.1016/J.COMNET.2023.109658},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JiangHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared with traditional ocean monitoring technologies, such as cable seabed observation networks, the Internet of Underwater Things (IoUT) can provide real-time underwater environment monitoring and improve efficiency of data collection. Data collection is one of the most essential functions of IoUT, while it is very challenging to achieve efficient data transmission in IoUT. Opportunistic routing is considered to have great advantages and potential in mitigating the drawbacks of underwater communication and improving the performance of data transmission. Therefore, in this paper, the opportunistic routing protocols for IoUT are reviewed and summarized. Taking advantage of the broadcast characteristic of wireless underwater communication, opportunistic routing selects a group of suitable neighbors as potential forwarders instead of selecting only one or a few specific next-hop nodes as in conventional routing, thus improving the success rate of data packet delivery. Critical to routing performance is which nodes participate in data forwarding as forwarders. The nodes act as forwarders are determined by user-defined routing parameters, which are referred to as route metrics in this paper. First, route metrics of opportunistic routing are analyzed and a comprehensive classification of opportunistic routing protocols is discussed based on the route metrics. Then, opportunistic routing protocols for IoUT are further investigated and compared in terms of the adopted communication technique, data delivery ratio, transmission delay efficiency, energy efficiency, buffer efficiency, reliability, etc. Based on the comprehensive classification and comparison, existing problems and future research issues regarding opportunistic routing in IoUT are finally presented.}
}


@article{DBLP:journals/cn/HongCZ23,
	author = {Hsiang{-}Jen Hong and
                  Sang{-}Yoon Chang and
                  Xiaobo Zhou},
	title = {Auto-tune: An efficient autonomous multi-path payment routing algorithm
                  for Payment Channel Networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109659},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109659},
	doi = {10.1016/J.COMNET.2023.109659},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HongCZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Payment Channel Network (PCN) is a scaling solution for Cryptocurrency networks. We advance the PCN multi-path routing by better modeling the system and incorporating the cost of routing fee and the privacy requirement of the channel balance. We design an autonomous routing algorithm, Auto-Tune, optimizing the routing concerning the success rate and the routing fee and utilizing the limited channel capacity information. The simulation result shows a significant performance gain in the success rate of Auto-Tune over the current PCN implementation based on single-path routing. To analyze the performance cost from the system requirement of the channel balance privacy, we compare Auto-Tune against the state-of-the-art Flash algorithm assuming the availability of the channel-balance information (such channel-balance information violates the PCN privacy requirement and does not comply with the current PCN implementations and practices). The simulation results show that the success rate and fee obtained by Auto-Tune are close to that obtained by Flash (which achieves the optimal fee result by using the exact channel-balance information).}
}


@article{DBLP:journals/cn/Goscien23,
	author = {R{\'{o}}za Goscien},
	title = {Traffic-aware service relocation in software-defined and intent-based
                  elastic optical networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109660},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109660},
	doi = {10.1016/J.COMNET.2023.109660},
	timestamp = {Sun, 02 Apr 2023 17:25:26 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Goscien23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper focuses on the efficient dynamic routing of unicast and data center (dc)-related requests in elastic optical networks (eons) implementing software-defined networking (sdn) and intend-based networking (ibn) paradigms. To improve the network performance (measured as a ratio of the accepted traffic), we apply the service relocation (i.e., the adaptive process of changing the assigned dc for an anycast request). To enable a realistic case study, we propose a novel traffic model for dc-oriented and intent-based transport networks. The model reflects patterns observed in real networks and relates them with the economic and demographic parameters of the cities associated with network nodes. Then, we propose a dedicated allocation algorithm and introduce 21 different service relocation policies. These are traffic- and network-aware approaches, which use three data types for decision-making — traffic prediction, bit-rate rejection history, and topological network characteristics. Finally, we perform extensive simulations to: (i) tune the proposed optimization approaches, (ii) compare their efficiency and select the best one, (iii) determine benefits provided by the traffic- and network-aware service relocation in sdn/ibn optical networks. The results prove a high efficiency of the proposed policies, which allowed to serve up to 5.39% more traffic compared to the network with the fixed dc assignment. They also reveal that the most efficient policy makes its decisions based on traffic prediction.}
}


@article{DBLP:journals/cn/GASK23,
	author = {Keerthan Kumar T. G. and
                  Sourav Kanti Addya and
                  Anurag Satpathy and
                  Shashidhar G. Koolagudi},
	title = {{NORD:} NOde Ranking-based efficient virtual network embedding over
                  single Domain substrate networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109661},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109661},
	doi = {10.1016/J.COMNET.2023.109661},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GASK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network virtualization (NV) allows the service providers (SPs) to partition the substrate resources in the form of isolated virtual networks (VNs) comprising multiple correlated virtual machines (VMs) and virtual links (VLs), capturing the dependencies. Though NV brought about multiple benefits, such as service isolation, improved quality-of-service (QoS), secure communication, and better utilization of substrate resources, it also introduced numerous research challenges. In this regard, one of the predominant challenges is assigning resources to the virtual components, i.e., VMs and VLs, also termed virtual network embedding (VNE). VNE comprises two closely related sub-problems, (i.) VM embedding and (ii.) VL embedding, and both the problems have been demonstrated to be\nN\nP\n-Hard. In the context of VNE, maximizing the revenue to cost ratio remains the focal point for the SPs as it not only boosts acceptance of VNRs but also effectively utilizes the substrate resources. However, the existing literature on VNE suffers from the following pitfalls: (i.) They only consider system resources or (ii.) limited topological attributes. However, both attributes are quintessential in accurately capturing the VNRs and the substrate network dependencies, thereby augmenting the revenue to cost ratio. This paper proposes an efficient VNE strategy called, NOde Ranking-based efficient virtual network embedding over single Domain substrate networks (NORD), to maximize the revenue to cost ratio. To address the problem of VM embedding, NORD utilizes a hybrid entropy and the technique for order of preference by similarity to ideal solution (TOPSIS) based ranking strategy for VMs and servers considering both system and topological attributes that effectively capture the dependencies. Once the ranking is generated, A greedy VM embedding followed by shortest path VL embedding completes the assignment. Simulation results confirm that NORD attains a 40% and 61% increment in average acceptance and revenue-to-cost ratios compared to the baselines.}
}


@article{DBLP:journals/cn/SyedGB23,
	author = {Naeem Firdous Syed and
                  Mengmeng Ge and
                  Zubair A. Baig},
	title = {Fog-cloud based intrusion detection system using Recurrent Neural
                  Networks and feature selection for IoT networks},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109662},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109662},
	doi = {10.1016/J.COMNET.2023.109662},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SyedGB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) techniques are being widely researched for their effectiveness in detecting cyber intrusions against the Internet of Things (IoT). Time sensitive Critical Infrastructures (CIs) that rely on IoT require rapid detection of cyber intrusions close to the constrained devices in order to prevent service delays. Deep learning techniques perform better in detecting attacks compared to shallow machine learning algorithms and can be used for intrusion detection. However, communication overheads due to large volume of IoT data and computation requirements for deep learning models prevents effective application of deep learning models closer to the constrained devices. Existing IDS techniques are either based on shallow learning algorithms or not trained on relevant IoT datasets and furthermore not designed for distributed fog-cloud deployment. To counter these issues, we propose a novel fog-cloud based IoT intrusion detection framework which incorporates a distributed processing by splitting the dataset according to attack class and a feature selection step on time-series IoT data. This is followed by a deep learning Recurrent Neural Network (SimpleRNN and Bi-directional Long Short-Term Memory (LSTM)) for attack detection. The effectiveness of the proposed approach was evaluated using the high-dimensional BoT-IoT dataset which contains large volumes of realistic IoT attack traffic. Results show that feature selection methods significantly reduced the dataset size by 90% under the computation requirements without compromising on the attack detection ability. The models built on reduced dataset achieved higher recall rate compared to models trained on full feature set without loosing class differentiation ability. The SimpleRNN and Bi-LSTM models also did not suffer any underfitting or overfitting with the reduced feature space. The proposed deep learning based IoT intrusion detection framework is suitable for fog-cloud based deployment and can scale well even with large volumes of IoT data.}
}


@article{DBLP:journals/cn/CarpiMDCYWF23,
	author = {Fabrizio Carpi and
                  Marco Martal{\`{o}} and
                  Luca Davoli and
                  Antonio Cilfone and
                  Yingjie Yu and
                  Yi Wang and
                  Gianluigi Ferrari},
	title = {Experimental analysis of RSSI-based localization algorithms with {NLOS}
                  pre-mitigation for IoT applications},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109663},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109663},
	doi = {10.1016/J.COMNET.2023.109663},
	timestamp = {Tue, 12 Sep 2023 07:58:35 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CarpiMDCYWF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an effective target localization strategy for Internet of Things (IoT) scenarios, where positioning is performed by resource-constrained devices. Target-anchor links may be impaired by Non-Line-Of-Sight (NLOS) communication conditions. In order to derive a feasible IoT-oriented positioning strategy, we rely on the acquisition, at the target, of a sequence of consecutive measurements of the Received Signal Strength Indicator (RSSI) of the wireless signals transmitted by the anchors. We then consider a pragmatic approach according to which the NLOS channels are pre-mitigated and “transformed” into equivalent Line-Of-Sight (LOS) channels to estimate more accurately each target-anchor distance. The estimated distances feed “agnostic” localization algorithms, operating as if all links were LOS. We experimentally assess the performance of our approach in indoor (IEEE 802.11-based) and outdoor (Long Term Evolution, LTE-based) scenarios, considering both geometric and Particle Swarm Optimization (PSO)-based localization algorithms. Even if NLOS mitigation per single communication link is very effective, our results show that, in a given environment, it is possible to derive an “average” NLOS mitigation strategy regardless of the specific position of the target in the given environment. This is crucial to limit the computational complexity at IoT nodes performing localization, yet guaranteeing a relatively high (for IoT scenarios) localization accuracy, especially in an IEEE 802.11-based indoor case (with six anchors). The obtained performance compares favorably (in relative terms) with that obtained with more sophisticated wireless technologies (e.g., Ultra-WideBand, UWB).}
}


@article{DBLP:journals/cn/DeebakH23,
	author = {Bakkiam David Deebak and
                  Seong Oun Hwang},
	title = {Intelligent drone-assisted robust lightweight multi-factor authentication
                  for military zone surveillance in the 6G era},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109664},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109664},
	doi = {10.1016/J.COMNET.2023.109664},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DeebakH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the diverse range of surveillance applications, large-scale deployment of next-generation communication technologies and the fast-growing development of unmanned aerial vehicles (UAVs) are envisioned as key innovations in the adoption of beyond-fifth generation (B5G) and 6G communication. Due to its self-reliance and versatility, a complex communication network can be formulated strategically to improve the application features of drone technology, including search-and-rescue, mission-critical services, and military surveillance. In recent times, technological advancements in hardware and software infrastructure have gained momentum toward seamless information interaction in aerial communication. Unfortunately, the recurrent process of user authentication causes severe communication instability in an unmanned aerial ad hoc network (UAANET) leading to some serious cyber threats, such as buffer overflow, denial of service, and spoofing. Therefore, building secure and reliable authentication is inevitable in order to protect drone-aided healthcare service environments. To protect aerial zones and improve security efficiency, this paper designs robust lightweight secure multi-factor authentication (RL-SMFA). The proposed RL-SMFA utilizes an AI-enabled, secure analytics phase to verify the genuineness of drone swarms for the ground control station. While protecting communication with drone vehicles, we also observe that power consumption by drones is reduced to a large extent. Using formal verification under a random oracle model, we show that the proposed RL-SMFA can functionally resist system vulnerabilities and constructively decrease the computation and communication costs of the UAANET. Lastly, the simulation study using ns3 shows that the proposed RL-SMFA achieves better performance efficiencies in terms of throughput rate, packet delivery ratio, and end-to-end delay than other state-of-the-art approaches to discovering a proper link establishment.}
}


@article{DBLP:journals/cn/VidalGP23,
	author = {Jos{\'{e}} Ramon Vidal and
                  Luis Guijarro and
                  Vicent Pla},
	title = {A user subscription model in mobile radio access networks with network
                  slicing},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109665},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109665},
	doi = {10.1016/J.COMNET.2023.109665},
	timestamp = {Mon, 26 Jun 2023 20:51:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/VidalGP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is an architectural enabling technology that logically decouples the current cellular networks into infrastructure providers (InPs) and Network Slice Tenants (NSTs). The network resources (e.g., radio access resources at each cell) are owned by the InP, and are shared by the NSTs to provide a service to their mobile users. In this context, we proposed a business model that includes resource allocation and user subscription to NSTs in a competitive setting, and provides, among other things, closed-form expressions for the subscription indicators in equilibrium of each NST at each cell. This model relies on the widely adopted logit model to characterize user subscriptions. However, as a consequence of user mobility and radio propagation, some of the underlying assumptions in the logit model do not hold. Therefore, further research is needed to assess the accuracy of the results provided by the logit model in a mobile radio scenario. We carry out a thorough evaluation of the validity of the model by comparing its results against those obtained through computer simulation. Our simulation model includes complete and realistic characterizations of user mobility and radio propagation. From the results, we conclude that in most cases the logit model provides valid results in a mobile radio scenario.}
}


@article{DBLP:journals/cn/ChuMGRWM23,
	author = {Yi Chu and
                  Paul D. Mitchell and
                  David Grace and
                  Jonathan Roberts and
                  Dominic White and
                  Tautvydas Mickus},
	title = {{IRIS:} {A} low duty cycle cross-layer protocol for long-range wireless
                  sensor networks with low power budget},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109666},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109666},
	doi = {10.1016/J.COMNET.2023.109666},
	timestamp = {Mon, 11 Nov 2024 09:19:21 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChuMGRWM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a lightweight cross-layer protocol relIable Routing with coordInated medium acceSs control (IRIS) which is designed for long-range pipeline Wireless Sensor Networks (WSNs) with extremely low power budget, typically seen in a range of monitoring applications. IRIS includes functions of network discovery, medium access control and routing designed for heavily energy constrained networks. The protocol is able to operate with less than 1% duty cycle, thereby conforming to ISM band spectrum regulations in the 868MHz band. The duty cycle can be flexibly configured to meet other regulations/power budgets as well as to improve the route forming performance. Simulation results show a guaranteed route formation in different network topologies with various protocol configurations. An analytical model is presented to validate the simulation results of route formation time. System robustness against unreliable wireless connections and node failures are also demonstrated by simulations. A network of 71 attenuated LoRa nodes is implemented to evaluate the capability of IRIS. Long-term operation of the network shows consistent performance compare with the simulations.}
}


@article{DBLP:journals/cn/JoJK23,
	author = {Hyun{-}Chul Jo and
                  Hyun{-}Wook Jin and
                  Joongheon Kim},
	title = {Self-adaptive end-to-end resource management for real-time monitoring
                  in cyber-physical systems},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109669},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109669},
	doi = {10.1016/J.COMNET.2023.109669},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JoJK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel self-adaptive resource management framework that preserves a low end-to-end monitoring delay in large-scale cyber–physical systems (CPS) while providing high monitoring resolution. According to the tradeoff relationship between the end-to-end delay and monitoring resolution, our proposed algorithm is designed inspired by Lyapunov optimization framework that is mathematically time-average utility optimal under delay/stability constraints. Our proposed Lyapunov optimization-based framework dynamically controls both sensing period on sensor nodes and analysis period on back-end server nodes based on the workload of network and CPU resources. Moreover, our framework does not require additional messages between sensor and server nodes. In our framework, each node autonomously recognizes the underuse or overuse of network and CPU resources with only local state information and self-adapts periods of sensing and analysis under Lyapunov optimization theory. Furthermore, our framework provides different priorities for classes of sensing data and adaptively limits periods of sensing and analysis of low-priority classes. Our proposed framework is designed and implemented on a well-known publish/subscribe (pub/sub) network protocol, Message Queuing Telemetry Transport (MQTT), and finally we can confirm that our proposed framework achieves low end-to-end monitoring delays and high monitoring resolutions.}
}


@article{DBLP:journals/cn/BorgohainC23,
	author = {Ponjit Borgohain and
                  Hiten Choudhury},
	title = {A lightweight {D2D} authentication protocol for relay coverage scenario
                  in 5G mobile network},
	journal = {Comput. Networks},
	volume = {225},
	pages = {109679},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109679},
	doi = {10.1016/J.COMNET.2023.109679},
	timestamp = {Sun, 16 Apr 2023 20:31:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BorgohainC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G cellular network is becoming a preferred communication network for deployment of the Internet of Things (IoT), due to its high speed, better connectivity, increased bandwidth, lower latency and flexibility. As a result, the traffic load of 5G cellular network is bound to increase manifold in the near future. Therefore, there is a requirement to explore the use of technologies that have the potential to ease the traffic load off the core network. Device-to-Device (D2D) communication in 5G cellular network for relay coverage is one such technology that enables two devices to communicate directly with each other through a local communication medium, even if one of them is not in the coverage area of the 5G cellular network. This technology has the potential to provide wider coverage, reduce the traffic load of the core network and minimize communication latency. For smooth D2D communication, the devices need to be authenticated. In technical specification-23.303 that is standardized by 3rd generation partnership project, basic guidelines for D2D communication are provided. However, in this specification no mechanism for authentication of the devices is proposed. It is also observed that in recent times a few authentication protocols have been proposed for D2D communication, but none of them are designed for relay coverage scenario in 5G cellular network. Hence, to fill the above discussed gap, in this paper we propose a lightweight D2D protocol for relay coverage scenario in 5G Mobile Network, that uses elliptic curve cryptography and symmetric key cryptography. To check the robustness of the proposed protocol, security analysis using formal verification techniques/tools like BAN logic, Scyther and AVISPA is done. The results of the analysis show the protocol to be safe against various attacks. Moreover, performance analysis of the protocol shows it to be lightweight and efficient, in terms of computation and communication cost, compared to other related works in the literature.}
}
