@article{DBLP:journals/cn/ZahediJ24,
	author = {Seyed Reza Zahedi and
                  Shahram Jamali},
	title = {A hybrid model for {VNF} deployment capable of responding to online
                  requests at network edge},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110385},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110385},
	doi = {10.1016/J.COMNET.2024.110385},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZahediJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network function virtualization (NFV) is a new network architecture that replaces dedicated hardware appliances with software instances and run them via software virtualization on general-purpose servers at edge clouds. As edge clouds are resource-constrained, optimal placement of virtual network functions (VNFs) is a challenging task. This problem has proven to be NP-hard, and thus, metaheuristic algorithms have been widely used to solve it. However, these techniques suffer from high computational complexity due to their iterative, time-consuming process and cannot be performed in real-time for online requests. This paper proposes a hybrid model based on heuristic algorithms and the Whale Optimization Algorithm (WOA) to optimally assign VNFs to physical servers at edge clouds. In this method, a multi-criteria heuristic model is used for online VNF deployment, while WOA is performed offline to tune the hyperparameters of the heuristic model. Simulation results demonstrate the efficiency of the proposed method against existing techniques in terms of power consumption and acceptance rate.}
}


@article{DBLP:journals/cn/LuanLJDZCL24,
	author = {Zeyu Luan and
                  Qing Li and
                  Yong Jiang and
                  Jingpu Duan and
                  Ruobin Zheng and
                  Dingding Chen and
                  Shaoteng Liu},
	title = {{MATE:} When multi-agent Deep Reinforcement Learning meets Traffic
                  Engineering in multi-domain networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110399},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110399},
	doi = {10.1016/J.COMNET.2024.110399},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuanLJDZCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the global network evolves into a large-scale interconnection system with multiple distributed domains, scalability and efficiency have become equally important for Traffic Engineering (TE). Traditional centralized TE suffers from significant computational complexity and privacy concerns, while distributed TE results in suboptimal routing decisions due to its lack of a coordination mechanism. This paper proposes MATE, an innovative, scalable, and efficient TE framework for multi-domain networks featuring a hierarchical control plane. MATE designs a three-stage workflow to optimize routing decisions for inter-domain traffic while ensuring their QoS guarantees. First, in the bottom-up topology abstraction stage, MATE aggregates network resources within each domain, creating an abstract view of the global network state while protecting each domain’s internal information. Second, in the top-down decision-making stage, MATE computes a QoS-constrained domain sequence based on the global network state and then converts it into multi-path routing in all related domains. Third, MATE conducts parallel inferences across relevant domains on traffic split ratios for multi-path routing using well-designed multi-agent reinforcement learning. We evaluate MATE on both real-world and synthetic topologies under various traffic patterns. In a large-scale topology encompassing 16 domains, MATE achieves near-optimal link utilization across 97% network scenarios, with an approximation ratio of below 1.3. The experimental results demonstrate MATE’s superiority in fulfilling QoS requirements, minimizing maximum link utilization, and maintaining robustness against traffic pattern variations and random link failures.}
}


@article{DBLP:journals/cn/ReganJKV24,
	author = {R. Regan and
                  R. Josphineleela and
                  Mohammad Khamruddin and
                  R. Vijay},
	title = {Balancing data privacy and sharing in IIoT: Introducing the {GFL-LFF}
                  aggregation algorithm},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110401},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110401},
	doi = {10.1016/J.COMNET.2024.110401},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ReganJKV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the Industrial Internet of Things (IIoT) continues to evolve, the need for effective data aggregation schemes that balance the imperatives of data sharing and individual data privacy becomes paramount. Traditional data aggregation methods often entail risks of sensitive information exposure during data transmission and storage. This paper proposes a novel Graphical Federated Learning-based Local Fennec Fox (GFL-LFF) algorithm to ensure secure aggregation by minimizing the revelation of individual data. In this study, a Graph Neural Network (GNN) with federated learning is employed to enhance privacy protection for IIoT's data aggregation scheme. A Local search-based fennec fox optimization is utilized for tuning the parameters of GNN. The GFL and LFF approaches are integrated to enhance the security of the data that can be shared through the IIoT system. The MNIST dataset, CIFAR-10 dataset, and LFW dataset were taken to conduct the experimentation results, and the evaluation measures, such as throughput, end-to-end delay, network lifetime, latency, and energy consumption, are utilized to evaluate the proposed GFL-LFF method, and these results are compared with other approaches. The GFL-LFF method achieved a throughput of 0.98 Mbps, an end-to-end delay of 1.2 s, a network lifetime of 5610 rounds, a latency of 1.6 s, and an energy consumption of 0.2 mJ. The experimental results illustrate the effectiveness of the proposed GFL-LFF method for IIoT's data aggregation scheme in privacy protection.}
}


@article{DBLP:journals/cn/YuDZY24,
	author = {Zhanwei Yu and
                  Tao Deng and
                  Yi Zhao and
                  Di Yuan},
	title = {Multi-cell content caching: Optimization for cost and information
                  freshness},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110420},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110420},
	doi = {10.1016/J.COMNET.2024.110420},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YuDZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-access edge computing (MEC) systems, there are multiple local cache servers caching contents to satisfy the users’ requests, instead of letting the users download via the remote cloud server. In this paper, a multi-cell content scheduling problem (MCSP) in MEC systems is considered. Taking into account jointly the freshness of the cached contents and the traffic data costs, we study how to schedule content updates along time in a multi-cell setting. Different from single-cell scenarios, a user may have multiple candidate local cache servers, and thus the caching decisions in all cells must be jointly optimized. We first prove that MCSP is NP-hard, then we formulate MCSP using integer linear programming, by which the optimal scheduling can be obtained for small-scale instances. For problem solving of large scenarios, via a mathematical reformulation, we derive a scalable optimization algorithm based on repeated column generation. Our performance evaluation shows the effectiveness of the proposed algorithm in comparison to an off-the-shelf commercial solver and a popularity-based caching.}
}


@article{DBLP:journals/cn/TaoWQSCZ24,
	author = {Yuan Tao and
                  Taochun Wang and
                  Yong Qiang and
                  Leilei Shen and
                  Fulong Chen and
                  Chuanxin Zhao},
	title = {A differential privacy location protect approach with intelligence
                  data collection paradigm for {MCS}},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110421},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110421},
	doi = {10.1016/J.COMNET.2024.110421},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TaoWQSCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location information is often required for task allocation in mobile crowdsensing. However, directly uploading location information can raise concerns about privacy leakage among users. To address this issue and protect users privacy when uploading location data to the server, this paper proposes a novel method based on differential privacy for preserving location information. The main idea is to utilize Hilbert mapping to transform the two-dimensional location coordinate of the user into a one-dimensional Hilbert index. Then, Laplace noise is added to the index to introduce a perturbed index, ensuring the protection of location privacy. To enhance the usability of the perturbed index, this paper treats the Laplace noise as a random variable and employs mathematical integration to measure the distance between users and tasks. Leveraging the characteristic of the Hilbert index, wherein a small change in the indexes can result in a large distance between corresponding coordinates, the combination of differential privacy and Hilbert mapping offers enhanced effectiveness compared to conventional differential privacy location preservation schemes. Finally, the proposed scheme is evaluated using the real dataset, and the experiments validate its efficacy.}
}


@article{DBLP:journals/cn/ChenWCWZHDYYS24,
	author = {Wenqi Chen and
                  Zhiliang Wang and
                  Liyuan Chang and
                  Kai Wang and
                  Ying Zhong and
                  Dongqi Han and
                  Chenxin Duan and
                  Xia Yin and
                  Jiahai Yang and
                  Xingang Shi},
	title = {Network anomaly detection via similarity-aware ensemble learning with
                  ADSim},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110423},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110423},
	doi = {10.1016/J.COMNET.2024.110423},
	timestamp = {Sun, 19 Jan 2025 14:22:34 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChenWCWZHDYYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The last decade has seen the increasing application of machine learning to various tasks, including network anomaly detection. But anomaly detection methods based on a single machine learning algorithm usually fail to achieve good results, since network traffic have complex and changeable patterns. Therefore, many solutions based on ensemble learning have been proposed to address this problem. However, most previous studies have the main drawback that they overlook the similarity between the weak classifiers, which may degrade the detection performance. What is more, most existing works use offline and supervised algorithms, which means a large number of computing resources and reliable labels are necessary during the training period.}
}


@article{DBLP:journals/cn/LiMHLXX24,
	author = {Xianglong Li and
                  Kaiwei Mo and
                  Yeqiao Hou and
                  Zongpeng Li and
                  Hong Xu and
                  Chun Jason Xue},
	title = {An auction approach to aircraft bandwidth scheduling in non-terrestrial
                  networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110424},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110424},
	doi = {10.1016/J.COMNET.2024.110424},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiMHLXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet Service has witnessed only limited deployment on commercial flights, where network infrastructure is lacking and subscription fees are high. It is natural to design an efficient scheduling strategy for Internet service access on flights, while maximizing social welfare. There are challenges to use either antennas on the ground or geostationary (GEO) satellites in space as Internet Service Provider (ISP) equipment. First, preemptive-purchase and resource-allocation in post-paid plans may not satisfy the dynamic demand for large amounts of flight-ISP machine links. Second, finding an optimal, dynamic, online solution based on flight demands is NP-hard. Third, existing solutions often fail to maximize either seller profit or social welfare. To address the aforementioned challenges, we formulate an online bandwidth scheduling integer linear program (ILP) among flights, antennas, and GEO satellites in a Non-Terrestrial Network (NTN). We transform the ILP into an exponential optimization problem, which is then relaxed, with its dual formulated. We iteratively solve a series of dual subproblems in polynomial time, towards social welfare maximization with a good competitive ratio in typical network settings. In empirical studies, our algorithm achieved a much better competitive ratio than the theoretical worst-case guarantee, and clearly outperforms alternative algorithms.}
}


@article{DBLP:journals/cn/LiHY24,
	author = {Sensen Li and
                  Yicai Huang and
                  Bin Yu},
	title = {A practical and flexible PUF-based end-to-end anonymous authentication
                  protocol for IoT},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110426},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110426},
	doi = {10.1016/J.COMNET.2024.110426},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive development of the Internet of Things (IoT), its security has become a critical concern. As a lightweight hardware primitive, Physical Unclonable Function (PUF) provides a promising solution for IoT security. Nevertheless, as per the knowledge of the authors, most of the existing PUF-based anonymous authentication protocols for IoT are not suitable for end-to-end IoT applications due to their flexibility or security. Specifically, there are two main issues with existing related protocols: (1) the end-to-end anonymous authentication between IoT devices requires the participation of a trusted third party, which seriously affects the flexibility of interaction; (2) most related protocols provide weak anonymity, that is, they are anonymous against only eavesdroppers. To solve these problems, a new PUF-based end-to-end anonymous authentication protocol is proposed. Without needing the real-time participation of the third party, the proposed protocol realizes end-to-end direct mutual authentication and session key agreement while maintaining strong anonymity. It also provides an online dynamic updating mechanism for security parameters. The security analysis shows that the proposed protocol has perfect forward secrecy while resisting various known attacks including physical attacks and modeling attacks. Meanwhile, it outperforms related protocols in terms of protocol rounds and communication costs.}
}


@article{DBLP:journals/cn/WangXZ24,
	author = {Xiaoyu Wang and
                  Sun Xu and
                  Yangming Zhao},
	title = {{SARS:} Towards minimizing average Coflow Completion Time in MapReduce
                  systems},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110429},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110429},
	doi = {10.1016/J.COMNET.2024.110429},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reducing the average Coflow Completion Time (CCT) plays a vital role to improve the responsiveness and throughput of distributed computing systems. However, most of the previous works on minimizing the average CCT scheduled all coflows following the Shortest Remaining Time First (SRTF) principle. In this paper, we argue that SRTF is not a necessary (even not a good) principle to follow in order to minimize the average CCT. Instead, we propose a Simulated Annealing based Reducer placement and coflow Scheduling approach named SARS to minimize the average CCT in MapReduce systems. In SARS, two powerful modules, i.e., the Neighboring State Searching (NSS) module and the System Energy Estimation (SEE) module, are designed to search a better coflow scheduling order and estimate the minimum average CCT given the scheduling order, respectively. Based on the interaction between these two modules, a near-optimal reducer placement and coflow scheduling scheme will be derived. Both testbed experiments and extensive simulations show that SARS can reduce the average CCT by up to 64.32% compared with the state-of-the-art technique.}
}


@article{DBLP:journals/cn/MontanariCZ24,
	author = {Antonio Montanari and
                  Filippo Campagnaro and
                  Michele Zorzi},
	title = {An adaptive MAC-agnostic ranging scheme for underwater acoustic mobile
                  networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110430},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110430},
	doi = {10.1016/J.COMNET.2024.110430},
	timestamp = {Tue, 18 Jun 2024 09:26:11 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MontanariCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the last ten years several simulation studies on Autonomous Underwater Vehicle (AUV) swarm fleet formation have been performed, and some preliminary sea demonstrations of proof-of-concept prototypes were carried out. However, their actual realization is hindered by the challenges imposed by the underwater acoustic channel and the difficulties of keeping track of the vehicles’ positions due to the long latency required by traditional Two-Way Travel-Time (TWTT) ranging measurements, that require a specific signaling, hence limiting the throughput of the underwater network. Although One-Way Travel-Time (OWTT) halves the latency, it requires a high precision oscillator, such as an atomic clock or an oven controlled crystal oscillator (OCXO), to be installed in each modem processing unit: while atomic clocks are still very expensive, OCXO are very power demanding, making their application to underwater acoustic networks not always possible, especially in the case of low cost vehicle swarms.}
}


@article{DBLP:journals/cn/ChaurasiaVV24,
	author = {Bhavana Chaurasia and
                  Anshul Verma and
                  Pradeepika Verma},
	title = {An in-depth and insightful exploration of failure detection in distributed
                  systems},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110432},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110432},
	doi = {10.1016/J.COMNET.2024.110432},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChaurasiaVV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In today’s world, everyone wants a good profit with a tiny investment and distributed computing is a boon for this purpose. Cloud computing, fog computing, and the Internet of Things (IoT) are well-known examples of distributed computing which provide good computing services and performance. However, providing reliable services in a real environment, which is failure-prone, remains a challenge. To address this issue, failure detectors are used in distributed systems, which are abstract modules responsible for detecting and monitoring the activity of nodes in order to determine whether they are faulty or not. In this paper, an approach is presented for the systematic literature review of failure detectors in distributed systems. Further, many existing review and survey papers on failure detectors are critically analyzed along with their key contributions and limitations. The classification of distributed systems is presented on the basis of the nodes’ properties and the components of system models are described in detail. Various issues and challenges related to agreement and failure problems are also explored. The strengths and limitations of various existing failure detectors are discussed along with their comparative evaluation. Finally, fault-tolerance and recovery techniques are discussed and analyzed.}
}


@article{DBLP:journals/cn/GorsheninKGK24,
	author = {Andrey Gorshenin and
                  Anastasia Kozlovskaya and
                  Sergey Gorbunov and
                  Irina A. Kochetkova},
	title = {Mobile network traffic analysis based on probability-informed machine
                  learning approach},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110433},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110433},
	doi = {10.1016/J.COMNET.2024.110433},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GorsheninKGK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper proposes an approach to the joint use of statistical and machine learning (ML) models to solve the problems of the precise reconstruction of historical events, real-time detection of ongoing incidents, and the prediction of future quality of service-related occurrences for prospective development of the modern networks. For forecasting, a regression version of the deep Gaussian mixture model (DGMM) is introduced. First, the preliminary clustering based on the finite normal mixtures is performed. This information is then used as an input for some supervised ML algorithm. It is the basic concept of the probability-informed ML approach in the field of telecommunications networks. Using the real-world datasets from a Portuguese mobile operator as well as public cellular traffic data, the article compares this approach with methods such as random forests, support vector machine regression, gradient boosting and LSTM. Vector autoregression, informed by the parameters of the generalized gamma (GG) distribution, which has also been successfully used to reconstruct past traffic patterns, is also used as a benchmark. We demonstrate that DGMM-based regression is 6.82–22.8 times faster than LSTM for the dataset. Moreover, DGMM-based regression can achieve better results for the most important traffic characteristics (average and total traffic, the number of users). For metrics MAPE and RMSE, it surpasses the results of statistical methods up to 46.7% (RMSE) and 91.5% (MAPE) (median increases are 28.0% and 80.1%, respectively), as well as for ML methods up to 13.0% (RMSE) and 35.7% (MAPE) (median increases are 0.39% and 2.5%, respectively). Thus, the use of a probability-informed approach for telecommunication data seems optimal for the computational speed and accuracy trade-off. Also, we introduce a novel statistical hypothesis testing method based on GG distribution for detecting suspected anomalies in traffic.}
}


@article{DBLP:journals/cn/MoreiraKB24,
	author = {Cristiano L. Moreira and
                  Carlos Alberto Kamienski and
                  Reinaldo A. C. Bianchi},
	title = {5G and edge: {A} reinforcement learning approach for Virtual Network
                  Embedding with cost optimization and improved acceptance rate},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110434},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110434},
	doi = {10.1016/J.COMNET.2024.110434},
	timestamp = {Mon, 03 Mar 2025 21:30:45 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MoreiraKB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G technologies are fueling a revolution across numerous industries, including manufacturing, healthcare, and entertainment, by enabling the development and deployment of novel applications at the network’s edge. To meet the demanding service level agreements of these industries, a dynamic and adaptable infrastructure strategy that combines cloud and edge computing models is needed. This hybrid approach offers the benefits of both centralized cloud processing and decentralized edge computing, optimized for responsiveness and efficiency. A key element for success is an orchestration mechanism that dynamically allocates resources to ensure the infrastructure can adapt to fluctuating demands in real time, optimizing resource utilization and meeting SLA requirements. Among these mechanisms, virtual network embedding (VNE) and dynamic resource management (DRM) have emerged as tools for defining where and how edge technology should be used. However, current VNE approaches struggle to adapt to real-time fluctuations in demand across geographically distributed edge resources. This work introduces a novel resource allocation algorithm, the VNE-CRS, which uses an Artificial Intelligence technique called Reinforcement Learning to orchestrate resources across multiple domains. This approach benefits from the strength of Reinforcement Learning: its ability to consider the entire problem from beginning to end while incorporating various aspects of 5G Quality of Service Indicators for optimal decision-making. Experiments were conducted to evaluate the performance of VNE-CRS against state-of-the-art algorithms for multi-domain edge environments. Results have shown that employing Reinforcement Learning techniques for VNE resource allocation yields performance gains of 12.32 percentage points in comparison with the GRC algorithm and 28.80 percentage points in comparison with the base edge environment, presenting an acceptability rate closer to the Public Cloud environment with all benefits of edge environment. In conclusion, VNE-CRS offers an efficient solution for resource allocation in 5G environments, achieving superior performance and transforming the VNE architecture into a comprehensive orchestration system that optimizes infrastructure utilization for strategic long-term benefits.}
}


@article{DBLP:journals/cn/ZhangWLL24,
	author = {Jin Zhang and
                  Daoye Wang and
                  Kai Liu and
                  Jianhua Lu},
	title = {Multi-Table Programmable Parser with online flow-level update consistency
                  for satellite networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110435},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110435},
	doi = {10.1016/J.COMNET.2024.110435},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable packet parsers in the space data plane, responsible for identifying protocols and extracting keywords on demand, are significant to realize protocol upgrading for satellite networks. Because flow-level update consistency is significant, onboard resources are limited, and space radiation may cause errors in packet parsers, programmable packet parsers with online flow-level update consistency, low complexity, and high reliability are urgently needed. In our previous work, a Multi-Table Programmable Parser (MTPP) [1] is designed for fast configuration and low storage cost by networking configuration and multiple tables. In this paper, we propose a Multi-Table Programmable Parser with Online Flow-Level Update Consistency (MTPP-OFLUC), consisting of five tables and eight models. To guarantee the flow-level update consistency, each table consists of a main table and a backup table. Moreover, to improve the reliability of MTPP-OFLUC, a fault tolerance mechanism is designed. Furthermore, an Online Configuration based on Super Parse Graph Segmentation (OC-SPGS) is proposed to achieve fast configuration with flow-level update consistency. The numerical analysis under typical parse graphs demonstrates that MTPP-OFLUC uses less storage than that of PISA-based parser. Due to backup tables adapted to guarantee the flow-level update consistency, the storage cost of MTPP-OFLUC is only 11.53% more than that of MTPP. Through OC-SPGS, the parsing success ratio of MTPP-OFLUC is up to 66.7% larger than that of MTPP. Finally, a prototype of MTPP-OFLUC is implemented on Xilinx Virtex-7 FPGA. The response time of MTPP-OFLUC and the network is 1.460 us and 4.196 ms, respectively. The size of configuration information is less than that of PISA-based parser. In summary, rapid response and flow-level update consistency are realized by MTPP-OFLUC. Moreover, security risks, user-friendly interfaces, potential capacity of upgrades, and supports for higher data rates still need to be considered in the future works.}
}


@article{DBLP:journals/cn/CentofantiTMGC24,
	author = {Carlo Centofanti and
                  Walter Tiberti and
                  Andrea Marotta and
                  Fabio Graziosi and
                  Dajana Cassioli},
	title = {Taming latency at the edge: {A} user-aware service placement approach},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110444},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110444},
	doi = {10.1016/J.COMNET.2024.110444},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CentofantiTMGC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern network and computing infrastructures are tasked with addressing the stringent demands of today’s applications. A pivotal concern is the minimization of latency experienced by end-users accessing services. While emerging network architectures provide a conducive setting for adept orchestration of microservices in terms of reliability, self-healing and resiliency, assimilating the awareness of the latency perceived by the user into placement decisions remains an unresolved problem. Current research addresses the problem of minimizing inter-service latency without any guarantee to the level of latency from the end-user to the cluster. In this research, we introduce an architectural approach for scheduling service workloads within a given cluster, prioritizing placement on the node that offers the lowest perceived latency to the end-user. To validate the proposed approach, we propose an implementation on Kubernetes (K8s), currently one of the most used workload orchestration platforms. Experimental results show that our approach effectively reduces the latency experienced by the end-user in a finite time without degrading the quality of service. We study the performance of the proposed approach analyzing different parameters with a particular focus on the size of the cluster and the number of replica pods involved to measure the latency. We provide insights on possible trade-offs between computational costs and convergence time.}
}


@article{DBLP:journals/cn/TsourdinisCMKNF24,
	author = {Theodoros Tsourdinis and
                  Ilias Chatzistefanidis and
                  Nikos Makris and
                  Thanasis Korakis and
                  Navid Nikaein and
                  Serge Fdida},
	title = {Service-aware real-time slicing for virtualized beyond 5G networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110445},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110445},
	doi = {10.1016/J.COMNET.2024.110445},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TsourdinisCMKNF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge Intelligence is expected to play a vital role in the evolution of 5G networks, empowering them with the capability to make real-time decisions regarding various allocations related to their management and service provisioning to end-users. This shift facilitates the transition from a network-aware approach, where applications are developed to manage network quality fluctuations, to a service-aware network that self-adjusts based on the hosted applications. In this paper, we design and implement a service-aware network managed from the network edge. We utilize and assess various Machine Learning models to classify cellular network traffic flows in the backhaul, aiming to predict their future impact on network load. Leveraging these predictions, the network can proactively and autonomously reallocate slices in the Radio Access Network via programmable APIs, ensuring the demands of the traffic-generating applications are met. The approach integrates innovative MLOps methodologies for distributed and online training, enabling continuous model refinement and adaptation to evolving network dynamics. Our framework was tested in a real-world environment with realistic traffic scenarios, and the results were evaluated in real-time, down to a granularity of 10ms. Our findings indicate that the network can swiftly adjust to traffic, providing users with slices tailored to their application needs. Notably, our experiments show that under the studied settings, the users experienced up to 4 times lower latency (jitter) and nearly 4 times higher throughput when interacting with various applications, compared to the standard non-AI/ML unit. Furthermore, our dynamic scheme significantly optimizes resource allocation, ensuring energy efficiency by avoiding over- and under-provisioning of resources.}
}


@article{DBLP:journals/cn/AlhidaifiAA24,
	author = {Saleh Mohamed Alhidaifi and
                  Muhammad Rizwan Asghar and
                  Imran Shafique Ansari},
	title = {Towards a Cyber Resilience Quantification Framework {(CRQF)} for {IT}
                  infrastructure},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110446},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110446},
	doi = {10.1016/J.COMNET.2024.110446},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AlhidaifiAA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber resilience quantification is the process of evaluating and measuring an organisation’s ability to withstand, adapt to, and recover from cyber-attacks. It involves estimating IT systems, networks, and response strategies to ensure robust defence and effective recovery mechanisms in the event of a cyber-attack. Quantifying cyber resilience can be difficult due to the constantly changing components of IT infrastructure. Traditional methods like vulnerability assessments and penetration testing may not be effective. Measuring cyber resilience is essential to evaluate and strengthen an organisation’s preparedness against evolving cyber-attacks. It helps identify weaknesses, allocate resources, and ensure the uninterrupted operation of critical systems and information. There are various methods for measuring cyber resilience, such as evaluating, teaming and testing, and creating simulated models. This article proposes a cyber resilience quantification framework for IT infrastructure that utilises a simulation approach. This approach enables organisations to simulate different attack scenarios, identify vulnerabilities, and improve their cyber resilience. The comparative analysis of cyber resilience factors highlights pre-configuration’s robust planning and adaptation (61.44%), buffering supported’s initial readiness (44.53%), and network topologies’ robust planning but weak recovery and adaptation (60.04% to 77.86%), underscoring the need for comprehensive enhancements across all phases. The utilisation of the proposed factors is crucial in conducting a comprehensive evaluation of IT infrastructure in the event of a cyber-attack.}
}


@article{DBLP:journals/cn/TanveerAKA24,
	author = {Muhammad Tanveer and
                  Abdallah Aldosary and
                  Neeraj Kumar and
                  Saud Alhajaj Aldossari},
	title = {SEAF-IoD: Secure and efficient user authentication framework for the
                  Internet of Drones},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110449},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110449},
	doi = {10.1016/J.COMNET.2024.110449},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/TanveerAKA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Drones (IoD) stands as a rapidly advancing technology, finding applications across civilian, military, and industrial domains, facilitating the efficient execution of challenging tasks. However, the IoD environment introduces a plethora of security concerns. Drones, for example, must exchange critical information with control station servers and users through open and unsecured wireless channels. Consequently, protecting these sensitive data becomes paramount, leading to the development of SEAF-IoD, a secure and efficient user authentication framework for the IoD, as presented in this paper. SEAF-IoD relies on symmetric encryption, physical uncloneable functions (PUF), and hash functions. This framework empowers users to securely establish connections and obtain real-time data from specific drones deployed within the IoD environment. Furthermore, the utilization of PUF holds great promise in achieving cost-effective security. PUF obviates the necessity to store confidential keys in device memory, offering a potential alternative for establishing a more secure and cost-efficient authentication framework for IoD systems. PUF empowers the SEAF-IoD to enhance its resilience against privileged insider attacks and drone/device capture attempts. To ensure the robustness of SEAF-IoD, it undergoes rigorous validation using BAN logic and Scyther simulation. Furthermore, an informal analysis affirms that SEAF-IoD fulfills critical security requirements and effectively thwarts various adversarial threats, including privileged insider, replay, and impersonation attacks. A comprehensive comparison of SEAF-IoD against existing schemes highlights its resilience and superiority in safeguarding IoD environments.}
}


@article{DBLP:journals/cn/HeR24,
	author = {Kaiyan He and
                  Zhe Ren},
	title = {A new three-factor authentication scheme using Chebyshev chaotic map
                  for peer-to-peer Industrial Internet of Things},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110450},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110450},
	doi = {10.1016/J.COMNET.2024.110450},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HeR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Industrial Internet of Things (IIoT) is an emerging technology for smart manufacturing and productivity improvement. Due to the increasing complexity of manufacturing, devices with smart industrial sensors from several domains work together on the same job, raising serious security and privacy concerns about peer-to-peer (P2P) communication. Specifically, P2P secure communication should be focused on when it comes to high efficiency and security of information exchange between two industrial sensors. However, none of the existing schemes can resist physical attacks towards industrial sensors. To resolve the problem, we propose a new three-factor authentication and key agreement scheme named NTFAK for P2P IIoT communication using Physical Unclonable Functions (PUF) and Chebyshev chaotic mapping, where PUF is used to replace the smart card factor in the traditional schemes. The unclonable property of PUF and its ability to securely identify a device by challenge-response can provide safety to physical attacks. Besides, the use of the Chebyshev chaotic map also can provide high computational efficiency while ensuring better security. The proposed scheme enables secure communication between any two industrial sensors, which can resist physical attacks and offer a smaller key size, faster computation, and higher efficiency for IIoT. The security of our scheme is formally proved by the random oracle model, and security properties are discussed to show our scheme resists various attacks. Moreover, experiments were implemented to prove the efficiency of our scheme. Specifically, the results show that NTFAK can improve the computation and communication overheads by 36.6% and 17.5% at least, compared to existing schemes, while also ensure a minimum time reduction of 30.4% in delay time. In terms of security and functionality features, NTFAK satisfies the proposed 14 properties, outperforming existing solutions.}
}


@article{DBLP:journals/cn/FanSLWZ24,
	author = {Huilong Fan and
                  Chongxiang Sun and
                  Jun Long and
                  Shangpeng Wang and
                  Fei Zeng},
	title = {A novel method for solving the multi-commodity flow problem on evolving
                  networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110451},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110451},
	doi = {10.1016/J.COMNET.2024.110451},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FanSLWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The minimum cost multi-commodity flow (MMCF) problem on evolving networks is the latest challenge to solving the min-cost network flow problem of multi-commodity flowing from multi-source and multi-sink on an evolving network. Previous researches primarily focus on the solution of MMCF problem within static networks. However, researchers ignored the MMCF problem within networks where the structure dynamically changes over time. Evolving networks have a dynamically changing and unpredictable network structure. The existing methods must recalculate when the network structure changes, leading to high computational complexity. This manuscript proposes a dynamic graph computing model for adaptive dynamic path selection and optimization. First, the model selects the path with the least cost for each commodity, assuming the route can transport a specified number of such commodities to find the optimal path faster. Second, we design path selection and flow update strategies for each node in the graph to adapt to changes in the graph structure. At the same time, we can prevent multiple commodity conflicts from exceeding the marginal capacity limit. Finally, the optimal path of each commodity is dynamically pruned based on the original optical path to obtain the tree structure and determine the absolute path to meet the specified transportation quantity of each commodity. Performance analysis results show that our method can efficiently cope with the dynamic changes of the network structure, which provides ideas for solving the multi-commodity network flow problem in dynamic networks.}
}


@article{DBLP:journals/cn/JerabekHR24,
	author = {Kamil Jer{\'{a}}bek and
                  Karel Hynek and
                  Ondrej Rysav{\'{y}}},
	title = {Comparative analysis of {DNS} over {HTTPS} detectors},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110452},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110452},
	doi = {10.1016/J.COMNET.2024.110452},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JerabekHR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DNS over HTTPS (DoH) is a protocol that encrypts DNS traffic to improve user privacy and security. However, its use also poses challenges for network operators and security analysts who need to detect and monitor network traffic for security purposes. Therefore, there are multiple DoH detection proposals that leverage machine learning to identify DoH connections; however, these proposals were often tested on different datasets, and their evaluation methodologies were not consistent enough to allow direct performance comparison. In this study, seven DoH detection proposals were recreated and evaluated with six different experiments to answer research questions that targeted specific deployment scenarios concerning ML-model transferability, usability, and longevity. For thorough testing, a large Collection of DoH datasets along with a novel 5-week dataset was used, which enabled the evaluation of models’ longevity. This study provides insights into the current state of DoH detection techniques and evaluates the models in scenarios that have not been previously tested. Therefore, this paper goes beyond classical replication studies and shows previously unknown properties of seven published DoH detectors.}
}


@article{DBLP:journals/cn/YanLZFZY24,
	author = {Dingyu Yan and
                  Yaping Liu and
                  Shuo Zhang and
                  Binxing Fang and
                  Feng Zhao and
                  Zhikai Yang},
	title = {{PCNP:} {A} RoCEv2 congestion control using precise {CNP}},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110453},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110453},
	doi = {10.1016/J.COMNET.2024.110453},
	timestamp = {Wed, 09 Oct 2024 17:15:28 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YanLZFZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of data centers, the traditional TCP/IP network stacks have become inadequate in addressing the high bandwidth and low latency demands of diverse upper-layer applications. Some cloud providers deploy RoCEv2 (RDMA over Converged Ethernet v2) technology to accelerate network transmission in their data centers, and the reliable network transmission in RoCEv2 relies on congestion control protocols. Currently, traditional congestion control protocols primarily employ heuristic rate adjustment strategies, which often exhibit inadequate performance in terms of convergence, stability, and fairness. Furthermore, the latest congestion control protocols rely on the support of costly switch hardware, which hinders the incremental expansion of data centers. This paper proposes PCNP, a precise congestion control protocol, it extends standard CNP (Congestion Notification Packet) and introduces a bounded rate adjustment strategy that leverages receiver information to achieve precise rate adjustment within existing data centers. Simulation results demonstrate that PCNP effectively decreases the rate fluctuation during the convergence process and enhances fairness. In comparison to DCQCN, HPCC and PCN, PCNP reduces the standard deviation of flow completion time under the incast traffic by 90%, 91% and 91%, respectively. Additionally, it can reduce the tail flow completion time by up to 83%, 40% and 46%.}
}


@article{DBLP:journals/cn/YungaicelaNaulaSS24,
	author = {Noe Marcelo Yungaicela{-}Naula and
                  Vishal Sharma and
                  Sandra Scott{-}Hayward},
	title = {Misconfiguration in {O-RAN:} Analysis of the impact of {AI/ML}},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110455},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110455},
	doi = {10.1016/J.COMNET.2024.110455},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YungaicelaNaulaSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User demand on network communication infrastructure has never been greater with applications such as extended reality, holographic telepresence, and wireless brain–computer interfaces challenging current networking capabilities. Open RAN (O-RAN) is critical to supporting new and anticipated uses of 6G and beyond. It promotes openness and standardization, increased flexibility through the disaggregation of Radio Access Network (RAN) components, supports programmability, flexibility, and scalability with technologies such as Software-Defined Networking (SDN), Network Function Virtualization (NFV), and cloud, and brings automation through the RAN Intelligent Controller (RIC). Furthermore, the use of xApps, rApps, and Artificial Intelligence/Machine Learning (AI/ML) within the RIC enables efficient management of complex RAN operations. However, due to the open nature of O-RAN and its support for heterogeneous systems, the possibility of misconfiguration problems becomes critical. In this paper, we present a thorough analysis of the potential misconfiguration issues in O-RAN with respect to integration and operation, the use of SDN and NFV, and, specifically, the use of AI/ML. The opportunity for AI/ML to be used to identify these misconfigurations is investigated. A case study is presented to illustrate the direct impact on the end user of conflicting policies amongst xApps along with a potential AI/ML-based solution to this problem. This research presents a first analysis of the impact of AI/ML on misconfiguration challenges in O-RAN.}
}


@article{DBLP:journals/cn/HaoNZ24,
	author = {Jialin Hao and
                  Rola Naja and
                  Djamal Zeghlache},
	title = {Adaptive federated reinforcement learning for critical realtime communications
                  in {UAV} assisted vehicular networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110456},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110456},
	doi = {10.1016/J.COMNET.2024.110456},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HaoNZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper sheds the light on road active safety measurements implemented in unmanned aerial vehicles assisted vehicular networks. Despite the great potential of deploying high computing drones, the drone battery life is the major concern, on one hand. On the other hand, road active safety is a critical real-time process that should be tackled in a tight time window in vehicular networks. To meet the mentioned concerns, we adopt federated machine learning on the local vehicles, sending local updates to drone servers. Moreover, a dynamic frequency adaptation framework is proposed to achieve the optimal trade-off between the road active safety performance and drone’s energy consumption. The thresholds for the local update frequency are calibrated according to road safety measurements (i.e., collision rate, risky and impolite driving time on the road) and drone energy consumption. Additionally, an accurate mathematical modeling based on M/G/1 multi-class was conducted in order to access the queuing time at the drone.}
}


@article{DBLP:journals/cn/LuMC24,
	author = {Yifei Lu and
                  Xu Ma and
                  Changjiang Cui},
	title = {{DCCS:} {A} dual congestion control signals based {TCP} for datacenter
                  networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110457},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110457},
	doi = {10.1016/J.COMNET.2024.110457},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuMC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As cloud computing advances, datacenter networks (DCNs) face unprecedented pressure due to the increasing prevalence of high bandwidth and low delay applications. Current reliable transmission protocols such as TCP harness packet loss, network delay, and Explicit Congestion Notification (ECN) for congestion control, aiming to maintain low latency and high throughput. We investigate the efficacy of ECN-based protocols against incast congestion in one-hop networks and delay-based protocols for maintaining low end-to-end delay in multi-hop networks, at the expense of throughput fairness. Thus, we introduce a novel TCP protocol, Dual Congestion Control Signals (DCCS), merging ECN and delay signals for a broader congestion control mechanism. DCCS effectively handles burst traffic, prevents packet loss, ensures low end-to-end delay, and resolves throughput unfairness by determining a precise minimum round-trip time (RTT) for each flow through a drain signal. Simulations on ns-3 confirm that DCCS surpasses ECN-based (i.e., DCTCP), delay-based (i.e., DC-Vegas and Swift), and hybrid-based protocols (i.e., EAR) in fat-tree topology under realistic workload, cutting down Flow Completion Time (FCT) by 3.8%–20.5%. This substantiates DCCS’s competency in providing high-quality service for DCNs.}
}


@article{DBLP:journals/cn/XiaLWW24,
	author = {Dongyu Xia and
                  Jiayi Liu and
                  Chen Wang and
                  Weihua Wu},
	title = {Comprehensive fault diagnosis for multiple coupled SFCs based on deep
                  learning},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110458},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110458},
	doi = {10.1016/J.COMNET.2024.110458},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XiaLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service Function Chains (SFCs) are deployed and executed in a complex network environment, where multiple SFCs are coupled by sharing various types of network functions and transmission links. As a result, diagnosing the state of these SFCs and accurately locating network fault becomes a challenge and plays a fundamental role in SFC management. In this paper, we introduce a comprehensive fault diagnosis mechanism for multiple coupled SFCs by applying the deep learning technique. By formulating the involved network nodes and transmission links into a heterogeneous graph model, we propose the Rendered Service Path Heterogeneous Graph Attention Network (RSP-HAN) model to comprehensively estimate the state of every network node and link. Typically, RSP-HAN first applies Bi-directional Long Short-Term Memory (Bi-LSTM) to normalize features from different types of network entities, then obtains the embedding results through intra-RSP and inter-RSP attention mechanisms. Finally, Multi-Layer Perceptron (MLP) is utilized to estimate the state of these network entities. We also setup an experiment platform and obtain extensive datasets to evaluate our RSP-HAN model. Experimental results show that RSP-HAN performs better in fault diagnosis for various types of network entities than several traditional methods and two deep learning-based baselines, the Graph Convolutional Network (GCN) and the Graph Attention Network (GAT).}
}


@article{DBLP:journals/cn/BhandariGTRSS24,
	author = {Abhay Bhandari and
                  Akhil Gupta and
                  Sudeep Tanwar and
                  Joel J. P. C. Rodrigues and
                  Ravi Sharma and
                  Anupam Singh},
	title = {Latency optimized {C-RAN} in optical backhaul and {RF} fronthaul architecture
                  for beyond 5G network: {A} comprehensive survey},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110459},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110459},
	doi = {10.1016/J.COMNET.2024.110459},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BhandariGTRSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need for high-speed data connection has significantly strained Mobile Network Operators (MNOs) due to an exponential increase in traffic. Numerous innovative technologies have been used to fulfill consumer service needs in capacity, high-speed data, network coverage, and Quality of Service (QoS), but they still require more thought. Because of several advantageous features in terms of resource utilization, service deployment, Capital Expenditure (CAPEX), Operating Expense (OPEX), and network management, the Cloud-based Radio Access Network (C-RAN) has played a significant role in mitigating the issues faced by mobile network operators. However, the C-RAN technology places high demands on the mobile fronthaul network due to the risk of connection interruptions. This article thoroughly analyzes the essential technologies, problems, design, needs, and solutions for effective fronthaul and backhaul communication a fifth-generation (5G) and beyond systems. There have been reviews of optical-based fronthaul and backhaul technologies. This article makes suggestions for ways to make fronthaul and backhaul communication systems more effective by lowering the costs, bandwidth, latency, and power requirements.}
}


@article{DBLP:journals/cn/JebriAZ24,
	author = {Sarra Jebri and
                  Arij Ben Amor and
                  Salah Zidi},
	title = {A seamless authentication for intra and inter metaverse platforms
                  using blockchain},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110460},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110460},
	doi = {10.1016/J.COMNET.2024.110460},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JebriAZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The metaverse is a concept that refers to a virtual reality space where people can interact with each other and their surroundings in a virtual world. It is a vital to prioritize security and adopt security measures in metaverse environment to protect users from potential risks and threats. By implementing strong mutual authentication and authorization controls, the metaverse can be a safer place where people can freely interact and engage without worrying about their security. In this paper, we aim to address the above challenges by integrating blockchain into the metaverse. Furthermore, the use of cryptography in blockchain ensures that each user’s identity and personal information remain safe and secure. Trust and confidence are ensured through a seamless mutual authentication algorithm between the server-avatar and the avatar–avatar in the intra and inter domains of the metaverse. Moreover, the proposed approach makes it possible to manage the personal information of avatars autonomously without resorting to a trusted authority. We prove that the proposed system can resist malicious security attacks and ensures privacy preservation by performing security analyses, using AVISPA tool and BAN logic. In addition, the results of computation cost, communication cost and round trip delay (RTD) prove that the performance of our proposal is better suited to the metaverse environment compared to competing systems.}
}


@article{DBLP:journals/cn/DjaidjaBBSG24,
	author = {Taki Eddine Toufik Djaidja and
                  Bouziane Brik and
                  Abdelwahab Boualouache and
                  Sidi Mohammed Senouci and
                  Yacine Ghamri{-}Doudane},
	title = {Time-efficient detection of false position attack in 5G and beyond
                  vehicular networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110461},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110461},
	doi = {10.1016/J.COMNET.2024.110461},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DjaidjaBBSG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G-V2X-enabled transportation systems rely on seamless cooperation between vehicles, infrastructure, and pedestrians, facilitated by the exchange of real-time position and state information among these entities. Misbehaving vehicles try injecting bogus messages into the network, thereby compromising its reliability and security. This work proposes an efficient Deep Learning (DL)-based Misbehavior Detection System (MDS) that leverages the use of Recurrent Neural Networks (RNNs) to analyze message consistency and detect bogus information in 5G-V2X networks. We highlight the significance of incorporating historical data to analyze consistency. Additionally, we emphasize the importance of evaluating the computational overhead induced by MDSs in V2X networks, given the critical need for low latency communication. We validate our work through both experimental and theoretical studies and compare it to existing works. The obtained results show the effectiveness of our work, achieving an accuracy of 95% in detecting false information injection attacks. Additionally, we guarantee a low required time/computing complexity of O(1), thereby avoiding significant overhead impacting the end-to-end latency when exchanging CAMs.}
}


@article{DBLP:journals/cn/LeeLPLJ24,
	author = {Ki{-}Hun Lee and
                  Seungmin Lee and
                  Jaedon Park and
                  Howon Lee and
                  Bang Chul Jung},
	title = {{MUSCAT:} Distributed multi-agent Q-learning-based minimum span channel
                  allocation technique for UAV-enabled wireless networks},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110462},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110462},
	doi = {10.1016/J.COMNET.2024.110462},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LeeLPLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a minimum span channel allocation problem (MS-CAP) to overcome spectrum scarcity and facilitate the efficiency of unmanned aerial vehicle (UAV)-enabled wireless networks. Basically, the MS-CAP minimizes the difference between the maximum and minimum used frequency, i.e., the required total bandwidth, while guaranteeing the quality-of-service (QoS) requirements for each wireless link in the network. The conventional optimal minimum span channel allocation (MS-CA) scheme is based on a centralized approach, assuming that global network information is available at the central controller. In practice, however, this may not be feasible for dynamic environments like UAV-enabled wireless networks since the real-time exchange of network information and channel allocation results with dynamically moving UAVs is formidable. Hence, we propose a novel practical MS-CA algorithm based on distributed multi-agent reinforcement learning (MARL), where each agent independently learns its best strategy with its local observations. To the best of our knowledge, the proposed technique is the first work of designing a distributed MARL for the MS-CAP for multi-UAV-enabled wireless networks in the literature. Numerical results reveal that the proposed distributed MS-CA technique can efficiently save the required total bandwidth while ensuring the QoS requirements of each link, represented by the signal-to-interference plus noise ratio (SINR) threshold, even in dynamic wireless networks. It validates the applicability of the proposed distributed MS-CA framework to dynamic networks.}
}


@article{DBLP:journals/cn/LiuLDZLAZLL24,
	author = {Jindian Liu and
                  Zhuo Li and
                  Huipeng Du and
                  Haodong Zhou and
                  Leyang Li and
                  Yi An and
                  Yu Zhang and
                  Kaihua Liu and
                  Qiang Li},
	title = {An effective and accurate flow size measurement using funnel-shaped
                  sketch},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110467},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110467},
	doi = {10.1016/J.COMNET.2024.110467},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuLDZLAZLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate measurement of network traffic is an essential part of current network management tasks. Sketch is a kind of probabilistic data structure widely accepted in network measurement. However, it is challenging for classic sketches to reduce the storage cost with little loss of accuracy for highly skewed network traffic. Therefore, most sketch-based schemes store elephant flows and mouse flows separately to deal with the skewed network traffic. But they are not usually suitable for estimating the size of mouse flows and thus may lose some information of traffic. To this end, a novel sketch, called Funnel Sketch (FS), is proposed in this paper. FS utilizes a funnel-shaped architecture to separately store the elephant flows and mouse flows, while maintaining the mouse flows as possible. Thus, FS can not only adapt to skewed network traffic to improve memory efficiency, but also accurately estimate the size of mouse and elephant flows with efficient memory consumption. Moreover, FS is implemented on CPU and OVS platform to evaluate its performance. The experimental results show that FS greatly reduces the error by 90% for flow size estimation and elephant flow detection compared with the state-of-the-art sketch-based schemes.}
}


@article{DBLP:journals/cn/PangSX24,
	author = {Zengyu Pang and
                  Ligen Shi and
                  Hua Xiang},
	title = {Asynchronous verifiable information dispersal protocol of achieving
                  optimal communication},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110470},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110470},
	doi = {10.1016/J.COMNET.2024.110470},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PangSX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The evolution of distributed systems necessitates robust protocols to facilitate secure and efficient data storage and retrieval amid challenges like asynchronous communication and adversarial threats. However, achieving the theoretical lower bounds of communication cost in Asynchronous Verifiable Information Dispersal (AVID) protocols remains an unresolved issue, prompting the need for innovation in this domain.}
}


@article{DBLP:journals/cn/LiuCZ24,
	author = {Yuanyuan Liu and
                  Kexin Chen and
                  Lu Zhu},
	title = {Efficient federated learning algorithm using sparse ternary compression
                  based on layer variation classification},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110471},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110471},
	doi = {10.1016/J.COMNET.2024.110471},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning is a distributed machine learning technique that ensures user privacy and enables multiple clients to jointly train a shared global model without transmitting local data. However, the frequent exchange of model parameters between numerous clients and the server results in heavy network delay and bandwidth limitation in federated learning. In view of that, we propose an efficient algorithm for federated learning using sparse ternary compression based on layer variation classification(LVC). First, use layer variation as a metric to assess the significance of each layer of the model parameters, and after client training, categorize the model parameters into different levels by the layer variation and sensitivity analysis. Then, during the upstream and downstream transmission of model parameters, we assign corresponding sparse and ternary quantization ratios for different levels to maximize compression efficiency while preserving crucial parameters. Finally, on the server side, a majority-layer aggregation strategy is adopted to further reduce the communication cost. Experimental results from image classification tasks conducted on MNIST and Fashion-MNIST datasets demonstrate that proposed LVC algorithm achieves high accuracy with minimal communication cost, thereby striking an optimal balance between communication efficiency and accuracy.}
}


@article{DBLP:journals/cn/SalimYP24,
	author = {Mikail Mohammed Salim and
                  Laurence Tianruo Yang and
                  Jong Hyuk Park},
	title = {Privacy-preserving and scalable federated blockchain scheme for healthcare
                  4.0},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110472},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110472},
	doi = {10.1016/J.COMNET.2024.110472},
	timestamp = {Wed, 12 Jun 2024 10:58:42 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SalimYP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT plays a significant role in the growth of clinical data for identifying hazardous diseases and building drugs for patient diagnosis and medical care. Blockchain and federated learning are widely proposed in existing studies for localized data training and storage in a secure, decentralized environment. However, several studies rely on the federated averaging method in federated learning, which introduces high energy consumption during several training rounds. Frequent transactions for storing patient records in blockchain and smart contract processing present network congestion challenges. In this paper, we propose a privacy-preserving federated learning and scalable blockchain scheme. First, we present a satisfaction scoring method for model aggregation to improve energy efficiency during the federated learning process. Secondly, we design an Ethereum-based blockchain network with sidechains to process smart contract transactions separately and reduce computation overload in the blockchain mainchain. Evaluation of the proposed scheme is compared with the baseline federated average method and transaction processing of smart contracts with the Ethereum main chain. Results demonstrate reduced CPU consumption using the proposed model aggregation method, an improvement over the federated average method, and an improvement of 43.65 % in transaction processing speed using two sidechains ensuring side.}
}


@article{DBLP:journals/cn/YuTXSW24,
	author = {Linxiao Yu and
                  Jun Tao and
                  Yifan Xu and
                  Weice Sun and
                  Zuyan Wang},
	title = {{TLS} fingerprint for encrypted malicious traffic detection with attributed
                  graph kernel},
	journal = {Comput. Networks},
	volume = {247},
	pages = {110475},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110475},
	doi = {10.1016/J.COMNET.2024.110475},
	timestamp = {Thu, 02 Jan 2025 19:03:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YuTXSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, more and more applications have adopted security protocols like Transport Layer Security (TLS) for data encryption. However, these privacy-enhancing approaches have also been abused by the attackers to deliver malicious payloads. Many existing encrypted network classification methods suffer from the imbalanced volume of normal and malicious traffic, which leads to bad model robustness. In this paper, we propose a novel TLS fingerprinting approach to capture the characteristics of encrypted network traffic. The fingerprints are attributed graphs obtained from TLS sessions, which can simultaneously take into consideration the sequential and statistical features of these sessions. As the communication patterns of different applications differ considerably, the graphs representing TLS connections could be used to characterize the network with the help of the graph kernel method, which results in a model with high accuracy in malicious TLS session detection and application discrimination. Moreover, we adopt Locality-Sensitive Hashing (LSH) and filtering techniques to reduce the time cost of our model. Model evaluation on real-world datasets shows that our model is more robust than existing methods presented in this work when the malicious traffic takes up an extremely small portion of the whole traffic.}
}
