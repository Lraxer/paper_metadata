@article{DBLP:journals/cn/BiSLZW23,
	author = {Xiang Bi and
                  Xiaokai Sun and
                  Zengwei Lyu and
                  Benhong Zhang and
                  Xing Wei},
	title = {A back adjustment based dependent task offloading scheduling algorithm
                  with fairness constraints in {VEC} networks},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109552},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2022.109552},
	doi = {10.1016/J.COMNET.2022.109552},
	timestamp = {Fri, 17 Mar 2023 14:56:58 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BiSLZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Vehicular Edge Computing (VEC) networks, task offloading scheduling has been drawing more and more attention as an effective way to relieve the computational burden of vehicles. However, with the intelligent and networked development of vehicles, the complex data dependency between in-vehicle tasks brings challenges to offloading scheduling. Moreover, scheduling fairness has a growing impact on the average Quality of Service (QoS) of vehicles in the network. To this end, we propose a dependent task offloading scheduling algorithm with fairness constraints based on a back adjustment mechanism. First, to solve the execution constraint problem caused by dependent tasks and the scheduling fairness problem in multi-user scenarios, a two-level task sorting algorithm is given to determine the scheduling sequence of tasks. Then, the sequential task offloading scheduling process is modeled as a Markov Decision Process (MDP) and solved by a reinforcement learning method. Finally, a back adjustment mechanism is designed to resort the task sequence and achieve the required scheduling fairness by iterative process. The simulation results show that the proposed algorithm significantly improves the scheduling fairness and reduces the average application completion time compared with other algorithms.}
}


@article{DBLP:journals/cn/ZhuCJWPZ23,
	author = {Konglin Zhu and
                  Wentao Chen and
                  Lei Jiao and
                  Jiaxing Wang and
                  Yuyang Peng and
                  Lin Zhang},
	title = {Online training data acquisition for federated learning in cloud-edge
                  networks},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109556},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109556},
	doi = {10.1016/J.COMNET.2023.109556},
	timestamp = {Mon, 26 Jun 2023 20:51:09 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuCJWPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an effective approach to exploiting different data sources for collaborative model training while maintaining the privacy of users. Adequate data is necessary to improve the accuracy of the federated learning. However, it is difficult for all data sources to acquire sufficient data for each iteration of federated training. In this paper, we study the data acquisition problem in cloud–edge networks, where edges acquire data from users and conduct the local training while the cloud aggregates the local models. Since data acquired by users with uncertainty, it is not easy to make the data acquisition decision for the FL online. Even worse, due to the unknown cost of data, it is difficult to make a macro-timescale decision. We propose a two-timescale online scheduling for federated learning to confront the uncertainty of the data acquisition. By learning empirical state information of the system with a carefully designed Lyapunov virtual queue and coordinating the data acquisition in different timescales in an online manner, the proposed approach minimizes the data acquisition cost of federated learning, reduces the data transmission delay and accelerates the convergence speed of federated learning. Rigorous theoretical analysis shows strong performance guarantees of the proposed two-timescale Lyapunov optimization algorithm and extensive trace-driven experimental results suggests that the algorithm achieves outstanding performance gains over existing benchmarks.}
}


@article{DBLP:journals/cn/MonfaredS23,
	author = {Saleh Khalaj Monfared and
                  Saeed Shokrollahi},
	title = {{DARVAN:} {A} fully decentralized anonymous and reliable routing for
                  VANets},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109561},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109561},
	doi = {10.1016/J.COMNET.2023.109561},
	timestamp = {Tue, 21 Mar 2023 21:08:27 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MonfaredS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the recent advances in the automobile industry and the emergence of smart self-driving vehicles, the increasing need for proper and optimal Vehicular Ad-hoc Network (VANet) protocols has drawn the attention of many researchers. Nevertheless, the VANet community has yet to agree on a unified and suitable set of protocols for such networks. One of the major concerns in these networks is security considerations, which are significant because of the specific characteristics of VANet. In this article, we aim to address the location privacy and reliability issues in VANet routing protocols. We propose DARVAN, a fully decentralized infrastructure that provides anonymous and reliable routing in VANets. By employing a distributed database and collective consensus, DARVAN minimizes the exposure of critical data, which is conventionally stored and processed in centralized units. DARVAN is deployed by modifying the I2P protocol, which aims to improve routing reliability and resilience to many adversary activities in VANets. Specifically, with DARVAN, we present an effective and efficient network-level mitigation for Sybil attacks in Vanets. Our extensive simulations on NS3 show that DARVAN performs well in terms of packet delivery ratio, overhead, delay, and reliability compared to the previous anonymous scheme proposed for VANet routing.}
}


@article{DBLP:journals/cn/QinGLZX23,
	author = {Yudong Qin and
                  Deke Guo and
                  Lailong Luo and
                  Jingyu Zhang and
                  Ming Xu},
	title = {Service function chain migration with the long-term budget in dynamic
                  networks},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109563},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109563},
	doi = {10.1016/J.COMNET.2023.109563},
	timestamp = {Sat, 13 May 2023 01:07:10 +0200},
	biburl = {https://dblp.org/rec/journals/cn/QinGLZX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing emerges as a new paradigm to provide low-latency network services in the close proximity to users. Based on the network function virtualization (NFV) technology, network services can be flexibly provisioned as service function chain (SFC) deployed at edge servers. In some scenarios, such as the vehicular or UAV-assisted edge computing, the network topology varies rapidly due to the mobile edge servers, which changes the routing path between adjacent VNFs in an SFC. Migrating SFC to adapt to the frequent topology change can reduce the SFC latency, and improve the quality of users’ experience. However, frequent SFC migration will unavoidably increase the operation cost. In this paper, to optimize the system performance in a cost-efficient manner, we study the SFC migration problem in dynamic networks with a long-term cost budget constraint. We then propose the Topology-aware Min-latency SFC Migration (TMSM) method to strike a desirable balance between the SFC latency and the migration cost. Specifically, we first apply the Lyapunov optimization to decompose the long-term optimization problem into a series of real-time optimization sub-problems. Since the decomposed problem is still NP-hard, a Markov approximation based heuristic is proposed to seek a near-optimal solution for each sub-problem. Compared with the rerouting-only strategy, which does not migrate any VNF, our TMSM reduces the latency by at least 21% on average in each time slot. Extensive evaluations show that the proposed algorithm achieves a better tradeoff between the SFC latency and migration cost than the baselines.}
}


@article{DBLP:journals/cn/JuSXW23,
	author = {Xiang Ju and
                  Shengchao Su and
                  Chaojie Xu and
                  Haoxuan Wang},
	title = {Computation offloading and tasks scheduling for the internet of vehicles
                  in edge computing: {A} deep reinforcement learning-based pointer network
                  approach},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109572},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109572},
	doi = {10.1016/J.COMNET.2023.109572},
	timestamp = {Fri, 17 Mar 2023 14:56:58 +0100},
	biburl = {https://dblp.org/rec/journals/cn/JuSXW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Vehicles, vehicles can offload computation tasks to edge servers for execution. So, execution delay of tasks and energy consumption of vehicles can be reduced. However, the current research on computation offloading has not fully considered the priority of computation offloading and tasks scheduling within the server. In this regard, a computation offloading and task scheduling scheme based on pointer network was proposed in the paper. This scheme can maximize the number of offloading executions of computation tasks with limited computing resources of the edge server while satisfying the priority of computation offloading. First, depending on whether the task can be executed in the vehicle's device, we divided the task into two types and gave a two-stage offloading policy. Next, according to the characteristics of the scheduling problem, a task offloading decision and scheduling scheme based on pointer network was proposed. Then, considering the uncertainty of the number of tasks, the extra time delay caused by task offloading, the waiting time of tasks, and the complexity of task scheduling and computing resource allocation, a deep reinforcement learning algorithm was used to train the pointer network. Finally, the trained pointer network was used for task offloading decision-making and scheduling. The experimental results revealed the effectiveness of the scheme proposed in this study.}
}


@article{DBLP:journals/cn/ShengHL23,
	author = {Siyuan Sheng and
                  Qun Huang and
                  Patrick P. C. Lee},
	title = {A general delta-based in-band network telemetry framework with extremely
                  low bandwidth overhead},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109573},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109573},
	doi = {10.1016/J.COMNET.2023.109573},
	timestamp = {Tue, 28 Mar 2023 19:51:08 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShengHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-band network telemetry (INT) enriches network management at scale through the embedding of complete device-internal states into each packet along its forwarding path, yet such embedding of INT information also incurs significant bandwidth overhead in the data plane. We propose DeltaINT, a general INT framework that achieves extremely low bandwidth overhead and supports various packet-level and flow-level applications in network management. DeltaINTbuilds on the insight that state changes are often negligible at most time, so it embeds the complete state information into a packet only when the state change is deemed significant. We propose two variants for DeltaINTthat trade between bandwidth usage and measurement accuracy, while both variants achieve significantly lower bandwidth overhead than the original INT framework. We theoretically derive the time/space complexities and the guarantees of bandwidth mitigation for DeltaINT. We implement DeltaINTin both software and P4. Our evaluation shows that DeltaINTsignificantly mitigates the bandwidth overhead, and the deployment in a Tofino switch incurs limited hardware resource usage.}
}


@article{DBLP:journals/cn/XuX23,
	author = {Dahu Xu and
                  Ding Xu},
	title = {Cooperative task offloading and resource allocation for UAV-enabled
                  mobile edge computing systems},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109574},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109574},
	doi = {10.1016/J.COMNET.2023.109574},
	timestamp = {Tue, 28 Mar 2023 19:51:08 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) systems can provide flexible computation services to user equipments (UEs). On the one hand, the UAV can be flexibly deployed to provide computation services to UEs in remote areas or where intensive computing is required. On the other hand, the UAV can approach UEs conveniently to enhance the offloading performance. However, when UEs are widely distributed, the UEs will consume high energy to offload tasks to the UAV, or the UAV will need to consume significant energy to fly close to different UEs to ensure reliable computation offloading, which is unfriendly to the energy-constrained UAV. To tackle these issues, in this paper, we propose a cooperative task offloading scheme for the UAV-enabled MEC systems. Specifically, we consider a UAV-enabled MEC system consisting of a UAV serving as a MEC server for multiple near and far UEs, where each UE has a dividable computation task to be computed and can offload a part of the task to the UAV for computing. The task offloading of each far UE is proposed to be assisted by an associated near UE, where each far UE first sends its task to the associated near UE, and then the near UE offloads its own task and the task from the far UE to the UAV. An iterative algorithm based on the block coordinate descent method is proposed to optimize the UAV’s trajectory, the computation and communication resources for minimizing the weighted sum energy consumption of the UEs and the UAV. Specifically, the UAV’s trajectory is optimized based on the successive convex approximation method, and the computation and communication resources are optimized via the Lagrangian dual method. Simulation results are presented to verify the effectiveness of the proposed algorithm. It is shown that compared to the state-of-the-art algorithms in existing literature, the proposed algorithm achieves much lower energy consumption, especially when the UEs carry more task data.}
}


@article{DBLP:journals/cn/TruongN23,
	author = {Truong Van Truong and
                  Anand Nayyar},
	title = {System performance and optimization in {NOMA} mobile edge computing
                  surveillance network using {GA} and {PSO}},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109575},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109575},
	doi = {10.1016/J.COMNET.2023.109575},
	timestamp = {Tue, 21 Mar 2023 21:08:29 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TruongN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the computing process gradually move to the edge network, close to the physical data source, to serve applications that require large computations with very little latency. However, the terminal wireless devices’ limited computing and energy resources pose obstacles to the practical implementation of these applications. Mobile Edge Computing (MEC) based non-orthogonal multiple access (NOMA) technology is solving this problem well thanks to its ability to serve many users with high data rates and spectrum utilization efficiency. This study investigates the performance and optimization of MEC surveillance systems using NOMA. Specifically, two camera units (CUs) perform the monitoring task to be accomplished by the MEC access point (AP) through Rayleigh fading wireless links. We then proposed the four-phase protocol for this system. Accordingly, we derive the closed-form exact expressions of the successful computation probability (SCP) and study the impact of the network parameters on the system performance. Furthermore, we propose and compare three meta-heuristic-based algorithms, namely MSCP-GA, MSCP-PSO, and MSCP-HGAPSO, to find the optimal parameters set to help the proposed system achieve the maximum SCP. The results show that the proposed algorithms can significantly improve the system’s performance by 40% higher than when the optimal algorithm is not used. Insights into the pros and cons of different algorithms are also discussed in this study. Finally, we use the Monte-Carlo simulation to verify the correctness of this study.}
}


@article{DBLP:journals/cn/CampoloGSM23,
	author = {Claudia Campolo and
                  Giacomo Genovese and
                  Gurtaj Singh and
                  Antonella Molinaro},
	title = {Scalable and interoperable edge-based federated learning in IoT contexts},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109576},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109576},
	doi = {10.1016/J.COMNET.2023.109576},
	timestamp = {Tue, 07 May 2024 20:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CampoloGSM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The analysis of data coming from massively deployed Internet of Things (IoT) devices pave the way to a myriad of intelligent applications in several vertical domains. Federated Learning (FL) has been recently proposed as a prominent solution to train Machine Learning (ML) models directly on top of devices (FL clients) generating data, instead of moving them to centralized servers in charge of training procedures. FL provides inherent benefits, mainly in terms of privacy preservation and reduction of network congestion for datasets exchange. Despite the huge recent research efforts, it still faces challenges for a practical implementation effectively targeting low communication footprint, robustness and interoperability. To fill this gap, in this work we propose a novel comprehensive framework built upon the Message Queue Telemetry Transport (MQTT) publish/subscribe messaging protocol and the Open Mobile Alliance (OMA) Lightweight Machine-to-Machine (LwM2M) semantics to facilitate FL operations and make them more suited to handle IoT devices acting as FL clients. The viability of the proposal as well as its communication efficiency compared to a literature solution are evaluated through a realistic Proof-of-Concept (PoC) under different link settings and for different datasets.}
}


@article{DBLP:journals/cn/AghapourST23,
	author = {Zahra Aghapour and
                  Saeed Sharifian and
                  Hassan Taheri},
	title = {Task offloading and resource allocation algorithm based on deep reinforcement
                  learning for distributed {AI} execution tasks in IoT edge computing
                  environments},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109577},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109577},
	doi = {10.1016/J.COMNET.2023.109577},
	timestamp = {Fri, 17 Mar 2023 14:56:58 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AghapourST23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the application of Artificial Intelligence (AI) in the Internet of Things (IoT) devices is increasing. As these devices are limited in processing and storing massive computations of AI applications, researchers are searching for methods to overcome these limitations. One of these applications is Convolutional Neural Network (CNN) processing, which is common in object detection and image classification. A neural network consists of layers with a large number of neurons and requires high processing power to run. A CNN can be partitioned into segments and offloaded as tasks of IoT devices to cloudlet servers on the edge. By utilizing the edge servers available in the environment, the total latency in the system and energy consumption by IoT devices can be optimized. Making decisions about offloading CNN segmented layers and allocating resources to each of them is the challenge. In this paper, we propose a method based on deep reinforcement learning that divides the offloading and resource allocation problem into two minor problems. This algorithm updates the offloading policy based on information from the environment, and with the help of the Salp Swarm Algorithm (SSA), optimizes resource allocation. The proposed method is tested for different deep-learning tasks of IoT devices under different capacities of cloudlet servers. The simulation results show the proposed algorithm has the least cost in terms of latency and power consumption and on average has improved 92%, 17%, and 12% compared to full local, full offload, and Jointly Resource allocation and computation Offloading PSO (JROPSO) methods respectively.}
}


@article{DBLP:journals/cn/LiFM23,
	author = {Shuyang Li and
                  Gianluca Francini and
                  Enrico Magli},
	title = {Temporal dynamics clustering for analyzing cell behavior in mobile
                  networks},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109578},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109578},
	doi = {10.1016/J.COMNET.2023.109578},
	timestamp = {Sat, 30 Sep 2023 10:07:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiFM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, management and resource allocation of mobile networks have become crucial because of a dramatic growth of mobile traffic. For Internet service providers, resource allocation is a challenging problem due to the inhomogeneous distributed nature of user demand, which increases the difficulty of configuring base stations in complicated urban environments. To manage mobile networks in an efficient way, understanding the typical usage behavior is very valuable for network operators. This paper aims at providing tools to analyze the behavior of mobile network cells in an urban environment. To achieve this goal, we propose a new time series clustering algorithm denoted as temporal dynamics clustering. Unlike conventional clustering approaches, this algorithm generates a new representation of time series by summarizing the distribution of the sequence of differences; in this way, temporal dynamics clustering creates clusters based on the variability of time series. We apply the proposed algorithm on a real-world mobile network dataset, showing its superior performance and much faster running time with respect to conventional methods such as hierarchical clustering and K-medoids. We also train neural networks on the clusters generated by temporal dynamics clustering, and the results show that the forecasting difficulty of time series is closely related to its temporal dynamics. Using this approach, we have analyzed the cell behavior with two weeks observation collected in Turin, and obtained interpretable results, in that the behavior of each cell is strongly related to the characteristics of the area and the related human activity. We also study the cell behavior in different time slots by considering time-dependent characteristic of human activity. Through experiments, we show the effectiveness of the proposed algorithm at providing a deeper understanding of traffic usage patterns in intricate urban environments.}
}


@article{DBLP:journals/cn/GulerKU23,
	author = {Evrim Guler and
                  Murat Karakus and
                  Suleyman Uludag},
	title = {Blockchain-enhanced cross-ISP spectrum assignment framework in SDONs:
                  SpectrumChain},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109579},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109579},
	doi = {10.1016/J.COMNET.2023.109579},
	timestamp = {Mon, 28 Aug 2023 21:39:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GulerKU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Huge potentials of optical networks provide an excellent choice in meeting the exponential growth in bandwidth demand. In particular, Elastic Optical Networks (EONs), with its promising potentials in increased availability, failure resilience, load-balancing, and efficient resource allocations, are good candidates for the future high-speed networks. When EONs are augmented with the emerging Software Defined Networking (SDN) architecture, which decouples the data and control planes, an agile and synergistic combination emerges under a general term of Software Defined Optical Networking (SDON), albeit with operational challenges such optimal and efficient spectrum assignments. In this paper, we present a blockchain-enhanced QoS-concentrated cross-Internet Service Provider (ISP) spectrum assignment framework for SDONs, SpectrumChain (SC), to eliminate centralized mediators while coordinating QoS-based inter-ISP traffic. This study introduces a new blockchain use-case with reduced QoS signaling overhead while performing inter-ISP routing in SDONs. We also experience the proposed framework under Hop-by-Hop Wavelength Switching (HWS), Border-Node-Only Wavelength Switching (BWS), and No Wavelength Switching (NWS) scenarios to analyze its performance. Our extensive simulation results show that the SC framework can effectively handle the QoS-enabled inter-ISP routing in SDON architecture in terms of Flow Setup Time (FST), Messages Processed (MEP), Requests Serviced (RS), and Acceptance Ratio (AR) metrics under HWS, BWS, and NWS scenarios.}
}


@article{DBLP:journals/cn/PatelD23,
	author = {Chintan Patel and
                  Nishant Doshi},
	title = {LDA-2IoT: {A} level dependent authentication using two factor for
                  IoT paradigm},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109580},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109580},
	doi = {10.1016/J.COMNET.2023.109580},
	timestamp = {Tue, 07 May 2024 20:23:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PatelD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread expansion of IoT-based services is transforming people’s living habits. With the vast data generation and intelligent decision making system, IoT supports many industries to improve their quality of services. The major challenge for IoT developers is to design a secure data transmission system and a trustworthy inter-device and user-device communication system. The data starts its journey from the sensing devices and reaches the user dashboard via a different medium. An authentication among IoT devices provides a reliable and lightweight key generation system. This paper puts forward a novel authentication approach for the IoT paradigm and postulates an ECC-based two-factor Level Dependent Authentication for Generic IoT (LDA-2IoT) in which users at a particular level in the hierarchy can access the sensors deployed below or at the equal level of the hierarchy. Security analysis is given based on the Dolev–Yao channel and widely accepted random oracle-based ROR model. A real-time implementation of the proposed scheme with MQTT protocol over a lab IoT environment shows that it is highly efficient in terms of network throughput, computation cost, and communication cost compared to existing schemes.}
}


@article{DBLP:journals/cn/ShiAB23,
	author = {Yao Shi and
                  Emad Alsusa and
                  Mohammed W. Baidas},
	title = {On the application of uplink/downlink decoupled access in heterogeneous
                  mobile edge computing},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109593},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109593},
	doi = {10.1016/J.COMNET.2023.109593},
	timestamp = {Sun, 04 Aug 2024 19:48:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShiAB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) is a key player in low latency 5G networks with the task to resolve the conflict between computationally-intensive mobile applications and resource-limited mobile devices (MDs). As such, there has been intense interest in this topic, especially in multi-user single-server and homogeneous multi-server scenarios. However, the research in the heterogeneous multi-server scenario is limited, where the servers are located at small base-stations (SBSs), macro base-stations (MBSs), or the cloud with different computing and communication capabilities. On the other hand, computational-tasks offloading is limited by the type of MD-BS association with almost all previous works focusing on offloading the MD’s computational tasks to the MEC servers/cloudlets at its serving BS. However, in multi-BS association, or downlink/uplink decoupled (DUDe) scenarios, an MD can be served by multiple BSs and hence has multiple offloading choices. Motivated by this, we proposed a joint BS association and subchannel allocation algorithm based on a student-project allocation (SPA) matching approach to minimize the network sum-latency, which breaks the constraint that one MD must connect to the same BS in the UL and DL, and jointly consider the communication and computational disparity of SBS and MBS cloudlets in heterogeneous MEC networks. Moreover, an optimal power allocation scheme is proposed to optimize the system performance subject to the predefined quality of service constraints. Our results show that the proposed scheme is superior to benchmark techniques in enabling the effective use of computational and communication resources in heterogeneous MEC networks.}
}


@article{DBLP:journals/cn/HoangDNVL23,
	author = {Tran Manh Hoang and
                  Le The Dung and
                  Ba Cao Nguyen and
                  Nguyen Van Vinh and
                  Gia Thien Luu},
	title = {Secrecy analysis of cooperative {NOMA-FDR} systems with imperfect
                  {CSI} and colluding eavesdroppers},
	journal = {Comput. Networks},
	volume = {223},
	pages = {109594},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109594},
	doi = {10.1016/J.COMNET.2023.109594},
	timestamp = {Mon, 05 Feb 2024 20:24:12 +0100},
	biburl = {https://dblp.org/rec/journals/cn/HoangDNVL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper analyzes the secrecy attributes of a full-duplex relay (FDR) non-orthogonal multiple access (NOMA) system, where the best relay is chosen from a set of numerous relays by using the partial relay selection (PRS) protocol. This best relay assists the communion from source to two NOMA users under the attendance of many colluding eavesdroppers. We obtain the expressions in closed-form of the exact secure outage probability (SOP) of each user, the approximate SOP, and the exact ergodic secrecy capacity (ESC) of the proposed system, taking into consideration the impacts of outdated channel state information (CSI) at source and imperfect CSI at colluding eavesdroppers. We also give the SOP and ESC of the NOMA-FDR system with PRS protocol for comparison with other benchmark systems such as half-duplex (HD)-NOMA, FD-orthogonal multiple access (OMA) systems with PRS protocol, and NOMA-FDR system with multiple-relay (MR) protocol. The simulation results verify the accuracy of our analysis results. Numerical results demonstrate that the SOP and ESC of the NOMA-FDR system greatly vary according to the number of relays and eavesdroppers, SI cancellation capability, channel estimation error, and the correlation coefficient of outdated channels. Most importantly, the NOMA-FDR system has better secrecy performance than all benchmark relay systems.}
}
