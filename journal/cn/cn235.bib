@article{DBLP:journals/cn/CaoDSZD23,
	author = {Chenhong Cao and
                  Miaoling Dai and
                  Bonan Shen and
                  Guobing Zou and
                  Wei Dong},
	title = {Neural adaptive IoT streaming analytics with RL-Adapt},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109924},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109924},
	doi = {10.1016/J.COMNET.2023.109924},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CaoDSZD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT stream processing is an emerging technology that plays a crucial role in enabling time-critical IoT applications, which often demand high accuracy and low latency. For example, Augmented Reality (AR) applications require high object detection precision and object localization precision, as well as low latency. Note that the exact definition of accuracy varies depending on the specific application Existing stream processing engines are insufficient to meet these requirements, since they could not integrate and respond timely to variable network conditions in the dynamic wireless environment. Recent efforts focusing on adaptive streaming support user-specified policies to adapt to the variable network conditions. However, existing works suffer from manual policies or simplified models of the deployment environment which limit their ability to achieve optimal performance across a broad set of network conditions and quality of experience (QoE) objectives. In this paper, we present a Reinforcement Learning-based Adaptive streaming system (RL-Adapt) that is capable of generating adaption policies using RL-strategy and providing declarative APIs for efficient development. RL-Adapt trains a neural network model that can automatically select the optimal policy based on the observed network conditions. RL-Adapt does not rely on pre-defined models or assumptions about the environment. Instead, it learns to make decisions solely through observations of the resulting performance of past decisions. We implemented RL-Adapt and evaluated its performance extensively in three representative real-world IoT applications. Our results show that RL-Adapt outperforms the state-of-the-art schemes in terms of QoE metrics.}
}


@article{DBLP:journals/cn/SouzaCG23,
	author = {Ot{\'{a}}vio Augusto de Oliviera Souza and
                  Caio Caldeira and
                  Olga Goussevskaia},
	title = {Randomized distributed self-adjusting tree networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109941},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109941},
	doi = {10.1016/J.COMNET.2023.109941},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SouzaCG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we investigate the benefits of randomization in the design of self-adjusting network topologies: networks that dynamically adapt themselves toward the demand they currently serve, in an online manner. We present a randomized self-adjusting tree network, which leverages randomization to reduce the expected network reconfiguration cost by a constant factor, compared to existing deterministic solutions. The new solution is simple, easy-to-implement, fully distributed and concurrent. We prove algorithm correctness, provide expected amortized cost limits, and present simulation results on workloads with variable spatial and temporal complexity.}
}


@article{DBLP:journals/cn/MaZLLHFZLQWHZZCZ23,
	author = {Zhaorui Ma and
                  Shicheng Zhang and
                  Na Li and
                  Tianao Li and
                  Xinhao Hu and
                  Hao Feng and
                  Qinglei Zhou and
                  Fenlin Liu and
                  Xiaowen Quan and
                  Hongjian Wang and
                  Guangwu Hu and
                  Shubo Zhang and
                  Yaqi Zhai and
                  Shuaibin Chen and
                  Shuaiwei Zhang},
	title = {GraphNEI: {A} GNN-based network entity identification method for {IP}
                  geolocation},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109946},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109946},
	doi = {10.1016/J.COMNET.2023.109946},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MaZLLHFZLQWHZZCZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network entity geolocation technology is a technique for inferring geographic location through features such as network measurements or IP address searchable information, also known as IP geolocation. By obtaining the device type of the target can assist in target IP geolocation or IP landmark mining. Most traditional rule-based or learning-based methods identify network entities. However, due to the existence of firewalls and the limitations of feature detection, the features of individual targets are prone to be missing or spoofed, which can lead to a decrease in the accuracy of network entity identification (NEI). This paper propose a graph neural network-based network entity identification method, GraphNEI model, which embeds network entities into the graph structure and uses graph neural networks to improve the current problem of missing or spoofed features in order to improve the accuracy of current network entity identification. It mainly includes five steps: data processing, subgraph partition, weight calculation, node update and classification. First, the acquired dataset is subjected to feature extraction and anonymization; second, the target nodes are subjected to network topology graph construction and community division; third, the nodes’ self-attention, structural attention and similarity of neighbors are combined and used to calculate the nodes’ combined attention; fourth, node aggregation and update, and update the node representation based on the combined attention results; finally, the nodes are classified. We successfully identified 7 different network entities on the publicly collected dataset, and the identification accuracy of network entities is above 95.49%, improved 0.51%–10.42% compared to typical rule-based or learning-based methods. It effectively improves the effective NEI in the absence of network entity features. In addition, we conduct IP geolocation research based on NEI, and the experimental results show that the method is effective in reducing IP geolocation errors distance.}
}


@article{DBLP:journals/cn/KumarTM23,
	author = {G. V. Pradeep Kumar and
                  V. V. Satyanarayana Tallapragada and
                  N. Alivelu Manga},
	title = {Optimized transmit antenna selection and self-attention based convolutional
                  resource allocation model for massive {MIMO} technology},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109948},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109948},
	doi = {10.1016/J.COMNET.2023.109948},
	timestamp = {Mon, 05 Feb 2024 20:24:12 +0100},
	biburl = {https://dblp.org/rec/journals/cn/KumarTM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive multiple input and multiple output (MIMO) plays an important role in enhancing the transmission reliability and capacity of the transmitting channel. However, the resource allocation (RA) and transmit antenna selection (TAS) scheme are essential for massive MIMO systems to reduce implementation costs and complex operations. Because the presence of a large antenna consumes enormous resources that must be reduced in order to develop a user-friendly model. Hence, this article introduces a novel TAS and RA scheme in a massive MIMO system that can enhance communication performance efficiently. In this study, an improved sheep flock optimization algorithm (ISFOA) is first emphasized to select the efficient antenna, thus effectively minimizing the cost and the design complexity. Then, a novel deep learning (DL) based self-attention aided deep convolutional neural network (SA-DCNN) model for the stable allocation of resources to all available users is proposed. Thus, the user equipment (UE) can develop a better quality path and reduce the power consumption of the entire system. In the experimental scenario, the performance of the proposed model is compared with an existing technique based on sum rate, spectral efficiency (SE) and energy efficiency (EE) by varying base station (BS) antennas, varying user, signal to noise ratio (SNR), and sub-carriers along with the computational performance. Particularly, when the SNR=10 dB, the proposed model obtains the SE of about 50 bits/s Hz compared to the existing techniques.}
}


@article{DBLP:journals/cn/AraujoPS23,
	author = {Guilherme B. Ara{\'{u}}jo and
                  Maycon L. M. Peixoto and
                  Leobino Sampaio},
	title = {A comprehensive and configurable simulation environment for supporting
                  vehicular named-data networking applications},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109949},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109949},
	doi = {10.1016/J.COMNET.2023.109949},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AraujoPS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Named Data Networking (NDN) architecture, with its network-layer features, services, and properties, is well-suited for vehicular applications and inter-vehicle communication (IVC). In contrast, IP-based host-centric architectures struggle with challenges inherent to the vehicular ad-hoc network (VANET) context, such as node mobility, data security, efficient data forwarding, and routing. Named Data Networking takes a fundamental departure from today’s IP-based architectures for VANET/IVC and thus requires extensive experimentation and evaluation. To facilitate experimentation with Vehicular Named-Data Networking, we present the NDN4IVC, an open-source simulator/framework that facilitates testing of more realistic VANET applications via the NDN stack. This project utilizes two popular simulators for VANET simulation: the NS3 network simulator with the ndnSIM module and SUMO, a simulator of urban mobility. NDN4IVC allows real-time bidirectional communication between SUMO and NS3 to support more data about road traffic and vehicular mobility. The framework can model the impact of vehicular networks on road traffic and investigate complex interactions between the two domains. We included two sample applications in VANET to demonstrate how different NDN properties can be used. Experiments were conducted as proof-of-concept studies to demonstrate the potential of the framework and its functionality.}
}


@article{DBLP:journals/cn/PerifanisPKE23,
	author = {Vasileios Perifanis and
                  Nikolaos Pavlidis and
                  Remous{-}Aris Koutsiamanis and
                  Pavlos S. Efraimidis},
	title = {Federated learning for 5G base station traffic forecasting},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109950},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109950},
	doi = {10.1016/J.COMNET.2023.109950},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PerifanisPKE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular traffic prediction is of great importance on the path of enabling 5G mobile networks to perform intelligent and efficient infrastructure planning and management. However, available data are limited to base station logging information. Hence, training methods for generating high-quality predictions that can generalize to new observations across diverse parties are in demand. Traditional approaches require collecting measurements from multiple base stations, transmitting them to a central entity and conducting machine learning operations using the acquire data. The dissemination of local observations raises concerns regarding confidentiality and performance, which impede the applicability of machine learning techniques. Although various distributed learning methods have been proposed to address this issue, their application to traffic prediction remains highly unexplored. In this work, we investigate the efficacy of federated learning applied to raw base station LTE data for time-series forecasting. We evaluate one-step predictions using five different neural network architectures trained with a federated setting on non-identically distributed data. Our results show that the learning architectures adapted to the federated setting yield equivalent prediction error to the centralized setting. In addition, preprocessing techniques on base stations enhance forecasting accuracy, while advanced federated aggregators do not surpass simpler approaches. Simulations considering the environmental impact suggest that federated learning holds the potential for reducing carbon emissions and energy consumption. Finally, we consider a large-scale scenario with synthetic data and demonstrate that federated learning reduces the computational and communication costs compared to centralized settings.}
}


@article{DBLP:journals/cn/VegaCHAML23,
	author = {Adri{\'{a}}n Campazas Vega and
                  Ignacio Samuel Crespo{-}Mart{\'{\i}}nez and
                  {\'{A}}ngel Manuel Guerrero{-}Higueras and
                  Claudia {\'{A}}lvarez{-}Aparicio and
                  Vicente Matell{\'{a}}n and
                  Camino Fern{\'{a}}ndez Llamas},
	title = {Analyzing the influence of the sampling rate in the detection of malicious
                  traffic on flow data},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109951},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109951},
	doi = {10.1016/J.COMNET.2023.109951},
	timestamp = {Sun, 19 Jan 2025 14:22:31 +0100},
	biburl = {https://dblp.org/rec/journals/cn/VegaCHAML23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyberattacks are a growing concern for companies and public administrations. The literature shows that analyzing network-layer traffic can detect intrusion attempts. However, such detection usually implies studying every datagram in a computer network. Therefore, routers routing a significant volume of network traffic do not perform an in-depth analysis of every packet. Instead, they analyze traffic patterns based on network flows. However, even gathering and analyzing flow data has a high-computational cost, and therefore routers usually apply a sampling rate to generate flow data. Adjusting the sampling rate is a tricky problem. If the sampling rate is low, much information is lost and some cyberattacks may be neglected, but if the sampling rate is high, routers cannot deal with it. This paper tries to characterize the influence of this parameter in different detection methods based on machine learning. To do so, we trained and tested malicious-traffic detection models using synthetic flow data gathered with several sampling rates. Then, we double-check the above models with flow data from the public BoT-IoT dataset and with actual flow data collected on RedCAYLE, the Castilla y León regional academic network.}
}


@article{DBLP:journals/cn/WangTJH23,
	author = {Kaiyu Wang and
                  Zhiying Tu and
                  Zhenzhou Ji and
                  Shufan He},
	title = {Multi-stage data synchronization for public blockchain in complex
                  network environment},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109952},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109952},
	doi = {10.1016/J.COMNET.2023.109952},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangTJH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The various node connecting ways make the public blockchain live in a complex network environment. Single network model, no matter whether synchronized network, partial asynchronous network, or pure asynchronous network, are failed to accurately describe the practical public network environment. This results in the data synchronization and consensus achievement in public blockchain facing a dilemma, either relies on a weak timing assumption or adopts the strict asynchronous assumption but accepts a complex method.}
}


@article{DBLP:journals/cn/AlkwaiBGA23,
	author = {Lamia Alkwai and
                  Abdelfettah Belghith and
                  Achraf Gazdar and
                  Saad Al{-}Ahmadi},
	title = {Transparent consumer mobility management in named data networking
                  under the push communication mode},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109953},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109953},
	doi = {10.1016/J.COMNET.2023.109953},
	timestamp = {Sun, 12 Nov 2023 02:17:56 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlkwaiBGA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Media streaming and consumer mobility are already prevalent on the internet. Audio and video streaming traffic continues to grow and currently accounts for over 70% of global traffic. User mobility has become a necessity due to the very rapid proliferation of smart phones and mobile devices. Mobility has become a basic premise of network communications and requires effective native integration into future and 5G networks.}
}


@article{DBLP:journals/cn/ChenLFHL23,
	author = {Junmei Chen and
                  Zongpeng Li and
                  Guang Fang and
                  Yeqiao Hou and
                  Xianglong Li},
	title = {A comprehensive repair scheme for distributed storage systems},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109954},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109954},
	doi = {10.1016/J.COMNET.2023.109954},
	timestamp = {Sun, 06 Oct 2024 21:22:02 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenLFHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data storage systems apply erasure codes to provide data reliability efficiently. Previous studies proposed a series of techniques to weigh repair/storage costs, reduce codec complexity, minimize repair time, improve fault tolerance, and enforce system-level service level agreement. These techniques have been designed in isolation, leading to performance limitations. We explore the potential advantages of combining these techniques to meet data storage systems’ requirements better and provide superior system performance. This work proposes a comprehensive repair scheme for fault data in distributed storage systems. First, we tailor design erasure codes in the presence of heterogeneity of storage devices. The core idea is to monitor device performance (e.g., access speed, reliability), compute two coefficients for each device, and use them to select the appropriate devices to create stripes of erasure codes. Second, we leverage the system hierarchy to perform intermediary repair operations, further minimizing cross-rack repair bandwidth. Finally, we propose a new repair scheme adapted to the skew of data access. To demonstrate the effectiveness of our comprehensive repair scheme, we evaluate various erasure codes via mathematical analysis and experiments in the Ceph cluster. In the mise-en-scène of traditional re-encoding methods and more recent adaptive erasure codes, our scheme stands out with significant savings in recovery bandwidth, code-switching bandwidth, repair time, and code-switching time.}
}


@article{DBLP:journals/cn/LeonSMCMG23,
	author = {Juan Pablo Astudillo Le{\'{o}}n and
                  Carlos Lester Due{\~{n}}as Santos and
                  Ahmad Mohamad Mezher and
                  Juli{\'{a}}n L. C{\'{a}}rdenas{-}Barrera and
                  Julian Meng and
                  Eduardo Castillo Guerra},
	title = {Exploring the potential, limitations, and future directions of wireless
                  technologies in smart grid networks: {A} comparative analysis},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109956},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109956},
	doi = {10.1016/J.COMNET.2023.109956},
	timestamp = {Mon, 05 Feb 2024 20:24:12 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LeonSMCMG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart Grids play a crucial role in managing electric power in a more efficient, secure, and sustainable manner. These grids rely on information and communication technologies to facilitate two-way communication between energy providers and consumers, enabling better coordination in the production and distribution of electricity. This article aims to compare the main technologies that can be used in Neighborhood Area Networks (NAN) for Smart Grid data communication networks. NANs are critical to Smart Grid infrastructure, providing advanced energy monitoring and control solutions like smart metering and demand management. The article surveys commonly used technologies such as IEEE 802.15.4 g, IEEE 802.11s, LoRa, and LTE, highlighting their pros and cons. Unlike many existing works in the literature, this article does not limit itself to a theoretical comparison but also conducts network simulations using widely-used scientific tools such as ns-3 and OMNeT++. The article evaluates the technologies in two scenarios, a grid-like scenario, and a real-world scenario using actual smart meter locations in Montreal. The simulations’ results are analyzed using widely-used metrics such as Packet Delivery Ratio (PDR), network transit time, and compliant factor, and offer insights into selecting the right technology for NANs in Smart Grids based on various network scenarios.}
}


@article{DBLP:journals/cn/NguyenNNL23,
	author = {Tien{-}Tung Nguyen and
                  Anh{-}Vinh Nguyen and
                  Tan{-}Loc Nguyen and
                  Van{-}Hung Le},
	title = {Two-way UAV-aided systems with short packet communications},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109957},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109957},
	doi = {10.1016/J.COMNET.2023.109957},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NguyenNNL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate a two-way unmanned aerial vehicle (UAV)-aided system, where two sources communicate with each other with the aid of an UAV relay. Two downlink transmission schemes consisting of non-orthogonal multiple access (NOMA) and network coding (NC) have been studied under finite blocklength regime. Considering imperfect successive interference cancellation (SIC), we have derived the exact and approximate expressions of the systems’ average block-error rates (BLERs). In particular, we propose a solution to maximize system throughput by optimizing packet length. In addition, we have also quantified the effects of the system parameters on the BLER performance, including the transmit power, the power allocation coefficients, and the UAV’s altitude. Numerical results demonstrate that the performance gap between the NOMA-based and NC-based systems is negligible.}
}


@article{DBLP:journals/cn/XuSGCW23,
	author = {Yaqi Xu and
                  Yan Shi and
                  Yuming Ge and
                  Shanzhi Chen and
                  Longxiang Wang},
	title = {Informer-based QoS prediction for {V2X} communication: {A} method
                  with verification using reality field test data},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109958},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109958},
	doi = {10.1016/J.COMNET.2023.109958},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuSGCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle-to-everything (V2X) communication plays a critical role in connected and automated driving applications, which requires strict Quality of Service (QoS) performance in terms of delay and reliability since human safety is involved. The QoS of V2X communications is impacted by various factors, such as radio interference, mobility, and user equipment (UE) density, making accurate prediction challenging. Various methods have been exploited to predict QoS deterioration. However, they usually require high computational complexity or may not capture the complex features of V2X communication environment. In this paper, a novel Informer-based QoS prediction model with a causal convolution self-attention mechanism is presented, which achieves high prediction performance with minimal computing resources. Notably, the test data used in the evaluation of the model is collected from the National Intelligent Vehicle and Intelligent Transportation Demonstration Zone in Daxing District, Beijing. The experimental results demonstrate that the proposed method exhibits superior accuracy and stability compared to existing methods such as Back Propagation (BP), ELMAN, LSTM, and CNN.}
}


@article{DBLP:journals/cn/GarciaLunaAcevesC23,
	author = {J. J. Garcia{-}Luna{-}Aceves and
                  Dylan Cirimelli{-}Low},
	title = {{ALOHA-NUI:} {A} collision-free version of {ALOHA} using a Neighborhood-Understood
                  Index},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109959},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109959},
	doi = {10.1016/J.COMNET.2023.109959},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GarciaLunaAcevesC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {ALOHA with priority acknowledgments (ACK) is transformed into a collision-free channel access method by means of a neighborhood-understood index (NUI). Each node maintains the NUI, which allows nodes to remember all nodes that requested to share the channel and the order in which they should be allowed to transmit. The resulting protocol, ALOHA-NUI, adds signaling packets and NUI information in data packets to maintain the NUI, rather than just remembering that a data packet was sent successfully as in ALOHA. This results in ALOHA-NUI, which is compared with TDMA assuming a fixed transmission schedule, ALOHA with priority ACK’s, and CSMA with priority ACK’s analytically and by simulation. ALOHA-NUI is shown to attain the high throughput of collision-free transmission scheduling methods that usually require clock synchronization while maintaining most of the simplicity of ALOHA with priority ACK’s. ALOHA-NUI is also shown to be fair and to reach stable transmission schedules very quickly.}
}


@article{DBLP:journals/cn/MukhtaruzzamanA23,
	author = {Mohammad Mukhtaruzzaman and
                  Mohammed Atiquzzaman},
	title = {Stable dynamic feedback-based predictive clustering protocol for vehicular
                  Ad hoc networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109960},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109960},
	doi = {10.1016/J.COMNET.2023.109960},
	timestamp = {Wed, 04 Oct 2023 16:26:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MukhtaruzzamanA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scalability presents a significant challenge in vehicular communication, particularly when there is no hierarchical structure in place to manage the increasing number of vehicles. As the number of vehicles increases, they may encounter the broadcast storm problem, which can cause network congestion and reduce communication efficiency. Clustering can solve these issues, but due to high vehicle mobility, clustering in vehicular ad hoc networks (VANET) suffers from stability issues. Most of the existing clustering protocols are optimized for highways only and some protocols are optimized for intersections only. The clusters that are created by the protocols which are optimized for highways breakdown at the intersections where clustering is needed more. On the other hand, the clusters that are created by the protocols optimized for intersections create extra clusters or break unnecessary on straight-roads which reduces cluster stability. Moreover, the lack of intelligent use of a combination of the mobility parameters, such as direction, movement, position, velocity, degree of vehicle, and movement at the intersection, also contributes to cluster stability problems. A dynamic clustering protocol that efficiently combines all the mobility parameters can resolve these issues in VANETs by providing stability in both intersections and highways. To achieve high stability in VANET clustering, a Stable Dynamic feedback-based Predictive Clustering (SDPC) protocol is proposed for VANET to ensure cluster stability in both highway and intersection scenarios. SDPC considers vehicle relative velocity, acceleration, position, distance, transmission range, moving direction at the intersection, and vehicle density to create a cluster. The cluster head is selected based on the future position of the vehicles, relative distance between vehicles, movement of vehicles at the intersection, degree of vehicles, and probable cluster head duration. The performance of SDPC is compared with four existing VANET clustering protocols in various road topologies, in terms of the average cluster head change rate, the average duration of cluster head, the average duration of cluster member, and the average clustering overhead. The simulation results show that SDPC outperforms the existing protocols, achieving higher clustering stability in terms of cluster head change rate (50%), cluster head duration (15%), cluster member duration (6%), and clustering overhead (35%).}
}


@article{DBLP:journals/cn/BeldaAGF23,
	author = {Rom{\'{a}}n Belda and
                  Pau Arce and
                  Juan Carlos Guerri and
                  Ismael de Fez},
	title = {A {DASH} server-side delay-based representation switching solution
                  to improve the quality of experience for low-latency live video streaming},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109961},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109961},
	doi = {10.1016/J.COMNET.2023.109961},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BeldaAGF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work addresses the integration of real-time transmission systems, including IP cameras and production systems (like OBS or vMix), that use protocols such as RTSP (Real Time Streaming Protocol) or SRT (Secure Reliable Transport), with content distribution technology based on LL-DASH (Low Latency DASH -Dynamic Adaptive Streaming over HTTP-), taking advantage of the fact that DASH offers significant well-known advantages for content distribution over the Internet and via CDNs (Content Delivery Networks). Considering the limitations of the LL-DASH standard regarding the adaptation to network conditions, this paper proposes a new solution called Server-Side Representation Switching (SSRS). SSRS uses an approach based on the server measuring the delay in the requests made by clients, whose variation may be due to a decrease in bandwidth, as occurs in Wi-Fi networks with a high number of clients. To evaluate the effectiveness of the proposed solution, a testbed has been developed that allows the performance evaluation of both the LL-DASH system and the solution based on server-side decision-making. In addition, the developed solution has been compared with known algorithms (L2A and LoL+) integrated into the Dash.js player. The results show that the Server-Side Representation Switching solution offers a good trade-off between the transmitted quality and the final delay measured at the client, compared to the other algorithms evaluated. Moreover, it holds the advantage of being straightforward to implement and does not require any modifications to the players used.}
}


@article{DBLP:journals/cn/SantanaSPSLM23,
	author = {Juan Ram{\'{o}}n Santana and
                  Pablo Sotres and
                  Jes{\'{u}}s P{\'{e}}rez and
                  Luis S{\'{a}}nchez and
                  Jorge Lanza and
                  Luis Mu{\~{n}}oz},
	title = {Assessing LoRaWAN radio propagation for smart parking service: An
                  experimental study},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109962},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109962},
	doi = {10.1016/J.COMNET.2023.109962},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SantanaSPSLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increase of the population living in cities has put a lot of pressure in urban transportation systems. It comes, among other things, with noteworthy air pollution, time and energy wastage, and drivers’ discomfort due to parking spots scarcity. Finding a parking spot has become a major problem for drivers. To overcome this problem, smart parking services that detect parking spot occupancy status and inform drivers about their availability have been proposed. The deployment of these solutions is reducing the costs associated to traffic jams or gas consumption. However, one of the main challenges faced when planning and deploying this kind of service is the communication technology used by the parking sensors, which are typically buried under the asphalt, as it has to cover large areas of the city in the most cost-effective way. In this paper, we analyze the behavior and performance of a LoRaWAN network supporting the smart parking service in the city of Santander. The sensors and network deployment are described and the thorough experimental evaluation of the network behavior is presented. The goal of this assessment is to provide insights that could be helpful for better understanding the key factors affecting the communication. The long-termed analysis of the parking sensors deployments studied, have allowed us to derive an experimental propagation model that could be useful for the planning of city-scale IoT infrastructures employing LoRaWAN networks as their wireless access technology. Moreover, we have also analyzed the behavior of the LoRa wireless network in order to evaluate the possibility of leveraging network information to increase the confidence on smart parking sensor readings through AI-based predictors.}
}


@article{DBLP:journals/cn/BineBRL23,
	author = {Lailla M. S. Bine and
                  Azzedine Boukerche and
                  Linnyer B. Ruiz and
                  Antonio A. F. Loureiro},
	title = {A Novel Ant Colony-inspired Coverage Path Planning for Internet of
                  Drones},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109963},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109963},
	doi = {10.1016/J.COMNET.2023.109963},
	timestamp = {Mon, 22 Jul 2024 20:56:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BineBRL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coverage Path Planning (CPP) is a fundamental problem widely used in various applications such as monitoring, patrol, mapping, and sweeping. Several types of robots can perform CPP, which has become very popular with the use of multiple drones. The Internet of Drones (IoD) is an emerging technology that aims to facilitate a multi-drone environment for various applications. In this context, coordination and organization of drones are crucial to avoid collisions and congestion in the airspace. One way to achieve this is by using well-defined airways. CPP tasks are often performed using multiple drones to cover a given area, which can be integrated into the IoD system. However, existing CPP solutions for drones do not fully consider collaboration capabilities between them, specific energy constraints, and the dynamic IoD scenario. To address these challenges, this work proposes an Ant Colony Optimization Algorithm for CPP in IoD (AntIoD), considering energy consumption. This scenario is particularly challenging due to the unique characteristics of being a mix of CPP for terrestrial vehicles with road constraints and CPP for drone networks. We compared our approach, AntIoD, with three recent solutions from the literature. The results reveal that AntIoD outperformed them in all simulated scenarios by at least 10% in terms of energy efficiency when compared to the best scenarios of the other algorithms.}
}


@article{DBLP:journals/cn/AbbasiRAM23,
	author = {Felor Beikzadeh Abbasi and
                  Ali Rezaee and
                  Sahar Adabi and
                  Ali Movaghar},
	title = {Fault-tolerant scheduling of graph-based loads on fog/cloud environments
                  with multi-level queues and LSTM-based workload prediction},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109964},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109964},
	doi = {10.1016/J.COMNET.2023.109964},
	timestamp = {Sun, 06 Oct 2024 21:22:02 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AbbasiRAM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Workflow scheduling in the fog/cloud environment is a complex challenge, particularly with the increasing number of IoT devices. Many workflow applications have time constraints. The duplication approach is one of the methods that can help tasks to meet their deadline. This paper presents an architecture that uses a multi-criteria scheduling algorithm and multi-level queues. The proposed architecture determines a duplication coefficient for each queue, and tasks are placed in these queues using a triple-tuple approach composed of the tasks' priority, time pressure, and workload prediction of required resources. The proposed architecture uses various methods for calculating desired parameters. CPE is used to calculate the priority of tasks. Workload prediction is performed by Long-Short Term Memory (LSTM) neural network which is known for its forecasting capabilities. To evaluate the effectiveness of our proposed method, we conducted a software simulation study comparing it with four recent algorithms. The simulation results confirmed the superiority of our approach in different areas. The throughput is improved by 48.6%, communication cost was reduced by 56%. Also, make-span, waiting time, and the parallelism degree are improved between 15 to 62%.}
}


@article{DBLP:journals/cn/XuCSXC23,
	author = {Yuwei Xu and
                  Jie Cao and
                  Kehui Song and
                  Qiao Xiang and
                  Guang Cheng},
	title = {FastTraffic: {A} lightweight method for encrypted traffic fast classification},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109965},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109965},
	doi = {10.1016/J.COMNET.2023.109965},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuCSXC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, most Internet communications have adopted encrypted network access technology for privacy protection, so encrypted traffic classification (ETC) has become a crucial research point to support network management and ensure cyberspace security. Meanwhile, off-the-shelf deep learning (DL)-based approaches suffer from long preprocessing time, large input size, and a trade-off between model complexity and accuracy. There is a tough challenge to deploy them on mainstream network devices and achieve fast and accurate traffic classification. In this paper, we design FastTraffic, a lightweight DL-based method for ETC on low-configuration network devices. To speed up processing, we set an IP packet as the granularity of FastTraffic, truncate the informative parts in packets as inputs, and utilize a text-like packet tokenization method. For a lightweight and effective model, we propose an N-gram feature embedding method to represent structured and sequential features of packets and design a three-layer MLP to complete fast classification. We compare FastTraffic with eight state-of-the-art ETC methods on three public benchmark datasets. The experimental results show that FastTraffic obtains better classification performance than the other seven methods with only 0.43M model parameters. Besides, it can also achieve high throughput on low-configuration devices and consume a small amount of computing and storage resources. Therefore, FastTraffic is a lightweight ETC method capable of large-scale deployment on Internet devices.}
}


@article{DBLP:journals/cn/RafiqueHC23,
	author = {Wajid Rafique and
                  Abdelhakim Senhaji Hafid and
                  Soumaya Cherkaoui},
	title = {SoftCaching: {A} framework for caching node selection and routing
                  in Software-Defined Information Centric Internet of Things},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109966},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109966},
	doi = {10.1016/J.COMNET.2023.109966},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RafiqueHC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A considerable demand for Internet of Things (IoT) services is causing exponential growth in Internet traffic. Software-Defined Networking (SDN) and Information-Centric Networking (ICN) are complementary technologies for IoT services provisioning that reduce network traffic and service provisioning delays by caching content at intermediate nodes in the network. Existing research on Software-Defined Information-Centric Networking (SDICN) for IoT service provisioning suffers from two major challenges: (a) optimal caching node selection and (b) optimal path computation from content consumers to caching nodes based on end-to-end delay and link utilization constraints. We propose a SoftCaching framework that optimizes caching node selection for content publishers and computes the optimal routing path to caching nodes for content consumers. It computes optimal locations for caching nodes and finds optimal routing paths for IoT services to access cached content. SoftCaching analyzes network traffic on all nodes in the network and collects traffic metrics. It estimates the best possible caching nodes by using Waypoint Enforcement (WPE) and solving the traffic matrix using Singular Value Decomposition (SVD) and QR Factorization. WPE and SVD-based QR factorization provide optimal locations to cache the content for IoT services. Subsequently, SoftCaching uses a constraint-based shortest path algorithm to compute the optimal routing path from content consumers to caching nodes. We make sure that the caching node and the routing path selection follow delay and link utilization constraints. We perform extensive experiments for evaluation, and the results show that SoftCaching outperforms state-of-the-art caching schemes in terms of delay, link load, hop count, path stretch, and energy consumption metrics.}
}


@article{DBLP:journals/cn/KumarP23,
	author = {E. Praveen Kumar and
                  S. Priyanka},
	title = {A comprehensive survey on hardware-assisted malware analysis and primitive
                  techniques},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109967},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109967},
	doi = {10.1016/J.COMNET.2023.109967},
	timestamp = {Wed, 04 Oct 2023 16:26:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KumarP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malware poses an extremely dangerous threat to the digital world, significantly impacting various domains such as smart cities, intelligent transportation, wireless sensor networks (WSNs), and the Internet of Vehicles (IoV). In the Internet of Things (IoT) context, which includes various applications, security threats, and vulnerabilities are high, especially for resource-constrained single-board devices. One of the critical challenges is the presence of unauthorised access to IoT devices that attempt to replicate authorized devices by mimicking their hardware and software specifications. This review paper provides a comprehensive overview of the security issues related to various hardware-level attacks, memory attacks, hardware Trojans, and their countermeasures in different domains. In addition, the paper addresses the study of hardware malware and explores their detection techniques for hardware-based malware, including side-channel and chip-based attacks using machine learning. Using primitive hardware techniques, such as Physically Unclonable Functions (PUFs), to generate the unique key for the device by preventing unauthorised access to the hardware during authentication for different domains is thoroughly investigated with different security features. This review analyzes the different types of PUFs, their characteristics, and their use for various security tasks, including device authentication and the key generation process. It critically evaluates the strengths and limitations of the PUF-based approach and identifies potential research directions to overcome the existing challenges.}
}


@article{DBLP:journals/cn/NaderiCG23,
	author = {Mohammad Naderi and
                  Jacob Chakareski and
                  Mohammed Ghanbari},
	title = {Hierarchical Q-learning-enabled neutrosophic {AHP} scheme in candidate
                  relay set size adaption in vehicular networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109968},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109968},
	doi = {10.1016/J.COMNET.2023.109968},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/NaderiCG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperative candidate relay sets (CRS) in opportunistic broadcast paradigms can significantly improve the performance efficiency of traditional broadcast schemes by simultaneously scheduling multiple vehicles in the data packet transmission process. Nonetheless, multiple relay synchronization may cause enormous data redundancy and high delays in data delivery, resulting in a high cooperative transmission cost. High vehicle velocities and vehicular topology restrictions, including intersections, exacerbate the cooperative transmission cost. Motivated by a trade-off between delay and delivery ratio via reducing duplicate data packets and redundant relays, we address this gap in this paper. To this end, we propose an adaptive candidate relay set (ACRS) size scheme based on the Neutrosophic Set Analytic Hierarchy Process (NS-AHP) technique assisted by a Q-learning algorithm. The NS-AHP pair-wise comparison matrix includes; redundancy, cooperative-based efficiency, and relay set stability as the decision-making attributes. Meanwhile, we use a hierarchical Q-learning approach based on the time-varying vehicular environment to adjust the relative preference of the attributes and alternatives (i.e., various CRS sizes) over each other. To optimally learn the relative preference values of the attributes and alternatives over each other, reward functions are set dynamically at both levels. Comprehensive simulations via NS-2 and SUMO are carried out to assess the efficacy of our ACRS scheme compared to three commonly used baseline techniques of fixed size, threshold-based, and probability-based techniques. Overall, the average yields obtained from the experimental outcomes in the urban and highway scenarios show that the ACRS scheme, on average, improves the packet delivery ratio by 3.77%, 5.36%, and 1.35%; reduces the single-hop delay by 34.78%, 42.085%, and 19.39%; reduces the end-to-end delay by 34.76%, 42.72%, and 21.35%; improves the transmission throughput by 34.24%, 41.54%, and 19.305%; and reduces the redundancy ratio by 43.69%, 50.66%, and 24.32% compared to the fixed size, threshold-based, and probability-based techniques, respectively.}
}


@article{DBLP:journals/cn/ShaoXP23,
	author = {Huishuang Shao and
                  Yurong Xia and
                  Changhao Piao},
	title = {Blockchain-assisted certificateless signcryption for vehicle-to-vehicle
                  communication in VANETs},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109969},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109969},
	doi = {10.1016/J.COMNET.2023.109969},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShaoXP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In vehicular ad-hoc networks (VANETs), real-time messages are used to guarantee transportation efficiency and passenger safety through the exchange among vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I). Therefore, the timely acceptance of safety messages is critical to establishing efficient and reliable communication in VANETs, which hinges on an efficient privacy-preserving authentication mechanism. However, these mechanisms face either security or performance problems. Although some state-of-the-art signcryption schemes meet the needs of security requirements of authentication, performance is not efficient in computational overhead for an increase of the bilinear pairing operations in batch unsigncryption verification. Therefore, we have designed an efficient and provably secure blockchain-assisted certificateless signcryption (BACLS) that provides a feasible security communication in theoretical analysis and experiments. The BACLS scheme ensures security against type-I and type-II attackers in regard to existential unforgeability against adaptively chosen message attacks (EUF-CMA) under the hardness assumption of the gap bilinear Diffie–Hellman problem and indistinguishability against adaptive chosen ciphertext attack (IND-CCA2) under the hardness assumption of the elliptic curve discrete logarithm problem in the random oracle model (ROM). The experiments demonstrate that the BACLS scheme significantly reduces unsigncryption computational costs and power consumption in contrast to state-of-the-art schemes, and promotes the development of a decentralized authentication system in VANETs.}
}


@article{DBLP:journals/cn/BatallaSKSZ23,
	author = {Jordi Mongay Batalla and
                  Slawomir Sujecki and
                  Jan M. Kelner and
                  Piotr Sliwka and
                  Dariusz Zmyslowski},
	title = {On studying active radio measurements estimating the mobile network
                  quality of service for the Regulatory Authority's purposes},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109980},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109980},
	doi = {10.1016/J.COMNET.2023.109980},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BatallaSKSZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Regulatory Authority monitors and regulates the telecom market at a national-wide range. One of its main tasks is to put spectrum at the disposal of the Mobile Network Operators (MNOs) for serving the increasing number of users and services. Another task of the Regulator is to demand a minimum quality of service to the network infrastructure. For this scope, the Regulator needs to have an independent estimation of the quality of service of the mobile network. This paper presents a methodology for Regulator-triggered monitoring of 5G radio resources and estimation of the maximum throughput offered to the end users. The requirements of the methodology are: (1) monitoring measurements must be external to the network, (2) all information needed for Quality of Service (QoS) estimation may be obtained only through active measurements in the network, and (3) the number of measurements should be as low as possible (for cost-saving) while offering statistical significance. The presented spot measurements validate the methodology and show the boundary use cases where the methodology would overestimate the transmission quality.}
}


@article{DBLP:journals/cn/JinQKP23,
	author = {Zilong Jin and
                  Jun Qian and
                  Zhixiang Kong and
                  Chengsheng Pan},
	title = {A mobility aware network traffic prediction model based on dynamic
                  graph attention spatio-temporal network},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109981},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109981},
	doi = {10.1016/J.COMNET.2023.109981},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JinQKP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic prediction is a critical research topic in network management and planning. Due to the growing service requirements and diverse service types, network traffic exhibits spatio-temporal variation characteristics. Consequently, employing a time-based dimensional method cannot effectively analyze the characteristics of network traffic. Existing traffic prediction models depend on pre-existing graph information,which poses challenges in effectively capturing the dynamic correlation of traffic data in the face of network topology changes. This paper addresses the aforementioned issues by proposing an efficient network traffic prediction model that utilizes spatio-temporal graph attention. The proposed model integrates a spatial attention mechanism to capture the correlations among node traffic sequences. The spatial attention mechanism quantifies the strength of connections between nodes by calculating a similarity matrix and constructs an adjacency matrix. Furthermore, the model achieves the fusion of spatio-temporal features in traffic data by combining the graph neural network with a temporal attention mechanism. The performance of the network traffic prediction model is evaluated using a real dataset of cellular network traffic. The experimental results demonstrate that the proposed method outperforms existing methods in terms of performance and long-term prediction capability.}
}


@article{DBLP:journals/cn/WangXL23,
	author = {Shiyu Wang and
                  Wenxiang Xu and
                  Yiwen Liu},
	title = {Res-TranBiLSTM: An intelligent approach for intrusion detection in
                  the Internet of Things},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109982},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109982},
	doi = {10.1016/J.COMNET.2023.109982},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangXL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT), as the information carrier of the Internet and telecommunications networks, is a new network technology comprising physical entities embedded with electronic components, software and sensors, and characterized by strong complexity and openness. With the massive amount of data, the occurrence of network intrusion is also increasingly frequent, involving industrial control systems, IoT devices, mobile security, cloud services, and telecommunications services. With the diversification and intelligence of cyberattack behaviors, traditional intrusion detection systems (IDSs) face problems—such as insufficient feature extraction and inaccurate model classification—when faced with high-dimensional features and nonlinear massive data. Due to their powerful data representation learning ability, deep learning methods save substantial time in processing high-dimensional and complex intrusion data. On this basis, we propose an intrusion detection model using ResNet, Transformer and BiLSTM (Res-TranBiLSTM) that takes into account both the spatial and temporal features of network traffic. We use the Synthetic Minor Overriding Technique (SMOTE) – Edited Nearest Neighbor (ENN) method to alleviate the degree of data imbalance. In addition, we respectively establish a spatial feature extraction model based on ResNet and a temporal feature extraction model based on Transformer and BiLSTM to extract spatial features and temporal features parallelly. Finally, spatiotemporal features are included to achieve attack detection and classification. Further, simulation experiments are conducted using the public data sets NSL-KDD and CIC-IDS2017. The experiments are implemented using Python programming language and Pytorch framework. The results reveal that the performance of our proposed model is better than that of other models, with accuracy reaching 90.99%, 99.15% and 99.56%, on NSL-KDD dataset, CIC-IDS2017 dataset and MQTTset dataset, respectively. It increased the detection accuracy by about 1%-10% on NSL-KDD dataset and about 0.2%-10% on CIC-IDS2017 dataset, and about 1%-10% on MQTTset dataset. These results demonstrate that this method is effective in constructing and optimizing large-scale IDS in the IoT environment.}
}


@article{DBLP:journals/cn/MinKKPG23,
	author = {Junhong Min and
                  Yongjun Kim and
                  Moonbeom Kim and
                  Jeongyeup Paek and
                  Ramesh Govindan},
	title = {Reinforcement learning based routing for time-aware shaper scheduling
                  in time-sensitive networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109983},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109983},
	doi = {10.1016/J.COMNET.2023.109983},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MinKKPG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To guarantee real-time performance and quality-of-service (QoS) of time-critical industrial systems, time-aware shaper (TAS) in time-sensitive networking (TSN) controls frame transmission times in a bridged network using a scheduled gate control mechanism. However, most TAS scheduling methods generate schedules based on pre-configured routes without exploring alternatives for better schedulability, and methods that jointly consider routing and scheduling require enormous runtime and computing resources. To address this problem, we propose a TSN Scheduler with Reinforcement Learning-based Routing (TSLR) that identifies improved load balanced routes for higher schedulability with acceptable complexity using distributional reinforcement learning. We evaluate TSLR through TSN simulations and compare it against state-of-the-art algorithms to demonstrate that TSLR effectively improves TAS schedulability and link utilization in TSN with lower complexity. Specifically, TSLR shows a more than 66% increase in schedulability compared to the other algorithms, and TSLR’s scheduling time is reduced by more than 1 h. It also shows flows’ transmission latency is less than 25% of their latency deadline requirement and reduces maximum link utilization by approximately 50%.}
}


@article{DBLP:journals/cn/HeRXGZZL23,
	author = {Rui He and
                  Bangbang Ren and
                  Junjie Xie and
                  Deke Guo and
                  Yuwen Zhou and
                  Laiping Zhao and
                  Yong Li},
	title = {A reinforcement learning method for scheduling service function chains
                  with multi-resource constraints},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109985},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109985},
	doi = {10.1016/J.COMNET.2023.109985},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HeRXGZZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional networks are usually equipped with many dedicated middleboxes to provide various network services. Though these hardware-based devices certainly improve network performance, they are usually expensive and difficult to upgrade. To overcome this shortcoming, network function virtualization (NFV), which accomplishes network services in the form of virtual network functions (VNF) has been presented. Compared to middleboxes, the VNFs are easy to deploy and migrate. Usually, multiple VNFs are chained in a specified order as a service function chain (SFC) to serve a given flow. There are many works to schedule SFCs to minimize the average flow completion time. However, they only consider single resource limitation. In this paper, we are committed to addressing the problem of multi-resource SFC scheduling (MR-SFCS) and minimizing the average flow completion time. We formulate this problem with an Integer Linear Programming (ILP) model and prove its NP-hardness. To well tackle this problem, we propose an approach based on deep reinforcement learning (DRL), which has specific reward design and state representations. Besides, we extend the offline approach to online SFC scheduling. The experiment results demonstrate that our DRL method can significantly reduce the average flow completion time and achieves a cost saving of 69.07% against the benchmark method.}
}


@article{DBLP:journals/cn/LiuWSL23,
	author = {Lingling Liu and
                  Aimin Wang and
                  Geng Sun and
                  Jiahui Li},
	title = {Maximizing data gathering and energy efficiency in UAV-assisted IoT:
                  {A} multi-objective optimization approach},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109986},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109986},
	doi = {10.1016/J.COMNET.2023.109986},
	timestamp = {Wed, 26 Jun 2024 19:56:33 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuWSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The data volume and energy consumption are important design aspects of data collection in the Internet of Things (IoT). In this paper, the unmanned aerial vehicles (UAVs)-assisted data gathering (DG) in IoT is investigated, in which multiple rotary-wing UAVs are dispatched to gather data from multiple terrestrial IoT devices within specified coverage range. In order to ensure the freshness of the collected data, we must consider that each UAV should spend as little time as possible to collect data. Moreover, it is assumed that UAVs collect data from the devices only when they are hovering, and the transmission distance and on-board energy of the UAVs are limited. Under these practical hypotheses, we formulate a UAV-assisted data gathering multi-objective optimization problem (UAVDGMOP) to simultaneously maximize the total data volume collected by UAVs, minimize the maximum-time for UAVs collecting data and approaching the optimal positions, and minimize the total energy consumptions of UAVs via jointly optimizing the locations of UAVs, the transmission power of UAVs, and the scheduling of UAV–device pairs. Moreover, the formulated UAVDGMOP consists of the hybrid and complex solutions, which is difficult to be solved by using the conventional methods, and thus we solve it by proposing a hybrid update multi-objective evolutionary algorithm based on decomposition (HUMOEA/D). Simulation results demonstrate that the proposed approach achieves higher data amount and energy efficiency of the network, and it is more suitable for solving the formulated UAVDGMOP comparing to other comparative approaches, including the random, uniform and layering deployment approaches, as well as other multi-objective evolutionary algorithms (MOEAs). Moreover, the proposed HUMOEA/D still achieves the overall excellent performance in more use cases.}
}


@article{DBLP:journals/cn/LiuJHHCC23,
	author = {Ning Liu and
                  Chunbo Jia and
                  Bingnan Hou and
                  Changsheng Hou and
                  Yingwen Chen and
                  Zhiping Cai},
	title = {6Search: {A} reinforcement learning-based traceroute approach for
                  efficient IPv6 topology discovery},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109987},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109987},
	doi = {10.1016/J.COMNET.2023.109987},
	timestamp = {Mon, 30 Sep 2024 07:54:39 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuJHHCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topology discovery can infer the interconnection relationship between network entities. A complete network topology is of great significance for network security analysis, application research, etc. However, due to the huge address space and uneven distribution of active addresses in the IPv6 Internet, it is infeasible to use brute-force traceroute to discover the entire topology. To address this problem, we propose 6Search, a target generation method based on reinforcement learning algorithm for IPv6 topology discovery. 6Search first obtains the routeable BGP prefixes and then carries out traceroute in each (/32) prefix. The number of probes allocated is dynamically adjusted based on the results of previous scans. Using the reinforcement learning algorithm, 6Search allocates more probes to prefixes with more address discovery in each scan iteration. Real-world experiments demonstrate that 6Search has better performance in terms of discovery efficiency, which is 24.1%–139.8% improvement over the existing methods.}
}


@article{DBLP:journals/cn/DongC23,
	author = {Xiaodong Dong and
                  Binlei Cai},
	title = {Balanar: Balancing deadline guarantee and Jain's fairness for
                  inter-datacenter transfers},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109988},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109988},
	doi = {10.1016/J.COMNET.2023.109988},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DongC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rising quality of service (QoS) requirements on network transmission, fairness and deadline guarantee have become the two most critical indicators on the rationality of network resource allocation and the timeliness of network transmission. However, deadline guarantee and fairness are conflicting objectives that cannot be ensured simultaneously. More specifically, preemptive scheduling policies ensure more inter-datacenter transfers can be finished before their deadlines but cause starvation, while fairly sharing network bandwidth among inter-datacenter transfers ensures fairness but cause deadlines missing. Although the state-of-the-art works have considered fairness and deadline guarantee simultaneously, a mechanism that can flexibly balance these two conflicting objectives is still missing. In this paper, we propose a decentralized traffic scheduling mechanism called Balanar. It achieves flexible trade-offs between long-term Jain’s fairness and utility of guaranteeing inter-datacenter transfers’ deadlines by finding the On-line Optimal Utility-Jain’s Fairness Trade-off (O2UJT) bandwidth allocation profiles, which maximize utility while ensuring long-term Jain’s fairness is higher than a threshold. However, finding the O2UJT bandwidth allocation profiles corresponds to solving a family of complex piece-wise non-convex optimization problems. Thus, we propose a dynamic particle swarm based heuristic algorithm to alleviate this difficulty. Finally, experimental results show Balanar can achieve flexible trade-offs between fairness and utility of guaranteeing transfers’ deadlines without damaging the other network performance indicators.}
}


@article{DBLP:journals/cn/WangFCDCW23,
	author = {Ting Wang and
                  Xi Fan and
                  Kai Cheng and
                  Xiao Du and
                  Haibin Cai and
                  Yang Wang},
	title = {Parameterized deep reinforcement learning with hybrid action space
                  for energy efficient data center networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109989},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109989},
	doi = {10.1016/J.COMNET.2023.109989},
	timestamp = {Wed, 24 Jul 2024 07:50:47 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangFCDCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To ensure the delivery of high-performance and reliable services, data center networks (DCNs) are often over-provisioned for peak workload and traffic bursts. However, in real-world data centers, network traffic seldom reaches peak capacity of the network, resulting in significant energy waste. Traditional energy conservation approaches either suffer from high computational complexity and low solution quality, or their strategies cannot be dynamically adjusted to accommodate changes in data center network traffic. Deep reinforcement learning (DRL) provides an effective way to deal with these issues. However, most of the existing DRL-based schemes only consider either a continuous action space or a discrete action space, which greatly restricts the optimality of decisions. To solve these problems, this paper proposes a novel DRL-based DCN energy optimization framework, named SmartDCN. Specifically, SmartDCN consists of a traffic prediction module (TPM) and an energy optimization module (EOM). TPM incorporates an improved LSTM model JANET with an attention mechanism providing a high prediction accuracy, while EOM integrates our newly proposed parameterized DRL algorithm, named PAS-DQN, combining with the discrete-continuous hybrid action space. PAS-DQN implements a two-level control mechanism for the network, using TPM to predict future traffic in the data center as input. It is devoted to dynamically aggregating current traffic and makes tradeoffs between energy efficiency, performance, and robustness to optimize the network’s power consumption by dynamically calculating the minimum required network subset and turning off the non-involved network devices to achieve power savings. Experimental results show that SmartDCN significantly outperforms the existing state-of-the-art schemes in terms of energy savings under various network conditions.}
}


@article{DBLP:journals/cn/WangTLZLW23,
	author = {Han Wang and
                  Zhangguo Tang and
                  Huanzhou Li and
                  Jian Zhang and
                  Shuangcheng Li and
                  Junfeng Wang},
	title = {CI{\_}GRU: An efficient {DGA} botnet classification model based on
                  an attention recurrence plot},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109992},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109992},
	doi = {10.1016/J.COMNET.2023.109992},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangTLZLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malware is often embedded with domain generation algorithms (DGAs) to prevent firewall interception and domain black-and-white list comparison detection while hiding command and control (C&C) servers to tighten the control of botnets. DGA domains are diverse and difficult to obtain, resulting in highly unbalanced datasets. Domain names generated by different DGA families do not differ much at the sequence data level and it is difficult to extract their features. The above characteristics lead to poor accuracy, poor generalization ability, and bloatedness of DGA domain name classification models based on deep learning. To solve the above problems, the visual representation of sequence data and the DGA domain classification model are presented in this paper. First, the DGA domain name is mapped to the attention recurrence plot (Att_RP) proposed in this paper, which can enrich the data phase space features and differentiate the key phase space features. After that, Att_RP is sent to a DGA domain name classification model (CI_GRU) proposed in this paper for data dimension transformation processing, followed by classification. Experiments show that the classification accuracy, F1_score, and recall of the model for a variety of DGA families in the wild are higher than 99%, and can also accurately classify four types of crafted DGA families. Compared with similar models, the model has high classification accuracy, low time consumption, low generalization error, and high efficiency, and the size of the model is less than one-tenth of similar models.}
}


@article{DBLP:journals/cn/ErazoAgredoDAGR23,
	author = {Cristian C. Erazo{-}Agredo and
                  Luis D{\'{\i}}ez and
                  Ram{\'{o}}n Ag{\"{u}}ero and
                  Mario Garza{-}Fabre and
                  Javier Rubio{-}Loyola},
	title = {Enabling realistic experimentation of disaggregated {RAN:} Design,
                  implementation, and validation of a modular multi-split eNodeB},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109993},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109993},
	doi = {10.1016/J.COMNET.2023.109993},
	timestamp = {Fri, 27 Oct 2023 20:40:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ErazoAgredoDAGR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-defined networking and network function virtualization are the key enablers in the division of base station functionalities and the deployment of some of these in central nodes. In this way, eNodeB tasks, traditionally deployed at dedicated nodes, are distributed between Central Units (CU) and Distributed Units (DU), which are closer to the Radio Units (RU). The 3GPP has specified eight possible functional splits between the CU and the DU. Despite their importance, to date, there is a lack of practical approaches that enable the real separation of multiple functional splits into different nodes, which would in turn enable the adaptation of the functional split configuration, according to network conditions and traffic load. This paper presents the design and implementation of an experimental architecture that allows the actual separation of protocol layers into different nodes, enabling the experimentation with up to four functional split configurations. The validation results demonstrate the feasibility of the proposed implementation, which copes with the delay requirements established by the Small Cell Forum in all configurations.}
}


@article{DBLP:journals/cn/UllahOP23,
	author = {Syed Sajid Ullah and
                  Vladimir A. Oleshchuk and
                  Harsha S. Gardiyawasam Pussewalage},
	title = {A survey on blockchain envisioned attribute based access control for
                  internet of things: Overview, comparative analysis, and open research
                  challenges},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109994},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109994},
	doi = {10.1016/J.COMNET.2023.109994},
	timestamp = {Wed, 01 Nov 2023 08:59:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/UllahOP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) network is rapidly expanding due to sudden technological advancements, which enable objects to become intelligent and contribute to the network. Before permitting a newly added IoT device to communicate with the network, it is essential to provide access and authenticate the device’s legitimacy by ensuring that it has not been tampered. Recently, blockchain technology has been integrated into Attribute Based Access Control (ABAC) protocols to supply a more robust security mechanism for access control in IoT. The IoT and blockchain-based ABAC serve as the foundation of this survey, offering a comprehensive introduction to both topics. Additionally, several security concerns and vulnerabilities associated with Blockchain Envisioned ABAC (BE-ABAC) are presented. A Comparison with related surveys has also been made. Besides, we also present a comparative analysis based on evaluation based on Distance from Average Solution (EDAS) to rank the best schemes among the suggested BE-ABAC schemes. In conclusion, we discuss some open research challenges in an IoT network that uses blockchain to manage access control.}
}


@article{DBLP:journals/cn/LinLLGX23,
	author = {Xu Lin and
                  Chuchu Liu and
                  Lailong Luo and
                  Deke Guo and
                  Ming Xu},
	title = {{NEST:} Optimal deploying DAG-SFCs to maximize the flows wholly served
                  in the network edge},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109995},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109995},
	doi = {10.1016/J.COMNET.2023.109995},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LinLLGX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared with the dedicated hardware-based middle-boxes, Network Function Virtualization (NFV) makes network services more flexible, scalable, and cost-efficient by replacing the network functions, e.g., Firewalls, Deep Packet Inspections (DPI), Load Balancers, by Virtual Network Functions (VNFs) running on general-purpose commercial servers. Multiple VNFs usually chain up to formulate Service Function Chains (SFC) to enable complex network services. With the exploitation of VNF parallelism, SFC with parallel VNFs (Para-SFC) is proposed to reduce the SFC execution delay. However, the inner-structure of a Para-SFC is usually complex and a Para-SFC may have multiple legitimate topologies. DAG-SFC is one of the basic sharping methods for standardizing the topology of Para-SFCs. To adapt to the fast development of edge computing, it is critical to find an optimal deployment of Para-SFCs in hybrid edge-and-cloud network with the purpose of maximizing the amount of the flows completely serviced in the edge. In this paper, we first analyze the target problem via integer program modeling and prove its NP-hardness. Then, we propose a heuristic algorithm named NEST, which is based on the combination of maximum spanning tree and Next Fit. With extensive evaluations, we demonstrate that, compared with the benchmark methods, NEST can achieve up to respective 14% and 37% performance gain on DAG-SFC edge deployment maximization while can always realize (up to respective 276% and 305%) higher efficiency of link usage.}
}


@article{DBLP:journals/cn/HidouriTHHMB23,
	author = {Abdelhak Hidouri and
                  Haifa Touati and
                  Mohamed Hadded and
                  Nasreddine Hajlaoui and
                  Paul Muhlethaler and
                  Samia Bouzefrane},
	title = {{Q-ICAN:} {A} Q-learning based cache pollution attack mitigation approach
                  for named data networking},
	journal = {Comput. Networks},
	volume = {235},
	pages = {109998},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109998},
	doi = {10.1016/J.COMNET.2023.109998},
	timestamp = {Sat, 14 Oct 2023 20:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HidouriTHHMB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Cache Pollution Attack (CPA) is a recent threat that poses a significant risk to Named Data Networks (NDN). This attack can impact the caching process in various ways, such as causing increased cache misses for legitimate users, delays in data retrieval, and exhaustion of resources in NDN routers. Despite the numerous countermeasures suggested in the literature for CPA, many of them have detrimental effects on the NDN components. In this paper, we introduce Q-ICAN, a novel intelligent technique for detecting and mitigating cache pollution attacks in NDN. More specifically, Q-ICAN uses Q-Learning as an automated CPA prediction mechanism. Each NDN router integrates a reinforcement learning agent that utilizes impactful metrics such as the variation of the Cache Hit Ratio (CHR) and the interest inter-arrival time to learn how to differentiate between malicious and legitimate interests. We conducted several simulations using NDNSim to assess the effectiveness of our solution in terms of Cache Hit Ratio (CHR), Average Retrieval Delay (ARD) and multiple artificial intelligence evaluation metrics such as accuracy, precision, recall, etc. The obtained results confirm that Q-ICAN detects CPA attacks with a 95.09% accuracy rate, achieves a 94% CHR, and reduces ARD by 18%. Additionally, Q-ICAN adheres to the security policy of the NDN architecture and consumes fewer resources from NDN routers compared to existing state-of-the-art solutions.}
}


@article{DBLP:journals/cn/ZhuZZZZW23,
	author = {Xiaojuan Zhu and
                  Tianhao Zhang and
                  Jinwei Zhang and
                  Bao Zhao and
                  Shunxiang Zhang and
                  Cai Wu},
	title = {Deep reinforcement learning-based edge computing offloading algorithm
                  for software-defined IoT},
	journal = {Comput. Networks},
	volume = {235},
	pages = {110006},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110006},
	doi = {10.1016/J.COMNET.2023.110006},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuZZZZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via east–west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.}
}


@article{DBLP:journals/cn/WangLMDCL23,
	author = {Xiang Wang and
                  Yang Liu and
                  Jiahui Ma and
                  Yunling Dong and
                  Jiaxing Chen and
                  Zhihua Liu},
	title = {A DV-Hop optimization localization algorithm based on topological
                  structure similarity in three-dimensional wireless sensor networks},
	journal = {Comput. Networks},
	volume = {235},
	pages = {110013},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110013},
	doi = {10.1016/J.COMNET.2023.110013},
	timestamp = {Wed, 04 Oct 2023 16:26:15 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangLMDCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Localization technology performs an essential part in wireless sensor networks (WSNs). The Distance vector hop (DV-Hop) localization algorithm has been frequently used in the WSNs with its simplicity, feasibility and minimal hardware requirements. However, the low localization accuracy of the DV-Hop algorithm has limited its further application in WSNs. To overcome the accuracy problem of the DV-Hop algorithm, we propose an improved DV-Hop algorithm based on topological structure similarity, called TDSDV-Hop. Different from the traditional DV-Hop algorithm, the topological structure similarity is applied to correct the hop count, reducing the inaccuracy caused by the minimum hop count. The hop counts will be converted from discrete to precise continuous values by the topological structure similarity. In this paper, firstly, the topological structure similarity is defined, followed by the proof of the theory that the distance between two nodes is inversely proportional to the topological structure similarity. Secondly, a hop-correction equation that can correct the hop count between neighbor nodes is set up. Then the TDSDV-Hop algorithm incorporates hop-correction formula and simulates the optimal hop-correction parameters in order to acquire more accurate values of the hop count. And the coordinates of unknown nodes are calculated by the least square method. Finally, the levy adaptive improved bird swarm algorithm (LSABSA) is utilized to optimize coordinates of unknown nodes by constructing a new objective function. The simulation results show that the localization performance of the TDSDV-Hop algorithm is better than that of DV-Hop, GWODV-Hop, CRWDV-Hop and CVLR algorithm in terms of localization accuracy, localization time and communication cost.}
}
