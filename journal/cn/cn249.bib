@article{DBLP:journals/cn/WangZWWH24,
	author = {Jitao Wang and
                  Bo Zhang and
                  Kai Wang and
                  Yuzhou Wang and
                  Weili Han},
	title = {BFTDiagnosis: An automated security testing framework with malicious
                  behavior injection for {BFT} protocols},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110404},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110404},
	doi = {10.1016/J.COMNET.2024.110404},
	timestamp = {Wed, 04 Dec 2024 20:53:21 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangZWWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In peer-to-peer computing networks, Byzantine Fault Tolerance (BFT) protocols are a popular solution to ensure the consistency security in the presence of some malicious nodes, thus are widely employed in blockchain systems. However, BFT protocols still face various security threats in practice. Currently, testing the security of BFT protocols often requires manual operations. I.e., it lacks comprehensive automation testing techniques. This makes it difficult to cover various scenarios of malicious node behaviors. Therefore, it is an urgent requirement to design an automated testing framework that can comprehensively and efficiently evaluate the security of BFT protocols.}
}


@article{DBLP:journals/cn/WangZXW24,
	author = {Chuanhua Wang and
                  Quan Zhang and
                  Xin Xu and
                  Huimin Wang},
	title = {{AS-T3BP:} An efficient assignment scheme for space TT{\&}C tasks
                  with bidirectional privacy-preserving under blockchain architecture},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110464},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110464},
	doi = {10.1016/J.COMNET.2024.110464},
	timestamp = {Wed, 12 Jun 2024 10:58:43 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangZXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the space Tracking Telemetry and Command (TT&C) network has attracted much attention because it can provide users with secure and convenient information services. However, there are still challenges in achieving efficient task assignments and providing privacy-preserving for both TT&C parties in precise space TT&C services. In this paper, we propose an efficient blockchain-based task assignment scheme (AS-T\n3\nBP) in space TT&C network. Specifically, we introduce authorized privacy set intersection to ensure bidirectional privacy security for task requesters and performers and integrate an improved cuckoo filter algorithm to achieve batch assignment for tasks. And AS-T\n3\nBP allows the executor to customize the access policies of TT&C information flexibly, and only authorized satellites can access the information and verify the assignment results. For tasks that fail to be assigned, we propose a distributed and credible task reassignment method, which publishes the tasks on the blockchain built by the TT&C base station, thereby ensuring the real-time and accuracy of task assignments. Performance analysis shows that AS-T\n3\nBP is effective and can minimize misinform rates and reassignment rates of 23.39% and 8.85%, alleviate the problem of inefficient task assignment, and still maintain system stability.}
}


@article{DBLP:journals/cn/XuMLCW24,
	author = {Yihang Xu and
                  Yuxing Mao and
                  Jian Li and
                  Xueshuo Chen and
                  Shunxin Wu},
	title = {Edge server enhanced secure and privacy preserving federated learning},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110465},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110465},
	doi = {10.1016/J.COMNET.2024.110465},
	timestamp = {Mon, 03 Mar 2025 21:30:47 +0100},
	biburl = {https://dblp.org/rec/journals/cn/XuMLCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has been smoothly embedded into current IoT edge computing architecture, and hatching advanced IoT-FL applications. While so, sensitive messages generated on terminal sides brings privacy issues; and vulnerable terminal devices are easy targets for Byzantine adversaries, they inject malicious data to manipulate the IoT-FL system. Now an inherent dilemma is, the privacy preserving requires anonymous indistinguishable individual characteristics, while the malicious adversary detection requires transparent distinguishable results. Existing fragmented schemes are not able to handle both problems coherently. This time, inherit our previous study, based on the IoT edge computing architecture, we utilize Homomorphic Encryption (HE) and Threshold Secret Sharing (SS) methods, propose a joint scheme (namely, Sec-IoTFL) for anonymous adversary detection. Specifically, we batch encode the clients’ training result vectors (message) as polynomials, and split them as secret pieces with a unified SS key set; then edge server and cloud server joint deal with these secret pieces in a specialized flow, where Byzantine adversaries will be filtered out. The theoretical analysis and solid experimental results suggest, in our scheme, local sensitive data is well preserved, and malicious behavior is precisely screened. Comparing with traditional methods, our Sec-IoTFL scheme shows the superiority in accuracy and efficiency.}
}


@article{DBLP:journals/cn/LiXLZCW24,
	author = {Yang Li and
                  Chunhe Xia and
                  Chunyan Li and
                  Yuan Zhao and
                  Chen Chen and
                  Tianbo Wang},
	title = {HL-DPoS: An enhanced anti-long-range attack DPoS algorithm},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110473},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110473},
	doi = {10.1016/J.COMNET.2024.110473},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiXLZCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The consensus algorithm is crucial in blockchain for ensuring the validity and security of transactions across the decentralized network. However, achieving consensus among nodes and packaging blocks in blockchain networks is a complex task that requires efficient and secure consensus algorithms. The DPoS consensus algorithm has emerged as a popular choice due to its fast transaction processing and high throughput. Despite these advantages, the algorithm still suffers from weaknesses such as centralization and vulnerability to long-range attacks, which can compromise the integrity of the blockchain network. To combat these problems, we developed an Enhanced Anti-Long-Range Attack DPoS algorithm (HL-DPoS). First, we split nodes into pieces to reduce centralization issues while giving witness nodes the power to report and benefit from malicious node’s reports, maintaining high efficiency and high security. Second, we propose a validation method in HL-DPoS that compares consensuses transactions with the longest chain to detect long-range attacks. Algorithm analysis and simulation experiment results demonstrate that our HL-DPoS consensus algorithm improves security while achieving better consensus performance.}
}


@article{DBLP:journals/cn/LianCYL24,
	author = {Linming Lian and
                  Ningjiang Chen and
                  Xuemei Yuan and
                  Jianbo Lu},
	title = {Low-complexity collaborative caching strategy based on spatio-temporal
                  graph convolutional model},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110490},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110490},
	doi = {10.1016/J.COMNET.2024.110490},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LianCYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the low delay requirements for data content access in industrial IoT, efficient content caching strategies need to be designed in the Mobile Edge Computing (MEC). Most existing caching policies reduce access delay by predicting content popularity and caching popular content earlier during off-peak traffic, but these strategies only focus on the temporal order of content popularity without fully considering the graph topology of MEC servers and ignore the spatial correlation of content popularity. This paper proposes a low-complexity collaborative caching strategy based on spatio-temporal graph convolutional model (STCC). We integrate graph convolutional neural network and gated recurrent unit to construct spatio-temporal graph convolutional model, which mines the spatio-temporal correlation features of content popularity and make effective predictions on the MEC server graph topology constructed by proximity and semantic relations. Furthermore, we use hierarchical clustering to classify MEC servers into collaborative domains and design a low-complexity heuristic collaborative caching content placement algorithm to minimize the average access delay. Compared with the existing MEC server caching strategy, simulation experiments show that STCC has a better prediction effect of content popularity, and achieves better performance in the two indicators of the cache hit ratio and average access delay.}
}


@article{DBLP:journals/cn/MezinaBO24,
	author = {Anzhelika Mezina and
                  Radim Burget and
                  Aleksandr Ometov},
	title = {Reinterpreting Usability of Semantic Segmentation Approach for Darknet
                  Traffic Analysis},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110493},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110493},
	doi = {10.1016/J.COMNET.2024.110493},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MezinaBO24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With a growing number of smart interconnected devices and services, managing and controlling network traffic is getting more complicated. Among the network traffic, the Darknet-related one is particularly interesting, as it is often used for anonymous and illicit activities that pose cyber security threats. Therefore, designing and developing methods for detecting and categorizing Darknet traffic is essential. Applying Deep Learning (DL) is one of the most suitable options in this case. The main reasons are the ability to process a large amount of data and detect the hidden patterns and relationships in these data. This work proposes a DL architecture based on UNet++, which can detect and categorize anonymous traffic. The core idea of this model is semantic segmentation, which can identify meaningful segments that share some common patterns in given data. Hereby, semantic segmentation is postulated as a possible way to investigate Darknet traffic to find some common and related features instead of widely used Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM). According to the results on comparison with other Machine Learning (ML) and DL models, the UNet++ model outperforms the methods with a higher accuracy of 98.19% and 87.27% for Darknet detection and traffic categorization. Our work shows the potential of using UNet++ for network traffic analysis and Darknet traffic detection. We have also demonstrated that more advanced architecture with skip connections and trainable blocks provides more accurate results than pure U-Net, CNN, and other evaluated models.}
}


@article{DBLP:journals/cn/ChenWZYS24,
	author = {Weihan Chen and
                  Zhiliang Wang and
                  Han Zhang and
                  Xia Yin and
                  Xingang Shi},
	title = {Cost-efficient flow migration for {SFC} dynamical scheduling in geo-distributed
                  clouds},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110496},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110496},
	doi = {10.1016/J.COMNET.2024.110496},
	timestamp = {Mon, 05 Aug 2024 08:28:09 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenWZYS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Combining Network Function Virtualization (NFV) and Software Defined Network (SDN) enables the delivery of network services in a flexible and cost-efficient manner. NFV allows elasticity scheduling of Virtual Network Function (VNF) instances to adopt the dynamic variation of network flows, such as VNF instance scale out or in and load balancing. To implement the elasticity scheduling, existing flows need to be re-routed through the VNF instances. One major problem is how to select appropriate flows for migration to reduce migration cost and improve network performance after migration. In this paper, we propose a Cost-efficient Flow Migration framework, CFM, to jointly optimize flow migration cost and network performance after migration (including flow routing cost and maximum VNF instance load) for flow migration in Service Function Chain manner. Experimental results demonstrate that CFM can reduce flow routing cost after migration by\n43\n%\n∼\n62\n%\ncompared with previous works, while achieving similar flow migration cost in scale out/in scenario. In load balancing scenario, CFM can also reduce flow migration cost by\n65\n%\n∼\n79\n%\nand flow routing cost after migration by\n43\n%\n∼\n79\n%\ncompared with previous works, while ensuring that the maximum VNF instance load does not significantly increase.}
}


@article{DBLP:journals/cn/dOreyGSRSA24,
	author = {Pedro M. d'Orey and
                  Miguel Guti{\'{e}}rrez Gait{\'{a}}n and
                  Pedro M. Santos and
                  Manuel Ribeiro and
                  Jo{\~{a}}o Borges de Sousa and
                  Lu{\'{\i}}s Almeida},
	title = {Assessing Short-range Shore-to-Shore {(S2S)} and Shore-to-Vessel {(S2V)}
                  WiFi Communications},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110505},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110505},
	doi = {10.1016/J.COMNET.2024.110505},
	timestamp = {Mon, 03 Mar 2025 21:30:48 +0100},
	biburl = {https://dblp.org/rec/journals/cn/dOreyGSRSA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communications increasingly enable ubiquitous connectivity for a large number of nodes, applications and scenarios. One of the less explored scenarios are aquatic ecosystems, specially when enabled by near-shore and short-range communications. Overwater communications are impaired by a number of distinguishing dynamic factors, such as tides, waves or node mobility, that lead to a widely fluctuating and unpredictable channel. In this work, we empirically characterize near-shore, overwater channels at 2.4 GHz under realistic conditions, including tidal variations, and relatively short TX-RX separations. To this end, we conducted experiments in a coastal estuarine region and on a harbor to characterize Shore-to-Shore (S2S) and Shore-to-Vessel (S2V) communication channels, respectively, and to identify major factors impairing communication in such scenarios. The empirical results show that constructive/destructive interference patterns, varying reflecting surface, and node mobility (i.e. travel direction and particular maneuvers) have a relevant and noticeable impact on the received signal strength. Thus, a set of parameters should be simultaneously considered for improving the performance of communication systems supporting S2S and S2V links, namely tidal variations, reflection surface changes, antenna height, TX-RX alignment and TX-RX separation. The results useful provide insights into realistic S2S and S2V link design and operation.}
}


@article{DBLP:journals/cn/ShaikNISVP24,
	author = {Razeena Begum Shaik and
                  Prabagarane Nagaradjane and
                  Iacovos Ioannou and
                  Vitawat Sittakul and
                  Vasos Vassiliou and
                  Andreas Pitsillides},
	title = {AI/ML-aided capacity maximization strategies for {URLLC} in 5G/6G
                  wireless systems: {A} survey},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110506},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110506},
	doi = {10.1016/J.COMNET.2024.110506},
	timestamp = {Tue, 10 Dec 2024 09:00:02 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ShaikNISVP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-reliable low-latency communication (URLLC) refers to cellular applications in fifth and sixth-generation (5G/6G) networks with specific latency, reliability, and availability demands. Most of the reported 5G/6G applications are focused on URLLC, which necessitates a latency of milliseconds and very high dependability for transmitted data. These systems encounter several obstacles since conventional networks cannot fulfill such demands. According to the standards of the 3rd generation partnership project URLLC, it is predicted that the dependability of a single transmission of a 32-byte packet would be no less than 99.999%, and the latency will not exceed 1 ms. The exceptional degree of dependability and minimal delay will result in the emergence of many novel applications, including smart grids, industrial automation, and intelligent transport systems. This review discusses several methods for maximizing capacity in URLLC, focusing on resource allocation strategies, multi-access approaches, and beamforming with massive MIMO. Furthermore, it explores the requirements and constraints of URLLC and the role of AI/ML in URLLC. Finally, this study examines possible future research areas and obstacles to achieving the URLLC standards.}
}


@article{DBLP:journals/cn/SongGMM24,
	author = {Tailai Song and
                  Paolo Garza and
                  Michela Meo and
                  Maurizio M. Munaf{\`{o}}},
	title = {DeX: Deep learning-based throughput prediction for real-time communications
                  with emphasis on traffic eXtremes},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110507},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110507},
	doi = {10.1016/J.COMNET.2024.110507},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SongGMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed a remarkable upsurge in the global proliferation of Real-Time Communications (RTC) applications, a trend propelled by the flourishing advancement of network technologies and further amplified by the COVID-19 pandemic. Within this context, there is a burgeoning interest in the innovation of sophisticated and intelligent network infrastructures and technologies. Positioned as a promising candidate for this purpose, real-time throughput prediction emerges as a key enabler to foster network observability and offer proactive functions, upholding advanced system management, including but not limited to, bandwidth allocation and adaptive streaming. Nonetheless, existing methodologies struggle with predicting extreme conditions of throughput, notably peaks, valleys, and abrupt changes, that are critical in RTC traffic. To surmount these obstacles, we introduce DeX, a Deep Learning (DL)-based framework, designed to predict short-term throughput, with a dexterous proficiency and dedicated focus on navigating the complexities of traffic eXtremes. In particular, DeX leverages solely packet-level information as features and is composed of three integral components: a packet selection module that opts for an optimal subset of input features, a feature extraction block that partially incorporates the Transformer architecture, and a multi-task learning pipeline that improves the proficiency in handling traffic extremes. Moreover, our work is anchored in extensive traffic traces garnered during actual video-teleconferencing calls, and we formulate a time-series regression problem, rigorously evaluating a spectrum of technologies ranging from an adaptive filter to diverse Machine Learning (ML) and DL approaches. Initially, we aim at predicting throughput within 500-ms time windows using historical 1024 packets out of 2048, and consequently, our methodology exhibits exceptional efficacy, especially in forecasting traffic extremities. Conclusively, we conduct a series of ablation experiments and thorough analyses to showcase the enhanced performance of various scenarios, further validating the effectiveness and robustness of DeX.}
}


@article{DBLP:journals/cn/CaoWW24,
	author = {Manxia Cao and
                  Qingshan Wang and
                  Qi Wang},
	title = {Federated learning in smart home: {A} dynamic contract-based incentive
                  approach with task preferences},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110510},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110510},
	doi = {10.1016/J.COMNET.2024.110510},
	timestamp = {Mon, 03 Mar 2025 21:30:42 +0100},
	biburl = {https://dblp.org/rec/journals/cn/CaoWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a new type of distributed machine learning paradigm aimed at addressing data privacy and security issues in the Internet of Things (IoT) context. In the training process of federated learning model of smart home, data owners need to constantly update cached data, which lead to resources consumption and service latency. Meanwhile, the update cost of the data owners will change with the passage of time when the model owner requests data update. Therefore, this paper first puts forward an incentive scheme based on two-period dynamic contract theory under the information asymmetry. The scheme can balance the weighted preference of the model owner for age of information (AoI) and service latency, as well as encourage more data owners to participate in model training to increase the utility of model owner. Then we formally prove the feasibility of the proposed dynamic contract, which satisfies the constraints of individual rationality and intertemporal incentive compatibility. The experimental results on MNIST dataset show that the accuracy of the proposed dynamic contract is improved by at least 4% compared with the existing contracts. Additionally, compared with traditional contract and consistent pricing strategy, the model owner obtains more profit from the proposed contract.}
}


@article{DBLP:journals/cn/YadavKJK24,
	author = {Rashmi Yadav and
                  Rashmi Kamran and
                  Pranav Jha and
                  Abhay Karandikar},
	title = {An architecture for control plane slicing in beyond 5G networks},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110511},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110511},
	doi = {10.1016/J.COMNET.2024.110511},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/YadavKJK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To accommodate various use cases with differing characteristics, the Fifth Generation (5G) mobile communications system intends to utilize network slicing. Network slicing enables the creation of multiple logical networks over a shared physical network infrastructure. While the problems such as resource allocation for multiple slices in mobile networks have been explored in considerable detail in the existing literature, the suitability of the existing mobile network architecture to support network slicing has not been analysed adequately. We think the existing 5G System (5GS) architecture suffers from certain limitations, such as a lack of slice isolation in its control plane. This work focuses on the future evolution of the Third Generation Partnership Project (3GPP) 5GS architecture from a slicing perspective, especially that of its control plane, addressing some of the limitations of the 3GPP 5GS architecture. We propose a new network architecture which enables efficient slicing in beyond 5G networks. The proposed architecture results in enhanced modularity and scalability of the control plane in sliced mobile networks. In addition, it also brings slice isolation to the control plane, which is not feasible in the existing 5G system. We also present a performance evaluation that confirms the improved performance and scalability of the proposed system viz-a-viz the existing 5G system.}
}


@article{DBLP:journals/cn/LimaP24,
	author = {Leandro Alvarez de Lima and
                  Gustavo Sousa Pavani},
	title = {Crosstalk- and fragmentation-aware survivable routing, modulation,
                  space, and spectrum assignment using Ant Colony Optimization},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110513},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110513},
	doi = {10.1016/J.COMNET.2024.110513},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LimaP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of Space Division Multiplexing (SDM) technology in Elastic Optical Networks (EONs) is a promising solution to improve the transport capability and flexibility required by next-generation applications. In this work, we have illustrated that Ant Colony Optimization (ACO) algorithms can be incorporated into the network control plane and associated with a crankback mechanism to provision and restore lightpaths in a fully distributed manner. In effect, by tackling the challenging Routing, Modulation, Spectrum, and Space Assignment (RMSSA) problem, ACO algorithms can address the inter-core crosstalk and spectrum fragmentation that may limit the potential of the SDM-EON. By comparing different levels of resource state accuracy at the control plane, the simulation results demonstrate the superior performance of the ACO algorithms compared to routing algorithms based on a centralized control plane with a link-state routing protocol, showcasing lower bandwidth blocking rate, comparable restorability, controlled crosstalk levels, and higher scalability, all achieved without a significant increase in setup and restoration times.}
}


@article{DBLP:journals/cn/LinHHZTGS24,
	author = {Na Lin and
                  Jinjiao Huang and
                  Ammar Hawbani and
                  Liang Zhao and
                  Hailun Tang and
                  Yunchong Guan and
                  Yunhe Sun},
	title = {Joint routing and computation offloading based deep reinforcement
                  learning for Flying Ad hoc Networks},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110514},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110514},
	doi = {10.1016/J.COMNET.2024.110514},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LinHHZTGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flying ad-hoc networks (FANETs) consisting of multiple Unmanned Aerial Vehicles (UAVs) are widely used due to their flexibility and low cost. In scenarios such as crowdsensing and data collection, data collected by UAVs are transmitted to base stations for processing and then sent to data centers. Still, the deployment of base stations is costly and inflexible. To address this issue, this paper introduces a position-based Computing First Routing (CFR) protocol designed for efficient task transmission and computation offloading in FANETs. This protocol facilitates task processing during data transfer and ensures the delivery of fully processed results to the data center. Considering the dynamically changing topology of the FANETs and the uneven distribution of the UAVs’ computation power, deep reinforcement learning is used to make multi-objective decisions based on the Q-values computed by the model. FANETs are centerless clusters, and two-hop neighbor tables containing position and computing power information are used to make less costly decisions. Simulation experiments demonstrate that CFR outperforms other benchmark schemes with an approximately 6% higher packet delivery rate, an approximately 21% reduction in end-to-end delay, and about a 34% decrease in total cost. Furthermore, it effectively ensures the completion of task offloading before reaching the destination node. This occurs due to the design of a hierarchical reward function that takes into account dynamic changes in delay and energy consumption, as well as the injection of neighbor computing power information into the two-hop neighbor table.}
}


@article{DBLP:journals/cn/FanYCWXTYW24,
	author = {Wentao Fan and
                  Jun Yao and
                  Shiyuan Cui and
                  Yan Wang and
                  Shuo Xu and
                  Yuehui Tan and
                  Fan Yang and
                  Weihong Wu},
	title = {Bi-LSTM/GRU-based anomaly diagnosis for virtual network function instance},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110515},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110515},
	doi = {10.1016/J.COMNET.2024.110515},
	timestamp = {Tue, 18 Jun 2024 09:26:12 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FanYCWXTYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In edge–cloud networks, Network Function Virtualization (NFV) technology provides users with high-performance Virtual Network Functions (VNFs) to replace dedicated hardware devices. In this paper, we focus on the VNF instance (VNFI) anomaly diagnosis (AD). In large-scale production data centers, VNFI anomalies are inevitable, causing degradation in network service performance, while accurate VNFI AD can minimize the losses. The AD task can be abstracted as a multi-classification problem, where the VNFI state is classified as normal or a specific fault type. Existing studies have not proposed mature VNFI AD systems and feature selection criteria, and the accuracy of AD tasks can still be improved. To address these gaps, we first design the Distributed Online Anomaly Diagnosis and Broadcasting (DOADB) system, providing online VNFI AD daemons, offline parameter training, and anomaly broadcasting. Then, we establish VNFI feature selection criteria and construct a compelling VNFI feature dataset containing simulated and real-time data for each VNFI. Based on the DOADB system and feature dataset, we adopt the Bi-LSTM and Bi-GRU networks to realize the AD multi-classification algorithm, leveraging the bidirectional network structure and long-term memory to enhance AD accuracy. Experimental results demonstrate that the proposed system and algorithm can provide effective AD daemons for VNFIs and outperform existing algorithms in both accuracy and macro F1-score metrics.}
}


@article{DBLP:journals/cn/SuZCS24,
	author = {Dongyuan Su and
                  Yipeng Zhou and
                  Laizhong Cui and
                  Quan Z. Sheng},
	title = {Communication cost-aware client selection in online federated learning:
                  {A} Lyapunov approach},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110517},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110517},
	doi = {10.1016/J.COMNET.2024.110517},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SuZCS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of intelligence services brings data breaches and privacy infringement concerns. To preserve data privacy when training machine learning models, the federated learning (FL) paradigm emerges. Most existing works assume that training data on FL clients are static during the entire learning process. Nevertheless, various real-time intelligent services call for timely processing of continuously generated data, which fosters the advent of online federated learning (OFL). Currently, how to reconcile model utility and communication cost in OFL is still an open problem. To address this challenge, we leverage the Lyapunov optimization framework to devise a novel Low Cost Client Selection (LCCS) algorithm for OFL, which can judiciously select participating clients to maximize model utility with a low communication cost. Specifically, we design the objective as the sum of a penalty function and a Lyapunov drift function to take both gradient-based client valuation and communication cost into account. By minimizing the objective, we further design the LCCS algorithm, which is lightweight for execution on clients. At last, we conduct extensive experiments with traces generated from public datasets. The experimental results demonstrate that LCCS achieves the highest model utility with a fixed communication cost in comparison with the state-of-the-art baselines.}
}


@article{DBLP:journals/cn/HusoOGRPB24,
	author = {Ingrid Huso and
                  Marco Olivieri and
                  Leonardo Galgano and
                  Adnan Rashid and
                  Giuseppe Piro and
                  Gennaro Boggia},
	title = {Design and implementation of a looking-forward Lawful Interception
                  architecture for future mobile communication systems},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110518},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110518},
	doi = {10.1016/J.COMNET.2024.110518},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/HusoOGRPB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Law Enforcement Agencies (LEAs) heavily rely on Lawful Interception (LI) tools to investigate criminal and terrorist activities. The growing frequency of cybercrime, terrorism-related offenses, and illegal trades in the European Union (EU) has driven LEAs to explore novel LI techniques that align with the developing 5G and Beyond 5G network architectures. Moreover, the emergence of extremely dynamic and distributed networks, the increased usage of end-to-end encryption applications, and privacy protections present limitations for traditional LI approaches. In order to provide a technological solution capable of extending the 3GPP LI standard, this paper presents a novel LI framework designed on top of the standardized 3GPP LI architecture, leveraging an inspection-friendly end-to-end cryptography mechanism (e.g., a Key Escrow algorithm) at the application layer. Moreover, the proposed Lawful Interception (LI) framework enables authorized LEAs to decrypt intercepted end-to-end encrypted data within the core network. Firstly, a security proof validates the security of the proposed LI framework under two attack scenarios. Subsequently, a proof-of-concept workstation implementation that emulates a 5G network for end-to-end data exchange and cloud-based deployment validates the suggested LI framework by affirming the LEA capabilities in decrypting intercepted data. Additionally, the system performance has been studied through experimental tests, ensuring the scalability of the conceived solution and revealing the possibility of intercepting data with mainly real-time latency without affecting the Quality of Service (QoS) experienced by the user.}
}


@article{DBLP:journals/cn/SunSWY24,
	author = {Jin Sun and
                  Wenjuan Su and
                  Lu Wang and
                  Kexin Ye},
	title = {Multi-keyword ranked search scheme for privacy protection in social
                  networks},
	journal = {Comput. Networks},
	volume = {249},
	pages = {110534},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110534},
	doi = {10.1016/J.COMNET.2024.110534},
	timestamp = {Wed, 12 Jun 2024 10:58:43 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SunSWY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a social network friend-matching scenario, a proposed blockchain-based privacy protection scheme with cloud-edge collaboration aims to improve search efficiency while protecting the personal privacy information of dating users. The scheme encrypts and stores massive dating data on the cloud server (CS), and uploading the corresponding keyword index to the nearby edge node (EN) for distributed storage. To enhance the search experience for users, this paper uses the Bisecting K-means clustering algorithm to classify all dating documents, categorizes the keywords with high relevance to form keyword grouping first, and then constructs the inverted index based on the keyword grouping results, so that we can quickly locate the position of the query keywords, and then find the matching document identifiers to speed up the ciphertext search. Simultaneously, this paper introduces the BM25 model to calculate the relevance scores of ciphertext documents to achieve the effect of ranking ciphertext documents. Under the general bilinear group model, the proposed scheme proves selectively secure against selective keyword attacks and plain text attacks. The simulation results indicate that this proposed scheme is a more efficient multi-keyword ranking search scheme for dating data.}
}
