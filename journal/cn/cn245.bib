@article{DBLP:journals/cn/GarimaJS24,
	author = {Garima and
                  Vivekanand Jha and
                  Rakesh Kumar Singh},
	title = {A deep learning based dynamic bandwidth allocation method for {XG-PON}
                  based mobile fronthaul for {CRAN}},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110344},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110344},
	doi = {10.1016/J.COMNET.2024.110344},
	timestamp = {Tue, 18 Jun 2024 12:34:40 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GarimaJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of 5G and beyond era, the use of Time Division Multiplexed Passive Optical Network (TDM-PON) for mobile fronthaul (MFH) in centralized/cloud radio access network (CRAN) has proven to be an optimal solution for addressing low-latency requirements. While existing literature has predominantly focused on optimizing mobile fronthaul latency rather than end-to-end latency, this paper introduces an end-to-end latency model for TDM-PON, specifically within a 10-gigabit Passive Optical Network (XG-PON) based MFH for CRAN. The paper then proposes an Intelligent Dynamic Bandwidth Allocation (DBA) scheme to minimize end-to-end latency of the network. The proposed scheme predicts the buffer occupancy reports using deep learning techniques at optical network units (ONUs). Thereafter, optical line terminal (OLT) schedules the available bandwidth using conventional DBA schemes (Group-GIANT i.e., Ggiant, Optimized Round Robin, Dynamic Service Interval). Primarily, the proposed DBA scheme transforms conventional DBA schemes (Ggiant, Optimized Round Robin, Dynamic Service Interval) into Intelligent versions (Intelligent Ggiant, Intelligent ORR, Intelligent DSI), showcasing a reduction of 28.63\u202f%, 45.86\u202f%, and 48.60\u202f% in end-to-end latency in the XG-PON-based MFH for CRAN. Further, the analysis of obtained results has confirmed the supremacy of the Intelligent DSI DBA scheme over the Intelligent Ggiant and the Intelligent ORR DBA scheme.}
}


@article{DBLP:journals/cn/Kim24,
	author = {Sungwook Kim},
	title = {Hierarchical aerial offload computing algorithm based on the Stackelberg-evolutionary
                  game model},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110348},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110348},
	doi = {10.1016/J.COMNET.2024.110348},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/Kim24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Aerial access networks have been envisioned as a promising 6 G technology to enhance the service experience in underserved areas where terrestrial base stations do not exist. In such scenarios, a hierarchical model of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs) is considered to provide aerial computing services for ground Internet of Things (IoT) devices. In this study, we investigate a hierarchical aerial computing system to optimally orchestrate the limited computation resources in both HAPs and UAVs. For offloading services, we formulate a joint resource allocation problem to maximize service satisfaction for terrestrial IoT devices. To solve this problem, we employ the ideas of game theory with centralized decision and decentralized execution. Through the Stackelberg-evolutionary game model, the HAP works as a leader, and selects its price strategy based on the evolutionary learning process. As followers, individual UAVs make decisions to partially offload their computing tasks by considering different objectives. According to the interactive control paradigm, our proposed method can get reciprocal advantages for HAPs, UAVs, and ground IoT devices while adaptively handling dynamic aerial network conditions. Finally, extensive simulation results verify the efficiency of our proposed algorithm to increase the usability of edge servers’ computational resources. Compared with other existing state-of-the-art aerial network offloading protocols, we can improve the profits of system throughput, resource usability and UAV fairness up to 10 %, 10 %, and 15 %, respectively.}
}


@article{DBLP:journals/cn/GuoXX24,
	author = {Yinzhi Guo and
                  Xiaolong Xu and
                  Fu Xiao},
	title = {{MADRLOM:} {A} Computation offloading mechanism for software-defined
                  cloud-edge computing power network},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110352},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110352},
	doi = {10.1016/J.COMNET.2024.110352},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GuoXX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud-edge computing power network often exhibits complex and heterogeneous structures, posing several challenges to computation offloading that significantly impact network performance and the efficient utilization of computation resources. In this paper, we propose a cloud-edge computing power network architecture that efficiently integrates cloud and edge computing resources into a single network system using software-defined networking technology to support upper-layer business applications. In this context, existing computation offloading methods struggle with issues like users' personalized requirements for latency and energy consumption, as well as the inability to adapt to dynamically complex environments. To overcome these challenges, we introduce a computation offloading mechanism, MADRLOM, focusing on the network's heterogeneity and modeling the computation offloading problem at the cloud-edge end. We formalize the offloading problem as a Markov process and employ a multi-agent deep reinforcement learning algorithm based on priority experience replay sampling to address the planning problem of computation offloading, allowing user equipment to continuously optimize offloading strategies in response to environmental changes and achieve rational resource allocation. Through dynamic offloading strategies, computation tasks can be intelligently allocated to appropriate computing nodes, thereby achieving optimal resource utilization. We utilized the Mininet simulation platform to construct the experimental environment for the software-defined cloud-edge computing power network and compared it with several representative computation offloading strategies. The experimental results demonstrate that the MADRLOM significantly reduces the total system overhead in the software-defined cloud-edge computing network and shows excellent adaptability to environmental changes.}
}


@article{DBLP:journals/cn/PengFZWK24,
	author = {Min Peng and
                  Xianxin Fu and
                  Haiyang Zhao and
                  Yu Wang and
                  Caihong Kai},
	title = {LiKey: Location-independent keystroke recognition on numeric keypads
                  using WiFi signal},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110354},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110354},
	doi = {10.1016/J.COMNET.2024.110354},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PengFZWK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an application of wireless sensing, keystroke recognition can be used in a variety of scenarios, such as password protection and analog input for devices. Existing WiFi-based keystroke recognition systems have strong dependencies on environment and location. Once the environment or keyboard position changes, systems need to re-collect data and re-train the model, otherwise the performance will degrade significantly.}
}


@article{DBLP:journals/cn/GuoSHZL24,
	author = {Feng Guo and
                  Shidong Sun and
                  Junjie Hu and
                  Ning Zhang and
                  Zhiqiang Lv},
	title = {{CIPO:} Efficient, lightweight and programmable packet scheduling},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110355},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110355},
	doi = {10.1016/J.COMNET.2024.110355},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GuoSHZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of network devices and increasingly high CPU workloads, packet scheduling will have to be offloaded to hardware. Programmable packet scheduling allows scheduling algorithms to be programmed into network devices without modifying the hardware. It not only retains the flexibility of software but also the scalability of hardware. In existing primitives, the most expressive Push-In-ExtractOut (PIEO) is prohibitively expensive to implement due to its complexity. While its variant, Push-In-Pick-Out (PIPO), offers some improvements, it suffers from insufficient scalability. In this paper, we propose the Classify-In-Push-Out (CIPO) primitive. The core idea of CIPO is to track the rank and predicate of recent packets through a sliding window, filter and classify packets using a prediction-based two-dimensional classification algorithm and a finite number of First-In-First-Out (FIFO) queues. Through theoretical analysis and evaluation with a range of real workloads, CIPO proves that it has a scheduling performance similar to the most expressive scheduling primitive. Importantly, CIPO requires fewer hardware resources while still providing sufficient expressiveness. Primitive on FPGA show that the CIPO-based scheduler achieves an average of\n1\n.\n24\n×\nhigher throughput than the PIEO-based scheduler but uses only an average of 26.2% of look-up tables (LUTs) and 12.2% of the block RAM of the latter.}
}


@article{DBLP:journals/cn/ZhangLHJ24,
	author = {Xuhui Zhang and
                  Qing Li and
                  Feixue Han and
                  Yong Jiang},
	title = {A receiver-driven transport protocol using differentiated algorithms
                  for differential congestion in datacenters},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110357},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110357},
	doi = {10.1016/J.COMNET.2024.110357},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangLHJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous development of applications, data centers run application traffic from various service providers, and the requirements for data center networks are also increasing. Congestion control has always been a hot topic of discussion. Prevalent sender-driven transport protocols require adjusting their sending rate after network congestion occurs, and the long feedback delay makes them susceptible to a buffer overflow. Therefore, receiver-driven transport protocols are proposed, and the receiver adopts credits to allocate downlink bandwidth to overcome network congestion. This type of scheme solves last-hop congestion and reduces congestion feedback latency. However, the existing receiver-driven transport protocol is insensitive to congestion location and thus reacts inaccurately to congestion. To address this problem, this paper proposes a congestion control algorithm with congestion location awareness. In this scheme, 1) we first design an ECN-based congestion detection method to achieve lightweight congestion types awareness, i.e., instantaneous congestion and continuous congestion. 2) Based on this, we further design a differentiated transmission control strategy. For instantaneous congestion, we adopt adaptive backoff and delay control algorithms. For continuous congestion, we employ a rate control algorithm to reduce in-network congestion. This does not require complex modifications to the switch. In our evaluation, the overall average flow completion time (FCT) of DCC is up to 21%, 89%, 1.5\n×\n, and 3.4\n×\nbetter than Homa, ExpressPass, Timely, and pHost, respectively.}
}


@article{DBLP:journals/cn/KazmiQHNA24,
	author = {Syed Hussain Ali Kazmi and
                  Faizan Qamar and
                  Rosilah Hassan and
                  Kashif Nisar and
                  Mohammed Azmi Al{-}Betar},
	title = {Security of federated learning in 6G era: {A} review on conceptual
                  techniques and software platforms used for research and analysis},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110358},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110358},
	doi = {10.1016/J.COMNET.2024.110358},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KazmiQHNA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is an emerging Artificial Intelligence (AI) paradigm enabling multiple parties to train a model collaboratively without sharing their data. With the upcoming Sixth Generation (6 G) era, FL is expected to adopt a more prevalent role as a potential solution to overcome the challenges of data privacy, security and scalability in distributed and heterogeneous systems. Presently, research works in the security domain of FL in 6 G communication are widely pursued. However, the outcome of research efforts is critically dependent upon the concepts and platforms used during analysis and evaluation. Therefore, after an overview of FL in 6 G networks, this study highlights the requirements of analysis for the security of FL in distributed and heterogeneously involved multiple entities in 6 G networks. This study comprehensively identifies and reviews the potential Conceptual Techniques and Software Platforms for analysis and evaluation in security-related areas of FL in 6 G communication. Further, this study highlights major challenges faced during the analysis of the security of FL in 6G. Finally, this review deliberates upon the potential open research issues that can be pursued using the identified techniques and platforms.}
}


@article{DBLP:journals/cn/DengYY24,
	author = {Tao Deng and
                  Zhanwei Yu and
                  Di Yuan},
	title = {Task offloading optimization in mobile edge computing under uncertain
                  processing cycles and intermittent communications},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110359},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110359},
	doi = {10.1016/J.COMNET.2024.110359},
	timestamp = {Thu, 23 May 2024 09:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DengYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has emerged as a promising solution for addressing the growing computational demands by enabling cloud computing capabilities at the network edge. However, existing MEC models typically make assumptions of known processing cycles and uninterrupted communications, which are not practical in real-world scenarios. This paper aims to tackle the challenges posed by uncertainties and intermittent communications in MEC systems in the context of task offloading. We first derive a closed-form expression for the average offloading success probability in a device-to-device (D2D) assisted MEC system. This expression accurately accounts for uncertain computation processing cycles and intermittent communications. Then, we formulate the task offloading maximization problem (TOMP) and prove its NP-hardness. For problem-solving, we propose two algorithms tailored to different scenarios. In instances exhibiting a uniform structure, we introduce a task scheduling algorithm based on dynamic programming (TSDP). When dealing with general scenarios, we reformulate the problem and propose a repeated matching algorithm (RMA). We use Monte Carlo simulations to validate the closed-form expression. The results demonstrate that gap between them is less than 0.55%, confirming the accuracy of the closed-form expression. Furthermore, the proposed algorithms outperform other algorithms by performance comparison.}
}


@article{DBLP:journals/cn/HuHBC24,
	author = {Jianwei Hu and
                  Kaiqi Huang and
                  Genqing Bian and
                  Yanpeng Cui},
	title = {Redact4Trace: {A} solution for auditing the data and tracing the users
                  in the redactable blockchain},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110360},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110360},
	doi = {10.1016/J.COMNET.2024.110360},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuHBC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the introduction of the General Data Protection Regulation and the advent of the Blockchain 3.0 era, redactable blockchain have become a popular area of research. However, auditing harmful data on redactable blockchain has become an urgent problem to be solved to promote redactable blockchain. Moreover, tracing the identity of users who submit harmful data is a concerning problem that urgently needs to be solved. To address these issues, we propose Redact4Trace, a solution for tracing and auditing data in redactable blockchains. In Redact4Trace, transactions are the unit for minor data editing and redaction, ensuring fine-grained data editing and access. We have designed a data auditing scheme based on the unique characteristics of redactable blockchains. This scheme, which uses a chameleon hash, depends only on the transaction’s hash and effectively protects user privacy. We have also designed an identity tracing scheme based on bilinear maps. Users must use the bilinear mapping to compute address information and store it in the transaction for identity tracing. If the identity information in the transaction is altered, our backup scheme based on the discrete logarithm problem can still trace the user’s identity effectively. Security analyses and experimental results confirm that Redact4Trace is both secure and efficient.}
}


@article{DBLP:journals/cn/WilhelmiAGD24,
	author = {Francesc Wilhelmi and
                  Nima Afraz and
                  Elia Guerra and
                  Paolo Dini},
	title = {The implications of decentralization in blockchained federated learning:
                  Evaluating the impact of model staleness and inconsistencies},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110361},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110361},
	doi = {10.1016/J.COMNET.2024.110361},
	timestamp = {Mon, 03 Mar 2025 21:30:47 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WilhelmiAGD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices’ synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic setting such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains’ modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation by applying two different ML models (ranging from low to high complexity) on the well-known MNIST and CIFAR-10 datasets, respectively, and focus on the accuracy and timeliness of the solutions. Our results show the high impact of model inconsistencies on the accuracy of the models (up to a 35% decrease in prediction accuracy), which underscores the importance of properly designing blockchain systems based on the characteristics of the underlying FL application.}
}


@article{DBLP:journals/cn/DaoTHNNLPNC24,
	author = {Nhu{-}Ngoc Dao and
                  Ngo Hoang Tu and
                  Trong{-}Dai Hoang and
                  Tri{-}Hai Nguyen and
                  Luong Vuong Nguyen and
                  Kyungchun Lee and
                  Laihyuk Park and
                  Woongsoo Na and
                  Sungrae Cho},
	title = {A review on new technologies in 3GPP standards for 5G access and beyond},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110370},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110370},
	doi = {10.1016/J.COMNET.2024.110370},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DaoTHNNLPNC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The world is witnessing the rapid development of commercial fifth-generation (5G) networks that enable diverse applications and services with enhanced mobile broadband, ultra-reliability, low latency, and massive connectivity performance. Technical specifications and requirements for 5G networks have been officially standardized by the Third Generation Partnership Project (3GPP) organization since Release (Rel) 15. Subsequently, several advanced technologies have been involved in the standards through recent updates. This paper investigated the evolution of 5G access networks derived from stable 3GPP standards to obtain a systematic view of the technology. In particular, Rel 15 initiated the first complete 5G network along with the standalone New Radio (NR) architecture. While Rel 16 focused on improving system performance, expanding user cases, and exploiting new spectrum resources, Rel 17 targeted the cellular Internet of Things, multimedia services, and intelligent computing capabilities. In addition, expected features in Rel 18 and Rel 19 are studied and considered for future research for 5G and beyond. This paper aims to provide interested readers with a reference framework on the standard 5G access technology development and trends.}
}


@article{DBLP:journals/cn/CentofantiSGK24,
	author = {Carlo Centofanti and
                  Jos{\'{e}} Santos and
                  Venkateswarlu Gudepu and
                  Koteswararao Kondepu},
	title = {Impact of power consumption in containerized clouds: {A} comprehensive
                  analysis of open-source power measurement tools},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110371},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110371},
	doi = {10.1016/J.COMNET.2024.110371},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CentofantiSGK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, container-based solutions have become de facto compute units of modern cloud-native applications. However, the exponential growth in data traffic and the power consumption of these technologies to handle high data traffic alarm the strong need for energy evaluation approaches in containerized clouds. Furthermore, the proliferation of highly distributed edge clouds raises additional concerns regarding the power consumption of future cloud architectures. This article presents a detailed overview of methods and techniques for monitoring power consumption within popular cloud platforms. The study offers an in-depth evaluation of these approaches, demonstrating variations in measured power consumption based on the chosen technique. A well-known container orchestration platform named Kubernetes (K8s) has been applied in our extensive measurements. This work argues that energy-efficient container clouds will play a vital role in building a more sustainable and eco-friendly digital infrastructure by optimizing power consumption and reducing carbon footprint, paving the way for a greener future. The paper also discusses open challenges and future research directions on energy sustainability, leading to the conclusion, offering lessons learned and prospects on potential solutions to foster sustainable practices within the container ecosystem.}
}


@article{DBLP:journals/cn/HanXZYYHM24,
	author = {Xinbo Han and
                  Guizhong Xu and
                  Meng Zhang and
                  Zheng Yang and
                  Ziyang Yu and
                  Weiqing Huang and
                  Chen Meng},
	title = {{DE-GNN:} Dual embedding with graph neural network for fine-grained
                  encrypted traffic classification},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110372},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110372},
	doi = {10.1016/J.COMNET.2024.110372},
	timestamp = {Mon, 03 Mar 2025 21:30:43 +0100},
	biburl = {https://dblp.org/rec/journals/cn/HanXZYYHM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, most network traffic is encrypted, which protects user privacy but complicates the task of analyzing and classifying encrypted traffic. Identifying the specific categories of encrypted traffic, such as application type or even the specific application, is of great significance for advanced network services and network security management. Many existing methods for encrypted traffic classification rely on machine learning and deep learning techniques, but they exhibit certain shortcomings. A considerable number of these methods rely on statistical features, which may lose their relevance as networks evolve and lead to the loss of important information. Additionally, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) face limitations in extracting features from encrypted traffic, specifically, their inability to learn traffic interaction information within a network flow.}
}


@article{DBLP:journals/cn/PeiYLDH24,
	author = {Hongmei Pei and
                  Peng Yang and
                  Weihao Li and
                  Miao Du and
                  Zhongjian Hu},
	title = {Proxy Re-Encryption for Secure Data Sharing with Blockchain in Internet
                  of Medical Things},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110373},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110373},
	doi = {10.1016/J.COMNET.2024.110373},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PeiYLDH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Medical Things (IoMT) is crucial in our daily health monitoring and assisting doctors in treatment. However, the existing schemes mainly use cloud servers to solve the storage of large amounts of data generated by smart devices, ignoring the guarantee mechanism of ciphertext integrity, which may result in data being tampered with or replaced in data sharing. To address this issue, we propose a secure data sharing scheme named Proxy Re-Encryption for Secure Data Sharing with Blockchain in Internet of Medical Things (PRE-IoMT). Specifically, we propose a new proxy re-encryption data sharing approach by introducing identity hash during the key generation phase to realize the binding of public key and user identity, thus enhancing the security of data sharing in PRE-IoMT. Moreover, we adopt BlockChain transactions to design a pairing function ciphertext check mechanism, which can effectively prevent the ciphertext stored in the cloud server from being tampered with. We add the user to the accumulator by invoking the BlockChain smart contract to realize the management and fast query of users. Finally, we conduct the performance analysis experiments on the four-stage computation of proxy re-encryption, communication overhead, storage overhead, transmission delay, and the delay of BlockChain transactions, compared with existing schemes, our scheme improves the computation efficiency of encryption, re-encryption, decryption, and re-decryption by about\n23\n.\n8\n%\n,\n71\n.\n4\n%\n,\n48\n%\n, and\n15\n.\n3\n%\nrespectively. The security proof also exhibits that PRE-IoMT can resist IND-ID-CPA.}
}


@article{DBLP:journals/cn/CerasuoloNBACPR24,
	author = {Francesco Cerasuolo and
                  Alfredo Nascita and
                  Giampaolo Bovenzi and
                  Giuseppe Aceto and
                  Domenico Ciuonzo and
                  Antonio Pescap{\`{e}} and
                  Dario Rossi},
	title = {{MEMENTO:} {A} novel approach for class incremental learning of encrypted
                  traffic},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110374},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110374},
	doi = {10.1016/J.COMNET.2024.110374},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CerasuoloNBACPR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the ever-changing digital environment, ensuring the ongoing effectiveness of traffic analysis and security measures is crucial. Therefore, Class Incremental Learning (CIL) in encrypted Traffic Classification (TC) is essential for adapting to evolving network behaviors and the rapid development of new applications. However, the application of CIL techniques in the TC domain is not straightforward, usually leading to unsatisfactory performance figures. Specifically, the improvement goal is to reduce forgetting on old apps and increase the capacity in learning new ones, in order to improve overall classification performance—reducing the drop from a model “trained-from-scratch”.}
}


@article{DBLP:journals/cn/LiuX24,
	author = {Libin Liu and
                  Xiuting Xu},
	title = {Marvel: Towards Efficient Federated Learning on IoT Devices},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110375},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110375},
	doi = {10.1016/J.COMNET.2024.110375},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has gained significant attention as a distributed machine learning paradigm, particularly due to its ability to preserve privacy by keeping data on IoT devices and only transmitting model parameters to the central server. However, existing federated learning systems face several challenges in their deployment. These challenges can be categorized into three main areas: data relation obliviousness, model characteristic obliviousness, and low communication bandwidth. To address these issues, we propose Marvel, a novel consolidated federated learning system designed specifically for efficient model training on IoT devices. Marvel tackles the challenges by employing a series of innovative techniques. Firstly, it clusters devices based on their data similarities, leveraging data fingerprints and the Hamming distance between them. This clustering process allows Marvel to group devices with similar data, enabling more efficient model training. Once the device clustering is complete, Marvel initiates the formal model training process. During this process, Marvel dynamically selects one device from each cluster to upload model parameters based on the predicted model convergence speed. This prediction is achieved using a lightweight LSTM-based prediction model, which helps determine the most suitable devices for parameter updates. Furthermore, Marvel employs a Tucker decomposition approach to decompose the parameters before transmission in each communication round. This decomposition reduces the size of the transmitted data and enables faster communication between devices and the central server. The parameters are then reconstructed on the central server, and vice versa when dispatching model parameters to devices. We evaluate Marvel using a 180-device testbed and implement a prototype using PyTorch. The results demonstrate the effectiveness of Marvel, as it achieves an average model training speedup of 59.81% and reduces communication rounds by up to 66.67% compared to existing schemes.}
}


@article{DBLP:journals/cn/ZhangZZCTZL24,
	author = {Jingci Zhang and
                  Jun Zheng and
                  Zheng Zhang and
                  Tian Chen and
                  Yu{-}an Tan and
                  Quanxin Zhang and
                  Yuanzhang Li},
	title = {ATT{\&}CK-based Advanced Persistent Threat attacks risk propagation
                  assessment model for zero trust networks},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110376},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110376},
	doi = {10.1016/J.COMNET.2024.110376},
	timestamp = {Mon, 27 Jan 2025 17:42:21 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhangZZCTZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, the growing frequency and intensity of Advanced Persistent Threats (APTs) have significantly undermined the legitimacy and financial stability of government agencies, enterprises, and other entities. Moreover, these attacks have shown the inherent vulnerabilities in conventional border defense strategies. The emergence of the zero trust network architecture can be attributed to the increasing complexity of the cyber threat landscape. With the application of risk assessment, this paper effectively tackles the challenges posed by conventional network defense limitations and enhances the efficiency of the access control decision-making process. Nevertheless, the existing risk assessment approaches primarily focus on conventional security assessment objectives, which exhibits a deficiency in the ability to dynamically assess APT attacks. The Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) Framework introduced in this paper is a novel approach to mitigating APT attacks. This paper aims to mine and analyze the frequent item set and correlation of cyber threat penetration attack techniques. The paper also intends to construct an attack technique relationship diagram and develop a tactical prediction model for cyber threat penetration attacks using the Markov chain model. Finally, our study aims to establish a risk propagation model for APT threats based on the aforementioned model. The approach presented in this paper significantly enhances the capacity of zero trust networks in addressing sophisticated cyber threats.}
}


@article{DBLP:journals/cn/StergiopoulosKAM24,
	author = {George Stergiopoulos and
                  Panagiotis Kotzanikolaou and
                  Konstantinos Adamos and
                  Lilian Mitrou},
	title = {Scaling private proximity testing protocols for geofenced information
                  exchange: {A} metropolitan-wide case study},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110381},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110381},
	doi = {10.1016/J.COMNET.2024.110381},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/StergiopoulosKAM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Private Proximity Testing (PPT) protocols allow two entities to exchange location-and-time-specific information without any tangible breach of privacy between the two or any third-party intermediary such as a server or a service provider. Numerous PPT protocols have been proposed, however, to our knowledge, experimentation relies on simulations and algorithmic complexity analysis, without tests on large-scale scenarios and actual information exchange over wide metropolitan areas. In this paper, we implement and evaluate a novel PPT geofencing algorithm based on RSA factorization. The prototype used in this work was built in Android and Kotlin, initially funded by Google Digital News Initiative Innovation Fund (digitalnewsinitiative.com). We utilize WoM to implement and test a geofenced PPT protocol using prime factorization over a wide area while users move around different municipalities and (i) present findings concerning efficiency and feasibility of prime factorization PPT implementations over large geographical distributions in terms of energy and data needs, (ii) evaluate complementary technologies needed for such private distribution of location-and-time-specific information like geospatial fences and fragmentation and (iii) consider numerous legal considerations under the EU legal context.}
}


@article{DBLP:journals/cn/GeT24,
	author = {Fei Ge and
                  Liansheng Tan},
	title = {HITLinQ: Improving path capacity through hybrid information-theoretic
                  link scheduling in multi-hop wireless networks},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110382},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110382},
	doi = {10.1016/J.COMNET.2024.110382},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/GeT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By taking advantage of full-duplex radios, a one-sided interference channel model in the case of co-directional transmission on two links having a common node is introduced. Analysis of this model shows that asymmetric increase of transmitter power or decrease of distance will change the path capacity. When the signal noise ratio is high in this channel, using full-duplex radios for simultaneous co-directional transmission will worsen the path capacity, while direct transmission is better. On this basis, a scheduling strategy, called the hybrid information-theoretic link scheduling (HITLinQ) algorithm, which combines self-interference and inter-link interference management with link reconstruction, is proposed. The simulation results show that HITLinQ can provide capacity of not less than several baseline algorithms on a long multi-hop path under given setting.}
}


@article{DBLP:journals/cn/ZhangHCH24,
	author = {Fan Zhang and
                  Hui Huang and
                  Zhixiong Chen and
                  Zhenjie Huang},
	title = {Robust and privacy-preserving federated learning with distributed
                  additive encryption against poisoning attacks},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110383},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110383},
	doi = {10.1016/J.COMNET.2024.110383},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangHCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy-preserving federated learning (PPFL) enables collaborative model training across multiple parties while protecting the privacy of sensitive data. However, PPFL is vulnerable to poisoning attacks, as the indistinguishability of ciphertext allows maliciously crafted gradients to bypass existing defense strategies. Currently, privacy-preserving defense strategies have been proposed to resist poisoning attacks by identifying anomalous gradients under ciphertext. Specifically, these schemes protect privacy by masking the gradient during detection. However, existing schemes come at the cost of reduced security since participants may collude to obtain the mask and then compromise user privacy. In this paper, we propose a robust-enhanced federated learning (REFL) framework to identify malicious gradients over ciphertext and enhance model trustworthiness in scenarios without a trusted entity. Specifically, we design a threshold-based secret generation technology that prevents any single entity from accessing the mask and the private key. Furthermore, We develop a secure consensus technique based on cosine similarity for the identification of maliciously encrypted gradients, enabling Byzantine fault-tolerant aggregation. Finally, we evaluated its defense performance against two backdoor poisoning attacks on the real dataset and compared its computational cost with the Paillier-based defense strategy. The experimental results demonstrate that REFL performs better than the baseline.}
}


@article{DBLP:journals/cn/ZorelloBTMV24,
	author = {Ligia Maria Moreira Zorello and
                  Laurens Bliek and
                  Sebastian Troia and
                  Guido Maier and
                  Sicco Verwer},
	title = {Black-box optimization for anticipated baseband-function placement
                  in 5G networks},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110384},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110384},
	doi = {10.1016/J.COMNET.2024.110384},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZorelloBTMV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of the ever-evolving 5G landscape, where network management and control are paramount, a new Radio Access Network (RAN) as emerged. This innovative RAN offers a revolutionary approach by enabling the flexible distribution of baseband functions across various nodes, all tailored to meet the ever-shifting demands of both system requirements and user traffic patterns. As users move within the network, the need to anticipate and strategically position these baseband functions becomes crucial for seamless network operation. Traditionally, this challenge has been tackled through a two-step process: first, forecasting traffic patterns, and then optimizing resource allocation accordingly. However, this approach falls short in guaranteeing an efficient placement when actual traffic demands surge onto the network. It often leads to resource overbooking, constraint violations, and excessive power consumption, putting strain on the network’s capabilities. In this paper, we introduce a novel framework based on a black-box optimization approach. This tool empowers prediction algorithms not just with historical traffic data but also with insights from optimization outcomes. The goal is to minimize a loss function related to power consumption and constraint violation: this ensures a predicted placement that is feasible and whose power is close to optimal. This approach ensures that the predicted placement is both feasible and power-efficient, bridging the gap between theoretical prediction and practical implementation. Remarkably, our proposed method, while potentially sacrificing some degree of traffic prediction accuracy, outperforms the conventional two-step approach by delivering a more efficient baseband function placement.}
}


@article{DBLP:journals/cn/LiH24,
	author = {Beibei Li and
                  Wei Hu},
	title = {A multi-node attack scheme based on community partitioning in large
                  scale infrastructure networks},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110386},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110386},
	doi = {10.1016/J.COMNET.2024.110386},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node attack plays a vital role in evaluating the reliability of large scale infrastructure networks, which has attracted intensive research interests. Unfortunately, most of them focus on single node attack. Multi-node attack based on MBA (Module based Adaptive) is a promising research vector with great potential in improving efficiency. However, MBA-based node attack typically needs to repetitively consider community structures of large networks during the entire node attack process, which brings significant loss in network reliability and causes computational overheads. In light of this, we propose a novel scheme based on MBA to implement multi-node attack, which adopts the Multilevel community partitioning algorithm to overcome the above limitations. We evaluate our scheme on two artificial synthetic networks and four real-world networks with comparison to four widely used node attack schemes, and demonstrate the non-negligible superiority of our scheme over all benchmarks. Especially in the US Powergrid, our scheme can always achieve robustness around 10 times that of other schemes when deleting the same percentage of nodes. To the best of our knowledge, we are the first to achieve an MBA-based node attack scheme that only needs to execute the community partitioning algorithm once during the entire node deletion process.}
}


@article{DBLP:journals/cn/HanH24,
	author = {Xiaoli Han and
                  Bo Hu},
	title = {A congestion-aware user-cell association scheme to assist with traffic
                  offloading in a three-tier heterogeneous network},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110388},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110388},
	doi = {10.1016/J.COMNET.2024.110388},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HanH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we aim to expand the cell range in congestion areas. To achieve this goal, we integrate UAV network into macro–micro cellular network to enhance coverage. However, the gain of deploying heterogeneous nodes is limit if there are no CRE (cell range expanding)-based method to steer users towards low-power nodes. Therefore, in order to further improve system access capacity, we propose a congestion-aware UA (user–cell association) scheme to offload queued traffic of cellular network into UAV network. To verify the superiority of proposed scheme, we utilize the low-complexity SG (stochastic-geometry) methods. Then in simulation analysis phase, we obtain five primary results:\n(\na\n)\n. The SG-based methods have lower computation complexity, and the convergence of derived expressions are verified.\n(\nb\n)\n. The SG-based methods do not need the exact users’ location but the users’ density, which have improved the operability and robustness in infrastructure construction of wireless network.\n(\nc\n)\n. The congestion-aware UA scheme can prompt network to adjust association tier flexibly, which is very suitable for traffic unevenly-distributed regions.\n(\nd\n)\n. The optimal offsets of virtual power are estimated as\nB\n2\n= −4 dB for small-cell and\nB\n3\n= 3 dB for drone-cell, the resulting improvement in communication capacity is 8%.\n(\ne\n)\n. We suggest the deployment density of UAV sites will no need to exceed\nλ\n3\n= 41\n(\npks/km\n2\n)\n.}
}


@article{DBLP:journals/cn/LiuLZL24,
	author = {Xiaoying Liu and
                  Xiaodong Li and
                  Kechen Zheng and
                  Jia Liu},
	title = {AoI minimization of ambient backscatter-assisted {EH-CRN} with cooperative
                  spectrum sensing},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110389},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110389},
	doi = {10.1016/J.COMNET.2024.110389},
	timestamp = {Wed, 24 Jul 2024 07:50:46 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To evaluate the timeliness and freshness of information from energy-constrained devices, we minimize the weighted average age of information (AoI) of the ambient backscatter-assisted energy harvesting cognitive radio network (AB-EH-CRN), where secondary transmitters (STs) follow the non-orthogonal multiple access (NOMA) strategy to transmit status updates. The secondary receiver (SR) schedules the selected actions for STs to perform spectrum access, and receives status updates from them. We formulate the AoI minimization problem where the action selection and transmission power of STs are considered as variables. To exploit spectrum resources in the AB-EH-CRN with unknown signal-to-noise ratios (SNRs) at the STs, we propose the deep neural network (DNN)-based cooperative spectrum sensing (CSS), and adopt an energy threshold approach for STs to determine the energy allocation between spectrum sensing (SS) and packet transmission. Moreover, we address the AoI minimization problem by the proposed hybrid action and energy penalty deep deterministic policy gradient (HAEP-DDPG) algorithm, which adapts to the hybrid continuous and discrete action spaces. Simulation results validate the advantage of the proposed HAEP-DDPG algorithm compared with comparison schemes in terms of AoI.}
}


@article{DBLP:journals/cn/WangDKJJ24,
	author = {Yi Wang and
                  Yu Deng and
                  Ling Kang and
                  Fang Jiang and
                  Fulin Jiang},
	title = {Reinforcement learning-based energy efficiency optimization for RIS-Assisted
                  {UAV} hybrid uplink and downlink system},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110390},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110390},
	doi = {10.1016/J.COMNET.2024.110390},
	timestamp = {Mon, 03 Mar 2025 21:30:46 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangDKJJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As Reconfigurable Intelligent Surface (RIS) technology can intelligently reflect the incident signal, it can be deployed to maintain a line-of-sight connection and improve the signal strength between unmanned Aerial Vehicles (UAV) and users. Current researches on RIS-assisted UAV communication typically consider optimizing data rates for single-line communications and do not focus on the energy consumption of the UAV. Furthermore, it is frequently challenging to apply conventional algorithms to RIS-assisted UAV optimization problems in a timely way, and the two-dimensional trajectory design cannot fully utilize the 3D mobility of the UAV. In order to increase the data rates for all ground users while improving the energy efficiency of the system, we consider a RIS-assisted full-duplex UAV communication system, in which a DDQN-based algorithm is proposed to jointly optimize the phase shift of the RIS and the 3D trajectory of the UAV. Simulation results demonstrate that the proposed algorithm can achieve a significant data rate gain and energy efficiency compared to time-division duplex systems and other deep reinforcement learning algorithms.}
}


@article{DBLP:journals/cn/KvasicANWAM24,
	author = {Igor Kvasic and
                  Derek W. Orbaugh Antillon and
                  Dula Nad and
                  Christopher R. Walker and
                  Iain A. Anderson and
                  Nikola Miskovic},
	title = {Diver-robot communication dataset for underwater hand gesture recognition},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110392},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110392},
	doi = {10.1016/J.COMNET.2024.110392},
	timestamp = {Sat, 08 Jun 2024 13:14:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KvasicANWAM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a dataset of diving gesture images used for human–robot interaction underwater. By offering this open access dataset, the paper aims at investigating the potential of using visual detection of diving gestures from an autonomous underwater vehicle (AUV) as a form of communication with a human diver. In addition to the image recording, the same dataset was recorded using a smart gesture recognition glove. The glove uses dielectric elastomer sensors and on-board processing to determine the selected gesture and transmit the command associated with the gesture to the AUV via acoustics. Although this method can be used under different visibility conditions and even without line of sight, it introduces a communication delay required for the acoustic transmission of the gesture command. To compare efficiency, the glove was equipped with visual markers proposed in a gesture-based language called CADDIAN and recorded with an underwater camera in parallel to the glove’s onboard recognition process. The dataset contains over 30,000 underwater frames of nearly 900 individual gestures annotated in corresponding snippet folders. The dataset was recorded in a balanced ratio with five different divers in sea and five different divers in pool conditions, with gestures recorded at 1, 2 and 3 metres from the camera. The glove gesture recognition statistics are reported in terms of average diver reaction time, average time taken to perform a gesture, recognition success rate, transmission times and more. The dataset presented should provide a good baseline for comparing the performance of state of the art visual diving gesture recognition techniques under different visibility conditions.}
}


@article{DBLP:journals/cn/LiNL24,
	author = {Junhao Li and
                  Qiang Nong and
                  Ziyu Liu},
	title = {{QBDD:} Quantum-resistant blockchain-assisted deep data deduplication
                  protocol for vehicular crowdsensing system},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110393},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110393},
	doi = {10.1016/J.COMNET.2024.110393},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiNL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicular Crowdsensing System (VCS) has emerged as a promising paradigm for alleviating traffic congestion and improving driving safety due to its convenient collection and aggregation of various driving and traffic-related reports. However, the rapidly growing number of vehicles means that VCS faces the severe challenge of large-capacity data storage. It has been observed that various data deduplication techniques have been devised for VCS. Unfortunately, in most solutions, the deduplication operations are centralized, unable to simultaneously reduce storage costs and bandwidth consumption. Moreover, these schemes are mainly constructed based on number-theoretic assumptions, which makes them susceptible to quantum attacks. Inspired by this fact, we introduce QBDD, a quantum-resistant blockchain-assisted second-level data deduplication protocol for privacy-preserving VCS, enabling efficient deduplication on the Road Side Unit (RSU) and Cloud Service Platform (CSP). We construct this protocol based on the Ring Learning with Errors (RLWE) problem signcryption and proxy re-encryption schemes to ensure data security. The improved Message Locking Encryption (MLE) mechanism hides the label of ciphertext and allows for quick duplicate crowdsensing report checks without compromising data privacy. Additionally, a blockchain-based distributed system is used to store crowdsensing data and maintain greedy lists, enhancing the system’s efficiency and fairness. We provide theoretical proof of QBDD’s security and conduct extensive experiments to demonstrate its practicality in post-quantum secure VCS.}
}


@article{DBLP:journals/cn/WangFJTH24,
	author = {Kaiyu Wang and
                  Guangsheng Feng and
                  Zhenzhou Ji and
                  Zhiying Tu and
                  Shufan He},
	title = {SP-PoR: Improve blockchain performance by semi-parallel processing
                  transactions},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110394},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110394},
	doi = {10.1016/J.COMNET.2024.110394},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangFJTH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As is well known, performance is always an obstacle to blockchain’s further development and application. However, limited by information propagation latency and the existence of conflicted transactions, neither shortening block generation interval, expanding block capacity, nor designing noval parallel processing supported blockchain structures cannot actually improve the performance. To address the performance improvement dilemma, in this paper, starting with the transaction relation, we propose a semi-parallel transaction processing protocol called the “Semi-Parallel Proof-of-Relevance” (SP-PoR). By analyzing the relevance determination condition of different types of transactions, we found that splitting and processing normal transactions and contract transactions respectively are key factors in boosting the entire blockchain performance. Then, theoretically, the parallel processing of unrelated blocks has been proven to have no influence on final consensus achievement. Based on this, a semi-parallel processing method is proposed to parallel and serially process normal and contract transactions based on relevance determination. Furthermore, multi-way tree DAG structure and tangle structure are designed to support our research. Finally, extensive experiments based on real Ethereum transaction data prove that our proposed method can indeed improve blockchain performance in which normal transactions and contract transactions of our method have 8.575 times and 2.54 times improvement respectively. The entire blockchain can experience 2.117 times performance improvement on blockchain synchronization considering practical execution environment deductions.}
}


@article{DBLP:journals/cn/RashidDHS24,
	author = {Fariza Rashid and
                  Ben Doyle and
                  Soyeon Caren Han and
                  Suranga Seneviratne},
	title = {Phishing {URL} detection generalisation using Unsupervised Domain
                  Adaptation},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110398},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110398},
	doi = {10.1016/J.COMNET.2024.110398},
	timestamp = {Sat, 08 Jun 2024 13:14:21 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RashidDHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phishing attacks are a prevailing problem in cybersecurity. In many data breaches, the initial entry can be traced back to phishing. URL-based phishing detection is one of the many ways of phishing attempt detection where solely the properties of the URLs are used to decide whether a given URL is phishing or not. While there are multiple existing works that use machine learning and deep learning to detect phishing URLs, in this paper, we show that such methods lack generalisation (i.e., they work effectively only when the test sets are split from the same training dataset). This is a significant issue since the vast majority of phishing attempts are short-lived and use freshly created domain names. Also, many network vantage points and middleboxes record URLs in slightly different formats and as such, URL data collected at various companies may be different. To address this, we propose an Unsupervised Domain Adaptation-based framework to increase the model transferability between datasets. We evaluate our approach using three datasets and show that the increase in cross-dataset F1 score performance is 0.06 on average and in some cases approximately as high as 0.2.}
}


@article{DBLP:journals/cn/XieSSZ24,
	author = {Mande Xie and
                  Xiangquan Su and
                  Hao Sun and
                  Guoping Zhang},
	title = {Online task offloading algorithm based on multi-objective optimization
                  caching strategy},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110400},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110400},
	doi = {10.1016/J.COMNET.2024.110400},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XieSSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the realm of Mobile Edge Computing (MEC), task offloading has consistently garnered significant attention. Within the context of intricate caching environments and multi-user scenarios, conventional solutions frequently encounter difficulties in simultaneously fulfilling the demands for latency reduction and energy consumption optimization. This paper presents a novel online task offloading algorithm that leverages a multi-objective optimization caching strategy. This algorithm addresses two challenges: the Online Task Offloading (OTO) problem and the Online Task File Caching (OTFC) problem. The OTO problem is conceptualized as a multi-user game, where Nash equilibrium is employed to effectively characterize and address it. This ensures the determination of the optimal offloading strategy in the presence of various caching scenarios. Meanwhile, the OTFC problem is transformed into a Markov decision process, and through the utilization of Deep Q-Networks, we can forecast the requirements of online tasks and subsequently determine the optimal caching vector. The incorporation of the Multi-Objective Cache Policy (MOCP) algorithm precedes the finalization of the caching vector. Rooted in multi-objective optimization, this algorithm adeptly balances various caching decisions, achieving a Pareto optimal outcome. The proposed offloading model that effectively caters to the requirements of task offloading while incorporating the demands of task file caching. Moreover, the MOCP algorithm ensures optimal caching decisions across a broad range of scenarios. Simulation tests reveal that this enhanced offloading algorithm, grounded in multi-objective optimization, outperforms traditional methods in energy conservation, boasting energy savings of up to 15%.}
}


@article{DBLP:journals/cn/LavernelleBGS24,
	author = {Jean de Bonfils Lavernelle and
                  Pierre{-}Fran{\c{c}}ois Bonnefoi and
                  Beno{\^{\i}}t Gonzalvo and
                  Damien Sauveron},
	title = {Assessment of spatial isolation in Jailhouse: Towards a generic approach},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110402},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110402},
	doi = {10.1016/J.COMNET.2024.110402},
	timestamp = {Tue, 18 Jun 2024 09:55:53 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LavernelleBGS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, virtualization has known a growing interest in the embedded systems world mainly for size, weight, power, and cost constraints. For that reason, mixed-criticality systems, which are hardware platforms that host multiple sub-systems with different levels of criticality, become more and more widespread. In mixed-criticality systems, sub-systems can be separated and isolated from each other by software raising various security challenges and considerations. However, having multiple sub-systems running on the same hardware platform, each with its own security requirements and level of criticality, requires strong isolation between them. Static partitioning hypervisors present promising solutions for isolation in embedded systems. Indeed, in addition to most of the benefits of traditional virtualization solutions, they offer good predictability and latency, and are more lightweight which makes them particularly suitable for the context of embedded systems. As mixed-criticality systems are often subject to high-security requirements, it is a matter of concern to ensure that virtualization solutions provide strong and reliable isolation. Among these possible solutions, Jailhouse is a type-1 static partitioning hypervisor whose temporal isolation capacities have been extensively evaluated and which provides good performance to virtual machines. However, the hypervisor has not been extensively explored in terms of the spatial isolation that it must guarantee. This paper aims to give a clear picture of the capacities of Jailhouse in terms of isolation. To this end, this paper first explains the architecture of Jailhouse, followed by an overview of recent related work. Then, the paper introduces the methodology followed for testing spatial isolation and suggests key generic points to consider for assessing hypervisors on this criterion. Finally, the paper details some of our tests that revealed or confirmed interesting insights related to spatial isolation in Jailhouse.}
}


@article{DBLP:journals/cn/WangYWGGYX24,
	author = {Xiangbin Wang and
                  Qingjun Yuan and
                  Yongjuan Wang and
                  Gaopeng Gou and
                  Chunxiang Gu and
                  Gang Yu and
                  Gang Xiong},
	title = {Combine intra- and inter-flow: {A} multimodal encrypted traffic classification
                  model driven by diverse features},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110403},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110403},
	doi = {10.1016/J.COMNET.2024.110403},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangYWGGYX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing prevalence of encryption for data security in network communication highlights the urgent need for effective encrypted traffic identification methods. Challenges such as payload concealment, diverse encryption protocols, and evasion tactics necessitate innovative approaches, including deep learning and multiple features integration. Multimodal learning, which utilizes features from multiple sources, proves more effective than single-modality methods, offering a more comprehensive analysis. However, current methods often overlook valuable inter-flow information, hindering optimal classification. Hence, we propose a novel multimodal encrypted traffic classification model driven by diverse features, named MeDF, which combines intra- and inter-flow features. MeDF leverages both intra-flow and inter-flow features: it constructs spectrograms of flows’ raw bytes and extracts statistical features, combining the two as the intra-flow features. MeDF also builds flow relation graph to extracts inter-flow features, helping the model learn the complex relationships between flows. The intra- and inter-flow features complement each other and extract the maximum amount of valid information from encrypted traffic. Thus, MeDF overcomes the performance limitations of existing multimodal models for encrypted traffic classification. MeDF is validated on 2 real-world traffic datasets with accuracy of 98.57% and 94.73%. The outperformance can be attributed to the complementary information of the employed features and the modeling of complex relationships, when compared to both classical single-modality classification methods and state-of-the-art multimodal methods.}
}


@article{DBLP:journals/cn/FengWLWHXM24,
	author = {Pengbin Feng and
                  Dawei Wei and
                  Qiaoyang Li and
                  Qin Wang and
                  Youbing Hu and
                  Ning Xi and
                  Jianfeng Ma},
	title = {GlareShell: Graph learning-based {PHP} webshell detection for web
                  server of industrial internet},
	journal = {Comput. Networks},
	volume = {245},
	pages = {110406},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110406},
	doi = {10.1016/J.COMNET.2024.110406},
	timestamp = {Fri, 31 May 2024 21:06:36 +0200},
	biburl = {https://dblp.org/rec/journals/cn/FengWLWHXM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of the Industrial Internet scale, cyberattacks targeting industrial control systems also increased. The management and operation of Industrial Internet are usually performed via web servers which retain a large attack surface. In the Industrial Internet, attackers usually exploit vulnerabilities to inject malicious codes for remotely executing commands, stealing confidential data, and invading web servers. Existing approaches capture statistical and contextual dependence information from Webshell using machine learning (ML) or deep learning (DL) algorithms. However, the semantic feature mining of program code within Webshell is not sufficient when entering new types of Webshell. In this paper, we propose a graph learning-based PHP Webshell detection framework, GlareShell, using the word embedding technique, a risk weight allocation mechanism, and the graph neural network (GNN). First, GlareShell leverages static analysis to extract interprocedural control flow graphs (ICFGs) from PHP script files and then prunes these ICFGs to remove noisy statements. Then, word embedding techniques are employed to generate semantic representations from PHP statements. Next, we design a risk weight allocation mechanism to derive the risk levels of statements and concatenate them with word embeddings as attributions. The identified risk levels could guide the passing of potential attack patterns inside GNN models. Finally, GlareShell builds a GNN classifier directly from the ICFG with corresponding node attributions to identify the malicious PHP scripts. Experiment results on collected datasets prove the promise of our graph learning framework in the Webshell detection domain.}
}
