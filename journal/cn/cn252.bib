@article{DBLP:journals/cn/ZhuBLY24,
	author = {Rongxin Zhu and
                  Azzedine Boukerche and
                  Deshun Li and
                  Qiuling Yang},
	title = {Delay-aware and reliable medium access control protocols for UWSNs:
                  Features, protocols, and classification},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110631},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110631},
	doi = {10.1016/J.COMNET.2024.110631},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhuBLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater Wireless Sensor Networks (UWSNs) are garnering significant interest for their broad potential in critical applications, including environmental monitoring, resource exploration, and disaster prevention. These networks, however, face unique challenges due to the complexities of underwater acoustic communication, such as extended propagation delays and environmental disturbances. These issues necessitate the development of sophisticated Medium Access Control (MAC) protocols specifically designed for UWSNs, distinct from those used in Terrestrial Wireless Sensor Networks (TWSNs). This paper provides a comprehensive review of MAC protocols tailored for various task-oriented scenarios in UWSNs, beginning with an in-depth exploration of the challenges specific to designing MAC protocols for these networks. The discussion emphasizes the importance of minimizing transmission latency and maximizing reliability, crucial for effective application-specific operations. This survey categorizes existing protocols based on these critical performance metrics, while also addressing the imperative of energy efficiency. Furthermore, it details the adaptive strategies these protocols employ to cope with the dynamic and challenging underwater acoustic channels. In addition to presenting a comparative analysis of the protocols, this work identifies pressing research gaps and poses open questions, establishing itself as an indispensable resource for researchers seeking to advance the field of MAC protocols in UWSNs.}
}


@article{DBLP:journals/cn/AljeriB24,
	author = {Noura Aljeri and
                  Azzedine Boukerche},
	title = {NEMa: {A} Novel Energy-Efficient Mobility Management Protocol for
                  5G/6G-Enabled Sustainable Vehicular Networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110638},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110638},
	doi = {10.1016/J.COMNET.2024.110638},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AljeriB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the advancement of 5G and the emergence of 6G technologies unfolds, it is anticipated to support a diverse set of technologies, services, energy resources, and storage capabilities, all while maintaining a high quality of service to users and balancing the load on network devices; however, challenges such as energy consumption, network densification, and rapid mobility of vehicles need to be addressed A critical aspect of this evolution is the mitigation of mobility management, communication, and tracking of vehicles’ rapid mobility in network-dense environments, which currently consumes significant power resources. Consequently, existing mobility management protocols must be adapted and reconfigured to meet the requirements and limitations of energy-constrained vehicular networks. In this article, we introduce a novel energy-efficient mobility management protocol, NEMa, specifically designed for vehicular networks. This protocol optimizes network resources and vehicles’ on-board sensing to minimize energy consumption while maintaining a high packet delivery ratio and minimal interruption time. We have evaluated the performance of the proposed protocol and compared it to existing benchmark mobility management solutions in terms of network overhead, latency, and energy efficiency. Our results demonstrate that our proposed protocol surpasses existing benchmark solutions, thereby contributing to the enhancement of 5G/6G and beyond vehicular networks by addressing the challenges of energy efficiency and network performance.}
}


@article{DBLP:journals/cn/HuZSXZ24,
	author = {Yahui Hu and
                  Ziqian Zeng and
                  Junping Song and
                  Luyang Xu and
                  Xu Zhou},
	title = {Online network traffic classification based on external attention
                  and convolution by {IP} packet header},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110656},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110656},
	doi = {10.1016/J.COMNET.2024.110656},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HuZSXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic classification is an important part of network monitoring and network management. Three traditional methods for network traffic classification are flow-based, session-based, and packet-based, while flow-based and session-based methods cannot meet the real-time requirements and existing packet-based methods will violate user’s privacy. To solve the above problems, we propose a network traffic classification method only by the IP packet header, which satisfies the requirements of both the user’s privacy protection and online classification performances. Through statistical analyses, we find that IP packet header information is effective on the network traffic classification tasks and this conclusion is also demonstrated by experiments. Furthermore, we propose a novel external attention and convolution mixed (ECM) model for online network traffic classification. This model adopts both low-computational complexity external attention and convolution to respectively extract the byte-level and packet-level characteristics for traffic classification. Therefore, it can achieve high classification accuracy and low time consumption. The experiments show that ECM can reach over 96% classification accuracy on four datasets and the classification time is 0.36 ms per packet which can meet the real-time requirements. The code is available at https://github.com/CNZZQ1030/ECM-for-Network-Traffic-Classification.}
}


@article{DBLP:journals/cn/FazioMV24,
	author = {Peppino Fazio and
                  Miralem Mehic and
                  Miroslav Vozn{\'{a}}k},
	title = {Next-cell and mobility prediction in new generation cellular systems
                  based on convolutional neural networks and encoding mobility data
                  as images},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110657},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110657},
	doi = {10.1016/J.COMNET.2024.110657},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FazioMV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobility prediction has been a popular research topic for many decades. With the advent of new generation technologies (5G and beyond) and smaller coverage cells, hand-over operations have become more frequent. Cellular system companies are therefore taking increasing interest in using the available predictive information on node movements to optimize and manage their bandwidth resources. In particular, the main challenging scope of our contribution consists in solving the issue of reliable next-cell prediction, aimed to call dropping probability minimization. In addition, our proposal is based on the innovative concept of mobility data to image encoding. The scheme is able to a-priori determine the next visited cells during host movements by applying a convolutional neural approach to mobility images. The power of machine learning is used to advantage, and highly accurate image classification is achieved for mobility prediction. We performed numerous simulation campaigns related to next-cell prediction in mobile cellular environments, obtaining very satisfactory results by the application of convolutional neural networks, which have an impressive history of effectiveness with image classification problems. The trained network has been associated to each coverage cell and the prediction accuracy has been evaluated.}
}


@article{DBLP:journals/cn/AshfaqS24,
	author = {Khuram Ashfaq and
                  Ghazanfar Ali Safdar},
	title = {Energy efficiency and interoperability through {O-RAN} Rapid Transition
                  Protocol {(ORTP)}},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110658},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110658},
	doi = {10.1016/J.COMNET.2024.110658},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AshfaqS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile network traffic is increasing accompanied by increased energy consumption. Traditional Radio Access Networks (RANs) have been used for decades as the foundation of mobile communications, providing mobile customers with reliable voice and data services. However, they encounter obstacles in terms of scalability, adaptability, and cost-effectiveness. Next-generation wireless networks need efficient Radio Access Network (RAN) solutions to achieve low latency and high throughput. The Open RAN (O-RAN) architecture is a potential solution for 5G and beyond networks due to its open interfaces, disaggregated network entities and services, network hardware and software virtualization, and intelligent control. Traditionally standard RAN accounts for the bulk of mobile network energy consumption, leading towards higher Operational expenditure (OPEX) costs. In the context of cellular networks. Handover strategies need careful development to cater for efficient resource usage as well as overall system energy efficiency. O-RAN Rapid Transition Protocol (ORTP) presented in this paper employs minimum value of handover margin (HOM) and is aimed at increased energy efficiency. 5G based System level simulations have been performed to investigate the efficacy of ORTP for key performance indicators including handover probability, Radio Link Failure, connection density, ping pong effect and dynamic power consumption. It is found that the usage of lower HOM values provide noticeable achievement in terms of power consumption, thereby rendering ORTP around 20 % more efficient compared to 5G networks, while keeping it interoperable.}
}


@article{DBLP:journals/cn/MaZZPLG24,
	author = {Zhongyu Ma and
                  Xueyao Zhang and
                  Shunbao Zhang and
                  Jianbing Pu and
                  Xianghong Lin and
                  Qun Guo},
	title = {Access strategies in mmWave cell-free network: {A} matching and auction
                  theory based approach},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110659},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110659},
	doi = {10.1016/J.COMNET.2024.110659},
	timestamp = {Mon, 03 Mar 2025 21:30:44 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MaZZPLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The mmWave cell-free network is viewed as an energy-efficient and emerging architecture for beyond 5G systems. However, many practical issues should be solved for this system before the benefits are enjoyed, whereof feasible implementation of access strategy is one of the most vital. To this end, the access strategy including user scheduling and bandwidth allocation is investigated in this paper. Firstly, to maximize the defined utility function, the joint optimization problem of user scheduling and bandwidth allocation is formulated as a mixed integer nonlinear programming problem, which is intractable to search for an optimal solution in polynomial time. Secondly, a two-stage matching and sealed-auction-based access strategy is proposed to obtain a sub-optimal solution, where UEs with high-quality channel conditions are scheduled in the first-stage matching as much as possible, and the rest unmatched UEs are scheduled in the second-stage matching with the sealed-auction based bandwidth allocation. Thirdly, a simplified full-matching based access strategy with lower complexity is proposed for the high-dynamic scenario caused by the high-speed movement of UEs, and the complexities and convergence of the two proposed strategies are quantitatively analyzed. Finally, the performance of proposed strategies is evaluated through abundant simulations in different scenarios, and the superiorities of the proposed strategies are demonstrated through the comparison with reference strategies.}
}


@article{DBLP:journals/cn/BiFZYLY24,
	author = {Ye Bi and
                  Kai Fan and
                  Zhilin Zeng and
                  Kan Yang and
                  Hui Li and
                  Yintang Yang},
	title = {Seamless group handover authentication protocol for vehicle networks:
                  Services continuity},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110661},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110661},
	doi = {10.1016/J.COMNET.2024.110661},
	timestamp = {Sun, 06 Oct 2024 21:22:02 +0200},
	biburl = {https://dblp.org/rec/journals/cn/BiFZYLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advantages of low latency, high reliability and high bandwidth of dense cellular network support real-time sensing, decision making and regulation of vehicular network. Network densification demands smaller coverage areas for base stations that are strategically deployed near consumers to support multiple services. This results in more frequently triggered handover processes. To reduce the latency induced by the handover process and ensure service continuity, this paper proposes a seamless vehicle group handover authentication protocol named SVGHP. The proposed protocol utilizes lightweight algorithms such as quadratic residual and symmetric encryption, and generates and distributes keys for group members based on the Chinese Residual Theorem to achieve handover authentication. Secondly, SVGHP includes a re-authentication phase, specifically, the vehicle group and the subsequent base stations receive the updated keys distributed by the previous RSUs for simplified authentication. And the involvement of the cloud server is avoided, eliminating the network communication delay to the core network. Detailed performance evaluation and comparison with existing protocols demonstrates that the proposed protocol is efficient in terms of computational cost and communication overhead. And in general, our scheme reduces the handover delay by at least 67.91% over the previous authentication schemes. We also provide formal and informal security proofs to prove that our protocol meets common security requirements.}
}


@article{DBLP:journals/cn/LinLLZZW24,
	author = {Jing Lin and
                  Weiwei Lin and
                  Hang Lin and
                  Longlong Zhu and
                  Dong Zhang and
                  Chunming Wu},
	title = {P4Rex: Accelerating regular expression matching with programmable
                  switches},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110662},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110662},
	doi = {10.1016/J.COMNET.2024.110662},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LinLLZZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Regular expression matching is pivotal in numerous network applications. With the ever-increasing scale of data center traffic, deploying regular expression matching modules on traditional servers struggles to meet throughput demands. Emerging programmable switches have brought new prospects for high-speed pattern matching. However, deploying regular expression matching onto programmable switches presents the challenge of space explosion incurred by compiling regular expressions into a Deterministic Finite Automata (DFA). In this paper, we introduce P4Rex, a regular expression matching system designed for programmable switches. P4Rex synergistically leverages two following techniques: an efficient regular expression grouping algorithm that partitions regular expressions into different groups to reduce memory consumption, and DFA compression technology to achieve transition sharing. Experimental results demonstrate that P4Rex exhibits an average improvement of 17% and maximum improvement of 30% on memory consumption compared to prior regular expression grouping schemes and saves more than 10x memory consumption compared to deploying DFA directly on programmable switch.}
}


@article{DBLP:journals/cn/JiaXZSW24,
	author = {Kunkun Jia and
                  Hui Xia and
                  Rui Zhang and
                  Yue Sun and
                  Kai Wang},
	title = {Multi-agent {DRL} for edge computing: {A} real-time proportional compute
                  offloading},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110665},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110665},
	doi = {10.1016/J.COMNET.2024.110665},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/JiaXZSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Industrial Internet of Things, devices with limited computing power and energy storage often rely on offloading tasks to edge servers for processing. However, existing methods are plagued by the high cost of device communication and unstable training processes. Consequently, Deep reinforcement learning (DRL) has emerged as a promising solution to tackle the computation offloading problem. In this paper, we propose a framework called multi-agent twin delayed shared deep deterministic policy gradient algorithm (MASTD3) based on DRL. Firstly, we formulate the task offloading conundrum as a long-term optimization problem, which aids in mitigating the challenge of deciding between local or remote task execution by a device, leading to more effective task offloading management. Secondly, we enhance MASTD3 by introducing a priority experience replay buffer mechanism and a model sample replay buffer mechanism, thus improving sample utilization and overcoming the cold-start problem associated with long-term optimization. Moreover, we refine the actor-critic structure, enabling all agents to share the same critic network. This modification accelerates convergence speed during the training process and reduces computational costs during runtime. Finally, experimental results demonstrate that MASTD3 effectively addresses the proportional offloading problem, which is optimized by 44.32%, 29.26%, and 17.47% compared to DDPQN, MADDPG, and FLoadNet.}
}


@article{DBLP:journals/cn/TangCZQLM24,
	author = {Dan Tang and
                  Hongbo Cao and
                  Jiliang Zhang and
                  Zheng Qin and
                  Wei Liang and
                  Xiaopu Ma},
	title = {{EXCLF:} {A} LDoS attack detection {\&} mitigation model based
                  on programmable data plane},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110666},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110666},
	doi = {10.1016/J.COMNET.2024.110666},
	timestamp = {Mon, 03 Mar 2025 21:30:46 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TangCZQLM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The SDN architecture decouples control plane from data plane, making it more susceptible to various abnormal traffic and network attacks, with LDoS attack being one of them. LDoS attackers periodically send short-duration pulses with high rate to bottleneck links to preempt legitimate TCP traffic bandwidth, severely disrupting the transmission of TCP traffic. Current researches on LDoS attacks are mostly implemented in SDN environments, making them exhibit poor portability during deployment and unavoidable time delays. This paper proposes EXCLF, a LDoS attack detection and mitigation model fully deployed on programmable data plane. To identify LDoS attacks, the model gathers features of the traffic going through the switch and feeds them into a decision tree. Once LDoS attack happens, the model collects data at flow level to pinpoint the attacker and initiates corresponding mitigation measures. Extensive experiments were conducted to evaluate the proposed model, and the results indicate that EXCLF achieves a correct rate of 96.39%, with false positive and false negative rates both below 3%. Additionally, the model demonstrates low detection latency and can quickly respond to attacks. The model proves to be an attack detection and mitigation method with good portability and efficiency.}
}


@article{DBLP:journals/cn/XieAJZL24,
	author = {Mangang Xie and
                  Baozhen An and
                  Xiangdong Jia and
                  Meng Zhou and
                  Jintian Lu},
	title = {Simultaneous update of sensing and control data using free-ride codes
                  in vehicular networks: An age and energy perspective},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110667},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110667},
	doi = {10.1016/J.COMNET.2024.110667},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XieAJZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-time Internet of Things networks, age of information (AoI) and energy cost (EC) are generally deemed as two significant performance metrics to characterize the information freshness and energy efficiency. This paper aims at updating not just sensing data but sensing data and control data simultaneously in vehicular networks. By employing the promising free-ride coding mechanism, the randomly encoded control data is superposed on the low-density parity-check coded sensing data and then is sent to the receiver, consuming neither additional bandwidth nor transmission power. Considering the time and energy of both status update generation and transmission, the average AoI (AAoI) and average EC (AEC) expressions are derived for preemptive and blocking updating schemes with truncated automatic repeat request protocol. Numerical simulations present the AAoI of both sensing data and control data, as well as the AEC of the transmitter, which demonstrate that free-ride coding mechanism not only can improve the freshness of control data but impose an insignificant influence on the performance of sensing data. Moreover, comparison with the blocking scheme suggests that the preemptive one obtains the smaller AAoI but the larger AEC.}
}


@article{DBLP:journals/cn/ChitiPP24,
	author = {Francesco Chiti and
                  Roberto Picchi and
                  Laura Pierucci},
	title = {A survey on non-terrestrial quantum networking: Challenges and trends},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110668},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110668},
	doi = {10.1016/J.COMNET.2024.110668},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChitiPP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellites and aerial platforms can be significant in the development of new telecommunications networks, allowing to deploy networks with extremely high performance even in remote areas. Recently, considering the development of quantum communication technologies, many studies are concentrated on Free Space Optic (FSO) Non-Terrestrial links, also considering that the studies performed so far on Optical Fibers (OFs) have not pointed out significant results, especially in terms of coverage. Moreover, the decrease of launch costs and the use of new satellite technologies such as CubeSats make the deployment of satellite constellations affordable. Thanks to the deployment of Non-Terrestrial Quantum Networks (NTQNs), the novel services offered by the Quantum Internet (QI) could be made easily accessible to all. In particular, Quantum Computers (QCs) can perform beyond the capabilities of any classical processor, and the interconnection of these devices on a global scale makes it possible to achieve extremely high computational capabilities, thus opening the way to new applications, such as Quantum Cloud (QCloud). Therefore, the recent studies in this area motivate this survey, which in addition to providing some base concept of quantum information, describes the current technologies needed to create quantum NT backbones. Specifically, this survey examines some studies over the period 2010–2023, which are related to the realization of quantum communications using satellites and other aerial platforms such as High Altitude Platforms (HAPS) and quantum drones. Some of the architectures that are described in this survey make use of the Software-Defined Networking (SDN) paradigm, which can be envisaged as a key enabler for the management of heterogeneous QNs in general. Furthermore, through the description of various experiments, the paper shows the applications that can be enabled by the future quantum NT backbones. In particular, in addition to improving the security of current systems through the Quantum Key Distribution (QKD), it could be possible to interconnect clusters of QCs over long distances and create extremely reliable positioning systems. Finally, the paper outlines some possible future developments.}
}


@article{DBLP:journals/cn/LiCZZ24,
	author = {Chao Li and
                  Yanan Cheng and
                  Zhaoxin Zhang and
                  Zundong Zhang},
	title = {Censorship data-driven {DNS} resolution anomaly detection: An ensemble
                  algorithm model with multivariate feature fusion},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110669},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110669},
	doi = {10.1016/J.COMNET.2024.110669},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiCZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Domain Name System (DNS), as a critical component of Internet infrastructure, is crucial for maintaining network stability and security. However, vulnerabilities in the DNS resolution process make it susceptible to various network attacks. Current DNS resolution anomaly detection methods are challenged by the scarcity of diverse anomalous sample data and the difficulty in accurately capturing such anomalies. To address those challenges, we firstly introduce a proactive anomaly detection method based on bidirectional national censorship behavior, abbreviated as CB-BiDAM. This method can collect diverse anomalous resolution data from multiple network spaces, covering common types of resolution anomalies and effectively solving the problem of sample scarcity. Further, we extract multidimensional features from four key aspects: response content, DNS attributes, resolution paths, and timing, to gain a deeper understanding of the relationship between multifaceted information in the DNS resolution process and anomalous events. Finally, based on these multidimensional features, we construct a DNS resolution anomaly detection model named DRADC, using a stacked ensemble approach. Through a series of comparative experiments, the DRADC model significantly outperforms existing machine learning algorithms in key metrics such as accuracy (97.48%), recall (96.87%), and F1 score (97.09%). Feature ablation experiments further demonstrates that incorporating multidimensional features significantly improves model performance, with a 3% increase in accuracy and a 3.5% increase in precision compared to models relying solely on response content features. By providing an accurate detection model for DNS resolution anomalies, this study aids network administrators in more effectively identifying and countering network attacks. Additionally, our research also contributes to the detection and response to domain censorship mechanisms, which is crucial for maintaining the openness and free flow of the Internet.}
}


@article{DBLP:journals/cn/AnbazhaganM24,
	author = {S. Anbazhagan and
                  R. K. Mugelan},
	title = {Next-gen resource optimization in NB-IoT networks: Harnessing soft
                  actor-critic reinforcement learning},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110670},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110670},
	doi = {10.1016/J.COMNET.2024.110670},
	timestamp = {Thu, 22 Aug 2024 20:25:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AnbazhaganM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource allocation in Narrowband Internet of Things (NB-IoT) networks is a complex challenge due to dynamic user demands, variable channel conditions, and distance considerations. Traditional approaches often struggle to adapt to the dynamic nature of these environments. In this study, we leverage reinforcement learning (RL) to address the intricate nature of NB-IoT resource allocation. Specifically, we employ the Soft Actor–Critic (SAC) algorithm, comparing its performance against conventional RL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). The Soft Actor–Critic (SAC) algorithm is employed to train an agent for adaptive resource allocation, considering energy efficiency, throughput, latency, fairness, and interference constraints. The agent adeptly balances these objectives through an intricate reward structure and penalty mechanisms. Through comprehensive analysis, we present performance metrics, including total reward, energy efficiency, throughput, fairness, and latency, showcasing the efficacy of SAC when compared to DQN and PPO. Our findings underscore the efficiency of SAC in optimizing resource allocation in NB-IoT networks, offering a promising solution to the complexities inherent in such dynamic environments. Resource allocation in Narrowband Internet of Things (NB-IoT) networks presents a complex challenge due to dynamic user demands, variable channel conditions, and distance considerations. Traditional approaches often struggle to adapt to these dynamic environments. This study leverages reinforcement learning (RL), specifically the Soft Actor–Critic (SAC) algorithm, to address the intricacies of NB-IoT resource allocation. We compare SAC’s performance against conventional RL algorithms, including Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). The SAC algorithm is utilized to train an agent for adaptive resource allocation, focusing on energy efficiency, throughput, latency, fairness, interference constraints, recovery time, and long-term performance stability. To demonstrate the scalability and effectiveness of SAC, we conducted experiments on NB-IoT networks with varying deployment types and configurations, including standard urban and suburban, high-density urban, industrial IoT, rural and low-density, and IoT service providers. To assess generalization capability, we tested SAC across applications like smart metering, smart cities, smart agriculture, and asset tracking & management. Our comprehensive analysis demonstrates that SAC significantly outperforms DQN and PPO across multiple performance metrics. Specifically, SAC improves energy efficiency by 5.60% over PPO and 10.25% over DQN. In terms of latency, SAC achieves a marginal reduction of approximately 0.0124% compared to PPO and 0.0126% compared to DQN. SAC enhances throughput by 214.98% over PPO and 15.72% over DQN. Additionally, SAC shows a substantial increase in fairness (Jain’s index), improving by 358.31% over PPO and 614.46% over DQN. SAC also demonstrates superior recovery time, improving by 18.99% over PPO and 25.07% over DQN. In both deployment scenarios and diverse IoT applications, SAC consistently achieves high total rewards, minimal fluctuations, and stable performance. Energy efficiency remains constant at 7.2 bits per Joule, and latency is approximately 0.080 s. Throughput is robust across different deployments, while fairness remains high, ensuring equitable resource allocation. Recovery times are stable, enhancing operational reliability. These results underscore SAC’s efficiency and robustness in optimizing resource allocation in NB-IoT networks, presenting a promising solution to the complexities of dynamic environments.}
}


@article{DBLP:journals/cn/PokhrelKFOHW24,
	author = {Shiva Raj Pokhrel and
                  Jonathan Kua and
                  Brenton Fleming and
                  Sebnem Ozer and
                  Jeff Howe and
                  Anwar Walid},
	title = {Multipath {TCP} implementation under FreeBSD-13 for pluggable machine
                  learning models},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110671},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110671},
	doi = {10.1016/J.COMNET.2024.110671},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/PokhrelKFOHW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath TCP (MPTCP) has been implemented and evaluated using the modular approach known as ModCC in FreeBSD. This research builds a foundation for developing an experimental platform and implementing MPTCP protocol stacks in FreeBSD-13 to plug machine learning (ML) models. Several independent and interoperable implementations are essential for the Internet Engineering Task Force (IETF) recognition of emerging ML-based MPTCP algorithms. We implement MPTCP under FreeBSD-13 and make them publicly available as dynamically pluggable user and kernel modules for testing and experimentation by the wider network and the Internet community. In particular, we redesign MPTCP over FreeBSD and implement enhanced ModCC in V13.1. In addition, we develop and implement a new kernel module CC-DRL to handle two system calls: (i) drl_get_buffer to marshal data from the kernel to ML models in the user space and (ii) drl_update_cwnd to marshal data from the ML models to the kernel space. CC-DRL operates in parallel with ModCC. We present new insights and concise descriptions of MPTCP implementation with modular multipath scheduling and highlight challenges to assist other parallel initiatives to develop compatible MPTCP stacks. The code and implementation are open source to FreeBSD 13.1.}
}


@article{DBLP:journals/cn/ZhengGLSYDSNJJ24,
	author = {Yulu Zheng and
                  Tiansheng Gu and
                  Shangdong Liu and
                  Sisi Shao and
                  Qian Yang and
                  Hongyu Du and
                  Yuchen Shi and
                  Yijun Nie and
                  Zongkai Ji and
                  Yimu Ji},
	title = {Highly reliable DHR-based polar compilation code communication method},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110673},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110673},
	doi = {10.1016/J.COMNET.2024.110673},
	timestamp = {Wed, 06 Nov 2024 15:05:30 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZhengGLSYDSNJJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of fifth-generation mobile communication technology, 5G, has raised the bar for data transmission rates, information security, and reliability. Polar codes, the channel coding scheme in the 5G communication standard, encounter issues of singularity, limitations, and passivity in the face of unknown attacks. Therefore, this study introduces a highly reliable DHR-based polar coding communication approach. Drawing parallels between cyber attacks and channel noise, the method incorporates reconfigurable executors, decoding decisions, strategy scheduling, and other mechanisms to enable active defense. The True Relatively Axiom (TRA) is employed to theoretically validate the method’s heterogeneity, redundancy, and dynamics. Comparative simulation experiments with traditional polar decoding algorithms, such as SCL, SC, BP, SCAN, and SSC decoding algorithms, demonstrate that the proposed method not only improves secure transmission efficiency but also guarantees secure and reliable transmission across various attack scenarios. Our code is available at https://github.com/yuluzi/my-code.git.}
}


@article{DBLP:journals/cn/GomezBCVG24,
	author = {Blas G{\'{o}}mez and
                  Suzan Bayhan and
                  Estefan{\'{\i}}a Coronado and
                  Jos{\'{e}} Villal{\'{o}}n and
                  Antonio Garrido},
	title = {{LESS-ON:} Load-aware edge server shutdown for energy saving in cellular
                  networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110675},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110675},
	doi = {10.1016/J.COMNET.2024.110675},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/GomezBCVG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While advances in wireless networks enable novel services with previously unreachable latency guarantees, edge computing becomes essential for delivering computing resources close to the users and meeting the strict latency requirements. However, addressing the energy footprint of computing resources is crucial amid the pressing sustainability concerns. The energy consumption of idle resources accounts for a significant part of the total energy footprint. While server shutdown during low-demand periods is common in cloud computing, it is challenging to determine which edge servers to shut down and how to route requests due to the stringent latency requirements of the applications. Thus, this work formulates an optimal orchestration policy to minimize the energy consumption of the edge computing infrastructure and presents LESS-ON, a strategy with a polynomial time complexity that reduces the operational energy footprint of edge computing by shutting down edge servers during low-demand periods. In contrast to previous studies, LESS-ON considers the energy requirements associated with routing requests to the designated edge servers. Our numerical evaluation shows that LESS-ON reduces the total consumption by 42% with respect to the common always-on approach during low-demand periods and by 35% over 24 h, all while meeting latency requirements.}
}


@article{DBLP:journals/cn/BusaccaGPPQP24,
	author = {Fabio Busacca and
                  Laura Galluccio and
                  Sergio Palazzo and
                  Andrea Panebianco and
                  Zhuoran Qi and
                  Dario Pompili},
	title = {Adaptive versus predictive techniques in underwater acoustic communication
                  networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110679},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110679},
	doi = {10.1016/J.COMNET.2024.110679},
	timestamp = {Mon, 09 Dec 2024 22:47:19 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BusaccaGPPQP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Underwater communications suffer from numerous challenges typically associated with relevant signal attenuation, long propagation delay, limited available bandwidth, and high error rates that severely affect underwater transmission performance. Therefore, it is crucial to apply adaptive or predictive techniques to ensure the best possible performance and guarantee reliability in underwater communication, especially in rapidly changing environments. Using adaptive (i.e., reactive) or predictive (i.e., proactive) methods, it is possible to avoid data retransmission, improve the lifetime of underwater nodes, reduce maintenance frequency and the necessary equipment replacement and recharge, and consequently optimize performance in general. In this regard, many works in the literature propose various adaptive or predictive techniques for UnderWater Acoustic (UWA) networks, which we critically classify and discuss in this qualitative survey.}
}


@article{DBLP:journals/cn/WangCZ24,
	author = {Jian Wang and
                  Xin Cheng and
                  Guosheng Zhao},
	title = {Social user role value analysis and trusted user autonomous diffusion
                  for participatory crowdsensing},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110680},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110680},
	doi = {10.1016/J.COMNET.2024.110680},
	timestamp = {Thu, 08 Aug 2024 09:22:50 +0200},
	biburl = {https://dblp.org/rec/journals/cn/WangCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social participatory-sensing, as an emerging research field, is extremely challenging to recruit trustworthy and active high-value users in a sparse user context. And the richness and closeness of users' social relationships provide new research ideas. Therefore, a participatory sensing user recruitment method based on dynamic social network role detection (DSRD) is proposed, in which firstly, multiple identity information of users is mined based on the decomposition of their overlapping social relationships to filter out high-quality sensing users. Secondly, based on role-oriented network representation learning, it models users' role information and establishes a role hierarchy model to evaluate users' social functions and role values. Finally, the concept of temporal social centrality is proposed for the first time for integrating users' social and network structural features to assess the overall value of users and ensure the coverage of task assignments under a sparse user pool. Experimental results on the open datasets Gowalla and Brightkite show that under the constraints of cost budget and number of users, the proposed user recruitment framework DSRD effectively improves task coverage with less time overhead compared to the baseline algorithm.}
}


@article{DBLP:journals/cn/SongWXHL24,
	author = {Chao Song and
                  Jie Wu and
                  Kunyang Xian and
                  Jianfeng Huang and
                  Li Lu},
	title = {Spatio-temporal graph learning: Traffic flow prediction of mobile
                  edge computing in 5G/6G vehicular networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110676},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110676},
	doi = {10.1016/J.COMNET.2024.110676},
	timestamp = {Wed, 27 Nov 2024 13:12:12 +0100},
	biburl = {https://dblp.org/rec/journals/cn/SongWXHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) is a key technology that emerged to address the increasing computational demands and communication requirements of vehicular networks. It is a form of edge computing that brings cloud computing capabilities closer to end-users, specifically within the context of vehicular networks, which are part of the broader Internet of Vehicles (IoV) ecosystem. However, the dynamic nature of traffic flows in MEC in 5G/6G vehicular networks poses challenges for accurate prediction and resource allocation when aiming to provide edge service for mobile vehicles. In this paper, we present a novel approach to predict the traffic flow of MEC in 5G/6G vehicular networks using graph-based learning. In our framework, MEC servers in vehicular networks are construed as nodes to construct a dynamic similarity graph and a dynamic transition graph over a duration of multiple days. We utilize Graph Attention Networks (GAT) to learn and fuse the node embeddings of these dynamic graphs. A transformer model is subsequently employed to predict the vehicle frequency accessing the edge computing services for the next day. Our experimental results have shown that the model achieves high accuracy in predicting edge service access volumes with low error metrics.}
}


@article{DBLP:journals/cn/LiPSSLT24,
	author = {Mianjie Li and
                  Qihan Pei and
                  Chun Shan and
                  Shen Su and
                  Yuan Liu and
                  Zhihong Tian},
	title = {Data tampering detection and recovery scheme based on multi-branch
                  target extraction for internet of vehicles},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110677},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110677},
	doi = {10.1016/J.COMNET.2024.110677},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiPSSLT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of new networks such as 5 G/6 G, the Internet of Vehicles has been given features such as hyper-connectivity and hyper-intelligence, promoting the implementation of new scenarios for autonomous vehicles. However, on the Internet of vehicles, vehicles use many cameras, radars and other sensors to sense the environment and execute instructions, facing security issues such as road data tampering and hijacking. Consequently, this paper presents a data tampering detection and recovery scheme based on multi-branch target extraction. Specifically, for road images collected by sensors, this paper presents a target extraction method based on multi-branch spatial feature pyramid blocks to obtain road salient targets. Then, a tamper detection and recovery algorithm based on interval mapping is presented. Once the image road target is tampered with, this method can quickly detect the tampering traces and restore them to achieve the authenticity and integrity of the road image on the Internet of Vehicles. The availability of the proposed scheme is verified through comparative experiments, and the performance is improved satisfactorily compared with other works.}
}


@article{DBLP:journals/cn/ShariqCSLDCM24,
	author = {Mohd Shariq and
                  Mauro Conti and
                  Karan Singh and
                  Chhagan Lal and
                  Ashok Kumar Das and
                  Shehzad Ashraf Chaudhry and
                  Mehedi Masud},
	title = {Anonymous and reliable ultralightweight RFID-enabled authentication
                  scheme for IoT systems in cloud computing},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110678},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110678},
	doi = {10.1016/J.COMNET.2024.110678},
	timestamp = {Sun, 06 Oct 2024 21:22:04 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShariqCSLDCM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuing growth in various research domains such as Artificial Intelligence (AI), Machine Learning (ML), Big Data Analytics (BDA), and Cloud Computing (CC), the Internet of Things (IoT) has become a popular technology nowadays. It provides a virtual interaction between physical objects and the cyber world over the Internet without human intervention. The IoT devices are embedded with sensors, software, or some other useful technologies for connecting, sharing, and exchanging data with other devices. Among recent technologies impacting human lives, Radio Frequency IDentification (RFID) has become a core identification technology that can be integrated into the IoT environments which connect billions of things or objects. However, an RFID system has two major issues: (i) an adversary can tamper or intercept the sensitive information of the RFID tags, which may cause forgery and privacy problems, and (ii) RFID tags have limited computational power capability which makes it challenging to use current security solutions. To deal with these issues, we propose an anonymous and reliable ultralightweight RFID-enabled authentication scheme (namely Anon\nR\n2\nAS) for IoT systems in a cloud computing environment. Anon\nR\n2\nAS integrates bitwise exclusive-OR, left–right rotations, and ultralightweight\nh\na\nl\nf\n-\nf\nl\ni\np\noperation, to reduce computational overheads on tag. The Anon\nR\n2\nAS provides stronger security (by preventing several attacks) and improves performance concerning low computational, communication, and storage costs. Also, it preserves information privacy and tags untraceability property by using Vaudenay privacy model. The Scyther simulation tool verification has been performed for its formal security analysis. The performance analysis ensures our Anon\nR\n2\nAS scheme is preferable for low-cost RFID systems.}
}


@article{DBLP:journals/cn/AlbertiPRLBFMS24,
	author = {Ant{\^{o}}nio Marcos Alberti and
                  Diego Gabriel Soares Pivoto and
                  Tib{\'{e}}rio Tavares Rezende and
                  Alexis V. A. Leal and
                  Cristiano Bonato Both and
                  Michelle S. P. Facina and
                  Rodrigo Moreira and
                  Fl{\'{a}}vio de Oliveira Silva},
	title = {Disruptive 6G architecture: Software-centric, AI-driven, and digital
                  market-based mobile networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110682},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110682},
	doi = {10.1016/J.COMNET.2024.110682},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AlbertiPRLBFMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile communications have followed a progression model detailed by the Gartner hype cycle, from a proof-of-concept to widespread productivity. As fifth-generation (5G) mobile networks are being deployed, their potential and constraints are becoming more evident. Although 5G boasts a flexible architecture, enhanced bandwidth, and data throughput, it still grapples with infrastructure challenges, security vulnerabilities, coverage issues, and limitations in fully enabling the Internet of Everything (IoE). As the world experiences exponential growth in Internet users and digitized devices, relying solely on evolutionary technologies seems inadequate. Recognizing this, global entities such as the 3rd Generation Partnership Project (3GPP) are laying the groundwork for 5G Advanced, a precursor to 6G. This article argues against a mere evolutionary leap from 5G to 6G. We propose a radical shift towards a disruptive 6G architecture (D6G) that harnesses the power of smart contracts, decentralized Artificial Intelligence (AI), and digital twins. This novel design offers a software-centric, AI-driven, and digital market-based redefinition of mobile technologies. As a result of an integrated collaboration among researchers from the Brazil 6G Project, this work identifies and synthesizes fifty-one key emerging enablers for 6G, devising a unique and holistic integration framework. Emphasizing flexibility, D6G promotes a digital market environment, allowing seamless resource sharing and solving several of 5G’s current challenges. This article comprehensively explores these enablers, presenting a groundbreaking approach to 6G’s design and implementation and setting the foundation for a more adaptable, autonomous, digitally monitored, and AI-driven mobile communication landscape. Finally, we developed a queuing theory model to evaluate the D6G architecture. Results show that the worst-case delay for deploying a smart contract in a 6G domain was 23 s. Furthermore, under high transaction rates of ten transactions per minute, the delay for contracting a 6G slice was estimated at 53.7 s, demonstrating the architecture’s capability to handle high transaction volumes efficiently.}
}


@article{DBLP:journals/cn/LiFLLFY24,
	author = {Wenxian Li and
                  Yong Feng and
                  Nianbo Liu and
                  Yingna Li and
                  Xiaodong Fu and
                  Yongtao Yu},
	title = {A secure and efficient log storage and query framework based on blockchain},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110683},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110683},
	doi = {10.1016/J.COMNET.2024.110683},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiFLLFY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Log data is crucial for security threat detection and audit analysis. However, traditional log systems are susceptible to tampering, posing a significant security risk to information systems. Although blockchain technology has been introduced to enhance tamper resistance, existing blockchain-based log systems still suffer from storage and query efficiency issues. In this paper, we propose a novel secure and efficient log storage and query framework that combines on-chain and off-chain collaboration. An inverted index table is constructed by extracting keywords from logs, which are stored on the blockchain as on-chain data, while the logs themselves are maintained as off-chain data. This approach facilitates the rapid retrieval of specific keywords and ensures the immutability of the logs. Furthermore, we propose a secure and efficient log query method featuring a smart contract designed to automatically handle requests from legitimate log queriers. We also design a data structure based on merkle adaptive radix tree (MART) and merkle B+ tree (MBT) to store index entries, thereby achieving efficient log retrieval. We provide formal security proofs and comprehensively evaluate the proposed framework’s performance experimentally. Results demonstrate that MBT and MART reduce average query times by 20.09% and 51% respectively, compared to the state-of-the-art schemes.}
}


@article{DBLP:journals/cn/ChenGWWC24,
	author = {Yitong Chen and
                  Chaoqin Gan and
                  Shibao Wu and
                  Xiaoqi Wang and
                  Yixin Chen},
	title = {(User behavior)-based {LS} prediction model and anti-LS reverse-leadership
                  strategy in the {(VLC/RF)-V2X} network},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110684},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110684},
	doi = {10.1016/J.COMNET.2024.110684},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChenGWWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The hybrid vehicle to everything (V2X) network based on visible light communication (VLC) and radio frequency (RF) is an excellent choice for future vehicle communication. In this paper, the (user behavior)-based longitudinal-separation (LS) prediction model and the reverse-leadership strategy in the hybrid V2X network are firstly proposed. By user behavior, the LS is divided into active separation (ASP) and passive separation (PSP). By characteristics of ASP and PSP, the longitudinal-separation prediction model is constructed. By this model, ASP and PSP can be predicted respectively before they occur. By the prediction result of the model, a reverse-leadership strategy for the platoon is presented. By this strategy, communication links can be self-adaptively changed to avoid outages caused by LS. In this scheme (model＋strategy), LS can be predicted timely and communication links can be handed over in advance. Finally, the validity of the model and the effectiveness of the scheme are demonstrated by simulations based on data from NGSIM data set and PTV-Vissim simulation software. Besides, the proposed scheme not only has its comparative advantages of fewer outages and handovers, but also can decrease the path loss, average outage time and handover delay significantly.}
}


@article{DBLP:journals/cn/LiuXW24,
	author = {Danyang Liu and
                  Yuanqing Xia and
                  Yongkang Wang},
	title = {{MSDQ:} Multi-Scheduling Dual-Queues coflow scheduling without prior
                  knowledge},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110685},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110685},
	doi = {10.1016/J.COMNET.2024.110685},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coflow scheduling is crucial for enhancing application-level communication performance in data-parallel clusters. While schemes like Varys can potentially achieve optimal performance, their dependence on a prior information about coflows poses practical challenges. Existing non-clairvoyant solutions, such as Aalo, approximate the classical online Shortest-Job-First (SJF) scheduling but fail to identify bottleneck flows in coflows. Consequently, they often allocate excessive bandwidth to non-bottleneck flows, leading to bandwidth wastage and reduced overall performance. In this paper, we introduce MSDQ, a coflow scheduling mechanism that operates without prior knowledge, utilizing multi-scheduling dual-priority queues, and using width estimates. This method adjusts coflow queue priorities and scheduling sequences based on the coflow’s width and the volume of data transmitted. By reallocating unused network bandwidth at multiple points during the scheduling process, MSDQ maximizes the bandwidth usage and significantly reduces the average coflow completion time. Our evaluation, using a publicly available production cluster trace from Facebook, demonstrates that MSDQ reduces the average coflow completion time by\n1\n.\n42\n×\ncompared to Aalo.}
}


@article{DBLP:journals/cn/DunnettPJDJ24,
	author = {Kealan Dunnett and
                  Shantanu Pal and
                  Zahra Jadidi and
                  Volkan Dedeoglu and
                  Raja Jurdak},
	title = {Priv-Share: {A} privacy-preserving framework for differential and
                  trustless delegation of cyber threat intelligence using blockchain},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110686},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110686},
	doi = {10.1016/J.COMNET.2024.110686},
	timestamp = {Mon, 09 Dec 2024 22:47:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/DunnettPJDJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of the Internet of Things (IoT), Industry 5.0 applications and associated services have caused a powerful transition in the cyber threat landscape. As a result, organisations require new ways to proactively manage the risks associated with their infrastructure. In response, a significant amount of research has focused on developing efficient Cyber Threat Intelligence (CTI) sharing. However, in many cases, CTI contains sensitive information that has the potential to leak valuable information or cause reputational damage to the sharing organisation. While a number of existing CTI sharing approaches have utilised blockchain to facilitate privacy, it can be highlighted that a comprehensive approach that enables dynamic trust-based decision-making, facilitates decentralised trust evaluation and provides CTI producers with highly granular sharing of CTI is lacking. Subsequently, in this paper, we propose a blockchain-based CTI sharing framework, called Priv-Share, as a promising solution towards this challenge. In particular, we highlight that the integration of differential sharing, trustless delegation, democratic group managers and incentives as part of Priv-Share ensures that it can satisfy these criteria. The results of an analytical evaluation of the proposed framework using both queuing and game theory demonstrate its ability to provide scalable CTI sharing in a trustless manner. Moreover, a quantitative evaluation of an Ethereum proof-of-concept prototype demonstrates that applying the proposed framework within real-world contexts is feasible.}
}


@article{DBLP:journals/cn/FengWLLZLZW24,
	author = {Xingbo Feng and
                  Yi Wang and
                  Jiashuo Lin and
                  Weichao Li and
                  Shuangping Zhan and
                  Yan Liu and
                  Jin Zhang and
                  Jianping Wang},
	title = {Advancing {TSN} flow scheduling: An efficient framework without flow
                  isolation constraint},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110688},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110688},
	doi = {10.1016/J.COMNET.2024.110688},
	timestamp = {Sun, 29 Dec 2024 16:29:54 +0100},
	biburl = {https://dblp.org/rec/journals/cn/FengWLLZLZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the domain of Time-Sensitive Networking (TSN), the quest for ultra-reliable low-latency communication is paramount. Current scheduling strategies, which hinge on strict isolation to ensure low latency and jitter, confront the challenges of high overhead in worst-case latency evaluation and consequent limitations in network flow capacity. This paper introduces an innovative framework that transcends traditional isolation constraints, thereby expanding the solution space and augmenting network schedulability. At the heart of this framework lies a novel latency jitter analysis method that assesses the viability of non-isolation scenarios with constant time complexity. This method underpins a heuristic scheduling algorithm that not only boasts the smallest time complexity among existing heuristics but also significantly increases the number of scheduled flows. Complementing this, we integrate a discrete time reference approach to hasten time-intensive scheduling operations, achieving an optimal balance between schedulability and runtime efficiency. The framework further incorporates a workload-shifting technique to enhance online scheduling responsiveness. It adeptly manages the variability in scheduling times caused by disharmonious flow periods, further bolstering the framework’s robustness. Experimental validations demonstrate that our framework can increase the scheduled flows up to 269%. It reduces scheduling runtime by up to 98.44% for medium-scale networks while maintaining a flat runtime growth curve, ensuring predictable performance in online scheduling scenarios.}
}


@article{DBLP:journals/cn/ShenLHYXFH24,
	author = {Fanfan Shen and
                  Qiwei Liang and
                  Lijie Hui and
                  Bofan Yang and
                  Chao Xu and
                  Jun Feng and
                  Yanxiang He},
	title = {{TPE-BFL:} Training Parameter Encryption scheme for Blockchain based
                  Federated Learning system},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110691},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110691},
	doi = {10.1016/J.COMNET.2024.110691},
	timestamp = {Mon, 26 Aug 2024 22:03:25 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ShenLHYXFH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Blockchain technology plays a pivotal role in addressing the single point of failure issues in federated learning systems, due to the immutable nature and decentralized architecture. However, traditional blockchain-based federated learning systems still face privacy and security challenges when transmitting training model parameters to individual nodes. Malicious nodes within the system can exploit this process to steal parameters and extract sensitive information, leading to data leakage. To address this problem, we propose a Training Parameter Encryption scheme for Blockchain based Federated Learning system (TPE-BFL). In TPE-BFL, the training parameters of the system model are encrypted using the paillier algorithm with the property of addition homomorphism. This encryption mechanism is integrated into the workflows of three distinct roles within the system: workers, validators, and miners. (1) Workers utilize the paillier encryption algorithm to encrypt training parameters for local training models. (2) Validators decrypt received encrypted training parameters using private keys to verify their validity. (3) Miners receive cryptographic training parameters from validators, validate them, and generate blocks for subsequent global model updates. By implementing the TPE-BFL mechanism, we not only preserve the immutability and decentralization advantages of blockchain technology but also significantly enhance the privacy protection capabilities during data transmission in federated learning systems. In order to verify the security of TPE-BFL, we leverage the semantic security inherent in the Paillier encryption algorithm to theoretically substantiate the security of our system. In addition, we conducted a large number of experiments on real-world data to prove the validity of our proposed TPE-BFL, and when 15% of malicious devices are present, TPE-BFL achieve 92% model accuracy, a 5% improvement over the blockchain-based decentralized FL framework (VBFL).}
}


@article{DBLP:journals/cn/ChaudharyAMS24,
	author = {Sameeksha Chaudhary and
                  Anirudh Agarwal and
                  Deepak Mishra and
                  Santosh Shah},
	title = {A review on green communication for wearable and implantable wireless
                  body area networks},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110693},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110693},
	doi = {10.1016/J.COMNET.2024.110693},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ChaudharyAMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the interconnected world, where wearable devices and implantable sensors have become integral to healthcare, Wireless Body Area Networks (WBANs) play a very pivotal role. These networks enable continuous monitoring of vital signs, early detection of health issues, and personalized patient care. However, ensuring the longevity and reliability of WBANs remains a multifaceted challenge. This comprehensive survey addresses this challenge by providing key insights and practical solutions. We investigate the intricacies of indoor communication links within WBANs, characterizing wireless channels thereby revealing factors that influence the quality of the received signal. By understanding multipath propagation, small-scale fading, and interference, we empower engineers to optimize communication protocols for reliable data transmission. In particular, we describe practical path loss models that have been successfully implemented in real-world WBANs. These models account for indoor scenarios, considering floor attenuation, wall partitions, and other environmental factors.}
}


@article{DBLP:journals/cn/KumarC24,
	author = {Naveen Kumar and
                  Ankit Chaudhary},
	title = {Surveying cybersecurity vulnerabilities and countermeasures for enhancing
                  {UAV} security},
	journal = {Comput. Networks},
	volume = {252},
	pages = {110695},
	year = {2024},
	url = {https://doi.org/10.1016/j.comnet.2024.110695},
	doi = {10.1016/J.COMNET.2024.110695},
	timestamp = {Sun, 08 Sep 2024 16:07:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KumarC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Drones and other forms of Unmanned Aerial Vehicles (UAVs) usages are increasing with time from military operations like surveillance, reconnaissance to commercial operations such as transportation, agriculture. The Drone market will be grown to around 43 billion USD by the 2025. With the increase in the usages, there is increase in cyber-attacks. So, Drone security and privacy are of major concern as they are used to perform the critical operations. The rapid adoption of UAVs in various sectors has prompted the need for robust and comprehensive cyber security measures. By implementing robust countermeasures, such as authentication, encryption etc. the risks posed by cyber threats can be significantly mitigated. The study investigates cyber security vulnerabilities and countermeasures in UAV systems within the scope of Authentication techniques, Physical Layer Security, Covert Communication. Relaying and Trajectory Optimization techniques are also discussed so that the flight trajectory can be optimized. It also discusses the communication modes used in UAV communication. An analysis of communication protocols is also carried out in this survey. The goal of the survey is to get the idea of cyber security threats involved in UAV communication. Our analysis emphasizes the importance of a holistic approach to UAV cyber security, leveraging the synergies among these domains to enhance overall resilience.}
}
