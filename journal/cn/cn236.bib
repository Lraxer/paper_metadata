@article{DBLP:journals/cn/XuTZWLH23,
	author = {Jing Xu and
                  Jiarun Tang and
                  Yuze Zou and
                  Ruikai Wen and
                  Wei Liu and
                  Jianhua He},
	title = {Sum throughput optimization of wireless powered IRS-assisted multi-user
                  {MISO} system},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109984},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109984},
	doi = {10.1016/J.COMNET.2023.109984},
	timestamp = {Wed, 14 Aug 2024 08:20:45 +0200},
	biburl = {https://dblp.org/rec/journals/cn/XuTZWLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS) is a promising technology for beyond-5G wireless communication systems. However, the energy demand of IRS is often overlooked in existing works, leading to performance issues in practical scenarios. To address this issue, this paper proposes an operating model based on time switching (TS) protocol for an IRS-assisted multi-user multiple-input single-output (MISO) system, which can provide energy for IRS through wireless power transfer (WPT) technology. The system throughput maximization problem is addressed to improve performance. Specifically, a two-stage algorithm combined with alternating optimization, denoted as TAO, is proposed. To further improve the optimization process in large-size IRS scenarios, an improved deep deterministic policy gradient (DDPG) method combined with TAO, denoted as TAO-DDPG, is also proposed. Numerical results demonstrate that the proposed TAO-DDPG algorithm achieves similar performance to TAO while greatly reducing the optimization time.}
}


@article{DBLP:journals/cn/LvWWWWZ23,
	author = {Sicai Lv and
                  Chao Wang and
                  Zibo Wang and
                  Shuo Wang and
                  Bailing Wang and
                  Yongzheng Zhang},
	title = {{AAE-DSVDD:} {A} one-class classification model for {VPN} traffic
                  identification},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109990},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109990},
	doi = {10.1016/J.COMNET.2023.109990},
	timestamp = {Wed, 08 Nov 2023 17:21:50 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LvWWWWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual Private Network(VPN) can provide a concealed transmission channel for communication and protect the privacy of users. However, it also brings hidden dangers to cybersecurity with its wide application. Malicious behavior or harmful information can be transmitted secretly through VPN tunnels to avoid firewall censorship. Therefore, VPN traffic identification is an important part of ensure cybersecurity. Although many efforts have been made for VPN traffic identification, existing methods mainly focus on supervised learning models. In this paper, we propose an one-class classification model called AAE-DSVDD for VPN traffic identification. First, we introduce Adversarial AutoEncoder(AAE) for preliminary modeling of VPN traffic. AAE can match the aggregated posterior distribution of the hidden layer to an arbitrary prior distribution. It associates the samples with a normal distribution in the hidden space. Secondly, We implement representation learning for VPN traffic via Deep Support Vector Data Description(DSVDD). A standardized method is designed to match the output distribution of DSVDD with the aggregated posterior distribution of AAE. It alleviates the hypersphere collapse problem of DSVDD and improves identification performance. Finally, we verify the abilities of the AAE-DSVDD model on the public dataset ISCXVPN. Compared with other one-class models, AAE-DSVDD achieved the best identification ability for VPN traffic identification. It also improves the recognition ability when identifying strange classes that are not included in the training data.}
}


@article{DBLP:journals/cn/DahanayakaGHJS23,
	author = {Thilini Dahanayaka and
                  Yasod Ginige and
                  Yi Huang and
                  Guillaume Jourjon and
                  Suranga Seneviratne},
	title = {Robust open-set classification for encrypted traffic fingerprinting},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109991},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109991},
	doi = {10.1016/J.COMNET.2023.109991},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/DahanayakaGHJS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Encrypted network traffic has been known to leak information about their underlying content through side-channel information leaks. Traffic fingerprinting attacks exploit this by using machine learning techniques to threaten user privacy by identifying user activities such as website visits, videos streamed, and messenger app activities. Although state-of-the-art traffic fingerprinting attacks have high performances, even undermining the latest defenses, most of them are developed under the closed-set assumption. To deploy them in practical situations, it is important to adapt them to the open-set scenario, which allows the attacker to identify its target content while rejecting other background traffic. At the same time, in practice, these models need to be deployed on in-networking devices such as programmable switches, which have limited memory and computation power. Model weight quantization can reduce the memory footprint of deep learning models while at the same time, allowing inference to be done as integer operations as opposed to floating point operations. Open-set classification in the domain of traffic fingerprinting has not been explored well in prior work and none of them explored the effect of quantization on the open-set performance of such models. In this work, we propose a framework for robust open-set classification of encrypted traffic based on three key ideas. First, we show that a well-regularized deep learning model improves the open-set classification and then we propose a novel open-set classification method with three variants that perform consistently over multiple datasets. Next, we show that traffic fingerprinting models can be quantized without a significant drop in both closed-set and open-set accuracy and therefore, they can be readily deployed on in-network computing devices. Finally, we show that when the above three components are combined, the resulting open-set classifier outperforms all other open-set classification methods evaluated across five datasets with a minimum and maximum increase in F1_Score of 8.9% and 77.3% respectively.}
}


@article{DBLP:journals/cn/ZafarRJUAK23,
	author = {Waseeq Ul Islam Zafar and
                  Muhammad Atif Ur Rehman and
                  Farhana Jabeen and
                  Rehmat Ullah and
                  Ghulam Abbas and
                  Abid Khan},
	title = {Decentralized Receiver-based Link Stability-aware Forwarding Scheme
                  for NDN-based VANETs},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109996},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109996},
	doi = {10.1016/J.COMNET.2023.109996},
	timestamp = {Sun, 04 Aug 2024 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZafarRJUAK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The inter-vehicle communication in the vehicular ad-hoc network (VANET) providing ubiquitous mobility to mobile users is considered an essential component of intelligent transportation systems (ITS), enabling a wide range of safety and non-safety applications in which the vehicles seamlessly interact and co-operate with each other. The recent paradigm shift in VANETs protocols has led to the adoption of name data networking (NDN) as an underlying communication protocol in VANETs that replaces traditional address-based communication philosophy, treats content as a first-class citizen, and assigns unique names to contents consequently enabling name-based routing and forwarding, caching, and security. Although NDN has significantly improved the underlying communication issues in VANETs, a plethora of challenges such as message broadcast storms, in-efficient packet suppression mechanisms, intermittent connectivity owing to frequent topological changes, and reverse path partioning (RPP) require further attention from the research community. To cope with these issues, this paper proposes decentralized receiver-based link stability-aware forwarding (DRLSF) protocol. The DRLSF is a beacon-less receiver-based multi-hop protocol, well-suited for pull-based applications where end users request desired data by sending packets. The comparative performance analysis validates the effectiveness of the DRLSF protocol in different VANET environments and application scenarios.}
}


@article{DBLP:journals/cn/KhahBB23,
	author = {Sahar Ahmadi Khah and
                  Ali Barati and
                  Hamid Barati},
	title = {A dynamic and multi-level key management method in wireless sensor
                  networks (WSNs)},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109997},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109997},
	doi = {10.1016/J.COMNET.2023.109997},
	timestamp = {Tue, 28 Nov 2023 20:05:04 +0100},
	biburl = {https://dblp.org/rec/journals/cn/KhahBB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless sensor networks include a set of ultralight sensor nodes with limited energy, low storage capacity, and constrained processing power. Security is a challenging issue in these networks. One approach to maintain security in these networks is key management. When designing key management schemes, the main challenge is to achieve the acceptable security level and manage resources, especially energy. Today, many key management methods use authentication mechanisms. However, these methods are faced with different challenges due to the limited computational capability of nodes and high energy consumption in the network. In this paper, we present a dynamic and multi-level key management method for homogeneous wireless sensor networks. The proposed key management approach uses an authentication mechanism based on message authentication code (MAC), which is calculated by the shared symmetric keys between the transmitter and receiver nodes. In our scheme, the network is divided into five levels. Then, the clustering process is performed at each level, so that the size of the clusters varies in different levels. The proposed approach includes four phases: (1) Network leveling phase (2) Clustering phase (3) Key establishment phase (cluster key, pairwise key, and gateway key) (4) Rekeying phase. In the proposed method, a symmetric encryption algorithm called RC5 is used for producing keys. Our method contributes to the state of the art in wireless sensor networks by proposing a dynamic and multi-level key management approach. The use of a MAC-based authentication mechanism and the division of the network into different levels and clusters allows for efficient key establishment and rekeying, while minimizing energy consumption and communication overhead. Our proposed method is implemented using the NS2 simulator. Then, the simulation results are compared with SKWN and KMP methods. The results show that our proposed method outperforms SKWN and KMP in terms of average energy consumption, required memory, and communication overhead. However, its computational overhead is slightly more than SKWN and KMP.}
}


@article{DBLP:journals/cn/CotterCOC23,
	author = {Jamie Cotter and
                  Ignacio Casti{\~{n}}eiras and
                  Donna O'Shea and
                  Victor Cionca},
	title = {A comparative analysis of proactive and reactive methods for privacy-aware
                  interleaved {DNN} offloading},
	journal = {Comput. Networks},
	volume = {236},
	pages = {109999},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.109999},
	doi = {10.1016/J.COMNET.2023.109999},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/CotterCOC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep Neural Networks (DNNs) are increasingly used on the edge in mobile and other resource-constrained devices for inference tasks ranging from object detection and image recognition to video processing. Many of these tasks have low latency requirements that cannot be satisfied if they are processed locally due to their high computational complexity. Offloading computation to the edge and cloud offers a way to alleviate this computational latency. Doing so introduces communication delays, which makes offloading a balancing act between the benefits of reduced processing time and the communication delays incurred. Existing algorithms for DNN offloading based on DNN partitioning are optimised for handling successive tasks on a single remote server, and perform sub-optimally when tasks are interleaved, or with multiple servers, or when privacy concerns require local processing of certain DNN layers. A viable alternative are generic computational offloading algorithms (GOAs), which can break down DNN tasks into their components and perform fine-grained offloading. We perform a simulation-based comparison of traditional GOAs with various levels of proactivity and offloading constraints and a naive DNN partitioning approach. We identify key requirements for offloading algorithms in the ability to create processing chains between several remote servers in order to reduce communication overheads, and the ability to prioritise already running tasks. The results confirm the expectations about the shortcomings of DNN partitioning, and show that GOA algorithms can provide a significant performance improvement.}
}


@article{DBLP:journals/cn/McManusCZHMMBMG23,
	author = {Maxwell McManus and
                  Yuqing Cui and
                  Josh Zhaoxi Zhang and
                  Jiangqi Hu and
                  Sabarish Krishna Moorthy and
                  Nicholas Mastronarde and
                  Elizabeth Serena Bentley and
                  Michael J. Medley and
                  Zhangyu Guan},
	title = {Digital twin-enabled domain adaptation for zero-touch {UAV} networks:
                  Survey and challenges},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110000},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110000},
	doi = {10.1016/J.COMNET.2023.110000},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/McManusCZHMMBMG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In existing wireless networks, the control programs have been designed manually and for certain predefined scenarios. This process is complicated and error-prone, and the resulting control programs are not resilient to disruptive changes. Data-driven control based on Artificial Intelligence and Machine Learning (AI/ML) has been envisioned as a key technique to automate the modeling, optimization and control of complex wireless systems. However, existing AI/ML techniques rely on sufficient well-labeled data and may suffer from slow convergence and poor generalizability. In this article, focusing on digital twin-assisted wireless unmanned aerial vehicle (UAV) systems, we provide a survey of emerging techniques that can enable fast-converging data-driven control of wireless systems with enhanced generalization capability to new environments. These include simultaneous localization and sensing (SLAM)-based sensing and network softwarization for digital twin construction, robust reinforcement learning and system identification for domain adaptation, and testing facility sharing and federation. The corresponding research opportunities are also discussed.}
}


@article{DBLP:journals/cn/CamargoCRCDPATGOR23,
	author = {Juan Sebastian Camargo and
                  Estefan{\'{\i}}a Coronado and
                  Wilson Ram{\'{\i}}rez and
                  Daniel Camps{-}Mur and
                  Sergi S{\'{a}}nchez Deutsch and
                  Jordi P{\'{e}}rez{-}Romero and
                  Angelos Antonopoulos and
                  {\'{O}}scar Trullols{-}Cruces and
                  Sergio Gonzalez{-}Diaz and
                  Borja Otura and
                  Giovanni Rigazzi},
	title = {Dynamic slicing reconfiguration for virtualized 5G networks using
                  {ML} forecasting of computing capacity},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110001},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110001},
	doi = {10.1016/J.COMNET.2023.110001},
	timestamp = {Tue, 28 Nov 2023 20:05:04 +0100},
	biburl = {https://dblp.org/rec/journals/cn/CamargoCRCDPATGOR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As 5G deployments continue to increase worldwide, new applications can fully leverage the exceptional features of the emerging mobile networks. Ultra-Reliable Low Latency Communications (URLLC) serve as an excellent example of applications highly sensitive to jitter and packet loss. To meet these demanding requirements, 5G relies on network slicing, network virtualization, and software-defined networks. This ecosystem enables the precise allocation of resources for each network slice. However, the applications’ resource demands may vary over time. In this challenging and overwhelming environment, traditional human decision-making for slice reconfiguration is not suitable anymore, due to the multitude of parameters and the need for extremely fast response times. Machine Learning (ML) comes as a tool that can enable better use of the available resources with faster and more intelligent management. This paper introduces an ML model that can predict slices’ traffic and dynamically reconfigure computational capacity. With these forecasting capabilities, the virtualized resources can be fine-tuned to suit the slices’ requirements, guaranteeing their Quality of Service (QoS). By doing so, Mobile Network Operators can make optimized use of the equipment, tailoring their needs to each service while complying with the QoS level. The results obtained demonstrate that the proposed ML model, in combination with a specific set of hysteresis rules, can accurately predict the saturation of virtualized capacity with up to 91% accuracy and proactively adapt it to the network slice requirements.}
}


@article{DBLP:journals/cn/HirayamaJK23,
	author = {Takahiro Hirayama and
                  Masahiro Jibiki and
                  Ved P. Kafle},
	title = {Salience-based {VNF} placement method for evolving multiple network
                  slices},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110002},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110002},
	doi = {10.1016/J.COMNET.2023.110002},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/HirayamaJK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Function Virtualization (NFV) technology enables the deployment of a distinct network slice composed of various virtualized network functions (VNFs) required for offering a communication service. As the user population or network traffic volume fluctuates, the capacity of the network slices must be dynamically adjusted. It is necessary to decide whether to place VNFs in appropriate locations such that the quality of services (QoS) is guaranteed while optimally utilizing the allocated resources. The above challenge can be addressed by solving an optimization problem, such as integer linear programming. However, as solving the optimization problem requires a considerable amount of time, it is not applicable in agile network control mechanisms. In this paper, we propose a network function placement algorithm based on salience, which is one of the characteristic metrics of links in a graph. Our proposal is aimed to provide the adequate placement in short time to achieve the following objectives of keeping the slice reconstruction cost low and avoiding the cases of QoS degradation. The simulation results show that our scheme performs equally well for different sized slices (i.e., the user population). We found that the network reconfiguration costs are low and almost the same for various sized slices when they are reconstructed by the proposed method in a substrate network of 200 nodes. Furthermore, we confirmed that the proposed method keeps the slice reconstruction cost low and avoids blocking of slice reconstruction even in the situations when a resource competition occurs among multiple slices.}
}


@article{DBLP:journals/cn/WangLCWH23,
	author = {Ying Wang and
                  Jiang Liu and
                  Mingwei Cui and
                  Weihong Wu and
                  Tao Huang},
	title = {{ED-VNE:} {A} profit-oriented {VNE} optimization scheme of energy
                  and delay in 5G SlaaS},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110003},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110003},
	doi = {10.1016/J.COMNET.2023.110003},
	timestamp = {Wed, 06 Dec 2023 13:50:56 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangLCWH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 5G Slice-as-a-Service (SlaaS), supporting ultra-low delay is an essential network capability for 5G infrastructure providers (InPs). However, network slicing in 5G SlaaS has a more critical conflict in delay and cost than previous generations, which brings profit-risk to InPs. An effective way to fix the issue is performing joint optimization of delay and energy in the virtual network embedding (VNE) of SlaaS. Previous studies in VNE are unsuitable for the optimization in 5G SlaaS because (i) the mutual feedback on VNE policy and substrate nodes’ status complicates the energy-aware VNE, and (ii) the tradeoff between delay and energy consumption in 5G InPs. Therefore, we propose a novel VNE scheme called ED-VNE, which jointly optimizes the end-to-end (E2E) delay and energy to improve the profit of InPs. ED-VNE first introduces the substrate nodes’ energy state into the VNE process. And then, ED-VNE performs optimization to synchronously reduce the E2E delay of network slicing and energy consumption, to finally improve the profit. Compared to existing solutions, simulation results indicate that ED-VNE can decrease the delay and energy consumption by up to 33.74% and 32.56% while increasing the profit of InPs by up to 50.44%.}
}


@article{DBLP:journals/cn/MohammadzadKM23,
	author = {Maryam Mohammadzad and
                  Jaber Karimpour and
                  Farnaz Mahan},
	title = {{MAGD:} Minimal Attack Graph Generation Dynamically in Cyber Security},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110004},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110004},
	doi = {10.1016/J.COMNET.2023.110004},
	timestamp = {Mon, 05 Feb 2024 20:24:11 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MohammadzadKM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber security faces challenges in detecting and mitigating complex attacks. Security solutions have employed Attack Graphs (AGs) for modeling multi-stage attacks, but traditional AGs suffer from scalability issues and may miss new vulnerabilities and attack paths. Also, traditional AGs construct the graph using information about previously known attacks. In this paper, we propose Minimal Attack Graph Generation Dynamically (MAGD), which leverages data from a deception system based on Honeypots to generate a minimal AG dynamically. In this paper, the AG has been constructed from real-time attacker’s behavior data directly. In addition, MAGD specifically focuses on modeling the attacker’s behavior at the host level, in contrast to traditional network-based AGs that encompass all possible attack paths at the network level. MAGD contains three custom algorithms to construct attacker behavior, generate a minimal AG, and continuously update the graph with new attack information. Complexity analyses demonstrate that MAGD’s generation process can accomplish within polynomial time. Our approach offers several advantages over traditional AGs, including the ability to model attackers’ real-time behavior, construct attackers’ action paths in the target host, and detect new vulnerabilities and attack paths in the victim host. Also, MAGD includes information about the effects of the actions in the target system. This information can be used for other security purposes. We demonstrate MAGD’s efficacy through a case study. MAGD provides a more effective way to detect and mitigate cyber threats by utilizing Honeypot data and proposed algorithms.}
}


@article{DBLP:journals/cn/AmiriZarandiDL23,
	author = {Mohammad Amiri{-}Zarandi and
                  Rozita A. Dara and
                  Xiaodong Lin},
	title = {{SIDS:} {A} federated learning approach for intrusion detection in
                  IoT using Social Internet of Things},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110005},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110005},
	doi = {10.1016/J.COMNET.2023.110005},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AmiriZarandiDL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) ecosystem needs Intrusion Detection Systems (IDS) to mitigate cyberattacks and exploit security vulnerabilities. Over the past years, utilizing machine learning in IDSs has gained a lot of attention. However, in many current works, the training data from different locations should be collected in a central server to be used in the learning process. This data-sharing procedure increases concerns regarding data privacy and decreases the data holders’ motivation to participate in the learning process. The use of distributed learning models has been considered a solution to overcome concerns related to privacy. However, these distributed learning models are vulnerable in the presence of untrusted nodes that can participate in the learning process and deteriorate performance. In this paper, we propose SIDS (Social Intrusion Detection System), a trust-oriented federated learning approach for intrusion detection in IoT that utilizes the Social Internet of Things (SIoT). The proposed approach leverages the social relationships among the objects in a system to provide a privacy-preserving collaborative mechanism for detecting intrusions in IoT environments. The experimental results show the proposed solution outperforms the learning models on individual servers while providing a privacy-preserving and trustable environment for collaboration.}
}


@article{DBLP:journals/cn/MattosDSRL23,
	author = {Ekler Paulino de Mattos and
                  Augusto C. S. A. Domingues and
                  Fabr{\'{\i}}cio A. Silva and
                  Heitor S. Ramos and
                  Antonio A. F. Loureiro},
	title = {Slicing who slices: Anonymization quality evaluation on deployment,
                  privacy, and utility in mix-zones},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110007},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110007},
	doi = {10.1016/J.COMNET.2023.110007},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MattosDSRL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the flowering of ubiquitous computing, technologies like the Internet of Things and the Internet of Vehicles have contributed to connecting objects and sharing location services in broad environments like smart cities bringing many benefits to citizens. However, these services yield massive and unrestricted mobility data of citizens that pose privacy concerns, among them recovering the identity of people with linking attacks. Although several privacy mechanisms have been proposed to solve anonymization problems, there are few studies about their behavior and analysis of the data quality anonymized by these techniques. This paper presents an anonymization quality framework for mix-zones enabling characterizing and evaluating the impacts of anonymization over time and space in mobility data. We conducted experiments with a cab mobility dataset and two positioning algorithms to explore one of the potentialities of the anonymization quality: elect mix-zones that do not consider the traffic but its operating requirements too. The results showed that the anonymization quality enabled the selection of mix-zones that yield data anonymization considering the quality, privacy, and utility analysis. This study is unique because it analyzes mix-zone coverage and quality metrics to observe the anonymization quality not found in the literature.}
}


@article{DBLP:journals/cn/KhanHDFMJA23,
	author = {F{\'{a}}tima Khan and
                  Cristina Hervella and
                  Luis D{\'{\i}}ez and
                  F{\'{a}}tima Fern{\'{a}}ndez and
                  N{\'{e}}stor J. Hern{\'{a}}ndez Marcano and
                  Rune Hylsberg Jacobsen and
                  Ram{\'{o}}n Ag{\"{u}}ero},
	title = {Realistic assessment of transport protocols performance over LEO-based
                  communications},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110008},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110008},
	doi = {10.1016/J.COMNET.2023.110008},
	timestamp = {Sun, 06 Oct 2024 21:22:03 +0200},
	biburl = {https://dblp.org/rec/journals/cn/KhanHDFMJA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the performance exhibited by the transport protocols, Transport Control Protocol (TCP) and QUIC, over realistic satellite networks. We propose a novel methodology, which combines real implementation (exploiting virtualization techniques) and simulation, to carry out systematic and repetitive experiments. We modify the default operation of the ns-3 framework and we integrate the dynamism that characterizes satellite communication links, particularly Low Earth Orbit (LEO). We carry out a thorough assessment over different setups, changing the operating frequency band and packet buffer lengths. In addition, we ascertain the impact of using the multi-streaming feature that QUIC integrates. The results show that QUIC yields lower delays than TCP, although it might suffer from higher jitter in particular setups. In addition, the results evince that using multiple streams in QUIC does not yield a relevant gain for the default Round-Robin (RR) scheduler. We propose more appropriate scheduling strategies, which are able to yield better performances with unbalanced traffic. Even if the behavior of transport protocols over non-terrestrial-networks might not be always appropriate, the obtained results evince that QUIC can definitively bring benefits when compared to TCP. Furthermore, we have shown that optimal scheduling policies yields a fairer performance when using multiple flows, having unbalanced traffic loads.}
}


@article{DBLP:journals/cn/LuYBDZ23,
	author = {Yuan Lu and
                  Guoyuan Yuan and
                  Yang Bai and
                  Dezun Dong and
                  Renjie Zhou},
	title = {EagerCC: An ultra-low latency congestion control mechanism in datacenter
                  networks},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110009},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110009},
	doi = {10.1016/J.COMNET.2023.110009},
	timestamp = {Wed, 08 Nov 2023 17:21:50 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LuYBDZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of cloud applications, the workload pattern of datacenters presents the characteristics of mixed long and short flows and frequent micro-burst traffic, which puts forward new requirements for network transmission performance, including ultra-low latency, high throughput, and strong stability. At the same time, datacenters begin to deploy the low-diameter topology to accommodate these new requirements, and the high-performance datacenter (HPDC) comes into being. However, due to the complexity of the load, existing congestion control mechanisms cannot control dynamic delay in the network well, which significantly restricts the development of the HPDC. Therefore, it is necessary to deploy the congestion control mechanism for the HPDC. So we propose EagerCC, an ultra-low latency, low-overhead, and accurate congestion control mechanism based on In-Network-Telemetry (INT) information for various datacenter scenarios, especially for HPDCs. EagerCC uses switch-feedback, ACK-padding, ACK-first to reduce feedback delay and uses switch-calculation, probabilistic ACK-padding to reduce the overhead of congestion signals. We conduct a lot of experiments and the result shows that EagerCC performs well in various datacenter scenarios, especially for HPDCs. Specifically, EagerCC reduces the 99th-FCT and avg-FCT by 52.3% and 13.3% for the HPC workload NPB-CG compared to HPCC in Dragonfly. In addition, EagerCC significantly reduces the network’s feedback delay and queue occupancy.}
}


@article{DBLP:journals/cn/OliveiraGMJGOF23,
	author = {Jonathas A. de Oliveira and
                  Vin{\'{\i}}cius P. Gon{\c{c}}alves and
                  Rodolfo I. Meneguette and
                  Rafael Tim{\'{o}}teo de Sousa J{\'{u}}nior and
                  Daniel L. Guidoni and
                  Jos{\'{e}} C. M. Oliveira and
                  Geraldo P. Rocha Filho},
	title = {{F-NIDS} - {A} Network Intrusion Detection System based on federated
                  learning},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110010},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110010},
	doi = {10.1016/J.COMNET.2023.110010},
	timestamp = {Tue, 25 Feb 2025 13:52:17 +0100},
	biburl = {https://dblp.org/rec/journals/cn/OliveiraGMJGOF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of IoT networks has presented fresh challenges in terms of scalability and security for distributed Network Intrusion Detection Systems (NIDS) due to privacy concerns. While some progress has been made in addressing these challenges, there are still unanswered questions regarding how to achieve a balance between performance and robustness to ensure privacy in a distributed manner. Additionally, there is a need to develop a reliable and scalable architecture for distributed NIDS that can be effectively deployed in various IoT scenarios. These questions about robustness relied mainly on choosing privacy-secured and distributed Machine Learning techniques. In this work, we propose the F-NIDS, an intrusion detector that utilizes federated artificial intelligence and asynchronous communication techniques between system entities to provide horizontal scalability, along with differential privacy techniques to address data confidentiality concerns. The architecture of F-NIDS is designed to be adaptable for usage in IoT networks, suited to be used in cloud or fog-based environments. Results from our experiments have shown that the confidential detection model employed in F-NIDS – considering multi-class accuracy, binary accuracy, precision, and recall metrics – was capable of predicting and determining the nature of attacks when they occur. In order to determine optimal parameters that strike a balance between data privacy and classification performance, three strategies were employed, each evaluated for its corresponding robustness performance. Firstly, models were trained with varying Gaussian noise values, and subjected to membership inference black box rule-based attacks. Secondly, regular membership inference black box attacks were performed, utilizing different stolen samples with varying sizes to determine the maximum amount of data that could be securely stored on the detection agents for training tasks. Lastly, the robustness of the trained models was evaluated against a model inversion attack, and the results were compared through graphical comparisons. Based on these evaluations, Gaussian noise level and sample size values of 21 were obtained for each detection agent in the system, with sample sizes ranging from 10K to 25K.}
}


@article{DBLP:journals/cn/BanotraGMM23,
	author = {Atul Banotra and
                  Sarbani Ghose and
                  Deepak Mishra and
                  Sudhakar Modem},
	title = {Energy harvesting in self-sustainable IoT devices and applications
                  based on cross-layer architecture design: {A} survey},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110011},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110011},
	doi = {10.1016/J.COMNET.2023.110011},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BanotraGMM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) is envisioned to become a driving force in the evolution of fifth-generation (5G) mobile networks, autonomous continuous monitoring and control platforms, and low-power consumption devices. Conventionally, these devices are battery-operated. The battery lifespan degrades faster in the case of continuous monitoring devices. Limited battery lifetime motivates us to investigate an eco-friendly solution to solve the issue of limited battery life and frequent replacement. Energy harvesting (EH) is a key-enabling technique that provides a viable solution to the challenge at hand. EH minimizes battery dependence by collecting energy from ambient sources. Although several studies have been conducted on EH-IoT networks, a tutorial on a possible mapping between the use cases/application to the IoT devices and the IoT devices to EH power source still needs to be included. To address this gap, we classify IoT devices based on their applications. Based on the different interfacing sections of layer architecture design of IoT device, this paper tries to fill the gap between IoT applications and the self-sustainable IoT system design. Next, each interfacing section is categorized into layer architecture of IoT device to study layer-specific power requirements (or demand) versus EH sources (or supply). We have considered various applications like agriculture, healthcare, industry, city, security, transportation, and water management. Further, we discuss different energy sources to harness ambient energy and the method to store the harvested energy. Then, the widely used standards and the various steps taken for the interoperability and compatibility of EH-IoT are outlined to understand the elements of the EH interface. Finally, we point out future research issues requiring specific attention for deploying self-sustainable IoT-based delay-limited applications.}
}


@article{DBLP:journals/cn/LuLYLNL23,
	author = {Qiuyu Lu and
                  June Li and
                  Kai Yuan and
                  Kaipei Liu and
                  Ming Ni and
                  Jianbo Luo},
	title = {{UDP-RT:} {A} UDP-based reliable transmission scheme for power {WAPS}},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110012},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110012},
	doi = {10.1016/J.COMNET.2023.110012},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LuLYLNL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is expected that TCP/IP networks take the place of point-to-point fiber channels for the communications of WAPS. In TCP/IP networks, TCP does not guarantee real-time while UDP does not guarantee reliability. An efficient method is demanded for WAPS to guarantee the real-time and reliability of messages transmitted in TCP/IP networks under congestion states. To address the challenge, we propose a UDP-based reliable transmission (UDP-RT) scheme, which achieves low latency by adopting UDP at the transport layer and high reliability by adding the mechanisms of error correction, error detection, resending and timeout retransmission at the application layer. The error correction mechanism employs TPCs, which has low complexity and good performance at high code rate, to correct errors in a message. A blocking rule for TPCs considering the features of communication channels and messages of WAPS is presented. The error detection mechanism is for detecting whether all errors in the message are corrected by the error correction mechanism. The resending mechanism and the timeout retransmission mechanism (optional) is for ensuring the reliability in the two cases of message loss and not all errors corrected. Additionally, algorithms for UDP-RT are presented, and their correctness are validated through experiment. Our analyses demonstrate that the proposed scheme can meet the real-time requirements of WAPS businesses when network congesting and has higher reliability than the TCP transmission scheme and other UDP transmission schemes.}
}


@article{DBLP:journals/cn/LiuQSZ23,
	author = {Pei Liu and
                  Bing Qian and
                  Qi Sun and
                  Longgang Zhao},
	title = {Prompt-WNQA: {A} prompt-based complex question answering for wireless
                  network over knowledge graph},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110014},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110014},
	doi = {10.1016/J.COMNET.2023.110014},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiuQSZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless networks, the scale of network devices is constantly expanding. Experts spend vast amounts of time consulting and dealing with various network problems of the same nature during the course of daily network operation and maintenance service, and there is no time to collect statistics and sort their information, which affects the overall operation and maintenance efficiency. This paper investigates complex Knowledge Graph Question Answering (KGQA) in the wireless network domain in order to improve operation and maintenance efficiency. Accordingly, we propose Wireless Network QA over the KG using Prompt learning (Prompt-WNQA), a novel method that deals with both constraint-based and multi-hop complex questions making use of prompt learning. Our method is also helpful in performing complex KGQA over incomplete KGs, which can complete missing relations between unconnected entities. Compared to state-of-the-art approaches, our Prompt-WNQA achieves significant improvement over extensive experiments on both the wireless network QA dataset and two public complex QA datasets.}
}


@article{DBLP:journals/cn/ChaudharyM23,
	author = {Shubhankar Chaudhary and
                  Pramod Kumar Mishra},
	title = {DDoS attacks in Industrial IoT: {A} survey},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110015},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110015},
	doi = {10.1016/J.COMNET.2023.110015},
	timestamp = {Wed, 08 Nov 2023 17:21:50 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChaudharyM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the IoT expands its influence, its effect is becoming macroscopic and pervasive. One of the most discernible effects is in the industries where it is known as Industrial IoT (IIoT). IIoT provides automated, comprehensive, regressive and easy-to-use methods to look over its components. Along with the benefits, it also brings concerns that spawn from the IoT itself. Moreover, the challenges in the industries also add up because they have their own set of requirements and procedures to perform. Among those challenges, one of the prominent is DDoS attacks. So, through this paper, the DDoS attacks in IIoT is studied. This paper has culminated the work done in the domain involving IoT and IIoT. With this different forms of attacks involved in DDoS, the tools involved in generating the attacks and the overall traffic generators is also discussed. To elucidate, IIoT architecture and various layers involved in communication is discussed to correlate the threat of DDoS attacks in IIoT. Further, the studies made in various categories such as machine learning, deep learning, federated learning and transfer learning is elaborated. Finally, the challenges present in IIoT and the security requirements needed to overcome challenges in IIoT is explained.}
}


@article{DBLP:journals/cn/MorenoRSC23,
	author = {Jes{\'{u}}s Fernando Cevallos Moreno and
                  Alessandra Rizzardi and
                  Sabrina Sicari and
                  Alberto Coen{-}Porisini},
	title = {Deep Reinforcement Learning for intrusion detection in Internet of
                  Things: Best practices, lessons learnt, and open challenges},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110016},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110016},
	doi = {10.1016/J.COMNET.2023.110016},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MorenoRSC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) scenario places important challenges even for deep learning-based intrusion detection systems. IoTs are highly heterogeneous networks in which multiple types of nodes and connections between them proliferate at a fast pace. From a deep learning perspective, such complexity translates into dynamic feature spaces where the extraction of semantic patterns and correlations among features may require sophisticated inductive biases to be learnt by gradient-based techniques. The research community has recently suggested using Deep Reinforcement Learning (DRL) as a potent approach to effectively identify cyber-threat attempts in IoTs.}
}


@article{DBLP:journals/cn/AminHA23,
	author = {Md. Ruhul Amin and
                  Md. Shohrab Hossain and
                  Mohammed Atiquzzaman},
	title = {Performance analysis of distributed coordination function with early
                  collision detection in In-band Full-duplex wireless networks},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110017},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110017},
	doi = {10.1016/J.COMNET.2023.110017},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AminHA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-band Full-duplex (IB-FD) wireless has the potential to double the spectral efficiency offering its innate capability to resolve conventional problems emerge owing to the imperfect medium access control (MAC) in In-band half-duplex (IB-HD) wireless networks. IEEE 802.11 distributed coordination function (DCF), a binary slotted exponential back-off with carrier sense multiple access scheme, has been used extensively in literature as the fundamental contention resolution technique to devise MAC protocols for IB-FD wireless networks. Unlike IB-HD, an IB-FD network can contain two simultaneously active transmission links within a single collision domain. This phenomenon invalidates the applicability of existing IB-HD models of the DCF to compute its performance (i.e., saturation throughput) accurately in IB-FD wireless networks. We have proposed two MAC protocols for two types of network scenarios taking early collision detection capability of an IB-FD wireless transceiver into consideration. This paper also presents a distinct analytical model to quantify the performance of the DCF in the context of IB-FD wireless assuming finite number of stations and idle channel conditions. We have applied the concept of primary–secondary transmission links and, thoroughly derived the probability that a station transmits either as primary or secondary in a randomly chosen time-slot taking secondary transmitter’s back-off counter into account. We have also investigated the effect of inter-station interference free IB-FD link on the performance of the DCF. Our protocol, when applied to network without hidden stations, has obtained the highest throughput gain of 1.59, 1.61, 1.68 while the DCF is configured with\nC\nW\nm\ni\nn\nof 16, 32, 64, respectively than IEEE 802.11 basic access scheme. We have observed increased throughput gain with hidden stations, thanks to inter-station interference free transmission region, and achieved 2.12, 2.14, 2.16 times higher throughput, respectively than that of IEEE rts-cts access scheme.}
}


@article{DBLP:journals/cn/OkhovvatKON23,
	author = {Morteza Okhovvat and
                  Mohammad Taghi Kheirabadi and
                  Mohammad Reza Okhovvat and
                  Ali Nodehi},
	title = {Joint time and energy-optimal approach to allocate task to actors
                  in wireless sensor actor networks},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110018},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110018},
	doi = {10.1016/J.COMNET.2023.110018},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/OkhovvatKON23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In most applications of wireless sensor actor networks (WSANs) in harsh environments, minimizing the make-span and maximizing the residual energy are of paramount importance. The majority of existing task allocation approaches is typically concerned with one of the energy savings or time constraints. These approaches do not consider the types and various features of tasks WSANs may need to perform. Moreover, the limitation on the capacity of task queues has been overlooked by these approaches, and thus may not be applicable to some types of real applications such as search and rescue missions. To this end, a novel queue length aware task allocation (QLATA) approach is proposed that considers the energy consumption as well as the make-span. QLATA is aware of task queues constraint, types of tasks, and the distribution necessities of WSANs with hybrid architecture. QLATA comprises of two protocols, namely a Make-span Calculation Protocol (MsCP) and an Energy Consumption Calculation Protocol (ECCP). Through considering both time and energy, QLATA makes a tradeoff between minimizing make-span and maximizing the residual energies of actors by solving the joint optimization for make-span and energy consumption, simultaneously. A series of extensive simulation results on typical scenarios show shorter make-span and higher remaining energy in comparison to when opportunistic load balancing (OLB), minimum make-span task allocation (MMTa), stochastic task allocation (STA), and task assignment algorithm based on quasi-newton interior point (TA-QNIP) approaches is used. It is also shown that the proposed approach performs significantly better than the four compared algorithms in terms of network lifetime.}
}


@article{DBLP:journals/cn/WangSYWL23,
	author = {Zhendong Wang and
                  Liwei Shao and
                  Shuxin Yang and
                  Junling Wang and
                  Dahai Li},
	title = {{CRLM:} {A} cooperative model based on reinforcement learning and
                  metaheuristic algorithms of routing protocols in wireless sensor networks},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110019},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110019},
	doi = {10.1016/J.COMNET.2023.110019},
	timestamp = {Fri, 10 Nov 2023 19:46:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangSYWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In wireless sensor networks, reasonable clustering and routing are keys to efficient energy utilization. However, the selection of cluster heads and routes is NP-hard. Most of the existing routing protocols use heuristic or metaheuristic optimization algorithms to solve this problem. Most protocols regard the selection of the cluster head and routing as two independent problems. However, the selection of cluster heads will affect the selection of routes, and there is a certain relationship between the two stages. Therefore, considering these two problems independently, the solution obtained is not necessarily the optimal solution in the network. In addition, most of the existing routing protocols are still subject to conventional clustering and conventional multi-hop communication in the network, which is extremely unfavorable for reducing the energy consumption of nodes. In this paper, we propose a cooperative model based on reinforcement learning and metaheuristic algorithms called CRLM, in which we use reinforcement learning to enhance the merit-seeking capability of the metaheuristic algorithm and use the algorithm to solve network communication schemes (clustering and routing are considered as one phase). The communication scheme also achieves load balancing of clusters within the network through pruning and employs a novel multi-hop model to reduce network energy waste. Compared to E-ALWO, ChOA-HGS, GATERP, GWO, IPSO-GWO, and LEACH, CRLM has 56%, 95%, 34.5%, 85.7%, 116.7%, and 140.7% improvements in network lifetime.}
}


@article{DBLP:journals/cn/LiangZ23,
	author = {Yu Liang and
                  Sheng Zhang},
	title = {Mobility-aware multi-user service placement and resource allocation
                  in edge computing},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110020},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110020},
	doi = {10.1016/J.COMNET.2023.110020},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiangZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rise of Mobile Edge Computing (MEC) has brought us its enormous potential and value in mobile service applications. By pushing computing and storage resources from the cloud to the network edge, it reduces transmission latency, and supports applications that require low latency, such as virtual reality and video analytics IoT services. However, many existing works only consider the problem of service placement in MEC, and the problem of computing resource allocation has received less attention. This paper focuses on the joint optimization of Service Placement and computational Resource Allocation (SPRA) in the MEC environment, with the goal of minimizing the total cost of service latency, communication latency, and service migration. For offline cases, we propose an optimal algorithm, D-SPA , based on dynamic programming and an improved algorithm, S-SPA, based on state sampling, which effectively alleviates the state explosion problem in D-SPA. In the online case, due to the unpredictability of future environmental information, we propose the online greedy algorithm OGA, and theoretically show the approximate ratio of OGA. Extensive experiments show that our algorithm is more efficient than other baseline algorithms, and it reduces the total cost by 28.6% on average.}
}


@article{DBLP:journals/cn/WangHMLCSLH23,
	author = {Zhangquan Wang and
                  Jiaxuan Huang and
                  Kelei Miao and
                  Xiaowen Lv and
                  Yourong Chen and
                  Bing Su and
                  Liyuan Liu and
                  Meng Han},
	title = {Lightweight zero-knowledge authentication scheme for IoT embedded
                  devices},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110021},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110021},
	doi = {10.1016/J.COMNET.2023.110021},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/WangHMLCSLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Authentication technology has been employed in a variety of Internet of Things (IoT) applications, such as electronic medical services, smart homes, and the Internet of Vehicles, to protect data privacy and device accessibility. However, current zero-knowledge authentication schemes for resource-limited embedded devices face challenges in computation, communication, and storage costs. In order to solve this problem, we propose a lightweight zero-knowledge authentication scheme for IoT-embedded devices (LZIA). In the proposed scheme, the identity information is associated with a unique identifier, and the device identities are allocated using hash commitment in the registration phase. Then, a lightweight registration method is proposed to reduce the computation and storage costs. In the secret key management phase, the direct communication between the embedded device and the authentication server (AS) for device key management is built. By combining device identity information and the Chebyshev polynomial chaotic map, we simplify the device’s secret key structure and propose a secret key generation equation, an update equation, and a secret key management method to reduce computation and storage costs. In the authentication phase, we replace traditional cryptographic operations with Chebyshev polynomial operations and consolidate all intermediate data into a polynomial for unified authentication. We then optimize the device’s proof generation and proof verification equations and propose a lightweight authentication protocol to effectively reduce the number of interactions and computational costs. Experimental results demonstrate that LZIA, whether in simulation or actual hardware environments, can reduce computation, communication, and storage costs while guaranteeing security. LZIA surpasses EEAS, LAKA, and SRAM in terms of both security and performance.}
}


@article{DBLP:journals/cn/AwadSAAMA23,
	author = {Sumaya D. Awad and
                  Aduwati Sali and
                  Mohanad M. Al{-}Wani and
                  Ali M. Al{-}Saegh and
                  Jit Singh Mandeep and
                  Raja Syamsul Azmir Raja Abdullah},
	title = {End-to-end {DVB-S2X} system design with DL-based channel estimation
                  over satellite fading channels at Ka-band},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110022},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110022},
	doi = {10.1016/J.COMNET.2023.110022},
	timestamp = {Sun, 12 Nov 2023 02:17:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AwadSAAMA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellite channels suffer from heavy fading due to the atmospheric impairments at high frequencies. Therefore, channel estimation is essential for coherent detection and demodulation in satellite-coded systems. The conventional minimum mean square error (MMSE) estimator (the theoretical upper bound) requires a priori knowledge about the channel statistics which is not feasible to obtain in a real transmission. Also, it suffers from high complexity. However, deep learning (DL) estimators do not require any information about the channel statistics. Therefore, in this paper, two DL-based channel estimators are proposed for digital and video broadcasting second generation extension (DVB-S2X) system with less complexity than the MMSE estimator. In particular, the bidirectional long-short term memory (BLSTM) and the gated recurrent unit (GRU) are adopted in our proposed estimators which are termed as\nDL\nBLSTM\nand\nDL\nGRU\n, respectively. The proposed estimators evaluated over two satellite fading channels; one with heavy fading and the other with low fading. Moreover, these estimators are compared with the conventional estimators, the least square (LS) and the MMSE, in terms of the normalized mean square error (NMSE) and the bit error rate (BER). Simulation results show that the proposed DL-based estimators have better performance than the LS estimator and the\nDL\nBLSTM\nhas better performance than the\nDL\nGRU\nestimator in terms of NMSE and BER with both satellite channels.}
}


@article{DBLP:journals/cn/XiaoZHC23,
	author = {Guangbing Xiao and
                  Haibo Zhang and
                  Zhiyi Huang and
                  Yawen Chen},
	title = {Decentralized piggybacking-based dissemination of Cooperative Awareness
                  Messages in vehicular ad-hoc networks},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110032},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110032},
	doi = {10.1016/J.COMNET.2023.110032},
	timestamp = {Sun, 12 Nov 2023 02:17:57 +0100},
	biburl = {https://dblp.org/rec/journals/cn/XiaoZHC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle-to-Vehicle (V2V) communication has shown great promise to improve road safety and traffic efficiency via the periodic broadcasting of Cooperative Awareness Messages (CAMs), in which traffic information could be exchanged among adjacent vehicles. Since each CAM contains local-observed traffic information such as geolocation, speed, and safety alerts, frequent CAM losses will decrease the reliability on avoiding crashes and injuries on the road. Existing solutions tend to recover the lost CAMs through either source-based re-transmission or neighbor-based forwarding, which may suffer from significant performance degradation due to the fast movement of vehicles and high dynamic network topology. In this paper, we develop a decentralized cooperative broadcasting protocol for CAM dissemination, where each vehicle can piggyback some received CAMs in its routine broadcasting to help others recover their lost CAMs. To promote cooperation of CAM piggybacking, a new data structure called bitmap is introduced to record the packet receiving status at each vehicle, based on which time slots could be allocated to vehicles with low data collisions, lost CAMs could be detected only using each vehicle’s local information, and the qualities of wireless links could be estimated dynamically. Then, we propose two piggybacking schemes, Benefit-based Piggybacking (BP) and Suggestion-based Piggybacking (SP), aiming to maximize the average CAM delivery ratio. We evaluate the proposed schemes in simulations with realistic vehicle mobility traces and channel models. Simulation results show that our bitmap-based lost CAM detection has superior performance than traditional request-based solutions. In comparison with forwarding-based schemes, our schemes can achieve much higher CAM broadcast reliability with short time delay and small communication overhead.}
}


@article{DBLP:journals/cn/LiDZQ23,
	author = {Man Li and
                  Shuangxing Deng and
                  Huachun Zhou and
                  Yajuan Qin},
	title = {A path selection scheme for detecting malicious behavior based on
                  deep reinforcement learning in SDN/NFV-Enabled network},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110034},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110034},
	doi = {10.1016/J.COMNET.2023.110034},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiDZQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The SDN/NFV network is prone to different types of attacks. The Distributed Denial of Service (DDoS) attack has the most severe impact as it can overwhelm the critical components of SDN/NFV to degrade its performance. We propose a closed-loop security architecture (SFCSA) and virtualize detection methods as network service functions in this article. Combining the detection methods forms detection paths, in which different detection paths affect security performance differently. Further, we model the path selection problem as a Markov Decision Process, where the reward balances the malicious traffic detection capability and end-to-end latency. Then, an integrated deep reinforcement learning and convolution neural network path selection algorithm (CNNQ) is proposed. Furthermore, we define a total path malicious traffic detection capability metric. The defined metrics and common metrics are applied to evaluate the building prototype, with the corresponding experimental results demonstrating that the detection performance when combining multiple detection modules outperforms a single detection-based module. Besides, we verify the effectiveness of the CNNQ method under various DDoS attacks scenarios and present the fine-grained classification results of the selected detection modules.}
}


@article{DBLP:journals/cn/LiZTLZ23,
	author = {Kun Li and
                  Huachun Zhou and
                  Zhe Tu and
                  Ouyang Liu and
                  Hongke Zhang},
	title = {{AT-GCN:} {A} DDoS attack path tracing system based on attack traceability
                  knowledge base and {GCN}},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110036},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110036},
	doi = {10.1016/J.COMNET.2023.110036},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiZTLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of the Internet provides sufficient attack entrances for massive malicious terminals, which makes it extremely challenging to effectively trace the path of DDoS attacks. Traditional solutions mainly trace distributed denial of service (DDoS) attacks by adding tags to data packet headers or querying logs, which increases the cost of tagging. There are also some works that implement attack tracking based on the network-wide perspective and centralized control of SDN, but these methods are difficult to deploy on a large scale. In order to solve this problem, we propose a DDoS attack path tracing system (AT-GCN) based on attack traceability knowledge base and graph convolutional network (GCN). We first propose the construction process of the attack traceability knowledge base, and design the intra-domain attack graph and the traceability algorithm recommendation graph to solve the problems of DDoS attack path traceability and optimal traceability solution recommendation. On this basis, we propose a GCN-based intra-domain attack traceability scheme, and design a subgraph sampling algorithm Tracing-Sample adapted to intra-domain DDoS attack traceability, aiming to efficiently use the graph structure in the knowledge base to reproduce DDoS attack paths. Additionally, we recommend the traceability algorithm based on user-based collaborative filtering (UBCF), and dynamically recommend the best traceability algorithm according to the different requirements of administrators for traceability performance. Compared with other GCN algorithms, the results show that the recall of the AT-GCN system is increased by 7.3% on average, and the FPR is reduced by 5.7% on average at the expense of the memory usage rate. Under different scale topologies, the recall of the AT-GCN system can be stabilized at 95%.}
}


@article{DBLP:journals/cn/RodriguezS23,
	author = {Jes{\'{u}}s Garc{\'{\i}}a Rodr{\'{\i}}guez and
                  Antonio F. Skarmeta},
	title = {A privacy-preserving attribute-based framework for IoT identity lifecycle
                  management},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110039},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110039},
	doi = {10.1016/J.COMNET.2023.110039},
	timestamp = {Sat, 08 Jun 2024 13:14:22 +0200},
	biburl = {https://dblp.org/rec/journals/cn/RodriguezS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) has brought a new era of interconnected devices and seamless data exchange. As the IoT ecosystem continues to expand, there is an increasing need for effective identity management mechanisms, specifically for authorization processes and access control. The pervasiveness of such devices demands that desirable solutions tackle not only security properties but also privacy aspects like granular control over which identity data is shared in authentication/authorization processes, covering aspects like bootstrapping, enrolment, and service provision. In this context, it is natural to turn to privacy-enhancing technologies, like (privacy-preserving) Attribute-Based Credentials (p-ABC), for achieving both high security and privacy guarantees. Nonetheless, these technical tools need to be accompanied by a comprehensive approach that deals with the particularities of IoT scenarios and covers the full lifetime of the device. In this work, we propose the use of a p-ABC scheme with support for distributed issuance (dp-ABC) as a keystone for privacy-preserving attribute-based authentication and authorization in IoT scenarios. We integrate said cryptographic scheme with W3C’s Verifiable Credentials standard, evaluating its impact to gauge its feasibility. The integration facilitates adoption and, particularly, allows the solution to transparently coexist with simpler techniques in heterogeneous scenarios that demand them. Moreover, we define and analyse a generic and comprehensive framework for identity management that identifies challenges throughout the device’s lifetime to achieve IoT privacy-preserving identity management following self-sovereign principles. We show how the various aspects identified in the framework are tackled in a concrete instantiation as part of the H2020 project ERATOSTHENES.}
}


@article{DBLP:journals/cn/LiuWL23,
	author = {Zhenpeng Liu and
                  Shuo Wang and
                  Yi Liu},
	title = {Blockchain-based integrity auditing for shared data in cloud storage
                  with file prediction},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110040},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110040},
	doi = {10.1016/J.COMNET.2023.110040},
	timestamp = {Wed, 08 Nov 2023 17:21:50 +0100},
	biburl = {https://dblp.org/rec/journals/cn/LiuWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing publicity of information, more and more users are willing to share their data with others. Therefore, in cloud storage services, the security of shared data has begun to attract attention. Most of the current integrity auditing schemes cannot balance data sharing and audit security well. In view of this, we propose an audit scheme based on blockchain and file prediction in the shared environment. On the one hand, a lightweight signature algorithm is designed to realize the revocation of group members without source data transmission, which greatly avoids the communication overhead between the user and the cloud. On the other hand, we introduce blockchain technology into auditing and use smart contracts to verify the integrity of cloud data, making the system more secure, tamper-resistant, and traceable. Finally, in order to ensure that a file is audited only when necessary, we propose the concept of valid auditing and predict the frequently accessed or likely accessed files in the sharing group, which greatly improves the valid auditing rate and saves system computing resources. The security analysis proves that our method is safe and reliable. The performance evaluation and experimental comparison prove that our scheme has high efficiency.}
}


@article{DBLP:journals/cn/TianGZS23,
	author = {He Tian and
                  Kaihong Guo and
                  Ran Zhang and
                  Shiliang Shao},
	title = {Prediction of evolution behavior of Internet bottleneck delay based
                  on improved Logistic equation},
	journal = {Comput. Networks},
	volume = {236},
	pages = {110041},
	year = {2023},
	url = {https://doi.org/10.1016/j.comnet.2023.110041},
	doi = {10.1016/J.COMNET.2023.110041},
	timestamp = {Thu, 09 Nov 2023 21:13:49 +0100},
	biburl = {https://dblp.org/rec/journals/cn/TianGZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As massive real-time systems and various types of networks connect to the Internet, real-time Internet connections are becoming increasingly prominent. Research on bottleneck delays provides a reference for solving real-time performance issues in data transmission for real-time systems on the Internet. In this study, under the Internet macro-topology, we utilized real probing data to conduct an in-depth study on the evolution behavior of bottleneck delays in end-to-end delays. We demonstrate that more than 90% of valid paths were 9 hops on average, and the ratio of the maximum link delay to the one-way delay was above 1/3. In addition, bottleneck delays on the valid paths were mainly concentrated in the range of [0, 130 ms], and the relative spatial positions of the bottleneck delays appearing in the links of one-way delays tended to be near the middle. The timing evolution sequence of the bottleneck delays exhibited a slow upward trend and quasiperiodic oscillations. On this basis, the Logistic equation was introduced to describe this evolution and establish the prediction model, and the gray Wolf Optimizer (GWO) algorithm was used to fit and optimize the model parameters. Finally, experiments were conducted to verify the validity and accuracy of the models. Their prediction accuracies exceeded 90% according to a quantitative evaluation, and the models fit well according to a\nχ\n2\ngoodness-of-fit test. This validates our modeling approach and demonstrates that the model can predict bottleneck delays accurately in a short period of time.}
}
