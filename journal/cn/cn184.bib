@article{DBLP:journals/cn/ZhangCWTJJ21,
	author = {Xiaoquan Zhang and
                  Lin Cui and
                  Kaimin Wei and
                  Fung Po Tso and
                  Yangyang Ji and
                  Weijia Jia},
	title = {A survey on stateful data plane in software defined networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107597},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107597},
	doi = {10.1016/J.COMNET.2020.107597},
	timestamp = {Tue, 16 Aug 2022 23:06:46 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZhangCWTJJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Networking (SDN), which decouples control plane and data plane, normally stores states on controllers to provide flexible programmability and convenient management. However, recent studies have shown that such configuration may cause frequent and unnecessary interactions between data plane and controllers in some cases. For example, a DDoS detection installed on a controller needs to fetch information from data plane periodically, introducing additional network delay and controller overhead. Thus, stateful data plane is proposed to offload states and operation logics from controller to data plane. Stateful data plane allows switches to perform some operations independently, accelerating packets processing while reducing overhead on both controllers and networks. However, stateful data plane increases the complexity of network devices and imposes many new challenges to the management and schedule of SDN enabled networks. This paper conducts a comprehensive survey on the latest research works and provides insights into stateful data plane. Both stateful data plane platforms and compilers are extensively summarized and analyzed, as well as explicit design of applications based on them. Afterwards, we dwell on the fundamental technologies and research challenges, including implementation considerations of stateful data plane. Finally, we conclude this survey paper with some future works and discuss open research issues.}
}


@article{DBLP:journals/cn/SuraciAABI21,
	author = {Chiara Suraci and
                  Giuseppe Araniti and
                  Andrea Abrardo and
                  Giuseppe Bianchi and
                  Antonio Iera},
	title = {A stakeholder-oriented security analysis in virtualized 5G cellular
                  networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107604},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107604},
	doi = {10.1016/J.COMNET.2020.107604},
	timestamp = {Mon, 28 Aug 2023 21:39:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SuraciAABI21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Besides significantly outperforming past generations in terms of capacity and throughput, 5G networks and systems will provide an infrastructure for the support of highly diversified services and “verticals”. Indeed, the major paradigm shift with respect to previous cellular network generations, specifically oriented to one class of terminals (namely, people’s cell phones), is the largely heterogeneous nature of the multiplicity of end systems supported. Within a 5G infrastructure, playing the role of “network of networks”, traditionally independent technical and business stakeholders are now called to cooperate in the deployment of crucial infrastructure components relying on innovative (for the Telecom world) technologies such as virtualization, not in the traditional operators’ portfolio, and eventually placed in security-critical parts of the network — think e.g. to Multi Access Edge Computing systems. Goal of this survey is to analyze the complex threat landscape of 5G systems, by taking the point of view of the involved stakeholders. The motivation behind our proposed analysis revolves on the observation that, in complex and virtualized systems such as the 5G infrastructure, an attack to a system component under the responsibility of a given stakeholder may yield a dramatic impact to a completely different player. Therefore, while reviewing the many 5G security risks and relevant threats which the main stakeholders operating in virtualized 5G cellular networks are exposed to, we will try to showcase the sometimes non-obvious relation between impact and responsibility, as well as identify shared responsibilities.}
}


@article{DBLP:journals/cn/ZakiGTFA21,
	author = {Faiz Zaki and
                  Abdullah Gani and
                  Hamid Tahaei and
                  Steven Furnell and
                  Nor Badrul Anuar},
	title = {Grano-GT: {A} granular ground truth collection tool for encrypted
                  browser-based Internet traffic},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107617},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107617},
	doi = {10.1016/J.COMNET.2020.107617},
	timestamp = {Mon, 28 Aug 2023 21:39:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZakiGTFA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern network traffic classification puts much attention toward producing a granular classification of the traffic, such as at the application service level. However, the classification process is often impaired by the lack of granular network traffic ground truth. Granular network traffic ground truth is critical to provide a benchmark for a fair evaluation of modern network traffic classification. Nevertheless, in modern network traffic classification, existing ground truth tools only managed to build the ground truth at the application name level at most. Application name level granularity is quickly becoming insufficient to address the current needs of network traffic classification and therefore; this paper presents the design, development and experimental evaluation of Grano-GT, a tool to build a reliable and highly granular network traffic ground truth for encrypted browser-based traffic at the application name and service levels. Grano-GT builds on four main engines which are packet capture, browser, application and service isolator engines. These engines work together to intercept the application requests and combine them with the support of temporal features and cascading filters to produce reliable and highly granular ground truth. Preliminary experimental results show that Grano-GT can classify the Internet traffic into respective application names with high reliability. Grano-GT achieved an average accuracy of more than 95% when validated using nDPI at the application name level. The remaining 5% loss of accuracy was primarily due to the unavailability of signatures in nDPI. In addition, Grano-GT managed to classify application service traffic with significant reliability and validated using the Kolmogorov-Smirnov test.}
}


@article{DBLP:journals/cn/AmadeoRCM21,
	author = {Marica Amadeo and
                  Giuseppe Ruggeri and
                  Claudia Campolo and
                  Antonella Molinaro},
	title = {Diversity-improved caching of popular transient contents in Vehicular
                  Named Data Networking},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107625},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107625},
	doi = {10.1016/J.COMNET.2020.107625},
	timestamp = {Thu, 23 Jun 2022 20:03:26 +0200},
	biburl = {https://dblp.org/rec/journals/cn/AmadeoRCM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-network caching, natively enabled by Named Data Networking (NDN), is effective to speed up content retrieval in vehicular environments, where a large variety of contents are typically transient, i.e., they expire after a certain amount of time, and may exhibit different popularity profiles. Lifetime and popularity play a crucial role in the content caching decision: intuitively, caching a popular content with long lifetime can be more useful than caching an unpopular one that is ready to expire and then to be dropped from the content store. At the same time, making nearby nodes caching different contents and, therefore, improving the caching diversity, can be crucial to get better delivery performance over the broadcast wireless medium. In this paper, we devise a novel distributed caching strategy where vehicles autonomously decide which content is to be locally cached according to the content residual lifetime, its popularity and the perceived availability of the same content in the neighborhood. The target is to cache with higher probability more popular contents with a longer lifetime, which are not already cached by a nearby node, thus improving the caching diversity in the neighborhood. As a result, vehicles can find the majority of distinct fresh and popular contents nearby, without flooding the network with content requests that have to reach the original source. Performance evaluation shows that the conceived solution outperforms representative benchmark schemes, by guaranteeing, among others, the shortest content retrieval time and the lowest network traffic load.}
}


@article{DBLP:journals/cn/MidogluKALARG21,
	author = {Cise Midoglu and
                  Konstantinos Kousias and
                  {\"{O}}zg{\"{u}} Alay and
                  Andra Lutu and
                  Antonios Argyriou and
                  Michael Riegler and
                  Carsten Griwodz},
	title = {Large scale "speedtest" experimentation in Mobile Broadband Networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107629},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107629},
	doi = {10.1016/J.COMNET.2020.107629},
	timestamp = {Wed, 08 Mar 2023 16:35:14 +0100},
	biburl = {https://dblp.org/rec/journals/cn/MidogluKALARG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Characterizing and evaluating the performance of Mobile Broadband (MBB) networks is a vital need for today’s societies. Testbed-based measurements are of great significance in this context, since they allow for controlled and longitudinal experimentation. In this work, we focus on “speed” as an important Quality of Service (QoS) indicator for MBB networks, and work with MONROE-Nettest, an open source speedtest tool running as an Experiment as a Service (EaaS) on the Measuring Mobile Broadband Networks in Europe (MONROE) testbed. We conduct an extensive longitudinal measurement campaign spanning 2 countries over 2 years, and provide our experiment results together with rich metadata as an open dataset. We characterize this open dataset in detail, as well as derive insights from it regarding the impact of network context, spatio-temporal effects, roaming, and mobility on network performance. We describe our experiences about conducting speedtest measurements in MBB, and discuss the challenges associated with large scale testbed experimentation in operational MBB networks. Tackling one of the said challenges further, we introduce the notion of adaptive speedtest duration, and leverage a Machine Learning (ML) based algorithm to provide a proof-of-concept implementation called “Speedtest++”. Finally, we describe the lessons we have learned, as well as provide an overall discussion of how open datasets can support MBB research, and comment on open challenges, in the hope that these can serve as discussion points for future work.}
}


@article{DBLP:journals/cn/SalamLPRZ21,
	author = {Ahmed Said Abdel Salam and
                  Michele Luglio and
                  Natale Patriciello and
                  Cesare Roseti and
                  Francesco Zampognaro},
	title = {{TCP} Wave over Linux: {A} disruptive alternative to the traditional
                  {TCP} window approach},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107633},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107633},
	doi = {10.1016/J.COMNET.2020.107633},
	timestamp = {Tue, 07 May 2024 20:23:17 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SalamLPRZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {TCP Wave relies on a disruptive communication paradigm based on bursts, to overcome limitations of legacy TCP with modern applications and challenging network scenarios. This protocol was already validated with appealing results via simulations. This work now presents its implementation in the Linux kernel, focusing on the adaptations needed to deal with real networking systems. A Linux implementation paves the way to validate the protocol algorithms and working principles on real communication environments, thus drawing its operational perimeter. In this regard, the most ambitious goal is to show the bright side of the burst transmission as a novel opportunity for performance optimization in the future Internet connectivity scenarios. TCP Wave performance is therefore compared with reference TCP congestion schemes currently included in the Linux kernel, namely BBR and Cubic, under a large set of network configurations. A vast gamut of performance indicators are presented and discussed, including throughput, latency, fairness, friendliness, loss management and reaction to network and traffic changes. Definitely, this paper is a manifest for the real TCP Wave implementation in Linux, which can act as reference for future works.}
}


@article{DBLP:journals/cn/SviridovBTGBB21,
	author = {German Sviridov and
                  Marco Bonola and
                  Angelo Tulumello and
                  Paolo Giaccone and
                  Andrea Bianco and
                  Giuseppe Bianchi},
	title = {LOcAl DEcisions on Replicated States {(LOADER)} in programmable dataplanes:
                  Programming abstraction and experimental evaluation},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107637},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107637},
	doi = {10.1016/J.COMNET.2020.107637},
	timestamp = {Tue, 07 May 2024 20:23:16 +0200},
	biburl = {https://dblp.org/rec/journals/cn/SviridovBTGBB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable data planes recently emerged as a prominent innovation in Software Defined Networking (SDN). They provide support for stateful per-packet/per-flow operations over hardware network switches specifically designed for network processing. Unlike early SDN solutions such as OpenFlow, modern stateful data planes permit to keep (and dynamically update) per-flow states local to each switch, thus dramatically improving reactiveness of network applications to different state changes. Still, in stateful data planes, the management of non-local states is assumed to be completely delegated to a centralized controller, thus requiring extra overhead to be accessed. Our LOADER proposal aims at contrasting the apparent dichotomy between local and non-local states. We do so by introducing a new possibility: permit to take localized (in-switch) decisions not only on local states but also on global replicated states, thus providing support for network-wide applications without incurring the drawbacks of classic approaches. To this purpose, (i) we provide high-level programming abstractions devised to define the states and the update logic of a generic network-wide application, and (ii) we detail the underlying low-level state management and replication mechanisms. We then show LOADER’s independence of the stateful data plane technology employed, by implementing it over two distinct stateful data planes (P4 switches and OPP – Open Packet Processor – switches), and by experimentally validating both implementations in an emulated testbed using a simple distributed Deny-of-Service (DoS) detection application.}
}


@article{DBLP:journals/cn/BaiXMLF21,
	author = {Yude Bai and
                  Zhenchang Xing and
                  Duoyuan Ma and
                  Xiaohong Li and
                  Zhiyong Feng},
	title = {Comparative analysis of feature representations and machine learning
                  methods in Android family classification},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107639},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107639},
	doi = {10.1016/J.COMNET.2020.107639},
	timestamp = {Sat, 09 Jan 2021 14:13:51 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BaiXMLF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to overcome the lasting increase of Android malware, malware family classification, which clusters malware with the same features into one family, has been proposed as an efficient way for malware analysis. Several machine learning based approaches have been proposed for such task of malware family classification. However, due to the adoption of very different features and learning methods in different approaches, it is still an open question to explore: which approach works better for malware family classification? In this paper, we conduct extensive experiments to answer this question. For three widely known Android malware datasets, we design five multi-classification methods for predicting Android malware family. Based on the survey of Android malware analysis literatures and the observation of a large number of Android malware, we construct a set of 250 common features shared by Android malware. And we also collect 16873 documentary features from Android Developer as a comparison. Furthermore, we investigate the effects of transfer learning for adapting the model on three malware datasets on different scales. Our empirical results show that (i) the classification methods perform very closely, with neural network model having marginally better performance (1% to 3% in F1-score), (ii) features contribute most for classification, especially to enhance API features on larger datasets, and (iii) it is model transferable across different malware datasets based on various transfer learning tasks.}
}


@article{DBLP:journals/cn/ZengYCLTWZZ21,
	author = {Yuwei Zeng and
                  Xiaochun Yun and
                  Xunxun Chen and
                  Boquan Li and
                  Haiwei Tsang and
                  Yipeng Wang and
                  Tianning Zang and
                  Yongzheng Zhang},
	title = {Finding disposable domain names: {A} linguistics-based stacking approach},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107642},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107642},
	doi = {10.1016/J.COMNET.2020.107642},
	timestamp = {Mon, 28 Aug 2023 08:02:20 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZengYCLTWZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An increasing number of Internet services tend to collect one-time information from clients via DNS queries. Notably, the uncertainty of such transient information makes these domain names be queried only once in their lifetime. This type of domain is called disposable domain. Although they do not involve any malicious activities, the efficiency of DNS infrastructures is still affected by their ever-increasing number. Existing approaches for detecting disposable domains have serious disadvantages, such as poor timeliness and high false positive rate. In this paper, we conduct an extensive measurement study of the ISP-level DNS traffic and find that the readability of domain name is suitable for identifying disposable domains. Therefore, we propose Vogers, a linguistics-based stacking model, to detect disposable domains from raw DNS traffic. Compared with the prior arts, Vogers decreases the false positive rate by more than 17%, while maintaining the true positive rate above 98.9%. In addition, Vogers generalizes quite well to unknown environments, whereby we are able to report new disposable domains. Our further application of Vogers in the real-world DNS traffic shows that filtering disposable domains can improve the efficiency of DNS infrastructures.}
}


@article{DBLP:journals/cn/RottondiMBCS21,
	author = {Cristina Rottondi and
                  Francesco Malandrino and
                  Andrea Bianco and
                  Carla{-}Fabiana Chiasserini and
                  Ioannis Stavrakakis},
	title = {Scheduling of emergency tasks for multiservice UAVs in post-disaster
                  scenarios},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107644},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107644},
	doi = {10.1016/J.COMNET.2020.107644},
	timestamp = {Thu, 07 Jan 2021 09:00:36 +0100},
	biburl = {https://dblp.org/rec/journals/cn/RottondiMBCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Single-task UAVs are increasingly being employed to carry out surveillance, parcel delivery, communication support, and other specific tasks. When the geographical area of operation of single-task missions is common, e.g., in post-disaster recovery scenarios, it is more efficient to have multiple tasks carried out as part of a single UAV mission. In these scenarios, the UAVs’ equipment and mission plan must be carefully selected to minimize the carried load and overall resource consumption. In this paper, we investigate the joint planning of multitask missions leveraging a fleet of UAVs equipped with a standard set of accessories enabling heterogeneous tasks. To this end, an optimization problem is formulated yielding the optimal joint planning and deriving the resulting quality of the delivered tasks. In addition, two heuristic solutions are developed for large-scale environments to cope with the increased complexity of the optimization framework. The joint planning is applied to a specific scenario of a flood in the San Francisco area. Results show the effectiveness of the proposed heuristic solutions, which provide good performance and allow for drastic savings in the computational time required to plan the UAVs’ trajectories with respect to the optimal approach, thus enabling prompt reaction to the emergency events.}
}


@article{DBLP:journals/cn/BalciS21,
	author = {Abdullah Balci and
                  Radosveta Sokullu},
	title = {Massive connectivity with machine learning for the Internet of Things},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107646},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107646},
	doi = {10.1016/J.COMNET.2020.107646},
	timestamp = {Thu, 21 Jan 2021 17:34:52 +0100},
	biburl = {https://dblp.org/rec/journals/cn/BalciS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by the need to ensure the connectivity of an unprecedentedly huge number of IoT devices with no human intervention the issues of massive connectivity have recently become one of the main research areas in IoT studies. Conventional wireless communication technologies are designed for Human-to-Human (H2H) communication which leads to major problems in primary access, channel utilization and spectrum efficiency when massive numbers of devices require connectivity. Current random access procedures are based on a four-step handshaking with control messages which contradicts the requirements of IoT applications in terms of small data payloads and low complexity. Targeted channel utilization and spectrum efficiency cannot be achieved using traditional orthogonal approaches. Thus the goal of our work is to review the most recent developments and critically evaluate the existing work related to the evolution of network access methods in the new communication era. The paper covers three major aspects: first the primary random access procedures, proposed for IoT communications are discussed. The second aspect focuses on the approaches for integration of existing random multiple access schemes with non-orthogonal multiple access methods (NOMA). This integration of random access procedures with NOMA opens a new research trend in the field of massive connectivity. Operating on space domains additional to the physical domain such as code and power domains, NOMA integration targets increased channel utilization and spectrum efficiency to complement the flexibility of random access. On the other hand, the design of efficient algorithms for massive connectivity in IoT is also challenged by the highly application and environmentally dependent traffic model. A new angle of tackling this problem has emerged thanks to the extensive developments in machine learning and the possibilities of their incorporation in communication networks. Thus, the final aspect this review paper addresses are the newly emerging research directions of incorporating machine learning (ML) methods for providing efficient IoT connectivity. Breakthrough ML techniques allow wireless networking devices to perform transmissions by learning and building knowledge about the communication and networking environment. A critical evaluation of the large body of work accumulated in this area in the most recent years and outlining of some major open research issues concludes the paper.}
}


@article{DBLP:journals/cn/QureshiJP21,
	author = {Kashif Naseer Qureshi and
                  Gwanggil Jeon and
                  Francesco Piccialli},
	title = {Anomaly detection and trust authority in artificial intelligence and
                  cloud computing},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107647},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107647},
	doi = {10.1016/J.COMNET.2020.107647},
	timestamp = {Thu, 14 Oct 2021 09:09:01 +0200},
	biburl = {https://dblp.org/rec/journals/cn/QureshiJP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of smart Internet of Things (IoT) and its multimedia applications with conventional cloud and edge computing platforms are driving a new trend that shifts the functions of centralized networks. These centralized cloud and edge computing networks are encountering various new routing and security challenges. These networks are vulnerable due to different malicious activities and security attacks. The malicious activities lead to link failure and wrong forwarding decisions and or divert the paths. To detect the malicious activities in these networks, we need an efficient detection system for malicious switches in Software Defined Networks (SDN) data plan and leads to data traffic diversion and degrades the network performance. Another aspect is the trust of edge devices and always need to trustworthy devices to forward the IoT devices data to SDN networks. In this paper, we proposed a Software-Defined Network-based Anomaly Detection System (SDN-ADS) for edge computing-based system architecture for IoT networks. Afterwards, we proposed an anomaly detection system to detect the device's behaviour for SDN and edge computing networks. Also, we proposed a Trusted Authority for Edge Computing (TA-Edge) to ensure the trust of edge devices for data forwarding. The edge device is acting as a certificate authority for the specified trusted domain. To overcome the edge devices overhead, in this proposed TA-Edge model, the edge node, only one time, verifies the certificate and when the trust is established, all communication can be done through local certificates. The simulation results show the better performance of proposed systems in terms of different performance parameters.}
}


@article{DBLP:journals/cn/AlonsoPDMNJ21,
	author = {Rodney Martinez Alonso and
                  David Plets and
                  Margot Deruyck and
                  Luc Martens and
                  Glauco Guillen Nieto and
                  Wout Joseph},
	title = {Multi-objective optimization of cognitive radio networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107651},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107651},
	doi = {10.1016/J.COMNET.2020.107651},
	timestamp = {Tue, 21 Mar 2023 21:08:29 +0100},
	biburl = {https://dblp.org/rec/journals/cn/AlonsoPDMNJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {New generation networks, based on Cognitive Radio technology, allow dynamic allocation of the spectrum, alleviating spectrum scarcity. These networks also have a resilient potential for dynamic operation for energy saving. In this paper, we present a novel wireless network optimization algorithm for cognitive radio networks based on a cloud sharing-decision mechanism. Three Key Performance Indicators (KPIs) were optimized: spectrum usage, power consumption, and exposure. For a realistic suburban scenario in Ghent city, Belgium, we determine the optimal trade-off between the KPIs. Compared to a traditional Cognitive Radio network design, our optimization algorithm for the cloud-based architecture reduced the network power consumption by 27.5%, the average global exposure by 34.3%, and spectrum usage by 34.5% at the same time. Even for the worst-case optimization (worst achieved result of a single KPI), our solution performs better than the traditional architecture by 4.8% in terms of network power consumption, 7.3% in terms of spectrum usage, and 4.3% in terms of global exposure.}
}


@article{DBLP:journals/cn/ChenZWQL21,
	author = {Ning Chen and
                  Sheng Zhang and
                  Jie Wu and
                  Zhuzhong Qian and
                  Sanglu Lu},
	title = {Learning scheduling bursty requests in Mobile Edge Computing using
                  DeepLoad},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107655},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107655},
	doi = {10.1016/J.COMNET.2020.107655},
	timestamp = {Thu, 07 Jan 2021 09:00:35 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChenZWQL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of Mobile Edge Computing (MEC) alleviates the large transmission latency resulting from the traditional cloud computing. For the compute-intensive requests such as video analysis, mobile users prefer to obtain a desired quality of experience (QoE) with neglected latency and reduced energy consumption. The popularity of smart devices allows users to release a run of compute-intensive as well as latency-sensitive requests anywhere, which may lead to bursty requests. A single resource-constrained edge server nearby is capable of handling a small amount of requests quickly, yet it seems helpless when encountering bursty compute-intensive requests. Despite the abundance of recently proposed schemes, the majority focus on efficiently scheduling pending requests in a single edge server, and ignored the potential role of edge collaboration to schedule bursty requests. Besides, while some recent studies proposed to finish a task using multiple devices, they focused on collaboration between mobile devices rather than between edge servers. Hence, we propose DeepLoad, a S2S system that schedules the bursty requests with a collaborative method using reinforcement learning (RL). DeepLoad decouples the scheduling decision into AP selection for setting the access point and workload redistribution for collaborative servers. DeepLoad trains a neural network model that picks decisions for each request based on observations collected by mobile devices. DeepLoad learns to make scheduling decisions solely through the resulting performance of historical decisions rather than rely on pre-programmed models or specific assumptions for the environment. Naturally, DeepLoad automatically learns the scheduling algorithm for each request and obtains a gratifying QoE. We aim to maximize the fraction of requests finished before their attached deadlines. Based on the Shanghai taxi trajectory data set, we design a simulator to obtain abundant samples, and leverage two GeForce GTX TITAN Xp GPUs to train the Actor–Critic network. Compared to the state-of-the-art bandwidth-based and server resources-based methods, DeepLoad can achieve a significant improvement in average fraction.}
}


@article{DBLP:journals/cn/MahbubHG21,
	author = {Mobasshir Mahbub and
                  M. Mofazzal Hossain and
                  Md. Shamrat Apu Gazi},
	title = {Cloud-Enabled IoT-based embedded system and software for intelligent
                  indoor lighting, ventilation, early stage fire detection and prevention},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107673},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107673},
	doi = {10.1016/J.COMNET.2020.107673},
	timestamp = {Thu, 14 Oct 2021 09:09:00 +0200},
	biburl = {https://dblp.org/rec/journals/cn/MahbubHG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This software development is focused on the design of an intelligent lighting and ventilation system capable of sensing human presence to control the lighting and monitoring humidity, temperature, CO2, and smoke to ensure efficient ventilation and accidents caused by fire and smoke. This article develops an embedded system with ready-to-deploy software for intelligent lighting and ventilation with HTTP protocol based real-time monitoring through smartphones or PCs. Moreover, the designed system can log real-time sensor data into a cloud server through which the client can also monitor real-time data from anywhere in the world.}
}


@article{DBLP:journals/cn/QuincozesAPM21,
	author = {Silvio E. Quincozes and
                  C{\'{e}}lio Albuquerque and
                  Diego G. Passos and
                  Daniel Moss{\'{e}}},
	title = {A survey on intrusion detection and prevention systems in digital
                  substations},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107679},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107679},
	doi = {10.1016/J.COMNET.2020.107679},
	timestamp = {Tue, 23 Aug 2022 09:19:55 +0200},
	biburl = {https://dblp.org/rec/journals/cn/QuincozesAPM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Smart Grids integrate the traditional power grid with information processing and communication technologies. In particular, substation intelligent devices can now communicate with each other digitally to enable remote information gathering, monitoring, and control. There have been many efforts to promote global communication standards. The IEC–61850 international standard addresses substation communication networks and systems. Despite the many benefits, this standardized communication poses new cyber-security challenges. Also, traditional Intrusion Detection Systems (IDSs) may not be suitable for digital substations, given their critical components and stringent time requirements. We present an in-depth analysis of attacks exploiting IEC–61850 substations and recent research efforts for detecting and preventing them. Our main contribution is an original taxonomy comprising design and evaluation aspects for substation-specific IDSs. This taxonomy includes IDS’s architectures, detection approaches, analysis, actions, data sources, detection range, validation strategies, and metrics. Additionally, we present a compilation of the detection rules deployed by the state-of-art IDSs and assess their resiliency to five types of attacks. Our assessment reveals that some attacks are covered by currently-deployed IDSs, but, particularly, further advancement is necessary to deal with masquerade attacks. Finally, we discuss trends, open issues, and future research topics.}
}


@article{DBLP:journals/cn/ChaoL21,
	author = {Chih{-}Min Chao and
                  Wei{-}Che Lee},
	title = {Load-aware anti-jamming channel hopping design for cognitive radio
                  networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107681},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107681},
	doi = {10.1016/J.COMNET.2020.107681},
	timestamp = {Thu, 07 Jan 2021 09:00:36 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ChaoL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cognitive radio networks (CRNs), access to unused portions of the licensed spectrum by secondary users renders the system vulnerable to malicious attack. A number of channel hopping schemes with anti-jamming capability have been proposed; however, existing solutions require the pre-sharing of access information or cannot guarantee rendezvous success. Furthermore, existing channel hopping schemes with anti-jamming capability do not consider variations in the traffic loads of different nodes. In this study, we developed a novel anti-jamming scheme, which allows for the dynamic adjustment of the sending/receiving ratio to maximize the throughput of an SU node. The proposed Load Awareness Anti-jamming channel hopping scheme is based on extended Langford pairing. The proposed scheme guarantees rendezvous for any pair of nodes, and was shown in simulations to outperform Sec-CH and Tri-CH in terms of throughput and TTR.}
}


@article{DBLP:journals/cn/ZebHN21,
	author = {Jehan Zeb and
                  Aamir Hassan and
                  Muhammad Danish Nisar},
	title = {Joint power and spectrum allocation for {D2D} communication overlaying
                  cellular networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107683},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107683},
	doi = {10.1016/J.COMNET.2020.107683},
	timestamp = {Sun, 06 Oct 2024 21:22:05 +0200},
	biburl = {https://dblp.org/rec/journals/cn/ZebHN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device to Device communication (D2D) for the cellular networks enables direct communication between the mobile devices that are in close proximity of each other. However, the scarce cellular spectrum for D2D in-band overlay communication must be intelligently assigned in order to optimize the overall network spectral and energy efficiency. With the help of graph theory, we demonstrate that the joint power and spectrum allocation optimization problem has an exponential time complexity that increases with the number of D2D nodes and frequency resources. We then propose an algorithm that solves the aforementioned optimization problem in two phases. The first phase performs admission control for the nodes that meet the power feasibility requirement along with the resource block assignment based on frequency reuse strategies with the aim to minimize the required frequency resources, whereas, the next phase minimizes the required transmission power of the nodes scheduled on those resource blocks while meeting the Quality of Service (QoS) requirements. We also propose several dynamic frequency reuse strategies and highlight the trade-off between spectrum and energy efficiency within an outage constraint for a given network with different service requirements and applications. The proposed strategies outperform other state-of-the-art strategies effectiveness of which is evaluated using extensive network level simulations.}
}


@article{DBLP:journals/cn/ZengYTGQ21,
	author = {Yue Zeng and
                  Baoliu Ye and
                  Bin Tang and
                  Songtao Guo and
                  Zhihao Qu},
	title = {Scheduling coflows of multi-stage jobs under network resource constraints},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107686},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107686},
	doi = {10.1016/J.COMNET.2020.107686},
	timestamp = {Tue, 15 Nov 2022 14:44:33 +0100},
	biburl = {https://dblp.org/rec/journals/cn/ZengYTGQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging network abstraction, coflow greatly improves the communication performance of data-parallel computing jobs. Many existing studies have focused on the design of coflow scheduling to minimize the completion time of jobs. However, they treat the underlying network as a large non-blocking switch without considering the constraints of network resources, which may increase network bottlenecks, reduce link capacity utilization, and extend job completion time. In this paper, we take network resource constraints into account and study how to schedule coflows in multi-stage jobs with the objective of minimizing the total weighted job completion time. We first formalize this multi-stage job scheduling problem as nonlinear programming and prove its NP-hardness. By introducing a priority order of jobs using a linear programming relaxation-based approach, we propose a polynomial-time algorithm with a performance guarantee, which can achieve a constant approximation ratio in many typical scenarios. Simulation results based on a Facebook trace show that, compared with a state-of-the-art approach, our algorithm can shorten the average weighted job completion time by up to 48.46% and run faster by up to 19.12\n×\n.}
}


@article{DBLP:journals/cn/YazdinejadPDK21,
	author = {Abbas Yazdinejad and
                  Reza M. Parizi and
                  Ali Dehghantanha and
                  Mohammad S. Khan},
	title = {A kangaroo-based intrusion detection system on software-defined networks},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107688},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107688},
	doi = {10.1016/J.COMNET.2020.107688},
	timestamp = {Mon, 05 Feb 2024 20:24:11 +0100},
	biburl = {https://dblp.org/rec/journals/cn/YazdinejadPDK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, a new generation of architecture has emerged in the world of computer networks, known as software-defined networking (SDN), that aims to improve and remove the limitations of traditional networks. Although SDN provides viable benefits, it has faced many security threats and vulnerability-related issues. To solve security issues in the SDN, one of the most vital solutions is employing an intrusion detection system (IDS). Merging IDS into the SDN network remains efficient due to the unique features of SDN, such as high manageability, flexibility, and programmability. In this paper, we propose a new approach as a kangaroo-based intrusion detection system (KIDS), which is an SDN-based architecture for attack detection and malicious behaviors in the data plane. Designing a zone-based architecture in the KIDS assists us in achieving a distributed architecture which is scalable in both area and anomaly detection. In the KIDS architecture, the IDS module supplies the flow-based and packet-based intrusion detection components based on monitoring packet parser and Flow tables of the SDN switches. In the proposed approach, the IDS uses consecutive jumps like a kangaroo for announcing the attacks both to the SDN controller and other IDSs, contributing to improved scalability and efficiency. The evaluation of the proposed approach shows an enhanced performance against that of peer approaches in detecting malicious packets.}
}


@article{DBLP:journals/cn/PalumboABCPP21,
	author = {Fabio Palumbo and
                  Giuseppe Aceto and
                  Alessio Botta and
                  Domenico Ciuonzo and
                  Valerio Persico and
                  Antonio Pescap{\`{e}}},
	title = {Characterization and analysis of cloud-to-user latency: The case of
                  Azure and {AWS}},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107693},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107693},
	doi = {10.1016/J.COMNET.2020.107693},
	timestamp = {Fri, 09 Apr 2021 18:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PalumboABCPP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growing adoption of cloud infrastructures to deliver a variety of IT services, monitoring cloud network performance has become crucial. However, cloud providers only disclose qualitative information about network performance, at most. This hinders efficient cloud adoption, resulting in uncertainties about the behavior of hosted services, and sub-optimal deployment choices. In this work, we focus on cloud-to-user latency, i.e. the latency of network paths interconnecting datacenters to worldwide-spread cloud users accessing their services. Specifically, we performed a 14-day measurement campaign from 25 vantage points deployed via the Planetlab infrastructure (emulating spatially-spread users) and considering services running in distinct locations on the infrastructures of Amazon Web Services and Microsoft Azure. First, our experimentation allows us to provide an in-depth performance characterization (based on multiple probing methods and fine-grained sampling rate) of such networks as perceived by users spread worldwide, highlighting both spatial and temporal latency trends. Then, our analysis is exploited with design purposes to support both cloud customers and providers with the assessment of cloud-network performance (via badness detection & imputation tools) and the making of deployment decisions (via the evaluation of multi-cloud benefits). The dataset gathered from the campaign is publicly released to foster reproducibility.}
}


@article{DBLP:journals/cn/LiuFCLT21,
	author = {Xiuwen Liu and
                  Jianming Fu and
                  Yanjiao Chen and
                  Weichen Luo and
                  Zihan Tang},
	title = {Trust-Aware sensing Quality estimation for team Crowdsourcing in social
                  IoT},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107695},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107695},
	doi = {10.1016/J.COMNET.2020.107695},
	timestamp = {Mon, 28 Aug 2023 21:39:18 +0200},
	biburl = {https://dblp.org/rec/journals/cn/LiuFCLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Things (IoT), the mobile smart devices with powerful sensing capability help mobile crowdsourcing become an important paradigm to sense environment information. The social Internet of Things paradigm can be exploited for complex task crowdsourcing by forming a collaborative team of socially connected nodes (i.e., smart devices). Few existing team crowdsourcing studies have ever satisfied requirements of trustworthy sensing data and collaborative communication among team members.}
}


@article{DBLP:journals/cn/PushpalathaARV21,
	author = {Mullur Pushpalatha and
                  T. Anusha and
                  T. Rama Rao and
                  Revathi Venkataraman},
	title = {{L-RPL:} {RPL} powered by laplacian energy for stable path selection
                  during link failures in an Internet of Things network},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107697},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107697},
	doi = {10.1016/J.COMNET.2020.107697},
	timestamp = {Sun, 02 Oct 2022 15:31:02 +0200},
	biburl = {https://dblp.org/rec/journals/cn/PushpalathaARV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The routing protocol RPL for low power and lossy networks (LLNs) suffers from reduced reliability, increase in control overhead, longer latency and higher energy utilization on experiencing high node density, link or node failures in the network. This paper aims to elaborately study the behaviour of RPL’s expected transmissions count (ETX) based path selection for lengthier routes and the impact of node density and link or node failures on them. Our study finds that the path selection along long routes suffers due to the lack of decisive information on the neighbourhood links. As an alternate, a laplacian energy based path selection is proposed to enrich RPL by bringing in additional neighbourhood connection information into the path selection process to provide stable paths. Laplacian energy drop, a two-walk neighbourhood connectivity measure that is incorporated in L-RPL helps a node to pick the most resilient path. By simulation and testbed experiment results, it is shown that the proposed L-RPL improves path selection and reduces control overhead (29.05% and 39.68% reduction in (Destination Advertisement Object) control data when 9% of nodes fail and during normal conditions respectively), latency and also improves energy utilization.}
}


@article{DBLP:journals/cn/IbrarWMASM21,
	author = {Muhammad Ibrar and
                  Lei Wang and
                  Gabriel{-}Miro Muntean and
                  Aamir Akbar and
                  Nadir Shah and
                  Kaleem Razzaq Malik},
	title = {PrePass-Flow: {A} Machine Learning based technique to minimize {ACL}
                  policy violation due to links failure in hybrid {SDN}},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107706},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107706},
	doi = {10.1016/J.COMNET.2020.107706},
	timestamp = {Fri, 09 Apr 2021 18:25:55 +0200},
	biburl = {https://dblp.org/rec/journals/cn/IbrarWMASM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The centralized architecture of Software-Defined Networking (SDN) reduces networking complexity and improves network manageability by omitting the need for box-by-box troubleshooting and management. However, due to both budget constraints and maturity level of the SDN-capable devices, organizations often are reluctant to adopt SDN in practice. Therefore, instead of migrating to a pure SDN architecture, an incremental SDN deployment strategy is preferred in practice. In this paper, we consider an incremental SDN deployment strategy known as hybrid SDN - involving simultaneous use of both SDN switches and legacy switches. The links connected to an SDN switch are called SDN links, and the rest are called legacy links. An SDN controller can directly poll the status of the SDN links via the connected SDN switches. At the same time, the status of the legacy links passes through SDN switches and reaches the controller, causing delay. As a result, the controller does not have the current status of legacy links in real-time. This delay may lead to undesired outcomes. For example, it causes network reachability problems due to Access Control List (ACL) policies. Therefore, to minimize the impact of network-layer failure in hybrid SDN, we propose a Machine Learning (ML) based technique called PrePass-Flow. PrePass-Flow predicts link failures before their occurrence, recomputes the locations of ACL policies, and installs the ACL policies in the recomputed locations in a proactive manner. The main objective of PrePass-Flow is to minimize the ACL policy violations and network reachability problems due to ACL policies in case of link failures. For the link status prediction, PrePass-Flow uses two supervised ML-based models: 1) a Logistic Regression (LR) model, and 2) a Support Vector Machine (SVM) model. Testing results show that the LR model performs better than both the SVM model and an existing approach in terms of Packet Delivery Ratio (PDR) and ACL policy violations. For instance, the LR model’s accuracy is 4% better, precision is 5% higher, sensitivity is 10% better, and Area Under the Curve (AUC) is 6% greater than the SVM model’s corresponding results.}
}


@article{DBLP:journals/cn/Adil21,
	author = {Muhammad Adil},
	title = {Congestion free opportunistic multipath routing load balancing scheme
                  for Internet of Things (IoT)},
	journal = {Comput. Networks},
	volume = {184},
	pages = {107707},
	year = {2021},
	url = {https://doi.org/10.1016/j.comnet.2020.107707},
	doi = {10.1016/J.COMNET.2020.107707},
	timestamp = {Fri, 02 Dec 2022 18:12:18 +0100},
	biburl = {https://dblp.org/rec/journals/cn/Adil21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) interconnects billions of devices to form a heterogeneous network over the Internet. The heterogeneous communication infrastructure of IoT opens a door for the research community to design new protocols by utilizing minimal resources of these tiny devices to achieve accurate results. Therefore, the research community suggests various techniques to resolve the load balancing issue of IoT and prolong the network lifetime. Although, these techniques are effective at some stage in managing the load balancing issue, they have some side effects on network performance such as high communication costs, end-to-end (E2E) delay, packet lost ratio (PLR), throughput, and individual sensor devices lifespan, etc. Therefore, an efficient lightweight load balancing routing protocol is needed to be developed to address the aforementioned issues in IoT networks. In this paper, Dynamic hop selection static routing protocol (DHSSRP) is proposed to resolve the load balancing issue of IoT networks in congestion-free and priority-based communication infrastructure. The proposed DHSSRP routing protocol priorities the sensitive/critical information of sensors devices with static routing and divert the neighbor’s sensor communication with an alternate hop selection path, which manages the network traffic in a congestion-free environment. The traffic management of the DHSSRP routing protocol based on priority-based information balances the energy consumption with a balanced traffic environment, which maximizes the lifespan of the deployed IoT devices in the network. The results captured during simulation for the proposed scheme were compared with the field-proven scheme, which showed a significant improvement for the metrics, such as communication cost, computational cost, traffic congestion, throughput, PLR, and network lifetime. Moreover, the results observed for an individuals sensor devices to participate in the network until the end-stage showed a 95.8 % result with the same onboard battery power.}
}
