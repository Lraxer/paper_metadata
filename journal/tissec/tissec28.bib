@article{DBLP:journals/tissec/HaydenWB25,
	author = {Blake Hayden and
                  Timothy Walsh and
                  Armon Barton},
	title = {Defending Against Deep Learning-Based Traffic Fingerprinting Attacks
                  With Adversarial Examples},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {1:1--1:23},
	year = {2025},
	url = {https://doi.org/10.1145/3698591},
	doi = {10.1145/3698591},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/HaydenWB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an increasingly digital and interconnected world, online anonymity and privacy are paramount issues for Internet users. To address this, tools like The Onion Router (Tor) offer anonymous and private communication by routing traffic through multiple relays with multiple layers of encryption. However, traffic fingerprinting attacks have threatened anonymity and privacy. In response, the community has proposed additional defenses for Tor, but fingerprinting techniques that utilize deep neural networkss (DNNs) have undermined many of these defenses. The latest defenses that are both lightweight and robust against DNNs use adversarial examples, but these defenses require either the full traffic trace beforehand or a database of pre-computed adversarial examples. We propose  Prism , a defense against fingerprinting attacks that utilizes adversarial examples with neither prior access to the full traffic trace nor a database. We describe a novel method of adversarial example generation as input is learned over time.  Prism  injects these adversarial examples into the Tor traffic stream to prevent DNNs from accurately classifying both websites and videos that a user is viewing, even if the DNN is hardened by adversarial training. We also show that the Tor network could implement  Prism  entirely on relays under certain conditions, extending the defense to users who may run Tor on devices without graphics processing units.}
}


@article{DBLP:journals/tissec/DaiCDWSLRYL25,
	author = {Huan Dai and
                  Yuefeng Chen and
                  Yicong Du and
                  Luping Wang and
                  Ziyu Shao and
                  Hongbo Liu and
                  Yanzhi Ren and
                  Jiadi Yu and
                  Bo Liu},
	title = {ArmSpy++: Enhanced {PIN} Inference through Video-based Fine-grained
                  Arm Posture Analysis},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {2:1--2:26},
	year = {2025},
	url = {https://doi.org/10.1145/3696418},
	doi = {10.1145/3696418},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/DaiCDWSLRYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the most common ways for user authentication, Personal Identification Number (PIN), due to its simplicity and convenience, has suffered from plenty of side-channel attacks, which pose a severe threat to people’s privacy and property. The success of existing attacks is usually built upon the premise of no occlusion between the attacker and the victim’s hand gesture, but it increases the difficulty of launching the attack and the possibility of exposure. To overcome such limitation, we propose ArmSpy++, an improved video-assisted PIN inference attack built upon our previous research, ArmSpy. Specifically, ArmSpy++ employs new modules to leverage more features like the keystroke-induced elbow bending, wrist speed variation, and the spatial relationship between different arm joints, to correctly detect Keystrokes. ArmSpy++ delves into the perspective relationship and natural typing habits to ensure a high success rate of PIN inference. We also re-designed the inferred PIN pattern coordination mechanism to accurately deduce the PINs. By using a pre-trained HigherHRNet model for posture estimation ArmSpy++ eliminates the necessity of additional training. The extensive experiments demonstrate that ArmSpy++ can achieve over 83.1% average accuracy with 3 attempts and even 92.5% for some victims, indicating the severity of the threat posed by ArmSpy++.}
}


@article{DBLP:journals/tissec/BertolissiFT25,
	author = {Clara Bertolissi and
                  Maribel Fern{\'{a}}ndez and
                  Bhavani Thuraisingham},
	title = {Category-Based Administrative Access Control Policies},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {3:1--3:35},
	year = {2025},
	url = {https://doi.org/10.1145/3698199},
	doi = {10.1145/3698199},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/BertolissiFT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As systems evolve, security administrators need to review and update access control policies. Such updates must be carefully controlled due to the risks associated with erroneous or malicious policy changes. We propose a category-based access control (CBAC) model, called  Admin-CBAC , to control administrative actions. Since most of the access control models in use nowadays (including the popular RBAC and ABAC models) are instances of CBAC, from  Admin-CBAC , we derive administrative models for RBAC and ABAC, too. We present a graph-based representation of  Admin-CBAC  policies and a formal operational semantics for administrative actions via graph rewriting. We also discuss implementations of  Admin-CBAC  exploiting the graph-based representation. Using the formal semantics, we show how properties (such as safety, liveness, and effectiveness of policies) and constraints (such as separation of duties) can be checked, and discuss the impact of policy changes. Although the most interesting properties of policies are generally undecidable in dynamic access control models, we identify particular cases where reachability properties are decidable and can be checked using our operational semantics, generalising previous results for RBAC and  ABAC α .}
}


@article{DBLP:journals/tissec/HosseyniKW25,
	author = {Pedram Hosseyni and
                  Ralf K{\"{u}}sters and
                  Tim W{\"{u}}rtele},
	title = {Formal Security Analysis of the OpenID {FAPI} 2.0 Family of Protocols:
                  Accompanying a Standardization Process},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {4:1--4:36},
	year = {2025},
	url = {https://doi.org/10.1145/3699716},
	doi = {10.1145/3699716},
	timestamp = {Fri, 07 Mar 2025 18:31:23 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/HosseyniKW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {FAPI 2.0 is a suite of Web protocols developed by the OpenID Foundation’s FAPI Working Group (FAPI WG) for third-party data sharing and digital identity in high-risk environments. Even though the specifications are not completely finished, several important entities have started to adopt the FAPI 2.0 protocols, including Norway’s national HelseID, Australia’s Consumer Data Standards, as well as private companies like Authlete and Australia-based connectID; the predecessor FAPI 1.0 is in widespread use with millions of users. The FAPI WG asked us to accompany the standardization of the FAPI 2.0 protocols with a formal security analysis to proactively identify vulnerabilities before widespread deployment and to provide formal security guarantees for the standards. In this paper, we report on our analysis and findings. Our analysis is based on a detailed model of the Web infrastructure, the so-called Web Infrastructure Model (WIM), which we extend to be able to carry out our analysis of the FAPI 2.0 protocols including important extensions like FAPI-CIBA. Based on the (extended) WIM and formalizations of the security goals and attacker model laid out in the FAPI 2.0 specifications, we provide a formal model of the protocols and carry out a formal security analysis, revealing several attacks. We have worked with the FAPI WG to fix the protocols, resulting in several amendments to the specifications. With these changes in place, we have adjusted our protocol model and formally proved that the security properties hold true under the strong attacker model defined by the FAPI WG.}
}


@article{DBLP:journals/tissec/CramptonEGKM25,
	author = {Jason Crampton and
                  Eduard Eiben and
                  Gregory Z. Gutin and
                  Daniel Karapetyan and
                  Diptapriyo Majumdar},
	title = {Bi-objective Optimization in Role Mining},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {5:1--5:22},
	year = {2025},
	url = {https://doi.org/10.1145/3697833},
	doi = {10.1145/3697833},
	timestamp = {Fri, 07 Mar 2025 18:31:23 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/CramptonEGKM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Role mining is a technique that is used to derive a role-based authorization policy from an existing policy. Given a set of users  U , a set of permissions  P , and a user–permission authorization relation  UPA ⊆ U × P , a role mining algorithm seeks to compute a set of roles  R , a user–role authorization relation  UA ⊆ U × R , and a permission–role authorization relation  PA ⊆ R × P , such that the composition of  UA  and  PA  is close (in some appropriate sense) to  UPA . Role mining is therefore a core problem in the specification of role-based authorization policies. Role mining is known to be hard in general and exact solutions are often impossible to obtain, so there exists an extensive literature on variants of the role mining problem that seek to find approximate solutions and algorithms that use heuristics to find reasonable solutions efficiently. In this article, we first introduce the  Generalized Noise Role Mining  problem (GNRM)—a generalization of the  MinNoise Role Mining  problem—which we believe has considerable practical relevance. In particular, GNRM can produce “security-aware” or “availability-aware” solutions. Extending the work of Fomin et\xa0al., we show that GNRM is fixed parameter tractable, with parameter  r + k , where  r  is the number of roles in the solution and  k  is the number of discrepancies between  UPA  and the relation defined by the composition of  UA  and  PA . We further introduce a bi-objective optimization variant of GNRM, where we wish to minimize both  r  and  k  subject to upper bounds  r ≤ r ¯  and  k ≤ k ¯ , where  r ¯  and  k ¯  are constants. We show that the Pareto front of this bi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter tractable time with parameter  r ¯ + k ¯ . From a practical perspective, a solution to BO-GNRM gives security managers the opportunity to identify a mined policy offering the best tradeoff between the number of policy discrepancies and the number of roles. We then report the results of our experimental work using the integer programming solver Gurobi to solve instances of BO-GNRM. Our key findings are that (a)\xa0we obtained strong support that Gurobi’s performance is fixed-parameter tractable, and (b)\xa0our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies. We observed that, in many cases, our solver is capable of obtaining optimal solutions when the values of either  k  or  r  are small.}
}


@article{DBLP:journals/tissec/BjurlingR25,
	author = {Bj{\"{o}}rn Bjurling and
                  Shahid Raza},
	title = {Cyber Threat Intelligence meets the Analytic Tradecraft},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {6:1--6:37},
	year = {2025},
	url = {https://doi.org/10.1145/3701299},
	doi = {10.1145/3701299},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/BjurlingR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The volumes and sophistication of cyber threats in today’s cyber threat landscape have risen to levels where automated quantitative tools for Cyber Threat Intelligence (CTI) have become an indispensable part in the cyber defense arsenals. The AI and cyber security research communities are producing novel automated tools for CTI that quickly find their ways into commercial products. However, the quality of such automated intelligence products is being questioned by the intelligence community. Cyber security operators are forced to complement the automated tools with costly and time-consuming human intelligence analysis in order to improve the quality of the end product. For improving the quality, it has been suggested that researchers should incorporate methods from traditional intelligence analysis into the quantitative algorithms. This article presents a novel approach to cyber intelligence analysis called AMBARGO, which takes the inherent ambiguity of evidence into account in the analysis, using the Choquet integral, in formalizing the re-evaluation of evidence and hypotheses made by human analysts. The development of AMBARGO revolves around a cyber attribution use case, one of the hardest problems in CTI. The results of our evaluating experiments show that the robustness of AMBARGO outperforms state-of-the-art quantitative approaches to CTI in the presence of ambiguous evidence and potentially deceptive threat actor tactics. AMBARGO has thus the potential to fill a gap in the CTI state-of-the-art, which currently handles ambiguity poorly. The findings are also confirmed in a large-scale realistic experimental setting based on data from an APT campaign obtained from the MITRE ATT&CK Framework.}
}


@article{DBLP:journals/tissec/ZhengCLCWWGJZ25,
	author = {Haibin Zheng and
                  Jinyin Chen and
                  Tao Liu and
                  Yao Cheng and
                  Zhao Wang and
                  Yun Wang and
                  Lan Gao and
                  Shouling Ji and
                  Xuhong Zhang},
	title = {DP-Poison: Poisoning Federated Learning under the Cover of Differential
                  Privacy},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {7:1--7:28},
	year = {2025},
	url = {https://doi.org/10.1145/3702325},
	doi = {10.1145/3702325},
	timestamp = {Mon, 19 Jan 2026 13:12:58 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/ZhengCLCWWGJZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables resource-constrained node devices to learn a shared model while keeping the training data local. Since recent research has demonstrated multiple privacy leakage attacks in FL, e.g., gradient inference attacks and membership inference attacks, differential privacy (DP) is applied to serve as one of the most effective privacy protection mechanisms. Despite the benefit DP brings, we observe that the introduction of DP also brings random changes to client updates, which will affect the robust aggregation algorithms. We reveal a novel poisoning attack under the cover of DP, named the  DP-Poison  attack in FL. Specifically, the DP-Poison attack is designed to achieve four goals: (1) maintaining the main task performance; (2) launching a successful attack; (3) escaping the robust aggregation algorithms in FL; and (4)\xa0keeping the effectiveness of DP privacy protection. To achieve these goals, we design multiple optimization goals to generate DP noise through a genetic algorithm. The optimization ensures that while the benign updates change randomly, the malicious updates can change toward the global model after adding the DP noise, so that it is easier to be accepted by the robust aggregation algorithms. Extensive experiments show that DP-Poison achieves a nearly 100% attack success rate while maintaining the proposed four goals.}
}


@article{DBLP:journals/tissec/SyrosYBNO25,
	author = {Georgios Syros and
                  G{\"{o}}kberk Yar and
                  Simona Boboila and
                  Cristina Nita{-}Rotaru and
                  Alina Oprea},
	title = {Backdoor Attacks in Peer-to-Peer Federated Learning},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {8:1--8:28},
	year = {2025},
	url = {https://doi.org/10.1145/3691633},
	doi = {10.1145/3691633},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/SyrosYBNO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most machine learning applications rely on centralized learning processes, opening up the risk of exposure of their training datasets. While federated learning (FL) mitigates to some extent these privacy risks, it relies on a trusted aggregation server for training a shared global model. Recently, new distributed learning architectures based on Peer-to-Peer Federated Learning (P2PFL) offer advantages in terms of both privacy and reliability. Still, their resilience to poisoning attacks during training has not been investigated. In this article, we propose new backdoor attacks for P2PFL that leverage structural graph properties to select the malicious nodes, and achieve high attack success, while remaining stealthy. We evaluate our attacks under various realistic conditions, including multiple graph topologies, limited adversarial visibility of the network, and clients with non-IID data. Finally, we show the limitations of existing defenses adapted from FL and design a new defense that successfully mitigates the backdoor attacks, without an impact on model accuracy.}
}


@article{DBLP:journals/tissec/BaoZYYL25,
	author = {Yubing Bao and
                  Jianping Zeng and
                  Jirui Yang and
                  Ruining Yang and
                  Zhihui Lu},
	title = {The Effect of Domain Terms on Password Security},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {9:1--9:29},
	year = {2025},
	url = {https://doi.org/10.1145/3703350},
	doi = {10.1145/3703350},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/BaoZYYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The predominant authentication method still relies on usernames and passwords. To enhance memorability, domain terms may have been opted to include as part of passwords. However, there is little analysis of the extent to which such practice affects password security, so there is a lack of guidance on how users use domain terms on websites with different domain characteristics. To address the problem, we propose a novel approach to analyze the security effect of using domain terms in passwords. The methodology primarily consists of three stages. First, we utilize Web crawlers to harvest domain vocabularies, subsequently leveraging the TextRank algorithm to rank their importance. Second, we propose an algorithm for constructing a simulated domain-specific password dataset by replacing password elements with domain terms. Third, password guessing experiments are done on the dataset using PCFG (Probabilistic Context-Free Grammar) and the Markov model to evaluate the impact of domain terms on password security. The experimental results indicate that, for systems without clear domain, 20% domain terms replacement in the test set can reduce the cracking rate by up to 5.45%. In contrast, for domain-specific systems, 20% domain terms replacement in the training set can increase the cracking rate by 6.45%. These findings provide practical guidance on the application of domain knowledge in password creation for different types of systems. In summary, this study offers a novel perspective for exploring the security implications of passwords influenced by specific domains.}
}


@article{DBLP:journals/tissec/YuanZCWLML25,
	author = {Xuejing Yuan and
                  Jiangshan Zhang and
                  Kai Chen and
                  Cheng'an Wei and
                  Ruiyuan Li and
                  Zhenkun Ma and
                  Xinqi Ling},
	title = {Adversarial Attack and Defense for Commercial Black-box Chinese-English
                  Speech Recognition Systems},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {10:1--10:27},
	year = {2025},
	url = {https://doi.org/10.1145/3701725},
	doi = {10.1145/3701725},
	timestamp = {Sun, 02 Nov 2025 21:29:35 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/YuanZCWLML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The attacker can generate adversarial examples (AEs) to stealthily mislead automatic speech recognition (ASR) models, raising significant concerns about the security of intelligent voice control (IVC) devices. Existing adversarial attacks mainly generate AEs to mislead ASR models to output specific target English commands (e.g., open the door). However, it remains unknown whether AEs can be used to issue commands in other languages to attack commercial black-box ASR models. In this article, taking Chinese phrases (e.g., 支付宝付款) and “Chinese–English code-switching” phrases (e.g., 关闭GPS) as the target commands, we propose adversarial attacks for commercial multilingual ASR models. In particular, if a multilingual speech recognition model can recognize Chinese and English, we call it a Chinese–English speech recognition model. In English, the meaning of “支付宝付款” and “关闭GPS” are “Alipay payment” and “turn off GPS”, respectively. In detail, we generate transferable AEs based on the open-sourced conventional DataTang Mandarin ASR model. Given 55 target commands, the success rate for generating AEs of them is up to 96% and 80% for Aliyun ASR API and Tencentyun ASR API, respectively. Our AEs can trigger actual attack actions on voice assistants (e.g., Apple Siri, Xiaomi Xiaoaitongxue) or spread malicious messages through ASR API services, while the target commands in the AEs are inaudible to human beings. 1  Finally, by analyzing the spectrum differences between benign audio clips and AEs, we propose a general defense against adversarial audio attacks.}
}


@article{DBLP:journals/tissec/LeEMS25,
	author = {Hieu Le and
                  Salma Elmalaki and
                  Athina Markopoulou and
                  Zubair Shafiq},
	title = {AutoFR: Automated Filter Rule Generation for Adblocking},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {11:1--11:36},
	year = {2025},
	url = {https://doi.org/10.1145/3703836},
	doi = {10.1145/3703836},
	timestamp = {Mon, 03 Mar 2025 22:25:15 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/LeEMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adblocking relies on filter lists, which are manually curated and maintained by a community of filter list authors. Filter list curation is a laborious process that does not scale well to a large number of sites or over time. In this article, we introduce AutoFR, a reinforcement learning framework to fully automate the process of filter rule creation and evaluation for sites of interest. We design an algorithm based on multi-arm bandits to generate filter rules that block ads while controlling the trade-off between blocking ads and avoiding visual breakage. We test AutoFR on thousands of sites and show that it is efficient: It takes only a few minutes to generate filter rules for a site of interest. AutoFR is effective: It optimizes filter rules for a particular site that can block 86% of the ads, as compared to 87% by EasyList, while achieving comparable visual breakage. Using AutoFR as a building block, we devise three methodologies that generate filter rules across sites based on: (1) a modified version of AutoFR, (2) rule popularity, and (3) site similarity. We conduct an in-depth comparative analysis of these approaches by considering their effectiveness, efficiency, and maintainability. We demonstrate that some of them can generalize well to new sites in both controlled and live settings. We envision that AutoFR can assist the adblocking community in automatically generating and updating filter rules at scale.}
}


@article{DBLP:journals/tissec/BaccariniBZ25,
	author = {Alessandro N. Baccarini and
                  Marina Blanton and
                  Shaofeng Zou},
	title = {Understanding Information Disclosure from Secure Computation Output:
                  {A} Comprehensive Study of Average Salary Computation},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {1},
	pages = {12:1--12:36},
	year = {2025},
	url = {https://doi.org/10.1145/3705004},
	doi = {10.1145/3705004},
	timestamp = {Fri, 07 Mar 2025 18:31:23 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/BaccariniBZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure multi-party computation has seen substantial performance improvements in recent years and is being increasingly used in commercial products. While a significant amount of work was dedicated to improving its efficiency under standard security models, the threat models do not account for information leakage from the output of secure function evaluation. Quantifying information disclosure about private inputs from observing the function outcome is the subject of this work. Motivated by the City of Boston gender pay gap studies, in this work, we focus on the computation of the average of salaries and quantify information disclosure about private inputs of one or more participants (the target) to an adversary via information-theoretic techniques. We study a number of distributions including log-normal, which is typically used for modeling salaries. We consequently evaluate information disclosure after repeated evaluation of the average function on overlapping inputs, as was done in the Boston gender pay study that ran multiple times, and provide recommendations for using the sum and average functions in secure computation applications. Our goal is to develop mechanisms that lower information disclosure about participants’ inputs to a desired level and provide guidelines for setting up real-world secure evaluation of this function.}
}


@article{DBLP:journals/tissec/RuggiaPDMAB25,
	author = {Antonio Ruggia and
                  Andrea Possemato and
                  Savino Dambra and
                  Alessio Merlo and
                  Simone Aonzo and
                  Davide Balzarotti},
	title = {The Dark Side of Native Code on Android},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {13:1--13:33},
	year = {2025},
	url = {https://doi.org/10.1145/3712308},
	doi = {10.1145/3712308},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/RuggiaPDMAB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {From a little research experiment to an essential component of military arsenals, malicious software has constantly been growing and evolving for more than three decades. On the other hand, from a negligible market share, the Android operating system is nowadays the most widely used mobile operating system, becoming a desirable target for large-scale malware distribution. While scientific literature has followed this trend, one aspect has been understudied: the role of native code in malicious Android apps. Android apps are written in high-level languages, but thanks to the Java Native Interface (JNI), Android also supports calling native (C/C++) library functions. While allowing native code in Android apps has a strong positive impact from a performance perspective, it dramatically complicates its analysis because bytecode and native code need different abstractions and analysis algorithms, and they thus pose different challenges and limitations. Consequently, these difficulties are often (ab)used to hide malicious payloads. In this work, we propose a novel methodology to reverse engineering Android apps focusing on  suspicious  patterns related to native components, i.e., surreptitious code that requires further inspection. We implemented a static analysis tool based on such methodology, which can bridge the “Java” and the native worlds and perform an in-depth analysis of  tag  code blocks responsible for suspicious behavior. These tags benefit the human facing the reverse engineering task: they clearly indicate which part of the code to focus on to find malicious code. Then, we performed a longitudinal analysis of Android malware over the past 10 years and compared the recent malicious samples with actual top apps on the Google Play Store. Our work depicts typical behaviors of modern malware, its evolution, and how it abuses the native layer to complicate the analysis, especially with dynamic code loading and novel anti-analysis techniques. Finally, we show a use case for our suspicious tags: we trained and tested a machine learning algorithm for a binary classification task. Even if suspicious does not imply malicious, our classifier obtained a remarkable F1-score of 0.97, showing that our methodology can be helpful to both humans and machines.}
}


@article{DBLP:journals/tissec/ChenLTY25,
	author = {Long Chen and
                  Ya{-}Nan Li and
                  Qiang Tang and
                  Moti Yung},
	title = {End-to-Same-End Encryption: Modularly Augmenting an App with an Efficient,
                  Portable, and Blind Cloud Storage},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {14:1--14:35},
	year = {2025},
	url = {https://doi.org/10.1145/3707460},
	doi = {10.1145/3707460},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/ChenLTY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cloud has become pervasive, and we ask: how can we protect cloud data against the cloud itself? For secure user-to-user communication via a cloud server, End-to-End encryption has been formally studied, building on existing TLS channels without requiring new primitives. However, enabling user-to-same-user secure outsourced data storage–solving the analogous problem of “privacy from the server” while (1) relying on existing infrastructure and (2) supporting user mobility, remains open. Existing proposals, like password-protected secret sharing, target the same goal but are incompatible with existing cloud storage services. Specifically, they lack the simplicity needed to directly utilize existing cloud  storage  without requiring changes on the cloud side. Here, we propose a novel system for securely storing private data in existing cloud storage with the help of a key server (necessary, given the requirements). In our system, user data is secure against threats from the cloud server, the key server, and illegitimate users. Only the legitimate user can access the data on any device using a correct passphrase. Most importantly, our system does not require the storage server to support any newly programmable operations. Moreover, leveraging the existing App login, our system requires only one passphrase, which never leaves the user’s device and remains hidden from both servers. The security is proved under formal models, and its efficiency is demonstrated by experiments conducted on Amazon S3. Notably, a preliminary variant, based on our principles, was deployed by Snapchat in their  My Eyes Only  module, serving hundreds of millions of users!}
}


@article{DBLP:journals/tissec/HoreGPSDB25,
	author = {Soumyadeep Hore and
                  Jalal Ghadermazi and
                  Diwas Paudel and
                  Ankit Shah and
                  Tapas K. Das and
                  Nathaniel D. Bastian},
	title = {Deep PackGen: {A} Deep Reinforcement Learning Framework for Adversarial
                  Network Packet Generation},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {15:1--15:33},
	year = {2025},
	url = {https://doi.org/10.1145/3712307},
	doi = {10.1145/3712307},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/HoreGPSDB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in artificial intelligence (AI) and machine learning (ML) algorithms, coupled with the availability of faster computing infrastructure, have enhanced the security posture of cybersecurity operations centers (defenders) through the development of ML-aided network intrusion detection systems (NIDS). Concurrently, the abilities of adversaries to evade security have also increased with the support of AI/ML models. Therefore, defenders need to proactively prepare for evasion attacks that exploit the detection mechanisms of NIDS. Recent studies have found that the perturbation of flow-based and packet-based features can deceive ML models, but these approaches have limitations. Perturbations made to the flow-based features are difficult to reverse-engineer, while samples generated with perturbations to the packet-based features are not playable. Our methodological framework, Deep PackGen, employs deep reinforcement learning to generate adversarial packets and aims to overcome the limitations of approaches in the literature. By taking raw malicious network packets as inputs and systematically making perturbations on them, Deep PackGen camouflages them as benign packets while still maintaining their functionality. In our experiments, using publicly available data, Deep PackGen achieved an average adversarial success rate of 66.4% against various ML models and across different attack types. Our investigation also revealed that more than 45% of the successful adversarial samples were out-of-distribution packets that evaded the decision boundaries of the classifiers. The knowledge gained from our study on the adversary’s ability to make specific evasive perturbations to different types of malicious packets can help defenders enhance the robustness of their NIDS against evolving adversarial attacks.}
}


@article{DBLP:journals/tissec/MorkondaCO25,
	author = {Srivathsan G. Morkonda and
                  Sonia Chiasson and
                  Paul C. van Oorschot},
	title = {"Sign in with ... \emph{Privacy}": Timely Disclosure of
                  Privacy Differences among Web {SSO} Login Options},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {16:1--16:28},
	year = {2025},
	url = {https://doi.org/10.1145/3711898},
	doi = {10.1145/3711898},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/MorkondaCO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The number of login options on websites has increased since the introduction of web single sign-on (SSO) protocols. Web SSO services allow users to grant websites or  relying parties  (RPs) access to their personal profile information from  identity provider  (IdP) accounts. Many RP sites fail to provide sufficient privacy-related information to allow users to make informed login decisions. Moreover, privacy differences in permission requests across login options are largely hidden from users and are time-consuming to manually extract and compare. In this article, we present an empirical analysis of popular RP implementations supporting three major IdP login options (Facebook, Google, and Apple) and categorize RPs in the top 500 sites into four client-side code patterns. Informed by these RP patterns, we design and implement SSOPrivateEye (SPEye), a browser extension prototype that extracts and displays to users permission request information from SSO login options in RPs covering the three IdPs.}
}


@article{DBLP:journals/tissec/BostaniZLM25,
	author = {Hamid Bostani and
                  Zhengyu Zhao and
                  Zhuoran Liu and
                  Veelasha Moonsamy},
	title = {Level Up with {ML} Vulnerability Identification: Leveraging Domain
                  Constraints in Feature Space for Robust Android Malware Detection},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {17:1--17:32},
	year = {2025},
	url = {https://doi.org/10.1145/3711899},
	doi = {10.1145/3711899},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/BostaniZLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine Learning (ML) promises to enhance the efficacy of Android Malware Detection (AMD); however, ML models are vulnerable to realistic evasion attacks—crafting realizable Adversarial Examples (AEs) that satisfy Android malware domain constraints. To eliminate ML vulnerabilities, defenders aim to identify susceptible regions in the feature space where ML models are prone to deception. The primary approach to identifying vulnerable regions involves investigating realizable AEs, but generating these feasible apps poses a challenge. For instance, previous work has relied on generating either feature-space norm-bounded AEs or problem-space realizable AEs in adversarial hardening. The former is efficient but lacks full coverage of vulnerable regions, whereas the latter can uncover these regions by satisfying domain constraints but is known to be time consuming. To address these limitations, we propose an approach to facilitate the identification of vulnerable regions. Specifically, we introduce a new interpretation of Android domain constraints in the feature space, followed by a novel technique that learns them. Our empirical evaluations across various evasion attacks indicate effective detection of AEs using learned domain constraints, with an average of 89.6%. Furthermore, extensive experiments on different Android malware detectors demonstrate that utilizing our learned domain constraints in adversarial training outperforms other adversarial training based defenses that rely on norm-bounded AEs or state-of-the-art non-uniform perturbations. Finally, we show that retraining a malware detector with a wide variety of feature-space realizable AEs results in a 77.9% robustness improvement against realizable AEs generated by unknown problem-space transformations, with up to 70× faster training than using problem-space realizable AEs.}
}


@article{DBLP:journals/tissec/WangWG25,
	author = {Liang Wang and
                  Zhuangkun Wei and
                  Weisi Guo},
	title = {Multi-agent Deep Reinforcement Learning-based Key Generation for Graph
                  Layer Security},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {18:1--18:18},
	year = {2025},
	url = {https://doi.org/10.1145/3711900},
	doi = {10.1145/3711900},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/WangWG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the emergence of Internet of Things (IoT) devices has posed a challenge for securing information and avoiding attacks. Most of the cryptography solutions are based on physical layer security (PLS), whose idea is to fully exploit the properties of wireless channel state information (CSI) for generating symmetric keys between two communication nodes. However, accurate channel estimation is vulnerable for attackers and relies on powerful signal processing capability, which is not suitable for low-power IoT devices. In this article, we expect to apply graph layer security (GLS) to exploit the common features of physical dynamics detected by IoT sensors placed in networked systems to generate keys for data encryption and decryption, which we believe is a new frontier to security for both industry and academic research. We propose a distributed key generation algorithm based on multi-agent deep reinforcement learning (MADRL) approach, which enables communication nodes to cooperatively generate symmetric keys based on their locally detected physical dynamics (e.g., water/gas/oil/electrical pressure/flow/voltage) with low computational complexity and without information exchange. In order to demonstrate the feasibility, we conduct and evaluate our key generation algorithm in both a simulated and real water distribution network. The experimental results show that the proposed algorithm has considerable performance in terms of randomness, bit agreement rate (BAR), and so on.}
}


@article{DBLP:journals/tissec/DongWLLTHG25,
	author = {Yingkai Dong and
                  Li Wang and
                  Zheng Li and
                  Hao Li and
                  Peng Tang and
                  Chengyu Hu and
                  Shanqing Guo},
	title = {Safe Driving Adversarial Trajectory Can Mislead: Toward More Stealthy
                  Adversarial Attack Against Autonomous Driving Prediction Module},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {19:1--19:28},
	year = {2025},
	url = {https://doi.org/10.1145/3705611},
	doi = {10.1145/3705611},
	timestamp = {Thu, 15 Jan 2026 07:56:55 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/DongWLLTHG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prediction module, powered by deep learning models, constitutes a fundamental component of high-level Autonomous Vehicles (AVs). Given the direct influence of the module’s prediction accuracy on AV driving behavior, ensuring its security is paramount. However, limited studies have explored the adversarial robustness of the prediction modules. Furthermore, existing methods still generate adversarial trajectories that deviate significantly from human driving behavior. These deviations can be easily identified as hazardous by AVs’ anomaly detection models and thus cannot effectively evaluate and reflect the robustness of the prediction modules. To bridge this gap, we propose a stealthy and more effective optimization-based attack method. Specifically, we reformulate the optimization problem using Lagrangian relaxation and design a Frenet-based objective function along with a distinct constraint space. We conduct extensive evaluations on 2 popular prediction models and 2 benchmark datasets. Our results show that our attack is highly effective, with over 87% attack success rates, outperforming all baseline attacks. Moreover, our attack method significantly improves the stealthiness of adversarial trajectories while guaranteeing adherence to physical constraints. Our attack is also found robust to noise from upstream modules, transferable across trajectory prediction models, and high realizability. Lastly, to verify its effectiveness in real-world applications, we conduct further simulation evaluations using a production-grade simulator. These simulations reveal that the adversarial trajectory we created could convincingly induce autonomous vehicles (AVs) to initiate hard braking.}
}


@article{DBLP:journals/tissec/BollegalaOMK25,
	author = {Danushka Bollegala and
                  Shuichi Otake and
                  Tomoya Machide and
                  Ken{-}ichi Kawarabayashi},
	title = {A Metric Differential Privacy Mechanism for Sentence Embeddings},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {20:1--20:34},
	year = {2025},
	url = {https://doi.org/10.1145/3708321},
	doi = {10.1145/3708321},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/BollegalaOMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sentence embeddings represent the meaning of a given sentence using a fixed dimensional vector. Different approaches have been proposed in the Natural Language Processing (NLP) community for learning encoders that can produce accurate sentence embeddings that perform well for diverse downstream tasks requiring sentence representations. Despite prior work focusing mainly on creating accurate sentence embeddings, how to keep private the sensitive information contained in the sentences remains an unexplored research problem. In this article, we propose Covering Metric Analytic Gaussian (CMAG), a  covering  metric Differential Privacy (DP) mechanism for sentence embeddings such that minimal random noise is added to a set of sentence embeddings produced by an encoder to protect the private information expressed in those sentences. Given a sentence embedding  s , CMAG considers the Mahalanobis distance between  s  and the other sentence embeddings  s ’ in the local neighbourhood of  s  to determine the minimal amount of random noise that must be added to  s  to obtain provable metric DP guarantees. Experimental results show that the proposed DP mechanism protects private information better than previously proposed DP mechanisms while reporting good performance in a broad range of downstream NLP tasks.}
}


@article{DBLP:journals/tissec/FangZS25,
	author = {Shuaijv Fang and
                  Zhiyong Zhang and
                  Bin Song},
	title = {Deepfake Detection Model Combining Texture Differences and Frequency
                  Domain Information},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {21:1--21:16},
	year = {2025},
	url = {https://doi.org/10.1145/3706636},
	doi = {10.1145/3706636},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/FangZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, public security incidents caused by deepfake technology have occurred frequently around the world, which makes an efficient and accurate deepfake detection model crucial. The existing advanced methods use the manipulation features in the image to realize the binary classification of real and fake images by training complex neural network models. However, these models rely on a single manipulation feature, and the detection accuracy of these methods will be greatly reduced when the forgery technology or image quality of the training dataset and the validation dataset are different. Inspired by the existing work, we propose a two-stream collaborative learning framework that combines spatial texture differences and frequency information. The average difference convolution (ADC) is designed to extract the spatial texture difference information of the image, and the gray image frequency-aware decomposition (GFAD) is used to extract the artifact information of the image in the frequency domain. At the same time, the ViT idea is combined with cross-attention mechanism for feature fusion to comprehensively mine forged features in forged images. Experimental results show that the proposed model has good detection effects on three benchmark datasets. In terms of cross-dataset evaluation, the AUC on Celeb-DF dataset reaches 82.86%, which is better than the existing advanced methods.}
}


@article{DBLP:journals/tissec/NeeraCAI25,
	author = {Jeyamohan Neera and
                  Xiaomin Chen and
                  Nauman Aslam and
                  Biju Issac},
	title = {A Trustworthy and Untraceable Centralised Payment Protocol for Mobile
                  Payment},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {22:1--22:29},
	year = {2025},
	url = {https://doi.org/10.1145/3706421},
	doi = {10.1145/3706421},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/NeeraCAI25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Current mobile payment schemes gather detailed information about purchases customers make. This data can then be used to infer a customer’s spending behaviour, potentially violating their privacy. To tackle this problem, we propose an untraceable mobile payment scheme that strikes a better balance, preserving user privacy while allowing the Third-Party Service Provider (TPSP) to collect necessary information such as card details and transaction amount for regulatory compliance. Our scheme offers untraceability for legitimate users from malicious adversaries and curious TPSPs using cryptographic primitives such as partially blind signatures, zero-knowledge proofs, and identity-based signatures. It also guarantees that only authorised TPSPs can issue valid payment tokens, and even with limited data, the TPSP can still prevent dishonest customers/merchants from double-spending a payment token. We also propose a comprehensive evaluation framework to assess the untraceable payment schemes against seven key criteria such as untraceability, exculpability—merchant double-spending, exculpability—customer double-spending, unforgeability, confidentiality, message authenticity, efficiency, and regulatory compliance. We rigorously benchmark the security and privacy of our proposed payment scheme against this framework and other established schemes. Furthermore, we formally verify these properties using complexity-based analysis and Proverif modelling.}
}


@article{DBLP:journals/tissec/KatsisB25,
	author = {Charalampos Katsis and
                  Elisa Bertino},
	title = {{ZT-SDN:} An ML-Powered Zero-Trust Architecture for Software-Defined
                  Networks},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {23:1--23:35},
	year = {2025},
	url = {https://doi.org/10.1145/3712262},
	doi = {10.1145/3712262},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/KatsisB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Zero Trust (ZT) is a security paradigm aiming to curtail an attacker’s lateral movements within a network by implementing least-privilege and per-request access control policies. However, its widespread adoption is hindered by the difficulty of generating proper rules owing to the lack of detailed knowledge of communication requirements and the characteristic behaviors of communicating entities under benign conditions. Consequently, manual rule generation becomes cumbersome and error prone. To address these problems, we propose  ZT-SDN , an automated framework for learning and enforcing network access control in Software-Defined Networks (SDNs). ZT-SDN collects data from the underlying network and models the network “transactions” performed by communicating entities as graphs. The nodes represent entities, whereas the directed edges represent transactions identified by different protocol stacks observed. It uses novel unsupervised learning approaches to extract transaction patterns directly from the network data, such as the allowed protocol stacks and port numbers and data transmission behavior. Finally, ZT-SDN uses an innovative approach to generate correct access control rules and infer strong associations between them, allowing proactive rule deployment in forwarding devices. We show the framework’s efficacy in detecting abnormal network accesses and abuses of permitted flows in changing network conditions with real network datasets. Additionally, we showcase ZT-SDN’s scalability and the network’s performance when applied in an SDN environment.}
}


@article{DBLP:journals/tissec/KarunanayakeSGSC25,
	author = {Naveen Karunanayake and
                  Bhanuka Silva and
                  Yasod Ginige and
                  Suranga Seneviratne and
                  Sanjay Chawla},
	title = {Quantifying and Exploiting Adversarial Vulnerability: Gradient-Based
                  Input Pre-Filtering for Enhanced Performance in Black-Box Attacks},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {24:1--24:30},
	year = {2025},
	url = {https://doi.org/10.1145/3716384},
	doi = {10.1145/3716384},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/KarunanayakeSGSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the vulnerability of inputs in an adversarial setting and demonstrate that certain samples are more susceptible to adversarial perturbations compared to others. Specifically, we employ a simple yet effective approach to quantify the adversarial vulnerability of inputs, which relies on the  clipped  gradients of the loss with respect to the input. Our observations indicate that inputs with a low percentage of zero gradient components tend to be more vulnerable to attacks. These findings are supported by a theoretical explanation on a linear model and empirical evidence on deep neural networks. Across all datasets we tested, we find that inputs with the lowest zero gradient percentage, on average, exhibit 34.5% more susceptibility to adversarial attacks than randomly selected inputs. Additionally, we demonstrate that the zero gradient percentage, as a metric, transfers across different model architectures. Finally, we propose a novel black-box attack pipeline that enhances the efficiency of conventional query-based black-box attacks and show that input pre-filtering based on Zero Gradient Percentage can boost the attack success rates, particularly under low perturbation levels. On average, across all datasets we test, our approach outperforms the conventional shadow model-based and query-based black-box attack pipelines by 44.9% and 30.4%, respectively.}
}


@article{DBLP:journals/tissec/ShamsL25,
	author = {Sulthana Shams and
                  Douglas J. Leith},
	title = {Attack Detection Using Item Vector Shift in Matrix Factorisation Recommenders},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {25:1--25:22},
	year = {2025},
	url = {https://doi.org/10.1145/3721285},
	doi = {10.1145/3721285},
	timestamp = {Sat, 31 May 2025 23:18:26 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/ShamsL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article proposes a novel method for detecting shilling attacks in Matrix Factorization (MF)–based Recommender Systems (RSs), in which attackers use false user–item feedback to promote a specific item. Unlike existing methods that use either supervised learning to distinguish between attack and genuine profiles or analyze target item rating distributions to detect false ratings, our method uses an unsupervised technique to detect false ratings by examining shifts in item preference vectors that exploit rating deviations and user characteristics, making it a promising new direction. The experimental results demonstrate the effectiveness of our approach in various attack scenarios, including those involving obfuscation techniques.}
}


@article{DBLP:journals/tissec/KimNGK25,
	author = {Chunghyo Kim and
                  Juhwan Noh and
                  Esmaeil Ghahremani and
                  Yongdae Kim},
	title = {Revisiting {GPS} Spoofing in Phasor Measurement: Real-World Exploitation
                  and Practical Detection in Power Grids},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {26:1--26:27},
	year = {2025},
	url = {https://doi.org/10.1145/3720543},
	doi = {10.1145/3720543},
	timestamp = {Wed, 11 Jun 2025 21:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/KimNGK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phasor Measurement Units (PMUs) are critical devices in modern power grids, providing precise voltage and current phasor measurements (synchrophasors) for real-time monitoring, fault detection, and stability assessment. While previous research suggested that arbitrary time manipulation through GPS spoofing could disrupt grid operations, our study reveals that successful attacks require specific conditions, contrary to earlier assumptions. Through careful analysis of the synchrophasor data specification (IEEE Standard C37.118.x), we demonstrate that arbitrary time manipulation  does not  directly lead to phase manipulation. Instead, arbitrary manipulations can cause GPS holdover (loss of lock), alert operators with erroneous timing, and ultimately invalidate the received synchrophasors. An experiment with a commercial PMU confirms our specification analysis. We identify the time spoofing conditions to avoid GPS holdover and discover that nanosecond-scale signal alignment (approximately 375 ns error) and gradual time manipulation (around 50 ns/s error) are required. Experiments on a commercial Wide Area Monitoring System (WAMS) testbed demonstrate that GPS spoofing meeting the identified criteria results in a 500-microsecond time error (10.8-degree phase error) after 12 hours without triggering alarms. Given that a 60-degree phase variation is considered a fault, triggering protection mechanisms, this GPS spoofing technique could potentially induce false faults within 70 hours. To counter this threat, we propose a practical method to distinguish GPS spoofing-induced false faults from actual faults caused by events like lightning strikes or ground shorts. Analysis of 10 real-world incidents from the past six months demonstrates that genuine faults consistently exhibit instantaneous phase variations within three electrical cycles, providing a basis for differentiation.}
}


@article{DBLP:journals/tissec/YangLJW25,
	author = {Yuxin Yang and
                  Qiang Li and
                  Yuede Ji and
                  Binghui Wang},
	title = {A Secret Sharing-Inspired Robust Distributed Backdoor Attack to Federated
                  Learning},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {2},
	pages = {27:1--27:19},
	year = {2025},
	url = {https://doi.org/10.1145/3725814},
	doi = {10.1145/3725814},
	timestamp = {Sat, 31 May 2025 23:18:26 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/YangLJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is vulnerable to backdoor attacks—especially distributed backdoor attacks (DBA) that are more persistent and stealthy than centralized backdoor attacks. However, we observe that the attack effectiveness of DBA can be largely reduced when encountering rebels, i.e., the agents promising to perform the attack but do not do so. To robustify DBAs, we present  SSRDBA , a secret sharing-inspired robust DBA to FL. To be specific, given a same global trigger as DBA,  SSRDBA  carefully divides it into different shares based on secret sharing and exploits these shares to poison local data on malicious devices, respectively.  SSRDBA  enjoys several merits, e.g., only partial malicious agents guarantee the reconstruction of the global trigger. Extensive experimental results show that  SSRDBA  is more robust to rebels than DBA and can evade the state-of-the-art FL defenses mainly for centralized backdoor attacks. To mitigate  SSRDBA , we further design a novel defense mechanism, termed NFDR, which shows great potential against  SSRDBA  on certain independent identically distributed datasets.}
}


@article{DBLP:journals/tissec/UllahR25,
	author = {Sami Ullah and
                  Awais Rashid},
	title = {Security Implications of the Morello Platform: An Empirical Threat
                  Model-Based Analysis},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {28:1--28:41},
	year = {2025},
	url = {https://doi.org/10.1145/3728360},
	doi = {10.1145/3728360},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/UllahR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article explores the software security potential of ARM’s Morello experimental hardware platform, an embodiment of the Capability Hardware Enhanced RISC Instructions (CHERI) model. We navigate the intricacies of Morello adoption, uncovering both the promise and the challenges it presents for bolstering software security assurance. Employing the Juliet Test Suite, we conduct a rigorous security assessment of Morello’s operational modes—Purecap and Hybrid—shedding light on the ramifications for the software development lifecycle and assurance processes. Our findings affirm the robust spatial safety Morello confers, especially in its Purecap mode, while also underscoring the persisting temporal vulnerabilities in the CheriBSD  version used  in our experiments. We discuss the novel challenges associated with Morello adoption, including the management of CHERI violation exceptions, the imperative of software-hardware co-validation, and the specialized training requisites for development and assurance teams. We draw attention to potential risks, like crashes from CHERI violations potentially metamorphosing into Denial of Service (DoS) attacks. Transitioning to the Morello model could necessitate substantial alterations in software design principles, development methodologies, and security assurance protocols.}
}


@article{DBLP:journals/tissec/MouchotteDS25,
	author = {Jean Mouchotte and
                  Maxence Delong and
                  Layth Sliman},
	title = {Beyond the Screen: Exploring Privacy Boundaries through Automated
                  User Profiling},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {29:1--29:32},
	year = {2025},
	url = {https://doi.org/10.1145/3725813},
	doi = {10.1145/3725813},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/MouchotteDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social Media Intelligence (SOCMINT) is widely used for gathering sensitive information about individuals, companies, or organizations, fueling potential attacks. This study highlights the inadequacy of current privacy protection measures and proposes an automated, sustainable approach to correlate user profiles, including homonyms and pseudonyms, solely through publicly available data. A sustainable method presents a solution to bypass API privatization and anti-scraping measures for data collection and formatting, emphasizing data correlation and the creation of cross-referenced dictionaries. Aggregated data enables indirect correlations, enhancing accuracy and relevance compared to existing methods. The purpose of this study is to create a self-learning knowledge base that addresses acronyms, homonyms, and pseudonyms while incorporating context to establish deeper connections between data. Comparisons will account for contextual relevance rather than relying solely on distances or metrics. Despite challenges posed by GDPR implementation and diminishing exploitable data, analysis of a database containing 7,600 accounts from major social platforms reveals over 90% accuracy in user-to-profile associations with 30% of homonyms and unpredictable pseudonyms or usernames. Notably, the study unveils the feasibility of retrieving information from private profiles solely through the application of dictionaries. The study demonstrates the limits of privacy on social networks, with users playing a passive role in the extensive dissemination of their data. Therefore, governance structures require innovative solutions to guarantee end-to-end protection and privacy.}
}


@article{DBLP:journals/tissec/AlZewairiAARMQ25,
	author = {Malek Al{-}Zewairi and
                  Sufyan Almajali and
                  Moussa Ayyash and
                  Mohamed Rahouti and
                  Fernando Martinez and
                  Nordine Quadar},
	title = {Multi-Stage Enhanced Zero Trust Intrusion Detection System for Unknown
                  Attack Detection in Internet of Things and Traditional Networks},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {30:1--30:28},
	year = {2025},
	url = {https://doi.org/10.1145/3725216},
	doi = {10.1145/3725216},
	timestamp = {Fri, 12 Sep 2025 07:39:22 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/AlZewairiAARMQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting unknown cyberattacks remains an open research problem and a significant challenge for the research community and the security industry. This article tackles the detection of unknown cybersecurity attacks in the Internet of Things (IoT) and traditional networks by categorizing them into two types: entirely new classes of unknown attacks (type-A) and unknown attacks within already known classes (type-B). To address this, we propose a novel multi-stage, multi-layer zero trust architecture for an intrusion detection system (IDS), uniquely designed to handle these attack types. The architecture employs a hybrid methodology that combines two supervised and one unsupervised learning stages in a funnel-like design, significantly advancing current detection capabilities. A key innovation is the layered filtering mechanism, leveraging type-A and type-B attack concepts to systematically classify traffic as malicious unless proven otherwise. Using four benchmark datasets, the proposed system demonstrates significant improvements in accuracy, recall, and error classification rates for unknown attacks, achieving an average accuracy and recall ranging between 88% and 95%. This work offers a robust, scalable framework for enhancing cybersecurity in diverse network environments.}
}


@article{DBLP:journals/tissec/AlEiadehA25,
	author = {Mohammad Ryiad Al{-}Eiadeh and
                  Mustafa Abdallah},
	title = {{CBDRA-IS:} Centrality-Based Defense Resource Allocation for Securing
                  Interdependent Systems},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {31:1--31:44},
	year = {2025},
	url = {https://doi.org/10.1145/3736760},
	doi = {10.1145/3736760},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/AlEiadehA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Interdependent systems, with multiple interconnected assets, face escalating cybersecurity threats from external attackers. This article explores security decision-making, operating on complex interdependent systems and proposes a security resource allocation methodology to enhance their proactive security. Using attack graphs, we model vulnerabilities and propose different defense mechanisms integrating different network analysis algorithms, including degree, betweenness, and harmonic centralities, TrustRank, and Katz centrality. We introduce Average Based Node Ranking (ABNR) to average ranks from these methods. The resource allocation methods leverage four different graph-theoretic methods. Each ranking algorithm is combined with these four allocation techniques. Our methods show low sensitivity to simultaneous attacks on interdependent systems. We validate our framework using 11 attack graphs representing real-world systems, measuring security improvements against four well-known allocation algorithms: behavioral decision-making, defense-in-depth, risk-based defense, and min-cut. Our framework outperformed the baselines in most cases, with superior outcomes confirmed by the Friedman statistical test. We show that the main components in our framework have low-time overhead. We also evaluate our framework against multi-stage attacks and cascading failures Our framework enhances security decision-making across different scenarios, including top-1 and all attack paths for different attacks. We release the implementation of our resource allocation methodology to the research community}
}


@article{DBLP:journals/tissec/LifandaliCAMMO25,
	author = {Oumaima Lifandali and
                  Zouhair Chiba and
                  Noreddine Abghour and
                  Khalid Moussaid and
                  Mounia Miyara and
                  Abdellah Ouaguid},
	title = {Performance Enhancement of Intrusion Detection System in Cloud by
                  Using Boruta Algorithm},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {32:1--32:33},
	year = {2025},
	url = {https://doi.org/10.1145/3736761},
	doi = {10.1145/3736761},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/LifandaliCAMMO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Presently, cloud computing stands as a dependable choice for enterprises seeking contemporary, adaptable IT solutions capable of managing vast volumes of business data. Its adoption holds the promise of enhancing operational efficiency and productivity. However, cloud computing remains a dynamic and evolving technology landscape, fraught with inherent security challenges. Malevolent actors perpetually scour for novel methodologies to compromise the integrity of data hosted within cloud environments. For instance, data theft, achieved through downloading or encrypting sensitive information, and Distributed Denial of Service (DDoS) assaults targeting cloud infrastructures, pose persistent threats. To address these pressing concerns, the solution outlined in this article advocates for intrusion detection within cloud environments employing a plethora of classification algorithms. To ensure the precision of outcomes, the proposed approach incorporates the meticulous selection of pertinent attributes from the dataset, leveraging the Boruta algorithm. Our research has demonstrated that combining Boruta with classifiers yields impressive results, achieving a recall of 100% with KNN on the CICIDS 2017 dataset and a precision of 100% with Naive Bayes on the CICDDOS 2019 dataset. These results underscore the significant role of feature selection in enhancing detection performance, affirming its importance for achieving optimal results in intrusion detection systems.}
}


@article{DBLP:journals/tissec/StarinkHPC25,
	author = {Jerre Starink and
                  Marieke Huisman and
                  Andreas Peter and
                  Andrea Continella},
	title = {Behavior Nets: Context-Aware Behavior Modeling for Code Injection-Based
                  Windows Malware},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {33:1--33:29},
	year = {2025},
	url = {https://doi.org/10.1145/3729228},
	doi = {10.1145/3729228},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/StarinkHPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite significant effort put into research and development of defense mechanisms, new malware is continuously developed rapidly, making it still one of the major threats on the Internet. For malware to be successful, it is in the developer’s best interest to evade detection as long as possible. One method in achieving this is using Code Injection, where malicious code is injected into another benign process, making it do something it was not intended to do. Automated detection and characterization of Code Injection is difficult. Many injection techniques depend solely on system calls that in isolation look benign and can easily be confused with other background system activity. There is therefore a need for models that can consider the context in which a single system event resides, such that relevant activity can be distinguished easily. In previous work, we conducted the first systematic study on code injection to gain more insights into the different techniques available to malware developers on the Windows platform. This paper extends this work by introducing and formalizing Behavior Nets: A novel, reusable, context-aware modeling language that expresses malicious software behavior in observable events and their general interdependence. This allows for matching on system calls, even if those system calls are typically used in a benign context. We evaluate Behavior Nets and experimentally confirm that introducing event context into behavioral signatures yields better results in characterizing malicious behavior than the state of the art. We conclude with valuable insights on how future malware research based on dynamic analysis should be conducted.}
}


@article{DBLP:journals/tissec/SperoB25,
	author = {Eric Spero and
                  Robert Biddle},
	title = {Site Inspector: Improving Browser Communication of Website Security
                  Information},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {34:1--34:31},
	year = {2025},
	url = {https://doi.org/10.1145/3726867},
	doi = {10.1145/3726867},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/SperoB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Phishing sites exploit users’ limited understanding of website identity to mimic legitimate sites. While X.509 certificates can provide crucial cues regarding a website’s identity, current browsers fail to effectively communicate this information to users, even as phishing becomes an increasingly serious issue. To address this, we developed Site Inspector (SI), a UI tool that conveys website identity and connection encryption information, along with brief explanations of the relevant underlying security concepts. SI is implemented as a Mozilla Firefox browser extension, but the basic design could be integrated into any web browser. SI organizes content in a three-tiered abstraction hierarchy, drawing on Ecological Interface Design. The top level presents an indicator of the website owner, if known, and also whether the connection is encrypted. The second and third levels offer progressively detailed explanations of the verification process. SI adheres to design principles aimed at educating users about security through the UI while overcoming associated challenges. Its text is concise and direct, respecting limitations in users’ attentional resources and motivation to engage with security matters. As a proof of concept for SI’s principled design, we conducted a user study with 30 participants to evaluate its effectiveness in helping users differentiate real from fraudulent websites. Results suggested that SI improved users’ ability to identify fraudulent sites. Future work will involve further testing with a larger user base, integrated SI directly into browsers, and ultimately a more widespread and improved validation process for certificates, with stronger verification and transparency.}
}


@article{DBLP:journals/tissec/BartonWIMW25,
	author = {Armon Barton and
                  Timothy Walsh and
                  Mohsen Imani and
                  Jiang Ming and
                  Matthew Wright},
	title = {PredicTor: {A} Global, Machine Learning Approach to Tor Path Selection},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {35:1--35:31},
	year = {2025},
	url = {https://doi.org/10.1145/3723356},
	doi = {10.1145/3723356},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/BartonWIMW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tor users derive anonymity in part from the size of the Tor user base, but Tor struggles to attract and support more users due to performance limitations. Previous works have proposed modifications to Tor’s path selection algorithm to enhance both performance and security, but many proposals have unintended consequences due to incorporating information related to client location. We instead propose selecting paths using a global view of the network, independent of client location, and we propose doing so with a machine learning classifier to predict the performance of a given path before building a circuit. We show through a variety of simulated and live experimental settings, across different time periods, that this approach can significantly improve performance compared to Tor’s default path selection algorithm and two previously proposed approaches. In addition to evaluating the security of our approach with traditional metrics, we propose a novel anonymity metric that captures information leakage resulting from location-aware path selection, and we show that our path selection approach leaks no more information than the default path selection algorithm.}
}


@article{DBLP:journals/tissec/SenarathnaTWK25,
	author = {Danushka Senarathna and
                  Spyros Tragoudas and
                  Jason Wibbenmeyer and
                  Nasser Khdeer},
	title = {Time Series Analysis Neural Networks for Detecting False Data Injection
                  Attacks of Different Rates on Power Grid State Estimation},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {36:1--36:29},
	year = {2025},
	url = {https://doi.org/10.1145/3723164},
	doi = {10.1145/3723164},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/SenarathnaTWK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {False Data Injection Attacks (FDIAs) that target the state estimation pose an immense threat to the security of power grids. Deep Neural Network (DNN)-based methods have shown promising results in detecting such FDIAs. Among the existing state-of-the-art DNN models, time series analysis DNNs have demonstrated superior FDIA detection capability. This article discusses the challenges associated with applying time series analysis DNNs for detecting FDIAs and emphasizes the impact of the attack rate on the detection rate of attacks. We demonstrate that existing time series analysis DNNs are highly vulnerable to FDIAs executed at low attack rates. This article presents various alternative implementations for time series classifiers and time series predictors to improve the FDIA detection rate. A novel method is proposed to train time series classification neural networks to detect FDIAs of any attack rate with high efficiency. Subsequently, an enhanced FDIA detection framework that includes a time series classifier and multiple predictors is presented. Furthermore, an analytical criterion is derived to estimate the FDIA detection rate of time series analysis DNNs under any attack rate. Experimental results obtained on IEEE bus systems using state-of-the-art DNN architectures support the effectiveness of the proposed training method and the proposed framework. The proposed training method significantly improved the detection rate of FDIAs at low attack rates. Up to a 48% improvement in the FDIA detection rate was observed in the proposed framework when compared to the state-of-the-art.}
}


@article{DBLP:journals/tissec/ChenCWZMZ25,
	author = {Jinyin Chen and
                  Zhiqi Cao and
                  Xiaojuan Wang and
                  Haibin Zheng and
                  Zhaoyan Ming and
                  Yayu Zheng},
	title = {FairQuanti: Enhancing Fairness in Deep Neural Network Quantization
                  via Neuron Role Contribution},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {37:1--37:28},
	year = {2025},
	url = {https://doi.org/10.1145/3744560},
	doi = {10.1145/3744560},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/ChenCWZMZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing complexity of deep neural networks (DNNs) poses significant resource challenges for edge devices, prompting the development of compression technologies like model quantization. However, while improving model efficiency, quantization can introduce or perpetuate the original model’s bias. Existing debiasing methods for quantized models often incur additional costs. To address this issue, we propose  FairQuanti , a novel quantization approach that leverages neuron role contribution to achieve fairness. By distinguishing between biased and normal neurons,  FairQuanti  employs mixed precision quantization to mitigate model bias during the quantization process.  FairQuanti  has four key differences from previous studies: (1)\xa0 Neuron Roles  - It formally defines biased and normal neuron roles, establishing a framework for feasible model quantization and bias mitigation; (2)\xa0 Effectiveness  - It introduces a fair quantization strategy that discriminatively quantizes neuron roles, balancing model accuracy and fairness through Bayesian optimization; (3)\xa0 Generality  - It applies to both structured and unstructured data across various quantization bit levels; (4)\xa0 Robustness  - It demonstrates resilience against adaptive attacks. Extensive experiments on five datasets (three structured and two unstructured) using five different models validate  FairQuanti ’s superior performance against eight baseline methods. Specifically, fairness metrics such as demographic parity (DP) improve by approximately 1.03 times, and the demographic parity ratio (DPR) improves by approximately 1.51 times compared to the baselines, with an average accuracy loss of less than 7.5% at 8-bit quantization.  FairQuanti  presents a promising solution for deploying fair and efficient deep models on resource-constrained devices and holds potential for application in large language models to reduce size and computational demands while minimizing bias. Our source code is available at  https://github.com/Caozq2/FairQuanti .}
}


@article{DBLP:journals/tissec/AlsakarAKS25,
	author = {Noora Alsakar and
                  Norah Mohsen T. Alotaibi and
                  Mohamed Khamis and
                  Simone Stumpf},
	title = {Assessing and Mitigating the Privacy Implications of Eye Tracking
                  on Handheld Mobile Devices},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {38:1--38:36},
	year = {2025},
	url = {https://doi.org/10.1145/3746452},
	doi = {10.1145/3746452},
	timestamp = {Tue, 14 Oct 2025 19:48:54 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/AlsakarAKS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While gaze data brings benefits like allowing hands-free interaction, it can also reveal sensitive information about people, such as their gender, age, and geographical origin. Privacy leakage and safeguards have been explored for gaze data collected through headsets and stationary eye trackers, but never for gaze data collected through handheld mobile devices, like smartphones. Eye tracking on handheld mobile devices has the potential to be ubiquitous, but gaze data is typically of lower quality, compounded by additional noise and instability due to less controlled environments and screen size constraints. To address this gap, we provide the first evidence of privacy leakage through gaze data collected on handheld mobile devices. In a user study (N=35), we collected our novel  SmartEyePhone  dataset of gaze data using a smartphone’s front-facing camera. Second, we present the first evaluation and comparison of three Differential Privacy (DP) techniques against our dataset and the TüEyeQ dataset, which was collected in prior work using a stationary remote eye tracker. We found that  SmartEyePhone  dataset leaks on average 65.5% of private data. DP mechanisms reduce privacy leakage in our data by 22.33% using Laplace mechanism, 10.43% using Exponential mechanism, 22.60% using Gaussian mechanism, and 28.34% using AI model perturbation. However, this also reduces the accuracy of the main task prediction, and is impacted by the choice of privacy parameter values. We present insights on the tradeoff between privacy preservation and the practical usefulness of gaze data. Our insights advance the understanding of privacy in mobile settings and pave the way for privacy preserving gaze-enabled handheld mobile devices.}
}


@article{DBLP:journals/tissec/DavoliAR25,
	author = {Davide Davoli and
                  Martin Avanzini and
                  Tamara Rezk},
	title = {Comprehensive Kernel Safety in the Spectre Era: Mitigations and Performance
                  Evaluation},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {39:1--39:39},
	year = {2025},
	url = {https://doi.org/10.1145/3743678},
	doi = {10.1145/3743678},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/DavoliAR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et\xa0al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and operate on a separate memory model with communication through system calls. In this work, we relax Abadi et\xa0al.’s language assumptions while demonstrating that layout randomization offers a comparable safety guarantee in a system with memory separation. However, in practice, speculative execution and side-channels are recognized threats to layout randomization. We show that kernel safety cannot be restored for attackers capable of using side-channels and speculative execution, and introduce enforcement mechanisms that can guarantee speculative kernel safety for safe system calls in the Spectre era. We implement three suitable mechanisms and we evaluate their performance overhead on the Linux kernel.}
}


@article{DBLP:journals/tissec/HuangLCB25,
	author = {Mengdie Huang and
                  Yingjun Lin and
                  Xiaofeng Chen and
                  Elisa Bertino},
	title = {Dimensional Robustness Certification for Deep Neural Networks in Network
                  Intrusion Detection Systems},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {40:1--40:33},
	year = {2025},
	url = {https://doi.org/10.1145/3715121},
	doi = {10.1145/3715121},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/HuangLCB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network intrusion detection systems based on deep learning are gaining significant traction in cyber security due to their high prediction accuracy and strong adaptability to evolving cyber threats. However, a serious drawback is their vulnerability to evasion attacks that rely on adversarial examples. To provide robustness guarantees for deep neural networks against any possible perturbations, certified defenses against perturbations within a  l p -bounded region around the input are being increasingly explored. Unfortunately, unlike existing image domain approaches that concentrate on homogeneous input feature spaces, the progress on certified defense for the network traffic domain, which is characterized by heterogeneous features, has been very limited. To address such a gap, we present the design and practicality of a novel framework, Multi-order Adaptive Randomized Smoothing (MARS), for certifying the robustness of network intrusion detectors based on deep neural networks. Experiments on various network intrusion detection systems show that MARS significantly improves the tightness of robustness certification (12.23% increase in l 2  certified radius), detection accuracy on evasion attack (7.17% improvement on  l ∞ -PGD, 10.11% improvement on l 1 -EAD), and prediction accuracy on natural corruption (16.65% enhancement on latency, 18.23% enhancement on packet loss) compared to the SOTA method. We have also conducted an extensive analysis of the dimension-wise certified robustness of the network intrusion detector. The results indicate that the dimensional certified radii obtained using MARS reveal the robustness differences across feature dimensions, aligning with the empirical evaluation findings.}
}


@article{DBLP:journals/tissec/OwusuRVXHS25,
	author = {Evans Owusu and
                  Mohamed Rahouti and
                  Dinesh C. Verma and
                  Yufeng Xin and
                  D. Frank Hsu and
                  Christina Schweikert},
	title = {Generalizable Multi-Model Fusion for Multi-Class DoS Detection Using
                  Cognitive Diversity and Rank-Score Analysis},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {3},
	pages = {41:1--41:37},
	year = {2025},
	url = {https://doi.org/10.1145/3749374},
	doi = {10.1145/3749374},
	timestamp = {Thu, 11 Sep 2025 20:24:53 +0200},
	biburl = {https://dblp.org/rec/journals/tissec/OwusuRVXHS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting and mitigating Denial-of-Service (DoS) attacks is crucial for ensuring the availability and security of online services. While various machine learning (ML) models have been utilized for DoS attack detection, there is a need for innovative approaches to improving their performance, especially for the more challenging multi-class detection problem. In this article, we propose adopting a cutting-edge approach called Combinatorial Fusion Analysis (CFA), which leverages a recently developed framework to combine multiple ML models for improved DoS attack detection. Our methodology involves advanced score combination, rank combination, weighted combination techniques, and the diversity strength of scoring systems. Through rigorous performance evaluations, we showcase the efficacy of the combinatorial fusion approach. Our evaluations encompass key metrics such as detection precision, recall, and F1-score, providing comprehensive insights into the interpretability and effectiveness of our approach. We highlight the challenge faced by individual models in classifying low-profiled attacks, while excelling in other attack types. To overcome this limitation, model fusion techniques were used to create a comprehensive model capable of addressing both low-profiled attacks and other traffic types. Furthermore, our findings highlight the potential of this approach for enhancing DoS attack detection capabilities and contributing to the development of more robust defense mechanisms.}
}


@article{DBLP:journals/tissec/CortellazziQAPPC25,
	author = {Jacopo Cortellazzi and
                  Erwin Quiring and
                  Daniel Arp and
                  Feargus Pendlebury and
                  Fabio Pierazzi and
                  Lorenzo Cavallaro},
	title = {Intriguing Properties of Adversarial {ML} Attacks in the Problem Space
                  [Extended Version]},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {42:1--42:37},
	year = {2025},
	url = {https://doi.org/10.1145/3742895},
	doi = {10.1145/3742895},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/CortellazziQAPPC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent research efforts on adversarial machine learning (ML) have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This article makes three major contributions. Firstly, we propose a general formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of  side-effect features  as the by-product of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. Secondly, building on our general formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations in terms of semantics and artifacts. We have tested our approach on a dataset with 150K Android apps from 2016 and 2018 which show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Thirdly, we explore the effectiveness of adversarial training as a possible approach to enforce robustness against adversarial samples, evaluating its effectiveness on the considered machine learning models under different scenarios. Our results demonstrate that “adversarial-malware as a service” is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial instance.}
}


@article{DBLP:journals/tissec/JaramilloMEJ25,
	author = {Daniel Cabarcas Jaramillo and
                  Hern{\'{a}}n Dar{\'{\i}}o Vanegas Madrigal and
                  Daniel Escudero and
                  Fernando Alberto Morales Jauregui},
	title = {Privacy-Preserving Training of Support Vector Machines via Secure
                  Multiparty Computation},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {43:1--43:27},
	year = {2025},
	url = {https://doi.org/10.1145/3749373},
	doi = {10.1145/3749373},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/JaramilloMEJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The power and ubiquity of machine learning demand security measures for protecting sensitive data. Secure multiparty computation (MPC) techniques enable a group of parties to jointly compute a given function while keeping the information private. In this work, we engineer a prototype for privately training support vector machines (SVMs) using MPC techniques. We conduct an extensive study on how different approaches for training SVMs interact with existing state-of-the-art MPC protocols. We identify the least squares (LS) approach as the best suited for privately training. We then optimize fixed-point precision, ensuring accuracy while keeping low running time and communication. The technical details of the optimization involve bounds on the step size of a gradient method to solve a linear system, which might be of independent interest. We further propose and analyse different alternatives to improve the LS approach on an MPC implementation, and we compare their performance. The best improvement yields up to 2× reduction of the running time and communication complexity, without affecting the accuracy of the trained model. In order to illustrate the feasibility of our solution, we securely train SVMs for two realistic tasks.}
}


@article{DBLP:journals/tissec/DigregorioSLCZ25,
	author = {Gabriele Digregorio and
                  Edoardo Saputelli and
                  Stefano Longari and
                  Michele Carminati and
                  Stefano Zanero},
	title = {Swarm: {A} Distributed Ledger-based Framework to Enhance Air Traffic
                  Control Security Using {ADS-B} Protocol},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {44:1--44:35},
	year = {2025},
	url = {https://doi.org/10.1145/3759456},
	doi = {10.1145/3759456},
	timestamp = {Sun, 11 Jan 2026 17:15:12 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/DigregorioSLCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In aviation, safety is paramount, with air traffic control (ATC) playing a crucial role in monitoring aircraft to prevent collisions and manage traffic flows. In response to increasing air traffic, a renewal process has been initiated. This includes deploying the automatic dependent surveillance-broadcast (ADS-B) communications protocol, which aims to enhance surveillance precision and increase the number of aircraft that can be handled simultaneously. This transition is transforming ATC from a radar-based system to a more advanced satellite-based global positioning system (GPS) location tracking system. However, due to its inherently open design, the ADS-B protocol lacks critical security features such as authentication, necessitating the adoption of additional security measures to mitigate potential cyber-attacks. To address these vulnerabilities, this work introduces Swarm, an innovative distributed ledger-based framework, built on top of the ADS-B protocol and aimed at enhancing the security of air traffic control (ATC) while avoiding single points of failure. Swarm can be integrated into existing ATC infrastructure without requiring any modifications to the ADS-B protocol. We evaluate Swarm through rigorous and realistic attack scenarios, using real-world aviation data, demonstrating its capability to enhance the security of the aviation domain.}
}


@article{DBLP:journals/tissec/AkpakuCAAO25,
	author = {Ernest Akpaku and
                  Jinfu Chen and
                  Mukhtar Ahmed and
                  Francis Kwadzo Agbenyegah and
                  Joshua Ofoeda},
	title = {{MGAN:} {A} Multi-view Graph Adaptive Network for Robust Malicious
                  Traffic Detection},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {45:1--45:35},
	year = {2025},
	url = {https://doi.org/10.1145/3757741},
	doi = {10.1145/3757741},
	timestamp = {Sun, 01 Feb 2026 13:44:04 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/AkpakuCAAO25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting malicious network traffic in large-scale, dynamic environments presents a significant challenge due to the complexity of network relationships and the evolving nature of cyber threats. Existing graph-based and sequence-based models often fail to capture both spatial dependencies and temporal patterns effectively, resulting in suboptimal detection. This study introduces the Multi-view Graph Adaptive Network (MGAN), a novel framework that integrates multi-hop graph neural network (GNN) aggregation with transformer-based sequence modeling to address these challenges. MGAN captures long-range spatial dependencies and temporal dynamics in network traffic, enabling the detection of complex attack patterns. It incorporates Dirichlet sampling for robust neighbor selection in sparse and noisy data environments and mutual information maximization to align multi-view representations for consistency. Additionally, a multi-view attention mechanism aggregates information across different hops, balancing local and global network context. Extensive experiments on four real-world datasets demonstrate MGAN’s superiority over 7 baseline models, achieving an average F1-Score above 97%, surpassing the best baseline by 2.35%. MGAN maintains detection accuracy above 97% and remains robust under data sparsity, achieving F1-Scores over 95% even when 40% of connectivity information is removed. Under noisy conditions, MGAN retains accuracy above 93%, outperforming baselines by over 4.5%. In zero-day attack scenarios, it achieves detection rates exceeding 96% for previously unseen attack categories. MGAN also exhibits exceptional computational efficiency, processing 2,034 samples per second with a detection time of 3.00 milliseconds per sample, outperforming all competing models in both accuracy and speed.}
}


@article{DBLP:journals/tissec/CuellarHPPST25,
	author = {Santiago Cu{\'{e}}llar and
                  Bill Harris and
                  James Parker and
                  Stuart Pernsteiner and
                  Ian Sweet and
                  Eran Tromer},
	title = {Cheesecloth: Zero-Knowledge Proofs of Real-World Vulnerabilities},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {46:1--46:35},
	year = {2025},
	url = {https://doi.org/10.1145/3747589},
	doi = {10.1145/3747589},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/CuellarHPPST25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, when a security analyst discovers a vulnerability in critical software system, they must navigate a fraught dilemma: immediately disclosing the vulnerability to the public could harm the system’s users; whereas disclosing the vulnerability only to the software’s vendor lets the vendor disregard or deprioritize the security risk, to the detriment of unwittingly-affected users. A compelling recent line of work aims to resolve this by using Zero Knowledge (ZK) protocols that let analysts prove that they know a vulnerability in a program, without revealing the details of the vulnerability or the inputs that exploit it. In principle, this could be achieved by generic ZK techniques. In practice, ZK vulnerability proofs to date have been restricted in scope and expressibility, due to challenges related to generating proof statements that model real-world software at scale and to directly formulating violated properties. This article presents  Cheesecloth , a novel proof-statement compiler, which proves practical vulnerabilities in ZK by soundly-but-aggressively preprocessing programs on public inputs, selectively revealing information about executed control segments, and formalizing information leakage using a novel storage-labeling scheme.  Cheesecloth  ’s practicality is demonstrated by generating ZK proofs of well-known vulnerabilities in (previous versions of) critical software, including the Heartbleed information leakage in OpenSSL, a memory vulnerability in the FFmpeg multimedia encoding framework, a cryptographic implementation bug in the Secure Scuttlebutt decentralised social network, and a denial of service vulnerability in OpenSSL.}
}


@article{DBLP:journals/tissec/HuZQY25,
	author = {Aodi Hu and
                  Zhiyong Zhang and
                  Gaoyuan Quan and
                  Xinxin Yue},
	title = {{IDPA:} Indiscriminate Data Poisoning Attacks Targeting Pre-trained
                  Encoder Based on Contrastive Learning},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {47:1--47:22},
	year = {2025},
	url = {https://doi.org/10.1145/3757916},
	doi = {10.1145/3757916},
	timestamp = {Sun, 01 Feb 2026 13:44:04 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/HuZQY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indiscriminate data poisoning attacks are highly effective against unsupervised learning. However, recent studies show that contrastive learning is also susceptible to data poisoning attacks. As a form of data poisoning attack, the attacker adds poison to the clean pre-training dataset. This article proposes IDPA, an indiscriminate data poisoning attack targeting the encoder in contrastive learning. where the attacker’s goal is to directly poison the pre-trained encoder. The feature vectors of any clean sample and the attacked sample from the attacker will exhibit high similarity, causing the downstream classifier to misclassify the clean sample as the samples designated by the attacker. Therefore, this article formulates IDPA as a dual optimization problem and defines two loss functions: the attack effectiveness loss and the model utility loss. These losses are associated with effectively poisoning the pre-trained encoder and maintaining the accuracy of the downstream classifier, respectively. During training, the attack affects the contrastive learning algorithm and predictions are made on multiple datasets. Experimental results show that the attack success rate of 92%. This article evaluates the effectiveness of IDPA on the CLIP dataset released by OpenAI, with attack success rate of 88%.}
}


@article{DBLP:journals/tissec/SureshJ25,
	author = {Akshaya Suresh and
                  Arun Cyril Jose},
	title = {Adaptive Network Intrusion Detection Using Reinforcement Learning
                  with Proximal Policy Optimization},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {48:1--48:24},
	year = {2025},
	url = {https://doi.org/10.1145/3764586},
	doi = {10.1145/3764586},
	timestamp = {Sun, 01 Feb 2026 13:44:05 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/SureshJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an increasingly digital and interconnected world, the need for robust network intrusion detection systems is crucial to ensure cybersecurity. This article presents a novel approach to network intrusion detection that integrates both traditional machine learning methods and advanced reinforcement learning techniques to enhance detection capabilities and accuracy. The proposed system uses Proximal Policy Optimization, a reinforcement learning algorithm, to dynamically adjust ensemble weights, thereby optimizing the contributions of base learners, such as Random Forest and CatBoost. Additionally, a Multi-layer Perceptron-based meta-learner is employed to refine the predictions, leading to an overall improvement in detection performance. The model was evaluated on five diverse datasets, including NSL-KDD, CICIDS, TON IoT, DDoS, and UNSW-NB15, achieving an average accuracy of 97.16%, and an average precision, recall, and F1-score of 97% across all datasets. The proposed work is compared with the existing state-of-the-art detection methods demonstrating its better performance in detecting both known and novel attack types. Furthermore, the integration of reinforcement learning allowed for dynamic and context-sensitive decision-making, enabling the system to handle complex attack patterns that traditional models struggle with. The training and validation results across all datasets showed rapid convergence and minimal overfitting, further supporting the model’s robustness.}
}


@article{DBLP:journals/tissec/ZhangL25,
	author = {Yue Zhang and
                  Zhiqiang Lin},
	title = {Breaking {BLE} {MAC} Address Randomization with Allowlist-Based Side
                  Channels and its Countermeasure},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {49:1--49:33},
	year = {2025},
	url = {https://doi.org/10.1145/3744559},
	doi = {10.1145/3744559},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/ZhangL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bluetooth Low Energy (BLE) is ubiquitous today. To prevent a BLE device (e.g., a smartphone) from being connected by unknown devices, it uses allowlisting to allow the connectivity from only recognized devices. Unfortunately, we show that this allowlist feature actually introduces a side channel for device tracking, since a device with the allowed list behaves differently even though it has used randomized MAC addresses. Even worse, we also find that the current MAC address randomization scheme specified in Bluetooth protocol is flawed, suffering from a replay attack with which an attacker can replay a sniffed MAC address to probe whether a targeted device will respond or not based on its allowlist. We have validated our allowlist-based side channel attacks with 43 BLE peripheral devices, 11 centrals, and 4 development boards, and found none of them once configured with allowlisting is immune to the proposed attacks. We advocate the use of an interval unpredictable, central and peripheral synchronized random MAC address randomization scheme to defeat passive device tracking (introducing 1% power consumption overhead for centrals and 6.75% for peripherals, and 88.49 μs performance overhead for centrals and 94.46 μs for peripherals), and the use of timestamps to derive randomized MAC addresses such that attackers can no longer be able to replay them to defeat active device tracking (introducing 3.04% overhead for peripherals, and 63.58 μs and 20.54 μs performance overhead for centrals and peripherals). Our field testing with a long range Bluetooth sniffer shows that 16,422 of 100,101 sniffed devices are subject to our  BAT  attacks. We have disclosed our findings to Bluetooth SIG and many other stakeholders in October 2020. Bluetooth SIG assigned CVE-2020-35473 to track this logical-level protocol flaw. Google assigned our findings as a high severity design flaw and awarded us with a bug bounty.}
}


@article{DBLP:journals/tissec/HuangBL25,
	author = {Ziyuan Huang and
                  Gergely Bicz{\'{o}}k and
                  Mingyan Liu},
	title = {Incentivizing Secure Software Development: The Role of Voluntary Audit
                  and Liability Waiver},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {50:1--50:30},
	year = {2025},
	url = {https://doi.org/10.1145/3765287},
	doi = {10.1145/3765287},
	timestamp = {Sun, 01 Feb 2026 13:44:04 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/HuangBL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Misaligned incentives in secure software development have long been a challenge in security economics. Product liability, a powerful legal framework in other industries, has been largely ineffective for software products until recent times. However, the rapid regulatory responses to recent global cyber attacks by both the US and EU, together with the (relative) success of the General Data Protection Regulation in defining both duty and standard of care for software vendors, may enable regulators to use liability to re-align incentives for the benefit of the digital society. The United States National Cybersecurity Strategy suggests shifting responsibility for cyber incidents back to software vendors and proposes the concept of the liability waiver: if a software company voluntarily undergoes and passes an IT security audit, its future product liability is (fully or partially) waived. This article examines this audit-liability framework from both vendor and auditor perspectives. For vendors, we model the decision process as a sequential problem: a vendor must pass an audit to release a product and can attempt the audit multiple times. We show that the optimal strategy for an opt-in vendor is to never quit and to exert cumulative investments in either a “one-and-done” or “incremental” manner. For auditors, we explore how to design audits that encourage voluntary participation while maximizing vendor effort. We further investigate dynamic audit designs that can amplify vendors’ cumulative investments in security. Our findings provide insights into how liability waivers and audit strategies can re-align incentives, fostering a more secure digital ecosystem.}
}


@article{DBLP:journals/tissec/DallAglioBCZP25,
	author = {Lorenzo Dall'Aglio and
                  Lorenzo Binosi and
                  Michele Carminati and
                  Stefano Zanero and
                  Mario Polino},
	title = {Highliner: Enhancing Binary Analysis through NLP-Based Instruction-Level
                  Detection of {C++} Inline Functions},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {51:1--51:22},
	year = {2025},
	url = {https://doi.org/10.1145/3765521},
	doi = {10.1145/3765521},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/DallAglioBCZP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The complexities introduced by compiler optimization have long stood as a significant obstacle in binary analysis and reverse engineering. Function inlining, in particular, complicates function recognition by replacing function calls with the entire body of the callee, mixing code from multiple functions. State-of-the-art approaches can identify inlined functions at basic block granularity, but cannot determine which instructions belong to each function and precisely deduce inlined boundaries. Without this information, further analyses such as decompilation cannot be performed effectively. This article presents Highliner, a novel approach that improves state-of-the-art approaches by identifying inline instances at instruction-level granularity. Highliner operates downstream of block-level detectors: given basic blocks reported by state-of-the-art approaches as belonging to a specific inlined function, it labels each instruction as  Inlined  or  Not inlined  and recovers the inlined-function boundaries. We treat the problem as a sequence tagging task typical of NLP and implement a learning-based technique involving instruction embedding and recurrent neural networks. We compile a dataset of open-source projects with different optimizations and use the DWARF debug information standard to construct labeled sequences of inline instructions. We use this dataset to train, validate, and test a sequence labeling architecture in which instructions are encoded via the pre-trained assembly language transformer PalmTree and then processed by an RNN-based classifier to produce binary predictions. When evaluated as a binary classifier, Highliner achieves an F1-score of 0.94 overall. In addition, when specifically tested on recognizing function boundaries, Highliner achieves an Accuracy of 0.82 on initial boundaries and 0.83 on final boundaries.}
}


@article{DBLP:journals/tissec/WangSLXLW25,
	author = {Yulong Wang and
                  Jiaxuan Song and
                  Tianxiang Li and
                  Yuan Xin and
                  Hong Li and
                  Ni Wei},
	title = {Rectifying Multi-Attack Adversarial Perturbations in Deep Neural Network
                  based Image Classifier},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {52:1--52:28},
	year = {2025},
	url = {https://doi.org/10.1145/3765757},
	doi = {10.1145/3765757},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/WangSLXLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep neural networks (DNNs) for image classification remain vulnerable to adversarial perturbations–subtle input manipulations that induce catastrophic misclassifications. To address this issue, we propose the Adversarial Image Rectifier (AIR), a linguistically inspired detection and mitigation framework that enhances DNN robustness by intercepting and inverting adversarial perturbations at the feature level. Unlike existing defenses, AIR operates without prior knowledge of attack patterns: it first encodes hierarchical hidden-layer feature maps of a DNN into semantically structured sentence representations, then identifies adversarial inputs through “sentiment” anomalies in these sentences–a linguistic metaphor for subtle adversarial traces. Crucially, we pinpoint a pivotal intermediate layer where adversarial perturbations dominantly propagate and train a lightweight rectifier network to selectively nullify adversarial features at this layer while preserving benign semantics. Extensive experiments on Tiny-ImageNet, CIFAR-10, SVHN, and MS COCO demonstrate that AIR achieves a correction rate of up to 95.02% and 94.62% when defending against known attacks and unknown attacks, respectively, significantly surpassing existing defense techniques.}
}


@article{DBLP:journals/tissec/ZhengLYNTA25,
	author = {Jingjing Zheng and
                  Kai Li and
                  Xin Yuan and
                  Wei Ni and
                  Eduardo Tovar and
                  {\"{O}}zg{\"{u}}r B. Akan},
	title = {GradCAM-AE: {A} New Shield Defense against Poisoning Attacks on Federated
                  Learning},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {53:1--53:23},
	year = {2025},
	url = {https://doi.org/10.1145/3765743},
	doi = {10.1145/3765743},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/ZhengLYNTA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent poisoning attacks on federated learning (FL) generate malicious model updates that circumvent widely adopted Euclidean distance-based detection methods. This article proposes a new defense mechanism, namely, GradCAM-AE, against model poisoning attacks on FL, which integrates Gradient-weighted Class Activation Mapping (GradCAM) and autoencoder (AE) to offer a substantially more powerful detection capability compared to existing Euclidean distance-based approaches. Particularly, GradCAM-AE generates a heat map for each uploaded local model update, transforming each local model update into a lower-dimensional, visual representation. An AE further reprojects the GradCAM heat maps of all local module updates with improved distinguishability, thereby accentuating the hidden features of the heat maps and increasing the success rate of identifying anomalous heat maps and malicious local models. A comprehensive evaluation of the proposed GradCAM-AE framework is conducted using the CIFAR-10 and GTSRB datasets under both Independent and Identically Distributed (IID) and Non-IID settings. The ResNet-18 and MobileNetV3-Large models are tested. The results substantiate that GradCAM-AE offers superior detection rates and test accuracy of FL global model, juxtaposed with contemporary state-of-the-art methods. Our code is available at:  https://github.com/jjzgeeks/GradCAM-AE .}
}


@article{DBLP:journals/tissec/TimmerLNK25,
	author = {Roelien C. Timmer and
                  David Liebowitz and
                  Surya Nepal and
                  Salil S. Kanhere},
	title = {Evaluating Honeyfile Realism and Enticement Metrics},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {54:1--54:34},
	year = {2025},
	url = {https://doi.org/10.1145/3763792},
	doi = {10.1145/3763792},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/TimmerLNK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deceptive files, often called honeyfiles, have become an established tool in cyber security. Advances in machine learning (ML) models for content generation now allow the synthesis of deceptive material automatically and at scale. Metrics to quantify honeyfile attributes are thus essential to creating and evaluating effective deceptions. The two critical aspects of honeyfiles for which metrics are useful are  enticement  and  realism . Enticement is the ability to attract the attention of intruders or users with malicious intent. Realism measures the similarity of deceptive artefacts to the objects they mimic. In the honeyfile literature, metrics for these attributes have been proposed: the Common Token Count (CTC)\xa0[ 1 ], and Topic Semantic Matching (TSM)\xa0[ 2 ] scores for enticement, and coherence and cohesion\xa0[ 3 ] for realism. In this study, we compare these metrics to the perceptions of human users exposed to text samples in a simulated data breach scenario on a crowd-sourcing platform. We recruited participants to judge the realism and enticement of honeyfile text generated using several techniques. The main findings are: (i) for the enticement metrics, TSM is aligned with the perceived enticement (p-value<0.001), while for the CTC score, we find inconsistent and inconclusive results, and (ii) for the realism metrics, cohesion and coherence, do not consistently align with perceived realism.}
}


@article{DBLP:journals/tissec/FuSDAA25,
	author = {Zhiwei Fu and
                  Leo Song and
                  Steven H. H. Ding and
                  Furkan Alaca and
                  Sudipta Acharya},
	title = {Toward a Robust Detection of PowerShell Malware against Code Mixing
                  and Obfuscation by Using Sentence Transformer and Similarity Learning},
	journal = {{ACM} Trans. Priv. Secur.},
	volume = {28},
	number = {4},
	pages = {55:1--55:23},
	year = {2025},
	url = {https://doi.org/10.1145/3771542},
	doi = {10.1145/3771542},
	timestamp = {Tue, 03 Feb 2026 08:25:13 +0100},
	biburl = {https://dblp.org/rec/journals/tissec/FuSDAA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Embedded PowerShell commands or scripts are among the most popular malware payloads. For malware that prioritizes stealthiness, such as fileless malware, PowerShell’s access to Windows API functions without additional libraries makes it useful for evading detection. Detecting malicious PowerShell scripts and commands is an open challenge for proactive endpoint protection due to three major issues: (1) The malicious commands are usually hidden in a long script beyond the processing limit of typical machine learning models. (2) They are usually mixed with bulky benign scripts. (3) Script obfuscation can easily conceal their potential matching signatures. In this article, we introduce a novel model addressing these challenges. It incorporates similarity learning, sentence transformer, sliding window method, and stochastic gradient descent (SGD) classifier. Our key insight is that malicious PowerShell code, particularly when obfuscated, exhibits semantic and statistical deviations from benign administrative usage, and these deviations can be captured by contrastive sentence embeddings without the need for de-obfuscation or handcrafted features. We operate this insight through a Siamese similarity learning framework that improves robustness against  Out-of-Vocabulary  tokens due to unseen code obfuscation methods. The sliding window method enables the model to handle long scripts, and the SGD classifier evaluates segment-level maliciousness. Our model achieves accuracies of 99.01%, 97.59%, 98.70%, and 99.73% across multiple obfuscated and mixed script benchmarks, outperforming existing baselines by over 30% in all cases. This work demonstrates a scalable and effective strategy for robust PowerShell malware detection in real-world scenarios.}
}
