@article{DBLP:journals/tkdd/WangDLLZ21,
	author = {Hao Wang and
                  Shuai Ding and
                  Yeqing Li and
                  Xiaojian Li and
                  Youtao Zhang},
	title = {Hierarchical Physician Recommendation via Diversity-enhanced Matrix
                  Factorization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {1:1--1:17},
	year = {2021},
	url = {https://doi.org/10.1145/3418227},
	doi = {10.1145/3418227},
	timestamp = {Tue, 07 May 2024 20:19:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangDLLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent studies have shown that there exhibits significantly imbalanced medical resource allocation across public hospitals. Patients, regardless of their diseases, tend to choose hospitals and physicians with a better reputation, which often overloads major hospitals while leaving others underutilized. Guiding patients to hospitals that can serve their treatment needs both timely and with good quality can make the best use of precious medical resources. Unfortunately, it remains one of the major challenges both for research and in practice. In this article, we propose a novel diversity-enhanced hierarchical physician recommendation approach to address this issue. We adopt matrix factorization to estimate physician competency and exploit implicit similarity relationships to improve the competency estimation of physicians that we are of little information of. We then balance the patient preference and physician diversity using two novel heuristic algorithms. We evaluate our proposed approach and compare it with the state of the art. Experiments show that our approach significantly improves both accuracy and recommendation diversity over existing approaches.}
}


@article{DBLP:journals/tkdd/GalimbertiCBBCG21,
	author = {Edoardo Galimberti and
                  Martino Ciaperoni and
                  Alain Barrat and
                  Francesco Bonchi and
                  Ciro Cattuto and
                  Francesco Gullo},
	title = {Span-core Decomposition for Temporal Networks: Algorithms and Applications},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {2:1--2:44},
	year = {2021},
	url = {https://doi.org/10.1145/3418226},
	doi = {10.1145/3418226},
	timestamp = {Wed, 16 Mar 2022 23:54:45 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GalimbertiCBBCG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When analyzing temporal networks, a fundamental task is the identification of dense structures (i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). In this article, we tackle this task by introducing a notion of temporal core decomposition where each core is associated with two quantities, its coreness, which quantifies how densely it is connected, and its span, which is a temporal interval: we call such cores span-cores. For a temporal network defined on a discrete temporal domain T, the total number of time intervals included in T is quadratic in |T|, so that the total number of span-cores is potentially quadratic in |T| as well. Our first main contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores, i.e., span-cores that are not dominated by any other span-core by both their coreness property and their span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directly extract the maximal ones without computing all span-cores. Finally, as a third contribution, we introduce the problem of temporal community search, where a set of query vertices is given as input, and the goal is to find a set of densely-connected subgraphs containing the query vertices and covering the whole underlying temporal domain T. We derive a connection between this problem and the problem of finding (maximal) span-cores. Based on this connection, we show how temporal community search can be solved in polynomial-time via dynamic programming, and how the maximal span-cores can be profitably exploited to significantly speed-up the basic algorithm. We provide an extensive experimentation on several real-world temporal networks of widely different origins and characteristics. Our results confirm the efficiency and scalability of the proposed methods. Moreover, we showcase the practical relevance of our techniques in a number of applications on temporal networks, describing face-to-face contacts between individuals in schools. Our experiments highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics, detecting/correcting anomalies in the data, and graph-embedding-based network classification.}
}


@article{DBLP:journals/tkdd/HuangYYT21,
	author = {Yu Huang and
                  Josh Jia{-}Ching Ying and
                  Philip S. Yu and
                  Vincent S. Tseng},
	title = {Dynamic Graph Mining for Multi-weight Multi-destination Route Planning
                  with Deadlines Constraints},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {3:1--3:32},
	year = {2021},
	url = {https://doi.org/10.1145/3412363},
	doi = {10.1145/3412363},
	timestamp = {Thu, 19 Jan 2023 18:35:30 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangYYT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Route planning satisfied multiple requests is an emerging branch in the route planning field and has attracted significant attention from the research community in recent years. The prevailing studies focus only on seeking a route by minimizing a single kind of Travel Cost, such as trip time or distance, among others. In reality, most users would like to choose an appropriate route, neither fastest nor shortest route. Usually, a user may have multiple requirements, and an appropriate route would satisfy all requirements requested by the user. In fact, planning an appropriate route could be formulated as a problem of Multi-weight Multi-destination Route Planning with Deadlines Constraints (MWMDRP-DC). In this article, we propose a framework, namely, MWMD-Router, which addresses the MWMDRP-DC problem comprehensively. To consider the travel costs with time-variation, we propose not only four novel dynamic graph miner to extract travel costs that reveal users’ requirements but also two new algorithms, namely, Basic MWMD Route Planning and Advanced MWMD Route Planning, to plan a route that satisfies deadline requirements and optimizes another criterion like travel cost with time-variation efficiently. To the best of our knowledge, this is the first work on route planning that considers handling multiple deadlines for multi-destination planning as well as optimizing multiple travel costs with time-variation simultaneously. Experimental results demonstrate that our proposed algorithms deliver excellent performance with respect to efficiency and effectiveness.}
}


@article{DBLP:journals/tkdd/SiersI21,
	author = {Michael J. Siers and
                  Md Zahidul Islam},
	title = {Class Imbalance and Cost-Sensitive Decision Trees: {A} Unified Survey
                  Based on a Core Similarity},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {4:1--4:31},
	year = {2021},
	url = {https://doi.org/10.1145/3415156},
	doi = {10.1145/3415156},
	timestamp = {Sun, 25 Jul 2021 11:43:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SiersI21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Class imbalance treatment methods and cost-sensitive classification algorithms are typically treated as two independent research areas. However, many of these techniques have properties in common. After providing a background to the two fields of research, this article identifies the fundamental mechanism which is common to both. Using this mechanism, a taxonomy is created which encompasses approaches to both class imbalance treatment and cost-sensitive classification. Through this survey, we aim to bridge the gap between the two fields such that lessons from one field may be applied to the other. Many data mining tasks are naturally both class imbalanced and cost-sensitive. This survey is useful for researchers and practitioners approaching these tasks as it provides a detailed overview of approaches in both fields. Many of the surveyed techniques are classifier independent. However, we chose to focus on techniques which were either decision tree-based or compatible with decision trees. This choice was based on the popularity and novelty of their application to both fields.}
}


@article{DBLP:journals/tkdd/HuangSYXSJ21,
	author = {Hong Huang and
                  Yu Song and
                  Fanghua Ye and
                  Xing Xie and
                  Xuanhua Shi and
                  Hai Jin},
	title = {Multi-Stage Network Embedding for Exploring Heterogeneous Edges},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {5:1--5:27},
	year = {2021},
	url = {https://doi.org/10.1145/3415157},
	doi = {10.1145/3415157},
	timestamp = {Sun, 06 Oct 2024 21:41:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangSYXSJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The relationships between objects in a network are typically diverse and complex, leading to the heterogeneous edges with different semantic information. In this article, we focus on exploring the heterogeneous edges for network representation learning. By considering each relationship as a view that depicts a specific type of proximity between nodes, we propose a multi-stage non-negative matrix factorization (MNMF) model, committed to utilizing abundant information in multiple views to learn robust network representations. In fact, most existing network embedding methods are closely related to implicitly factorizing the complex proximity matrix. However, the approximation error is usually quite large, since a single low-rank matrix is insufficient to capture the original information. Through a multi-stage matrix factorization process motivated by gradient boosting, our MNMF model achieves lower approximation error. Meanwhile, the multi-stage structure of MNMF gives the feasibility of designing two kinds of non-negative matrix factorization (NMF) manners to preserve network information better. The united NMF aims to preserve the consensus information between different views, and the independent NMF aims to preserve unique information of each view. Concrete experimental results on realistic datasets indicate that our model outperforms three types of baselines in practical applications.}
}


@article{DBLP:journals/tkdd/HuW21,
	author = {Yue Hu and
                  Daniel B. Work},
	title = {Robust Tensor Recovery with Fiber Outliers for Traffic Events},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {6:1--6:27},
	year = {2021},
	url = {https://doi.org/10.1145/3417337},
	doi = {10.1145/3417337},
	timestamp = {Tue, 09 Mar 2021 15:06:29 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Event detection is gaining increasing attention in smart cities research. Large-scale mobility data serves as an important tool to uncover the dynamics of urban transportation systems, and more often than not the dataset is incomplete. In this article, we develop a method to detect extreme events in large traffic datasets, and to impute missing data during regular conditions. Specifically, we propose a robust tensor recovery problem to recover low-rank tensors under fiber-sparse corruptions with partial observations, and use it to identify events, and impute missing data under typical conditions. Our approach is scalable to large urban areas, taking full advantage of the spatio-temporal correlations in traffic patterns. We develop an efficient algorithm to solve the tensor recovery problem based on the alternating direction method of multipliers (ADMM) framework. Compared with existing l1 norm regularized tensor decomposition methods, our algorithm can exactly recover the values of uncorrupted fibers of a low-rank tensor and find the positions of corrupted fibers under mild conditions. Numerical experiments illustrate that our algorithm can achieve exact recovery and outlier detection even with missing data rates as high as 40% under 5% gross corruption, depending on the tensor size and the Tucker rank of the low rank tensor. Finally, we apply our method on a real traffic dataset corresponding to downtown Nashville, TN and successfully detect the events like severe car crashes, construction lane closures, and other large events that cause significant traffic disruptions.}
}


@article{DBLP:journals/tkdd/ZhuLWZF21,
	author = {Xiaoyan Zhu and
                  Yingbin Li and
                  Jiayin Wang and
                  Tian Zheng and
                  Jingwen Fu},
	title = {Automatic Recommendation of a Distance Measure for Clustering Algorithms},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {7:1--7:22},
	year = {2021},
	url = {https://doi.org/10.1145/3418228},
	doi = {10.1145/3418228},
	timestamp = {Tue, 02 Aug 2022 22:23:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuLWZF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With a large number of distance measures, the appropriate choice for clustering a given data set with a specified clustering algorithm becomes an important problem. In this article, an automatic distance measure recommendation method for clustering algorithms is proposed. The recommendation method consists of the following steps: (1) metadata extraction, including meta-feature collection and meta-target identification; (2) recommendation model construction using metadata; and (3) distance measure recommendation for a new data set by the recommendation model. Two different types of meta-targets and meta-learning techniques are utilized considering the possible different requirements of users. To validate the necessity and effectiveness of the distance measure recommendation method, an empirical study is conducted with 199 publicly available data sets, 9 distance measures, and 2 widely used clustering algorithms. The experimental results indicate that distance measure significantly influences the performance of the clustering algorithm for a given data set. Furthermore, performance analysis of the proposed recommendation method proves its effectiveness.}
}


@article{DBLP:journals/tkdd/BernardiniCCGLP21,
	author = {Giulia Bernardini and
                  Huiping Chen and
                  Alessio Conte and
                  Roberto Grossi and
                  Grigorios Loukides and
                  Nadia Pisanti and
                  Solon P. Pissis and
                  Giovanna Rosone and
                  Michelle Sweering},
	title = {Combinatorial Algorithms for String Sanitization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {8:1--8:34},
	year = {2021},
	url = {https://doi.org/10.1145/3418683},
	doi = {10.1145/3418683},
	timestamp = {Mon, 26 Jun 2023 20:57:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BernardiniCCGLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {String data are often disseminated to support applications such as location-based service provision or DNA sequence analysis. This dissemination, however, may expose sensitive patterns that model confidential knowledge (e.g., trips to mental health clinics from a string representing a user’s location history). In this article, we consider the problem of sanitizing a string by concealing the occurrences of sensitive patterns, while maintaining data utility, in two settings that are relevant to many common string processing tasks. In the first setting, we aim to generate the minimal-length string that preserves the order of appearance and frequency of all non-sensitive patterns. Such a string allows accurately performing tasks based on the sequential nature and pattern frequencies of the string. To construct such a string, we propose a time-optimal algorithm, TFS-ALGO. We also propose another time-optimal algorithm, PFS-ALGO, which preserves a partial order of appearance of non-sensitive patterns but produces a much shorter string that can be analyzed more efficiently. The strings produced by either of these algorithms are constructed by concatenating non-sensitive parts of the input string. However, it is possible to detect the sensitive patterns by “reversing” the concatenation operations. In response, we propose a heuristic, MCSR-ALGO, which replaces letters in the strings output by the algorithms with carefully selected letters, so that sensitive patterns are not reinstated, implausible patterns are not introduced, and occurrences of spurious patterns are prevented. In the second setting, we aim to generate a string that is at minimal edit distance from the original string, in addition to preserving the order of appearance and frequency of all non-sensitive patterns. To construct such a string, we propose an algorithm, ETFS-ALGO, based on solving specific instances of approximate regular expression matching. We implemented our sanitization approach that applies TFS-ALGO, PFS-ALGO, and then MCSR-ALGO, and experimentally show that it is effective and efficient. We also show that TFS-ALGO is nearly as effective at minimizing the edit distance as ETFS-ALGO, while being substantially more efficient than ETFS-ALGO.}
}


@article{DBLP:journals/tkdd/RossiACARKK21,
	author = {Ryan A. Rossi and
                  Nesreen K. Ahmed and
                  Aldo G. Carranza and
                  David Arbour and
                  Anup B. Rao and
                  Sungchul Kim and
                  Eunyee Koh},
	title = {Heterogeneous Graphlets},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {9:1--9:43},
	year = {2021},
	url = {https://doi.org/10.1145/3418773},
	doi = {10.1145/3418773},
	timestamp = {Mon, 26 Jun 2023 20:57:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RossiACARKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we introduce a generalization of graphlets to heterogeneous networks called typed graphlets. Informally, typed graphlets are small typed induced subgraphs. Typed graphlets generalize graphlets to rich heterogeneous networks as they explicitly capture the higher-order typed connectivity patterns in such networks. To address this problem, we describe a general framework for counting the occurrences of such typed graphlets. The proposed algorithms leverage a number of combinatorial relationships for different typed graphlets. For each edge, we count a few typed graphlets, and with these counts along with the combinatorial relationships, we obtain the exact counts of the other typed graphlets in o(1) constant time. Notably, the worst-case time complexity of the proposed approach matches the time complexity of the best known untyped algorithm. In addition, the approach lends itself to an efficient lock-free and asynchronous parallel implementation. While there are no existing methods for typed graphlets, there has been some work that focused on computing a different and much simpler notion called colored graphlet. The experiments confirm that our proposed approach is orders of magnitude faster and more space-efficient than methods for computing the simpler notion of colored graphlet. Unlike these methods that take hours on small networks, the proposed approach takes only seconds on large networks with millions of edges. Notably, since typed graphlet is more general than colored graphlet (and untyped graphlets), the counts of various typed graphlets can be combined to obtain the counts of the much simpler notion of colored graphlets. The proposed methods give rise to new opportunities and applications for typed graphlets.}
}


@article{DBLP:journals/tkdd/JiYYZZSF21,
	author = {Yugang Ji and
                  Mingyang Yin and
                  Hongxia Yang and
                  Jingren Zhou and
                  Vincent W. Zheng and
                  Chuan Shi and
                  Yuan Fang},
	title = {Accelerating Large-Scale Heterogeneous Interaction Graph Embedding
                  Learning via Importance Sampling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {10:1--10:23},
	year = {2021},
	url = {https://doi.org/10.1145/3418684},
	doi = {10.1145/3418684},
	timestamp = {Fri, 09 Apr 2021 18:34:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiYYZZSF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world problems, heterogeneous entities are often related to each other through multiple interactions, forming a Heterogeneous Interaction Graph (HIG). While modeling HIGs to deal with fundamental tasks, graph neural networks present an attractive opportunity that can make full use of the heterogeneity and rich semantic information by aggregating and propagating information from different types of neighborhoods. However, learning on such complex graphs, often with millions or billions of nodes, edges, and various attributes, could suffer from expensive time cost and high memory consumption. In this article, we attempt to accelerate representation learning on large-scale HIGs by adopting the importance sampling of heterogeneous neighborhoods in a batch-wise manner, which naturally fits with most batch-based optimizations. Distinct from traditional homogeneous strategies neglecting semantic types of nodes and edges, to handle the rich heterogeneous semantics within HIGs, we devise both type-dependent and type-fusion samplers where the former respectively samples neighborhoods of each type and the latter jointly samples from candidates of all types. Furthermore, to overcome the imbalance between the down-sampled and the original information, we respectively propose heterogeneous estimators including the self-normalized and the adaptive estimators to improve the robustness of our sampling strategies. Finally, we evaluate the performance of our models for node classification and link prediction on five real-world datasets, respectively. The empirical results demonstrate that our approach performs significantly better than other state-of-the-art alternatives, and is able to reduce the number of edges in computation by up to 93%, the memory cost by up to 92% and the time cost by up to 86%.}
}


@article{DBLP:journals/tkdd/XunJZ21,
	author = {Guangxu Xun and
                  Kishlay Jha and
                  Aidong Zhang},
	title = {MeSHProbeNet-P: Improving Large-scale MeSH Indexing with Personalizable
                  MeSH Probes},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {11:1--11:14},
	year = {2021},
	url = {https://doi.org/10.1145/3421713},
	doi = {10.1145/3421713},
	timestamp = {Mon, 26 Jun 2023 20:57:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XunJZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Indexing biomedical research articles with Medical Subject Headings (MeSH) can greatly facilitate biomedical research and information retrieval. Currently MeSH indexing is performed by human experts. To alleviate the time consumption and monetary cost caused by manual indexing, many automatic MeSH indexing models have been developed, such as MeSHProbeNet, DeepMeSH, and NLM’s official model Medical Text Indexer. In this article, we propose an end-to-end framework, MeSHProbeNet-P, which extends MeSHProbeNet with personalizable MeSH probes. In MeSHProbeNet-P, each MeSH probe carries certain aspects of biomedical knowledge and extracts related information from input articles. MeSHProbeNet-P is able to automatically personalize its MeSH probes for different input articles to ensure that the current MeSH probes best fit the current input article and the most informative features can be extracted from the article. We demonstrate the effectiveness of MeSHProbeNet-P in a real-world large-scale MeSH indexing challenge. MeSHProbeNet-P won the first place in the first batch of Task A in the 2019 BioASQ challenge. The result on the first test set of the challenge is reported in this article. We also provide ablation studies to show the advantages of personalizable MeSH probes.}
}


@article{DBLP:journals/tkdd/TuYWDGZ21,
	author = {Jinzheng Tu and
                  Guoxian Yu and
                  Jun Wang and
                  Carlotta Domeniconi and
                  Maozu Guo and
                  Xiangliang Zhang},
	title = {CrowdWT: Crowdsourcing via Joint Modeling of Workers and Tasks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {12:1--12:24},
	year = {2021},
	url = {https://doi.org/10.1145/3421712},
	doi = {10.1145/3421712},
	timestamp = {Mon, 28 Aug 2023 21:37:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/TuYWDGZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsourcing is a relatively inexpensive and efficient mechanism to collect annotations of data from the open Internet. Crowdsourcing workers are paid for the provided annotations, but the task requester usually has a limited budget. It is desirable to wisely assign the appropriate task to the right workers, so the overall annotation quality is maximized while the cost is reduced. In this article, we propose a novel task assignment strategy (CrowdWT) to capture the complex interactions between tasks and workers, and properly assign tasks to workers. CrowdWT first develops a Worker Bias Model (WBM) to jointly model the worker’s bias, the ground truths of tasks, and the task features. WBM constructs a mapping between task features and worker annotations to dynamically assign the task to a group of workers, who are more likely to give correct annotations for the task. CrowdWT further introduces a Task Difficulty Model (TDM), which builds a Kernel ridge regressor based on task features to quantify the intrinsic difficulty of tasks and thus to assign the difficult tasks to more reliable workers. Finally, CrowdWT combines WBM and TDM into a unified model to dynamically assign tasks to a group of workers and recall more reliable and even expert workers to annotate the difficult tasks. Our experimental results on two real-world datasets and two semi-synthetic datasets show that CrowdWT achieves high-quality answers within a limited budget, and has the best performance against competitive methods.<?vsp -1.5pt?>}
}


@article{DBLP:journals/tkdd/AzevedoMGC21,
	author = {Ricardo de Azevedo Brand{\~{a}}o and
                  Gabriel Resende Machado and
                  Ronaldo Ribeiro Goldschmidt and
                  Ricardo Choren},
	title = {A Reduced Network Traffic Method for IoT Data Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {1},
	pages = {13:1--13:23},
	year = {2021},
	url = {https://doi.org/10.1145/3423139},
	doi = {10.1145/3423139},
	timestamp = {Wed, 07 Dec 2022 23:04:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AzevedoMGC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) systems usually involve interconnected, low processing capacity, and low memory sensor nodes (devices) that collect data in several sorts of applications that interconnect people and things. In this scenario, mining tasks, such as clustering, have been commonly deployed to detect behavioral patterns from the collected data. The centralized clustering of IoT data demands high network traffic to transmit the data from the devices to a central node, where a clustering algorithm must be applied. This approach does not scale as the number of devices increases, and the amount of data grows. However, distributing the clustering process through the devices may not be a feasible approach as well, since the devices are usually simple and may not have the ability to execute complex procedures. This work proposes a centralized IoT data clustering method that demands reduced network traffic and low processing power in the devices. The proposed method uses a data grid to summarize the information at the devices before transmitting it to the central node, reducing network traffic. After the data transfer, the proposed method applies a clustering algorithm that was developed to process data in the summarized representation. Tests with seven datasets provided experimental evidence that the proposed method reduces network traffic and produces results comparable to the ones generated by DBSCAN and HDBSCAN, two robust centralized clustering algorithms.}
}


@article{DBLP:journals/tkdd/RossiBFMM21,
	author = {Andrea Rossi and
                  Denilson Barbosa and
                  Donatella Firmani and
                  Antonio Matinata and
                  Paolo Merialdo},
	title = {Knowledge Graph Embedding for Link Prediction: {A} Comparative Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {14:1--14:49},
	year = {2021},
	url = {https://doi.org/10.1145/3424672},
	doi = {10.1145/3424672},
	timestamp = {Fri, 13 Jan 2023 16:47:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/RossiBFMM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graphs (KGs) have found many applications in industrial and in academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even the largest KGs suffer from incompleteness; Link Prediction (LP) techniques address this issue by identifying missing facts among entities already in the KG. Among the recent LP techniques, those based on KG embeddings have achieved very promising performance in some benchmarks. Despite the fast-growing literature on the subject, insufficient attention has been paid to the effect of the design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are vastly more represented than others; this allows LP methods to exhibit good results by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare the effectiveness and efficiency of 18 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.}
}


@article{DBLP:journals/tkdd/Amornbunchornvej21,
	author = {Chainarong Amornbunchornvej and
                  Navaporn Surasvadi and
                  Anon Plangprasopchok and
                  Suttipong Thajchayapong},
	title = {Identifying Linear Models in Multi-Resolution Population Data Using
                  Minimum Description Length Principle to Predict Household Income},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {15:1--15:30},
	year = {2021},
	url = {https://doi.org/10.1145/3424670},
	doi = {10.1145/3424670},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Amornbunchornvej21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One shirt size cannot fit everybody, while we cannot make a unique shirt that fits perfectly for everyone because of resource limitations. This analogy is true for policy making as well. Policy makers cannot make a single policy to solve all problems for all regions because each region has its own unique issue. At the other extreme, policy makers also cannot make a policy for each small village due to resource limitations. Would it be better if we can find a set of largest regions such that the population of each region within this set has common issues and we can make a single policy for them? In this work, we propose a framework using regression analysis and Minimum Description Length (MDL) to find a set of largest areas that have common indicators, which can be used to predict household incomes efficiently. Given a set of household features, and a multi-resolution partition that represents administrative divisions, our framework reports a set C* of largest subdivisions that have a common predictive model for population-income prediction. We formalize the problem of finding C* and propose an algorithm that can find C* correctly. We use both simulation datasets as well as a real-world dataset of Thailand’s population household information to demonstrate our framework performance and application. The results show that our framework performance is better than the baseline methods. Moreover, we demonstrate that the results of our method can be used to find indicators of income prediction for many areas in Thailand. By adjusting these indicator values via policies, we expect people in these areas to gain more incomes. Hence, the policy makers will be able to make policies by using these indicators in our results as a guideline to solve low-income issues. Our framework can be used to support policy makers in making policies regarding any other dependent variable beyond income in order to combat poverty and other issues. We provide the R package, MRReg, which is the implementation of our framework in the R language. The MRReg package comes with a documentation for anyone who is interested in analyzing linear regression on multi-resolution population data.}
}


@article{DBLP:journals/tkdd/FengLGLN21,
	author = {Yi Feng and
                  Chuanyi Li and
                  Jidong Ge and
                  Bin Luo and
                  Vincent Ng},
	title = {Recommending Statutes: {A} Portable Method Based on Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {16:1--16:22},
	year = {2021},
	url = {https://doi.org/10.1145/3424671},
	doi = {10.1145/3424671},
	timestamp = {Wed, 22 Mar 2023 13:04:25 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/FengLGLN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Legal judgment prediction, which aims at predicting judgment results such as penalty, charges, and statutes for cases, has attracted much attention recently. In this article, we focus on building a recommender system to predict the associated statutes for a case given the facts of the case as input. For this purpose, we propose a two-step neural network-based machine learning framework to assist judges as well as ordinary people to reduce their effort in finding applicable statutes. The proposed model takes advantage of recurrent neural networks with a max-pooling layer to obtain contextual representations of documents, i.e., the facts associated with the cases. Moreover, an attention mechanism is used to automatically focus on the important words contributing to the prediction of statutes. In addition, we apply an encoder--decoder ranking approach to extract correlations between statutes to achieve more accurate recommendation results. We evaluate our model on a real-world dataset. Experimental results show that, compared with existing baseline methods, our method can predict statutes that are more likely to appear in real judgments.}
}


@article{DBLP:journals/tkdd/WangZ21,
	author = {Yashen Wang and
                  Huanhuan Zhang},
	title = {{HARP:} {A} Novel Hierarchical Attention Model for Relation Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {17:1--17:22},
	year = {2021},
	url = {https://doi.org/10.1145/3424673},
	doi = {10.1145/3424673},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed great advancement of representation learning (RL)-based models for the knowledge graph relation prediction task. However, they generally rely on structure information embedded in the encyclopedic knowledge graph, while the beneficial semantic information provided by lexical knowledge graph is ignored, leading the problem of shallow understanding and coarse-grained analysis for knowledge acquisition. Therefore, this article introduces concept information derived from the lexical knowledge graph (e.g., Probase), and proposes a novel Hierarchical Attention model for Relation Prediction, which consists of entity-level attention mechanism and concept-level attention mechanism, to throughly integrate multiple semantic signals. Experimental results demonstrate the efficiency of the proposed method on two benchmark datasets.}
}


@article{DBLP:journals/tkdd/ZhouZYATDH21,
	author = {Dawei Zhou and
                  Si Zhang and
                  Mehmet Yigit Yildirim and
                  Scott Alcorn and
                  Hanghang Tong and
                  Hasan Davulcu and
                  Jingrui He},
	title = {High-Order Structure Exploration on Massive Graphs: {A} Local Graph
                  Clustering Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {18:1--18:26},
	year = {2021},
	url = {https://doi.org/10.1145/3425637},
	doi = {10.1145/3425637},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhouZYATDH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modeling and exploring high-order connectivity patterns, also called network motifs, are essential for understanding the fundamental structures that control and mediate the behavior of many complex systems. For example, in social networks, triangles have been proven to play the fundamental role in understanding social network communities; in online transaction networks, detecting directed looped transactions helps identify money laundering activities; in personally identifiable information networks, the star-shaped structures may correspond to a set of synthetic identities. Despite the ubiquity of such high-order structures, many existing graph clustering methods are either not designed for the high-order connectivity patterns, or suffer from the prohibitive computational cost when modeling high-order structures in the large-scale networks. This article generalizes the challenges in multiple dimensions. First (Model), we introduce the notion of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by the user-specified high-order network structure. Second (Algorithm), we propose a novel high-order structure-preserving graph clustering framework named HOSGRAP, which partitions the graph into structure-rich clusters in polylogarithmic time with respect to the number of edges in the graph. Third (Generalization), we generalize our proposed algorithm to solve the real-world problems on various types of graphs, such as signed graphs, bipartite graphs, and multi-partite graphs. Experimental results on both synthetic and real graphs demonstrate the effectiveness and efficiency of the proposed algorithms.}
}


@article{DBLP:journals/tkdd/LiCLL21,
	author = {Chongshou Li and
                  Brenda Cheang and
                  Zhixing Luo and
                  Andrew Lim},
	title = {An Exponential Factorization Machine with Percentage Error Minimization
                  to Retail Sales Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {19:1--19:32},
	year = {2021},
	url = {https://doi.org/10.1145/3426238},
	doi = {10.1145/3426238},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiCLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article proposes a new approach to sales forecasting for new products (stock-keeping units [SKUs]) with long lead time but short product life cycle. These SKUs are usually sold for one season only, without any replenishments. An exponential factorization machine (EFM) sales forecast model is developed to solve this problem which not only takes into account SKU attributes, but also pairwise interactions. The EFM model is significantly different from the original Factorization Machines (FM) from two fold: (1) the attribute-level formulation for explanatory/input variables; and (2) exponential formulation for the positive response/output/target variable. The attribute-level formation excludes infeasible intra-attribute interactions and results in more efficient feature engineering comparing with the conventional one-hot encoding, while the exponential formulation is demonstrated more effective than the log-transformation for the positive but not skewed distributed responses. In order to estimate the parameters, percentage error squares (PES) and error squares (ES) are minimized by a proposed adaptive batch gradient descent method over the training set. To overcome the over-fitting problem, a greedy forward stepwise feature selection method is proposed to select the most useful attributes and interactions. Real-world data provided by a footwear retailer in Singapore are used for testing the proposed approach. The forecasting performance in terms of both mean absolute percentage error (MAPE) and mean absolute error (MAE) compares favorably with not only off-the-shelf models but also results reported by extant sales and demand forecasting studies. The effectiveness of the proposed approach is also demonstrated by two external public datasets. Moreover, we prove the theoretical relationships between PES and ES minimization, and present an important property of the PES minimization for regression models; that it trains models to underestimate data. This property fits the situation of sales forecasting where unit-holding cost is much greater than the unit-shortage cost (e.g., perishable products).}
}


@article{DBLP:journals/tkdd/DjenouriDL21,
	author = {Youcef Djenouri and
                  Djamel Djenouri and
                  Jerry Chun{-}Wei Lin},
	title = {Trajectory Outlier Detection: New Problems and Solutions for Smart
                  Cities},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {20:1--20:28},
	year = {2021},
	url = {https://doi.org/10.1145/3425867},
	doi = {10.1145/3425867},
	timestamp = {Thu, 23 Jun 2022 20:05:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DjenouriDL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article introduces two new problems related to trajectory outlier detection: (1) group trajectory outlier (GTO) detection and (2) deviation point detection for both individual and group of trajectory outliers. Five algorithms are proposed for the first problem by adapting DBSCAN, k nearest neighbors (kNN), and feature selection (FS). DBSCAN-GTO first applies DBSCAN to derive the micro clusters, which are considered as potential candidates. A pruning strategy based on density computation measure is then suggested to find the group of trajectory outliers. kNN-GTO recursively derives the trajectory candidates from the individual trajectory outliers and prunes them based on their density. The overall process is repeated for all individual trajectory outliers. FS-GTO considers the set of individual trajectory outliers as the set of all features, while the FS process is used to retrieve the group of trajectory outliers. The proposed algorithms are improved by incorporating ensemble learning and high-performance computing during the detection process. Moreover, we propose a general two-phase-based algorithm for detecting the deviation points, as well as a version for graphic processing units implementation using sliding windows. Experiments on a real trajectory dataset have been carried out to demonstrate the performance of the proposed approaches. The results show that they can efficiently identify useful patterns represented by group of trajectory outliers, deviation points, and that they outperform the baseline group detection algorithms.}
}


@article{DBLP:journals/tkdd/WangXBZGGXHD21,
	author = {Kafeng Wang and
                  Haoyi Xiong and
                  Jiang Bian and
                  Zhanxing Zhu and
                  Qian Gao and
                  Zhishan Guo and
                  Cheng{-}Zhong Xu and
                  Jun Huan and
                  Dejing Dou},
	title = {Sampling Sparse Representations with Randomized Measurement Langevin
                  Dynamics},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {21:1--21:21},
	year = {2021},
	url = {https://doi.org/10.1145/3427585},
	doi = {10.1145/3427585},
	timestamp = {Sun, 06 Oct 2024 21:41:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangXBZGGXHD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stochastic Gradient Langevin Dynamics (SGLD) have been widely used for Bayesian sampling from certain probability distributions, incorporating derivatives of the log-posterior. With the derivative evaluation of the log-posterior distribution, SGLD methods generate samples from the distribution through performing as a thermostats dynamics that traverses over gradient flows of the log-posterior with certainly controllable perturbation. Even when the density is not known, existing solutions still can first learn the kernel density models from the given datasets, then produce new samples using the SGLD over the kernel density derivatives. In this work, instead of exploring new samples from kernel spaces, a novel SGLD sampler, namely, Randomized Measurement Langevin Dynamics (RMLD) is proposed to sample the high-dimensional sparse representations from the spectral domain of a given dataset. Specifically, given a random measurement matrix for sparse coding, RMLD first derives a novel likelihood evaluator of the probability distribution from the loss function of LASSO, then samples from the high-dimensional distribution using stochastic Langevin dynamics with derivatives of the logarithm likelihood and Metropolis–Hastings sampling. In addition, new samples in low-dimensional measuring spaces can be regenerated using the sampled high-dimensional vectors and the measurement matrix. The algorithm analysis shows that RMLD indeed projects a given dataset into a high-dimensional Gaussian distribution with Laplacian prior, then draw new sparse representation from the dataset through performing SGLD over the distribution. Extensive experiments have been conducted to evaluate the proposed algorithm using real-world datasets. The performance comparisons on three real-world applications demonstrate the superior performance of RMLD beyond baseline methods.}
}


@article{DBLP:journals/tkdd/BelohlavekT21,
	author = {Radim Belohl{\'{a}}vek and
                  Martin Trnecka},
	title = {The 8M Algorithm from Today's Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {22:1--22:22},
	year = {2021},
	url = {https://doi.org/10.1145/3428078},
	doi = {10.1145/3428078},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BelohlavekT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We provide a detailed analysis and a first complete description of 8M—an old but virtually unknown algorithm for Boolean matrix factorization. Even though the algorithm uses a rather limited insight into the factorization problem from today’s perspective, we demonstrate that its performance is reasonably good compared to the currently available algorithms. Our analysis reveals that this is due to certain concepts employed by 8M that are not exploited by the current algorithms. We discuss the prospect of these concepts, utilize them to improve two well-known current factorization algorithms, and, furthermore, propose an improvement of 8M itself, which significantly enhances the performance of the original 8M. Our findings are illustrated by experimental evaluation.}
}


@article{DBLP:journals/tkdd/XuYGC21,
	author = {En Xu and
                  Zhiwen Yu and
                  Bin Guo and
                  Helei Cui},
	title = {Core Interest Network for Click-Through Rate Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {23:1--23:16},
	year = {2021},
	url = {https://doi.org/10.1145/3428079},
	doi = {10.1145/3428079},
	timestamp = {Sun, 02 Oct 2022 15:51:31 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XuYGC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern online advertising systems, the click-through rate (CTR) is an important index to measure the popularity of an item. It refers to the ratio of users who click on a specific advertisement to the number of total users who view it. Predicting the CTR of an item in advance can improve the accuracy of the advertisement recommendation. And it is commonly calculated based on users’ interests. Thus, extracting users’ interests is of great importance in CTR prediction tasks. In the literature, a lot of studies treat the interaction between users and items as sequential data and apply the recurrent neural network (RNN) model to extract users’ interests. However, these solutions cannot handle the case when the sequence length is relatively long, e.g., over 100. This is because of the vanishing gradient problem of RNN, i.e., the model cannot learn a users’ previous behaviors that are too far away from the current moment. To address this problem, we propose a new Core Interest Network (CIN) model to mitigate the problem of a long sequence in the CTR prediction task with sequential data. In brief, we first extract the core interests of users and then use the refined data as the input of subsequent learning tasks. Extensive evaluations on real dataset show that our CIN model can outperform the state-of-the-art solutions in terms of prediction accuracy.}
}


@article{DBLP:journals/tkdd/GhoshNMQ21,
	author = {Aindrila Ghosh and
                  Mona Nashaat and
                  James Miller and
                  Shaikh Quader},
	title = {Context-Based Evaluation of Dimensionality Reduction Algorithms -
                  Experiments and Statistical Significance Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {24:1--24:40},
	year = {2021},
	url = {https://doi.org/10.1145/3428077},
	doi = {10.1145/3428077},
	timestamp = {Thu, 23 Jun 2022 20:05:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GhoshNMQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners’ guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.}
}


@article{DBLP:journals/tkdd/LiuVHLPSM21,
	author = {Shikang Liu and
                  Fatemeh Vahedian and
                  David Hachen and
                  Omar Lizardo and
                  Christian Poellabauer and
                  Aaron Striegel and
                  Tijana Milenkovic},
	title = {Heterogeneous Network Approach to Predict Individuals' Mental Health},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {25:1--25:26},
	year = {2021},
	url = {https://doi.org/10.1145/3429446},
	doi = {10.1145/3429446},
	timestamp = {Sun, 06 Oct 2024 21:41:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuVHLPSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Depression and anxiety are critical public health issues affecting millions of people around the world. To identify individuals who are vulnerable to depression and anxiety, predictive models have been built that typically utilize data from one source. Unlike these traditional models, in this study, we leverage a rich heterogeneous dataset from the University of Notre Dame’s NetHealth study that collected individuals’ (student participants’) social interaction data via smartphones, health-related behavioral data via wearables (Fitbit), and trait data from surveys. To integrate the different types of information, we model the NetHealth data as a heterogeneous information network (HIN). Then, we redefine the problem of predicting individuals’ mental health conditions (depression or anxiety) in a novel manner, as applying to our HIN a popular paradigm of a recommender system (RS), which is typically used to predict the preference that a person would give to an item (e.g., a movie or book). In our case, the items are the individuals’ different mental health states. We evaluate four state-of-the-art RS approaches. Also, we model the prediction of individuals’ mental health as another problem type—that of node classification (NC) in our HIN, evaluating in the process four node features under logistic regression as a proof-of-concept classifier. We find that our RS and NC network methods produce more accurate predictions than a logistic regression model using the same NetHealth data in the traditional non-network fashion as well as a random-approach. Also, we find that the best of the considered RS approaches outperforms all considered NC approaches. This is the first study to integrate smartphone, wearable sensor, and survey data in a HIN manner and use RS or NC on the HIN to predict individuals’ mental health conditions.}
}


@article{DBLP:journals/tkdd/ZhouH21,
	author = {Zhengze Zhou and
                  Giles Hooker},
	title = {Unbiased Measurement of Feature Importance in Tree-Based Methods},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {26:1--26:21},
	year = {2021},
	url = {https://doi.org/10.1145/3429445},
	doi = {10.1145/3429445},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhouH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a modification that corrects for split-improvement variable importance measures in Random Forests and other tree-based methods. These methods have been shown to be biased towards increasing the importance of features with more potential splits. We show that by appropriately incorporating split-improvement as measured on out of sample data, this bias can be corrected yielding better summaries and screening tools.}
}


@article{DBLP:journals/tkdd/AlmeidaZDCC21,
	author = {Matthew Almeida and
                  Yong Zhuang and
                  Wei Ding and
                  Scott E. Crouter and
                  Ping Chen},
	title = {Mitigating Class-Boundary Label Uncertainty to Reduce Both Model Bias
                  and Variance},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {27:1--27:18},
	year = {2021},
	url = {https://doi.org/10.1145/3429447},
	doi = {10.1145/3429447},
	timestamp = {Sat, 08 Jan 2022 02:24:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AlmeidaZDCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study of model bias and variance with respect to decision boundaries is critically important in supervised learning and artificial intelligence. There is generally a tradeoff between the two, as fine-tuning of the decision boundary of a classification model to accommodate more boundary training samples (i.e., higher model complexity) may improve training accuracy (i.e., lower bias) but hurt generalization against unseen data (i.e., higher variance). By focusing on just classification boundary fine-tuning and model complexity, it is difficult to reduce both bias and variance. To overcome this dilemma, we take a different perspective and investigate a new approach to handle inaccuracy and uncertainty in the training data labels, which are inevitable in many applications where labels are conceptual entities and labeling is performed by human annotators. The process of classification can be undermined by uncertainty in the labels of the training data; extending a boundary to accommodate an inaccurately labeled point will increase both bias and variance. Our novel method can reduce both bias and variance by estimating the pointwise label uncertainty of the training set and accordingly adjusting the training sample weights such that those samples with high uncertainty are weighted down and those with low uncertainty are weighted up. In this way, uncertain samples have a smaller contribution to the objective function of the model’s learning algorithm and exert less pull on the decision boundary. In a real-world physical activity recognition case study, the data present many labeling challenges, and we show that this new approach improves model performance and reduces model variance.}
}


@article{DBLP:journals/tkdd/MunozYLSLPR21,
	author = {Mario Andr{\'{e}}s Mu{\~{n}}oz and
                  Tao Yan and
                  Matheus R. Leal and
                  Kate Smith{-}Miles and
                  Ana Carolina Lorena and
                  Gisele L. Pappa and
                  R{\^{o}}mulo Madureira Rodrigues},
	title = {An Instance Space Analysis of Regression Problems},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {28:1--28:25},
	year = {2021},
	url = {https://doi.org/10.1145/3436893},
	doi = {10.1145/3436893},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/MunozYLSLPR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The quest for greater insights into algorithm strengths and weaknesses, as revealed when studying algorithm performance on large collections of test problems, is supported by interactive visual analytics tools. A recent advance is Instance Space Analysis, which presents a visualization of the space occupied by the test datasets, and the performance of algorithms across the instance space. The strengths and weaknesses of algorithms can be visually assessed, and the adequacy of the test datasets can be scrutinized through visual analytics. This article presents the first Instance Space Analysis of regression problems in Machine Learning, considering the performance of 14 popular algorithms on 4,855 test datasets from a variety of sources. The two-dimensional instance space is defined by measurable characteristics of regression problems, selected from over 26 candidate features. It enables the similarities and differences between test instances to be visualized, along with the predictive performance of regression algorithms across the entire instance space. The purpose of creating this framework for visual analysis of an instance space is twofold: one may assess the capability and suitability of various regression techniques; meanwhile the bias, diversity, and level of difficulty of the regression problems popularly used by the community can be visually revealed. This article shows the applicability of the created regression instance space to provide insights into the strengths and weaknesses of regression algorithms, and the opportunities to diversify the benchmark test instances to support greater insights.}
}


@article{DBLP:journals/tkdd/Coscia21,
	author = {Michele Coscia},
	title = {Noise Corrected Sampling of Online Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {29:1--29:21},
	year = {2021},
	url = {https://doi.org/10.1145/3434749},
	doi = {10.1145/3434749},
	timestamp = {Sat, 08 Jan 2022 02:24:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Coscia21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a new method to perform topological network sampling. Topological network sampling is a process for extracting a subset of nodes and edges from a network, such that analyses on the sample provide results and conclusions comparable to the ones they would return if run on whole structure. We need network sampling because the largest online network datasets are accessed through low-throughput application programming interface (API) systems, rendering the collection of the whole network infeasible. Our method is inspired by the literature on network backboning, specifically the noise-corrected backbone. We select the next node to explore by following the edge we identify as the one providing the largest information gain, given the topology of the sample explored so far. We evaluate our method against the most commonly used sampling methods. We do so in a realistic framework, considering a wide array of network topologies, network analysis, and features of API systems. There is no method that can provide the best sample in all possible scenarios, thus in our results section, we show the cases in which our method performs best and the cases in which it performs worst. Overall, the noise-corrected network sampling performs well: it has the best rank average among the tested methods across a wide range of applications.}
}


@article{DBLP:journals/tkdd/SteinbussB21,
	author = {Georg Steinbuss and
                  Klemens B{\"{o}}hm},
	title = {Generating Artificial Outliers in the Absence of Genuine Ones - {A}
                  Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {30:1--30:37},
	year = {2021},
	url = {https://doi.org/10.1145/3447822},
	doi = {10.1145/3447822},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SteinbussB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By definition, outliers are rarely observed in reality, making them difficult to detect or analyze. Artificial outliers approximate such genuine outliers and can, for instance, help with the detection of genuine outliers or with benchmarking outlier-detection algorithms. The literature features different approaches to generate artificial outliers. However, systematic comparison of these approaches remains absent. This surveys and compares these approaches. We start by clarifying the terminology in the field, which varies from publication to publication, and we propose a general problem formulation. Our description of the connection of generating outliers to other research fields like experimental design or generative models frames the field of artificial outliers. Along with offering a concise description, we group the approaches by their general concepts and how they make use of genuine instances. An extensive experimental study reveals the differences between the generation approaches when ultimately being used for outlier detection. This survey shows that the existing approaches already cover a wide range of concepts underlying the generation, but also that the field still has potential for further development. Our experimental study does confirm the expectation that the quality of the generation approaches varies widely, for example, in terms of the dataset they are used on. Ultimately, to guide the choice of the generation approach in a specific context, we propose an appropriate general-decision process. In summary, this survey comprises, describes, and connects all relevant work regarding the generation of artificial outliers and may serve as a basis to guide further research in the field.}
}


@article{DBLP:journals/tkdd/ZhuLW21,
	author = {Yi Zhu and
                  Lei Li and
                  Xindong Wu},
	title = {Stacked Convolutional Sparse Auto-Encoders for Representation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {31:1--31:21},
	year = {2021},
	url = {https://doi.org/10.1145/3434767},
	doi = {10.1145/3434767},
	timestamp = {Tue, 11 Jan 2022 16:58:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning seeks to achieve excellent performance for representation learning in image datasets. However, supervised deep learning models such as convolutional neural networks require a large number of labeled image data, which is intractable in applications, while unsupervised deep learning models like stacked denoising auto-encoder cannot employ label information. Meanwhile, the redundancy of image data incurs performance degradation on representation learning for aforementioned models. To address these problems, we propose a semi-supervised deep learning framework called stacked convolutional sparse auto-encoder, which can learn robust and sparse representations from image data with fewer labeled data records. More specifically, the framework is constructed by stacking layers. In each layer, higher layer feature representations are generated by features of lower layers in a convolutional way with kernels learned by a sparse auto-encoder. Meanwhile, to solve the data redundance problem, the algorithm of Reconstruction Independent Component Analysis is designed to train on patches for sphering the input data. The label information is encoded using a Softmax Regression model for semi-supervised learning. With this framework, higher level representations are learned by layers mapping from image data. It can boost the performance of the base subsequent classifiers such as support vector machines. Extensive experiments demonstrate the superior classification performance of our framework compared to several state-of-the-art representation learning methods.}
}


@article{DBLP:journals/tkdd/SunKWWY21,
	author = {Bin Sun and
                  Dehui Kong and
                  Shaofan Wang and
                  Lichun Wang and
                  Baocai Yin},
	title = {Joint Transferable Dictionary Learning and View Adaptation for Multi-view
                  Human Action Recognition},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {32:1--32:23},
	year = {2021},
	url = {https://doi.org/10.1145/3434746},
	doi = {10.1145/3434746},
	timestamp = {Mon, 26 Apr 2021 16:14:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunKWWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view human action recognition remains a challenging problem due to large view changes. In this article, we propose a transfer learning-based framework called transferable dictionary learning and view adaptation (TDVA) model for multi-view human action recognition. In the transferable dictionary learning phase, TDVA learns a set of view-specific transferable dictionaries enabling the same actions from different views to share the same sparse representations, which can transfer features of actions from different views to an intermediate domain. In the view adaptation phase, TDVA comprehensively analyzes global, local, and individual characteristics of samples, and jointly learns balanced distribution adaptation, locality preservation, and discrimination preservation, aiming at transferring sparse features of actions of different views from the intermediate domain to a common domain. In other words, TDVA progressively bridges the distribution gap among actions from various views by these two phases. Experimental results on IXMAS, ACT42, and NUCLA action datasets demonstrate that TDVA outperforms state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/GarciarenaMS21,
	author = {Unai Garciarena and
                  Alexander Mendiburu and
                  Roberto Santana},
	title = {Towards Automatic Construction of Multi-Network Models for Heterogeneous
                  Multi-Task Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {2},
	pages = {33:1--33:23},
	year = {2021},
	url = {https://doi.org/10.1145/3434748},
	doi = {10.1145/3434748},
	timestamp = {Wed, 12 Apr 2023 18:38:33 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GarciarenaMS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task learning, as it is understood nowadays, consists of using one single model to carry out several similar tasks. From classifying hand-written characters of different alphabets to figuring out how to play several Atari games using reinforcement learning, multi-task models have been able to widen their performance range across different tasks, although these tasks are usually of a similar nature. In this work, we attempt to expand this range even further, by including heterogeneous tasks in a single learning procedure. To do so, we firstly formally define a multi-network model, identifying the necessary components and characteristics to allow different adaptations of said model depending on the tasks it is required to fulfill. Secondly, employing the formal definition as a starting point, we develop an illustrative model example consisting of three different tasks (classification, regression, and data sampling). The performance of this illustrative model is then analyzed, showing its capabilities. Motivated by the results of the analysis, we enumerate a set of open challenges and future research lines over which the full potential of the proposed model definition can be exploited.}
}


@article{DBLP:journals/tkdd/YingWWLZSHCYG21,
	author = {Shi Ying and
                  Bingming Wang and
                  Lu Wang and
                  Qingshan Li and
                  Yishi Zhao and
                  Jianga Shang and
                  Hao Huang and
                  Guoli Cheng and
                  Zhe Yang and
                  Jiangyi Geng},
	title = {An Improved KNN-Based Efficient Log Anomaly Detection Method with
                  Automatically Labeled Samples},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {34:1--34:22},
	year = {2021},
	url = {https://doi.org/10.1145/3441448},
	doi = {10.1145/3441448},
	timestamp = {Wed, 01 Mar 2023 22:21:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YingWWLZSHCYG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Logs that record system abnormal states (anomaly logs) can be regarded as outliers, and the k-Nearest Neighbor (kNN) algorithm has relatively high accuracy in outlier detection methods. Therefore, we use the kNN algorithm to detect anomalies in the log data. However, there are some problems when using the kNN algorithm to detect anomalies, three of which are: excessive vector dimension leads to inefficient kNN algorithm, unlabeled log data cannot support the kNN algorithm, and the imbalance of the number of log data distorts the classification decision of kNN algorithm. In order to solve these three problems, we propose an efficient log anomaly detection method based on an improved kNN algorithm with an automatically labeled sample set. This method first proposes a log parsing method based on N-gram and frequent pattern mining (FPM) method, which reduces the dimension of the log vector converted with Term frequency.Inverse Document Frequency (TF-IDF) technology. Then we use clustering and self-training method to get labeled log data sample set from historical logs automatically. Finally, we improve the kNN algorithm using average weighting technology, which improves the accuracy of the kNN algorithm on unbalanced samples. The method in this article is validated on six log datasets with different types.}
}


@article{DBLP:journals/tkdd/SalveMGRP21,
	author = {Andrea De Salve and
                  Paolo Mori and
                  Barbara Guidi and
                  Laura Ricci and
                  Roberto Di Pietro},
	title = {Predicting Influential Users in Online Social Network Groups},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {35:1--35:50},
	year = {2021},
	url = {https://doi.org/10.1145/3441447},
	doi = {10.1145/3441447},
	timestamp = {Sun, 16 May 2021 00:14:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SalveMGRP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of Online Social Networks (OSNs), the ever-increasing amount of information produced by their users, and the corresponding capacity to influence markets, politics, and society, have led both industrial and academic researchers to focus on how such systems could be influenced. While previous work has mainly focused on measuring current influential users, contents, or pages on the overall OSNs, the problem of predicting influencers in OSNs has remained relatively unexplored from a research perspective. Indeed, one of the main characteristics of OSNs is the ability of users to create different groups types, as well as to join groups defined by other users, in order to share information and opinions. In this article, we formulate the Influencers Prediction problem in the context of groups created in OSNs, and we define a general framework and an effective methodology to predict which users will be able to influence the behavior of the other ones in a future time period, based on historical interactions that occurred within the group. Our contribution, while rooted in solid rationale and established analytical tools, is also supported by an extensive experimental campaign. We investigate the accuracy of the predictions collecting data concerning the interactions among about 800,000 users from 18 Facebook groups belonging to different categories (i.e., News, Education, Sport, Entertainment, and Work). The achieved results show the quality and viability of our approach. For instance, we are able to predict, on average, for each group, around a third of what an ex-post analysis will show being the 10 most influential members of that group. While our contribution is interesting on its own and—to the best of our knowledge—unique, it is worth noticing that it also paves the way for further research in this field.}
}


@article{DBLP:journals/tkdd/XieZLL21,
	author = {Hong Xie and
                  Mingze Zhong and
                  Yongkun Li and
                  John C. S. Lui},
	title = {Understanding Persuasion Cascades in Online Product Rating Systems:
                  Modeling, Analysis, and Inference},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {36:1--36:29},
	year = {2021},
	url = {https://doi.org/10.1145/3440887},
	doi = {10.1145/3440887},
	timestamp = {Mon, 16 Jan 2023 09:00:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XieZLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online product rating systems have become an indispensable component for numerous web services such as Amazon, eBay, Google Play Store, and TripAdvisor. One functionality of such systems is to uncover the product quality via product ratings (or reviews) contributed by consumers. However, a well-known psychological phenomenon called “message-based persuasion” lead to “biased” product ratings in a cascading manner (we call this the persuasion cascade). This article investigates:  (1) How does the persuasion cascade influence the product quality estimation accuracy? (2) Given a real-world product rating dataset, how to infer the persuasion cascade and analyze it to draw practical insights? We first develop a mathematical model to capture key factors of a persuasion cascade. We formulate a high-order Markov chain to characterize the opinion dynamics of a persuasion cascade and prove the convergence of opinions. We further bound the product quality estimation error for a class of rating aggregation rules including the averaging scoring rule, via the matrix perturbation theory and the Chernoff bound. We also design a maximum likelihood algorithm to infer parameters of the persuasion cascade. We conduct experiments on both synthetic data and real-world data from Amazon and TripAdvisor. Experiment results show that our inference algorithm has a high accuracy. Furthermore, persuasion cascades notably exist, but the average scoring rule has a small product quality estimation error under practical scenarios.}
}


@article{DBLP:journals/tkdd/ZhangZLZ21,
	author = {Zheng Zhang and
                  Xiaofeng Zhu and
                  Guangming Lu and
                  Yudong Zhang},
	title = {Probability Ordinal-Preserving Semantic Hashing for Large-Scale Image
                  Retrieval},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {37:1--37:22},
	year = {2021},
	url = {https://doi.org/10.1145/3442204},
	doi = {10.1145/3442204},
	timestamp = {Mon, 26 Jun 2023 20:57:03 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangZLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic hashing enables computation and memory-efficient image retrieval through learning similarity-preserving binary representations. Most existing hashing methods mainly focus on preserving the piecewise class information or pairwise correlations of samples into the learned binary codes while failing to capture the mutual triplet-level ordinal structure in similarity preservation. In this article, we propose a novel Probability Ordinal-preserving Semantic Hashing (POSH) framework, which for the first time defines the ordinal-preserving hashing concept under a non-parametric Bayesian theory. Specifically, we derive the whole learning framework of the ordinal similarity-preserving hashing based on the maximum posteriori estimation, where the probabilistic ordinal similarity preservation, probabilistic quantization function, and probabilistic semantic-preserving function are jointly considered into one unified learning framework. In particular, the proposed triplet-ordering correlation preservation scheme can effectively improve the interpretation of the learned hash codes under an economical anchor-induced asymmetric graph learning model. Moreover, the sparsity-guided selective quantization function is designed to minimize the loss of space transformation, and the regressive semantic function is explored to promote the flexibility of the formulated semantics in hash code learning. The final joint learning objective is formulated to concurrently preserve the ordinal locality of original data and explore potentials of semantics for producing discriminative hash codes. Importantly, an efficient alternating optimization algorithm with the strictly proof convergence guarantee is developed to solve the resulting objective problem. Extensive experiments on several large-scale datasets validate the superiority of the proposed method against state-of-the-art hashing-based retrieval methods.}
}


@article{DBLP:journals/tkdd/ShinLOHF21,
	author = {Kijung Shin and
                  Euiwoong Lee and
                  Jinoh Oh and
                  Mohammad Hammoud and
                  Christos Faloutsos},
	title = {CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {38:1--38:30},
	year = {2021},
	url = {https://doi.org/10.1145/3441487},
	doi = {10.1145/3441487},
	timestamp = {Sun, 06 Oct 2024 21:41:28 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ShinLOHF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a graph stream, how can we estimate the number of triangles in it using multiple machines with limited storage? Specifically, how should edges be processed and sampled across the machines for rapid and accurate estimation? The count of triangles (i.e., cliques of size three) has proven useful in numerous applications, including anomaly detection, community detection, and link recommendation. For triangle counting in large and dynamic graphs, recent work has focused largely on streaming algorithms and distributed algorithms but little on their combinations for “the best of both worlds.” In this work, we propose CoCoS, a fast and accurate distributed streaming algorithm for estimating the counts of global triangles (i.e., all triangles) and local triangles incident to each node. Making one pass over the input stream, CoCoS carefully processes and stores the edges across multiple machines so that the redundant use of computational and storage resources is minimized. Compared to baselines, CoCoS is: (a) accurate: giving up to  smaller estimation error; (b) fast: up to  faster, scaling linearly with the size of the input stream; and (c) theoretically sound: yielding unbiased estimates.}
}


@article{DBLP:journals/tkdd/AtaFWSKL21,
	author = {Sezin Kircali Ata and
                  Yuan Fang and
                  Min Wu and
                  Jiaqi Shi and
                  Chee Keong Kwoh and
                  Xiaoli Li},
	title = {Multi-View Collaborative Network Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {39:1--39:18},
	year = {2021},
	url = {https://doi.org/10.1145/3441450},
	doi = {10.1145/3441450},
	timestamp = {Tue, 21 Mar 2023 21:07:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AtaFWSKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-world networks often exist with multiple views, where each view describes one type of interaction among a common set of nodes. For example, on a video-sharing network, while two user nodes are linked, if they have common favorite videos in one view, then they can also be linked in another view if they share common subscribers. Unlike traditional single-view networks, multiple views maintain different semantics to complement each other. In this article, we propose Multi-view collAborative Network Embedding (MANE), a multi-view network embedding approach to learn low-dimensional representations. Similar to existing studies, MANE hinges on diversity and collaboration—while diversity enables views to maintain their individual semantics, collaboration enables views to work together. However, we also discover a novel form of second-order collaboration that has not been explored previously, and further unify it into our framework to attain superior node representations. Furthermore, as each view often has varying importance w.r.t.\xa0different nodes, we propose MANE, an attention-based extension of MANE, to model node-wise view importance. Finally, we conduct comprehensive experiments on three public, real-world multi-view networks, and the results demonstrate that our models consistently outperform state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/WangXWGTD21,
	author = {Wei Wang and
                  Feng Xia and
                  Jian Wu and
                  Zhiguo Gong and
                  Hanghang Tong and
                  Brian D. Davison},
	title = {Scholar2vec: Vector Representation of Scholars for Lifetime Collaborator
                  Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {40:1--40:19},
	year = {2021},
	url = {https://doi.org/10.1145/3442199},
	doi = {10.1145/3442199},
	timestamp = {Sat, 30 Sep 2023 10:29:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangXWGTD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While scientific collaboration is critical for a scholar, some collaborators can be more significant than others, e.g., lifetime collaborators. It has been shown that lifetime collaborators are more influential on a scholar’s academic performance. However, little research has been done on investigating predicting such special relationships in academic networks. To this end, we propose Scholar2vec, a novel neural network embedding for representing scholar profiles. First, our approach creates scholars’ research interest vector from textual information, such as demographics, research, and influence. After bridging research interests with a collaboration network, vector representations of scholars can be gained with graph learning. Meanwhile, since scholars are occupied with various attributes, we propose to incorporate four types of scholar attributes for learning scholar vectors. Finally, the early-stage similarity sequence based on Scholar2vec is used to predict lifetime collaborators with machine learning methods. Extensive experiments on two real-world datasets show that Scholar2vec outperforms state-of-the-art methods in lifetime collaborator prediction. Our work presents a new way to measure the similarity between two scholars by vector representation, which tackles the knowledge between network embedding and academic relationship mining.}
}


@article{DBLP:journals/tkdd/BaiMZY21,
	author = {Luyi Bai and
                  Xiangnan Ma and
                  Mingcheng Zhang and
                  Wenting Yu},
	title = {TPmod: {A} Tendency-Guided Prediction Model for Temporal Knowledge
                  Graph Completion},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {41:1--41:17},
	year = {2021},
	url = {https://doi.org/10.1145/3443687},
	doi = {10.1145/3443687},
	timestamp = {Sun, 16 May 2021 00:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BaiMZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal knowledge graphs (TKGs) have become useful resources for numerous Artificial Intelligence applications, but they are far from completeness. Inferring missing events in temporal knowledge graphs is a fundamental and challenging task. However, most existing methods solely focus on entity features or consider the entities and relations in a disjoint manner. They do not integrate the features of entities and relations in their modeling process. In this paper, we propose TPmod, a tendency-guided prediction model, to predict the missing events for TKGs (extrapolation). Differing from existing works, we propose two definitions for TKGs: the Goodness of relations and the Closeness of entity pairs. More importantly, inspired by the attention mechanism, we propose a novel tendency strategy to guide our aggregated process. It integrates the features of entities and relations, and assigns varying weights to different past events. What is more, we select the Gate Recurrent Unit (GRU) as our sequential encoder to model the temporal dependency in TKGs. Besides, the Softmax function is employed to generate the final decreasing group of candidate entities. We evaluate our model on two TKG datasets: GDELT-5 and ICEWS-250. Experimental results show that our method has a significant and consistent improvement compared to state-of-the-art baselines.}
}


@article{DBLP:journals/tkdd/WangJLL21,
	author = {Jingjing Wang and
                  Wenjun Jiang and
                  Kenli Li and
                  Keqin Li},
	title = {Reducing Cumulative Errors of Incremental {CP} Decomposition in Dynamic
                  Online Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {42:1--42:33},
	year = {2021},
	url = {https://doi.org/10.1145/3441645},
	doi = {10.1145/3441645},
	timestamp = {Sun, 16 May 2021 00:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangJLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {CANDECOMP/PARAFAC (CP) decomposition is widely used in various online social network (OSN) applications. However, it is inefficient when dealing with massive and incremental data. Some incremental CP decomposition (ICP) methods have been proposed to improve the efficiency and process evolving data, by updating decomposition results according to the newly added data. The ICP methods are efficient, but inaccurate because of serious error accumulation caused by approximation in the incremental updating. To promote the wide use of ICP, we strive to reduce its cumulative errors while keeping high efficiency. We first differentiate all possible errors in ICP into two types: the cumulative reconstruction error and the prediction error. Next, we formulate two optimization problems for reducing the two errors. Then, we propose several restarting strategies to address the two problems. Finally, we test the effectiveness in three typical dynamic OSN applications. To the best of our knowledge, this is the first work on reducing the cumulative errors of the ICP methods in dynamic OSNs.}
}


@article{DBLP:journals/tkdd/WuGYC21,
	author = {Guanhao Wu and
                  Xiaofeng Gao and
                  Ge Yan and
                  Guihai Chen},
	title = {Parallel Greedy Algorithm to Multiple Influence Maximization in Social
                  Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {43:1--43:21},
	year = {2021},
	url = {https://doi.org/10.1145/3442341},
	doi = {10.1145/3442341},
	timestamp = {Sun, 03 Sep 2023 15:45:46 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuGYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence Maximization (IM) problem is to select  influential users to maximize the influence spread, which plays an important role in many real-world applications such as product recommendation, epidemic control, and network monitoring. Nowadays multiple kinds of information can propagate in online social networks simultaneously, but current literature seldom discuss about this phenomenon. Accordingly, in this article, we propose Multiple Influence Maximization (MIM) problem where multiple information can propagate in a single network with different propagation probabilities. The goal of MIM problems is to maximize the overall accumulative influence spreads of different information with the limit of seed budget . To solve MIM problems, we first propose a greedy framework to solve MIM problems which maintains an -approximate ratio. We further propose parallel algorithms based on semaphores, an inter-thread communication mechanism, which significantly improves our algorithms efficiency. Then we conduct experiments for our framework using complex social network datasets with 12k, 154k, 317k, and 1.1m nodes, and the experimental results show that our greedy framework outperforms other heuristic algorithms greatly for large influence spread and parallelization of algorithms reduces running time observably with acceptable memory overhead.}
}


@article{DBLP:journals/tkdd/YangYCLZ21,
	author = {Lei Yang and
                  Xi Yu and
                  Jiannong Cao and
                  Xuxun Liu and
                  Pan Zhou},
	title = {Exploring Deep Reinforcement Learning for Task Dispatching in Autonomous
                  On-Demand Services},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {44:1--44:23},
	year = {2021},
	url = {https://doi.org/10.1145/3442343},
	doi = {10.1145/3442343},
	timestamp = {Thu, 20 Jun 2024 15:06:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YangYCLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous on-demand services, such as GOGOX (formerly GoGoVan) in Hong Kong, provide a platform for users to request services and for suppliers to meet such demands. In such a platform, the suppliers have autonomy to accept or reject the demands to be dispatched to him/her, so it is challenging to make an online matching between demands and suppliers. Existing methods use round-based approaches to dispatch demands. In these works, the dispatching decision is based on the predicted response patterns of suppliers to demands in the current round, but they all fail to consider the impact of future demands and suppliers on the current dispatching decision. This could lead to taking a suboptimal dispatching decision from the future perspective. To solve this problem, we propose a novel demand dispatching model using deep reinforcement learning. In this model, we make each demand as an agent. The action of each agent, i.e., the dispatching decision of each demand, is determined by a centralized algorithm in a coordinated way. The model works in the following two steps. (1) It learns the demand’s expected value in each spatiotemporal state using historical transition data. (2) Based on the learned values, it conducts a Many-To-Many dispatching using a combinatorial optimization algorithm by considering both immediate rewards and expected values of demands in the next round. In order to get a higher total reward, the demands with a high expected value (short response time) in the future may be delayed to the next round. On the contrary, the demands with a low expected value (long response time) in the future would be dispatched immediately. Through extensive experiments using real-world datasets, we show that the proposed model outperforms the existing models in terms of Cancellation Rate and Average Response Time.}
}


@article{DBLP:journals/tkdd/ChengSZWC21,
	author = {Lin Cheng and
                  Yuliang Shi and
                  Kun Zhang and
                  Xinjun Wang and
                  Zhiyong Chen},
	title = {{GGATB-LSTM:} Grouping and Global Attention-based Time-aware Bidirectional
                  {LSTM} Medical Treatment Behavior Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {45:1--45:16},
	year = {2021},
	url = {https://doi.org/10.1145/3441454},
	doi = {10.1145/3441454},
	timestamp = {Sun, 16 May 2021 00:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChengSZWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In China, with the continuous development of national health insurance policies, more and more people have joined the health insurance. How to accurately predict patients future medical treatment behavior becomes a hotspot issue. The biggest challenge in this issue is how to improve the prediction performance by modeling health insurance data with high-dimensional time characteristics. At present, most of the research is to solve this issue by using Recurrent Neural Networks (RNNs) to construct an overall prediction model for the medical visit sequences. However, RNNs can not effectively solve the long-term dependence, and RNNs ignores the importance of time interval of the medical visit sequence. Additionally, the global model may lose some important content to different groups. In order to solve these problems, we propose a Grouping and Global Attention based Time-aware Bidirectional Long Short-Term Memory (GGATB-LSTM) model to achieve medical treatment behavior prediction. The model first constructs a heterogeneous information network based on health insurance data, and uses a tensor CANDECOMP/PARAFAC decomposition method to achieve similarity grouping. In terms of group prediction, a global attention and time factor are introduced to extend the bidirectional LSTM. Finally, the proposed model is evaluated by using real dataset, and conclude that GGATB-LSTM is better than other methods.}
}


@article{DBLP:journals/tkdd/LiuYCMCLZ21,
	author = {Xueyan Liu and
                  Bo Yang and
                  Hechang Chen and
                  Katarzyna Musial and
                  Hongxu Chen and
                  Yang Li and
                  Wanli Zuo},
	title = {A Scalable Redefined Stochastic Blockmodel},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {46:1--46:28},
	year = {2021},
	url = {https://doi.org/10.1145/3442589},
	doi = {10.1145/3442589},
	timestamp = {Wed, 29 Nov 2023 13:05:09 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuYCMCLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stochastic blockmodel (SBM) is a widely used statistical network representation model, with good interpretability, expressiveness, generalization, and flexibility, which has become prevalent and important in the field of network science over the last years. However, learning an optimal SBM for a given network is an NP-hard problem. This results in significant limitations when it comes to applications of SBMs in large-scale networks, because of the significant computational overhead of existing SBM models, as well as their learning methods. Reducing the cost of SBM learning and making it scalable for handling large-scale networks, while maintaining the good theoretical properties of SBM, remains an unresolved problem. In this work, we address this challenging task from a novel perspective of model redefinition. We propose a novel redefined SBM with Poisson distribution and its block-wise learning algorithm that can efficiently analyse large-scale networks. Extensive validation conducted on both artificial and real-world data shows that our proposed method significantly outperforms the state-of-the-art methods in terms of a reasonable trade-off between accuracy and scalability.1}
}


@article{DBLP:journals/tkdd/LiuGZZCHZZY21,
	author = {Yan Liu and
                  Bin Guo and
                  Daqing Zhang and
                  Djamal Zeghlache and
                  Jingmin Chen and
                  Ke Hu and
                  Sizhe Zhang and
                  Dan Zhou and
                  Zhiwen Yu},
	title = {Knowledge Transfer with Weighted Adversarial Network for Cold-Start
                  Store Site Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {47:1--47:27},
	year = {2021},
	url = {https://doi.org/10.1145/3442203},
	doi = {10.1145/3442203},
	timestamp = {Fri, 14 May 2021 15:37:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuGZZCHZZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Store site recommendation aims to predict the value of the store at candidate locations and then recommend the optimal location to the company for placing a new brick-and-mortar store. Most existing studies focus on learning machine learning or deep learning models based on large-scale training data of existing chain stores in the same city. However, the expansion of chain enterprises in new cities suffers from data scarcity issues, and these models do not work in the new city where no chain store has been placed (i.e., cold-start problem). In this article, we propose a unified approach for cold-start store site recommendation, Weighted Adversarial Network with Transferability weighting scheme (WANT), to transfer knowledge learned from a data-rich source city to a target city with no labeled data. In particular, to promote positive transfer, we develop a discriminator to diminish distribution discrepancy between source city and target city with different data distributions, which plays the minimax game with the feature extractor to learn transferable representations across cities by adversarial learning. In addition, to further reduce the risk of negative transfer, we design a transferability weighting scheme to quantify the transferability of examples in source city and reweight the contribution of relevant source examples to transfer useful knowledge. We validate WANT using a real-world dataset, and experimental results demonstrate the effectiveness of our proposed model over several state-of-the-art baseline models.}
}


@article{DBLP:journals/tkdd/NasirAMR21,
	author = {Muhammad Anis Uddin Nasir and
                  {\c{C}}igdem Aslay and
                  Gianmarco De Francisci Morales and
                  Matteo Riondato},
	title = {TipTap: Approximate Mining of Frequent \emph{k}-Subgraph Patterns
                  in Evolving Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {48:1--48:35},
	year = {2021},
	url = {https://doi.org/10.1145/3442590},
	doi = {10.1145/3442590},
	timestamp = {Sun, 16 May 2021 00:14:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NasirAMR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {S.\xa0Beckett, Waiting for Godot Given a labeled graph, the collection of k-vertex induced connected subgraph patterns that appear in the graph more frequently than a user-specified minimum threshold provides a compact summary of the characteristics of the graph, and finds applications ranging from biology to network science. However, finding these patterns is challenging, even more so for dynamic graphs that evolve over time, due to the streaming nature of the input and the exponential time complexity of the problem. We study this task in both incremental and fully-dynamic streaming settings, where arbitrary edges can be added or removed from the graph. We present TipTap, a suite of algorithms to compute high-quality approximations of the frequent k-vertex subgraphs w.r.t.\xa0a given threshold, at any time (i.e., point of the stream), with high probability. In contrast to existing state-of-the-art solutions that require iterating over the entire set of subgraphs in the vicinity of the updated edge, TipTap operates by efficiently maintaining a uniform sample of connected k-vertex subgraphs, thanks to an optimized neighborhood-exploration procedure. We provide a theoretical analysis of the proposed algorithms in terms of their unbiasedness and of the sample size needed to obtain a desired approximation quality. Our analysis relies on sample-complexity bounds that use Vapnik–Chervonenkis dimension, a key concept from statistical learning theory, which allows us to derive a sufficient sample size that is independent from the size of the graph. The results of our empirical evaluation demonstrates that TipTap returns high-quality results more efficiently and accurately than existing baselines.}
}


@article{DBLP:journals/tkdd/LinOWLH21,
	author = {Chen Lin and
                  Zhichao Ouyang and
                  Xiaoli Wang and
                  Hui Li and
                  Zhenhua Huang},
	title = {Preserve Integrity in Realtime Event Summarization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {49:1--49:29},
	year = {2021},
	url = {https://doi.org/10.1145/3442344},
	doi = {10.1145/3442344},
	timestamp = {Wed, 13 Nov 2024 14:22:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LinOWLH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online text streams such as Twitter are the major information source for users when they are looking for ongoing events. Realtime event summarization aims to generate and update coherent and concise summaries to describe the state of a given event. Due to the enormous volume of continuously coming texts, realtime event summarization has become the de facto tool to facilitate information acquisition. However, there exists a challenging yet unexplored issue in current text summarization techniques: how to preserve the integrity, i.e., the accuracy and consistency of summaries during the update process. The issue is critical since online text stream is dynamic and conflicting information could spread during the event period. For example, conflicting numbers of death and injuries might be reported after an earthquake. Such misleading information should not appear in the earthquake summary at any timestamp. In this article, we present a novel realtime event summarization framework called IAEA (i.e., Integrity-Aware Extractive-Abstractive realtime event summarization). Our key idea is to integrate an inconsistency detection module into a unified extractive–abstractive framework. In each update, important new tweets are first extracted in an extractive module, and the extraction is refined by explicitly detecting inconsistency between new tweets and previous summaries. The extractive module is able to capture the sentence-level attention which is later used by an abstractive module to obtain the word-level attention. Finally, the word-level attention is leveraged to rephrase words. We conduct comprehensive experiments on real-world datasets. To reduce efforts required for building sufficient training data, we also provide automatic labeling steps of which the effectiveness has been empirically verified. Through experiments, we demonstrate that IAEA can generate better summaries with consistent information than state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/JiangKPGHR21,
	author = {Jie Jiang and
                  Qiuqiang Kong and
                  Mark D. Plumbley and
                  Nigel Gilbert and
                  Mark Hoogendoorn and
                  Diederik M. Roijers},
	title = {Deep Learning-Based Energy Disaggregation and On/Off Detection of
                  Household Appliances},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {50:1--50:21},
	year = {2021},
	url = {https://doi.org/10.1145/3441300},
	doi = {10.1145/3441300},
	timestamp = {Tue, 07 May 2024 20:19:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiangKPGHR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy disaggregation, a.k.a. Non-Intrusive Load Monitoring, aims to separate the energy consumption of individual appliances from the readings of a mains power meter measuring the total energy consumption of, e.g., a whole house. Energy consumption of individual appliances can be useful in many applications, e.g., providing appliance-level feedback to the end users to help them understand their energy consumption and ultimately save energy. Recently, with the availability of large-scale energy consumption datasets, various neural network models such as convolutional neural networks and recurrent neural networks have been investigated to solve the energy disaggregation problem. Neural network models can learn complex patterns from large amounts of data and have been shown to outperform the traditional machine learning methods such as variants of hidden Markov models. However, current neural network methods for energy disaggregation are either computational expensive or are not capable of handling long-term dependencies. In this article, we investigate the application of the recently developed WaveNet models for the task of energy disaggregation. Based on a real-world energy dataset collected from 20 households over 2 years, we show that WaveNet models outperforms the state-of-the-art deep learning methods proposed in the literature for energy disaggregation in terms of both error measures and computational cost. On the basis of energy disaggregation, we then investigate the performance of two deep-learning based frameworks for the task of on/off detection which aims at estimating whether an appliance is in operation or not. The first framework obtains the on/off states of an appliance by binarising the predictions of a regression model trained for energy disaggregation, while the second framework obtains the on/off states of an appliance by directly training a binary classifier with binarised energy readings of the appliance serving as the target values. Based on the same dataset, we show that for the task of on/off detection the second framework, i.e., directly training a binary classifier, achieves better performance in terms of F1 score.}
}


@article{DBLP:journals/tkdd/ZhangHLLZZ21,
	author = {Haida Zhang and
                  Zengfeng Huang and
                  Xuemin Lin and
                  Zhe Lin and
                  Wenjie Zhang and
                  Ying Zhang},
	title = {Efficient and High-Quality Seeded Graph Matching: Employing Higher-order
                  Structural Information},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {51:1--51:31},
	year = {2021},
	url = {https://doi.org/10.1145/3442340},
	doi = {10.1145/3442340},
	timestamp = {Tue, 21 Mar 2023 21:07:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangHLLZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by many real applications, we study the problem of seeded graph matching. Given two graphs  and , and a small set  of pre-matched node pairs  where  and , the problem is to identify a matching between  and  growing from , such that each pair in the matching corresponds to the same underlying entity. Recent studies on efficient and effective seeded graph matching have drawn a great deal of attention and many popular methods are largely based on exploring the similarity between local structures to identify matching pairs. While these recent techniques work provably well on random graphs, their accuracy is low over many real networks. In this work, we propose to utilize higher-order neighboring information to improve the matching accuracy and efficiency. As a result, a new framework of seeded graph matching is proposed, which employs Personalized PageRank (PPR) to quantify the matching score of each node pair. To further boost the matching accuracy, we propose a novel postponing strategy, which postpones the selection of pairs that have competitors with similar matching scores. We show that the postpone strategy indeed significantly improves the matching accuracy. To improve the scalability of matching large graphs, we also propose efficient approximation techniques based on algorithms for computing PPR heavy hitters. Our comprehensive experimental studies on large-scale real datasets demonstrate that, compared with state-of-the-art approaches, our framework not only increases the precision and recall both by a significant margin but also achieves speed-up up to more than one order of magnitude.}
}


@article{DBLP:journals/tkdd/BarlaugG21,
	author = {Nils Barlaug and
                  Jon Atle Gulla},
	title = {Neural Networks for Entity Matching: {A} Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {52:1--52:37},
	year = {2021},
	url = {https://doi.org/10.1145/3442200},
	doi = {10.1145/3442200},
	timestamp = {Sun, 16 May 2021 00:14:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BarlaugG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.}
}


@article{DBLP:journals/tkdd/ChenPYT21,
	author = {Chen Chen and
                  Ruiyue Peng and
                  Lei Ying and
                  Hanghang Tong},
	title = {Fast Connectivity Minimization on Large-Scale Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {3},
	pages = {53:1--53:25},
	year = {2021},
	url = {https://doi.org/10.1145/3442342},
	doi = {10.1145/3442342},
	timestamp = {Sun, 12 Nov 2023 02:17:44 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenPYT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The connectivity of networks has been widely studied in many high-impact applications, ranging from immunization, critical infrastructure analysis, social network mining, to bioinformatic system studies. Regardless of the end application domains, connectivity minimization has always been a fundamental task to effectively control the functioning of the underlying system. The combinatorial nature of the connectivity minimization problem imposes an exponential computational complexity to find the optimal solution, which is intractable in large systems. To tackle the computational barrier, greedy algorithm is extensively used to ensure a near-optimal solution by exploiting the diminishing returns property of the problem. Despite the empirical success, the theoretical and algorithmic challenges of the problems still remain wide open. On the theoretical side, the intrinsic hardness and the approximability of the general connectivity minimization problem are still unknown except for a few special cases. On the algorithmic side, existing algorithms are hard to balance between the optimization quality and computational efficiency. In this article, we address the two challenges by (1) proving that the general connectivity minimization problem is NP-hard and  is the best approximation ratio for any polynomial algorithms, and (2) proposing the algorithm CONTAIN and its variant CONTAIN+ that can well balance optimization effectiveness and computational efficiency for eigen-function based connectivity minimization problems in large networks.}
}


@article{DBLP:journals/tkdd/WangBL21,
	author = {Yunzhe Wang and
                  George Baciu and
                  Chenhui Li},
	title = {A Layout-Based Classification Method for Visualizing Time-Varying
                  Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {54:1--54:24},
	year = {2021},
	url = {https://doi.org/10.1145/3441301},
	doi = {10.1145/3441301},
	timestamp = {Sat, 08 Jan 2022 02:24:21 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangBL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connectivity analysis between the components of large evolving systems can reveal significant patterns of interaction. The systems can be simulated by topological graph structures. However, such analysis becomes challenging on large and complex graphs. Tasks such as comparing, searching, and summarizing structures, are difficult due to the enormous number of calculations required. For time-varying graphs, the temporal dimension even intensifies the difficulty. In this article, we propose to reduce the complexity of analysis by focusing on subgraphs that are induced by closely related entities. To summarize the diverse structures of subgraphs, we build a supervised layout-based classification model. The main premise is that the graph structures can induce a unique appearance of the layout. In contrast to traditional graph theory-based and contemporary neural network-based methods of graph classification, our approach generates low costs and there is no need to learn informative graph representations. Combined with temporally stable visualizations, we can also facilitate the understanding of sub-structures and the tracking of graph evolution. The method is evaluated on two real-world datasets. The results show that our system is highly effective in carrying out visual-based analytics of large graphs.}
}


@article{DBLP:journals/tkdd/OuyangGTHXY21,
	author = {Yi Ouyang and
                  Bin Guo and
                  Xing Tang and
                  Xiuqiang He and
                  Jian Xiong and
                  Zhiwen Yu},
	title = {Mobile App Cross-Domain Recommendation with Multi-Graph Neural Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {55:1--55:21},
	year = {2021},
	url = {https://doi.org/10.1145/3442201},
	doi = {10.1145/3442201},
	timestamp = {Mon, 26 Jun 2023 20:57:02 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/OuyangGTHXY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of mobile app ecosystem, mobile apps have grown greatly popular. The explosive growth of apps makes it difficult for users to find apps that meet their interests. Therefore, it is necessary to recommend user with a personalized set of apps. However, one of the challenges is data sparsity, as users’ historical behavior data are usually insufficient. In fact, user’s behaviors from different domains in app store regarding the same apps are usually relevant. Therefore, we can alleviate the sparsity using complementary information from correlated domains. It is intuitive to model users’ behaviors using graph, and graph neural networks have shown the great power for representation learning. In this article, we propose a novel model, Deep Multi-Graph Embedding (DMGE), to learn cross-domain app embedding. Specifically, we first construct a multi-graph based on users’ behaviors from different domains, and then propose a multi-graph neural network to learn cross-domain app embedding. Particularly, we present an adaptive method to balance the weight of each domain and efficiently train the model. Finally, we achieve cross-domain app recommendation based on the learned app embedding. Extensive experiments on real-world datasets show that DMGE outperforms other state-of-art embedding methods.}
}


@article{DBLP:journals/tkdd/Dornaika21,
	author = {Fadi Dornaika},
	title = {Elastic Embedding through Graph Convolution-based Regression for Semi-supervised
                  Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {56:1--56:11},
	year = {2021},
	url = {https://doi.org/10.1145/3441456},
	doi = {10.1145/3441456},
	timestamp = {Sat, 08 Jan 2022 02:24:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Dornaika21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article introduces a scheme for semi-supervised learning by estimating a flexible non-linear data representation that exploits Spectral Graph Convolutions structure. Structured data are exploited in order to determine non-linear and linear models. The introduced scheme takes advantage of data-driven graphs at two levels. First, it incorporates manifold smoothness that is naturally encoded by the graph itself. Second, the regression model is built on the convolved data samples that are derived from the data and their associated graph. The proposed semi-supervised embedding can tackle the problem of over-fitting on neighborhood structures for image data. The proposed Graph Convolution-based Semi-supervised Embedding paves the way to new theoretical and application perspectives related to the non-linear embedding. Indeed, building flexible models that adopt convolved data samples can enhance both the data representation and the final performance of the learning system. Several experiments are conducted on six image datasets for comparing the introduced scheme with some state-of-the-art semi-supervised approaches. This empirical evaluation shows the effectiveness of the proposed embedding scheme.}
}


@article{DBLP:journals/tkdd/LiLYLSC21,
	author = {Yanni Li and
                  Bing Liu and
                  Yongbo Yu and
                  Hui Li and
                  Jiacan Sun and
                  Jiangtao Cui},
	title = {3E-LDA: Three Enhancements to Linear Discriminant Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {57:1--57:20},
	year = {2021},
	url = {https://doi.org/10.1145/3442347},
	doi = {10.1145/3442347},
	timestamp = {Sat, 20 May 2023 22:47:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLYLSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Linear discriminant analysis (LDA) is one of the important techniques for dimensionality reduction, machine learning, and pattern recognition. However, in many applications, applying the classical LDA often faces the following problems: (1) sensitivity to outliers, (2) absence of local geometric information, and (3) small sample size or matrix singularity that can result in weak robustness and efficiency. Although several researchers have attempted to address one or more of the problems, little work has been done to address all of them together to produce a more effective and efficient LDA algorithm. This article proposes 3E-LDA, an enhanced LDA algorithm, that deals with all three problems as an attempt to further improve LDA. It proposes to learn a weighted median rather than the mean of the samples to deal with (1), to embed both between-class and within-class local geometric information to deal with (2), and to calculate the projection vectors in the null space of the matrix to deal with (3). Experiments on six benchmark datasets show that these three enhancements enable 3E-LDA to markedly outperform state-of-the-art LDA baselines in both accuracy and efficiency.}
}


@article{DBLP:journals/tkdd/ZangZXY21,
	author = {Tianzi Zang and
                  Yanmin Zhu and
                  Yanan Xu and
                  Jiadi Yu},
	title = {Jointly Modeling Spatio-Temporal Dependencies and Daily Flow Correlations
                  for Crowd Flow Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {58:1--58:20},
	year = {2021},
	url = {https://doi.org/10.1145/3439346},
	doi = {10.1145/3439346},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZangZXY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowd flow prediction is a vital problem for an intelligent transportation system construction in a smart city. It plays a crucial role in traffic management and behavioral analysis, thus it has raised great attention from many researchers. However, predicting crowd flows timely and accurately is a challenging task that is affected by many complex factors such as the dependencies of adjacent regions or recent crowd flows. Existing models mainly focus on capturing such dependencies in spatial or temporal domains and fail to model relations between crowd flows of distant regions. We notice that each region has a relatively fixed daily flow and some regions (even very far away from each other) may share similar flow patterns which show strong correlations among them. In this article, we propose a novel model named Double-Encoder which follows a general encoder–decoder framework for multi-step citywide crowd flow prediction. The model consists of two encoder modules named ST-Encoder and FR-Encoder to model spatial-temporal dependencies and daily flow correlations, respectively. We conduct extensive experiments on two real-world datasets to evaluate the performance of the proposed model and show that our model consistently outperforms state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/AhmedDR21,
	author = {Nesreen K. Ahmed and
                  Nick Duffield and
                  Ryan A. Rossi},
	title = {Online Sampling of Temporal Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {59:1--59:27},
	year = {2021},
	url = {https://doi.org/10.1145/3442202},
	doi = {10.1145/3442202},
	timestamp = {Tue, 21 Mar 2023 21:07:47 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AhmedDR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal networks representing a stream of timestamped edges are seemingly ubiquitous in the real world. However, the massive size and continuous nature of these networks make them fundamentally challenging to analyze and leverage for descriptive and predictive modeling tasks. In this work, we propose a general framework for temporal network sampling with unbiased estimation. We develop online, single-pass sampling algorithms, and unbiased estimators for temporal network sampling. The proposed algorithms enable fast, accurate, and memory-efficient statistical estimation of temporal network patterns and properties. In addition, we propose a temporally decaying sampling algorithm with unbiased estimators for studying networks that evolve in continuous time, where the strength of links is a function of time, and the motif patterns are temporally weighted. In contrast to the prior notion of a △ t-temporal motif, the proposed formulation and algorithms for counting temporally weighted motifs are useful for forecasting tasks in networks such as predicting future links, or a future time-series variable of nodes and links. Finally, extensive experiments on a variety of temporal networks from different domains demonstrate the effectiveness of the proposed algorithms. A detailed ablation study is provided to understand the impact of the various components of the proposed framework.}
}


@article{DBLP:journals/tkdd/ZhaoYSKL21,
	author = {Huan Zhao and
                  Quanming Yao and
                  Yangqiu Song and
                  James T. Kwok and
                  Dik Lun Lee},
	title = {Side Information Fusion for Recommender Systems over Heterogeneous
                  Information Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {60:1--60:32},
	year = {2021},
	url = {https://doi.org/10.1145/3441446},
	doi = {10.1145/3441446},
	timestamp = {Mon, 09 Sep 2024 19:07:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoYSKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative filtering (CF) has been one of the most important and popular recommendation methods, which aims at predicting users’ preferences (ratings) based on their past behaviors. Recently, various types of side information beyond the explicit ratings users give to items, such as social connections among users and metadata of items, have been introduced into CF and shown to be useful for improving recommendation performance. However, previous works process different types of information separately, thus failing to capture the correlations that might exist across them. To address this problem, in this work, we study the application of heterogeneous information network (HIN), which offers a unifying and flexible representation of different types of side information, to enhance CF-based recommendation methods. However, we face challenging issues in HIN-based recommendation, i.e., how to capture similarities of complex semantics between users and items in a HIN, and how to effectively fuse these similarities to improve final recommendation performance. To address these issues, we apply metagraph to similarity computation and solve the information fusion problem with a “matrix factorization (MF) + factorization machine (FM)” framework. For the MF part, we obtain the user-item similarity matrix from each metagraph and then apply low-rank matrix approximation to obtain latent features for both users and items. For the FM part, we apply FM with Group lasso (FMG) on the features obtained from the MF part to train the recommending model and, at the same time, identify the useful metagraphs. Besides FMG, a two-stage method, we further propose an end-to-end method, hierarchical attention fusing, to fuse metagraph-based similarities for the final recommendation. Experimental results on four large real-world datasets show that the two proposed frameworks significantly outperform existing state-of-the-art methods in terms of recommendation performance.}
}


@article{DBLP:journals/tkdd/ZhangYZZ21,
	author = {Daokun Zhang and
                  Jie Yin and
                  Xingquan Zhu and
                  Chengqi Zhang},
	title = {Search Efficient Binary Network Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {61:1--61:27},
	year = {2021},
	url = {https://doi.org/10.1145/3436892},
	doi = {10.1145/3436892},
	timestamp = {Sat, 08 Jan 2022 02:24:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangYZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional network embedding primarily focuses on learning a continuous vector representation for each node, preserving network structure and/or node content information, such that off-the-shelf machine learning algorithms can be easily applied to the vector-format node representations for network analysis. However, the learned continuous vector representations are inefficient for large-scale similarity search, which often involves finding nearest neighbors measured by distance or similarity in a continuous vector space. In this article, we propose a search efficient binary network embedding algorithm called BinaryNE to learn a binary code for each node, by simultaneously modeling node context relations and node attribute relations through a three-layer neural network. BinaryNE learns binary node representations using a stochastic gradient descent-based online learning algorithm. The learned binary encoding not only reduces memory usage to represent each node, but also allows fast bit-wise comparisons to support faster node similarity search than using Euclidean or other distance measures. Extensive experiments and comparisons demonstrate that BinaryNE not only delivers more than 25 times faster search speed, but also provides comparable or better search quality than traditional continuous vector based network embedding methods. The binary codes learned by BinaryNE also render competitive performance on node classification and node clustering tasks. The source code of the BinaryNE algorithm is available at https://github.com/daokunzhang/BinaryNE.}
}


@article{DBLP:journals/tkdd/SongWDLW21,
	author = {Guojie Song and
                  Yun Wang and
                  Lun Du and
                  Yi Li and
                  Junshan Wang},
	title = {Network Embedding on Hierarchical Community Structure Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {62:1--62:23},
	year = {2021},
	url = {https://doi.org/10.1145/3434747},
	doi = {10.1145/3434747},
	timestamp = {Thu, 08 Aug 2024 07:48:11 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SongWDLW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network embedding is a method of learning a low-dimensional vector representation of network vertices under the condition of preserving different types of network properties. Previous studies mainly focus on preserving structural information of vertices at a particular scale, like neighbor information or community information, but cannot preserve the hierarchical community structure, which would enable the network to be easily analyzed at various scales. Inspired by the hierarchical structure of galaxies, we propose the Galaxy Network Embedding (GNE) model, which formulates an optimization problem with spherical constraints to describe the hierarchical community structure preserving network embedding. More specifically, we present an approach of embedding communities into a low-dimensional spherical surface, the center of which represents the parent community they belong to. Our experiments reveal that the representations from GNE preserve the hierarchical community structure and show advantages in several applications such as vertex multi-class classification, network visualization, and link prediction. The source code of GNE is available online.}
}


@article{DBLP:journals/tkdd/YuLL21,
	author = {Kui Yu and
                  Lin Liu and
                  Jiuyong Li},
	title = {A Unified View of Causal and Non-causal Feature Selection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {63:1--63:46},
	year = {2021},
	url = {https://doi.org/10.1145/3436891},
	doi = {10.1145/3436891},
	timestamp = {Thu, 14 Oct 2021 09:42:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YuLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we can interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-world data.}
}


@article{DBLP:journals/tkdd/YinSGHWY21,
	author = {Shuai Yin and
                  Yanfeng Sun and
                  Junbin Gao and
                  Yongli Hu and
                  Boyue Wang and
                  Baocai Yin},
	title = {Robust Image Representation via Low Rank Locality Preserving Projection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {64:1--64:22},
	year = {2021},
	url = {https://doi.org/10.1145/3434768},
	doi = {10.1145/3434768},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YinSGHWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Locality preserving projection (LPP) is a dimensionality reduction algorithm preserving the neighhorhood graph structure of data. However, the conventional LPP is sensitive to outliers existing in data. This article proposes a novel low-rank LPP model called LR-LPP. In this new model, original data are decomposed into the clean intrinsic component and noise component. Then the projective matrix is learned based on the clean intrinsic component which is encoded in low-rank features. The noise component is constrained by the ℓ1-norm which is more robust to outliers. Finally, LR-LPP model is extended to LR-FLPP in which low-dimensional feature is measured by F-norm. LR-FLPP will reduce aggregated error and weaken the effect of outliers, which will make the proposed LR-FLPP even more robust for outliers. The experimental results on public image databases demonstrate the effectiveness of the proposed LR-LPP and LR-FLPP.}
}


@article{DBLP:journals/tkdd/SteinbussB21a,
	author = {Georg Steinbuss and
                  Klemens B{\"{o}}hm},
	title = {Benchmarking Unsupervised Outlier Detection with Realistic Synthetic
                  Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {65:1--65:20},
	year = {2021},
	url = {https://doi.org/10.1145/3441453},
	doi = {10.1145/3441453},
	timestamp = {Sat, 08 Jan 2022 02:24:21 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SteinbussB21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benchmarking unsupervised outlier detection is difficult. Outliers are rare, and existing benchmark data contains outliers with various and unknown characteristics. Fully synthetic data usually consists of outliers and regular instances with clear characteristics and thus allows for a more meaningful evaluation of detection methods in principle. Nonetheless, there have only been few attempts to include synthetic data in benchmarks for outlier detection. This might be due to the imprecise notion of outliers or to the difficulty to arrive at a good coverage of different domains with synthetic data. In this work, we propose a generic process for the generation of datasets for such benchmarking. The core idea is to reconstruct regular instances from existing real-world benchmark data while generating outliers so that they exhibit insightful characteristics. We propose and describe a generic process for the benchmarking of unsupervised outlier detection, as sketched so far. We then describe three instantiations of this generic process that generate outliers with specific characteristics, like local outliers. To validate our process, we perform a benchmark with state-of-the-art detection methods and carry out experiments to study the quality of data reconstructed in this way. Next to showcasing the workflow, this confirms the usefulness of our proposed process. In particular, our process yields regular instances close to the ones from real data. Summing up, we propose and validate a new and practical process for the benchmarking of unsupervised outlier detection.}
}


@article{DBLP:journals/tkdd/LinLSNWL21,
	author = {Mingkai Lin and
                  Wenzhong Li and
                  Lynda J. Song and
                  Cam{-}Tu Nguyen and
                  Xiaoliang Wang and
                  Sanglu Lu},
	title = {{SAKE:} Estimating Katz Centrality Based on Sampling for Large-Scale
                  Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {66:1--66:21},
	year = {2021},
	url = {https://doi.org/10.1145/3441646},
	doi = {10.1145/3441646},
	timestamp = {Sun, 06 Oct 2024 21:41:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LinLSNWL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Katz centrality is a fundamental concept to measure the influence of a vertex in a social network. However, existing approaches to calculating Katz centrality in a large-scale network are unpractical and computationally expensive. In this article, we propose a novel method to estimate Katz centrality based on graph sampling techniques, which object to achieve comparable estimation accuracy of the state-of-the-arts with much lower computational complexity. Specifically, we develop a Horvitz–Thompson estimate for Katz centrality by using a multi-round sampling approach and deriving an unbiased mean value estimator. We further propose SAKE, a Sampling-based Algorithm for fast Katz centrality Estimation. We prove that the estimator calculated by SAKE is probabilistically guaranteed to be within an additive error from the exact value. Extensive evaluation experiments based on four real-world networks show that the proposed algorithm can estimate Katz centralities for partial vertices with low sampling rate, low computation time, and it works well in identifying high influence vertices in social networks.}
}


@article{DBLP:journals/tkdd/Amornbunchornvej21a,
	author = {Chainarong Amornbunchornvej and
                  Elena Zheleva and
                  Tanya Y. Berger{-}Wolf},
	title = {Variable-lag Granger Causality and Transfer Entropy for Time Series
                  Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {67:1--67:30},
	year = {2021},
	url = {https://doi.org/10.1145/3441452},
	doi = {10.1145/3441452},
	timestamp = {Sun, 25 Jul 2021 11:43:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Amornbunchornvej21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. Typical operationalizations of Granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. The assumption of fixed time delay also exists in Transfer Entropy, which is considered to be a non-linear version of Granger causality. However, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. To address this issue, we develop Variable-lag Granger causality and Variable-lag Transfer Entropy, generalizations of both Granger causality and Transfer Entropy that relax the assumption of the fixed time delay and allow causes to influence effects with arbitrary time delays. In addition, we propose methods for inferring both Variable-lag Granger causality and Transfer Entropy relations. In our approaches, we utilize an optimal warping path of Dynamic Time Warping to infer variable-lag causal relations. We demonstrate our approaches on an application for studying coordinated collective behavior and other real-world casual-inference datasets and show that our proposed approaches perform better than several existing methods in both simulated and real-world datasets. Our approaches can be applied in any domain of time series analysis. The software of this work is available in the R-CRAN package: VLTimeCausality.}
}


@article{DBLP:journals/tkdd/XiaJWXW21,
	author = {Peike Xia and
                  Wenjun Jiang and
                  Jie Wu and
                  Surong Xiao and
                  Guojun Wang},
	title = {Exploiting Temporal Dynamics in Product Reviews for Dynamic Sentiment
                  Prediction at the Aspect Level},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {68:1--68:29},
	year = {2021},
	url = {https://doi.org/10.1145/3441451},
	doi = {10.1145/3441451},
	timestamp = {Mon, 06 Mar 2023 15:44:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaJWXW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online reviews and ratings play an important role in shaping the purchase decisions of customers in e-commerce. Many researches have been done to make proper recommendations for users, by exploiting reviews, ratings, user profiles, or behaviors. However, the dynamic evolution of user preferences and item properties haven’t been fully exploited. Moreover, it lacks fine-grained studies at the aspect level. To address the above issues, we define two concepts of user maturity and item popularity, to better explore the dynamic changes for users and items. We strive to exploit fine-grained information at the aspect level and the evolution of users and items, for dynamic sentiment prediction. First, we analyze three real datasets from both the overall level and the aspect level, to discover the dynamic changes (i.e., gradual changes and sudden changes) in user aspect preferences and item aspect properties. Next, we propose a novel model of Aspect-based Sentiment Dynamic Prediction (ASDP), to dynamically capture and exploit the change patterns with uniform time intervals. We further propose the improved model ASDP+ with a bin segmentation algorithm to set the time intervals non-uniformly based on the sudden changes. Experimental results on three real-world datasets show that our work leads to significant improvements.}
}


@article{DBLP:journals/tkdd/KumarS21,
	author = {Suhansanu Kumar and
                  Hari Sundaram},
	title = {Attribute-Guided Network Sampling Mechanisms},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {69:1--69:24},
	year = {2021},
	url = {https://doi.org/10.1145/3441445},
	doi = {10.1145/3441445},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KumarS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article introduces a novel task-independent sampler for attributed networks. The problem is important because while data mining tasks on network content are common, sampling on internet-scale networks is costly. Link-trace samplers such as Snowball sampling, Forest Fire, Random Walk, and Metropolis–Hastings Random Walk are widely used for sampling from networks. The design of these attribute-agnostic samplers focuses on preserving salient properties of network structure, and are not optimized for tasks on node content. This article has three contributions. First, we propose a task-independent, attribute aware link-trace sampler grounded in Information Theory. Our sampler greedily adds to the sample the node with the most informative (i.e., surprising) neighborhood. The sampler tends to rapidly explore the attribute space, maximally reducing the surprise of unseen nodes. Second, we prove that content sampling is an NP-hard problem. A well-known algorithm best approximates the optimization solution within 1 − 1/e, but requires full access to the entire graph. Third, we show through empirical counterfactual analysis that in many real-world datasets, network structure does not hinder the performance of surprise based link-trace samplers. Experimental results over 18 real-world datasets reveal: surprise-based samplers are sample efficient and outperform the state-of-the-art attribute-agnostic samplers by a wide margin (e.g., 45% performance improvement in clustering tasks).}
}


@article{DBLP:journals/tkdd/GhasemiFM21,
	author = {Negin Ghasemi and
                  Ramin Fatourechi and
                  Saeedeh Momtazi},
	title = {User Embedding for Expert Finding in Community Question Answering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {70:1--70:16},
	year = {2021},
	url = {https://doi.org/10.1145/3441302},
	doi = {10.1145/3441302},
	timestamp = {Sat, 08 Jan 2022 02:24:20 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GhasemiFM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The number of users who have the appropriate knowledge to answer asked questions in community question answering is lower than those who ask questions. Therefore, finding expert users who can answer the questions is very crucial and useful. In this article, we propose a framework to find experts for given questions and assign them the related questions. The proposed model benefits from users’ relations in a community along with the lexical and semantic similarities between new question and existing answers. Node embedding is applied to the community graph to find similar users. Our experiments on four different Stack Exchange datasets show that adding community relations improves the performance of expert finding models.}
}


@article{DBLP:journals/tkdd/YanLLWZW21,
	author = {Ruidong Yan and
                  Yi Li and
                  Deying Li and
                  Yongcai Wang and
                  Yuqing Zhu and
                  Weili Wu},
	title = {A Stochastic Algorithm Based on Reverse Sampling Technique to Fight
                  Against the Cyberbullying},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {71:1--71:22},
	year = {2021},
	url = {https://doi.org/10.1145/3441455},
	doi = {10.1145/3441455},
	timestamp = {Sat, 30 Sep 2023 10:29:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YanLLWZW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyberbullying has caused serious consequences especially for social network users in recent years. However, the challenge is how to fight against the cyberbullying effectively from the algorithmic perspective. In this article, we study the fighting against the cyberbullying problem, i.e., identify an initial witness set with a budget to spread the positive influence to protect the users in a specific target set such that the number of cybervictim users in the target set being activated by the seed set of cyberbullying is minimized. We first formulate this problem and show its NP-hardness. We further prove that the objective function is submodular with respect to the size of witnesses set when we convert the original problem into the maximal version. Then we propose a stochastic approach to solve this maximal version problem based on the Reverse Sampling Technique with a constant factor guarantee. In addition, we provide theoretical analysis and discuss the relationship between the optimal value and the value returned by the proposed algorithm. To evaluate the proposed approach, we implement extensive experiments on synthetic and real datasets. The experimental results show our approach is superior to the comparison methods.}
}


@article{DBLP:journals/tkdd/LiHWHLC21,
	author = {Juan{-}Hui Li and
                  Ling Huang and
                  Chang{-}Dong Wang and
                  Dong Huang and
                  Jian{-}Huang Lai and
                  Pei Chen},
	title = {Attributed Network Embedding with Micro-Meso Structure},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {72:1--72:26},
	year = {2021},
	url = {https://doi.org/10.1145/3441486},
	doi = {10.1145/3441486},
	timestamp = {Tue, 12 Nov 2024 16:50:49 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiHWHLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, network embedding has received a large amount of attention in network analysis. Although some network embedding methods have been developed from different perspectives, on one hand, most of the existing methods only focus on leveraging the plain network structure, ignoring the abundant attribute information of nodes. On the other hand, for some methods integrating the attribute information, only the lower-order proximities (e.g., microscopic proximity structure) are taken into account, which may suffer if there exists the sparsity issue and the attribute information is noisy. To overcome this problem, the attribute information and mesoscopic community structure are utilized. In this article, we propose a novel network embedding method termed Attributed Network Embedding with Micro-Meso structure, which is capable of preserving both the attribute information and the structural information including the microscopic proximity structure and mesoscopic community structure. In particular, both the microscopic proximity structure and node attributes are factorized by Nonnegative Matrix Factorization (NMF), from which the low-dimensional node representations can be obtained. For the mesoscopic community structure, a community membership strength matrix is inferred by a generative model (i.e., BigCLAM) or modularity from the linkage structure, which is then factorized by NMF to obtain the low-dimensional node representations. The three components are jointly correlated by the low-dimensional node representations, from which two objective functions (i.e., ANEM_B and ANEM_M) can be defined. Two efficient alternating optimization schemes are proposed to solve the optimization problems. Extensive experiments have been conducted to confirm the superior performance of the proposed models over the state-of-the-art network embedding methods.}
}


@article{DBLP:journals/tkdd/ZhangGHM21,
	author = {Benhui Zhang and
                  Maoguo Gong and
                  Jianbin Huang and
                  Xiaoke Ma},
	title = {Clustering Heterogeneous Information Network by Joint Graph Embedding
                  and Nonnegative Matrix Factorization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {4},
	pages = {73:1--73:25},
	year = {2021},
	url = {https://doi.org/10.1145/3441449},
	doi = {10.1145/3441449},
	timestamp = {Wed, 11 Dec 2024 17:20:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangGHM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many complex systems derived from nature and society consist of multiple types of entities and heterogeneous interactions, which can be effectively modeled as heterogeneous information network (HIN). Structural analysis of heterogeneous networks is of great significance by leveraging the rich semantic information of objects and links in the heterogeneous networks. And, clustering heterogeneous networks aims to group vertices into classes, which sheds light on revealing the structure–function relations of the underlying systems. The current algorithms independently perform the feature extraction and clustering, which are criticized for not fully characterizing the structure of clusters. In this study, we propose a learning model by joint <underline>G</underline>raph <underline>E</underline>mbedding and <underline>N</underline>onnegative <underline>M</underline>atrix <underline>F</underline>actorization (aka GEjNMF), where feature extraction and clustering are simultaneously learned by exploiting the graph embedding and latent structure of networks. We formulate the objective function of GEjNMF and transform the heterogeneous network clustering problem into a constrained optimization problem, which is effectively solved by l0-norm optimization. The advantage of GEjNMF is that features are selected under the guidance of clustering, which improves the performance and saves the running time of algorithms at the same time. The experimental results on three benchmark heterogeneous networks demonstrate that GEjNMF achieves the best performance with the least running time compared with the best state-of-the-art methods. Furthermore, the proposed algorithm is robust across heterogeneous networks from various fields. The proposed model and method provide an effective alternative for heterogeneous network clustering.}
}


@article{DBLP:journals/tkdd/YaoCLLGZ21,
	author = {Liuyi Yao and
                  Zhixuan Chu and
                  Sheng Li and
                  Yaliang Li and
                  Jing Gao and
                  Aidong Zhang},
	title = {A Survey on Causal Inference},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {74:1--74:46},
	year = {2021},
	url = {https://doi.org/10.1145/3444944},
	doi = {10.1145/3444944},
	timestamp = {Sun, 02 Oct 2022 15:51:31 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YaoCLLGZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.}
}


@article{DBLP:journals/tkdd/JurdiADM21,
	author = {Wissam Al Jurdi and
                  Jacques Bou Abdo and
                  Jacques Demerjian and
                  Abdallah Makhoul},
	title = {Critique on Natural Noise in Recommender Systems},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {75:1--75:30},
	year = {2021},
	url = {https://doi.org/10.1145/3447780},
	doi = {10.1145/3447780},
	timestamp = {Sun, 02 Oct 2022 15:51:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JurdiADM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems have been upgraded, tested, and applied in many, often incomparable ways. In attempts to diligently understand user behavior in certain environments, those systems have been frequently utilized in domains like e-commerce, e-learning, and tourism. Their increasing need and popularity have allowed the existence of numerous research paths on major issues like data sparsity, cold start, malicious noise, and natural noise, which immensely limit their performance. It is typical that the quality of the data that fuel those systems should be extremely reliable. Inconsistent user information in datasets can alter the performance of recommenders, albeit running advanced personalizing algorithms. The consequences of this can be costly as such systems are employed in abundant online businesses. Successfully managing these inconsistencies results in more personalized user experiences. In this article, the previous works conducted on natural noise management in recommender datasets are thoroughly analyzed. We adequately explore the ways in which the proposed methods measure improved performances and touch on the different natural noise management techniques and the attributes of the solutions. Additionally, we test the evaluation methods employed to assess the approaches and discuss several key gaps and other improvements the field should realize in the future. Our work considers the likelihood of a modern research branch on natural noise management and recommender assessment.}
}


@article{DBLP:journals/tkdd/DuongRND21,
	author = {Quang{-}Huy Duong and
                  Heri Ramampiaro and
                  Kjetil N{\o}rv{\aa}g and
                  Thu{-}Lan Dam},
	title = {Density Guarantee on Finding Multiple Subgraphs and Subtensors},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {76:1--76:32},
	year = {2021},
	url = {https://doi.org/10.1145/3446668},
	doi = {10.1145/3446668},
	timestamp = {Tue, 07 May 2024 20:19:55 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DuongRND21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dense subregion (subgraph & subtensor) detection is a well-studied area, with a wide range of applications, and numerous efficient approaches and algorithms have been proposed. Approximation approaches are commonly used for detecting dense subregions due to the complexity of the exact methods. Existing algorithms are generally efficient for dense subtensor and subgraph detection, and can perform well in many applications. However, most of the existing works utilize the state-or-the-art greedy 2-approximation algorithm to capably provide solutions with a loose theoretical density guarantee. The main drawback of most of these algorithms is that they can estimate only one subtensor, or subgraph, at a time, with a low guarantee on its density. While some methods can, on the other hand, estimate multiple subtensors, they can give a guarantee on the density with respect to the input tensor for the first estimated subsensor only. We address these drawbacks by providing both theoretical and practical solution for estimating multiple dense subtensors in tensor data and giving a higher lower bound of the density. In particular, we guarantee and prove a higher bound of the lower-bound density of the estimated subgraph and subtensors. We also propose a novel approach to show that there are multiple dense subtensors with a guarantee on its density that is greater than the lower bound used in the state-of-the-art algorithms. We evaluate our approach with extensive experiments on several real-world datasets, which demonstrates its efficiency and feasibility.}
}


@article{DBLP:journals/tkdd/Burkhardt21,
	author = {Paul Burkhardt},
	title = {Optimal Algebraic Breadth-First Search for Sparse Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {77:1--77:19},
	year = {2021},
	url = {https://doi.org/10.1145/3446216},
	doi = {10.1145/3446216},
	timestamp = {Thu, 14 Oct 2021 09:42:37 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Burkhardt21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been a rise in the popularity of algebraic methods for graph algorithms given the development of the GraphBLAS library and other sparse matrix methods. An exemplar for these approaches is Breadth-First Search (BFS). The algebraic BFS algorithm is simply a recurrence of matrix-vector multiplications with the n × n adjacency matrix, but the many redundant operations over nonzeros ultimately lead to suboptimal performance. Therefore an optimal algebraic BFS should be of keen interest especially if it is easily integrated with existing matrix methods. Current methods, notably in the GraphBLAS, use a Sparse Matrix masked-Sparse Vector multiplication in which the input vector is kept in a sparse representation in each step of the BFS, and nonzeros in the vector are masked in subsequent steps. This has been an area of recent research in GraphBLAS and other libraries. While in theory, these masking methods are asymptotically optimal on sparse graphs, many add work that leads to suboptimal runtime. We give a new optimal, algebraic BFS for sparse graphs, thus closing a gap in the literature. Our method multiplies progressively smaller submatrices of the adjacency matrix at each step. Let n and m refer to the number of vertices and edges, respectively. On a sparse graph, our method takes O(n) algebraic operations as opposed to O(m) operations needed by theoretically optimal sparse matrix approaches. Thus, for sparse graphs, it matches the bounds of the best-known sequential algorithm, and on a Parallel Random Access Machine, it is work-optimal. Our result holds for both directed and undirected graphs. Compared to a leading GraphBLAS library, our method achieves up to 24x faster sequential time, and for parallel computation, it can be 17x faster on large graphs and 12x faster on large-diameter graphs.}
}


@article{DBLP:journals/tkdd/MauryaLM21,
	author = {Sunil Kumar Maurya and
                  Xin Liu and
                  Tsuyoshi Murata},
	title = {Graph Neural Networks for Fast Node Ranking Approximation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {78:1--78:32},
	year = {2021},
	url = {https://doi.org/10.1145/3446217},
	doi = {10.1145/3446217},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/MauryaLM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs arise naturally in numerous situations, including social graphs, transportation graphs, web graphs, protein graphs, etc. One of the important problems in these settings is to identify which nodes are important in the graph and how they affect the graph structure as a whole. Betweenness centrality and closeness centrality are two commonly used node ranking measures to find out influential nodes in the graphs in terms of information spread and connectivity. Both of these are considered as shortest path based measures as the calculations require the assumption that the information flows between the nodes via the shortest paths. However, exact calculations of these centrality measures are computationally expensive and prohibitive, especially for large graphs. Although researchers have proposed approximation methods, they are either less efficient or suboptimal or both. We propose the first graph neural network (GNN) based model to approximate betweenness and closeness centrality. In GNN, each node aggregates features of the nodes in multihop neighborhood. We use this feature aggregation scheme to model paths and learn how many nodes are reachable to a specific node. We demonstrate that our approach significantly outperforms current techniques while taking less amount of time through extensive experiments on a series of synthetic and real-world datasets. A benefit of our approach is that the model is inductive, which means it can be trained on one set of graphs and evaluated on another set of graphs with varying structures. Thus, the model is useful for both static graphs and dynamic graphs. Source code is available at https://github.com/sunilkmaurya/GNN_Ranking}
}


@article{DBLP:journals/tkdd/StefaniTU21,
	author = {Lorenzo De Stefani and
                  Erisa Terolli and
                  Eli Upfal},
	title = {Tiered Sampling: An Efficient Method for Counting Sparse Motifs in
                  Massive Graph Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {79:1--79:52},
	year = {2021},
	url = {https://doi.org/10.1145/3441299},
	doi = {10.1145/3441299},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/StefaniTU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce Tiered Sampling, a novel technique for estimating the count of sparse motifs in massive graphs whose edges are observed in a stream. Our technique requires only a single pass on the data and uses a memory of fixed size M, which can be magnitudes smaller than the number of edges. Our methods address the challenging task of counting sparse motifs—sub-graph patterns—that have a low probability of appearing in a sample of M edges in the graph, which is the maximum amount of data available to the algorithms in each step. To obtain an unbiased and low variance estimate of the count, we partition the available memory into tiers (layers) of reservoir samples. While the base layer is a standard reservoir sample of edges, other layers are reservoir samples of sub-structures of the desired motif. By storing more frequent sub-structures of the motif, we increase the probability of detecting an occurrence of the sparse motif we are counting, thus decreasing the variance and error of the estimate. While we focus on the designing and analysis of algorithms for counting 4-cliques, we present a method which allows generalizing Tiered Sampling to obtain high-quality estimates for the number of occurrence of any sub-graph of interest, while reducing the analysis effort due to specific properties of the pattern of interest. We present a complete analytical analysis and extensive experimental evaluation of our proposed method using both synthetic and real-world data. Our results demonstrate the advantage of our method in obtaining high-quality approximations for the number of 4 and 5-cliques for large graphs using a very limited amount of memory, significantly outperforming the single edge sample approach for counting sparse motifs in large scale graphs.}
}


@article{DBLP:journals/tkdd/BauerJ21,
	author = {Josef Bauer and
                  Dietmar Jannach},
	title = {Improved Customer Lifetime Value Prediction With Sequence-To-Sequence
                  Learning and Feature-Based Models},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {80:1--80:37},
	year = {2021},
	url = {https://doi.org/10.1145/3441444},
	doi = {10.1145/3441444},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BauerJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prediction of the Customer Lifetime Value (CLV) is an important asset for tool-supported marketing by customer relationship managers. Since standard methods based on purchase recency, frequency, and past profit and revenue statistics often have limited predictive power, advanced machine learning (ML) techniques were applied to this task in recent years. However, existing approaches are often not fully capable of modeling certain temporal patterns that can be commonly found in practice, such as periodic purchasing behavior of customers. To address these shortcomings, we propose a novel method for CLV prediction based on a combination of several ML techniques. At its core, our method consists of a tailored deep learning approach based on encoder–decoder sequence-to-sequence recurrent neural networks with augmented temporal convolutions. This model is then combined with gradient boosting machines (GBMs) and a set of novel features in a hybrid framework. Empirical evaluations based on real-world data from a larger e-commerce company and a public dataset from the domain of online retail show that already the sequence-based model leads to competitive performance results. Stacking it with the GBM model is synergistic and further improves accuracy, indicating that the two models capture different patterns in the data.}
}


@article{DBLP:journals/tkdd/Sanei-MehriDHT21,
	author = {Seyed{-}Vahid Sanei{-}Mehri and
                  Apurba Das and
                  Hooman Hashemi and
                  Srikanta Tirthapura},
	title = {Mining Largest Maximal Quasi-Cliques},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {81:1--81:21},
	year = {2021},
	url = {https://doi.org/10.1145/3446637},
	doi = {10.1145/3446637},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Sanei-MehriDHT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quasi-cliques are dense incomplete subgraphs of a graph that generalize the notion of cliques. Enumerating quasi-cliques from a graph is a robust way to detect densely connected structures with applications in bioinformatics and social network analysis. However, enumerating quasi-cliques in a graph is a challenging problem, even harder than the problem of enumerating cliques. We consider the enumeration of top-k degree-based quasi-cliques and make the following contributions: (1)\xa0we show that even the problem of detecting whether a given quasi-clique is maximal (i.e., not contained within another quasi-clique) is NP-hard. (2)\xa0We present a novel heuristic algorithm KernelQC to enumerate the k largest quasi-cliques in a graph. Our method is based on identifying kernels of extremely dense subgraphs within a graph, followed by growing subgraphs around these kernels, to arrive at quasi-cliques with the required densities. (3)\xa0Experimental results show that our algorithm accurately enumerates quasi-cliques from a graph, is much faster than current state-of-the-art methods for quasi-clique enumeration (often more than three orders of magnitude faster), and can scale to larger graphs than current methods.}
}


@article{DBLP:journals/tkdd/GanLZYFCY21,
	author = {Wensheng Gan and
                  Jerry Chun{-}Wei Lin and
                  Jiexiong Zhang and
                  Hongzhi Yin and
                  Philippe Fournier{-}Viger and
                  Han{-}Chieh Chao and
                  Philip S. Yu},
	title = {Utility Mining Across Multi-Dimensional Sequences},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {82:1--82:24},
	year = {2021},
	url = {https://doi.org/10.1145/3446938},
	doi = {10.1145/3446938},
	timestamp = {Sun, 02 Oct 2022 15:51:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GanLZYFCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge extraction from database is the fundamental task in database and data mining community, which has been applied to a wide range of real-world applications and situations. Different from the support-based mining models, the utility-oriented mining framework integrates the utility theory to provide more informative and useful patterns. Time-dependent sequence data are commonly seen in real life. Sequence data have been widely utilized in many applications, such as analyzing sequential user behavior on the Web, influence maximization, route planning, and targeted marketing. Unfortunately, all the existing algorithms lose sight of the fact that the processed data not only contain rich features (e.g., occur quantity, risk, and profit), but also may be associated with multi-dimensional auxiliary information, e.g., transaction sequence can be associated with purchaser profile information. In this article, we first formulate the problem of utility mining across multi-dimensional sequences, and propose a novel framework named MDUS to extract <underline>M</underline>ulti-<underline>D</underline>imensional <underline>U</underline>tility-oriented <underline>S</underline>equential useful patterns. To the best of our knowledge, this is the first study that incorporates the time-dependent sequence-order, quantitative information, utility factor, and auxiliary dimension. Two algorithms respectively named MDUSEM and MDUSSD are presented to address the formulated problem. The former algorithm is based on database transformation, and the later one performs pattern joins and a searching method to identify desired patterns across multi-dimensional sequences. Extensive experiments are carried on six real-life datasets and one synthetic dataset to show that the proposed algorithms can effectively and efficiently discover the useful knowledge from multi-dimensional sequential databases. Moreover, the MDUS framework can provide better insight, and it is more adaptable to real-life situations than the current existing models.}
}


@article{DBLP:journals/tkdd/HaoGWLYWY21,
	author = {Shaoyang Hao and
                  Bin Guo and
                  Hao Wang and
                  Yunji Liang and
                  Lina Yao and
                  Qianru Wang and
                  Zhiwen Yu},
	title = {DeepDepict: Enabling Information Rich, Personalized Product Description
                  Generation With the Deep Multiple Pointer Generator Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {83:1--83:16},
	year = {2021},
	url = {https://doi.org/10.1145/3446982},
	doi = {10.1145/3446982},
	timestamp = {Tue, 16 Apr 2024 15:32:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HaoGWLYWY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users’ interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits.}
}


@article{DBLP:journals/tkdd/GuoW21,
	author = {Jianxiong Guo and
                  Weili Wu},
	title = {Adaptive Influence Maximization: If Influential Node Unwilling to
                  Be the Seed},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {84:1--84:23},
	year = {2021},
	url = {https://doi.org/10.1145/3447396},
	doi = {10.1145/3447396},
	timestamp = {Sat, 08 Jan 2022 02:24:22 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GuoW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence maximization problem attempts to find a small subset of nodes that makes the expected influence spread maximized, which has been researched intensively before. They all assumed that each user in the seed set we select is activated successfully and then spread the influence. However, in the real scenario, not all users in the seed set are willing to be an influencer. Based on that, we consider each user associated with a probability with which we can activate her as a seed, and we can attempt to activate her many times. In this article, we study the adaptive influence maximization with multiple activations (Adaptive-IMMA) problem, where we select a node in each iteration, observe whether she accepts to be a seed, if yes, wait to observe the influence diffusion process; if no, we can attempt to activate her again with a higher cost or select another node as a seed. We model the multiple activations mathematically and define it on the domain of integer lattice. We propose a new concept, adaptive dr-submodularity, and show our Adaptive-IMMA is the problem that maximizing an adaptive monotone and dr-submodular function under the expected knapsack constraint. Adaptive dr-submodular maximization problem is never covered by any existing studies. Thus, we summarize its properties and study its approximability comprehensively, which is a non-trivial generalization of existing analysis about adaptive submodularity. Besides, to overcome the difficulty to estimate the expected influence spread, we combine our adaptive greedy policy with sampling techniques without losing the approximation ratio but reducing the time complexity. Finally, we conduct experiments on several real datasets to evaluate the effectiveness and efficiency of our proposed policies.}
}


@article{DBLP:journals/tkdd/ChengSHZ21,
	author = {Weiyu Cheng and
                  Yanyan Shen and
                  Linpeng Huang and
                  Yanmin Zhu},
	title = {Dual-Embedding based Deep Latent Factor Models for Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {85:1--85:24},
	year = {2021},
	url = {https://doi.org/10.1145/3447395},
	doi = {10.1145/3447395},
	timestamp = {Fri, 16 Jul 2021 08:51:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChengSHZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Among various recommendation methods, latent factor models are usually considered to be state-of-the-art techniques, which aim to learn user and item embeddings for predicting user-item preferences. When applying latent factor models to the recommendation with implicit feedback, the quality of embeddings always suffers from inadequate positive feedback and noisy negative feedback. Inspired by the idea of NSVD that represents users based on their interacted items, this article proposes a dual-embedding based deep latent factor method for recommendation with implicit feedback. In addition to learning a primitive embedding for a user (resp. item), we represent each user (resp. item) with an additional embedding from the perspective of the interacted items (resp. users) and propose attentive neural methods to discriminate the importance of interacted users/items for dual-embedding learning. We design two dual-embedding based deep latent factor models, DELF\xa0and DESEQ, for pure collaborative filtering and temporal collaborative filtering (i.e., sequential recommendation), respectively. The novel attempt of the proposed models is to capture each user-item interaction with four deep representations that are subtly fused for preference prediction. We conducted extensive experiments on four real-world datasets. The results verify the effectiveness of user/item dual embeddings and the superior performance of our methods on item recommendation.}
}


@article{DBLP:journals/tkdd/SharmaM21,
	author = {Shalini Sharma and
                  Angshul Majumdar},
	title = {Sequential Transform Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {86:1--86:18},
	year = {2021},
	url = {https://doi.org/10.1145/3447394},
	doi = {10.1145/3447394},
	timestamp = {Sat, 08 Jan 2022 02:24:21 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SharmaM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work proposes a new approach for dynamical modeling; we call it sequential transform learning. This is loosely based on the transform (analysis dictionary) learning formulation. This is the first work on this topic. Transform learning, was originally developed for static problems; we modify it to model dynamical systems by introducing a feedback loop. The learnt transform coefficients for the tth instant are fed back along with the t + 1st sample, thereby establishing a Markovian relationship. Furthermore, the formulation is made supervised by the label consistency cost. Our approach keeps the best of two worlds, marrying the interpretability and uncertainty measure of signal processing with the function approximation ability of neural networks. We have carried out experiments on one of the most challenging problems in dynamical modeling - stock forecasting. Benchmarking with the state-of-the-art has shown that our method excels over the rest.}
}


@article{DBLP:journals/tkdd/LiuLWWYZW21,
	author = {Kai Liu and
                  Hongbo Liu and
                  Tomas E. Ward and
                  Hua Wang and
                  Yu Yang and
                  Bo Zhang and
                  Xindong Wu},
	title = {Self-Adaptive Skeleton Approaches to Detect Self-Organized Coalitions
                  From Brain Functional Networks Through Probabilistic Mixture Models},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {87:1--87:26},
	year = {2021},
	url = {https://doi.org/10.1145/3447570},
	doi = {10.1145/3447570},
	timestamp = {Sun, 02 Oct 2022 15:51:30 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuLWWYZW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Detecting self-organized coalitions from functional networks is one of the most important ways to uncover functional mechanisms in the brain. Determining these raises well-known technical challenges in terms of scale imbalance, outliers and hard-examples. In this article, we propose a novel self-adaptive skeleton approach to detect coalitions through an approximation method based on probabilistic mixture models. The nodes in the networks are characterized in terms of robust k-order complete subgraphs (k-clique) as essential substructures. The k-clique enumeration algorithm quickly enumerates all k-cliques in a parallel manner for a given network. Then, the cliques, from max-clique down to min-clique, of each order k, are hierarchically embedded into a probabilistic mixture model. They are self-adapted to the corresponding structure density of coalitions in the brain functional networks through different order k. All the cliques are merged and evolved into robust skeletons to sustain each unbalanced coalition by eliminating outliers and separating overlaps. We call this the k-CLIque Merging Evolution (CLIME) algorithm. The experimental results illustrate that the proposed approaches are robust to density variation and coalition mixture and can enable the effective detection of coalitions from real brain functional networks. There exist potential cognitive functional relations between the regions of interest in the coalitions revealed by our methods, which suggests the approach can be usefully applied in neuroscientific studies.}
}


@article{DBLP:journals/tkdd/LingWWPMXLWJ21,
	author = {Xiang Ling and
                  Lingfei Wu and
                  Saizhuo Wang and
                  Gaoning Pan and
                  Tengfei Ma and
                  Fangli Xu and
                  Alex X. Liu and
                  Chunming Wu and
                  Shouling Ji},
	title = {Deep Graph Matching and Searching for Semantic Code Retrieval},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {88:1--88:21},
	year = {2021},
	url = {https://doi.org/10.1145/3447571},
	doi = {10.1145/3447571},
	timestamp = {Fri, 07 Jun 2024 23:10:40 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LingWWPMXLWJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Code retrieval is to find the code snippet from a large corpus of source code repositories that highly matches the query of natural language description. Recent work mainly uses natural language processing techniques to process both query texts (i.e., human natural language) and code snippets (i.e., machine programming language), however, neglecting the deep structured features of query texts and source codes, both of which contain rich semantic information. In this article, we propose an end-to-end deep graph matching and searching (DGMS) model based on graph neural networks for the task of semantic code retrieval. To this end, we first represent both natural language query texts and programming language code snippets with the unified graph-structured data, and then use the proposed graph matching and searching model to retrieve the best matching code snippet. In particular, DGMS not only captures more structural information for individual query texts or code snippets, but also learns the fine-grained similarity between them by cross-attention based semantic matching operations. We evaluate the proposed DGMS model on two public code retrieval datasets with two representative programming languages (i.e., Java and Python). Experiment results demonstrate that DGMS significantly outperforms state-of-the-art baseline models by a large margin on both datasets. Moreover, our extensive ablation studies systematically investigate and illustrate the impact of each part of DGMS.}
}


@article{DBLP:journals/tkdd/PengLSYRYH21,
	author = {Hao Peng and
                  Jianxin Li and
                  Yangqiu Song and
                  Renyu Yang and
                  Rajiv Ranjan and
                  Philip S. Yu and
                  Lifang He},
	title = {Streaming Social Event Detection and Evolution Discovery in Heterogeneous
                  Information Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {89:1--89:33},
	year = {2021},
	url = {https://doi.org/10.1145/3447585},
	doi = {10.1145/3447585},
	timestamp = {Thu, 15 Feb 2024 19:05:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PengLSYRYH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Events are happening in real world and real time, which can be planned and organized for occasions, such as social gatherings, festival celebrations, influential meetings, or sports activities. Social media platforms generate a lot of real-time text information regarding public events with different topics. However, mining social events is challenging because events typically exhibit heterogeneous texture and metadata are often ambiguous. In this article, we first design a novel event-based meta-schema to characterize the semantic relatedness of social events and then build an event-based heterogeneous information network (HIN) integrating information from external knowledge base. Second, we propose a novel Pairwise Popularity Graph Convolutional Network, named as PP-GCN, based on weighted meta-path instance similarity and textual semantic representation as inputs, to perform fine-grained social event categorization and learn the optimal weights of meta-paths in different tasks. Third, we propose a streaming social event detection and evolution discovery framework for HINs based on meta-path similarity search, historical information about meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive experiments on real-world streaming social text data are conducted to compare various social event detection and evolution discovery algorithms. Experimental results demonstrate that our proposed framework outperforms other alternative social event detection and evolution discovery techniques.}
}


@article{DBLP:journals/tkdd/YueSWBLCZ21,
	author = {Lin Yue and
                  Hao Shen and
                  Sen Wang and
                  Robert Boots and
                  Guodong Long and
                  Weitong Chen and
                  Xiaowei Zhao},
	title = {Exploring {BCI} Control in Smart Environments: Intention Recognition
                  Via {EEG} Representation Enhancement Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {90:1--90:20},
	year = {2021},
	url = {https://doi.org/10.1145/3450449},
	doi = {10.1145/3450449},
	timestamp = {Sat, 06 Jan 2024 16:57:42 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YueSWBLCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The brain–computer interface (BCI) control technology that utilizes motor imagery to perform the desired action instead of manual operation will be widely used in smart environments. However, most of the research lacks robust feature representation of multi-channel EEG series, resulting in low intention recognition accuracy. This article proposes an EEG2Image based Denoised-ConvNets (called EID) to enhance feature representation of the intention recognition task. Specifically, we perform signal decomposition, slicing, and image mapping to decrease the noise from the irrelevant frequency bands. After that, we construct the Denoised-ConvNets structure to learn the colorspace and spatial variations of image objects without cropping new training images precisely. Toward further utilizing the color and spatial transformation layers, the colorspace and colored area of image objects have been enhanced and enlarged, respectively. In the multi-classification scenario, extensive experiments on publicly available EEG datasets confirm that the proposed method has better performance than state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/LiuLLSZ21,
	author = {Huawen Liu and
                  Enhui Li and
                  Xinwang Liu and
                  Kaile Su and
                  Shichao Zhang},
	title = {Anomaly Detection With Kernel Preserving Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {91:1--91:18},
	year = {2021},
	url = {https://doi.org/10.1145/3447684},
	doi = {10.1145/3447684},
	timestamp = {Thu, 04 Apr 2024 17:05:46 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuLLSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity representation plays a central role in increasingly popular anomaly detection techniques, which have been successfully applied in various realistic scenes. Until now, many low-rank representation techniques have been introduced to measure the similarity relations of data; yet, they only concern to minimize reconstruction errors, without involving the structural information of data. Besides, the traditional low-rank representation methods often take nuclear norm as their low-rank constraints, easily yielding a suboptimal solution. To address the problems above, in this article, we propose a novel anomaly detection method, which exploits kernel preserving embedding, as well as the double nuclear norm, to explore the similarity relations of data. Based on the similarity relations, a kind of probability transition matrix is derived, and a tailored random walk is further adopted to reveal anomalies. The proposed method can not only preserve the manifold structural properties of the data, but also alleviate the suboptimal problem. To validate the superiority of our method, extensive experiments with eight popular anomaly detection algorithms were conducted on 12 widely used datasets. The experimental results show that our detection method outperformed the state-of-the-art anomaly detection algorithms in most cases.}
}


@article{DBLP:journals/tkdd/LiuHSLQLG21,
	author = {Bo Liu and
                  Xi He and
                  Mingdong Song and
                  Jianqiang Li and
                  Guangzhi Qu and
                  Jianlei Lang and
                  Rentao Gu},
	title = {A Method for Mining Granger Causality Relationship on Atmospheric
                  Visibility},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {5},
	pages = {92:1--92:16},
	year = {2021},
	url = {https://doi.org/10.1145/3447681},
	doi = {10.1145/3447681},
	timestamp = {Mon, 30 May 2022 13:49:57 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuHSLQLG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Atmospheric visibility is an indicator of atmospheric transparency and its range directly reflects the quality of the atmospheric environment. With the acceleration of industrialization and urbanization, the natural environment has suffered some damages. In recent decades, the level of atmospheric visibility shows an overall downward trend. A decrease in atmospheric visibility will lead to a higher frequency of haze, which will seriously affect people's normal life, and also have a significant negative economic impact. The causal relationship mining of atmospheric visibility can reveal the potential relation between visibility and other influencing factors, which is very important in environmental management, air pollution control and haze control. However, causality mining based on statistical methods and traditional machine learning techniques usually achieve qualitative results that are hard to measure the degree of causality accurately. This article proposed the seq2seq-LSTM Granger causality analysis method for mining the causality relationship between atmospheric visibility and its influencing factors. In the experimental part, by comparing with methods such as linear regression, random forest, gradient boosting decision tree, light gradient boosting machine, and extreme gradient boosting, it turns out that the visibility prediction accuracy based on the seq2seq-LSTM model is about 10% higher than traditional machine learning methods. Therefore, the causal relationship mining based on this method can deeply reveal the implicit relationship between them and provide theoretical support for air pollution control.}
}


@article{DBLP:journals/tkdd/PaulKSM21,
	author = {Dipanjyoti Paul and
                  Rahul Kumar and
                  Sriparna Saha and
                  Jimson Mathew},
	title = {Multi-objective Cuckoo Search-based Streaming Feature Selection for
                  Multi-label Dataset},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {93:1--93:24},
	year = {2021},
	url = {https://doi.org/10.1145/3447586},
	doi = {10.1145/3447586},
	timestamp = {Wed, 01 Sep 2021 12:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/PaulKSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The feature selection method is the process of selecting only relevant features by removing irrelevant or redundant features amongst the large number of features that are used to represent data. Nowadays, many application domains especially social media networks, generate new features continuously at different time stamps. In such a scenario, when the features are arriving in an online fashion, to cope up with the continuous arrival of features, the selection task must also have to be a continuous process. Therefore, the streaming feature selection based approach has to be incorporated, i.e., every time a new feature or a group of features arrives, the feature selection process has to be invoked. Again, in recent years, there are many application domains that generate data where samples may belong to more than one classes called multi-label dataset. The multiple labels that the instances are being associated with, may have some dependencies amongst themselves. Finding the co-relation amongst the class labels helps to select the discriminative features across multiple labels. In this article, we develop streaming feature selection methods for multi-label data where the multiple labels are reduced to a lower-dimensional space. The similar labels are grouped together before performing the selection method to improve the selection quality and to make the model time efficient. The multi-objective version of the cuckoo search-based approach is used to select the optimal feature set. The proposed method develops two versions of the streaming feature selection method: ) when the features arrive individually and ) when the features arrive in the form of a batch. Various multi-label datasets from various domains such as text, biology, and audio have been used to test the developed streaming feature selection methods. The proposed methods are compared with many previous feature selection methods and from the comparison, the superiority of using multiple objectives and label co-relation in the feature selection process can be established.}
}


@article{DBLP:journals/tkdd/CoroDV21,
	author = {Federico Coro and
                  Gianlorenzo D'Angelo and
                  Yllka Velaj},
	title = {Link Recommendation for Social Influence Maximization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {94:1--94:23},
	year = {2021},
	url = {https://doi.org/10.1145/3449023},
	doi = {10.1145/3449023},
	timestamp = {Wed, 07 Dec 2022 23:04:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/CoroDV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social link recommendation systems, like “People-you-may-know” on Facebook, “Who-to-follow” on Twitter, and “Suggested-Accounts” on Instagram assist the users of a social network in establishing new connections with other users. While these systems are becoming more and more important in the growth of social media, they tend to increase the popularity of users that are already popular. Indeed, since link recommenders aim to predict user behavior, they accelerate the creation of links that are likely to be created in the future and, consequently, reinforce social bias by suggesting few (popular) users, giving few chances to most users to create new connections and increase their popularity. In this article, we measure the popularity of a user by means of her social influence, which is her capability to influence other users’ opinions, and we propose a link recommendation algorithm that evaluates the links to suggest according to their increment in social influence instead of their likelihood of being created. In detail, we give a  factor approximation algorithm for the problem of maximizing the social influence of a given set of target users by suggesting a fixed number of new connections considering the Linear Threshold model as model for diffusion. We experimentally show that, with few new links and small computational time, our algorithm is able to increase by far the social influence of the target users. We compare our algorithm with several baselines and show that it is the most effective one in terms of increased influence.}
}


@article{DBLP:journals/tkdd/GaoXLC21,
	author = {Xiaofeng Gao and
                  Wenyi Xu and
                  Mingding Liao and
                  Guihai Chen},
	title = {Trust Prediction for Online Social Networks with Integrated Time-Aware
                  Similarity},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {95:1--95:30},
	year = {2021},
	url = {https://doi.org/10.1145/3447682},
	doi = {10.1145/3447682},
	timestamp = {Mon, 13 Jun 2022 14:32:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GaoXLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online social networks gain increasing popularity in recent years. In online social networks, trust prediction is significant for recommendations of high reputation users as well as in many other applications. In the literature, trust prediction problem can be solved by several strategies, such as matrix factorization, trust propagation, and -NN search. However, most of the existing works have not considered the possible complementarity among these mainstream strategies to optimize their effectiveness and efficiency. In this article, we propose a novel trust prediction approach named iSim: an integrated time-aware similarity-based collaborative filtering approach leveraging on user similarity, which integrates three kinds of factors to measure user similarity, including vector space similarity, time-aware matrix factorization, and propagated trust. This article is the first work in the literature employing time-aware matrix factorization and propagated trust in the study of similarity. Additionally, we use several methods like adding inverted index to reduce the time complexity of iSim, and provide its theoretical time bound. Moreover, we also provide the detailed overview and theoretical analysis of the existing works. Finally, the extensive experiments with real-world datasets show that iSim achieves great improvement for both efficiency and effectiveness over the state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/BressanLP21,
	author = {Marco Bressan and
                  Stefano Leucci and
                  Alessandro Panconesi},
	title = {Faster Motif Counting via Succinct Color Coding and Adaptive Sampling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {96:1--96:27},
	year = {2021},
	url = {https://doi.org/10.1145/3447397},
	doi = {10.1145/3447397},
	timestamp = {Wed, 01 Sep 2021 12:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BressanLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the problem of computing the distribution of induced connected subgraphs, aka graphlets or motifs, in large graphs. The current state-of-the-art algorithms estimate the motif counts via uniform sampling by leveraging the color coding technique by Alon, Yuster, and Zwick. In this work, we extend the applicability of this approach by introducing a set of algorithmic optimizations and techniques that reduce the running time and space usage of color coding and improve the accuracy of the counts. To this end, we first show how to optimize color coding to efficiently build a compact table of a representative subsample of all graphlets in the input graph. For 8-node motifs, we can build such a table in one hour for a graph with 65M nodes and 1.8B edges, which is  times larger than the state of the art. We then introduce a novel adaptive sampling scheme that breaks the “additive error barrier” of uniform sampling, guaranteeing multiplicative approximations instead of just additive ones. This allows us to count not only the most frequent motifs, but also extremely rare ones. For instance, on one graph we accurately count nearly 10.000 distinct 8-node motifs whose relative frequency is so small that uniform sampling would literally take centuries to find them. Our results show that color coding is still the most promising approach to scalable motif counting.}
}


@article{DBLP:journals/tkdd/ZengZGXH21,
	author = {Shaoning Zeng and
                  Bob Zhang and
                  Jianping Gou and
                  Yong Xu and
                  Wei Huang},
	title = {Fast and Robust Dictionary-based Classification for Image Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {97:1--97:22},
	year = {2021},
	url = {https://doi.org/10.1145/3449360},
	doi = {10.1145/3449360},
	timestamp = {Sun, 12 Nov 2023 02:17:44 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZengZGXH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dictionary-based classification has been promising in knowledge discovery from image data, due to its good performance and interpretable theoretical system. Dictionary learning effectively supports both small- and large-scale datasets, while its robustness and performance depends on the atoms of the dictionary most of the time. Empirically, using a large number of atoms is helpful to obtain a robust classification, while robustness cannot be ensured when setting a small number of atoms. However, learning a huge dictionary dramatically slows down the speed of classification, which is especially worse on the large-scale datasets. To address the problem, we propose a Fast and Robust Dictionary-based Classification (FRDC) framework, which fully utilizes the learned dictionary for classification by staging - and -norms to obtain a robust sparse representation. The new objective function, on the one hand, introduces an additional -norm term upon the conventional -norm optimization, which generates a more robust classification. On the other hand, the optimization based on both - and -norms is solved in two stages, which is much easier and faster than current solutions. In this way, even when using a limited size of dictionary, which makes sure the classification runs very fast, it still can gain higher robustness for multiple types of image data. The optimization is then theoretically analyzed in a new formulation, close but distinct to elastic-net, to prove it is crucial to improve the performance under the premise of robustness. According to our extensive experiments conducted on four image datasets for face and object classification, FRDC keeps generating a robust classification no matter whether using a small or large number of atoms. This guarantees a fast and robust dictionary-based image classification. Furthermore, when simply using deep features extracted via some popular pre-trained neural networks, it outperforms many state-of-the-art methods on the specific datasets.}
}


@article{DBLP:journals/tkdd/LiTNJZCXY21,
	author = {Chenglin Li and
                  Carrie Lu Tong and
                  Di Niu and
                  Bei Jiang and
                  Xiao Zuo and
                  Lei Cheng and
                  Jian Xiong and
                  Jianming Yang},
	title = {Similarity Embedding Networks for Robust Human Activity Recognition},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {98:1--98:17},
	year = {2021},
	url = {https://doi.org/10.1145/3448021},
	doi = {10.1145/3448021},
	timestamp = {Wed, 07 Dec 2022 23:04:50 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiTNJZCXY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning models for human activity recognition (HAR) based on sensor data have been heavily studied recently. However, the generalization ability of deep models on complex real-world HAR data is limited by the availability of high-quality labeled activity data, which are hard to obtain. In this article, we design a similarity embedding neural network that maps input sensor signals onto real vectors through carefully designed convolutional and Long Short-Term Memory (LSTM) layers. The embedding network is trained with a pairwise similarity loss, encouraging the clustering of samples from the same class in the embedded real space, and can be effectively trained on a small dataset and even on a noisy dataset with mislabeled samples. Based on the learned embeddings, we further propose both nonparametric and parametric approaches for activity recognition. Extensive evaluation based on two public datasets has shown that the proposed similarity embedding network significantly outperforms state-of-the-art deep models on HAR classification tasks, is robust to mislabeled samples in the training set, and can also be used to effectively denoise a noisy dataset.}
}


@article{DBLP:journals/tkdd/KoleySBGD21,
	author = {Paramita Koley and
                  Avirup Saha and
                  Sourangshu Bhattacharya and
                  Niloy Ganguly and
                  Abir De},
	title = {Demarcating Endogenous and Exogenous Opinion Dynamics: An Experimental
                  Design Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {99:1--99:25},
	year = {2021},
	url = {https://doi.org/10.1145/3449361},
	doi = {10.1145/3449361},
	timestamp = {Wed, 01 Sep 2021 12:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KoleySBGD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The networked opinion diffusion in online social networks is often governed by the two genres of opinions—endogenous opinions that are driven by the influence of social contacts among users, and exogenous opinions which are formed by external effects like news and feeds. Accurate demarcation of endogenous and exogenous messages offers an important cue to opinion modeling, thereby enhancing its predictive performance. In this article, we design a suite of unsupervised classification methods based on experimental design approaches, in which, we aim to select the subsets of events which minimize different measures of mean estimation error. In more detail, we first show that these subset selection tasks are NP-Hard. Then we show that the associated objective functions are weakly submodular, which allows us to cast efficient approximation algorithms with guarantees. Finally, we validate the efficacy of our proposal on various real-world datasets crawled from Twitter as well as diverse synthetic datasets. Our experiments range from validating prediction performance on unsanitized and sanitized events to checking the effect of selecting optimal subsets of various sizes. Through various experiments, we have found that our method offers a significant improvement in accuracy in terms of opinion forecasting, against several competitors.}
}


@article{DBLP:journals/tkdd/JoaristiS21,
	author = {Mikel Joaristi and
                  Edoardo Serra},
	title = {{SIR-GN:} {A} Fast Structural Iterative Representation Learning Approach
                  For Graph Nodes},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {100:1--100:39},
	year = {2021},
	url = {https://doi.org/10.1145/3450315},
	doi = {10.1145/3450315},
	timestamp = {Wed, 01 Sep 2021 12:46:18 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JoaristiS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning methods have attracted an increasing amount of attention in recent years. These methods focus on learning a numerical representation of the nodes in a graph. Learning these representations is a powerful instrument for tasks such as graph mining, visualization, and hashing. They are of particular interest because they facilitate the direct use of standard machine learning models on graphs. Graph representation learning methods can be divided into two main categories: methods preserving the connectivity information of the nodes and methods preserving nodes’ structural information. Connectivity-based methods focus on encoding relationships between nodes, with connected nodes being closer together in the resulting latent space. While methods preserving structure generate a latent space where nodes serving a similar structural function in the network are encoded close to each other, independently of them being connected or even close to each other in the graph. While there are a lot of works that focus on preserving node connectivity, only a few works focus on preserving nodes’ structure. Properly encoding nodes’ structural information is fundamental for many real-world applications as it has been demonstrated that this information can be leveraged to successfully solve many tasks where connectivity-based methods usually fail. A typical example is the task of node classification, i.e., the assignment or prediction of a particular label for a node. Current limitations of structural representation methods are their scalability, representation meaning, and no formal proof that guaranteed the preservation of structural properties. We propose a new graph representation learning method, called Structural Iterative Representation learning approach for Graph Nodes (SIR-GN). In this work, we propose two variations (SIR-GN:\xa0GMM and SIR-GN:\xa0K-Means) and show how our best variation SIR-GN:\xa0K-Means: (1) theoretically guarantees the preservation of graph structural similarities, (2) provides a clear meaning about its representation and a way to interpret it with a specifically designed attribution procedure, and (3) is scalable and fast to compute. In addition, from our experiment, we show that SIR-GN:\xa0K-Means is often better or, in the worst-case comparable than the existing structural graph representation learning methods present in the literature. Also, we empirically show its superior scalability and computational performance when compared to other existing approaches.}
}


@article{DBLP:journals/tkdd/WuPDZ21,
	author = {Man Wu and
                  Shirui Pan and
                  Lan Du and
                  Xingquan Zhu},
	title = {Learning Graph Neural Networks with Positive and Unlabeled Nodes},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {101:1--101:25},
	year = {2021},
	url = {https://doi.org/10.1145/3450316},
	doi = {10.1145/3450316},
	timestamp = {Thu, 23 Jun 2022 20:05:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuPDZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) are important tools for transductive learning tasks, such as node classification in graphs, due to their expressive power in capturing complex interdependency between nodes. To enable GNN learning, existing works typically assume that labeled nodes, from two or multiple classes, are provided, so that a discriminative classifier can be learned from the labeled data. In reality, this assumption might be too restrictive for applications, as users may only provide labels of interest in a single class for a small number of nodes. In addition, most GNN models only aggregate information from short distances (e.g., 1-hop neighbors) in each round, and fail to capture long-distance relationship in graphs. In this article, we propose a novel GNN framework, long-short distance aggregation networks, to overcome these limitations. By generating multiple graphs at different distance levels, based on the adjacency matrix, we develop a long-short distance attention model to model these graphs. The direct neighbors are captured via a short-distance attention mechanism, and neighbors with long distance are captured by a long-distance attention mechanism. Two novel risk estimators are further employed to aggregate long-short-distance networks, for PU learning and the loss is back-propagated for model learning. Experimental results on real-world datasets demonstrate the effectiveness of our algorithm.}
}


@article{DBLP:journals/tkdd/LiLCZCY21,
	author = {Dongsheng Li and
                  Haodong Liu and
                  Chao Chen and
                  Yingying Zhao and
                  Stephen M. Chu and
                  Bo Yang},
	title = {NeuSE: {A} Neural Snapshot Ensemble Method for Collaborative Filtering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {102:1--102:20},
	year = {2021},
	url = {https://doi.org/10.1145/3450526},
	doi = {10.1145/3450526},
	timestamp = {Sun, 04 Aug 2024 19:52:24 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLCZCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In collaborative filtering (CF) algorithms, the optimal models are usually learned by globally minimizing the empirical risks averaged over all the observed data. However, the global models are often obtained via a performance tradeoff among users/items, i.e., not all users/items are perfectly fitted by the global models due to the hard non-convex optimization problems in CF algorithms. Ensemble learning can address this issue by learning multiple diverse models but usually suffer from efficiency issue on large datasets or complex algorithms. In this article, we keep the intermediate models obtained during global model learning as the snapshot models, and then adaptively combine the snapshot models for individual user-item pairs using a memory network-based method. Empirical studies on three real-world datasets show that the proposed method can extensively and significantly improve the accuracy (up to 15.9% relatively) when applied to a variety of existing collaborative filtering methods.}
}


@article{DBLP:journals/tkdd/DengCFJST21,
	author = {Jinliang Deng and
                  Xiusi Chen and
                  Zipei Fan and
                  Renhe Jiang and
                  Xuan Song and
                  Ivor W. Tsang},
	title = {The Pulse of Urban Transport: Exploring the Co-evolving Pattern for
                  Spatio-temporal Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {103:1--103:25},
	year = {2021},
	url = {https://doi.org/10.1145/3450528},
	doi = {10.1145/3450528},
	timestamp = {Tue, 07 May 2024 20:19:56 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DengCFJST21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transportation demand forecasting is a topic of large practical value. However, the model that fits the demand of one transportation by only considering the historical data of its own could be vulnerable since random fluctuations could easily impact the modeling. On the other hand, common factors like time and region attribute, drive the evolution demand of different transportation, leading to a co-evolving intrinsic property between different kinds of transportation. In this work, we focus on exploring the co-evolution between different modes of transport, e.g., taxi demand and shared-bike demand. Two significant challenges impede the discovery of the co-evolving pattern: (1) diversity of the co-evolving correlation, which varies from region to region and time to time. (2) Multi-modal data fusion. Taxi demand and shared-bike demand are time-series data, which have different representations with the external factors. Moreover, the distribution of taxi demand and bike demand are not identical. To overcome these challenges, we propose a novel method, known as co-evolving spatial temporal neural network (CEST). CEST learns a multi-view demand representation for each mode of transport, extracts the co-evolving pattern, then predicts the demand for the target transportation based on multi-scale representation, which includes fine-scale demand information and coarse-scale pattern information. We conduct extensive experiments to validate the superiority of our model over the state-of-art models.}
}


@article{DBLP:journals/tkdd/WangZLZ21,
	author = {Yashen Wang and
                  Huanhuan Zhang and
                  Zhirun Liu and
                  Qiang Zhou},
	title = {Hierarchical Concept-Driven Language Model},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {104:1--104:22},
	year = {2021},
	url = {https://doi.org/10.1145/3451167},
	doi = {10.1145/3451167},
	timestamp = {Tue, 17 Aug 2021 21:08:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-to-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-to-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-the-art Transformer-based models.}
}


@article{DBLP:journals/tkdd/AlarteS21,
	author = {Juli{\'{a}}n Alarte and
                  Josep Silva},
	title = {Page-Level Main Content Extraction From Heterogeneous Webpages},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {105:1--105:105},
	year = {2021},
	url = {https://doi.org/10.1145/3451168},
	doi = {10.1145/3451168},
	timestamp = {Sun, 06 Oct 2024 21:41:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/AlarteS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The main content of a webpage is often surrounded by other boilerplate elements related to the template, such as menus, advertisements, copyright notices, and comments. For crawlers and indexers, isolating the main content from the template and other noisy information is an essential task, because processing and storing noisy information produce a waste of resources such as bandwidth, storage space, and computing time. Besides, the detection and extraction of the main content is useful in different areas, such as data mining, web summarization, and content adaptation to low resolutions. This work introduces a new technique for main content extraction. In contrast to most techniques, this technique not only extracts text, but also other types of content, such as images, and animations. It is a Document Object Model-based page-level technique, thus it only needs to load one single webpage to extract the main content. As a consequence, it is efficient enough as to be used online (in real-time). We have empirically evaluated the technique using a suite of real heterogeneous benchmarks producing very good results compared with other well-known content extraction techniques.}
}


@article{DBLP:journals/tkdd/NettasingheK21,
	author = {Buddhika Nettasinghe and
                  Vikram Krishnamurthy},
	title = {Maximum Likelihood Estimation of Power-law Degree Distributions via
                  Friendship Paradox-based Sampling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {106:1--106:28},
	year = {2021},
	url = {https://doi.org/10.1145/3451166},
	doi = {10.1145/3451166},
	timestamp = {Tue, 17 Aug 2021 21:08:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NettasingheK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article considers the problem of estimating a power-law degree distribution of an undirected network using sampled data. Although power-law degree distributions are ubiquitous in nature, the widely used parametric methods for estimating them\xa0(e.g.,\xa0linear regression on double-logarithmic axes and maximum likelihood estimation with uniformly sampled nodes) suffer from the large variance introduced by the lack of data-points from the tail portion of the power-law degree distribution. As a solution, we present a novel maximum likelihood estimation approach that exploits the friendship paradox to sample more efficiently from the tail of the degree distribution. We analytically show that the proposed method results in a smaller bias, variance and a Cramèr–Rao lower bound compared to the vanilla maximum likelihood estimate obtained with uniformly sampled nodes\xa0(which is the most commonly used method in literature). Detailed numerical and empirical results are presented to illustrate the performance of the proposed method under different conditions and how it compares with alternative methods. We also show that the proposed method and its desirable properties (i.e.,\xa0smaller bias, variance, and Cramèr–Rao lower bound compared to vanilla method based on uniform samples) extend to parametric degree distributions other than the power-law such as exponential degree distributions as well. All the numerical and empirical results are reproducible and the code is publicly available on Github.}
}


@article{DBLP:journals/tkdd/AnaissiSZ21,
	author = {Ali Anaissi and
                  Basem Suleiman and
                  Seid Miad Zandavi},
	title = {Online Tensor-Based Learning Model for Structural Damage Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {107:1--107:18},
	year = {2021},
	url = {https://doi.org/10.1145/3451217},
	doi = {10.1145/3451217},
	timestamp = {Mon, 03 Jan 2022 22:12:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AnaissiSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The online analysis of multi-way data stored in a tensor  has become an essential tool for capturing the underlying structures and extracting the sensitive features that can be used to learn a predictive model. However, data distributions often evolve with time and a current predictive model may not be sufficiently representative in the future. Therefore, incrementally updating the tensor-based features and model coefficients are required in such situations. A new efficient tensor-based feature extraction, named Nesterov Stochastic Gradient Descent (NeSGD), is proposed for online  (CP) decomposition. According to the new features obtained from the resultant matrices of NeSGD, a new criterion is triggered for the updated process of the online predictive model. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets shows that our methods provide more accurate results compared with existing online tensor analysis and model learning. The results showed that the proposed methods significantly improved the classification error rates, were able to assimilate the changes in the positive data distribution over time, and maintained a high predictive accuracy in all case studies.}
}


@article{DBLP:journals/tkdd/WangLLXXL21,
	author = {Rui Wang and
                  Yongkun Li and
                  Shuai Lin and
                  Hong Xie and
                  Yinlong Xu and
                  John C. S. Lui},
	title = {On Modeling Influence Maximization in Social Activity Networks under
                  General Settings},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {108:1--108:28},
	year = {2021},
	url = {https://doi.org/10.1145/3451218},
	doi = {10.1145/3451218},
	timestamp = {Mon, 27 Nov 2023 13:31:32 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLLXXL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding the set of most influential users in online social networks (OSNs) to trigger the largest influence cascade is meaningful, e.g., companies may leverage the “word-of-mouth” effect to trigger a large cascade of purchases by offering free samples/discounts to those most influential users. This task is usually modeled as an influence maximization problem, and it has been widely studied in the past decade. However, considering that users in OSNs may participate in various online activities, e.g., joining discussion groups and commenting on same pages or products, influence diffusion through online activities becomes even more significant. In this article, we study the impact of online activities by formulating social-activity networks which contain both users and online activities, and thus induce two types of weighted edges, i.e., edges between users and edges between users and activities. To address the computation challenge, we define an influence centrality via random walks, and use the Monte Carlo framework to efficiently estimate the centrality. Furthermore, we develop a greedy-based algorithm with novel optimizations to find the most influential users for node recommendation. Experiments on real-world datasets show that our approach is very computationally efficient under different influence models, and also achieves larger influence spread by considering online activities.}
}


@article{DBLP:journals/tkdd/ChenSX21,
	author = {Zhe Chen and
                  Aixin Sun and
                  Xiaokui Xiao},
	title = {Incremental Community Detection on Large Complex Attributed Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {109:1--109:20},
	year = {2021},
	url = {https://doi.org/10.1145/3451216},
	doi = {10.1145/3451216},
	timestamp = {Sat, 14 Dec 2024 21:39:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenSX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection on network data is a fundamental task, and has many applications in industry. Network data in industry can be very large, with incomplete and complex attributes, and more importantly, growing. This calls for a community detection technique that is able to handle both attribute and topological information on large scale networks, and also is incremental. In this article, we propose inc-AGGMMR, an incremental community detection framework that is able to effectively address the challenges that come from scalability, mixed attributes, incomplete values, and evolving of the network. Through construction of augmented graph, we map attributes into the network by introducing attribute centers and belongingness edges. The communities are then detected by modularity maximization. During this process, we adjust the weights of belongingness edges to balance the contribution between attribute and topological information to the detection of communities. The weight adjustment mechanism enables incremental updates of community membership of all vertices. We evaluate inc-AGGMMR on five benchmark datasets against eight strong baselines. We also provide a case study to incrementally detect communities on a PayPal payment network which contains  users with  transactions. The results demonstrate inc-AGGMMR’s effectiveness and practicability.}
}


@article{DBLP:journals/tkdd/XiaLLFHSGJ21,
	author = {Tong Xia and
                  Junjie Lin and
                  Yong Li and
                  Jie Feng and
                  Pan Hui and
                  Funing Sun and
                  Diansheng Guo and
                  Depeng Jin},
	title = {3DGCN: 3-Dimensional Dynamic Graph Convolutional Network for Citywide
                  Crowd Flow Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {110:1--110:21},
	year = {2021},
	url = {https://doi.org/10.1145/3451394},
	doi = {10.1145/3451394},
	timestamp = {Sun, 04 Aug 2024 19:52:23 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaLLFHSGJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowd flow prediction is an essential task benefiting a wide range of applications for the transportation system and public safety. However, it is a challenging problem due to the complex spatio-temporal dependence and the complicated impact of urban structure on the crowd flow patterns. In this article, we propose a novel framework, 3-Dimensional Graph Convolution Network (3DGCN), to predict citywide crowd flow. We first model it as a dynamic spatio-temporal graph prediction problem, where each node represents a region with time-varying flows, and each edge represents the origin–destination (OD) flow between its corresponding regions. As such, OD flows among regions are treated as a proxy for the spatial interactions among regions. To tackle the complex spatio-temporal dependence, our proposed 3DGCN can model the correlation among graph spatial and temporal neighbors simultaneously. To learn and incorporate urban structures in crowd flow prediction, we design the GCN aggregator to be learned from both crowd flow prediction and region function inference at the same time. Extensive experiments with real-world datasets in two cities demonstrate that our model outperforms state-of-the-art baselines by 9.6%∼19.5% for the next-time-interval prediction.}
}


@article{DBLP:journals/tkdd/LiuLZBW21,
	author = {Kai Liu and
                  Xiangyu Li and
                  Zhihui Zhu and
                  Lodewijk Brand and
                  Hua Wang},
	title = {Factor-Bounded Nonnegative Matrix Factorization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {111:1--111:18},
	year = {2021},
	url = {https://doi.org/10.1145/3451395},
	doi = {10.1145/3451395},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuLZBW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nonnegative Matrix Factorization (NMF) is broadly used to determine class membership in a variety of clustering applications. From movie recommendations and image clustering to visual feature extractions, NMF has applications to solve a large number of knowledge discovery and data mining problems. Traditional optimization methods, such as the Multiplicative Updating Algorithm\xa0(MUA), solves the NMF problem by utilizing an auxiliary function to ensure that the objective monotonically decreases. Although the objective in MUA converges, there exists no proof to show that the learned matrix factors converge as well. Without this rigorous analysis, the clustering performance and stability of the NMF algorithms cannot be guaranteed. To address this knowledge gap, in this article, we study the factor-bounded NMF problem and provide a solution algorithm with proven convergence by rigorous mathematical analysis, which ensures that both the objective and matrix factors converge. In addition, we show the relationship between MUA and our solution followed by an analysis of the convergence of MUA. Experiments on both toy data and real-world datasets validate the correctness of our proposed method and its utility as an effective clustering algorithm.}
}


@article{DBLP:journals/tkdd/WangLDLJ21,
	author = {Huandong Wang and
                  Yong Li and
                  Mu Du and
                  Zhenhui Li and
                  Depeng Jin},
	title = {App2Vec: Context-Aware Application Usage Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {15},
	number = {6},
	pages = {112:1--112:21},
	year = {2021},
	url = {https://doi.org/10.1145/3451396},
	doi = {10.1145/3451396},
	timestamp = {Tue, 17 Aug 2021 21:08:11 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLDLJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Both app developers and service providers have strong motivations to understand when and where certain apps are used by users. However, it has been a challenging problem due to the highly skewed and noisy app usage data. Moreover, apps are regarded as independent items in existing studies, which fail to capture the hidden semantics in app usage traces. In this article, we propose App2Vec, a powerful representation learning model to learn the semantic embedding of apps with the consideration of spatio-temporal context. Based on the obtained semantic embeddings, we develop a probabilistic model based on the Bayesian mixture model and Dirichlet process to capture when, where, and what semantics of apps are used to predict the future usage. We evaluate our model using two different app usage datasets, which involve over 1.7 million users and 2,000+ apps. Evaluation results show that our proposed App2Vec algorithm outperforms the state-of-the-art algorithms in app usage prediction with a performance gap of over 17.0%.}
}
