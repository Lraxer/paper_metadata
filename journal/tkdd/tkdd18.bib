@article{DBLP:journals/tkdd/LiZGLYZ24,
	author = {Youru Li and
                  Zhenfeng Zhu and
                  Xiaobo Guo and
                  Shaoshuai Li and
                  Yuchen Yang and
                  Yao Zhao},
	title = {HGV4Risk: Hierarchical Global View-guided Sequence Representation
                  Learning for Risk Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {1:1--1:21},
	year = {2024},
	url = {https://doi.org/10.1145/3605895},
	doi = {10.1145/3605895},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZGLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Risk prediction, usually achieved by learning representations from patient’s physiological sequence or user’s behavioral sequence data, and has been widely applied in healthcare and finance. Despite that, some recent time-aware deep learning methods have led to superior performances in such sequence representation learning tasks, such improvement is limited due to a lack of guidance from hierarchical global view. To address this issue, we propose a novel end-to-end Hierarchical Global View-guided (HGV) sequence representation learning framework. Specifically, the Global Graph Embedding (GGE) module is proposed to learn sequential clip-aware representations from temporal correlation graph (TCG) at instance level. Furthermore, following the way of key-query attention, the harmonic β-attention (β-Attn) is also developed for making a global tradeoff between time-aware decay and observation significance at channel level adaptively. Moreover, the hierarchical representations at both instance level and channel level can be coordinated by the heterogeneous information aggregation under the guidance of global view. Experimental results on both healthcare risk prediction benchmark and SMEs credit overdue risk prediction task from the real-world industrial scenario in MYBank, Ant Group, have illustrated that the proposed model can achieve competitive prediction performance compared with other known baselines. The code has been released public available at: https://github.com/LiYouru0228/HGV.}
}


@article{DBLP:journals/tkdd/AmagataH24,
	author = {Daichi Amagata and
                  Takahiro Hara},
	title = {Efficient Density-peaks Clustering Algorithms on Static and Dynamic
                  Data in Euclidean Space},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {2:1--2:27},
	year = {2024},
	url = {https://doi.org/10.1145/3607873},
	doi = {10.1145/3607873},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AmagataH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering multi-dimensional points is a fundamental task in many fields, and density-based clustering supports many applications because it can discover clusters of arbitrary shapes. This article addresses the problem of Density-Peaks Clustering (DPC) in Euclidean space. DPC already has many applications, but its straightforward implementation incurs O(n2) time, where n is the number of points, thereby does not scale to large datasets. To enable DPC on large datasets, we first propose empirically efficient exact DPC algorithm, Ex-DPC. Although this algorithm is much faster than the straightforward implementation, it still suffers from O(n2) time theoretically. We hence propose a new exact algorithm, Ex-DPC++, that runs in o(n2) time. We accelerate their efficiencies by leveraging multi-threading. Moreover, real-world datasets may have arbitrary updates (point insertions and deletions). It is hence important to support efficient cluster updates. To this end, we propose D-DPC for fully dynamic DPC. We conduct extensive experiments using real datasets, and our experimental results demonstrate that our algorithms are efficient and scalable.}
}


@article{DBLP:journals/tkdd/DengDYJS24,
	author = {Jiewen Deng and
                  Jinliang Deng and
                  Du Yin and
                  Renhe Jiang and
                  Xuan Song},
	title = {TTS-Norm: Forecasting Tensor Time Series via Multi-Way Normalization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {3:1--3:25},
	year = {2024},
	url = {https://doi.org/10.1145/3605894},
	doi = {10.1145/3605894},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/DengDYJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world applications. Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. However, properly coping with the TTS is a much more challenging task, due to its high-dimensional and complex inner structure. In this article, we start by revealing the structure of TTS data from afn statistical view of point. Then, in line with this analysis, we perform Tensor Time Series forecasting via a proposed Multi-way Normalization (TTS-Norm), which effectively disentangles multiple heterogeneous low-dimensional substructures from the original high-dimensional structure. Finally, we design a novel objective function for TTS forecasting, accounting for the numerical heterogeneity among different low-dimensional subspaces of TTS. Extensive experiments on two real-world datasets verify the superior performance of our proposed model.}
}


@article{DBLP:journals/tkdd/MoreoFS24,
	author = {Alejandro Moreo and
                  Manuel Francisco and
                  Fabrizio Sebastiani},
	title = {Multi-Label Quantification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {4:1--4:36},
	year = {2024},
	url = {https://doi.org/10.1145/3606264},
	doi = {10.1145/3606264},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/MoreoFS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantification, variously called supervised prevalence estimation or learning to quantify, is the supervised learning task of generating predictors of the relative frequencies (a.k.a. prevalence values) of the classes of interest in unlabelled data samples. While many quantification methods have been proposed in the past for binary problems and, to a lesser extent, single-label multiclass problems, the multi-label setting (i.e., the scenario in which the classes of interest are not mutually exclusive) remains by and large unexplored. A straightforward solution to the multi-label quantification problem could simply consist of recasting the problem as a set of independent binary quantification problems. Such a solution is simple but naïve, since the independence assumption upon which it rests is, in most cases, not satisfied. In these cases, knowing the relative frequency of one class could be of help in determining the prevalence of other related classes. We propose the first truly multi-label quantification methods, i.e., methods for inferring estimators of class prevalence values that strive to leverage the stochastic dependencies among the classes of interest in order to predict their relative frequencies more accurately. We show empirical evidence that natively multi-label solutions outperform the naïve approaches by a large margin. The code to reproduce all our experiments is available online.}
}


@article{DBLP:journals/tkdd/ZhangYDGY24,
	author = {Chunkai Zhang and
                  Yuting Yang and
                  Zilin Du and
                  Wensheng Gan and
                  Philip S. Yu},
	title = {{HUSP-SP:} Faster Utility Mining on Sequence Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {5:1--5:21},
	year = {2024},
	url = {https://doi.org/10.1145/3597935},
	doi = {10.1145/3597935},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangYDGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low-utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this article, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely, discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns’ utilities and upper-bound values. Furthermore, a new upper bound on utility, namely, tighter reduced sequence utility and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability.}
}


@article{DBLP:journals/tkdd/ChenFXWPG24,
	author = {Zhaoliang Chen and
                  Lele Fu and
                  Shunxin Xiao and
                  Shiping Wang and
                  Claudia Plant and
                  Wenzhong Guo},
	title = {Multi-View Graph Convolutional Networks with Differentiable Node Selection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {6:1--6:21},
	year = {2024},
	url = {https://doi.org/10.1145/3608954},
	doi = {10.1145/3608954},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenFXWPG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in the real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this article, we apply Graph Convolutional Network (GCN) to cope with heterogeneous graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top-k nodes according to pre-defined confidence values. Nonetheless, this is problematic due to the non-differentiable sorting operators and inflexible graph embedding learning, which may result in blocked gradient computations and undesired performance. To cope with these issues, we propose a joint framework dubbed Multi-view Graph Convolutional Network with Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive graph fusion layer, a graph learning module, and a differentiable node selection schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims to learn more robust graph fusion through a differentiable neural network. The effectiveness of the proposed method is verified by rigorous comparisons with considerable state-of-the-art approaches in terms of multi-view semi-supervised classification tasks, and the experimental results indicate that MGCN-DNS achieves pleasurable performance on several benchmark multi-view datasets.}
}


@article{DBLP:journals/tkdd/LuoWW24,
	author = {Fangyuan Luo and
                  Jun Wu and
                  Tao Wang},
	title = {Discrete Listwise Content-aware Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {7:1--7:20},
	year = {2024},
	url = {https://doi.org/10.1145/3609334},
	doi = {10.1145/3609334},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LuoWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To perform online inference efficiently, hashing techniques, devoted to encoding model parameters as binary codes, play a key role in reducing the computational cost of content-aware recommendation (CAR), particularly on devices with limited computation resource. However, current hashing methods for CAR fail to align their learning objectives (e.g., squared loss) with the ranking-based metrics (e.g., Normalized Discounted Cumulative Gain (NDCG)), resulting in suboptimal recommendation accuracy. In this article, we propose a novel ranking-based CAR hashing method based on Factorization Machine (FM), called Discrete Listwise FM (DLFM), for fast and accurate recommendation. Concretely, our DLFM is to optimize NDCG in the Hamming space for preserving the listwise user-item relationships. We devise an efficient algorithm to resolve the challenging DLFM problem, which can directly learn binary parameters in a relaxed continuous solution space, without additional quantization. Particularly, our theoretical analysis shows that the optimal solution to the relaxed continuous optimization problem is approximately the same as that of the original discrete optimization problem. Through extensive experiments on two real-world datasets, we show that DLFM consistently outperforms state-of-the-art hashing-based recommendation techniques.}
}


@article{DBLP:journals/tkdd/LiYNLD24,
	author = {Huiyuan Li and
                  Li Yu and
                  Xi Niu and
                  Youfang Leng and
                  Qihan Du},
	title = {Sequential and Graphical Cross-Domain Recommendations with a Multi-View
                  Hierarchical Transfer Gate},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {8:1--8:28},
	year = {2024},
	url = {https://doi.org/10.1145/3604615},
	doi = {10.1145/3604615},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiYNLD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-domain recommender systems could potentially improve the recommendation performance by means of transferring abundant knowledge from the auxiliary domain to the target domain. They could help address some key challenges in recommender systems, such as data sparsity and cold start. However, most existing cross-domain recommendation approaches represent the user preferences based on a single kind of user’s feature or behavior and fail to explore the hidden interaction effects of different kinds of features or behaviors. In this article, we propose the Sequential and Graphical Cross-Domain Recommendations with a Multi-View Hierarchical Transfer Gate (SGCross) to transfer user representations from multiple perspectives. The SGCross model constructs a user profile by learning the personal preference from a personal view, the dynamic preference from a temporal view, as well as the collaborative preference from a collaborative view. Specifically, a Multi-view Hierarchical Gate (MHG) is designed to transfer the informative representations of user knowledge on different views from the auxiliary domain separately, aiming to enhance the user representations. Furthermore, a two-stage attentive fusion module is designed to integrate transferred information at two levels: the domain level and the view level. Extensive experiments on the Amazon dataset and the Douban dataset have demonstrated that SGCross effectively improves the accuracy of cross-domain recommendations and outperforms the state-of-the-art baseline models.}
}


@article{DBLP:journals/tkdd/MaiYGZCH24,
	author = {Weiming Mai and
                  Jiangchao Yao and
                  Chen Gong and
                  Ya Zhang and
                  Yiu{-}Ming Cheung and
                  Bo Han},
	title = {Server-Client Collaborative Distillation for Federated Reinforcement
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {9:1--9:22},
	year = {2024},
	url = {https://doi.org/10.1145/3604939},
	doi = {10.1145/3604939},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/MaiYGZCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) learns a global model in a distributional manner, which does not require local clients to share private data. Such merit has drawn lots of attention in the interaction scenarios, where Federated Reinforcement Learning (FRL) emerges as a cross-field research direction focusing on the robust training of agents. Different from FL, the heterogeneity problem in FRL is more challenging because the data depends on the policy of agents and the environment dynamics. FRL learns to interact under the non-stationary environment feedback, while the typical FL methods aim at handling the constant data heterogeneity. In this article, we are among the first attempts to analyze the heterogeneity problem in FRL and propose an off-policy FRL framework. Specifically, a student–teacher–student model learning and fusion method, termed as Server-Client Collaborative Distillation (SCCD), is introduced. Unlike the traditional FL, we distill all local models on the server side for model fusion. To reduce the variance of the training, a local distillation is also conducted every time the agent receives the global model. Experimentally, we compare SCCD with a range of straightforward combinations between FL methods and RL. The results demonstrate that SCCD has a superior performance in four classical continuous control tasks with non-IID environments.}
}


@article{DBLP:journals/tkdd/WuCX24,
	author = {Yao Wu and
                  Jian Cao and
                  Guandong Xu},
	title = {Fairness in Recommender Systems: Evaluation Approaches and Assurance
                  Strategies},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {10:1--10:37},
	year = {2024},
	url = {https://doi.org/10.1145/3604558},
	doi = {10.1145/3604558},
	timestamp = {Thu, 02 May 2024 20:50:53 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuCX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide application of recommender systems, the potential impacts of recommender systems on customers, item providers and other parties have attracted increasing attention. Fairness, which is the quality of treating people equally, is also becoming important in recommender system evaluation and algorithm design. Therefore, in the past years, there has been a growing interest in fairness measurement and assurance in recommender systems. Although there are several reviews on related topics, such as fairness in machine learning and debias in recommender systems, they do not present a systematic view on fairness in recommender systems, which is context aware and has a multi-sided meaning. Therefore, in this review, the concept of fairness is discussed in detail in the various contexts of recommender systems. Specifically, a comprehensive framework to classify fairness metrics is proposed from four dimensions, i.e., Fairness for Whom, Demographic Unit, Time Frame, and Quantification Method. Then the strategies for eliminating unfairness in recommendations, fairness in different recommendation tasks and datasets are reviewed and summarized. Finally, the challenges and future work are discussed.}
}


@article{DBLP:journals/tkdd/WangLH24,
	author = {Huan Wang and
                  Guoquan Liu and
                  Po Hu},
	title = {{TDAN:} Transferable Domain Adversarial Network for Link Prediction
                  in Heterogeneous Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {11:1--11:22},
	year = {2024},
	url = {https://doi.org/10.1145/3610229},
	doi = {10.1145/3610229},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link prediction has received increased attention in social network analysis. One of the unique challenges in heterogeneous social networks is link prediction in new link types without verified link information, such as recommending products to new overseas groups. Existing link prediction models tend to learn type-specific knowledge on specific link types and predict missing or future links on the same link types. However, because of the uncertainty of new link types in the evolving process of social networks, it is difficult to collect sufficient verified link information in new link types. Therefore, we propose the Transferable Domain Adversarial Network (TDAN) based on transfer learning to handle the challenge. TDAN exploits transferable type-shared knowledge in historical link types to help predict the unobserved links in new link types. TDAN mainly comprises a structural encoder, a domain discriminator, and an optimization decoder. The structural encoder learns the link representations in a heterogeneous social network. Subsequently, to learn transferable type-shared knowledge, the domain discriminator distinguishes link representations into different link types while minimizing the differences between type-specific knowledge in adversarial training. Inspired by the denoising auto-encoder, the optimization decoder reconstructs the learned type-shared knowledge to eliminate the noise generated during the adversarial training. Extensive experiments on Facebook and YouTube show that TDAN can outperform the state-of-the-art models.}
}


@article{DBLP:journals/tkdd/CorbaraMS24,
	author = {Silvia Corbara and
                  Alejandro Moreo and
                  Fabrizio Sebastiani},
	title = {Same or Different? Diff-Vectors for Authorship Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {12:1--12:36},
	year = {2024},
	url = {https://doi.org/10.1145/3609226},
	doi = {10.1145/3609226},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/CorbaraMS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In “classic” authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online.1}
}


@article{DBLP:journals/tkdd/HuangLHCZ24,
	author = {Jincheng Huang and
                  Ping Li and
                  Rui Huang and
                  Na Chen and
                  Acong Zhang},
	title = {Revisiting the Role of Heterophily in Graph Representation Learning:
                  An Edge Classification Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {13:1--13:17},
	year = {2024},
	url = {https://doi.org/10.1145/3603378},
	doi = {10.1145/3603378},
	timestamp = {Thu, 23 Jan 2025 15:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangLHCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph representation learning aims at integrating node contents with graph structure to learn nodes/graph representations. Nevertheless, it is found that many existing graph learning methods do not work well on data with high heterophily level that accounts for a large proportion of edges between different class labels. Recent efforts to this problem focus on improving the message passing mechanism. However, it remains unclear whether heterophily truly does harm to the performance of graph neural networks (GNNs). The key is to unfold the relationship between a node and its immediate neighbors, e.g., are they heterophilous or homophilious? From this perspective, here we study the role of heterophily in graph representation learning before/after the relationships between connected nodes are disclosed. In particular, we propose an end-to-end framework that both learns the type of edges (i.e., heterophilous/homophilious) and leverage edge type information to improve the expressiveness of graph neural networks. We implement this framework in two different ways. Specifically, to avoid messages passing through heterophilous edges, we can optimize the graph structure to be homophilious by dropping heterophilous edges identified by an edge classifier. Alternatively, it is possible to exploit the information about the presence of heterophilous neighbors for feature learning, so a hybrid message passing approach is devised to aggregate homophilious neighbors and diversify heterophilous neighbors based on edge classification. Extensive experiments demonstrate the remarkable performance improvement of GNNs with the proposed framework on multiple datasets across the full spectrum of homophily level.}
}


@article{DBLP:journals/tkdd/LuoWGCLMZDHH24,
	author = {Xiao Luo and
                  Daqing Wu and
                  Yiyang Gu and
                  Chong Chen and
                  Luchen Liu and
                  Jinwen Ma and
                  Ming Zhang and
                  Minghua Deng and
                  Jianqiang Huang and
                  Xian{-}Sheng Hua},
	title = {Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior
                  Implicit Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {14:1--14:26},
	year = {2024},
	url = {https://doi.org/10.1145/3611310},
	doi = {10.1145/3611310},
	timestamp = {Mon, 20 Nov 2023 17:25:12 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LuoWGCLMZDHH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed the explosive growth of interaction behaviors in multimedia information systems, where multi-behavior recommender systems have received increasing attention by leveraging data from various auxiliary behaviors such as tip and collect. Among various multi-behavior recommendation methods, non-sampling methods have shown superiority over negative sampling methods. However, two observations are usually ignored in existing state-of-the-art non-sampling methods based on binary regression: (1) users have different preference strengths for different items, so they cannot be measured simply by binary implicit data; (2) the dependency across multiple behaviors varies for different users and items. To tackle the above issue, we propose a novel non-sampling learning framework named Criterion-guided Heterogeneous Collaborative Filtering (CHCF). CHCF introduces both upper and lower thresholds to indicate selection criteria, which will guide user preference learning. Besides, CHCF integrates criterion learning and user preference learning into a unified framework, which can be trained jointly for the interaction prediction of the target behavior. We further theoretically demonstrate that the optimization of Collaborative Metric Learning can be approximately achieved by the CHCF learning framework in a non-sampling form effectively. Extensive experiments on three real-world datasets show the effectiveness of CHCF in heterogeneous scenarios.}
}


@article{DBLP:journals/tkdd/LiuZLQZW24,
	author = {Huiting Liu and
                  Yu Zhang and
                  Peipei Li and
                  Cheng Qian and
                  Peng Zhao and
                  Xindong Wu},
	title = {DeepCPR: Deep Path Reasoning Using Sequence of User-Preferred Attributes
                  for Conversational Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {15:1--15:22},
	year = {2024},
	url = {https://doi.org/10.1145/3610775},
	doi = {10.1145/3610775},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuZLQZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conversational recommender systems (CRS) have garnered significant attention in academia and industry because of their ability to capture user preferences via system questions and user responses. Typically, in a CRS, reinforcement learning (RL) is utilized to determine the optimal timing for requesting attribute information or suggesting items. However, existing methods consider user-preferred attributes independently and ignore that attributes may be of different importance to the same user, in the attribute and item selection phases, which limits the accuracy and interpretability of CRS. Inspired by this, we propose deep conversational path reasoning (DeepCPR), which involves constructing a reasoning path on a graph with a series of user-favored attributes. It utilizes the attention mechanism to thoroughly examine the connections between these attributes and provide improved explanations for which attributes to inquire about or which items to recommend. In DeepCPR, two deep-learning-based modules are proposed to realize attribute and item selection. In the first module, the sequence of attributes confirmed by the user in conversation is encoded with a gated graph neural network to obtain the user’s long-term preference using a self-attention mechanism for the selection of candidate attributes. In the second module, a self-attention approach with more appropriate strategies is developed to dynamically select candidate items. In addition, to achieve fine-grained user preference modeling, a recurrent neural network is employed to aggregate the sequence of attributes that interact with the users. Numerous experimental evaluations conducted on four real CRS datasets show that the proposed method significantly outperforms existing advanced methods in terms of conversational recommendations.}
}


@article{DBLP:journals/tkdd/ZhangZYLZL24,
	author = {Qiuyue Zhang and
                  Yunfeng Zhang and
                  Xunxiang Yao and
                  Shilong Li and
                  Caiming Zhang and
                  Peide Liu},
	title = {A Dynamic Attributes-driven Graph Attention Network Modeling on Behavioral
                  Finance for Stock Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {16:1--16:29},
	year = {2024},
	url = {https://doi.org/10.1145/3611311},
	doi = {10.1145/3611311},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangZYLZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stock prediction is a challenging task due to multiple influencing factors and complex market dependencies. Traditional solutions are based on a single type of information. With the success of multi-source information in different fields, the combination of different types of information such as numerical and textual information has become a promising option. Although multi-source information provides rich multi-view information, how to mine and construct structured relationships from them is a difficult problem. Specifically, most existing methods usually extract features from commonly used multi-source information as predictive information sources, without further pre-constructing stock relationship graphs with dependencies using broader information. More importantly, they typically treat each stock as an isolated forecasting, or employ stock market correlations based on a fixed predefined graph structure, but current methods are not sensitive enough to aggregate the attribute features extracted from multi-source information and stock relationship graph, to obtain the dynamic update of market relations and relationship strength. The stock market is highly temporally, and the attributes of nodes are affected by the time perception of other attributes, which is not fully considered. To address these problems, we propose a novel dynamic attributes-driven graph attention networks incorporating sentiment (DGATS) information, transaction data, and text data. Inspired by behavioral finance, we separately extract sentiment information as a factor of technical indicators, and further realize the early fusion of technical indicators and textual data through Kronecker product-based tensor fusion. In particular, by LSTM and temporal attention network, the short-term and long-term transition features are gradually grasped from the local composition of the fused stock trading sequence. Furthermore, real-time intra-market dependencies and key attributes information are captured with graph networks, enabling dynamic updates of relationships and relationship strengths in predefined graphs. Experiments on the real datasets show that the architecture can outperform the previous methods in prediction performance.}
}


@article{DBLP:journals/tkdd/ChowdhurySMBG24,
	author = {Anjan Chowdhury and
                  Sriram Srinivasan and
                  Animesh Mukherjee and
                  Sanjukta Bhowmick and
                  Kuntal Ghosh},
	title = {Improving Node Classification Accuracy of {GNN} through Input and
                  Output Intervention},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {17:1--17:31},
	year = {2024},
	url = {https://doi.org/10.1145/3610535},
	doi = {10.1145/3610535},
	timestamp = {Tue, 28 Nov 2023 20:05:58 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChowdhurySMBG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) are a popular machine learning framework for solving various graph processing applications. This framework exploits both the graph topology and the feature vectors of the nodes. One of the important applications of GNN is in the semi-supervised node classification task. The accuracy of the node classification using GNN depends on (i) the number and (ii) the choice of the training nodes. In this article, we demonstrate that increasing the training nodes by selecting nodes from the same class that are spread out across non-contiguous subgraphs, can significantly improve the accuracy. We accomplish this by presenting a novel input intervention technique that can be used in conjunction with different GNN classification methods to increase the non-contiguous training nodes and, thereby, improve the accuracy. We also present an output intervention technique to identify misclassified nodes and relabel them with their potentially correct labels. We demonstrate on real-world networks that our proposed methods, both individually and collectively, significantly improve the accuracy in comparison to the baseline GNN algorithms. Both our methods are agnostic. Apart from the initial set of training nodes generated by the baseline GNN methods, our techniques do not need any other extra knowledge about the classes of the nodes. Thus, our methods are modular and can be used as pre-and post-processing steps with many of the currently available GNN methods to improve their accuracy.}
}


@article{DBLP:journals/tkdd/ChenLJC24,
	author = {Ke{-}Jia Chen and
                  Linsong Liu and
                  Linpu Jiang and
                  Jingqiang Chen},
	title = {Self-Supervised Dynamic Graph Representation Learning via Temporal
                  Subgraph Contrast},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {18:1--18:20},
	year = {2024},
	url = {https://doi.org/10.1145/3612931},
	doi = {10.1145/3612931},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenLJC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-supervised learning on graphs has recently drawn a lot of attention due to its independence from labels and its robustness in representation. Current studies on this topic mainly use static information such as graph structures but cannot well capture dynamic information such as timestamps of edges. Realistic graphs are often dynamic, which means the interaction between nodes occurs at a specific time. This article proposes a self-supervised dynamic graph representation learning framework DySubC, which defines a temporal subgraph contrastive learning task to simultaneously learn the structural and evolutional features of a dynamic graph. Specifically, a novel temporal subgraph sampling strategy is firstly proposed, which takes each node of the dynamic graph as the central node and uses both neighborhood structures and edge timestamps to sample the corresponding temporal subgraph. The subgraph representation function is then designed according to the influence of neighborhood nodes on the central node after encoding the nodes in each subgraph. Finally, the structural and temporal contrastive loss are defined to maximize the mutual information between node representation and temporal subgraph representation. Experiments on five real-world datasets demonstrate that (1) DySubC performs better than the related baselines including two graph contrastive learning models and five dynamic graph representation learning models, especially in the link prediction task, and (2) the use of temporal information cannot only sample more effective subgraphs, but also learn better representation by temporal contrastive loss.}
}


@article{DBLP:journals/tkdd/SunHF24,
	author = {Yan Sun and
                  Yi Han and
                  Jicong Fan},
	title = {Laplacian-based Cluster-Contractive t-SNE for High-Dimensional Data
                  Visualization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {19:1--19:22},
	year = {2024},
	url = {https://doi.org/10.1145/3612932},
	doi = {10.1145/3612932},
	timestamp = {Thu, 20 Jun 2024 17:03:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunHF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this article, we propose LaptSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, LaptSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of LaptSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also provide out-of-sample extension, large-scale extension, and mini-batch extension for our LaptSNE to facilitate dimensionality reduction in various scenarios.}
}


@article{DBLP:journals/tkdd/GeBWCZ24,
	author = {Yong{-}Feng Ge and
                  Elisa Bertino and
                  Hua Wang and
                  Jinli Cao and
                  Yanchun Zhang},
	title = {Distributed Cooperative Coevolution of Data Publishing Privacy and
                  Transparency},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {20:1--20:23},
	year = {2024},
	url = {https://doi.org/10.1145/3613962},
	doi = {10.1145/3613962},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GeBWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data transparency is beneficial to data participants’ awareness, users’ fairness, and research work’s reproducibility. However, when addressing transparency requirements, we cannot ignore data privacy. This article defines the multi-objective data publishing (MODP) problem, optimizing data privacy and transparency at the same time. Accordingly, we propose a distributed cooperative coevolutionary genetic algorithm (DCCGA) to optimize the MODP problem. In the population of DCCGA, each individual represents an anonymization solution to MODP. Three modules in DCCGA, i.e., grouping module, cooperative coevolutionary module, and evolving module, are proposed for distributed sub-population update and evaluation, improving DCCGA’s optimization performance and parallel efficiency. Moreover, a matrix-based crossover operator and a matrix-based mutation operator are designed to exchange and adjust anonymization information in the individuals efficiently. Experimental results demonstrate that the proposed DCCGA outperforms the competitors with respect to solution accuracy, convergence speed, and scalability. Besides, we verify the effectiveness of all the proposed components in DCCGA.}
}


@article{DBLP:journals/tkdd/StrukovaVM24,
	author = {Sofia Strukova and
                  Jos{\'{e}} A. Ruip{\'{e}}rez{-}Valiente and
                  F{\'{e}}lix G{\'{o}}mez M{\'{a}}rmol},
	title = {Adapting Knowledge Inference Algorithms to Measure Geometry Competencies
                  through a Puzzle Game},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {21:1--21:23},
	year = {2024},
	url = {https://doi.org/10.1145/3614436},
	doi = {10.1145/3614436},
	timestamp = {Mon, 27 May 2024 22:16:32 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/StrukovaVM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid technological evolution of the last years has motivated students to develop capabilities that will prepare them for an unknown future in the 21st century. In this context, many teachers intend to optimise the learning process, making it more dynamic and exciting through the introduction of gamification. Thus, this article focuses on a data-driven assessment of geometry competencies, which are essential for developing problem-solving and higher-order thinking skills. Our main goal is to adapt, evaluate and compare Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Elo, and Deep Knowledge Tracing (DKT) algorithms applied to the data of a geometry game named Shadowspect, in order to predict students’ performance by means of several classifier metrics. We analysed two algorithmic configurations, with and without prioritisation of Knowledge Components (KCs) – the skills needed to complete a puzzle successfully, and we found Elo to be the algorithm with the best prediction power with the ability to model the real knowledge of students. However, the best results are achieved without KCs because it is a challenging task to differentiate between KCs effectively in game environments. Our results prove that the above-mentioned algorithms can be applied in formal education to improve teaching, learning, and organisational efficiency.}
}


@article{DBLP:journals/tkdd/LiuLXWHLCH24,
	author = {Bo Liu and
                  Liangjiao Li and
                  Yanshan Xiao and
                  Kai Wang and
                  Jian Hu and
                  Junrui Liu and
                  Qihang Chen and
                  Ruiguang Huang},
	title = {An Efficient Transfer Learning Method with Auxiliary Information},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {22:1--22:23},
	year = {2024},
	url = {https://doi.org/10.1145/3612930},
	doi = {10.1145/3612930},
	timestamp = {Sat, 02 Dec 2023 13:23:42 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuLXWHLCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transfer learning (TL) is an information reuse learning tool, which can help us learn better classification effect than traditional single task learning, because transfer learning can share information within the task-to-task model. Most TL algorithms are studied in the field of data improvement, doing some data extraction and transformation. However, it ignores that existing the additional information to improve the model’s accuracy, like Universum samples in the training data with privileged information. In this article, we focus on considering prior data to improve the TL algorithm, and the additional features also called privileged information are incorporated into the learning to improve the learning paradigm. In addition, we also carry out the Universum samples which do not belong to any indicated categories into the transfer learning paradigm to improve the utilization of prior knowledge. We propose a new TL Model (PU-TLSVM), in which each task with corresponding privileged features and Universum data is considered in the proposed model, so as to apply tasks with a priori data to the training stage. Then, we use Lagrange duality theorem to optimize our model to obtain the optimal discriminant for target task classification. Finally, we make a lot of predictions and tests to compare the actual effectiveness of the proposed method with the previous methods. The experiment results indicate that the proposed method is more effective and robust than other baselines.}
}


@article{DBLP:journals/tkdd/LiZL24,
	author = {Zhong Li and
                  Yuxuan Zhu and
                  Matthijs van Leeuwen},
	title = {A Survey on Explainable Anomaly Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {23:1--23:54},
	year = {2024},
	url = {https://doi.org/10.1145/3609333},
	doi = {10.1145/3609333},
	timestamp = {Tue, 13 Feb 2024 09:20:08 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.}
}


@article{DBLP:journals/tkdd/Jiang24,
	author = {Meng Jiang},
	title = {Transfer Learning across Graph Convolutional Networks: Methods, Theory,
                  and Applications},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {24:1--24:23},
	year = {2024},
	url = {https://doi.org/10.1145/3617376},
	doi = {10.1145/3617376},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Jiang24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks have been widely used for learning representations of nodes for many downstream tasks on graph data. Existing models were designed for the nodes on a single graph, which would not be able to utilize information across multiple graphs. The real world does have multiple graphs where the nodes are often partially aligned. For examples, knowledge graphs share a number of named entities though they may have different relation schema; collaboration networks on publications and awarded projects share some researcher nodes who are authors and investigators, respectively; people use multiple web services, shopping, tweeting, rating movies, and some may register the same e-mail account across the platforms. In this article, we propose partially aligned graph convolutional networks to learn node representations across the models. We provide multiple methods such as model sharing, regularization, and alignment reconstruction, as well as theoretical analysis to positively transfer knowledge across the set of partially aligned nodes. Extensive experiments on real-world knowledge graphs, collaboration networks, and bipartite rating graphs show the superior performance of our proposed methods on relation classification, link prediction, and item recommendation.}
}


@article{DBLP:journals/tkdd/WangTD24,
	author = {Lizhen Wang and
                  Vanha Tran and
                  Thanhcong Do},
	title = {A Clique-Querying Mining Framework for Discovering High Utility Co-Location
                  Patterns without Generating Candidates},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {25:1--25:42},
	year = {2024},
	url = {https://doi.org/10.1145/3617378},
	doi = {10.1145/3617378},
	timestamp = {Fri, 15 Mar 2024 17:12:01 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangTD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Groups of spatial features whose instances frequently appear together in nearby areas are regarded as prevalent co-location patterns (PCPs). Traditional PCP mining ignores the significance of instances and features. However, in reality, these instances and features have different significance, the traditional PCPs may not sufficiently expose knowledge from spatial data. This study focuses on discovering high utility co-location patterns (HUCPs) in which each instance is assigned a utility to reflect its significance. To filter HUCPs, an adaptive utility participation index (UPI) is designed. Unfortunately, the UPI does not hold the downward closure property. The performance of mining HUCPs is very inefficient since unnecessary candidates cannot be early pruned. Thus, an efficient clique-querying mining framework is devised without generating candidates. This framework first divides neighboring instances into cliques, then compacts these cliques into a hash table structure. Next, the adaptive UPI of any patterns can be quickly calculated based on their participating instances that are obtained by executing a querying scheme on the hash table. Finally, HUCPs are filtered efficiently. The effectiveness and efficiency of the proposed method are proved in both theory and experiments to make a promise that the patterns mined are more meaningful and the mining performance is significantly improved compared to the previous methods.}
}


@article{DBLP:journals/tkdd/FuN24,
	author = {Zhe Fu and
                  Xi Niu},
	title = {Modeling Users' Curiosity in Recommender Systems},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {26:1--26:23},
	year = {2024},
	url = {https://doi.org/10.1145/3617598},
	doi = {10.1145/3617598},
	timestamp = {Tue, 28 Nov 2023 20:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/FuN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s recommender systems are criticized for recommending items that are too obvious to arouse users’ interests. Therefore, the research community has advocated some “beyond accuracy” evaluation metrics such as novelty, diversity, and serendipity with the hope of promoting information discovery and sustaining users’ interests over a long period of time. While bringing in new perspectives, most of these evaluation metrics have not considered individual users’ differences in their capacity to experience those “beyond accuracy” items. Open-minded users may embrace a wider range of recommendations than conservative users. In this article, we proposed to use curiosity traits to capture such individual users’ differences. We developed a model to approximate an individual’s curiosity distribution over different stimulus levels. We used an item’s surprise level to estimate the stimulus level and whether such a level is in the range of the user’s appetite for stimulus, called Comfort Zone. We then proposed a recommender system framework that considers both user preference and their Comfort Zone where the curiosity is maximally aroused. Our framework differs from a typical recommender system in that it leverages human’s Comfort Zone for stimuli to promote engagement with the system. A series of evaluation experiments have been conducted to show that our framework is able to rank higher the items with not only high ratings but also high curiosity stimulation. The recommendation list generated by our algorithm has a higher potential of inspiring user curiosity compared to the state-of-the-art deep learning approaches. The personalization factor for assessing the surprise stimulus levels further helps the recommender model achieve smaller (better) inter-user similarity.}
}


@article{DBLP:journals/tkdd/LiFXC24,
	author = {Hui{-}Jia Li and
                  Yuhao Feng and
                  Chengyi Xia and
                  Jie Cao},
	title = {Overlapping Graph Clustering in Attributed Networks via Generalized
                  Cluster Potential Game},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {27:1--27:26},
	year = {2024},
	url = {https://doi.org/10.1145/3597436},
	doi = {10.1145/3597436},
	timestamp = {Tue, 28 Nov 2023 20:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiFXC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Overlapping graph clustering is essential to understand the nature and behavior of real complex systems including human interactions, technical systems and transportation network. However, in addition of topological structure, many real-world networked systems contain spare factors, i.e., attributes of networks. Despite the considerable efforts that have been made in graph clustering, they only concentrate on the topological structure, which lack a profound understanding of cluster configuration on attributed graphs. To address this great challenge, in this article, we propose a new overlapping graph clustering algorithm by integrating the topological and attributive information into a cluster potential game (CPG). Firstly, a generalized definition of the utility function is provided, which measures the payoff of each node based on different node-to-cluster distance functions. It is worth mentioning that the model we proposed is able to associate with the classic ordinal potential game well. Then, we define the measures of both tightness and the homogeneity in each cluster, and introduce a novel two-way selection mechanism. The goal is to extend the flexibility of the cluster potential game, so that one can achieve a win-win situation between nodes and clusters. Finally, a distributed and heterogeneous multiagent system (DHMAS) is carefully designed based on a fast self-learning algorithm (SLA) for attributed overlapping graph clustering. Two series of experiments are implemented in multi-types datasets and the results verify the effectiveness and the scalability after the comparison with the most advanced approaches of literature.}
}


@article{DBLP:journals/tkdd/LiuZLJ24,
	author = {Yu Liu and
                  Zhilun Zhou and
                  Yong Li and
                  Depeng Jin},
	title = {Urban Knowledge Graph Aided Mobile User Profiling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {28:1--28:30},
	year = {2024},
	url = {https://doi.org/10.1145/3596604},
	doi = {10.1145/3596604},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuZLJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, the explosive growth of personalized web applications and the rapid development of artificial intelligence technology have flourished the recent research on mobile user profiling, i.e., inferring the user profile from mobile behavioral data. Particularly, existing studies mainly follow the data-driven paradigm to develop feature engineering and representation learning on such data, which however suffer from the robustness issue, i.e., generalizing poorly across datasets and profiles without considering semantic knowledge therein. In comparison, the rising knowledge-driven paradigm built upon the knowledge graph (KG) offers a potential solution to mitigate such weakness. Therefore, in this article, we propose a Knowledge Graph aided framework for Mobile User Profiling (KG-MUP). Specifically, to distil semantic knowledge among data, we firstly construct an urban knowledge graph (UrbanKG) with domain entities like users, regions, point of interests (POIs), and so on. identified, as well as semantic relations for home, workplace, spatiality, and so on. extracted. Moreover, we leverage tensor decomposition and graph neural network to obtain knowledgeable user representations from UrbanKG. In addition, we introduce several customized features to quantify individual mobility characteristics for mobile user profiling. Extensive experiments on three real-world mobility datasets demonstrate that KG-MUP achieves state-of-the-art performance on user profile inference tasks. Moreover, further results also reveal the importance of various semantic knowledge to user profile inference, which provides meaningful insights on user modeling with mobile behavioral data.}
}


@article{DBLP:journals/tkdd/LinCCYYML24,
	author = {Kaibiao Lin and
                  Jinpo Chen and
                  Ruicong Chen and
                  Fan Yang and
                  Zhang Yang and
                  Lin Min and
                  Ping Lu},
	title = {Adaptive Neighbor Graph Aggregated Graph Attention Network for Heterogeneous
                  Graph Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {29:1--29:21},
	year = {2024},
	url = {https://doi.org/10.1145/3616377},
	doi = {10.1145/3616377},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LinCCYYML24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph attention network can generate effective feature embedding by specifying different weights to different nodes. The key of the research on heterogeneous graph embedding is the way to combine its rich structural information with semantic relations to aggregate the neighborhood information. Most of the existing heterogeneous graph representation learning methods guide the selection of neighbors by defining various meta-paths on heterogeneous graphs. However, these models only consider the information contained in the nodes under different paths and ignore the potential semantic relationships of nodes in different neighbor graph structures, which leads to the underutilization of graph structure information. In this article, we propose a novel adaptive framework named Neighbor Graph Aggregated Graph Attention Network (NGGAN) to fully exploit graph topological details in heterogeneous graph, and aggregates their information to obtain an effective embedding. The key idea is to use different levels of sampling methods to define neighborhood, and use neighbor graphs to represent the complex structural interaction between nodes. In this way, the high-order relationship between nodes and the latent semantics of neighbor graphs can be fully explored. Afterward, a hierarchical attention mechanism is applied to adaptively learn the importance of different objects, including node information, path information, and neighbor graph information. Multiple downstream tasks are performed on four real-world heterogeneous graph datasets, and the experimental results demonstrate the effectiveness of NGGAN.}
}


@article{DBLP:journals/tkdd/WangOGZ24,
	author = {Yashen Wang and
                  Xiaoye Ouyang and
                  Dayu Guo and
                  Xiaoling Zhu},
	title = {{MEGA:} Meta-Graph Augmented Pre-Training Model for Knowledge Graph
                  Completion},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {30:1--30:24},
	year = {2024},
	url = {https://doi.org/10.1145/3617379},
	doi = {10.1145/3617379},
	timestamp = {Tue, 28 Nov 2023 20:05:59 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangOGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, a large number of Knowledge Graph Completion (KGC) methods have been proposed by using embedding based manners, to overcome the incompleteness problem faced with knowledge graph (KG). One important recent innovation in Natural Language Processing (NLP) domain is the employ of deep neural models that make the most of pre-training, culminating in BERT, the most popular example of this line of approaches today. Recently, a series of new KGC methods introducing a pre-trained language model, such as KG-BERT, have been developed and released compelling performance. However, previous pre-training based KGC methods usually train the model by using simple training task and only utilize one-hop relational signals in KG, which leads that they cannot model high-order semantic contexts and multi-hop complex relatedness. To overcome this problem, this article presents a novel pre-training framework for KGC task, which especially consists of both one-hop relation level task (low-order) and multi-hop meta-graph level task (high-order). Hence, the proposed method can capture not only the elaborate sub-graph structure but also the subtle semantic information on the given KG. The empirical results show the efficiency of the proposed method on the widely used real-world datasets.}
}


@article{DBLP:journals/tkdd/WangJLY24,
	author = {Xiang Wang and
                  Liping Jing and
                  Huafeng Liu and
                  Jian Yu},
	title = {Structure-Driven Representation Learning for Deep Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {31:1--31:25},
	year = {2024},
	url = {https://doi.org/10.1145/3623400},
	doi = {10.1145/3623400},
	timestamp = {Thu, 07 Nov 2024 15:00:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangJLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an important branch of unsupervised learning methods, clustering makes a wide contribution in the area of data mining. It is well known that capturing the group-discriminative properties of each sample for clustering is crucial. Among them, deep clustering delivers promising results due to the strong representational power of neural networks. However, most of them adopt sample-level learning strategies, and the standalone data point barely captures its holistic cluster’s context and may undergo sub-optimal cluster assignment. To tackle this issue, we propose a Structure-driven Representation Learning (SRL) method by introducing latent structure information into the representation learning process at both the local and global levels. Specifically, a local-structure-driven sample representation strategy is proposed to approximate the estimation of data distribution, which models the neighborhood distribution of samples with potential structure information and exploits statistical dependencies between them to improve cluster consistency. A global-structure-driven cluster representation strategy is designed, where the context of each cluster is sufficiently encoded according to its samples (exemplar-theory) and corresponding prototype (prototype-theory). In this case, each cluster can only be related to its most similar samples, and different clusters are separated as much as possible. These two models are seamlessly combined into a joint optimization problem, which can be efficiently solved. Experiments on six widely-used datasets demonstrate the superiority of SRL over state-of-the-art clustering methods.}
}


@article{DBLP:journals/tkdd/AlamASL24,
	author = {Md. Tanvir Alam and
                  Chowdhury Farhan Ahmed and
                  Md. Samiullah and
                  Carson Kai{-}Sang Leung},
	title = {Discovering Interesting Patterns from Hypergraphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {1},
	pages = {32:1--32:34},
	year = {2024},
	url = {https://doi.org/10.1145/3622940},
	doi = {10.1145/3622940},
	timestamp = {Sat, 20 Apr 2024 12:53:29 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/AlamASL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A hypergraph is a complex data structure capable of expressing associations among any number of data entities. Overcoming the limitations of traditional graphs, hypergraphs are useful to model real-life problems. Frequent pattern mining is one of the most popular problems in data mining with a lot of applications. To the best of our knowledge, there exists no flexible pattern mining framework for hypergraph databases decomposing associations among data entities. In this article, we propose a flexible and complete framework for mining frequent patterns from a collection of hypergraphs. To discover more interesting patterns beyond the traditional frequent patterns, we propose frameworks for weighted and uncertain hypergraph mining also. We develop three algorithms for mining frequent, weighted, and uncertain hypergraph patterns efficiently by introducing a canonical labeling technique for isomorphic hypergraphs. Extensive experiments have been conducted on real-life hypergraph databases to show both the effectiveness and efficiency of our proposed frameworks and algorithms.}
}


@article{DBLP:journals/tkdd/LiLDMSZ24,
	author = {Fangfang Li and
                  Zhi Liu and
                  Junwen Duan and
                  Xingliang Mao and
                  Heyuan Shi and
                  Shichao Zhang},
	title = {Exploiting Conversation-Branch-Tweet HyperGraph Structure to Detect
                  Misinformation on Social Media},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {33:1--33:20},
	year = {2024},
	url = {https://doi.org/10.1145/3610297},
	doi = {10.1145/3610297},
	timestamp = {Mon, 18 Nov 2024 08:02:15 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLDMSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The spread of misinformation on social media is a serious issue that can have negative consequences for public health and political stability. While detecting and identifying misinformation can be challenging, many attempts have been made to address this problem. However, traditional models that focus on pairwise relationships on misinformation propagation paths may not be effective in capturing the underlying connections among multiple tweets. To address this limitation, the proposed “Conversation-Branch-Tweet” hypergraph convolutional network (CBT-HGCN) uses a hypergraph to represent the internal structure and content of tweet data, with tweets and their replies viewed as nodes and hyperedges, respectively. The model first pre-processes the tweets of a conversation and then uses a pre-trained model as an encoder to extract node information. Finally, a hypergraph convolution network is used as an information fuser for classification. Experimental results on three benchmark datasets (Twitter15, Twitter16, and Pheme) show that the proposed model outperforms several strong baseline models and achieves state-of-the-art performance. This indicates that the CBT-HGCN approach is effective in detecting and identifying misinformation on social media by capturing the underlying connections among multiple tweets.}
}


@article{DBLP:journals/tkdd/LuoJGMLYZ24,
	author = {Xiao Luo and
                  Wei Ju and
                  Yiyang Gu and
                  Zhengyang Mao and
                  Luchen Liu and
                  Yuhui Yuan and
                  Ming Zhang},
	title = {Self-supervised Graph-level Representation Learning with Adversarial
                  Contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {34:1--34:23},
	year = {2024},
	url = {https://doi.org/10.1145/3624018},
	doi = {10.1145/3624018},
	timestamp = {Mon, 08 Jan 2024 07:53:03 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LuoJGMLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recently developed unsupervised graph representation learning approaches apply contrastive learning into graph-structured data and achieve promising performance. However, these methods mainly focus on graph augmentation for positive samples, while the negative mining strategies for graph contrastive learning are less explored, leading to sub-optimal performance. To tackle this issue, we propose a Graph Adversarial Contrastive Learning (GraphACL) scheme that learns a bank of negative samples for effective self-supervised whole-graph representation learning. Our GraphACL consists of (i) a graph encoding branch that generates the representations of positive samples and (ii) an adversarial generation branch that produces a bank of negative samples. To generate more powerful hard negative samples, our method minimizes the contrastive loss during encoding updating while maximizing the contrastive loss adversarially over the negative samples for providing the challenging contrastive task. Moreover, the quality of representations produced by the adversarial generation branch is enhanced through the regularization of carefully designed bank divergence loss and bank orthogonality loss. We optimize the parameters of the graph encoding branch and adversarial generation branch alternately. Extensive experiments on 14 real-world benchmarks on both graph classification and transfer learning tasks demonstrate the effectiveness of the proposed approach over existing graph self-supervised representation learning methods.}
}


@article{DBLP:journals/tkdd/MillerSRVEA24,
	author = {Benjamin A. Miller and
                  Zohair Shafi and
                  Wheeler Ruml and
                  Yevgeniy Vorobeychik and
                  Tina Eliassi{-}Rad and
                  Scott Alfeld},
	title = {Attacking Shortest Paths by Cutting Edges},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {35:1--35:42},
	year = {2024},
	url = {https://doi.org/10.1145/3622941},
	doi = {10.1145/3622941},
	timestamp = {Tue, 02 Jan 2024 12:25:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/MillerSRVEA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying shortest paths between nodes in a network is a common graph analysis problem that is important for many applications involving routing of resources. An adversary that can manipulate the graph structure could alter traffic patterns to gain some benefit (e.g., make more money by directing traffic to a toll road). This article presents the Force Path Cut problem, in which an adversary removes edges from a graph to make a particular path the shortest between its terminal nodes. We prove that the optimization version of this problem is APX-hard but introduce PATHATTACK, a polynomial-time approximation algorithm that guarantees a solution within a logarithmic factor of the optimal value. In addition, we introduce the Force Edge Cut and Force Node Cut problems, in which the adversary targets a particular edge or node, respectively, rather than an entire path. We derive a nonconvex optimization formulation for these problems and derive a heuristic algorithm that uses PATHATTACK as a subroutine. We demonstrate all of these algorithms on a diverse set of real and synthetic networks, illustrating where the proposed algorithms provide the greatest improvement over baseline methods.}
}


@article{DBLP:journals/tkdd/SpinnatoGMNPG24,
	author = {Francesco Spinnato and
                  Riccardo Guidotti and
                  Anna Monreale and
                  Mirco Nanni and
                  Dino Pedreschi and
                  Fosca Giannotti},
	title = {Understanding Any Time Series Classifier with a Subsequence-based
                  Explainer},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {36:1--36:34},
	year = {2024},
	url = {https://doi.org/10.1145/3624480},
	doi = {10.1145/3624480},
	timestamp = {Tue, 02 Jan 2024 12:25:39 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SpinnatoGMNPG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box’s decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.}
}


@article{DBLP:journals/tkdd/YuLLLWL24,
	author = {Kui Yu and
                  Zhaolong Ling and
                  Lin Liu and
                  Peipei Li and
                  Hao Wang and
                  Jiuyong Li},
	title = {Feature Selection for Efficient Local-to-global Bayesian Network Structure
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {37:1--37:27},
	year = {2024},
	url = {https://doi.org/10.1145/3624479},
	doi = {10.1145/3624479},
	timestamp = {Tue, 28 May 2024 21:58:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YuLLLWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local-to-global learning approach plays an essential role in Bayesian network (BN) structure learning. Existing local-to-global learning algorithms first construct the skeleton of a DAG (directed acyclic graph) by learning the MB (Markov blanket) or PC (parents and children) of each variable in a dataset, then orient edges in the skeleton. However, existing MB or PC learning methods are often computationally expensive especially with a large-sized BN, resulting in inefficient local-to-global learning algorithms. To tackle the problem, in this article, we link feature selection with local BN structure learning and develop an efficient local-to-global learning approach using filtering feature selection. Specifically, we first analyze the rationale of the well-known Minimum-Redundancy and Maximum-Relevance (MRMR) feature selection approach for learning a PC set of a variable. Based on the analysis, we propose an efficient F2SL (feature selection-based structure learning) approach to local-to-global BN structure learning. The F2SL approach first employs the MRMR approach to learn the skeleton of a DAG, then orients edges in the skeleton. Employing independence tests or score functions for orienting edges, we instantiate the F2SL approach into two new algorithms, F2SL-c (using independence tests) and F2SL-s (using score functions). Compared to the state-of-the-art local-to-global BN learning algorithms, the experiments validated that the proposed algorithms in this article are more efficient and provide competitive structure learning quality than the compared algorithms.}
}


@article{10.1145/3624986,
	author = {Cai, Ruichu and Wu, Fengzhu and Li, Zijian and Qiao, Jie and Chen, Wei and Hao, Yuexing and Gu, Hao},
	title = {REST: Debiased Social Recommendation via Reconstructing Exposure Strategies},
	journal = {ACM Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	year = {2023},
	url = {https://doi.org/10.1145/3624986},
	doi = {10.1145/3624986},
	abstract = {The recommendation system, relying on historical observational data to model the complex relationships among users and items, has achieved great success in real-world applications. Selection bias is one of the most important issues of the existing observational data-based approaches, which is actually caused by multiple types of unobserved exposure strategies (e.g., promotions and holiday effects). Though various methods have been proposed to address this problem, they are mainly relying on the implicit debiasing techniques but not explicitly modeling the unobserved exposure strategies. By explicitly Reconstructing Exposure STrategies (REST), we formalize the recommendation problem as the counterfactual reasoning and propose the debiased social recommendation method. In REST, we assume that the exposure of an item is controlled by the latent exposure strategies, the user, and the item. Based on the above generation process, we first provide the theoretical guarantee of our method via identification analysis. Second, we employ a variational auto-encoder to reconstruct the latent exposure strategies, with the help of the social networks and the items. Third, we devise a counterfactual reasoning based recommendation algorithm by leveraging the recovered exposure strategies. Experiments on four real-world datasets, including three published datasets and one private WeChat Official Account dataset, demonstrate significant improvements over several state-of-the-art methods.},
	articleno = {38},
	numpages = {24},
	keywords = {variational auto-encoders, causal effect, social recommendation system, Recommendation system}
}


@article{DBLP:journals/tkdd/YeSLL24,
	author = {Xiaoqing Ye and
                  Yang Sun and
                  Dun Liu and
                  Tianrui Li},
	title = {A Multisource Data Fusion-based Heterogeneous Graph Attention Network
                  for Competitor Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {39:1--39:20},
	year = {2024},
	url = {https://doi.org/10.1145/3625101},
	doi = {10.1145/3625101},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YeSLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.}
}


@article{DBLP:journals/tkdd/ZhangMCCSZ24,
	author = {Taolin Zhang and
                  Chengyuan Mai and
                  Yaomin Chang and
                  Chuan Chen and
                  Lin Shu and
                  Zibin Zheng},
	title = {FedEgo: Privacy-preserving Personalized Federated Graph Learning with
                  Ego-graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {40:1--40:27},
	year = {2024},
	url = {https://doi.org/10.1145/3624017},
	doi = {10.1145/3624017},
	timestamp = {Mon, 18 Nov 2024 16:04:56 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangMCCSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As special information carriers containing both structure and feature information, graphs are widely used in graph mining, e.g., Graph Neural Networks (GNNs). However, graph data are stored separately in multiple distributed parties in some practical scenarios, which may not be directly shared due to conflicts of interest. Hence, federated graph neural networks are proposed to address such data silo issues while preserving each party’s privacy (or client). Nevertheless, different graph data distributions of various parties, which is known as the statistical heterogeneity, may degrade the performance of naive federated learning algorithms like FedAvg. In this article, we propose FedEgo, a federated graph learning framework based on ego-graphs to tackle the challenges above, in which each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. Extensive experimental results and in-depth analysis demonstrate the effectiveness of FedEgo.}
}


@article{DBLP:journals/tkdd/LuWHLY24,
	author = {Xiangkui Lu and
                  Jun Wu and
                  Junheng Huang and
                  Fangyuan Luo and
                  Jianbo Yuan},
	title = {Co-Training-Teaching: {A} Robust Semi-Supervised Framework for Review-Aware
                  Rating Regression},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {41:1--41:16},
	year = {2024},
	url = {https://doi.org/10.1145/3625391},
	doi = {10.1145/3625391},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LuWHLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Review-aware Rating Regression (RaRR) suffers the severe challenge of extreme data sparsity as the multi-modality interactions of ratings accompanied by reviews are costly to obtain. Although some studies of semi-supervised rating regression are proposed to mitigate the impact of sparse data, they bear the risk of learning from noisy pseudo-labeled data. In this article, we propose a simple yet effective paradigm, called co-training-teaching (CoT2), for integrating the merits of both co-training and co-teaching toward robust semi-supervised RaRR. CoT2 employs two predictors trained with different feature sets of textual reviews, each of which functions as both “labeler” and “validator.” Specifically, one predictor (labeler) first labels unlabeled data for its peer predictor (validator); after that, the validator samples reliable instances from the noisy pseudo-labeled data it received and sends them back to the labeler for updating. By exchanging and validating pseudo-labeled instances, the two predictors are reinforced by each other in an iterative learning process. The final prediction is made by averaging the outputs of both the refined predictors. Extensive experiments show that our CoT2 considerably outperforms the state-of-the-art recommendation techniques in the RaRR task, especially when the training data is severely insufficient.}
}


@article{DBLP:journals/tkdd/ZareieS24,
	author = {Ahmad Zareie and
                  Rizos Sakellariou},
	title = {Maximizing the Diversity of Exposure in Online Social Networks by
                  Identifying Users with Increased Susceptibility to Persuasion},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {42:1--42:21},
	year = {2024},
	url = {https://doi.org/10.1145/3625826},
	doi = {10.1145/3625826},
	timestamp = {Sat, 13 Jan 2024 17:35:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZareieS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Individuals may have a range of opinions on controversial topics. However, the ease of making friendships in online social networks tends to create groups of like-minded individuals, who propagate messages that reinforce existing opinions and ignore messages expressing opposite opinions. This creates a situation where there is a decrease in the diversity of messages to which users are exposed (diversity of exposure). This means that users do not easily get the chance to be exposed to messages containing alternative viewpoints; it is even more unlikely that they forward such messages to their friends. Increasing the chance that such messages are propagated implies that an individuals’ susceptibility to persuasion is increased, something that may ultimately increase the diversity of messages to which users are exposed. This article formulates a novel problem which aims to identify a small set of users for whom increasing susceptibility to persuasion maximizes the diversity of exposure of all users in the network. We study the properties of this problem and develop a method to find a solution with an approximation guarantee. For this, we first prove that the problem is neither submodular nor supermodular and then we develop submodular bounds for it. These bounds are used in the Sandwich framework to propose a method which approximates the solution using reverse sampling. The proposed method is validated using four real-world datasets. The obtained results demonstrate the superiority of the proposed method compared to baseline approaches.}
}


@article{DBLP:journals/tkdd/XuXOWWZ24,
	author = {Hui Xu and
                  Liyao Xiang and
                  Junjie Ou and
                  Yuting Weng and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Open-World Graph Active Learning for Node Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {43:1--43:20},
	year = {2024},
	url = {https://doi.org/10.1145/3607144},
	doi = {10.1145/3607144},
	timestamp = {Sat, 13 Jan 2024 17:35:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XuXOWWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The great power of Graph Neural Networks (GNNs) relies on a large number of labeled training data, but obtaining the labels can be costly in many cases. Graph Active Learning (GAL) is proposed to reduce such annotation costs, but the existing methods mainly focus on improving labeling efficiency with fixed classes, and are limited to handle the emergence of novel classes. We term the problem as Open-World Graph Active Learning (OWGAL) and propose a framework of the same name. The key is to recognize novel-class as well as informative nodes in a unified framework. Instead of a fully connected neural network classifier, OWGAL employs prototype learning and label propagation to assign high uncertainty scores to the targeted nodes in the representation and topology space, respectively. Weighted sampling further suppresses the impact of unimportant classes by weighing both the node and class importance. Experimental results on four large-scale datasets demonstrate that our framework achieves a substantial improvement of 5.97% to 16.57% on Macro-F1 over state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/ChenQHLLTYGZ24,
	author = {Ying Chen and
                  Siwei Qiang and
                  Mingming Ha and
                  Xiaolei Liu and
                  Shaoshuai Li and
                  Jiabi Tong and
                  Lingfeng Yuan and
                  Xiaobo Guo and
                  Zhenfeng Zhu},
	title = {Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data
                  Augmentation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {44:1--44:27},
	year = {2024},
	url = {https://doi.org/10.1145/3608953},
	doi = {10.1145/3608953},
	timestamp = {Sat, 13 Jan 2024 17:35:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenQHLLTYGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, semi-supervised graph learning with data augmentation (DA) has been the most commonly used and best-performing method to improve model robustness in sparse scenarios with few labeled samples. However, most existing DA methods are based on the homogeneous graph, but none are specific for the heterogeneous graph. Differing from the homogeneous graph, DA in the heterogeneous graph faces greater challenges: heterogeneity of information requires DA strategies to effectively handle heterogeneous relations, which considers the information contribution of different types of neighbors and edges to the target nodes. Furthermore, over-squashing of information is caused by the negative curvature formed by the non-uniformity distribution and the strong clustering in a complex graph. To address these challenges, this article presents a novel method named HG-MDA (Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation). For the problem of heterogeneity of information in DA, node and topology augmentation strategies are proposed for the characteristics of the heterogeneous graph. Additionally, meta-relation-based attention is applied as one of the indexes for selecting augmented nodes and edges. For the problem of over-squashing of information, triangle-based edge adding and removing are designed to alleviate the negative curvature and bring the gain of topology. Finally, the loss function consists of the cross-entropy loss for labeled data and the consistency regularization for unlabeled data. To effectively fuse the prediction results of various DA strategies, sharpening is used. Existing experiments on public datasets (i.e., ACM, DBLP, and OGB) and the industry dataset MB show that HG-MDA outperforms current SOTA models. Additionally, HG-MDA is applied to user identification in internet finance scenarios, helping the business to add 30% key users, and increase loans and balances by 3.6%, 11.1%, and 9.8%.}
}


@article{DBLP:journals/tkdd/WangLSCFLG24,
	author = {Huan Wang and
                  Ruigang Liu and
                  Chuanqi Shi and
                  Junyang Chen and
                  Lei Fang and
                  Shun Liu and
                  Zhiguo Gong},
	title = {Resisting the Edge-Type Disturbance for Link Prediction in Heterogeneous
                  Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {45:1--45:24},
	year = {2024},
	url = {https://doi.org/10.1145/3614099},
	doi = {10.1145/3614099},
	timestamp = {Thu, 08 Aug 2024 12:56:51 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLSCFLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of heterogeneous networks has proposed new challenges to the long-standing link prediction problem. Existing models trained on the verified edge samples from different types usually learn type-specific knowledge, and their type-specific predictions may be contradictory for unverified edge samples with uncertain types. This challenge is termed edge-type disturbance in link prediction in heterogeneous networks. To address this challenge, we develop a disturbance-resilient prediction method (DRPM) comprising a structural characterizer, a type differentiator, and a resilient predictor. The structural characterizer is responsible for learning edge representations for link prediction. Concurrently, the type differentiator distinguishes type-specific edge representations to generate diverse type experts while maximizing their link prediction performances on specific types. Furthermore, the resilient predictor evaluates the reliability weights of different type experts to develop a resilient prediction mechanism to aggregate discriminable predictions. Extensive experiments conducted on various real-world datasets demonstrate the importance of the explainable introduction of the edge-type disturbance and the superiority of DRPM over state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/LiCW24,
	author = {Xiaoting Li and
                  Lingwei Chen and
                  Dinghao Wu},
	title = {Adversary for Social Good: Leveraging Adversarial Attacks to Protect
                  Personal Attribute Privacy},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {46:1--46:24},
	year = {2024},
	url = {https://doi.org/10.1145/3614098},
	doi = {10.1145/3614098},
	timestamp = {Sat, 13 Jan 2024 17:35:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiCW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials have been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (NLP) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users’ attribute privacy at risk. To address this challenge, in this article, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adversarial attack for Social Good, called Adv4SG. In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against NLP-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.}
}


@article{DBLP:journals/tkdd/YanYSFLYD24,
	author = {Bo Yan and
                  Cheng Yang and
                  Chuan Shi and
                  Yong Fang and
                  Qi Li and
                  Yanfang Ye and
                  Junping Du},
	title = {Graph Mining for Cybersecurity: {A} Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {2},
	pages = {47:1--47:52},
	year = {2024},
	url = {https://doi.org/10.1145/3610228},
	doi = {10.1145/3610228},
	timestamp = {Fri, 21 Feb 2025 15:09:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YanYSFLYD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.}
}


@article{DBLP:journals/tkdd/HuangMLGSC24,
	author = {Qiang Huang and
                  Jing Ma and
                  Jundong Li and
                  Ruocheng Guo and
                  Huiyan Sun and
                  Yi Chang},
	title = {Modeling Interference for Individual Treatment Effect Estimation from
                  Networked Observational Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {48:1--48:21},
	year = {2024},
	url = {https://doi.org/10.1145/3628449},
	doi = {10.1145/3628449},
	timestamp = {Sun, 19 Jan 2025 14:58:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangMLGSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating individual treatment effect (ITE) from observational data has attracted great interest in recent years, which plays a crucial role in decision-making across many high-impact domains such as economics, medicine, and e-commerce. Most existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of other units connected to it in the network, which is referred to as the interference or spillover effect. In this article, we study an important problem of ITE estimation from networked observational data by modeling the interference between different units and provide a principled framework to support such study. Methodologically, we propose a novel framework, SPNet, that first captures the influence of hidden confounders with the aid of graph convolutional network and then models the interference by introducing an environment summary variable and developing a masked attention mechanism. Experimental evaluations on several semi-synthetic datasets based on real-world networks corroborate the superiority of our proposed framework over state-of-the-art individual treatment effect estimation methods.}
}


@article{DBLP:journals/tkdd/GonzalezZelayaSMM24,
	author = {Vladimiro Gonz{\'{a}}lez{-}Zelaya and
                  Juli{\'{a}}n Salas and
                  David Meg{\'{\i}}as and
                  Paolo Missier},
	title = {Fair and Private Data Preprocessing through Microaggregation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {49:1--49:24},
	year = {2024},
	url = {https://doi.org/10.1145/3617377},
	doi = {10.1145/3617377},
	timestamp = {Thu, 29 Feb 2024 20:54:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GonzalezZelayaSMM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy protection for personal data and fairness in automated decisions are fundamental requirements for responsible Machine Learning. Both may be enforced through data preprocessing and share a common target: data should remain useful for a task, while becoming uninformative of the sensitive information. The intrinsic connection between privacy and fairness implies that modifications performed to guarantee one of these goals, may have an effect on the other, e.g., hiding a sensitive attribute from a classification algorithm might prevent a biased decision rule having such attribute as a criterion. This work resides at the intersection of algorithmic fairness and privacy. We show how the two goals are compatible, and may be simultaneously achieved, with a small loss in predictive performance. Our results are competitive with both state-of-the-art fairness correcting algorithms and hybrid privacy-fairness methods. Experiments were performed on three widely used benchmark datasets: Adult Income, COMPAS, and German Credit.}
}


@article{DBLP:journals/tkdd/ChengZWLL24,
	author = {Ling Cheng and
                  Feida Zhu and
                  Yong Wang and
                  Ruicheng Liang and
                  Huiwen Liu},
	title = {From Asset Flow to Status, Action, and Intention Discovery: Early
                  Malice Detection in Cryptocurrency},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {50:1--50:27},
	year = {2024},
	url = {https://doi.org/10.1145/3626102},
	doi = {10.1145/3626102},
	timestamp = {Thu, 13 Feb 2025 14:31:48 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChengZWLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of early detection, good interpretability, and versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention Monitor for early malice detection in Bitcoin, where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision Tree based feature Selection and Complement to build different feature sets for different malice types. Then, the Status/Action Proposal module and the Intention-VAE module generate the status, action, intent-snippet, and hidden intent-snippet embedding. With all these modules, our model is highly interpretable and can detect various illegal activities. Moreover, well-designed loss functions further enhance the prediction speed and the model’s interpretability. Extensive experiments on three real-world datasets demonstrate that our proposed algorithm outperforms the state-of-the-art methods. Furthermore, additional case studies justify that our model not only explains existing illicit patterns but also can find new suspicious characters.}
}


@article{DBLP:journals/tkdd/HuangWZWFHY24,
	author = {Yimin Huang and
                  Wanwan Wang and
                  Xingying Zhao and
                  Yukun Wang and
                  Xinyu Feng and
                  Hao He and
                  Ming Yao},
	title = {{EFMVFL:} An Efficient and Flexible Multi-party Vertical Federated
                  Learning without a Third Party},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {51:1--51:20},
	year = {2024},
	url = {https://doi.org/10.1145/3627993},
	doi = {10.1145/3627993},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangWZWFHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a machine learning setting which allows multiple participants collaboratively to train a model under the orchestration of a server without disclosing their local data. Vertical federated learning (VFL) is a special structure in FL. It handles the situation where participants have the same ID space but different feature spaces. In order to guarantee the security and privacy of the local data of each participant, homomorphic encryption (HE) is often used to transmit intermediate parameters or data during the training process. In most VFL frameworks, a trusted third-party server is necessary because the plaintexts of the parameters need to be revealed for the computation. However, it is hard to find such a credible entity in the real world. Existing methods for solving this problem are either communication-intensive or unsuitable for multi-party scenarios. By combining secret sharing (SS) and HE, we propose a novel VFL framework without any trusted third parties called EFMVFL. It allows intermediate parameters to be transmitted among multiple parties without revealing the plaintexts. EFMVFL is applicable to generalized linear models (GLMs) and supports flexible expansion to multiple participants. Extensive experiments under Logistic Regression and Poisson Regression show that our framework is outstanding in communication (reduced by 3.2×– 6.8×) and efficiency (accelerated by 1.6×– 3.1×).}
}


@article{DBLP:journals/tkdd/PellegrinaV24,
	author = {Leonardo Pellegrina and
                  Fabio Vandin},
	title = {{SILVAN:} Estimating Betweenness Centralities with Progressive Sampling
                  and Non-uniform Rademacher Bounds},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {52:1--52:55},
	year = {2024},
	url = {https://doi.org/10.1145/3628601},
	doi = {10.1145/3628601},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PellegrinaV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {“Sim Sala Bim!” —Silvan, https://en.wikipedia.org/wiki/Silvan_(illusionist) Betweenness centrality is a popular centrality measure with applications in several domains and whose exact computation is impractical for modern-sized networks. We present SILVAN, a novel, efficient algorithm to compute, with high probability, accurate estimates of the betweenness centrality of all nodes of a graph and a high-quality approximation of the top-k betweenness centralities. SILVAN follows a progressive sampling approach and builds on novel bounds based on Monte Carlo Empirical Rademacher Averages, a powerful and flexible tool from statistical learning theory. SILVAN relies on a novel estimation scheme providing non-uniform bounds on the deviation of the estimates of the betweenness centrality of all the nodes from their true values and a refined characterisation of the number of samples required to obtain a high-quality approximation. Our extensive experimental evaluation shows that SILVAN extracts high-quality approximations while outperforming, in terms of number of samples and accuracy, the state-of-the-art approximation algorithm with comparable quality guarantees.}
}


@article{DBLP:journals/tkdd/XiaLQFXSGJ24,
	author = {Tong Xia and
                  Yong Li and
                  Yunhan Qi and
                  Jie Feng and
                  Fengli Xu and
                  Funing Sun and
                  Diansheng Guo and
                  Depeng Jin},
	title = {History-enhanced and Uncertainty-aware Trajectory Recovery via Attentive
                  Neural Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {53:1--53:22},
	year = {2024},
	url = {https://doi.org/10.1145/3615660},
	doi = {10.1145/3615660},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaLQFXSGJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A considerable amount of mobility data has been accumulated due to the proliferation of location-based services. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even if it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. In addition, to guarantee the robustness of the generated trajectories to avoid harming downstream applications, we also exploit the Bayesian approximate neural network to estimate the uncertainty of each imputation. As a result, locations generated by the model with high uncertainty will be excluded. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. In-depth analyses of each design of our model have been conducted to understand their contribution. We also show that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented downstream applications.}
}


@article{DBLP:journals/tkdd/AoLWYCQHX24,
	author = {Xiang Ao and
                  Ling Luo and
                  Xiting Wang and
                  Zhao Yang and
                  Jiun{-}Hung Chen and
                  Ying Qiao and
                  Qing He and
                  Xing Xie},
	title = {Put Your Voice on Stage: Personalized Headline Generation for News
                  Articles},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {54:1--54:20},
	year = {2024},
	url = {https://doi.org/10.1145/3629168},
	doi = {10.1145/3629168},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AoLWYCQHX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we study the problem of personalized news headline generation, which aims to produce not only concise and fact-consistent titles for news articles but also decorate these titles as personalized irresistible reading invitations by incorporating readers’ preferences. We propose an approach named PNG\xa0(Personalized News headline Generator) by utilizing distant supervision in readers’ past click behaviors to resolve. First, user preference representations are learned through a knowledge-aware user encoder that comprehensively captures the genuine, sequential, and flash interests of users reflected in their historical clicked news. Then, a user-perturbed pointer-generator network is devised to accomplish the headline generation in which the learned user representations implicitly affect the word prediction. The proposed model is optimized by reinforcement learning solvers where indicators on factual, personalized, and linguistic aspects of the generated headline are regarded as rewards. Extensive experiments are conducted on the real-world dataset PENS, which is a large-scale benchmark collected from Microsoft News. Both the quantitative and qualitative results validate the effectiveness of our approach.}
}


@article{DBLP:journals/tkdd/ChenXWH24,
	author = {Ling Chen and
                  Jiahui Xu and
                  Binqing Wu and
                  Jianlong Huang},
	title = {Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {55:1--55:20},
	year = {2024},
	url = {https://doi.org/10.1145/3631713},
	doi = {10.1145/3631713},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenXWH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of air pollution threatens public health. Air quality forecasting can provide the air quality index hours or even days later, which can help the public to prevent air pollution in advance. Previous works focus on citywide air quality forecasting and cannot solve nationwide city forecasting problems, whose difficulties lie in capturing the latent dependencies between geographically distant but highly correlated cities. In this article, we propose the group-aware graph neural network (GAGNN), a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces a differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups. The evaluation experiments on two real-world nationwide city air quality datasets, including the China dataset and the US dataset, indicate that our GAGNN outperforms existing forecasting models.}
}


@article{DBLP:journals/tkdd/LiangLHSYZ24,
	author = {Yunji Liang and
                  Lei Liu and
                  Luwen Huangfu and
                  Sagar Samtani and
                  Zhiwen Yu and
                  Daniel Dajun Zeng},
	title = {Learning Entangled Interactions of Complex Causality via Self-Paced
                  Contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {56:1--56:24},
	year = {2024},
	url = {https://doi.org/10.1145/3632406},
	doi = {10.1145/3632406},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiangLHSYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning causality from large-scale text corpora is an important task with numerous applications—for example, in finance, biology, medicine, and scientific discovery. Prior studies have focused mainly on simple causality, which only includes one cause-effect pair. However, causality is notoriously difficult to understand and analyze because of multiple cause spans and their entangled interactions. To detect complex causality, we propose a self-paced contrastive learning model, namely N2NCause, to learn entangled interactions between multiple spans. Specifically, N2NCause introduces data enhancement operations to convert implicit expressions into explicit expressions with the most rational causal connectives for the synthesis of positive samples and to invert the directed connection between a cause-effect pair for the synthesis of negative samples. To learn the semantic dependency and causal direction of positive and negative samples, self-paced contrastive learning is proposed to learn the entangled interactions among spans, including the interaction direction and interaction field. We evaluated the performance of N2NCause in three cause-effect detection tasks. The experimental results show that, with the least data annotation efforts, N2NCause demonstrates competitive performance in detecting simple cause-effect relations, and it is superior to existing solutions for the detection of complex causality.}
}


@article{DBLP:journals/tkdd/HsuCH24,
	author = {Chi{-}Wei Hsu and
                  Chiao{-}Ting Chen and
                  Szu{-}Hao Huang},
	title = {Adaptive Adversarial Contrastive Learning for Cross-Domain Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {57:1--57:34},
	year = {2024},
	url = {https://doi.org/10.1145/3630259},
	doi = {10.1145/3630259},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HsuCH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based cross-domain recommendations (CDRs) are useful for suggesting appropriate items because of their promising ability to extract features from user–item interactions and transfer knowledge across domains. Thus, the model can effectively alleviate cold start and data sparsity issues. Although the graph-based CDRs can capture valuable information, they still have some limitations. First, embeddings are highly vulnerable to noisy interactions, because the message aggregation in the graph convolutional network can further enlarge the impact. Second, because of the property of graph-structured data, the influence of high-degree nodes on representation learning is more than that of the long-tail items, and this can cause a poor recommendation performance. In this study, we devised a novel Adaptive Adversarial Contrastive Learning framework for graph-based Cross-Domain Recommendation (ACLCDR). The ACLCDR introduces reinforcement learning to generate adaptive augmented samples for contrastive learning tasks. Then, we leveraged a multitask training strategy to jointly optimize the model with auxiliary tasks. Finally, we verified the effectiveness of the ACLCDR through nine real-world cross-domain tasks adopted from Amazon and Douban. We observed that ACLCDR exceeded the best state-of-the-art baseline by 25%, 42.5%, 16.3%, and 23.8% in terms of HR@10 and NDCG@10 for the Music & Movie task from Amazon.}
}


@article{DBLP:journals/tkdd/OuCDZLTY24,
	author = {Weitong Ou and
                  Bo Chen and
                  Xinyi Dai and
                  Weinan Zhang and
                  Weiwen Liu and
                  Ruiming Tang and
                  Yong Yu},
	title = {A Survey on Bid Optimization in Real-Time Bidding Display Advertising},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {58:1--58:31},
	year = {2024},
	url = {https://doi.org/10.1145/3628603},
	doi = {10.1145/3628603},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/OuCDZLTY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-Time Bidding (RTB) is one of the most important forms of online advertising, where an auction is hosted in real time to sell the individual ad impression. How to design an automated bidding strategy in response to the dynamic auction environment is crucial for improving user experience, protecting the interests of advertisers, and promoting the long-term development of the advertising platform. As an exciting topic in the real-world industry, it has attracted great research interest from several disciplines, most notably data science. There have been abundant studies on bidding strategy design which are based on the large volume of historical ad requests. Despite its popularity and significance, few works provide a summary for bid optimization. In this survey, we present the latest overview of the recent works to shed light on the optimization techniques where most of them are validated in practice. We first explore the optimization problem in different works, explaining how these different settings affect the bidding strategy designs. Then, some forms of bidding functions and specific optimization techniques are illustrated. Further, we specifically discuss a new trend about bidding in first-price auctions, which have gradually become popular in recent years. From this survey, both practitioners and researchers can gain insights of the challenges and future prospects of bid optimization in RTB.}
}


@article{DBLP:journals/tkdd/NiXZLHS24,
	author = {Li Ni and
                  Hefei Xu and
                  Yiwen Zhang and
                  Wenjian Luo and
                  Yingying Huang and
                  Victor S. Sheng},
	title = {Local Overlapping Spatial-aware Community Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {59:1--59:21},
	year = {2024},
	url = {https://doi.org/10.1145/3634707},
	doi = {10.1145/3634707},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/NiXZLHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Local spatial-aware community detection refers to detecting a spatial-aware community for a given node using local information. A spatial-aware community means that nodes in the community are tightly connected in structure, and their locations are close to each other. Existing studies focus on detecting the local non-overlapping spatial-aware community, i.e., detecting a spatial-aware community containing the given node. However, many geosocial networks often contain overlapping spatial-aware communities. Therefore, we propose a local overlapping spatial-aware community detection (LOSCD) problem, which aims to detect all spatial-aware communities that contain a given node with local information. To address LOSCD problem, we design an algorithm based on Spatial Modularity and Edge Similarity, called SMES. SMES contains two processes: spatial expansion and structure detection. The spatial expansion process involves using spatial modularity to identify nodes that are spatially close, while the structural detection process employs edge similarity to identify nodes that are structurally close. Experimental results demonstrate that SMES outperforms comparison algorithms in terms of both structural and spatial cohesiveness.}
}


@article{DBLP:journals/tkdd/CaiWLWYZ24,
	author = {Ruichu Cai and
                  Fengzhu Wu and
                  Zijian Li and
                  Pengfei Wei and
                  Lingling Yi and
                  Kun Zhang},
	title = {Graph Domain Adaptation: {A} Generative View},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {60:1--60:24},
	year = {2024},
	url = {https://doi.org/10.1145/3631712},
	doi = {10.1145/3631712},
	timestamp = {Tue, 20 Aug 2024 19:58:50 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CaiWLWYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowds or communities but also by domain-specific policies and background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms. The code is available at https://github.com/rynewu224/GraphDA.}
}


@article{DBLP:journals/tkdd/SuYZWWZD24,
	author = {Cong Su and
                  Guoxian Yu and
                  Yongqing Zheng and
                  Jun Wang and
                  Zhengtian Wu and
                  Xiangliang Zhang and
                  Carlotta Domeniconi},
	title = {Causality-Based Fair Multiple Decision by Response Functions},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {61:1--61:23},
	year = {2024},
	url = {https://doi.org/10.1145/3632529},
	doi = {10.1145/3632529},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SuYZWWZD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A recent trend of fair machine learning is to build a decision model subjected to causality-based fairness requirements, which concern with the causality between sensitive attributes and decisions. Almost all (if not all) solutions focus on a single fair decision model and assume no hidden confounder to model causal effects in a too simplified way. However, multiple interdependent decision models are actually used and discrimination may transmit among them. The hidden confounder is another inescapable fact and causal effects cannot be computed from observational data in the unidentifiable situation. To address these problems, we propose a method called CMFL (Causality-based Multiple Fairness Learning). CMFL parameterizes the causal model by response-function variables, whose distributions capture the randomness of causal models. CMFL treats each classifier as a soft intervention to infer the post-intervention distribution, and combines the fairness constraints with the classification loss to train multiple decision classifiers. In this way, all classifiers can make approximately fair decisions. Experiments on synthetic and benchmark datasets confirm its effectiveness, the response-function variables can deal with the unidentifiable issue and hidden confounders.}
}


@article{DBLP:journals/tkdd/HanHLLL24,
	author = {Di Han and
                  Yifan Huang and
                  Junmin Liu and
                  Kai Liao and
                  Kunling Lin},
	title = {{LSAB:} User Behavioral Pattern Modeling in Sequential Recommendation
                  by Learning Self-Attention Bias},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {62:1--62:20},
	year = {2024},
	url = {https://doi.org/10.1145/3632625},
	doi = {10.1145/3632625},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HanHLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the weight of a self-attention model is not affected by the sequence interval, it can more accurately and completely describe the user interests, so it is widely used in processing sequential recommendation. However, the mainstream self-attention model focuses on the similarity between items when calculating the attention weight of user behavioral patterns but fails to reflect the impact of user sudden drift decisions on the model in time. In this article, we introduce a bias strategy in the self-attention module, referred to as Learning Self-Attention Bias (LSAB) to more accurately learn the fast-changing user behavioral patterns. The introduction of LSAB allows for the adjustment of bias resulting from self-attention weights, leading to enhanced prediction performance in sequential recommendation. In addition, this article designs four attention-weight bias types catering to diverse user behavior preferences. After testing on the benchmark datasets, each bias strategy in LSAB is useful for state-of-the-art and can improve the performance of the models by nearly 5% on average. The source code listing is publicly available at https://gitee.com/kyle-liao/lsab.}
}


@article{DBLP:journals/tkdd/HuangCHRR24,
	author = {Shenyang Huang and
                  Samy Coulombe and
                  Yasmeen Hitti and
                  Reihaneh Rabbany and
                  Guillaume Rabusseau},
	title = {Laplacian Change Point Detection for Single and Multi-view Dynamic
                  Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {63:1--63:32},
	year = {2024},
	url = {https://doi.org/10.1145/3631609},
	doi = {10.1145/3631609},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangCHRR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic graphs are rich data structures that are used to model complex relationships between entities over time. In particular, anomaly detection in temporal graphs is crucial for many real-world applications such as intrusion identification in network systems, detection of ecosystem disturbances, and detection of epidemic outbreaks. In this article, we focus on change point detection in dynamic graphs and address three main challenges associated with this problem: (i) how to compare graph snapshots across time, (ii) how to capture temporal dependencies, and (iii) how to combine different views of a temporal graph. To solve the above challenges, we first propose Laplacian Anomaly Detection (LAD) which uses the spectrum of graph Laplacian as the low dimensional embedding of the graph structure at each snapshot. LAD explicitly models short-term and long-term dependencies by applying two sliding windows. Next, we propose MultiLAD, a simple and effective generalization of LAD to multi-view graphs. MultiLAD provides the first change point detection method for multi-view dynamic graphs. It aggregates the singular values of the normalized graph Laplacian from different views through the scalar power mean operation. Through extensive synthetic experiments, we show that (i) LAD and MultiLAD are accurate and outperforms state-of-the-art baselines and their multi-view extensions by a large margin, (ii) MultiLAD’s advantage over contenders significantly increases when additional views are available, and (iii) MultiLAD is highly robust to noise from individual views. In five real-world dynamic graphs, we demonstrate that LAD and MultiLAD identify significant events as top anomalies such as the implementation of government COVID-19 interventions which impacted the population mobility in multi-view traffic networks.}
}


@article{DBLP:journals/tkdd/SahaI24,
	author = {Swapnil Saha and
                  Hafiz Imtiaz},
	title = {Privacy-Preserving Non-Negative Matrix Factorization with Outliers},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {64:1--64:26},
	year = {2024},
	url = {https://doi.org/10.1145/3632961},
	doi = {10.1145/3632961},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SahaI24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-negative matrix factorization is a popular unsupervised machine learning algorithm for extracting meaningful features from inherently non-negative data. Such data often contain privacy-sensitive user information. Additionally, the dataset can contain outliers, which may lead to extracting sub-optimal features from the data. It is, therefore, necessary to address these two issues while analyzing privacy-sensitive data that may contain outliers. In this work, we develop a non-negative matrix factorization algorithm in the privacy-preserving framework that (i) considers the presence of outliers in the data, and (ii) can achieve results comparable to those of the non-private algorithm. We design our method in such a way that one has the control to select the degree of privacy grantee based on the required utility gap. We show the effectiveness of our proposed algorithm’s performance on six real and diverse datasets. The experimental results show that our proposed method can achieve a performance that closely approximates the performance of the non-private algorithm under some parameter choices, while ensuring strict privacy guarantees.}
}


@article{DBLP:journals/tkdd/YangWC24,
	author = {Ming{-}Chuan Yang and
                  Guo{-}Wei Wong and
                  Meng Chang Chen},
	title = {Sparse Grid Imputation Using Unpaired Imprecise Auxiliary Data: Theory
                  and Application to {PM2.5} Estimation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {65:1--65:26},
	year = {2024},
	url = {https://doi.org/10.1145/3634751},
	doi = {10.1145/3634751},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangWC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse grid imputation (SGI) is a challenging problem, as its goal is to infer the values of the entire grid from a limited number of cells with values. Traditionally, the problem is solved using regression methods such as KNN and kriging, whereas in the real world, there is often extra information—usually imprecise—that can aid inference and yield better performance. In the SGI problem, in addition to the limited number of fixed grid cells with precise target domain values, there are contextual data and imprecise observations over the whole grid. To solve this problem, we propose a distribution estimation theory for the whole grid and realize the theory via the composition architecture of the Target-Embedding and the Contextual CycleGAN trained with contextual information and imprecise observations. Contextual CycleGAN is structured as two generator–discriminator pairs and uses different types of contextual loss to guide the training. We consider the real-world problem of fine-grained PM2.5 inference with realistic settings: a few (less than 1%) grid cells with precise PM2.5 data and all grid cells with contextual information concerning weather and imprecise observations from satellites and microsensors. The task is to infer reasonable values for all grid cells. As there is no ground truth for empty cells, out-of-sample mean squared error and Jensen–Shannon divergence measurements are used in the empirical study. The results show that Contextual CycleGAN supports the proposed theory and outperforms the methods used for comparison.}
}


@article{DBLP:journals/tkdd/ZhaoYD24,
	author = {Han Zhao and
                  Xu Yang and
                  Cheng Deng},
	title = {Parameter-Agnostic Deep Graph Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {66:1--66:20},
	year = {2024},
	url = {https://doi.org/10.1145/3633783},
	doi = {10.1145/3633783},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoYD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep graph clustering, efficiently dividing nodes into multiple disjoint clusters in an unsupervised manner, has become a crucial tool for analyzing ubiquitous graph data. Existing methods have acquired impressive clustering effects by optimizing the clustering network under the parametric condition—predefining the true number of clusters (Ktr). However, Ktr is inaccessible in pure unsupervised scenarios, in which existing methods are incapable of inferring the number of clusters (K), causing limited feasibility. This article proposes the first Parameter-Agnostic Deep Graph Clustering method (PADGC), which consists of two core modules: K-guidence clustering and topological-hierarchical inference, to infer K efficiently and gain impressive clustering predictions. Specifically, K-guidence clustering is employed to optimize the cluster assignments and discriminative embeddings in a mutual promotion manner under the latest updated K, even though K may deviate from Ktr. In turn, such optimized cluster assignments are utilized to explore more accurate K in the topological-hierarchical inference, which can split the dispersive clusters and merge the coupled ones. In this way, these two modules are complementarily optimized until generating the final convergent K and discriminative cluster assignments. Extensive experiments on several benchmarks, including graphs and images, can demonstrate the superiority of our method. The mean values of our inferred K, in 11 out of 12 datasets, deviates from Ktr by less than 1. Our method can also achieve competitive clustering effects with existing parametric deep graph clustering.}
}


@article{DBLP:journals/tkdd/WangDHCL24,
	author = {Song Wang and
                  Yushun Dong and
                  Xiao Huang and
                  Chen Chen and
                  Jundong Li},
	title = {Learning Hierarchical Task Structures for Few-shot Graph Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {67:1--67:20},
	year = {2024},
	url = {https://doi.org/10.1145/3635473},
	doi = {10.1145/3635473},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangDHCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of few-shot graph classification targets at assigning class labels for graph samples, where only limited labeled graphs are provided for each class. To solve the problem brought by label scarcity, recent studies have proposed to adopt the prevalent few-shot learning framework to achieve fast adaptations to graph classes with limited labeled graphs. In particular, these studies typically propose to accumulate meta-knowledge across a large number of meta-training tasks, and then generalize such meta-knowledge to meta-test tasks sampled from a disjoint class set. Nevertheless, existing studies generally ignore the crucial task correlations among meta-training tasks and treat them independently. In fact, such task correlations can help promote the model generalization to meta-test tasks and result in better classification performance. On the other hand, it remains challenging to capture and utilize task correlations due to the complex components and interactions in meta-training tasks. To deal with this, we propose a novel few-shot graph classification framework FAITH to capture task correlations via learning a hierarchical task structure at different granularities. We further propose a task-specific classifier to incorporate the learned task correlations into the few-shot graph classification process. Moreover, we derive FAITH+, a variant of FAITH that can improve the sampling process for the hierarchical task structure. The extensive experiments on four prevalent graph datasets further demonstrate the superiority of FAITH and FAITH+ over other state-of-the-art baselines.}
}


@article{DBLP:journals/tkdd/RongYMSZA24,
	author = {Huan Rong and
                  Xin Yu and
                  Tinghuai Ma and
                  Victor S. Sheng and
                  Yang Zhou and
                  Mznah Al{-}Rodhaan},
	title = {Three-stage Transferable and Generative Crowdsourced Comment Integration
                  Framework Based on Zero- and Few-shot Learning with Domain Distribution
                  Alignment},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {68:1--68:43},
	year = {2024},
	url = {https://doi.org/10.1145/3636511},
	doi = {10.1145/3636511},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/RongYMSZA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online shopping has become a crucial way to encourage daily consumption, where the User-generated, or crowdsourced product comments, can offer a broad range of feedback on e-commerce products. As a result, integrating critical opinions or major attitudes from the crowdsourced comments can provide valuable feedback for marketing strategy adjustment or product-quality monitoring. Unfortunately, the scarcity of annotated ground truth on the integrated comment, or the limited gold integration reference, has incurred the infeasibility of the regular supervised-learning-based comment integration. To resolve this problem, in this article, inspired by the principle of Transfer Learning, we propose a three-stage transferable and generative crowdsourced comment integration framework (TTGCIF) based on zero-and-few-shot learning with the support of domain distribution alignment. The proposed framework aims at generating abstractive integrated comment in target domain via the enhanced neural text generation model, by referring the available integration resource in related source domains, to avoid the exhausted effort on resource annotation devoted to the target domain. Specifically, at the first stage, to enhance the domain transferability, representations on the crowdsourced comments have been aligned up between the source and target domain, by minimizing the domain distribution discrepancy in the kernel space. At the second stage, Zero-shot comment integration mechanism has been adopted to deal with the dilemma that none of the gold integration reference may be available in target domain. In other words, taking the sample-level semantic prototype as input, the enhanced neural text generation model in TTGCIF is trained to learn data semantic association among different domains via semantic prototype transduction, so that the “unlabeled” crowdsourced comments in target domain can be associated with existing integration references in related source domains. At the third stage, based on the parameters trained at the second stage, fast domain adaptation mechanism in a Few-shot manner has also been adopted by seeking most potential parameters along the gradient direction constrained by instances across multiple source domains. In this way, parameters in TTGCIF can be sensitive to any alteration on training data, ensuring that even if only few annotated resource in target domain are available for “Fine-tune,” TTGCIF can still react promptly to achieve effective target domain adaptation. According to the experimental results, TTGCIF can achieve the best transferable product comment integration performance in target domain, with fast and stable domain adaption effect depending on no more than 10% annotated resource in target domain. More importantly, even if TTGCIF has not been fine-tuned on the target domain, yet by referring to the available integration resource in related source domains, the integrated comments generated by TTGCIF on the target domain are still superior to those generated by models already fine-tuned on the target domain.}
}


@article{DBLP:journals/tkdd/PalmaDL24,
	author = {Luciano Di Palma and
                  Yanlei Diao and
                  Anna Liu},
	title = {Efficient Version Space Algorithms for Human-in-the-loop Model Development},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {69:1--69:49},
	year = {2024},
	url = {https://doi.org/10.1145/3637443},
	doi = {10.1145/3637443},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PalmaDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When active learning (AL) is applied to help users develop a model on a large dataset through interactively presenting data instances for labeling, existing AL techniques often suffer from two main drawbacks: First, to reach high accuracy they may require the user to label hundreds of data instances, which is an onerous task for the user. Second, retrieving the next instance to label from a large dataset can be time-consuming, making it incompatible with the interactive nature of the human exploration process. To address these issues, we introduce a novel version-space-based active learner for kernel classifiers, which possesses strong theoretical guarantees on performance and efficient implementation in time and space. In addition, by leveraging additional insights obtained in the user labeling process, we can factorize the version space to perform active learning in a set of subspaces, which further reduces the user labeling effort. Evaluation results show that our algorithms significantly outperform state-of-the-art version space strategies, as well as a recent factorization-aware algorithm, for model development over large datasets.}
}


@article{DBLP:journals/tkdd/TipirneniZR24,
	author = {Sindhu Tipirneni and
                  Ming Zhu and
                  Chandan K. Reddy},
	title = {StructCoder: Structure-Aware Transformer for Code Generation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {70:1--70:20},
	year = {2024},
	url = {https://doi.org/10.1145/3636430},
	doi = {10.1145/3636430},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/TipirneniZR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at https://github.com/reddy-lab-code-research/StructCoder/.}
}


@article{DBLP:journals/tkdd/FanFZBZX24,
	author = {Wei Fan and
                  Yanjie Fu and
                  Shun Zheng and
                  Jiang Bian and
                  Yuanchun Zhou and
                  Hui Xiong},
	title = {{DEWP:} Deep Expansion Learning for Wind Power Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {71:1--71:21},
	year = {2024},
	url = {https://doi.org/10.1145/3637552},
	doi = {10.1145/3637552},
	timestamp = {Sat, 10 Feb 2024 18:05:19 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/FanFZBZX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wind is one kind of high-efficient, environmentally-friendly, and cost-effective energy source. Wind power, as one of the largest renewable energy in the world, has been playing a more and more important role in supplying electricity. Though growing dramatically in recent years, the amount of generated wind power can be directly or latently affected by multiple uncertain factors, such as wind speed, wind direction, temperatures, and so on. More importantly, there exist very complicated dependencies of the generated power on the latent composition of these multiple time-evolving variables, which are always ignored by existing works and thus largely hinder the prediction performances. To this end, we propose DEWP, a novel Deep Expansion learning for Wind Power forecasting framework to carefully model the complicated dependencies with adequate expressiveness. DEWP starts with a stack-by-stack architecture, where each stack is composed of (i) a variable expansion block that makes use of convolutional layers to capture dependencies among multiple variables; (ii) a time expansion block that applies Fourier series and backcast/forecast mechanism to learn temporal dependencies in sequential patterns. These two tailored blocks expand raw inputs into different latent feature spaces which can model different levels of dependencies of time-evolving sequential data. Moreover, we propose an inference block corresponding for each stack, which applies multi-head self-attentions to acquire attentive features and maps expanded latent representations into generated wind power. In addition, to make DEWP more expressive in handling deep neural architectures, we adapt doubly residue learning to process stack-by-stack outputs. Accurate wind power forecasting (WPF) is then better achieved through fine-grained outputs by continuously removing stack residues and accumulating useful stack forecasts. Finally, we present extensive experiments in the real-world WPF application on two datasets from two different turbines, in order to demonstrate the effectiveness of our approach.}
}


@article{DBLP:journals/tkdd/LiuL24,
	author = {Zhe Liu and
                  Sukumar Letchmunan},
	title = {Enhanced Fuzzy Clustering for Incomplete Instance with Evidence Combination},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {3},
	pages = {72:1--72:20},
	year = {2024},
	url = {https://doi.org/10.1145/3638061},
	doi = {10.1145/3638061},
	timestamp = {Mon, 05 Feb 2024 20:24:42 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering incomplete instance is still a challenging task since missing values maybe make the cluster information ambiguous, leading to the uncertainty and imprecision in results. This article investigates an enhanced fuzzy clustering with evidence combination method based on Dempster-Shafer theory (DST) to address this problem. First, the dataset is divided into several subsets, and missing values are imputed by neighbors with different weights in each subset. It aims to model missing values locally to reduce the negative impact of the bad estimations. Second, an objective function of enhanced fuzzy clustering is designed and then optimized until the best membership and reliability matrices are found. Each subset has a membership matrix that contains all sub-instances’ membership to different clusters. The fuzzy reliability matrix is employed to characterize the reliability of each subset on different clusters. Third, an adaptive evidence combination rule based on the DST is developed to combine the discounted subresults (memberships) with different reliability to make the final decision for each instance. The proposed method can characterize uncertainty and imprecision by assigning instances to specific clusters or meta-clusters composed of several specific clusters. Once an instance is assigned to a meta-cluster, the cluster information of this instance is (locally) imprecise. The effectiveness of proposed method is demonstrated on several real-world datasets by comparing with existing techniques.}
}


@article{DBLP:journals/tkdd/JianBGW24,
	author = {Meng Jian and
                  Yulong Bai and
                  Jingjing Guo and
                  Lifang Wu},
	title = {Swarm Self-supervised Hypergraph Embedding for Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {73:1--73:19},
	year = {2024},
	url = {https://doi.org/10.1145/3638058},
	doi = {10.1145/3638058},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JianBGW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The information era brings both opportunities and challenges to information services. Confronting information overload, recommendation technology is dedicated to filtering personalized content to meet users’ requirements. The extremely sparse interaction records and their imbalanced distribution become a big obstacle to building a high-quality recommendation model. In this article, we propose a swarm self-supervised hypergraph embedding (SHE) model to predict users’ interests by hypergraph convolution and self-supervised discrimination. SHE builds a hypergraph with multiple interest clues to alleviate the interaction sparsity issue and performs interest propagation to embed CF signals in hybrid learning on the hypergraph. It follows an auxiliary local view by similar hypergraph construction and interest propagation to restrain unnecessary propagation between user swarms. Besides, interest contrast further inserts self-discrimination to deal with long-tail bias issue and enhance interest modeling, which aid recommendation by a multi-task learning optimization. Experiments on public datasets show that the proposed SHE outperforms the state-of-the-art models demonstrating the effectiveness of hypergraph-based interest propagation and swarm-aware interest contrast to enhance embedding for recommendation.}
}


@article{DBLP:journals/tkdd/WangMZL24,
	author = {Wentao Wang and
                  Huifang Ma and
                  Yan Zhao and
                  Zhixin Li},
	title = {Pre-training Question Embeddings for Improving Knowledge Tracing with
                  Self-supervised Bi-graph Co-contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {74:1--74:20},
	year = {2024},
	url = {https://doi.org/10.1145/3638055},
	doi = {10.1145/3638055},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangMZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning high-quality vector representations (aka. embeddings) of educational questions lies at the core of knowledge tracing (KT), which defines a task of estimating students’ knowledge states by predicting the probability that they correctly answer questions. Although existing KT efforts have leveraged question information to achieve remarkable improvements, most of them learn question embeddings by following the supervised learning paradigm. In this article, we propose a novel question embedding pre-training method for improving knowledge tracing with self-supervised Bi-graph Co-contrastive learning (BiCo). Technically, on the basis of self-supervised learning paradigm, we first select two similar but distinct views (i.e., representing objective and subjective semantic perspectives) as the semantic source of question embeddings. Then, we design a primary task (structure recovery) together with two auxiliary tasks (question difficulty recovery and contrastive learning) to further enhance the representativeness of questions. Finally, extensive experiments conducted on two real-world datasets show BiCo has a higher expressive power that enables KT methods to effectively predict students’ performances.}
}


@article{DBLP:journals/tkdd/PiaoSYJ24,
	author = {Minghao Piao and
                  Yi Sheng and
                  Jinda Yan and
                  Cheng Hao Jin},
	title = {Image Hash Layer Triggered {CNN} Framework for Wafer Map Failure Pattern
                  Retrieval and Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {75:1--75:26},
	year = {2024},
	url = {https://doi.org/10.1145/3638053},
	doi = {10.1145/3638053},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PiaoSYJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, deep learning methods are often used in wafer map failure pattern classification. CNN requires less feature engineering but still needs preprocessing, e.g., denoising and resizing. Denoising is used to improve the quality of the input data, and resizing is used to transform the input into an identical size when the input data sizes are various. However, denoising and resizing may distort the original data information. Nevertheless, CNN-based applications are focusing on studying different feature map architectures and the input data manipulation is less attractive. In this study, we proposed an image hash layer triggered CNN framework for wafer map failure pattern retrieval and classification. The motivation and novelty are to design a CNN layer that can play as a resizing, information retrieval-preservation method in one step. The experiments proved that the proposed hash layer can retrieve the failure pattern information while maintaining the classification performance even though the input data size is decreased significantly. In the meantime, it can prevent overfitting, false negatives, and false positives, and save computing costs to a certain extent.}
}


@article{DBLP:journals/tkdd/XiaoWWLXZF24,
	author = {Meng Xiao and
                  Dongjie Wang and
                  Min Wu and
                  Kunpeng Liu and
                  Hui Xiong and
                  Yuanchun Zhou and
                  Yanjie Fu},
	title = {Traceable Group-Wise Self-Optimizing Feature Transformation Learning:
                  {A} Dual Optimization Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {76:1--76:22},
	year = {2024},
	url = {https://doi.org/10.1145/3638059},
	doi = {10.1145/3638059},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaoWWLXZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. Despite the impressive strides made, there was room for enhancing its effectiveness and generalization capability. In this extended journal version, we advance our initial work from two distinct yet interconnected perspectives: 1) We propose a refinement of the original framework, which integrates a graph-based state representation method to capture the feature interactions more effectively and develop different Q-learning strategies to alleviate Q-value overestimation further. 2) We utilize a new optimization technique (actor-critic) to train the entire self-optimizing framework in order to accelerate the model convergence and improve the feature transformation performance. Finally, to validate the improved effectiveness and generalization capability of our framework, we perform extensive experiments and conduct comprehensive analyses. These provide empirical evidence of the strides made in this journal version over the initial work, solidifying our framework’s standing as a substantial contribution to the field of automated feature transformation. To improve the reproducibility, we have released the associated code and data by the Github link\xa0https://github.com/coco11563/TKDD2023_code.}
}


@article{DBLP:journals/tkdd/LiWWW24,
	author = {Ximing Li and
                  Bing Wang and
                  Yang Wang and
                  Meng Wang},
	title = {Graph-based Text Classification by Contrastive Learning with Text-level
                  Graph Augmentation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {77:1--77:21},
	year = {2024},
	url = {https://doi.org/10.1145/3638353},
	doi = {10.1145/3638353},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiWWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGNcl). We conduct a number of experiments to evaluate the proposed TGNcl model. The empirical results demonstrate that TGNcl can outperform the existing state-of-the-art TC models.}
}


@article{DBLP:journals/tkdd/LiuHGSY24,
	author = {Tengfei Liu and
                  Yongli Hu and
                  Junbin Gao and
                  Yanfeng Sun and
                  Baocai Yin},
	title = {Cross-modal Multiple Granularity Interactive Fusion Network for Long
                  Document Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {78:1--78:24},
	year = {2024},
	url = {https://doi.org/10.1145/3631711},
	doi = {10.1145/3631711},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuHGSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Long Document Classification (LDC) has attracted great attention in Natural Language Processing and achieved considerable progress owing to the large-scale pre-trained language models. In spite of this, as a different problem from the traditional text classification, LDC is far from being settled. Long documents, such as news and articles, generally have more than thousands of words with complex structures. Moreover, compared with flat text, long documents usually contain multi-modal content of images, which provide rich information but not yet being utilized for classification. In this article, we propose a novel cross-modal method for long document classification, in which multiple granularity feature shifting networks are proposed to integrate the multi-scale text and visual features of long documents adaptively. Additionally, a multi-modal collaborative pooling block is proposed to eliminate redundant fine-grained text features and simultaneously reduce the computational complexity. To verify the effectiveness of the proposed model, we conduct experiments on the Food101 dataset and two constructed multi-modal long document datasets. The experimental results show that the proposed cross-modal method outperforms the single-modal text methods and defeats the state-of-the-art related multi-modal baselines.}
}


@article{DBLP:journals/tkdd/BozdagSK24,
	author = {Mustafa Bozdag and
                  Nurullah Sevim and
                  Aykut Ko{\c{c}}},
	title = {Measuring and Mitigating Gender Bias in Legal Contextualized Language
                  Models},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {79:1--79:26},
	year = {2024},
	url = {https://doi.org/10.1145/3628602},
	doi = {10.1145/3628602},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/BozdagSK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transformer-based contextualized language models constitute the state-of-the-art in several natural language processing (NLP) tasks and applications. Despite their utility, contextualized models can contain human-like social biases, as their training corpora generally consist of human-generated text. Evaluating and removing social biases in NLP models has been a major research endeavor. In parallel, NLP approaches in the legal domain, namely, legal NLP or computational law, have also been increasing. Eliminating unwanted bias in legal NLP is crucial, since the law has the utmost importance and effect on people. In this work, we focus on the gender bias encoded in BERT-based models. We propose a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database. This method quantifies the gender bias present in BERT-based models for legal applications. Furthermore, we propose a new fine-tuning-based debiasing method using the European Court of Human Rights (ECtHR) corpus to debias legal pre-trained models. We test the debiased models’ language understanding performance on the LexGLUE benchmark to confirm that the underlying semantic vector space is not perturbed during the debiasing process. Finally, we propose a bias penalty for the performance scores to emphasize the effect of gender bias on model performance.}
}


@article{DBLP:journals/tkdd/ZhangLGY24,
	author = {Chunkai Zhang and
                  Maohua Lyu and
                  Wensheng Gan and
                  Philip S. Yu},
	title = {Totally-ordered Sequential Rules for Utility Maximization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {80:1--80:23},
	year = {2024},
	url = {https://doi.org/10.1145/3628450},
	doi = {10.1145/3628450},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangLGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-utility sequential pattern mining (HUSPM) is a significant and valuable activity in knowledge discovery and data analytics with many real-world applications. In some cases, HUSPM can not provide an excellent measure to predict what will happen. High-utility sequential rule mining (HUSRM) discovers high utility and high confidence sequential rules, so it can solve the issue in HUSPM. However, all existing HUSRM algorithms aim to find high-utility partially-ordered sequential rules (HUSRs), which are not consistent with reality and may generate fake HUSRs. Therefore, in this article, we formulate the problem of high-utility totally-ordered sequential rule mining and propose a novel algorithm, called TotalSR, which aims to identify all high-utility totally-ordered sequential rules (HTSRs). TotalSR introduces a left-first expansion strategy that can utilize the anti-monotonic property to use a confidence pruning strategy. TotalSR also designs a new utility upper bound: RSPEU, which is tighter than the existing upper bounds. TotalSR can drastically reduce the search space with the help of utility upper bounds pruning strategies, avoiding much more meaningless computation. To effectively compute the information, TotalSR proposes an auxiliary antecedent record table that can efficiently calculate the antecedent’s support and a utility prefix sum list that can compute the upper bound in O(1) time for a sequence. Finally, there are numerous experimental results on both real and synthetic datasets demonstrating that TotalSR is more efficient than the existing algorithms.}
}


@article{DBLP:journals/tkdd/YangZWZLBL24,
	author = {Yi Yang and
                  Zhong{-}Qiu Zhao and
                  Gongqing Wu and
                  Xingrui Zhuo and
                  Qing Liu and
                  Quan Bai and
                  Weihua Li},
	title = {A Lightweight, Effective, and Efficient Model for Label Aggregation
                  in Crowdsourcing},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {81:1--81:27},
	year = {2024},
	url = {https://doi.org/10.1145/3630102},
	doi = {10.1145/3630102},
	timestamp = {Thu, 23 May 2024 22:02:54 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YangZWZLBL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the presence of noise in crowdsourced labels, label aggregation (LA) has become a standard procedure for post-processing these labels. LA methods estimate true labels from crowdsourced labels by modeling worker quality. However, most existing LA methods are iterative in nature. They require multiple passes through all crowdsourced labels, jointly and iteratively updating true labels and worker qualities until a termination condition is met. As a result, these methods are burdened with high space and time complexities, which restrict their applicability in scenarios where scalability and online aggregation are essential. Furthermore, defining a suitable termination condition for iterative algorithms can be challenging. In this article, we view LA as a dynamic system and represent it as a Dynamic Bayesian Network. From this dynamic model, we derive two lightweight and scalable algorithms: LAonepass and LAtwopass. These algorithms can efficiently and effectively estimate worker qualities and true labels by traversing all labels at most twice, thereby eliminating the need for explicit termination conditions and multiple traversals over the crowdsourced labels. Due to their dynamic nature, the proposed algorithms are also capable of performing label aggregation online. We provide theoretical proof of the convergence property of the proposed algorithms and bound the error of the estimated worker qualities. Furthermore, we analyze the space and time complexities of our proposed algorithms, demonstrating their equivalence to those of majority voting. Through experiments conducted on 20 real-world datasets, we demonstrate that our proposed algorithms can effectively and efficiently aggregate labels in both offline and online settings, even though they traverse all labels at most twice. The code is on https://github.com/yyang318/LA_onepass.}
}


@article{DBLP:journals/tkdd/FengJZT24,
	author = {Shengyu Feng and
                  Baoyu Jing and
                  Yada Zhu and
                  Hanghang Tong},
	title = {ArieL: Adversarial Graph Contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {82:1--82:22},
	year = {2024},
	url = {https://doi.org/10.1145/3638054},
	doi = {10.1145/3638054},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/FengJZT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning is an effective unsupervised method in graph representation learning. The key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works have extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and it is much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning (ArieL), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use subgraph sampling for scalability. We generalize our method from node-level contrastive learning to the graph level by treating each graph instance as a super-node. ArieL consistently outperforms the current graph contrastive learning methods for both node-level and graph-level classification tasks on real-world datasets. We further demonstrate that ArieL is more robust in the face of adversarial attacks.}
}


@article{DBLP:journals/tkdd/DingWLCL24,
	author = {Kaize Ding and
                  Jianling Wang and
                  Jundong Li and
                  James Caverlee and
                  Huan Liu},
	title = {Robust Graph Meta-Learning for Weakly Supervised Few-Shot Node Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {83:1--83:18},
	year = {2024},
	url = {https://doi.org/10.1145/3630260},
	doi = {10.1145/3630260},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DingWLCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph machine learning (Graph ML) models typically require abundant labeled instances to provide sufficient supervision signals, which is commonly infeasible in real-world scenarios since labeled data for newly emerged concepts (e.g., new categorizations of nodes) on graphs is rather limited. To efficiently learn with a small amount of data on graphs, meta-learning has been investigated in Graph ML. By transferring the knowledge learned from previous experiences to new tasks, graph meta-learning approaches have demonstrated promising performance on few-shot graph learning problems. However, most existing efforts predominately assume that all the data from the seen classes is gold labeled, yet those methods may lose their efficacy when the seen data is weakly labeled with severe label noise. As such, we aim to investigate a novel problem of weakly supervised graph meta-learning for improving the model robustness in terms of knowledge transfer. To achieve this goal, we propose Meta-GIN (Meta Graph Interpolation Network), a new graph meta-learning framework. Based on a new robustness-enhanced episodic training paradigm, Meta-GIN is meta-learned to interpolate node representations from weakly labeled data and extracts highly transferable meta-knowledge, which enables the model to quickly adapt to unseen tasks with few labeled instances. Extensive experiments demonstrate the superiority of Meta-GIN over existing graph meta-learning studies on the task of weakly supervised few-shot node classification.}
}


@article{DBLP:journals/tkdd/ZhuWSD24,
	author = {Weiyao Zhu and
                  Ou Wu and
                  Fengguang Su and
                  Yingjun Deng},
	title = {Exploring the Learning Difficulty of Data: Theory and Measure},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {84:1--84:37},
	year = {2024},
	url = {https://doi.org/10.1145/3636512},
	doi = {10.1145/3636512},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuWSD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {‘‘Easy/hard sample” is a popular parlance in machine learning. Learning difficulty of samples refers to how easy/hard a sample is during a learning procedure. An increasing need of measuring learning difficulty demonstrates its importance in machine learning (e.g., difficulty-based weighting learning strategies). Previous literature has proposed a number of learning difficulty measures. However, no comprehensive investigation for learning difficulty is available to date, resulting in that nearly all existing measures are heuristically defined without a rigorous theoretical foundation. This study attempts to conduct a pilot theoretical study for learning difficulty of samples. First, influential factors for learning difficulty are summarized. Under various situations conducted by summarized influential factors, correlations between learning difficulty and two vital criteria of machine learning, namely, generalization error and model complexity, are revealed. Second, a theoretical definition of learning difficulty is proposed on the basis of these two criteria. A practical measure of learning difficulty is proposed under the direction of the theoretical definition by importing the bias-variance trade-off theory. Subsequently, the rationality of theoretical definition and the practical measure are demonstrated, respectively, by analysis of several classical weighting methods and abundant experiments realized under all situations conducted by summarized influential factors. The mentioned weighting methods can be reasonably explained under the proposed theoretical definition and concerned propositions. The comparison in these experiments indicates that the proposed measure significantly outperforms the other measures throughout the experiments.}
}


@article{DBLP:journals/tkdd/ChenHYJD24,
	author = {Wei Chen and
                  Chao Huang and
                  Yanwei Yu and
                  Yongguo Jiang and
                  Junyu Dong},
	title = {Trajectory-User Linking via Hierarchical Spatio-Temporal Attention
                  Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {85:1--85:22},
	year = {2024},
	url = {https://doi.org/10.1145/3635718},
	doi = {10.1145/3635718},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenHYJD24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trajectory-User Linking (TUL) is crucial for human mobility modeling by linking different trajectories to users with the exploration of complex mobility patterns. Existing works mainly rely on the recurrent neural framework to encode the temporal dependencies in trajectories, have fall short in capturing spatial-temporal global context for TUL prediction. To fill this gap, this work presents a new hierarchical spatio-temporal attention neural network, called AttnTUL, to jointly encode the local trajectory transitional patterns and global spatial dependencies for TUL. Specifically, our first model component is built over the graph neural architecture to preserve the local and global context and enhance the representation paradigm of geographical regions and user trajectories. Additionally, a hierarchically structured attention network is designed to simultaneously encode the intra-trajectory and inter-trajectory dependencies, with the integration of the temporal attention mechanism and global elastic attentional encoder. Extensive experiments demonstrate the superiority of our AttnTUL method as compared to state-of-the-art baselines on various trajectory datasets. The source code of our model is available at https://github.com/Onedean/AttnTUL.}
}


@article{DBLP:journals/tkdd/LiuILJ24,
	author = {Gang Liu and
                  Eric Inae and
                  Tengfei Luo and
                  Meng Jiang},
	title = {Rationalizing Graph Neural Networks with Data Augmentation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {86:1--86:23},
	year = {2024},
	url = {https://doi.org/10.1145/3638781},
	doi = {10.1145/3638781},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuILJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph rationales are representative subgraph structures that best explain and support the graph neural network (GNN) predictions. Graph rationalization involves the joint identification of these subgraphs during GNN training, resulting in improved interpretability and generalization. GNN is widely used for node-level tasks such as paper classification and graph-level tasks such as molecular property prediction. However, on both levels, little attention has been given to GNN rationalization and the lack of training examples makes it difficult to identify the optimal graph rationales. In this work, we address the problem by proposing a unified data augmentation framework with two novel operations on environment subgraphs to rationalize GNN prediction. We define the environment subgraph as the remaining subgraph after rationale identification and separation. The framework efficiently performs rationale–environment separation in the representation space for a node’s neighborhood graph or a graph’s complete structure to avoid the high complexity of explicit graph decoding and encoding. We conduct experiments on 17 datasets spanning node classification, graph classification, and graph regression. Results demonstrate that our framework is effective and efficient in rationalizing and enhancing GNNs for different levels of tasks on graphs.}
}


@article{DBLP:journals/tkdd/LongYFXLSZ24,
	author = {Chao Long and
                  Huanhuan Yuan and
                  Junhua Fang and
                  Xuefeng Xian and
                  Guanfeng Liu and
                  Victor S. Sheng and
                  Pengpeng Zhao},
	title = {Learning Global and Multi-granularity Local Representation with {MLP}
                  for Sequential Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {87:1--87:15},
	year = {2024},
	url = {https://doi.org/10.1145/3638562},
	doi = {10.1145/3638562},
	timestamp = {Thu, 30 Jan 2025 15:11:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LongYFXLSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation aims to predict the next item of interest to users based on their historical behavior data. Usually, users’ global and local preferences jointly affect the final recommendation result in different ways. Most existing works use transformers to globally model sequences, which makes them face the dilemma of quadratic computational complexity when dealing with long sequences. Moreover, the scope setting of the user’s local preference is usually static and single, and cannot cover richer multi-level local semantics. To this end, we proposed a parallel architecture for capturing global representation and Multi-granularity Local dependencies with MLP for sequential Recommendation (MLM4Rec). For global representation, we utilize modified MLP-Mixer to capture global information of user sequences due to its simplicity and efficiency. For local representation, we incorporate convolution into MLP and propose a multi-granularity local awareness mechanism for capturing richer local semantic information. Moreover, we introduced a weight pooling method to adaptively fuse local-global representations instead of directly concatenation. Our model has the advantages of low complexity and high efficiency thanks to its simple MLP structure. Experimental results on three public datasets demonstrate the effectiveness of our proposed model. Our code is available here.}
}


@article{DBLP:journals/tkdd/XuXGFWZ24,
	author = {Hui Xu and
                  Liyao Xiang and
                  Xiaoying Gan and
                  Luoyi Fu and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Distributional Learning for Network Alignment with Global Constraints},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {88:1--88:16},
	year = {2024},
	url = {https://doi.org/10.1145/3638056},
	doi = {10.1145/3638056},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XuXGFWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network alignment, pairing corresponding nodes across the source and target networks, plays an important role in many data mining tasks. Extensive studies focus on learning node embeddings across different networks in a unified space. However, these methods have not taken the large structural discrepancy between aligned nodes into account and, thus, are largely confined by the deterministic representations of nodes. In this work, we propose a novel network alignment framework highlighted by distributional learning and globally optimal alignment. By modeling the uncertainty of each node by Gaussian distribution, our framework builds similarity matrices on the Wasserstein distance between distributions and applies Sinkhorn operation, which learns the globally optimal mapping in an end-to-end fashion. We show that each integrated part of the framework contributes to the overall performance. Under a variety of experimental settings, our alignment framework shows superior accuracy and efficiency to the state-of-the-art.}
}


@article{DBLP:journals/tkdd/ZhangZY24,
	author = {Guixian Zhang and
                  Shichao Zhang and
                  Guan Yuan},
	title = {Bayesian Graph Local Extrema Convolution with Long-tail Strategy for
                  Misinformation Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {89:1--89:21},
	year = {2024},
	url = {https://doi.org/10.1145/3639408},
	doi = {10.1145/3639408},
	timestamp = {Fri, 05 Apr 2024 12:23:10 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It has become a cardinal task to identify fake information (misinformation) on social media, because it has significantly harmed the government and the public. There are many spam bots maliciously retweeting misinformation. This study proposes an efficient model for detecting misinformation with self-supervised contrastive learning. A Bayesian graph Local extrema Convolution (BLC) is first proposed to aggregate node features in the graph structure. The BLC approach considers unreliable relationships and uncertainties in the propagation structure, and the differences between nodes and neighboring nodes are emphasized in the attributes. Then, a new long-tail strategy for matching long-tail users with the global social network is advocated to avoid over-concentration on high-degree nodes in graph neural networks. Finally, the proposed model is experimentally evaluated with two public Twitter datasets and demonstrates that the proposed long-tail strategy significantly improves the effectiveness of existing graph-based methods in terms of detecting misinformation. The robustness of BLC has also been examined on three graph datasets and demonstrates that it consistently outperforms traditional algorithms when perturbed by 15% of a dataset.}
}


@article{DBLP:journals/tkdd/ShuWYWJFZ24,
	author = {Senlin Shu and
                  Deng{-}Bao Wang and
                  Suqin Yuan and
                  Hongxin Wei and
                  Jiuchuan Jiang and
                  Lei Feng and
                  Min{-}Ling Zhang},
	title = {Multiple-instance Learning from Triplet Comparison Bags},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {90:1--90:18},
	year = {2024},
	url = {https://doi.org/10.1145/3638776},
	doi = {10.1145/3638776},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ShuWYWJFZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple-instance learning (MIL) solves the problem where training instances are grouped in bags, and a binary (positive or negative) label is provided for each bag. Most of the existing MIL studies need fully labeled bags for training an effective classifier, while it could be quite hard to collect such data in many real-world scenarios, due to the high cost of data labeling process. Fortunately, unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Therefore, in this article, we for the first time investigate MIL from only triplet comparison bags, where a triplet (Xa, Xb, Xc) contains the weak supervision information that bag Xa is more similar to Xb than to Xc. To solve this problem, we propose to train a bag-level classifier by the empirical risk minimization framework and theoretically provide a generalization error bound. We also show that a convex formulation can be obtained only when specific convex binary losses such as the square loss and the double hinge loss are used. Extensive experiments validate that our proposed method significantly outperforms other baselines.}
}


@article{DBLP:journals/tkdd/WangZZWXTLLHP24,
	author = {Yilin Wang and
                  Sha Zhao and
                  Shiwei Zhao and
                  Runze Wu and
                  Yuhong Xu and
                  Jianrong Tao and
                  Tangjie Lv and
                  Shijian Li and
                  Zhipeng Hu and
                  Gang Pan},
	title = {PU-Detector: {A} {PU} Learning-based Framework for Real Money Trading
                  Detection in {MMORPG}},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {91:1--91:26},
	year = {2024},
	url = {https://doi.org/10.1145/3638561},
	doi = {10.1145/3638561},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZZWXTLLHP24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive multiplayer online role-playing games (MMORPG) have been becoming one of the most popular and exciting online games. In recent years, a cheating phenomenon called real money trading (RMT) has arisen and damaged the fantasy world in many ways. RMT is the sale of in-game items, currency, or even characters to earn real money, breaking the balance of the game economy ecosystem and damaging the game experience. Therefore, some studies have emerged to address the problem of RMT detection. However, they cannot well handle the label uncertainty problem in practice, where there are only labeled RMT samples (positive samples) and unlabeled samples, which could either be RMT samples or normal transactions (negative samples). Meanwhile, the trading relationship between RMTers is modeled in a simple way, leading to some normal transactions being falsely classified as RMT. In this article, we propose PU-Detector, a novel framework based on PU learning (learning from positive and unlabeled data) for RMT detection, considering the fact that there are only labeled RMT samples and other unlabeled transactions. We first automatically estimate the likelihood of one transaction being RMT by developing an improved PU learning method and proposing an assessment rule. Sequentially, we use the estimated likelihood as edge weight to construct a trading graph to learn trader representation. Then, with the trader representations and basic trading features, we detect RMT samples by the improved PU learning method. PU-Detector is evaluated on a large-scale real world dataset consisting of 33,809,956 transaction logs generated by 43,217 unique players. Compared with other approaches, it achieves the state-of-the-art performance and demonstrates its advantages in detecting underlying RMT samples.}
}


@article{DBLP:journals/tkdd/ZhangWYW24,
	author = {Yuhong Zhang and
                  Jianqing Wu and
                  Kui Yu and
                  Xindong Wu},
	title = {Diverse Structure-Aware Relation Representation in Cross-Lingual Entity
                  Alignment},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {92:1--92:23},
	year = {2024},
	url = {https://doi.org/10.1145/3638778},
	doi = {10.1145/3638778},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangWYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-lingual entity alignment (CLEA) aims to find equivalent entity pairs between knowledge graphs (KGs) in different languages. It is an important way to connect heterogeneous KGs and facilitate knowledge completion. Existing methods have found that incorporating relations into entities can effectively improve KG representation and benefit entity alignment, and these methods learn relation representation depending on entities, which cannot capture the diverse structures of relations. However, multiple relations in KG form diverse structures, such as adjacency structure and ring structure. This diversity of relation structures makes the relation representation challenging. Therefore, we propose to construct the weighted line graphs to model the diverse structures of relations and learn relation representation independently from entities. Especially, owing to the diversity of adjacency structures and ring structures, we propose to construct adjacency line graph and ring line graph, respectively, to model the structures of relations and to further improve entity representation. In addition, to alleviate the hubness problem in alignment, we introduce the optimal transport into alignment and compute the distance matrix in a different way. From a global perspective, we calculate the optimal 1-to-1 alignment bi-directionally to improve the alignment accuracy. Experimental results on two benchmark datasets show that our proposed method significantly outperforms state-of-the-art CLEA methods in both supervised and unsupervised manners.}
}


@article{DBLP:journals/tkdd/GeeganageXL24,
	author = {Dakshi Kapugama Geeganage and
                  Yue Xu and
                  Yuefeng Li},
	title = {A Semantics-enhanced Topic Modelling Technique: Semantic-LDA},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {93:1--93:27},
	year = {2024},
	url = {https://doi.org/10.1145/3639409},
	doi = {10.1145/3639409},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GeeganageXL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept–word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word–concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations.}
}


@article{DBLP:journals/tkdd/RenZFLZWCLZ24,
	author = {Yuyang Ren and
                  Haonan Zhang and
                  Luoyi Fu and
                  Shiyu Liang and
                  Lei Zhou and
                  Xinbing Wang and
                  Xinde Cao and
                  Fei Long and
                  Chenghu Zhou},
	title = {Hi-PART: Going Beyond Graph Pooling with Hierarchical Partition Tree
                  for Graph-Level Representation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {94:1--94:20},
	year = {2024},
	url = {https://doi.org/10.1145/3636429},
	doi = {10.1145/3636429},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RenZFLZWCLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pooling refers to the operation that maps a set of node representations into a compact form for graph-level representation learning. However, existing graph pooling methods are limited by the power of the Weisfeiler–Lehman (WL) test in the performance of graph discrimination. In addition, these methods often suffer from hard adaptability to hyper-parameters and training instability. To address these issues, we propose Hi-PART, a simple yet effective graph neural network (GNN) framework with Hierarchical Partition Tree (HPT). In HPT, each layer is a partition of the graph with different levels of granularities that are going toward a finer grain from top to bottom. Such an exquisite structure allows us to quantify the graph structure information contained in HPT with the aid of structural information theory. Algorithmically, by employing GNNs to summarize node features into the graph feature based on HPT’s hierarchical structure, Hi-PART is able to adequately leverage the graph structure information and provably goes beyond the power of the WL test. Due to the separation of HPT optimization from graph representation learning, Hi-PART involves the height of HPT as the only extra hyper-parameter and enjoys higher training stability. Empirical results on graph classification benchmarks validate the superior expressive power and generalization ability of Hi-PART compared with state-of-the-art graph pooling approaches.}
}


@article{DBLP:journals/tkdd/BalalauBCGSX24,
	author = {Oana Balalau and
                  Francesco Bonchi and
                  T.{-}H. Hubert Chan and
                  Francesco Gullo and
                  Mauro Sozio and
                  Hao Xie},
	title = {Finding Subgraphs with Maximum Total Density and Limited Overlap in
                  Weighted Hypergraphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {95:1--95:21},
	year = {2024},
	url = {https://doi.org/10.1145/3639410},
	doi = {10.1145/3639410},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BalalauBCGSX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding dense subgraphs in large (hyper)graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs, which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs in weighted hypergraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work, we define and study a natural generalization of the densest subgraph problem in weighted hypergraphs, where the main goal is to find at most k subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise weighted Jaccard coefficient, i.e., the ratio of weights of intersection divided by weights of union on two nodes sets of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world hypergraphs confirms the efficiency and effectiveness of our algorithms.}
}


@article{DBLP:journals/tkdd/LiYZLX24,
	author = {Jinpeng Li and
                  Hang Yu and
                  Zhenyu Zhang and
                  Xiangfeng Luo and
                  Shaorong Xie},
	title = {Concept Drift Adaptation by Exploiting Drift Type},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {96:1--96:22},
	year = {2024},
	url = {https://doi.org/10.1145/3638777},
	doi = {10.1145/3638777},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiYZLX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Concept drift is a phenomenon where the distribution of data streams changes over time. When this happens, model predictions become less accurate. Hence, models built in the past need to be re-learned for the current data. Two design questions need to be addressed in designing a strategy to re-learn models: which type of concept drift has occurred, and how to utilize the drift type to improve re-learning performance. Existing drift detection methods are often good at determining when drift has occurred. However, few retrieve information about how the drift came to be present in the stream. Hence, determining the impact of the type of drift on adaptation is difficult. Filling this gap, we designed a framework based on a lazy strategy called Type-Driven Lazy Drift Adaptor (Type-LDA). Type-LDA first retrieves information about both how and when a drift has occurred, then it uses this information to re-learn the new model. To identify the type of drift, a drift type identifier is pre-trained on synthetic data of known drift types. Furthermore, a drift point locator locates the optimal point of drift via a sharing loss. Hence, Type-LDA can select the optimal point, according to the drift type, to re-learn the new model. Experiments validate Type-LDA on both synthetic data and real-world data, and the results show that accurately identifying drift type can improve adaptation accuracy.}
}


@article{DBLP:journals/tkdd/XiaoLS24,
	author = {Feng Xiao and
                  Youfa Liu and
                  Jia Shao},
	title = {{NNC-GCN:} Neighbours-to-Neighbours Contrastive Graph Convolutional
                  Network for Semi-Supervised Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {97:1--97:18},
	year = {2024},
	url = {https://doi.org/10.1145/3638780},
	doi = {10.1145/3638780},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaoLS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contrastive learning (CL) is a popular learning paradigm in deep learning, which uses contrastive principle to learn low-dimensional embeddings, and has been applied in Graph Neural Networks (GNNs) successfully. Existing works of contrastive multi-view GNNs usually focus on point-to-point contrastive learning strategies. However, they neglect the local information in neighbors, which brings isolated positive samples. The quality of selected positive samples is hard to evaluate, and these samples may lead to invalid contrastiveness. Therefore, we propose a simple and efficient neighbors-to-neighbors contrastive graph neural network (NNC-GCN), which constructs a consistent multi-view by using the topologies of original input graphs. Moreover, we raise a new learning problem of unlabeled data base on these constructed multi-view topologies and propose a loss function NNC-InfoNCE to guide its learning process. The NNC-InfoNCE is an improved version of InfoNCE, which can be adapted to neighborhood-level contrast learning. Specifically, the neighborhoods and the remaining nodes of the selected anchor are weighted and treated as positive and negative sample sets. The experimental results show that our method is effective on public benchmark datasets.}
}


@article{DBLP:journals/tkdd/LaiWCZ24,
	author = {Jinrong Lai and
                  Tong Wang and
                  Chuan Chen and
                  Zibin Zheng},
	title = {Information-aware Multi-view Outlier Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {98:1--98:16},
	year = {2024},
	url = {https://doi.org/10.1145/3638354},
	doi = {10.1145/3638354},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LaiWCZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of multi-view learning, multi-view outlier detection has received increasing attention in recent years. However, the current research still faces two challenges: (1) The current research lacks theoretical analysis tools for multi-view outliers. (2) Most current multi-view outlier detection algorithms are based on shallow structural assumptions of the data, such as cluster assumptions and subspace assumptions, thus they are not suitable for more complex data distributions. In addressing these two issues, this article proposes three occurrence mechanisms of multi-view outlier, which serve as foundational theoretical analysis tools for multi-view outliers. Utilizing proposed mechanisms, we analyze the impact of multi-view outliers and the information structure of multi-view data and validate our findings through experiments. Finally, we propose a novel algorithm referred to as Information-Aware Multi-View Outlier Detection (IAMOD). In contrast to other methods, IAMOD focuses on the information structure of multi-view data without relying on shallow structural assumptions. By learning a compact representation of the sample that is semantically rich and non-redundant, IAMOD can accurately identify multi-view outliers by comparing the consistency of the representations’ neighbors and views. Extensive experimental results demonstrate that our approach outperforms several state-of-the-art multi-view outlier detection methods.}
}


@article{DBLP:journals/tkdd/TuHYL24,
	author = {Jingke Tu and
                  Jiaming Huang and
                  Lei Yang and
                  Wanyu Lin},
	title = {Personalized Federated Learning with Layer-Wise Feature Transformation
                  via Meta-Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {99:1--99:21},
	year = {2024},
	url = {https://doi.org/10.1145/3638252},
	doi = {10.1145/3638252},
	timestamp = {Mon, 01 Apr 2024 11:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/TuHYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning enables multiple clients to collaboratively learn machine learning models in a privacy-preserving manner. However, in real-world scenarios, a key challenge encountered in federated learning is the statistical heterogeneity among clients. Existing work mainly focused on a single global model shared across the clients, making it hard to generalize well to all clients due to the large discrepancy in the data distributions. To address this challenge, we propose pFedLT, a novel approach that can adapt the single global model to different data distributions. Specifically, we propose to perform a pluggable layer-wise transformation during the local update phase based on scaling and shifting operations. In particular, these operations are learned with a meta-learning strategy. By doing so, pFedLT can capture the diversity of data distribution among clients, therefore, can generalize well even when the data distributions among clients exhibit high statistical heterogeneity. We conduct extensive experiments on synthetic and real-world datasets (MNIST, Fashion_MNIST, CIFAR-10, and Office+Caltech10) under different Non-IID settings. Experimental results demonstrate that pFedLT significantly improves the model accuracy by up to 11.67% and reduces the communication costs compared with state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/KhanMHEY24,
	author = {Mehak Khan and
                  Gustavo B. M. Mello and
                  Laurence Habib and
                  Paal Engelstad and
                  Anis Yazidi},
	title = {HITS-based Propagation Paradigm for Graph Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {100:1--100:23},
	year = {2024},
	url = {https://doi.org/10.1145/3638779},
	doi = {10.1145/3638779},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KhanMHEY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we present a new propagation paradigm based on the principle of Hyperlink-Induced Topic Search (HITS) algorithm. The HITS algorithm utilizes the concept of a “self-reinforcing” relationship of authority-hub. Using HITS, the centrality of nodes is determined via repeated updates of authority-hub scores that converge to a stationary distribution. Unlike PageRank-based propagation methods, which rely solely on the idea of authorities (in-links), HITS considers the relevance of both authorities (in-links) and hubs (out-links), thereby allowing for a more informative graph learning process. To segregate node prediction and propagation, we use a Multilayer Perceptron in combination with a HITS-based propagation approach and propose two models: HITS-GNN and HITS-GNN+. We provided additional validation of our models’ efficacy by performing an ablation study to assess the performance of authority-hub in independent models. Moreover, the effect of the main hyper-parameters and normalization is also analyzed to uncover how these techniques influence the performance of our models. Extensive experimental results indicate that the proposed approach significantly improves baseline methods on the graph (citation network) benchmark datasets by a decent margin for semi-supervised node classification, which can aid in predicting the categories (labels) of scientific articles not exclusively based on their content but also based on the type of articles they cite.}
}


@article{DBLP:journals/tkdd/GhahramanianBBC24,
	author = {Pouya Ghahramanian and
                  Sepehr Bakhshi and
                  Hamed R. Bonab and
                  Fazli Can},
	title = {A Novel Neural Ensemble Architecture for On-the-fly Classification
                  of Evolving Text Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {101:1--101:24},
	year = {2024},
	url = {https://doi.org/10.1145/3639054},
	doi = {10.1145/3639054},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GhahramanianBBC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study on-the-fly classification of evolving text streams in which the relation between the input data and target labels changes over time—i.e., “concept drift.” These variations decrease the model’s performance, as predictions become less accurate over time and they necessitate a more adaptable system. While most studies focus on concept drift detection and handling with ensemble approaches, the application of neural models in this area is relatively less studied. We introduce Adaptive Neural Ensemble Network (AdaNEN), a novel ensemble-based neural approach, capable of handling concept drift in data streams. With our novel architecture, we address some of the problems neural models face when exploited for online adaptive learning environments. Most current studies address concept drift detection and handling in numerical streams, and the evolving text stream classification remains relatively unexplored. We hypothesize that the lack of public and large-scale experimental data could be one reason. To this end, we propose a method based on an existing approach for generating evolving text streams by introducing various types of concept drifts to real-world text datasets. We provide an extensive evaluation of our proposed approach using 12 state-of-the-art baselines and 13 datasets. We first evaluate concept drift handling capability of AdaNEN and the baseline models on evolving numerical streams; this aims to demonstrate the concept drift handling capabilities of our method on a general spectrum and motivate its use in evolving text streams. The models are then evaluated in evolving text stream classification. Our experimental results show that AdaNEN consistently outperforms the existing approaches in terms of predictive performance with conservative efficiency.}
}


@article{DBLP:journals/tkdd/ZhangZF24,
	author = {Ying Zhang and
                  Zhiqiang Zhao and
                  Zhuo Feng},
	title = {diGRASS: Directed Graph Spectral Sparsification via Spectrum-Preserving
                  Symmetrization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {102:1--102:25},
	year = {2024},
	url = {https://doi.org/10.1145/3639568},
	doi = {10.1145/3639568},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent spectral graph sparsification research aims to construct ultra-sparse subgraphs for preserving the original graph spectral (structural) properties, such as the first few Laplacian eigenvalues and eigenvectors, which has led to the development of a variety of nearly-linear time numerical and graph algorithms. However, there is very limited progress for spectral sparsification of directed graphs. In this work, we prove the existence of nearly-linear-sized spectral sparsifiers for directed graphs under certain conditions. Furthermore, we introduce a practically-efficient spectral algorithm (diGRASS) for sparsifying real-world, large-scale directed graphs leveraging spectral matrix perturbation analysis. The proposed method has been evaluated using a variety of directed graphs obtained from real-world applications, showing promising results for solving directed graph Laplacians, spectral partitioning of directed graphs, and approximately computing (personalized) PageRank vectors.}
}


@article{DBLP:journals/tkdd/LiWYZG24,
	author = {Xinjiao Li and
                  Guowei Wu and
                  Lin Yao and
                  Zhaolong Zheng and
                  Shisong Geng},
	title = {Utility-aware Privacy Perturbation for Training Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {103:1--103:21},
	year = {2024},
	url = {https://doi.org/10.1145/3639411},
	doi = {10.1145/3639411},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiWYZG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget\xa0allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget\xa0allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget\xa0allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget\xa0allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget\xa0allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility.}
}


@article{DBLP:journals/tkdd/WeiSY24,
	author = {Wanxu Wei and
                  Yitong Song and
                  Bin Yao},
	title = {Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based
                  Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {4},
	pages = {104:1--104:20},
	year = {2024},
	url = {https://doi.org/10.1145/3639472},
	doi = {10.1145/3639472},
	timestamp = {Mon, 01 Apr 2024 11:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WeiSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of KGs, they are becoming inaccurate and incomplete. This problem can be solved by the KG completion methods, of which graph attention network (GAT)-based methods stand out because of their superior performance. However, existing GAT-based KG completion methods often suffer from overfitting issues when dealing with heterogeneous KGs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduce novel encoding and feature transformation approaches, enabling the robust performance of GATH in scenarios with imbalanced samples. Comprehensive experiments are conducted to evaluate GATH’s performance. Compared with the existing state-of-the-art GAT-based model on Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the FB15K-237 dataset and by 4.5% and 14.6% on the WN18RR dataset, respectively.}
}


@article{DBLP:journals/tkdd/PanZJMW24,
	author = {Yicheng Pan and
                  Yifan Zhang and
                  Xinrui Jiang and
                  Meng Ma and
                  Ping Wang},
	title = {EffCause: Discover Dynamic Causal Relationships Efficiently from Time-Series},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {105:1--105:21},
	year = {2024},
	url = {https://doi.org/10.1145/3640818},
	doi = {10.1145/3640818},
	timestamp = {Thu, 13 Feb 2025 18:54:28 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PanZJMW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the proposal of Granger causality, many researchers have followed the idea and developed extensions to the original algorithm. The classic Granger causality test aims to detect the existence of the static causal relationship. Notably, a fundamental assumption underlying most previous studies is the stationarity of causality, which requires the causality between variables to keep stable. However, this study argues that it is easy to break in real-world scenarios. Fortunately, our paper presents an essential observation: if we consider a sufficiently short window when discovering the rapidly changing causalities, they will keep approximately static and thus can be detected using the static way correctly. In light of this, we develop EffCause, bringing dynamics into classic Granger causality. Specifically, to efficiently examine the causalities on different sliding window lengths, we design two optimization schemes in EffCause and demonstrate the advantage of EffCause through extensive experiments on both simulated and real-world datasets. The results validate that EffCause achieves state-of-the-art accuracy in continuous causal discovery tasks while achieving faster computation. Case studies from cloud system failure analysis and traffic flow monitoring show that EffCause effectively helps us understand real-world time-series data and solve practical problems.}
}


@article{DBLP:journals/tkdd/YaoL24,
	author = {Kai{-}Lang Yao and
                  Wu{-}Jun Li},
	title = {Asymmetric Learning for Graph Neural Network based Link Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {106:1--106:18},
	year = {2024},
	url = {https://doi.org/10.1145/3640347},
	doi = {10.1145/3640347},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YaoL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link prediction is a fundamental problem in many graph-based applications, such as protein-protein interaction prediction. Recently, graph neural network\xa0(GNN) has been widely used for link prediction. However, existing GNN-based link prediction\xa0(GNN-LP) methods suffer from scalability problem during training for large-scale graphs, which has received little attention from researchers. In this paper, we first analyze the computation complexity of existing GNN-LP methods, revealing that one reason for the scalability problem stems from their symmetric learning strategy in applying the same class of GNN models to learn representation for both head nodes and tail nodes. We then propose a novel method, called asymmetric learning\xa0(AML), for GNN-LP. More specifically, AML applies a GNN model to learn head node representation while applying a multi-layer perceptron\xa0(MLP) model to learn tail node representation. To the best of our knowledge, AML is the first GNN-LP method to adopt an asymmetric learning strategy for node representation learning. Furthermore, we design a novel model architecture and apply a row-wise mini-batch sampling strategy to ensure promising model accuracy and training efficiency for AML. Experiments on three real large-scale datasets show that AML is 1.7×∼7.3× faster in training than baselines with a symmetric learning strategy while having almost no accuracy loss.}
}


@article{DBLP:journals/tkdd/GuoHTLLZSM24,
	author = {Xiaobo Guo and
                  Mingming Ha and
                  Xuewen Tao and
                  Shaoshuai Li and
                  Youru Li and
                  Zhenfeng Zhu and
                  Zhiyong Shen and
                  Li Ma},
	title = {Multi-Task Learning with Sequential Dependence Toward Industrial Applications:
                  {A} Systematic Formulation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {107:1--107:29},
	year = {2024},
	url = {https://doi.org/10.1145/3640468},
	doi = {10.1145/3640468},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GuoHTLLZSM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-task learning (MTL) is widely used in the online recommendation and financial services for multi-step conversion estimation, but current works often overlook the sequential dependence among tasks. In particular, sequential dependence multi-task learning (SDMTL) faces challenges in dealing with complex task correlations and extracting valuable information in real-world scenarios, leading to negative transfer and a deterioration in the performance. Herein, a systematic learning paradigm of the SDMTL problem is established for the first time, which applies to more general multi-step conversion scenarios with longer conversion paths or various task dependence relationships. Meanwhile, an SDMTL architecture, named Task-Aware Feature Extraction (TAFE), is designed to enable the dynamic task representation learning from a sample-wise view. TAFE selectively reconstructs the implicit shared information corresponding to each sample case and performs the explicit task-specific extraction under dependence constraints, which can avoid the negative transfer, resulting in more effective information sharing and joint representation learning. Extensive experiment results demonstrate the effectiveness and applicability of the proposed theoretical and implementation frameworks. Furthermore, the online evaluations at MYbank showed that TAFE had an average increase of 9.22% and 3.76% in various scenarios on the post-view click-through & conversion rate (CTCVR) estimation task. Currently, TAFE is deployed in an online platform to provide various traffic services.}
}


@article{DBLP:journals/tkdd/ZhangLZCWXSC24,
	author = {Lei Zhang and
                  Yong Liu and
                  Zhiwei Zeng and
                  Yiming Cao and
                  Xingyu Wu and
                  Yonghui Xu and
                  Zhiqi Shen and
                  Lizhen Cui},
	title = {Package Arrival Time Prediction via Knowledge Distillation Graph Neural
                  Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {108:1--108:19},
	year = {2024},
	url = {https://doi.org/10.1145/3643033},
	doi = {10.1145/3643033},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangLZCWXSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurately estimating packages’ arrival time in e-commerce can enhance users’ shopping experience and improve the placement rate of products. This problem is often formalized as an Origin-Destination (OD)-based ETA (i.e., estimated time of arrival) prediction task, where the delivery time is estimated mainly based on sender and receiver addresses and other context information. One inherent challenge of the OD-based ETA problem is that the delivery time highly depends on the actual delivery trajectory which is unknown at the time of prediction. In this article, we tackle this challenge by effectively exploiting historical delivery trajectories. We propose a novel Knowledge Distillation Graph neural network-based package ETA prediction (KDG-ETA) model, which uses knowledge distillation in the training phase to distill the knowledge of historical trajectories into OD pair embeddings. In KDG-ETA, a multi-level trajectory graph representation model is proposed to fully exploit trajectory information at the node-level, edge-level, and path-level. Then, the OD representations embedded with trajectory knowledge are combined with context embeddings from feature extraction module for delivery time prediction using an adaptive attention module. KDG-ETA consistently outperforms existing state-of-the-art OD-based ETA prediction methods on three real-world Alibaba datasets, reducing the Mean Absolute Error (MAE) by 3.0%–39.1% as demonstrated in our extensive empirical evaluation.}
}


@article{DBLP:journals/tkdd/KuoCPHS24,
	author = {Chuan{-}Wei Kuo and
                  Bo{-}Yu Chen and
                  Wen{-}Chih Peng and
                  Chih{-}Chieh Hung and
                  Hsin{-}Ning Su},
	title = {Correlation-aware Graph Data Augmentation with Implicit and Explicit
                  Neighbors},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {109:1--109:23},
	year = {2024},
	url = {https://doi.org/10.1145/3638057},
	doi = {10.1145/3638057},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KuoCPHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, there has been a significant surge in commercial demand for citation graph-based tasks, such as patent analysis, social network analysis, and recommendation systems. Graph Neural Networks (GNNs) are widely used for these tasks due to their remarkable performance in capturing topological graph information. However, GNNs’ output results are highly dependent on the composition of local neighbors within the topological structure. To address this issue, we identify two types of neighbors in a citation graph: explicit neighbors based on the topological structure and implicit neighbors based on node features. Our primary motivation is to clearly define and visualize these neighbors, emphasizing their importance in enhancing graph neural network performance. We propose a Correlation-aware Network (CNet) to re-organize the citation graph and learn more valuable informative representations by leveraging these implicit and explicit neighbors. Our approach aims to improve graph data augmentation and classification performance, with the majority of our focus on stating the importance of using these neighbors, while also introducing a new graph data augmentation method. We compare CNet with state-of-the-art (SOTA) GNNs and other graph data augmentation approaches acting on GNNs. Extensive experiments demonstrate that CNet effectively extracts more valuable informative representations from the citation graph, significantly outperforming baselines. The code is available on public GitHub.}
}


@article{DBLP:journals/tkdd/DuanLZQX24,
	author = {Mingxing Duan and
                  Kenli Li and
                  Weinan Zhang and
                  Jiarui Qin and
                  Bin Xiao},
	title = {Attacking Click-through Rate Predictors via Generating Realistic Fake
                  Samples},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {110:1--110:24},
	year = {2024},
	url = {https://doi.org/10.1145/3643685},
	doi = {10.1145/3643685},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DuanLZQX24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to construct imperceptible (realistic) fake samples is critical in adversarial attacks. Due to the sample feature diversity of a recommender system (containing both discrete and continuous features), traditional gradient-based adversarial attack methods may fail to construct realistic fake samples. Meanwhile, most recommendation models adopt click-through rate (CTR) predictors, which usually utilize black-box deep models with discrete features as input. Thus, how to efficiently construct realistic fake samples for black-box recommender systems is still full of challenges. In this article, we propose a hierarchical adversarial attack method against black-box CTR models via generating realistic fake samples, named CTRAttack. To better train the generation network, the weights of its embedding layer are shared with those of the substitute model, with both the similarity loss and classification loss used to update the generation network. To ensure that the discrete features of the generated fake samples are all real, we first adopt the similarity loss to ensure that the distribution of the generated perturbed samples is sufficiently close to the distribution of the real features, and then the nearest neighbor algorithm is used to retrieve the most appropriate features for non-existent discrete features from the candidate instance set. Extensive experiments demonstrate that CTRAttack can not only effectively attack the black-box recommender systems but also improve the robustness of these models while maintaining prediction accuracy.}
}


@article{DBLP:journals/tkdd/KhodabandehlouG24,
	author = {Samira Khodabandehlou and
                  Alireza Hashemi Golpayegani},
	title = {FiFrauD: Unsupervised Financial Fraud Detection in Dynamic Graph Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {111:1--111:29},
	year = {2024},
	url = {https://doi.org/10.1145/3641857},
	doi = {10.1145/3641857},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/KhodabandehlouG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a stream of financial transactions between traders in an e-market, how can we accurately detect fraudulent traders and suspicious behaviors in real time? Despite the efforts made in detecting these fraudsters, this field still faces serious challenges, including the ineffectiveness of existing methods for the complex and streaming environment of e-markets. As a result, it is still difficult to quickly and accurately detect suspected traders and behavior patterns in real-time transactions, and it is still considered an open problem. To solve this problem and alleviate the existing challenges, in this article, we propose FiFrauD, which is an unsupervised, scalable approach that depicts the behavior of manipulators in a transaction stream. In this approach, real-time transactions between traders are converted into a stream of graphs and, instead of using supervised and semi-supervised learning methods, fraudulent traders are detected precisely by exploiting density signals in graphs. Specifically, we reveal the traits of fraudulent traders in the market and propose a novel metric from this perspective, i.e., graph topology, time, and behavior. Then, we search for suspicious blocks by greedily optimizing the proposed metric. Theoretical analysis demonstrates upper bounds for FiFrauD's effectiveness in catching suspicious trades. Extensive experiments on five real-world datasets with both actual and synthetic labels demonstrate that FiFrauD achieves significant accuracy improvements compared with state-of-the-art fraud detection methods. Also, it can find various suspicious behavior patterns in a linear runtime and provide interpretable results. Furthermore, FiFrauD is resistant to the camouflage tactics used by fraudulent traders.}
}


@article{DBLP:journals/tkdd/SunMYJC24,
	author = {Jianshan Sun and
                  Suyuan Mei and
                  Kun Yuan and
                  Yuanchun Jiang and
                  Jie Cao},
	title = {Prerequisite-Enhanced Category-Aware Graph Neural Networks for Course
                  Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {112:1--112:21},
	year = {2024},
	url = {https://doi.org/10.1145/3643644},
	doi = {10.1145/3643644},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunMYJC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Massive Open Online Courses (MOOCs) platforms has created an urgent need for an efficient personalized course recommender system that can assist learners of all backgrounds and levels of knowledge in selecting appropriate courses. Currently, most existing methods utilize a sequential recommendation paradigm that captures the user’s learning interests from their learning history, typically through recurrent or graph neural networks. However, fewer studies have explored how to incorporate principles of human learning at both the course and category levels to enhance course recommendations. In this article, we aim at addressing this gap by introducing a novel model, named Prerequisite-Enhanced Catory-Aware Graph Neural Network (PCGNN), for course recommendation. Specifically, we first construct a course prerequisite graph that reflects the human learning principles and further pre-train the course prerequisite relationships as the base embeddings for courses and categories. Then, to capture the user’s complex learning patterns, we build an item graph and a category graph from the user’s historical learning records, respectively: (1) the item graph reflects the course-level local learning transition patterns and (2) the category graph provides insight into the user’s long-term learning interest. Correspondingly, we propose a user interest encoder that employs a gated graph neural network to learn the course-level user interest embedding and design a category transition pattern encoder that utilizes GRU to yield the category-level user interest embedding. Finally, the two fine-grained user interest embeddings are fused to achieve precise course prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of PCGNN compared with other state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/DengSLSR24,
	author = {Songgaojun Deng and
                  Olivier Sprangers and
                  Ming Li and
                  Sebastian Schelter and
                  Maarten de Rijke},
	title = {Domain Generalization in Time Series Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {113:1--113:24},
	year = {2024},
	url = {https://doi.org/10.1145/3643035},
	doi = {10.1145/3643035},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/DengSLSR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain generalization aims to design models that can effectively generalize to unseen target domains by learning from observed source domains. Domain generalization poses a significant challenge for time series data, due to varying data distributions and temporal dependencies. Existing approaches to domain generalization are not designed for time series data, which often results in suboptimal or unstable performance when confronted with diverse temporal patterns and complex data characteristics. We propose a novel approach to tackle the problem of domain generalization in time series forecasting. We focus on a scenario where time series domains share certain common attributes and exhibit no abrupt distribution shifts. Our method revolves around the incorporation of a key regularization term into an existing time series forecasting model: domain discrepancy regularization. In this way, we aim to enforce consistent performance across different domains that exhibit distinct patterns. We calibrate the regularization term by investigating the performance within individual domains and propose the domain discrepancy regularization with domain difficulty awareness. We demonstrate the effectiveness of our method on multiple datasets, including synthetic and real-world time series datasets from diverse domains such as retail, transportation, and finance. Our method is compared against traditional methods, deep learning models, and domain generalization approaches to provide comprehensive insights into its performance. In these experiments, our method showcases superior performance, surpassing both the base model and competing domain generalization models across all datasets. Furthermore, our method is highly general and can be applied to various time series models.}
}


@article{DBLP:journals/tkdd/HuangGY24,
	author = {Gengsen Huang and
                  Wensheng Gan and
                  Philip S. Yu},
	title = {TaSPM: Targeted Sequential Pattern Mining},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {114:1--114:18},
	year = {2024},
	url = {https://doi.org/10.1145/3639827},
	doi = {10.1145/3639827},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangGY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential pattern mining (SPM) is an important technique in the field of pattern mining, which has many applications in reality. Although many efficient SPM algorithms have been proposed, there are few studies that can focus on targeted tasks. Targeted querying of the concerned sequential patterns can not only reduce the number of patterns generated, but also increase the efficiency of users in performing related analysis. The current algorithms available for targeted sequence querying are based on specific scenarios and can not be extended to other applications. In this article, we formulate the problem of targeted sequential pattern mining and propose a generic algorithm, namely TaSPM. What is more, to improve the efficiency of TaSPM on large-scale datasets and multiple-item-based sequence datasets, we propose several pruning strategies to reduce meaningless operations in the mining process. Totally four pruning strategies are designed in TaSPM, and hence TaSPM can terminate unnecessary pattern extensions quickly and achieve better performance. Finally, we conducted extensive experiments on different datasets to compare the baseline SPM algorithm with TaSPM. Experiments show that the novel targeted mining algorithm TaSPM can achieve faster running time and less memory consumption.}
}


@article{DBLP:journals/tkdd/ZhuJJZGHLW24,
	author = {Yichen Zhu and
                  Bo Jiang and
                  Haiming Jin and
                  Mengtian Zhang and
                  Feng Gao and
                  Jianqiang Huang and
                  Tao Lin and
                  Xinbing Wang},
	title = {Networked Time-series Prediction with Incomplete Data via Generative
                  Adversarial Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {115:1--115:25},
	year = {2024},
	url = {https://doi.org/10.1145/3643822},
	doi = {10.1145/3643822},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuJJZGHLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A networked time series (NETS) is a family of time series on a given graph, one for each node. It has a wide range of applications from intelligent transportation to environment monitoring to smart grid management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, and so on. In this article, we study the problem of NETS prediction with incomplete data. We propose networked time series Imputation Generative Adversarial Network (NETS-ImpGAN), a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose Graph Temporal Attention Networks, which incorporate the attention mechanism to capture both inter-time series and temporal correlations. We conduct extensive experiments on four real-world datasets under different missing patterns and missing rates. The experimental results show that NETS-ImpGAN outperforms existing methods, reducing the Mean Absolute Error by up to 25%.}
}


@article{DBLP:journals/tkdd/RongQMJS24,
	author = {Huan Rong and
                  Minfeng Qian and
                  Tinghuai Ma and
                  Di Jin and
                  Victor S. Sheng},
	title = {CoBjeason: Reasoning Covered Object in Image by Multi-Agent Collaboration
                  Based on Informed Knowledge Graph},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {116:1--116:56},
	year = {2024},
	url = {https://doi.org/10.1145/3643565},
	doi = {10.1145/3643565},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/RongQMJS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Object detection is a widely studied problem in existing works. However, in this paper, we turn to a more challenging problem of “Covered Object Reasoning”, aimed at reasoning the category label of target object in the given image particularly when it has been totally covered (or invisible). To resolve this problem, we propose CoBjeason to seize the opportunity when visual reasoning meets the knowledge graph, where “empirical cognition” on common visual contexts have been incorporated as knowledge graph to conduct reinforced multi-hop reasoning via two collaborative agents. Such two agents, for one thing, stand at the covered object (or unknown entity) to observe the surrounding visual cues in the given image and gradually select entities and relations from the global gallery-level knowledge graph which contains entity-pairs frequently occurring across the entire image-collection, so as to infer the main structure of image-level knowledge graph forward expanded from the unknown entity. In turn, for another, based on the reasoned image-level knowledge graph, the semantic context among entities will be aggregated backward into unknown entity to select an appropriate entity from the global gallery-level knowledge graph as the reasoning result. Moreover, such two agents will collaborate with each other, securing that the above Forward & Backward Reasoning will step towards the same destination of the higher performance on covered object reasoning. To our best knowledge, this is the first work on Covered Object Reasoning with Knowledge Graphs and reinforced Multi-Agent collaboration. Particularly, our study on Covered Object Reasoning and the proposed model CoBjeason could offer novel insights into more basic Computer Vision (CV) tasks, such as Semantic Segmentation with better understanding on the current scene when some objects are blurred or covered, Visual Question Answering with enhancement on the inference in more complicated visual context when some objects are covered or invisible, and Image Caption Generation with the augmentation on the richness of visual context for images containing partially visible objects. The improvement on the above basic CV tasks can further refine more complicated ones involved with nuanced visual interpretation like Autonomous Driving, where the recognition and reasoning on partially visible or covered object are critical. According to the experimental results, our proposed CoBjeason can achieve the best overall ranking performance on covered object reasoning compared with other models, meanwhile enjoying the advantage of lower “exploration cost”, with the insensitivity against the long-tail covered objects and the acceptable time complexity.}
}


@article{DBLP:journals/tkdd/PandeyAS24,
	author = {Pradumn Kumar Pandey and
                  Aikta Arya and
                  Akrati Saxena},
	title = {X-distribution: Retraceable Power-law Exponent of Complex Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {117:1--117:12},
	year = {2024},
	url = {https://doi.org/10.1145/3639413},
	doi = {10.1145/3639413},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/PandeyAS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network modeling has been explored extensively by means of theoretical analysis as well as numerical simulations for Network Reconstruction (NR). The network reconstruction problem requires the estimation of the power-law exponent (γ) of a given input network. Thus, the effectiveness of the NR solution depends on the accuracy of the calculation of γ. In this article, we re-examine the degree distribution-based estimation of γ, which is not very accurate due to approximations. We propose X-distribution, which is more accurate than degree distribution. Various state-of-the-art network models, including CPM, NRM, RefOrCite2, BA, CDPAM, and DMS, are considered for simulation purposes, and simulated results support the proposed claim. Further, we apply X-distribution over several real-world networks to calculate their power-law exponents, which differ from those calculated using respective degree distributions. It is observed that X-distributions exhibit more linearity (straight line) on the log-log scale than degree distributions. Thus, X-distribution is more suitable for the evaluation of power-law exponent using linear fitting (on the log-log scale). The MATLAB implementation of power-law exponent (γ) calculation using X-distribution for different network models and the real-world datasets used in our experiments are available at https://github.com/Aikta-Arya/X-distribution-Retraceable-Power-Law-Exponent-of-Complex-Networks.git.}
}


@article{DBLP:journals/tkdd/MoradniaG24,
	author = {Sajedeh Moradnia and
                  Mousa Golalizadeh},
	title = {Supervised Clustering of Persian Handwritten Images Using Regularization
                  and Dimension Reduction Methods},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {118:1--118:19},
	year = {2024},
	url = {https://doi.org/10.1145/3638060},
	doi = {10.1145/3638060},
	timestamp = {Tue, 14 May 2024 19:49:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/MoradniaG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering, as a fundamental exploratory data technique, not only is used to discover patterns and structures in complex datasets but also is utilized to group variables in high-dimensional data analysis. Dimension reduction through clustering helps identify important variables and reduce data dimensions without losing significant information. High-dimensional image datasets, such as Persian handwritten images, have numerous pixels, making statistical inference difficult. Such high-dimensionality property pose challenges for analysis and processing, requiring specialized techniques like clustering to extract information. Incorporating response variable information enhances clustering analysis, transforming it into a supervised method. This article evaluates a supervised clustering approach using Ridge and Lasso penalties, comparing them in analyzing a real dataset while identifying important variables. We demonstrate that despite choosing a small number of variables as important variables, Lasso penalty performs relatively well in predicting the labels of new observations for this multi-class dataset.}
}


@article{DBLP:journals/tkdd/ChenE24,
	author = {Hongjie Chen and
                  Hoda Eldardiry},
	title = {Graph Time-series Modeling in Deep Learning: {A} Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {119:1--119:35},
	year = {2024},
	url = {https://doi.org/10.1145/3638534},
	doi = {10.1145/3638534},
	timestamp = {Mon, 10 Feb 2025 16:57:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-series and graphs have been extensively studied for their ubiquitous existence in numerous domains. Both topics have been separately explored in the field of deep learning. For time-series modeling, recurrent neural networks or convolutional neural networks model the relations between values across timesteps, while for graph modeling, graph neural networks model the inter-relations between nodes. Recent research in deep learning requires simultaneous modeling for time-series and graphs when both representations are present. For example, both types of modeling are necessary for time-series classification, regression, and anomaly detection in graphs. This article aims to provide a comprehensive summary of these models, which we call graph time-series models. To the best of our knowledge, this is the first survey article that provides a picture of related models from the perspective of deep graph time-series modeling to address a range of time-series tasks, including regression, classification, and anomaly detection. Graph time-series models are split into two categories: (a) graph recurrent/convolutional neural networks and (b) graph attention neural networks. Under each category, we further categorize models based on their properties. Additionally, we compare representative models and discuss how distinctive model characteristics are utilized with respect to various model components and data challenges. Pointers to commonly used datasets and code are included to facilitate access for further research. In the end, we discuss potential directions for future research.}
}


@article{DBLP:journals/tkdd/PoulakisDK24,
	author = {Yannis Poulakis and
                  Christos Doulkeridis and
                  Dimosthenis Kyriazis},
	title = {A Survey on AutoML Methods and Systems for Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {120:1--120:30},
	year = {2024},
	url = {https://doi.org/10.1145/3643564},
	doi = {10.1145/3643564},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/PoulakisDK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Automated Machine Learning (AutoML) aims to identify the best-performing machine learning algorithm along with its input parameters for a given dataset and a specific machine learning task. This is a challenging problem, as the process of finding the best model and tuning it for a particular problem at hand is both time-consuming for a data scientist and computationally expensive. In this survey, we focus on unsupervised learning, and we turn our attention on AutoML methods for clustering. We present a systematic review that includes many recent research works for automated clustering. Furthermore, we provide a taxonomy for the classification of existing works, and we perform a qualitative comparison. As a result, this survey provides a comprehensive overview of the field of AutoML for clustering. Moreover, we identify open challenges for future research in this field.}
}


@article{DBLP:journals/tkdd/GonzalezAADR24,
	author = {Ansel Y. Rodr{\'{\i}}guez{-}Gonz{\'{a}}lez and
                  Ram{\'{o}}n Aranda and
                  Miguel {\'{A}}ngel {\'{A}}lvarez{-}Carmona and
                  {\'{A}}ngel D{\'{\i}}az{-}Pacheco and
                  Rosa Mar{\'{\i}}a Valdovinos Rosas},
	title = {X-FSPMiner: {A} Novel Algorithm for Frequent Similar Pattern Mining},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {121:1--121:26},
	year = {2024},
	url = {https://doi.org/10.1145/3643820},
	doi = {10.1145/3643820},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GonzalezAADR24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequent similar pattern mining (FSP mining) allows for finding frequent patterns hidden from the classical approach. However, the use of similarity functions implies more computational effort, necessitating the development of more efficient algorithms for FSP mining. This work aims to improve the efficiency of mining all FSPs when using Boolean and non-increasing monotonic similarity functions. A data structure to condense an object description collection, named FV-Tree, and an algorithm for mining all FSPs from the FV-Tree, named X-FSPMiner, are proposed. The experimental results reveal that the novel algorithm X-FSPMiner vastly outperforms the state-of-the-art algorithms for mining all FSPs using Boolean and non-increasing monotonic similarity functions.}
}


@article{DBLP:journals/tkdd/LuanGTZH24,
	author = {Tianxiang Luan and
                  Shilin Gu and
                  Xijia Tang and
                  Wenzhang Zhuge and
                  Chenping Hou},
	title = {Multi-Instance Learning with One Side Label Noise},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {122:1--122:24},
	year = {2024},
	url = {https://doi.org/10.1145/3644076},
	doi = {10.1145/3644076},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LuanGTZH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-instance Learning (MIL) is a popular learning paradigm arising from many real applications. It assigns a label to a set of instances, which is called a bag, and the bag’s label is determined by the instances within it. A bag is positive if and only if it has at least one positive instance. Since labeling bags is more complicated than labeling each instance, we will often face the mislabeling problem in MIL. Furthermore, it is more common that a negative bag has been mislabeled to a positive one, since one mislabeled instance will lead to the change of the whole bag label. This is an important problem that originated from real applications, e.g., web mining and image classification, but little research has concentrated on it as far as we know. In this article, we focus on this MIL problem with one side label noise that the negative bags are mislabeled as positive ones. To address this challenging problem, we propose, to the best our our knowledge, a novel multi-instance learning method with one side label noise. We design a new double weighting approach under traditional framework to characterize the “faithfulness” of each instance and each bag in learning the classifier. Briefly, on the instance level, we employ a sparse weighting method to select the key instances, and the MIL problem with one size label noise is converted to a mislabeled supervised learning scenario. On the bag level, the weights of bags, together with the selected key instances, will be utilized to identify the real positive bags. In addition, we have solved our proposed model by an alternative iteration method with proved convergence behavior. Empirical studies on various datasets have validated the effectiveness of our method.}
}


@article{DBLP:journals/tkdd/QinWHWLH24,
	author = {Wei Qin and
                  Xiaowei Wang and
                  Zhenzhen Hu and
                  Lei Wang and
                  Yunshi Lan and
                  Richang Hong},
	title = {Math Word Problem Generation via Disentangled Memory Retrieval},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {123:1--123:21},
	year = {2024},
	url = {https://doi.org/10.1145/3639569},
	doi = {10.1145/3639569},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/QinWHWLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The task of math word problem (MWP) generation, which generates an MWP given an equation and relevant topic words, has increasingly attracted researchers’ attention. In this work, we introduce a simple memory retrieval module to search related training MWPs, which are used to augment the generation. To retrieve more relevant training data, we also propose a disentangled memory retrieval module based on the simple memory retrieval module. To this end, we first disentangle the training MWPs into logical description and scenario description and then record them in respective memory modules. Later, we use the given equation and topic words as queries to retrieve relevant logical descriptions and scenario descriptions from the corresponding memory modules, respectively. The retrieved results are then used to complement the process of the MWP generation. Extensive experiments and ablation studies verify the superior performance of our method and the effectiveness of each proposed module. The code is available at https://github.com/mwp-g/MWPG-DMR.}
}


@article{DBLP:journals/tkdd/WangPDFLHCC24,
	author = {Haobo Wang and
                  Cheng Peng and
                  Hede Dong and
                  Lei Feng and
                  Weiwei Liu and
                  Tianlei Hu and
                  Ke Chen and
                  Gang Chen},
	title = {On the Value of Head Labels in Multi-Label Text Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {124:1--124:21},
	year = {2024},
	url = {https://doi.org/10.1145/3643853},
	doi = {10.1145/3643853},
	timestamp = {Mon, 10 Feb 2025 15:25:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangPDFLHCC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A formidable challenge in the multi-label text classification (MLTC) context is that the labels often exhibit a long-tailed distribution, which typically prevents deep MLTC models from obtaining satisfactory performance. To alleviate this problem, most existing solutions attempt to improve tail performance by means of sampling or introducing extra knowledge. Data-rich labels, though more trustworthy, have not received the attention they deserve. In this work, we propose a multiple-stage training framework to exploit both model- and feature-level knowledge from the head labels, to improve both the representation and generalization ability of MLTC models. Moreover, we theoretically prove the superiority of our framework design over other alternatives. Comprehensive experiments on widely used MLTC datasets clearly demonstrate that the proposed framework achieves highly superior results to state-of-the-art methods, highlighting the value of head labels in MLTC.}
}


@article{DBLP:journals/tkdd/HuF24,
	author = {Wentao Hu and
                  Hui Fang},
	title = {Towards Differential Privacy in Sequential Recommendation: {A} Noisy
                  Graph Neural Network Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {125:1--125:21},
	year = {2024},
	url = {https://doi.org/10.1145/3643821},
	doi = {10.1145/3643821},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, we are the first to achieve differential privacy in sequential recommendation with dependent interactions. Specifically, in DIPSGNN, we first leverage piecewise mechanism to protect sensitive user features. Then, we innovatively add calibrated noise into aggregation step of graph neural network based on aggregation perturbation mechanism. And, this noisy graph neural network can protect sequentially dependent interactions and capture user preferences simultaneously. Extensive experiments demonstrate the superiority of our method over state-of-the-art differentially private recommender systems in terms of better balance between privacy and accuracy.}
}


@article{DBLP:journals/tkdd/NiYLZ24,
	author = {Li Ni and
                  Rui Ye and
                  Wenjian Luo and
                  Yiwen Zhang},
	title = {Local Community Detection in Multiple Private Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {126:1--126:21},
	year = {2024},
	url = {https://doi.org/10.1145/3644078},
	doi = {10.1145/3644078},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NiYLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Individuals are often involved in multiple online social networks. Considering that owners of these networks are unwilling to share their networks, some global algorithms combine information from multiple networks to detect all communities in multiple networks without sharing their edges. When data owners are only interested in the community containing a given node, it is unnecessary and computationally expensive for multiple networks to interact with each other to mine all communities. Moreover, data owners who are specifically looking for a community typically prefer to provide less data than the global algorithms require. Therefore, we propose the Local Collaborative Community Detection problem (LCCD). It exploits information from multiple networks to jointly detect the local community containing a given node without directly sharing edges between networks. To address the LCCD problem, we present a method developed from M method, called colM, to detect the local community in multiple networks. This method adopts secure multiparty computation protocols to protect each network’s private information. Our experiments were conducted on real-world and synthetic datasets. Experimental results show that colM method could effectively identify community structures and outperform comparison algorithms.}
}


@article{DBLP:journals/tkdd/SuiMWWWHC24,
	author = {Yongduo Sui and
                  Wenyu Mao and
                  Shuyao Wang and
                  Xiang Wang and
                  Jiancan Wu and
                  Xiangnan He and
                  Tat{-}Seng Chua},
	title = {Enhancing Out-of-distribution Generalization on Graphs via Causal
                  Attention Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {127:1--127:24},
	year = {2024},
	url = {https://doi.org/10.1145/3644392},
	doi = {10.1145/3644392},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SuiMWWWHC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In graph classification, attention- and pooling-based graph neural networks (GNNs) predominate to extract salient features from the input graph and support the prediction. They mostly follow the paradigm of “learning to attend,” which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm causes GNN classifiers to indiscriminately absorb all statistical correlations between input features and labels in the training data without distinguishing the causal and noncausal effects of features. Rather than emphasizing causal features, the attended graphs tend to rely on noncausal features as shortcuts to predictions. These shortcut features may easily change outside the training distribution, thereby leading to poor generalization for GNN classifiers. In this article, we take a causal view on GNN modeling. Under our causal assumption, the shortcut feature serves as a confounder between the causal feature and prediction. It misleads the classifier into learning spurious correlations that facilitate prediction in in-distribution (ID) test evaluation while causing significant performance drop in out-of-distribution (OOD) test data. To address this issue, we employ the backdoor adjustment from causal theory—combining each causal feature with various shortcut features, to identify causal patterns and mitigate the confounding effect. Specifically, we employ attention modules to estimate the causal and shortcut features of the input graph. Then, a memory bank collects the estimated shortcut features, enhancing the diversity of shortcut features for combination. Simultaneously, we apply the prototype strategy to improve the consistency of intra-class causal features. We term our method as CAL+, which can promote stable relationships between causal estimation and prediction, regardless of distribution changes. Extensive experiments on synthetic and real-world OOD benchmarks demonstrate our method’s effectiveness in improving OOD generalization. Our codes are released at https://github.com/shuyao-wang/CAL-plus.}
}


@article{DBLP:journals/tkdd/SunJHY24,
	author = {Kai Sun and
                  Huajie Jiang and
                  Yongli Hu and
                  Baocai Yin},
	title = {Incorporating Multi-Level Sampling with Adaptive Aggregation for Inductive
                  Knowledge Graph Completion},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {128:1--128:16},
	year = {2024},
	url = {https://doi.org/10.1145/3644822},
	doi = {10.1145/3644822},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunJHY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, Graph Neural Networks (GNNs) have achieved unprecedented success in handling graph-structured data, thereby driving the development of numerous GNN-oriented techniques for inductive knowledge graph completion (KGC). A key limitation of existing methods, however, is their dependence on pre-defined aggregation functions, which lack the adaptability to diverse data, resulting in suboptimal performance on established benchmarks. Another challenge arises from the exponential increase in irrelated entities as the reasoning path lengthens, introducing unwarranted noise and consequently diminishing the model’s generalization capabilities. To surmount these obstacles, we design an innovative framework that synergizes Multi-Level Sampling with an Adaptive Aggregation mechanism (MLSAA). Distinctively, our model couples GNNs with enhanced set transformers, enabling dynamic selection of the most appropriate aggregation function tailored to specific datasets and tasks. This adaptability significantly boosts both the model’s flexibility and its expressive capacity. Additionally, we unveil a unique sampling strategy designed to selectively filter irrelevant entities, while retaining potentially beneficial targets throughout the reasoning process. We undertake an exhaustive evaluation of our novel inductive KGC method across three pivotal benchmark datasets and the experimental results corroborate the efficacy of MLSAA.}
}


@article{DBLP:journals/tkdd/YaoW24,
	author = {Rujing Yao and
                  Ou Wu},
	title = {A Taxonomy for Learning with Perturbation and Algorithms},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {129:1--129:38},
	year = {2024},
	url = {https://doi.org/10.1145/3644391},
	doi = {10.1145/3644391},
	timestamp = {Fri, 17 May 2024 21:40:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YaoW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.}
}


@article{DBLP:journals/tkdd/Han24,
	author = {Yuehui Han},
	title = {Generation-based Multi-view Contrast for Self-supervised Graph Representation
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {130:1--130:17},
	year = {2024},
	url = {https://doi.org/10.1145/3645095},
	doi = {10.1145/3645095},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Han24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph contrastive learning has made remarkable achievements in the self-supervised representation learning of graph-structured data. By employing perturbation function (i.e., perturbation on the nodes or edges of graph), most graph contrastive learning methods construct contrastive samples on the original graph. However, the perturbation-based data augmentation methods randomly change the inherent information (e.g., attributes or structures) of the graph. Therefore, after nodes embedding on the perturbed graph, we cannot guarantee the validity of the contrastive samples as well as the learned performance of graph contrastive learning. To this end, in this article, we propose a novel generation-based multi-view contrastive learning framework (GMVC) for self-supervised graph representation learning, which generates the contrastive samples based on our generator rather than perturbation function. Specifically, after nodes embedding on the original graph we first employ random walk in the neighborhood to develop multiple relevant node sequences for each anchor node. We then utilize the transformer to generate the representations of relevant contrastive samples of anchor node based on the features and structures of the sampled node sequences. Finally, by maximizing the consistency between the anchor view and the generated views, we force the model to effectively encode graph information into nodes embeddings. We perform extensive experiments of node classification and link prediction tasks on eight benchmark datasets, which verify the effectiveness of our generation-based multi-view graph contrastive learning method.}
}


@article{DBLP:journals/tkdd/SinghB24,
	author = {Kuldeep Singh and
                  Bhaskar Biswas},
	title = {Mining Top-k High On-shelf Utility Itemsets Using Novel Threshold
                  Raising Strategies},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {131:1--131:23},
	year = {2024},
	url = {https://doi.org/10.1145/3645115},
	doi = {10.1145/3645115},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SinghB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High utility itemsets (HUIs) mining is an emerging area of data mining which discovers sets of items generating a high profit from transactional datasets. In recent years, several algorithms have been proposed for this task. However, most of them do not consider the on-shelf time period of items and negative utility of items. High on-shelf utility itemset (HOUIs) mining is more difficult than traditional HUIs mining because it deals with on-shelf-based time period and negative utility of items. Moreover, most algorithms need minimum utility threshold (min_util) to find rules. However, specifying the appropriate min_util threshold is a difficult problem for users. A smaller min_util threshold may generate too many rules and a higher one may generate a few rules, which can degrade performance. To address these issues, a novel top-k HOUIs mining algorithm named TKOS (Top-K high On-Shelf utility itemsets miner) is proposed which considers on-shelf time period and negative utility. TKOS presents a novel branch and bound-based strategy to raise the internal min_util threshold efficiently. It also presents two pruning strategies to speed up the mining process. In order to reduce the dataset scanning cost, we utilize transaction merging and dataset projection techniques. Extensive experiments have been conducted on real and synthetic datasets having various characteristics. Experimental results show that the proposed algorithm outperforms the state-of-the-art algorithms. The proposed algorithm is up to 42 times faster and uses up-to 19 times less memory compared to the state-of-the-art KOSHU. Moreover, the proposed algorithm has excellent scalability in terms of time periods and the number of transactions.}
}


@article{DBLP:journals/tkdd/ToraoPingiNB24,
	author = {Sharon Torao{-}Pingi and
                  Richi Nayak and
                  Md. Abul Bashar},
	title = {Conditional Generative Adversarial Network for Early Classification
                  of Longitudinal Datasets Using an Imputation Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {132:1--132:25},
	year = {2024},
	url = {https://doi.org/10.1145/3644821},
	doi = {10.1145/3644821},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ToraoPingiNB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Early classification of longitudinal data remains an active area of research today. The complexity of these datasets and the high rates of missing data caused by irregular sampling present data-level challenges for the Early Longitudinal Data Classification (ELDC) problem. Coupled with the algorithmic challenge of optimising the opposing objectives of early classification (i.e., earliness and accuracy), ELDC becomes a non-trivial task. Inspired by the generative power and utility of the Generative Adversarial Network (GAN), we propose a novel context-conditional, longitudinal early classifier GAN (LEC-GAN). This model utilises informative missingness, static features and earlier observations to improve the ELDC objective. It achieves this by incorporating ELDC as an auxiliary task within an imputation optimization process. Our experiments on several datasets demonstrate that LEC-GAN outperforms all relevant baselines in terms of F1 scores while increasing the earliness of prediction.}
}


@article{DBLP:journals/tkdd/DornaikaIB24,
	author = {Fadi Dornaika and
                  Zoulfikar Ibrahim and
                  Alireza Bosaghzadeh},
	title = {Scalable and Inductive Semi-supervised Classifier with Sample Weighting
                  Based on Graph Topology},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {5},
	pages = {133:1--133:18},
	year = {2024},
	url = {https://doi.org/10.1145/3643645},
	doi = {10.1145/3643645},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DornaikaIB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, graph-based semi-supervised learning (GSSL) has garnered significant interest in the realms of machine learning and pattern recognition. Although some of the proposed methods have made some progress, there are still some shortcomings that need to be overcome. There are three main limitations. First, the graphs used in these approaches are usually predefined regardless of the task at hand. Second, due to the use of graphs, almost all approaches are unable to process and consider data with a very large number of unlabeled samples. Thirdly, the imbalance of the topology of the samples is very often not taken into account. In particular, processing large datasets with GSSL might pose challenges in terms of computational resource feasibility. In this article, we present a scalable and inductive GSSL method. We broaden the scope of the graph topology imbalance paradigm to extensive databases. Second, we employ the calculated weights of the labeled sample for the label-matching term in the global objective function. This leads to a unified, scalable, semi-supervised learning model that allows simultaneous labeling of unlabeled data, projection of the feature space onto the labeling space, along with the graph matrix of anchors. In the proposed scheme, the integration of labels and features from anchors is applied for the adaptive construction of the anchor graph. Experimental results were performed on four large databases: NORB, RCV1, Covtype, and MNIST. These experiments demonstrate that the proposed method exhibits superior performance when compared to existing scalable semi-supervised learning models.}
}


@article{DBLP:journals/tkdd/YangWZJY24,
	author = {Yang Yang and
                  Feifei Wang and
                  Enqiang Zhu and
                  Fei Jiang and
                  Wen Yao},
	title = {Social Behavior Analysis in Exclusive Enterprise Social Networks by
                  FastHAND},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {134:1--134:32},
	year = {2024},
	url = {https://doi.org/10.1145/3646552},
	doi = {10.1145/3646552},
	timestamp = {Thu, 14 Nov 2024 13:41:51 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangWZJY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an emerging trend in the Chinese automobile industries that automakers are introducing exclusive enterprise social networks (EESNs) to expand sales and provide after-sale services. The traditional online social networks (OSNs) and enterprise social networks (ESNs), such as X (formerly known as Twitter) and Yammer, are ingeniously designed to facilitate unregulated communications among equal individuals. However, users in EESNs are naturally social stratified, consisting of both enterprise staffs and customers. In addition, the motivation to operate EESNs can be quite complicated, including providing customer services and facilitating communication among enterprise staffs. As a result, the social behaviors in EESNs can be quite different from those in OSNs and ESNs. In this work, we aim to analyze the social behaviors in EESNs. We consider the Chinese car manufacturer NIO as a typical example of EESNs and provide the following contributions. First, we formulate the social behavior analysis in EESNs as a link prediction problem in heterogeneous social networks. Second, to analyze this link prediction problem, we derive plentiful user features and build multiple meta-path graphs for EESNs. Third, we develop a novel Fast (H)eterogeneous graph (A)ttention (N)etwork algorithm for (D)irected graphs (FastHAND) to predict directed social links among users in EESNs. This algorithm introduces feature group attention at the node-level and uses an edge sampling algorithm over directed meta-path graphs to reduce the computation cost. By conducting various experiments on the NIO community data, we demonstrate the predictive power of our proposed FastHAND method. The experimental results also verify our intuitions about social affinity propagation in EESNs.}
}


@article{DBLP:journals/tkdd/ChenCGLPS24,
	author = {Huiping Chen and
                  Alessio Conte and
                  Roberto Grossi and
                  Grigorios Loukides and
                  Solon P. Pissis and
                  Michelle Sweering},
	title = {On Breaking Truss-based and Core-based Communities},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {135:1--135:43},
	year = {2024},
	url = {https://doi.org/10.1145/3644077},
	doi = {10.1145/3644077},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenCGLPS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce the general problem of identifying a smallest edge subset of a given graph whose deletion makes the graph community-free. We consider this problem under two community notions that have attracted significant attention: k-truss and k-core. We also introduce a problem variant where the identified subset contains edges incident to a given set of nodes and ensures that these nodes are not contained in any community: k-truss or k-core, in our case. These problems are directly applicable in social networks: The identified edges can be hidden by users or sanitized from the output graph; or in communication networks: the identified edges correspond to vital network connections. We present a series of theoretical and practical results. On the theoretical side, we show through non-trivial reductions that the problems we introduce are NP-hard and, in fact, hard to approximate. For the k-truss-based problems, we also show exact exponential-time algorithms, as well as a non-trivial lower bound on the size of an optimal solution. On the practical side, we develop a series of heuristics that are sped up by efficient data structures that we propose for updating the truss or core decomposition under edge deletions. In addition, we develop an algorithm to compute the lower bound. Extensive experiments on 11 real-world and synthetic graphs show that our heuristics are effective, outperforming natural baselines, and also efficient (up to two orders of magnitude faster than a natural baseline), thanks to our data structures. Furthermore, we present a case study on a co-authorship network and experiments showing that the removal of edges identified by our heuristics does not substantially affect the clustering structure of the input graph. This work extends a KDD 2021 paper, providing new theoretical results as well as introducing core-based problems and algorithms.}
}


@article{DBLP:journals/tkdd/LiZYLLL24,
	author = {Xuefei Li and
                  Huiwei Zhou and
                  Weihong Yao and
                  Wenchu Li and
                  Baojie Liu and
                  Yingyu Lin},
	title = {Intricate Spatiotemporal Dependency Learning for Temporal Knowledge
                  Graph Reasoning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {136:1--136:19},
	year = {2024},
	url = {https://doi.org/10.1145/3648366},
	doi = {10.1145/3648366},
	timestamp = {Tue, 14 May 2024 19:49:47 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZYLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph (KG) reasoning has been an interesting topic in recent decades. Most current researches focus on predicting the missing facts for incomplete KG. Nevertheless, Temporal KG (TKG) reasoning, which is to forecast future facts, still faces with a dilemma due to the complex interactions between entities over time. This article proposes a novel intricate Spatiotemporal Dependency learning Network (STDN) based on Graph Convolutional Network (GCN) to capture the underlying correlations of an entity at different timestamps. Specifically, we first learn an adaptive adjacency matrix to depict the direct dependencies from the temporally adjacent facts of an entity, obtaining its previous context embedding. Then, a Spatiotemporal feature Encoding GCN (STE-GCN) is proposed to capture the latent spatiotemporal dependencies of the entity, getting the spatiotemporal embedding. Finally, a time gate unit is used to integrate the previous context embedding and the spatiotemporal embedding at the current timestamp to update the entity evolutional embedding for predicting future facts. STDN could generate the more expressive embeddings for capturing the intricate spatiotemporal dependencies in TKG. Extensive experiments on WIKI, ICEWS14, and ICEWS18 datasets prove our STDN has the advantage over state-of-the-art baselines for the temporal reasoning task.}
}


@article{DBLP:journals/tkdd/ZhengJ24,
	author = {Yimei Zheng and
                  Caiyan Jia},
	title = {ProtoMGAE: Prototype-Aware Masked Graph Auto-Encoder for Graph Representation
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {137:1--137:22},
	year = {2024},
	url = {https://doi.org/10.1145/3649143},
	doi = {10.1145/3649143},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhengJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph self-supervised representation learning has gained considerable attention and demonstrated remarkable efficacy in extracting meaningful representations from graphs, particularly in the absence of labeled data. Two representative methods in this domain are graph auto-encoding and graph contrastive learning. However, the former methods primarily focus on global structures, potentially overlooking some fine-grained information during reconstruction. The latter methods emphasize node similarity across correlated views in the embedding space, potentially neglecting the inherent global graph information in the original input space. Moreover, handling incomplete graphs in real-world scenarios, where original features are unavailable for certain nodes, poses challenges for both types of methods. To alleviate these limitations, we integrate masked graph auto-encoding and prototype-aware graph contrastive learning into a unified model to learn node representations in graphs. In our method, we begin by masking a portion of node features and utilize a specific decoding strategy to reconstruct the masked information. This process facilitates the recovery of graphs from a global or macro level and enables handling incomplete graphs easily. Moreover, we treat the masked graph and the original one as a pair of contrasting views, enforcing the alignment and uniformity between their corresponding node representations at a local or micro level. Last, to capture cluster structures from a meso level and learn more discriminative representations, we introduce a prototype-aware clustering consistency loss that is jointly optimized with the preceding two complementary objectives. Extensive experiments conducted on several datasets demonstrate that the proposed method achieves significantly better or competitive performance on downstream tasks, especially for graph clustering, compared with the state-of-the-art methods, showcasing its superiority in enhancing graph representation learning.}
}


@article{DBLP:journals/tkdd/ChenRPTWYKDA24,
	author = {April Chen and
                  Ryan A. Rossi and
                  Namyong Park and
                  Puja Trivedi and
                  Yu Wang and
                  Tong Yu and
                  Sungchul Kim and
                  Franck Dernoncourt and
                  Nesreen K. Ahmed},
	title = {Fairness-Aware Graph Neural Networks: {A} Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {138:1--138:23},
	year = {2024},
	url = {https://doi.org/10.1145/3649142},
	doi = {10.1145/3649142},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenRPTWYKDA24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.}
}


@article{DBLP:journals/tkdd/ZhangHLZ24,
	author = {Acong Zhang and
                  Jincheng Huang and
                  Ping Li and
                  Kai Zhang},
	title = {Building Shortcuts between Distant Nodes with Biaffine Mapping for
                  Graph Convolutional Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {139:1--139:21},
	year = {2024},
	url = {https://doi.org/10.1145/3650113},
	doi = {10.1145/3650113},
	timestamp = {Thu, 23 Jan 2025 15:31:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangHLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple recent studies show a paradox in graph convolutional networks (GCNs)—that is, shallow architectures limit the capability of learning information from high-order neighbors, whereas deep architectures suffer from over-smoothing or over-squashing. To enjoy the simplicity of shallow architectures and overcome their limits of neighborhood extension, in this work we introduce a biaffine technique to improve the expressiveness of GCNs with a shallow architecture. The core design of our method is to learn direct dependency on long-distance neighbors for nodes, with which only 1-hop message passing is capable of capturing rich information for node representation. Besides, we propose a multi-view contrastive learning method to exploit the representations learned from long-distance dependencies. Extensive experiments on nine graph benchmark datasets suggest that the shallow biaffine graph convolutional networks (BAGCN) significantly outperform state-of-the-art GCNs (with deep or shallow architectures) on semi-supervised node classification. We further verify the effectiveness of biaffine design in node representation learning and the performance consistency on different sizes of training data.}
}


@article{DBLP:journals/tkdd/ChenS24,
	author = {Zhe Chen and
                  Aixin Sun},
	title = {{DP-GCN:} Node Classification by Connectivity and Local Topology Structure
                  on Real-World Network},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {140:1--140:20},
	year = {2024},
	url = {https://doi.org/10.1145/3649460},
	doi = {10.1145/3649460},
	timestamp = {Sat, 14 Dec 2024 21:39:13 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node classification is to predict the class label of a node by analyzing its properties and interactions in a network. We note that many existing solutions for graph-based node classification only consider node connectivity but not the node’s local topology structure. However, nodes residing in different parts of a real-world network may share similar local topology structures. For example, local topology structures in a payment network may reveal sellers’ business roles (e.g., supplier or retailer). To model both connectivity and local topology structure for better node classification performance, we present DP-GCN, a dual-path graph convolution network. DP-GCN consists of three main modules: (i) a C-GCN module to capture the connectivity relationships between nodes, (ii) a T-GCN module to capture the topology structure similarity among nodes, and (iii) a multi-head self-attention module to align both properties. We evaluate DP-GCN on seven benchmark datasets against diverse baselines to demonstrate its effectiveness. We also provide a case study of running DP-GCN on three large-scale payment networks from PayPal, a leading payment service provider, for risky seller detection. Experimental results show DP-GCN’s effectiveness and practicability in large-scale settings. PayPal’s internal testing also shows DP-GCN’s effectiveness in defending against real risks from transaction networks.}
}


@article{DBLP:journals/tkdd/AliABKFK24,
	author = {Sarwan Ali and
                  Muhammad Ahmad and
                  Maham Anwer Beg and
                  Imdad Ullah Khan and
                  Safiullah Faizullah and
                  Muhammad Asad Khan},
	title = {SsAG: Summarization and Sparsification of Attributed Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {141:1--141:22},
	year = {2024},
	url = {https://doi.org/10.1145/3651619},
	doi = {10.1145/3651619},
	timestamp = {Wed, 26 Feb 2025 21:07:18 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/AliABKFK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph summarization has become integral for managing and analyzing large-scale graphs in diverse real-world applications, including social networks, biological networks, and communication networks. Existing methods for graph summarization often face challenges, being either computationally expensive, limiting their applicability to large graphs, or lacking the incorporation of node attributes. In response, we introduce SsAG, an efficient and scalable lossy graph summarization method designed to preserve the essential structure of the original graph. SsAG computes a sparse representation (summary) of the input graph, accommodating graphs with node attributes. The summary is structured as a graph on supernodes (subsets of vertices of G), where weighted superedges connect pairs of supernodes. The methodology focuses on constructing a summary graph with k supernodes, aiming to minimize the reconstruction error (the difference between the original graph and the graph reconstructed from the summary) while maximizing homogeneity with respect to the node attributes. The construction process involves iteratively merging pairs of nodes. To enhance computational efficiency, we derive a closed-form expression for efficiently computing the reconstruction error (RE) after merging a pair, enabling constant-time approximation of this score. We assign a weight to each supernode, quantifying their contribution to the score of pairs, and utilize a weighted sampling strategy to select the best pair for merging. Notably, a logarithmic-sized sample achieves a summary comparable in quality based on various measures. Additionally, we propose a sparsification step for the constructed summary, aiming to reduce storage costs to a specified target size with a marginal increase in RE. Empirical evaluations across diverse real-world graphs demonstrate that SsAG exhibits superior speed, being up to 17 × faster, while generating summaries of comparable quality. This work represents a significant advancement in the field, addressing computational challenges and showcasing the effectiveness of SsAG in graph summarization.}
}


@article{DBLP:journals/tkdd/SongYGSJW24,
	author = {Derun Song and
                  Enneng Yang and
                  Guibing Guo and
                  Li Shen and
                  Linying Jiang and
                  Xingwei Wang},
	title = {Multi-Scenario and Multi-Task Aware Feature Interaction for Recommendation
                  System},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {142:1--142:20},
	year = {2024},
	url = {https://doi.org/10.1145/3651312},
	doi = {10.1145/3651312},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SongYGSJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-scenario and multi-task recommendation can use various feedback behaviors of users in different scenarios to learn users’ preferences and then make recommendations, which has attracted attention. However, the existing work ignores feature interactions and the fact that a pair of feature interactions will have differing levels of importance under different scenario-task pairs, leading to sub-optimal user preference learning. In this article, we propose a Multi-scenario and Multi-task aware Feature Interaction model, dubbed MMFI, to explicitly model feature interactions and learn the importance of feature interaction pairs in different scenarios and tasks. Specifically, MMFI first incorporates a pairwise feature interaction unit and a scenario-task interaction unit to effectively capture the interaction of feature pairs and scenario-task pairs. Then MMFI designs a scenario-task aware attention layer for learning the importance of feature interactions from coarse-grained to fine-grained, improving the model’s performance on various scenario-task pairs. More specifically, this attention layer consists of three modules: a fully shared bottom module, a partially shared middle module, and a specific output module. Finally, MMFI adapts two sparsity-aware functions to remove some useless feature interactions. Extensive experiments on two public datasets demonstrate the superiority of the proposed method over the existing multi-task recommendation, multi-scenario recommendation, and multi-scenario & multi-task recommendation models.}
}


@article{DBLP:journals/tkdd/ConnorV24,
	author = {Richard Connor and
                  Lucia Vadicamo},
	title = {\emph{nSimplex} \emph{Zen}: {A} Novel Dimensionality Reduction for
                  Euclidean and Hilbert Spaces},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {143:1--143:44},
	year = {2024},
	url = {https://doi.org/10.1145/3647642},
	doi = {10.1145/3647642},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ConnorV24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological. Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen, a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of distances, rather than coordinates, allows the technique to be applied to both Euclidean and other Hilbert spaces, including those governed by Cosine, Jensen–Shannon and Quadratic Form distances. We show that in almost all cases, due to geometric properties of high-dimensional spaces, our new technique gives better properties than others, especially with reduction to very low dimensions.}
}


@article{DBLP:journals/tkdd/JiSFCRL24,
	author = {Taoran Ji and
                  Nathan Self and
                  Kaiqun Fu and
                  Zhiqian Chen and
                  Naren Ramakrishnan and
                  Chang{-}Tien Lu},
	title = {Citation Forecasting with Multi-Context Attention-Aided Dependency
                  Modeling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {144:1--144:23},
	year = {2024},
	url = {https://doi.org/10.1145/3649140},
	doi = {10.1145/3649140},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiSFCRL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Forecasting citations of scientific patents and publications is a crucial task for understanding the evolution and development of technological domains and for foresight into emerging technologies. By construing citations as a time series, the task can be cast into the domain of temporal point processes. Most existing work on forecasting with temporal point processes, both conventional and neural network-based, only performs single-step forecasting. In citation forecasting, however, the more salient goal is n-step forecasting: predicting the arrival of the next n citations. In this article, we propose Dynamic Multi-Context Attention Networks (DMA-Nets), a novel deep learning sequence-to-sequence (Seq2Seq) model with a novel hierarchical dynamic attention mechanism for long-term citation forecasting. Extensive experiments on two real-world datasets demonstrate that the proposed model learns better representations of conditional dependencies over historical sequences compared to state-of-the-art counterparts and thus achieves significant performance for citation predictions.}
}


@article{DBLP:journals/tkdd/ZhouLSC24,
	author = {Houquan Zhou and
                  Shenghua Liu and
                  Huawei Shen and
                  Xueqi Cheng},
	title = {Node Embedding Preserving Graph Summarization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {145:1--145:19},
	year = {2024},
	url = {https://doi.org/10.1145/3649505},
	doi = {10.1145/3649505},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhouLSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph summarization is a useful tool for analyzing large-scale graphs. Some works tried to preserve original node embeddings encoding rich structural information of nodes on the summary graph. However, their algorithms are designed heuristically and not theoretically guaranteed. In this article, we theoretically study the problem of preserving node embeddings on summary graph. We prove that three matrix-factorization-based node embedding methods of the original graph can be approximated by that of the summary graph, and we propose a novel graph summarization method, named HCSumm, based on this analysis. Extensive experiments are performed on real-world datasets to evaluate the effectiveness of our proposed method. The experimental results show that our method outperforms the state-of-the-art methods in preserving node embeddings.}
}


@article{DBLP:journals/tkdd/TheocharidisKTSL24,
	author = {Konstantinos Theocharidis and
                  Panagiotis Karras and
                  Manolis Terrovitis and
                  Spiros Skiadopoulos and
                  Hady W. Lauw},
	title = {Adaptive Content-Aware Influence Maximization via Online Learning
                  to Rank},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {146:1--146:35},
	year = {2024},
	url = {https://doi.org/10.1145/3651987},
	doi = {10.1145/3651987},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/TheocharidisKTSL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How can we adapt the composition of a post over a series of rounds to make it more appealing in a social network? Techniques that progressively learn how to make a fixed post more influential over rounds have been studied in the context of the Influence Maximization (IM) problem, which seeks a set of seed users that maximize a post’s influence. However, there is no work on progressively learning how a post’s features affect its influence. In this article, we propose and study the problem of Adaptive Content-Aware Influence Maximization (ACAIM), which calls to find k features to form a post in each round so as to maximize the cumulative influence of those posts over all rounds. We solve ACAIM by applying, for the first time, an Online Learning to Rank (OLR) framework for IM purposes. We introduce the CATRID propagation model, which expresses how posts disseminate in a social network using click probabilities and post visibility criteria and develop a simulator that runs CATRID via a training-testing scheme based on real posts of the VK social network, so as to realistically represent the learning environment. We deploy three learners that solve ACAIM in an online (real-time) manner. We experimentally prove the practical suitability of our solutions via exhaustive experiments on multiple brands (operating as different case studies) and several VK datasets; the best learner is evaluated on 45 separate case studies yielding convincing results.}
}


@article{DBLP:journals/tkdd/PaterakisFCCT24,
	author = {George Paterakis and
                  Stefanos Fafalios and
                  Paulos Charonyktakis and
                  Vassilis Christophides and
                  Ioannis Tsamardinos},
	title = {Do We Really Need Imputation in AutoML Predictive Modeling?},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {147:1--147:64},
	year = {2024},
	url = {https://doi.org/10.1145/3643643},
	doi = {10.1145/3643643},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/PaterakisFCCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Numerous real-world data contain missing values, while in contrast, most Machine Learning (ML) algorithms assume complete datasets. For this reason, several imputation algorithms have been proposed to predict and fill in the missing values. Given the advances in predictive modeling algorithms tuned in an Automated Machine Learning context (AutoML) setting, a question that naturally arises is to what extent sophisticated imputation algorithms (e.g., Neural Network based) are really needed, or we can obtain a descent performance using simple methods like Mean/Mode (MM). In this article, we experimentally compare six state-of-the-art representatives of different imputation algorithmic families from an AutoML predictive modeling perspective, including a feature selection step and combined algorithm and hyper-parameter selection. We used a commercial AutoML tool for our experiments, in which we included the selected imputation methods. Experiments ran on 25 binary classification real-world incomplete datasets with missing values and 10 binary classification complete datasets in which synthetic missing values are introduced according to different missingness mechanisms, at varying missing frequencies. The main conclusion drawn from our experiments is that the best method on average is the Denoise AutoEncoder on real-world datasets and the MissForest in simulated datasets, followed closely by MM. In addition, binary indicator variables encoding missingness patterns actually improve predictive performance, on average. Last, although there are cases where Neural-Network-based imputation significantly improves predictive performance, this comes at a great computational cost and requires measuring all feature values to impute new samples.}
}


@article{DBLP:journals/tkdd/ZhangCCLC24,
	author = {Chi Zhang and
                  Linhao Cai and
                  Meng Chen and
                  Xiucheng Li and
                  Gao Cong},
	title = {DeepMeshCity: {A} Deep Learning Model for Urban Grid Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {148:1--148:26},
	year = {2024},
	url = {https://doi.org/10.1145/3652859},
	doi = {10.1145/3652859},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangCCLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban grid prediction can be applied to many classic spatial-temporal prediction tasks such as air quality prediction, crowd density prediction, and traffic flow prediction, which is of great importance to smart city building. In light of its practical values, many methods have been developed for it and have achieved promising results. Despite their successes, two main challenges remain open: (a) how to well capture the global dependencies and (b) how to effectively model the multi-scale spatial-temporal correlations? To address these two challenges, we propose a novel method—DeepMeshCity, with a carefully-designed Self-Attention Citywide Grid Learner (SA-CGL) block comprising of a self-attention unit and a Citywide Grid Learner (CGL) unit. The self-attention block aims to capture the global spatial dependencies, and the CGL unit is responsible for learning the spatial-temporal correlations. In particular, a multi-scale memory unit is proposed to traverse all stacked SA-CGL blocks along a zigzag path to capture the multi-scale spatial-temporal correlations. In addition, we propose to initialize the single-scale memory units and the multi-scale memory units by using the corresponding ones in the previous fragment stack, so as to speed up the model training. We evaluate the performance of our proposed model by comparing with several state-of-the-art methods on four real-world datasets for two urban grid prediction applications. The experimental results verify the superiority of DeepMeshCity over the existing ones. The code is available at https://github.com/ILoveStudying/DeepMeshCity.}
}


@article{DBLP:journals/tkdd/YangHZWJ24,
	author = {Hongwei Yang and
                  Hui He and
                  Weizhe Zhang and
                  Yan Wang and
                  Lin Jing},
	title = {Multi-Source and Multi-modal Deep Network Embedding for Cross-Network
                  Node Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {149:1--149:26},
	year = {2024},
	url = {https://doi.org/10.1145/3653304},
	doi = {10.1145/3653304},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangHZWJ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, to address the issue of networked data sparsity in node classification tasks, cross-network node classification (CNNC) leverages the richer information from a source network to enhance the performance of node classification in the target network, which typically has sparser information. However, in real-world applications, labeled nodes may be collected from multiple sources with multiple modalities (e.g., text, vision, and video). Naive application of single-source and single-modal CNNC methods may result in sub-optimal solutions. To this end, in this article, we propose a model called Multi-source and Multi-modal Cross-network Deep Network Embedding (M2CDNE) for cross-network node classification. In M2CDNE, we propose a deep multi-modal network embedding approach that combines the extracted deep multi-modal features to make the node vector representations network invariant. In addition, we apply dynamic adversarial adaptation to assess the significance of marginal and conditional probability distributions between each source and target network to make node vector representations label discriminative. Furthermore, we devise to classify nodes in the target network through the related source classifier and aggregate different predictions utilizing respective network weights, corresponding to the discrepancy between each source and target network. Extensive experiments performed on real-world datasets demonstrate that the proposed M2CDNE significantly outperforms the state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/RongLDL24,
	author = {Can Rong and
                  Zhicheng Liu and
                  Jingtao Ding and
                  Yong Li},
	title = {Learning to Generate Temporal Origin-destination Flow Based-on Urban
                  Regional Features and Traffic Information},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {150:1--150:17},
	year = {2024},
	url = {https://doi.org/10.1145/3649141},
	doi = {10.1145/3649141},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RongLDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Origin-destination (OD) flow contains population mobility information between every two regions in the city, which is of great value in urban planning and transportation management. Nevertheless, the collection of OD flow data is extremely difficult due to the hindrance of privacy issues and collection costs. Significant efforts have been made to generate OD flow based on urban regional features, e.g., demographics, land use, and so on, since spatial heterogeneity of urban function is the primary cause that drives people to move from one place to another. On the other hand, people travel through various routes between OD, which will have effects on urban traffic, e.g., road travel speed and time. These effects of OD flows reveal the fine-grained spatiotemporal patterns of population mobility. Few works have explored the effectiveness of incorporating urban traffic information into OD generation. To bridge this gap, we propose to generate real-world daily temporal OD flows enhanced by urban traffic information in this paper. Our model consists of two modules: Urban2OD and OD2Traffic. In the Urban2OD module, we devise a spatiotemporal graph neural network to model the complex dependencies between daily temporal OD flows and regional features. In the OD2Traffic module, we introduce an attention-based neural network to predict urban traffic based on OD flow from the Urban2OD module. Then, by utilizing gradient backpropagation, these two modules are able to enhance each other to generate high-quality OD flow data. Extensive experiments conducted on real-world datasets demonstrate the superiority of our proposed model over the state of the art.}
}


@article{DBLP:journals/tkdd/GuFLYL24,
	author = {Zhibin Gu and
                  Songhe Feng and
                  Zhendong Li and
                  Jiazheng Yuan and
                  Jun Liu},
	title = {{NOODLE:} Joint Cross-View Discrepancy Discovery and High-Order Correlation
                  Detection for Multi-View Subspace Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {151:1--151:23},
	year = {2024},
	url = {https://doi.org/10.1145/3653305},
	doi = {10.1145/3653305},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GuFLYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from the effective exploration of the valuable topological pair-wise relationship of data points across multiple views, multi-view subspace clustering (MVSC) has received increasing attention in recent years. However, we observe that existing MVSC approaches still suffer from two limitations that need to be further improved to enhance the clustering effectiveness. Firstly, previous MVSC approaches mainly prioritize extracting multi-view consistency, often neglecting the cross-view discrepancy that may arise from noise, outliers, and view-inherent properties. Secondly, existing techniques are constrained by their reliance on pair-wise sample correlation and pair-wise view correlation, failing to capture the high-order correlations that are enclosed within multiple views. To address these issues, we propose a novel MVSC framework via joiNt crOss-view discrepancy discOvery anD high-order correLation dEtection (NOODLE), seeking an informative target subspace representation compatible across multiple features to facilitate the downstream clustering task. Specifically, we first exploit the self-representation mechanism to learn multiple view-specific affinity matrices, which are further decomposed into cohesive factors and incongruous factors to fit the multi-view consistency and discrepancy, respectively. Additionally, an explicit cross-view sparse regularization is applied to incoherent parts, ensuring the consistency and discrepancy to be precisely separated from the initial subspace representations. Meanwhile, the multiple cohesive parts are stacked into a three-dimensional tensor associated with a tensor-Singular Value Decomposition (t-SVD) based weighted tensor nuclear norm constraint, enabling effective detection of the high-order correlations implicit in multi-view data. Our proposed method outperforms state-of-the-art methods for multi-view clustering on six benchmark datasets, demonstrating its effectiveness.}
}


@article{DBLP:journals/tkdd/WangXTLYL24,
	author = {Yuhan Wang and
                  Qing Xie and
                  Mengzi Tang and
                  Lin Li and
                  Jingling Yuan and
                  Yongjian Liu},
	title = {A Dual Perspective Framework of Knowledge-correlation for Cross-domain
                  Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {152:1--152:28},
	year = {2024},
	url = {https://doi.org/10.1145/3652520},
	doi = {10.1145/3652520},
	timestamp = {Sun, 19 Jan 2025 14:58:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangXTLYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender System provides users with online services in a personalized way. The performance of traditional recommender systems may deteriorate because of problems such as cold-start and data sparsity. Cross-domain Recommendation System utilizes the richer information from auxiliary domains to guide the task in the target domain. However, direct knowledge transfer may lead to a negative impact due to data heterogeneity and feature mismatch between domains. In this article, we innovatively explore the cross-domain correlation from the perspectives of content semanticity and structural connectivity to fully exploit the information of Knowledge Graph. First, we adopt domain adaptation that automatically extracts transferable features to capture cross-domain semantic relations. Second, we devise a knowledge-aware graph neural network to explicitly model the high-order connectivity across domains. Third, we develop feature fusion strategies to combine the advantages of semantic and structural information. By simulating the cold-start scenario on two real-world datasets, the experimental results show that our proposed method has superior performance in accuracy and diversity compared with the SOTA methods. It demonstrates that our method can accurately predict users’ expressed preferences while exploring their potential diverse interests.}
}


@article{DBLP:journals/tkdd/ZhaoMWJKC24,
	author = {Chen Zhao and
                  Feng Mi and
                  Xintao Wu and
                  Kai Jiang and
                  Latifur Khan and
                  Feng Chen},
	title = {Dynamic Environment Responsive Online Meta-Learning with Fairness
                  Awareness},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {153:1--153:23},
	year = {2024},
	url = {https://doi.org/10.1145/3648684},
	doi = {10.1145/3648684},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoMWJKC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner’s objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it’s crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this article, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model’s primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation of various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.}
}


@article{DBLP:journals/tkdd/ZhaoLHZ24,
	author = {Hong Zhao and
                  Zhengyu Li and
                  Wenwei He and
                  Yan Zhao},
	title = {Hierarchical Convolutional Neural Network with Knowledge Complementation
                  for Long-Tailed Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {154:1--154:22},
	year = {2024},
	url = {https://doi.org/10.1145/3653717},
	doi = {10.1145/3653717},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoLHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing methods based on transfer learning leverage auxiliary information to help tail generalization and improve the performance of the tail classes. However, they cannot fully exploit the relationships between auxiliary information and tail classes and bring irrelevant knowledge to the tail classes. To solve this problem, we propose a hierarchical CNN with knowledge complementation, which regards hierarchical relationships as auxiliary information and transfers relevant knowledge to tail classes. First, we integrate semantics and clustering relationships as hierarchical knowledge into the CNN to guide feature learning. Then, we design a complementary strategy to jointly exploit the two types of knowledge, where semantic knowledge acts as a prior dependence and clustering knowledge reduces the negative information caused by excessive semantic dependence (i.e., semantic gaps). In this way, the CNN facilitates the utilization of the two complementary hierarchical relationships and transfers useful knowledge to tail data to improve long-tailed classification accuracy. Experimental results on public benchmarks show that the proposed model outperforms existing methods. In particular, our model improves accuracy by 3.46% compared with the second-best method on the long-tailed tieredImageNet dataset.}
}


@article{DBLP:journals/tkdd/SahebiYZF24,
	author = {Sherry Sahebi and
                  Mengfan Yao and
                  Siqian Zhao and
                  Reza Feyzi{-}Behnagh},
	title = {MoMENt: Marked Point Processes with Memory-Enhanced Neural Networks
                  for User Activity Modeling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {155:1--155:32},
	year = {2024},
	url = {https://doi.org/10.1145/3649504},
	doi = {10.1145/3649504},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SahebiYZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Marked temporal point process models (MTPPs) aim to model event sequences and event markers (associated features) in continuous time. These models have been applied to various application domains where capturing event dynamics in continuous time is beneficial, such as education systems, social networks, and recommender systems. However, current MTPPs suffer from two major limitations, i.e., inefficient representation of event dynamic’s influence on marker distribution and losing fine-grained representation of historical marker distributions in the modeling. Motivated by these limitations, we propose a novel model called Marked Point Processes with Memory-Enhanced Neural Networks (MoMENt) that can capture the bidirectional interrelations between markers and event dynamics while providing fine-grained marker representations. Specifically, MoMENt is constructed of two concurrent networks: Recurrent Activity Updater (RAU) to capture model event dynamics and Memory-Enhanced Marker Updater (MEMU) to represent markers. Both RAU and MEMU components are designed to update each other at every step to model the bidirectional influence of markers and event dynamics. To obtain a fine-grained representation of maker distributions, MEMU is devised with external memories that model detailed marker-level features with latent component vectors. Our extensive experiments on six real-world user interaction datasets demonstrate that MoMENt can accurately represent users’ activity dynamics, boosting time, type, and marker predictions, as well as recommendation performance up to 76.5%, 65.6%, 77.2%, and 57.7%, respectively, compared to baseline approaches. Furthermore, our case studies show the effectiveness of MoMENt in providing meaningful and fine-grained interpretations of user-system relations over time, e.g., how user choices influence their future preferences in the recommendation domain.}
}


@article{DBLP:journals/tkdd/ChoeYLBKS24,
	author = {Minyoung Choe and
                  Jaemin Yoo and
                  Geon Lee and
                  Woonsung Baek and
                  U Kang and
                  Kijung Shin},
	title = {Representative and Back-In-Time Sampling from Real-world Hypergraphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {156:1--156:48},
	year = {2024},
	url = {https://doi.org/10.1145/3653306},
	doi = {10.1145/3653306},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChoeYLBKS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs are widely used for representing pairwise interactions in complex systems. Since such real-world graphs are large and often evergrowing, sampling subgraphs is useful for various purposes, including simulation, visualization, stream processing, representation learning, and crawling. However, many complex systems consist of group interactions (e.g., collaborations of researchers and discussions on online Q&A platforms) and thus are represented more naturally and accurately by hypergraphs than by ordinary graphs. Motivated by the prevalence of large-scale hypergraphs, we study the problem of sampling from real-world hypergraphs, aiming at answering (Q1) how can we measure the goodness of sub-hypergraphs, and (Q2) how can we efficiently find a “good” sub-hypergraph. Regarding Q1, we distinguish between two goals: (a) representative sampling, which aims at capturing the characteristics of the input hypergraph, and (b) back-in-time sampling, which aims at closely approximating a past snapshot of the input time-evolving hypergraph. To evaluate the similarity of the sampled sub-hypergraph to the target (i.e., the input hypergraph or its past snapshot), we consider 10 graph-level, hyperedge-level, and node-level statistics. Regarding Q2, we first conduct a thorough analysis of various intuitive approaches using 11 real-world hypergraphs. Then, based on this analysis, we propose MiDaS and MiDaS-B, designed for representative sampling and back-in-time sampling, respectively. Regarding representative sampling, we demonstrate through extensive experiments that MiDaS, which employs a sampling bias toward high-degree nodes in hyperedge selection, is (a) Representative: finding overall the most representative samples among 15 considered approaches, (b) Fast: several orders of magnitude faster than the strongest competitors, and (c) Automatic: automatically tuning the degree of sampling bias. Regarding back-in-time sampling, we demonstrate that MiDaS-B inherits the strengths of MiDaS despite an additional challenge—the unavailability of the target (i.e., past snapshot). It effectively handles this challenge by focusing on replicating universal evolutionary patterns, rather than directly replicating the target.}
}


@article{DBLP:journals/tkdd/CuiWWL24,
	author = {Guosheng Cui and
                  Ruxin Wang and
                  Dan Wu and
                  Ye Li},
	title = {Semi-supervised Multi-view Clustering based on {NMF} with Fusion Regularization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {157:1--157:26},
	year = {2024},
	url = {https://doi.org/10.1145/3653022},
	doi = {10.1145/3653022},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CuiWWL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view clustering has attracted significant attention and application. Nonnegative matrix factorization is one popular feature of learning technology in pattern recognition. In recent years, many semi-supervised nonnegative matrix factorization algorithms were proposed by considering label information, which has achieved outstanding performance for multi-view clustering. However, most of these existing methods have either failed to consider discriminative information effectively or included too much hyper-parameters. Addressing these issues, a semi-supervised multi-view nonnegative matrix factorization with a novel fusion regularization (FRSMNMF) is developed in this article. In this work, we uniformly constrain alignment of multiple views and discriminative information among clusters with designed fusion regularization. Meanwhile, to align the multiple views effectively, two kinds of compensating matrices are used to normalize the feature scales of different views. Additionally, we preserve the geometry structure information of labeled and unlabeled samples by introducing the graph regularization simultaneously. Due to the proposed methods, two effective optimization strategies based on multiplicative update rules are designed. Experiments implemented on six real-world datasets have demonstrated the effectiveness of our FRSMNMF comparing with several state-of-the-art unsupervised and semi-supervised approaches.}
}


@article{DBLP:journals/tkdd/HanTTXZ24,
	author = {Jiadi Han and
                  Yufei Tang and
                  Qian Tao and
                  Yuhan Xia and
                  Liming Zhang},
	title = {Dual Homogeneity Hypergraph Motifs with Cross-view Contrastive Learning
                  for Multiple Social Recommendations},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {158:1--158:24},
	year = {2024},
	url = {https://doi.org/10.1145/3653976},
	doi = {10.1145/3653976},
	timestamp = {Fri, 17 May 2024 21:40:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HanTTXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social relations are often used as auxiliary information to address data sparsity and cold-start issues in social recommendations. In the real world, social relations among users are complex and diverse. Widely used graph neural networks (GNNs) can only model pairwise node relationships and are not conducive to exploring higher-order connectivity, while hypergraph provides a natural way to model high-order relations between nodes. However, recent studies show that social recommendations still face the following challenges: 1) a majority of social recommendations ignore the impact of multifaceted social relationships on user preferences; 2) the item homogeneity is often neglected, mainly referring to items with similar static attributes have similar attractiveness when exposed to users that indicating hidden links between items; and 3) directly combining the representations learned from different independent views cannot fully exploit the potential connections between different views. To address these challenges, in this article, we propose a novel method DH-HGCN++ for multiple social recommendations. Specifically, dual homogeneity (i.e., social homogeneity and item homogeneity) is introduced to mine the impact of diverse social relations on user preferences and enrich item representations. Hypergraph convolution networks with motifs are further exploited to model the high-order relations between nodes. Finally, cross-view contrastive learning is proposed as an auxiliary task to jointly optimize the DH-HGCN++. Real-world datasets are used to validate the effectiveness of the proposed model, where we use sentiment analysis to extract comment relations and employ the k-means clustering algorithm to construct the item-item correlation graph. Experiment results demonstrate that our proposed method consistently outperforms the state-of-the-art baselines on Top-N recommendations.}
}


@article{DBLP:journals/tkdd/ZhangELS24,
	author = {Wentai Zhang and
                  Haihong E and
                  Haoran Luo and
                  Mingzhi Sun},
	title = {FulBM: Fast Fully Batch Maintenance for Landmark-based 3-hop Cover
                  Labeling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {159:1--159:26},
	year = {2024},
	url = {https://doi.org/10.1145/3650035},
	doi = {10.1145/3650035},
	timestamp = {Tue, 21 Jan 2025 10:50:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangELS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Landmark-based 3-hop cover labeling is a category of approaches for shortest distance/path queries on large-scale complex networks. It pre-computes an index offline to accelerate the online distance/path query. Most real-world graphs undergo rapid changes in topology, which makes index maintenance on dynamic graphs necessary. So far, the majority of index maintenance methods can handle only one edge update (either an addition or deletion) each time. To keep up with frequently changing graphs, we research the fully batch maintenance problem for the 3-hop cover labeling, and proposed the method called FulBM. FulBM is composed of two algorithms: InsBM and DelBM, which are designed to handle batch edge insertions and deletions, respectively. This separation is motivated by the insight that batch maintenance for edge insertions are much more time-efficient and the fact that most edge updates in the real world are incremental. Both InsBM and DelBM are equipped with well-designed pruning strategies to minimize the number of vertex accesses. We have conducted comprehensive experiments on both synthetic and real-world graphs to verify the efficiency of FulBM and its variants for weighted graphs. The results show that our methods achieve 5.5× to 228× speedup compared with the state-of-the-art method.}
}


@article{DBLP:journals/tkdd/YangJTHFJZYH24,
	author = {Jingfeng Yang and
                  Hongye Jin and
                  Ruixiang Tang and
                  Xiaotian Han and
                  Qizhang Feng and
                  Haoming Jiang and
                  Shaochen Zhong and
                  Bing Yin and
                  Xia Ben Hu},
	title = {Harnessing the Power of LLMs in Practice: {A} Survey on ChatGPT and
                  Beyond},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {6},
	pages = {160:1--160:32},
	year = {2024},
	url = {https://doi.org/10.1145/3649506},
	doi = {10.1145/3649506},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangJTHFJZYH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.}
}


@article{DBLP:journals/tkdd/CuiXLFWPFZZL24,
	author = {Jingyi Cui and
                  Guangquan Xu and
                  Jian Liu and
                  Shicheng Feng and
                  Jianli Wang and
                  Hao Peng and
                  Shihui Fu and
                  Zhaohua Zheng and
                  James Xi Zheng and
                  Shaoying Liu},
	title = {{ID-SR:} Privacy-Preserving Social Recommendation Based on Infinite
                  Divisibility for Trustworthy {AI}},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {161},
	year = {2024},
	url = {https://doi.org/10.1145/3639412},
	doi = {10.1145/3639412},
	timestamp = {Mon, 26 Aug 2024 10:57:06 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CuiXLFWPFZZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems powered by artificial intelligence (AI) are widely used to improve user experience. However, AI inevitably raises privacy leakage and other security issues due to the utilization of extensive user data. Addressing these challenges can protect users’ personal information, benefit service providers, and foster service ecosystems. Presently, numerous techniques based on differential privacy have been proposed to solve this problem. However, existing solutions encounter issues such as inadequate data utilization and a tenuous trade-off between privacy protection and recommendation effectiveness. To enhance recommendation accuracy and protect users’ private data, we propose ID-SR, a novel privacy-preserving social recommendation scheme for trustworthy AI based on the infinite divisibility of Laplace distribution. We first introduce a novel recommendation method adopted in ID-SR, which is established based on matrix factorization with a newly designed social regularization term for improving recommendation effectiveness. We then propose a differential privacy-preserving scheme tailored to the above method that leverages the Laplace distribution’s characteristics to safeguard user data. Theoretical analysis and experimentation evaluation on two publicly available datasets demonstrate that our scheme achieves a superior balance between privacy protection and recommendation effectiveness, ultimately delivering an enhanced user experience.}
}


@article{DBLP:journals/tkdd/YangWLWMZLZ24,
	author = {Xihong Yang and
                  Yiqi Wang and
                  Yue Liu and
                  Yi Wen and
                  Lingyuan Meng and
                  Sihang Zhou and
                  Xinwang Liu and
                  En Zhu},
	title = {Mixed Graph Contrastive Network for Semi-supervised Node Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {162},
	year = {2024},
	url = {https://doi.org/10.1145/3641549},
	doi = {10.1145/3641549},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YangWLWMZLZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) have achieved promising performance in semi-supervised node classification in recent years. However, the problem of insufficient supervision, together with representation collapse, largely limits the performance of the GNNs in this field. To alleviate the collapse of node representations in semi-supervised scenario, we propose a novel graph contrastive learning method, termed Mixed Graph Contrastive Network (MGCN). In our method, we improve the discriminative capability of the latent embeddings by an interpolation-based augmentation strategy and a correlation reduction mechanism. Specifically, we first conduct the interpolation-based augmentation in the latent space and then force the prediction model to change linearly between samples. Second, we enable the learned network to tell apart samples across two interpolation-perturbed views through forcing the correlation matrix across views to approximate an identity matrix. By combining the two settings, we extract rich supervision information from both the abundant unlabeled nodes and the rare yet valuable labeled nodes for discriminative representation learning. Extensive experimental results on six datasets demonstrate the effectiveness and the generality of MGCN compared to the existing state-of-the-art methods. The code of MGCN is available at https://github.com/xihongyang1999/MGCN on Github.}
}


@article{DBLP:journals/tkdd/LingXZDYW24,
	author = {Zhaolong Ling and
                  Enqi Xu and
                  Peng Zhou and
                  Liang Du and
                  Kui Yu and
                  Xindong Wu},
	title = {Fair Feature Selection: {A} Causal Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {163},
	year = {2024},
	url = {https://doi.org/10.1145/3643890},
	doi = {10.1145/3643890},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LingXZDYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a fair causal feature selection algorithm, called FairCFS. Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has accuracy comparable to eight state-of-the-art feature selection algorithms while presenting more superior fairness.}
}


@article{DBLP:journals/tkdd/KoseS24,
	author = {{\"{O}}yk{\"{u}} Deniz K{\"{o}}se and
                  Yanning Shen},
	title = {FairGAT: Fairness-Aware Graph Attention Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {164},
	year = {2024},
	url = {https://doi.org/10.1145/3645096},
	doi = {10.1145/3645096},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KoseS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)–based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction.}
}


@article{DBLP:journals/tkdd/LiuZYDYW24,
	author = {Shenghao Liu and
                  Yu Zhang and
                  Lingzhi Yi and
                  Xianjun Deng and
                  Laurence T. Yang and
                  Bang Wang},
	title = {Dual-Side Adversarial Learning Based Fair Recommendation for Sensitive
                  Attribute Filtering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {165},
	year = {2024},
	url = {https://doi.org/10.1145/3648683},
	doi = {10.1145/3648683},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuZYDYW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of recommendation algorithms, researchers are paying increasing attention to fairness issues such as user discrimination in recommendations. To address these issues, existing works often filter users’ sensitive information that may cause discrimination during the process of learning user representations. However, these approaches overlook the latent relationship between items’ content attributes and users’ sensitive information. In this article, we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. First, we conduct a statistical analysis to demonstrate the latent relationship between items’ information and users’ sensitive attributes. Then, we design a dual-side adversarial learning network that simultaneously filters out users’ sensitive information on the user and item side. Additionally, we propose a new evaluation strategy that leverages the latent relationship between items’ content attributes and users’ sensitive attributes to better assess the algorithm’s ability to reduce discrimination. Our experiments on three real datasets demonstrate the superiority of our proposed algorithm over state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/YeCWLG24,
	author = {Tiandi Ye and
                  Cen Chen and
                  Yinggui Wang and
                  Xiang Li and
                  Ming Gao},
	title = {BapFL: You can Backdoor Personalized Federated Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {166},
	year = {2024},
	url = {https://doi.org/10.1145/3649316},
	doi = {10.1145/3649316},
	timestamp = {Mon, 10 Feb 2025 13:32:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YeCWLG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In federated learning (FL), malicious clients could manipulate the predictions of the trained model through backdoor attacks, posing a significant threat to the security of FL systems. Existing research primarily focuses on backdoor attacks and defenses within the generic federated learning scenario, where all clients collaborate to train a single global model. A recent study conducted by Qin et\xa0al. [24] marks the initial exploration of backdoor attacks within the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with parameter decoupling can significantly enhance robustness against backdoor attacks. However, in this article, we whistleblow that pFL methods with parameter decoupling are still vulnerable to backdoor attacks. The resistance of pFL methods with parameter decoupling is attributed to the heterogeneous classifiers between malicious clients and benign counterparts. We analyze two direct causes of the heterogeneous classifiers: (1) data heterogeneity inherently exists among clients and (2) poisoning by malicious clients further exacerbates the data heterogeneity. To address these issues, we propose a two-pronged attack method, BapFL, which comprises two simple yet effective strategies: (1) poisoning only the feature encoder while keeping the classifier fixed and (2) diversifying the classifier through noise introduction to simulate that of the benign clients. Extensive experiments on three benchmark datasets under varying conditions demonstrate the effectiveness of our proposed attack. Additionally, we evaluate the effectiveness of six widely used defense methods and find that BapFL still poses a significant threat even in the presence of the best defense, Multi-Krum. We hope to inspire further research on attack and defense strategies in pFL scenarios. The code is available at: https://github.com/BapFL/code}
}


@article{DBLP:journals/tkdd/LiWBSC24,
	author = {Feifei Li and
                  Yuanbin Wang and
                  Oya Beyan and
                  Mirjam Sch{\"{o}}neck and
                  Liliana Lourenco Caldeira},
	title = {Voxel-Wise Medical Image Generalization for Eliminating Distribution
                  Shift},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {167},
	year = {2024},
	url = {https://doi.org/10.1145/3643034},
	doi = {10.1145/3643034},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiWBSC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, the medical field is witnessing an increase in the use of machine learning techniques. Supervised learning methods adopted in classification, prediction, and segmentation tasks for medical images always experience decreased performance when the training and testing datasets do not follow the independent and identically distributed assumption. These distribution shift situations seriously influence machine learning applications’ robustness, fairness, and trustworthiness in the medical domain. Hence, in this article, we adopt the CycleGAN (generative adversarial network) method to cycle train the computed tomography data from different scanners/manufacturers. It aims to eliminate the distribution shift from diverse data terminals based on our previous work [14]. However, due to the model collapse problem and generative mechanisms of the GAN-based model, the images we generated contained serious artifacts. To remove the boundary marks and artifacts, we adopt score-based diffusion generative models to refine the images voxel-wisely. This innovative combination of two generative models enhances the quality of data providers while maintaining significant features. Meanwhile, we use five paired patients’ medical images to deal with the evaluation experiments with structural similarity index measure metrics and the segmentation model’s performance comparison. We conclude that CycleGAN can be utilized as an efficient data augmentation technique rather than a distribution-shift-eliminating method. In contrast, the denoising diffusion the denoising diffusion model is more suitable for dealing with the distribution shift problem aroused by the different terminal modules. The limitation of generative methods applied in medical images is the difficulty in obtaining large and diverse datasets that accurately capture the complexity of biological structure and variability. In our following research, we plan to assess the initial and generated datasets to explore more possibilities to overcome the above limitation. We will also incorporate the generative methods into the federated learning architecture, which can maintain their advantages and resolve the distribution shift issue on a larger scale.}
}


@article{DBLP:journals/tkdd/WenCZSXLQMT24,
	author = {Cheng Wen and
                  Yuandao Cai and
                  Bin Zhang and
                  Jie Su and
                  Zhiwu Xu and
                  Dugang Liu and
                  Shengchao Qin and
                  Zhong Ming and
                  Cong Tian},
	title = {Automatically Inspecting Thousands of Static Bug Warnings with Large
                  Language Model: How Far Are We?},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {168},
	year = {2024},
	url = {https://doi.org/10.1145/3653718},
	doi = {10.1145/3653718},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WenCZSXLQMT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task. This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness}
}


@article{DBLP:journals/tkdd/WuLGZC24,
	author = {Chenwang Wu and
                  Defu Lian and
                  Yong Ge and
                  Min Zhou and
                  Enhong Chen},
	title = {Attacking Social Media via Behavior Poisoning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {169},
	year = {2024},
	url = {https://doi.org/10.1145/3654673},
	doi = {10.1145/3654673},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WuLGZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since social media such as Facebook and X (formerly known as Twitter) have permeated various aspects of daily life, people have strong incentives to influence information dissemination on these platforms and differentiate their content from the fierce competition. Existing dissemination strategies typically employ marketing techniques, such as seeking publicity through renowned actors or targeted advertising placements. Despite their various forms, most simply spread information to strengthen user impressions without conducting formal analyses of specific influence enhancement. And coupled with high costs, most fall short of expectations. To this end, we ingeniously formulate the task of social media dissemination as poisoning attacks, which influence specified content’s dissemination among target users by intervening in some users’ social media behaviors (including retweeting, following, and profile modifying). Correspondingly, we propose a novel poisoning attack, Influence-based Social Media Attack (ISMA) to generate discrete poisoning behaviors, which is difficult to achieve with existing attacks. In ISMA, we first contribute an efficient influence evaluator to quantify the spread influence of poisoning behaviors. Based on the estimated influence, we then present an imperceptible hierarchical selector and a profile modification method ProMix to select influential behaviors to poison. Notably, our attack is driven by custom attack objectives, which allows one to flexibly design different optimization goals to change the information flow, which could solve the blindness of existing influence maximization methods. Besides, behaviors such as retweeting are gentle and simple to implement. These properties make our attack more cost-effective and practical. Extensive experiments on two large-scale real-world datasets demonstrate the superiority of our method as it significantly outperforms baselines, and additionally, the proposed evaluator’s analysis of user influence provides new insights for influence maximization on social media.}
}


@article{DBLP:journals/tkdd/BrzezinskiSSSSAIY24,
	author = {Dariusz Brzezinski and
                  Julia Stachowiak and
                  Jerzy Stefanowski and
                  Izabela Szczech and
                  Robert Susmaga and
                  Sofya Aksenyuk and
                  Uladzimir Ivashka and
                  Oleksandr Yasinskyi},
	title = {Properties of Fairness Measures in the Context of Varying Class Imbalance
                  and Protected Group Ratios},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {170},
	year = {2024},
	url = {https://doi.org/10.1145/3654659},
	doi = {10.1145/3654659},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BrzezinskiSSSSAIY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.}
}


@article{DBLP:journals/tkdd/ChenWWGXH24,
	author = {Yunkai Chen and
                  Qimeng Wang and
                  Shiwei Wu and
                  Yan Gao and
                  Tong Xu and
                  Yao Hu},
	title = {{TOMGPT:} Reliable Text-Only Training Approach for Cost-Effective
                  Multi-modal Large Language Model},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {171},
	year = {2024},
	url = {https://doi.org/10.1145/3654674},
	doi = {10.1145/3654674},
	timestamp = {Tue, 04 Mar 2025 13:53:55 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenWWGXH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-modal large language models (MLLMs), such as GPT-4, exhibit great comprehension capabilities on human instruction, as well as zero-shot ability on new downstream multi-modal tasks. To integrate the different modalities within a unified embedding space, previous MLLMs attempted to conduct visual instruction tuning with massive and high-quality image-text pair data, which requires substantial costs in data collection and training resources. In this article, we propose TOMGPT (Text-Only training Multi-modal GPT), a cost-effective MLLM tuned solely on easily accessible text data with much fewer resources. Along with pre-trained visual-linguistic coupled modality space (e.g., CLIP and ALIGN model), a text-only training strategy is devised to further project the aligned multi-modal latent space to that of LLM, endowing the LLM with visual comprehension capabilities in an efficient manner. Instead of enormous image-text training data required by previous MLLMs, we find that TOMGPT can be well-tuned with fewer yet diverse GPT-generated free-form text data, as we establish the semantic connection between LLM and pre-trained vision-language model. A quantitative evaluation is conducted on both MME and LVLM, which are recently released and extensively utilized MLLM benchmarks. The experiments reveal that TOMGPT achieved reliable performance compared to numerous models trained on a large amount of image-text pair data. Case studies are also presented, demonstrating TOMGPT’s broad understanding and dialogue capabilities across diverse image categories.}
}


@article{DBLP:journals/tkdd/ZhangWYZ24,
	author = {Jiaxin Zhang and
                  Yiqi Wang and
                  Xihong Yang and
                  En Zhu},
	title = {A Fully Test-time Training Framework for Semi-supervised Node Classification
                  on Out-of-Distribution Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {172},
	year = {2024},
	url = {https://doi.org/10.1145/3649507},
	doi = {10.1145/3649507},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangWYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have shown great potential in representation learning for various graph tasks. However, the distribution shift between the training and test sets poses a challenge to the efficiency of GNNs. To address this challenge, HomoTTT \xa0 proposes a fully test-time training framework for GNNs to enhance the model’s generalization capabilities for node classification tasks. Specifically, our proposed HomoTTT \xa0 designs a homophily-based and parameter-free graph contrastive learning task with adaptive augmentation to guide the model’s adaptation during the test-time training, allowing the model to adapt for specific target data. In the inference stage, HomoTTT \xa0 proposes to integrate the original GNN model and the adapted model after TTT using a homophily-based model selection method, which prevents potential performance degradation caused by unconstrained model adaptation. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of our proposed framework. Additionally, the exploratory study further validates the rationality of the homophily-based graph contrastive learning task with adaptive augmentation and the homophily-based model selection designed in\xa0HomoTTT.}
}


@article{DBLP:journals/tkdd/ZhuGQCYL24,
	author = {Ronghang Zhu and
                  Dongliang Guo and
                  Daiqing Qi and
                  Zhixuan Chu and
                  Xiang Yu and
                  Sheng Li},
	title = {A Survey of Trustworthy Representation Learning Across Domains},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {173},
	year = {2024},
	url = {https://doi.org/10.1145/3657301},
	doi = {10.1145/3657301},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuGQCYL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As AI systems have obtained significant performance to be deployed widely in our daily lives and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts of AI systems, and representation learning is the fundamental technology in machine learning. How to make representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.}
}


@article{DBLP:journals/tkdd/LiLYZLWKN24,
	author = {Mengyao Li and
                  Zhiyong Li and
                  Zhibang Yang and
                  Xu Zhou and
                  Yifan Li and
                  Ziyan Wu and
                  Lingzhao Kong and
                  Ke Nai},
	title = {{SA2E-AD:} {A} Stacked Attention Autoencoder for Anomaly Detection
                  in Multivariate Time Series},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {174},
	year = {2024},
	url = {https://doi.org/10.1145/3653677},
	doi = {10.1145/3653677},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLYZLWKN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection for multivariate time series is an essential task in the modern industrial field. Although several methods have been developed for anomaly detection, they usually fail to effectively exploit the metrical-temporal correlation and the other dependencies among multiple variables. To address this problem, we propose a stacked attention autoencoder for anomaly detection in multivariate time series (SA2E-AD); it focuses on fully utilizing the metrical and temporal relationships among multivariate time series. We design a multiattention block, alternately containing the temporal attention and metrical attention components in a hierarchical structure to better reconstruct normal time series, which is helpful in distinguishing the anomalies from the normal time series. Meanwhile, a two-stage training strategy is designed to further separate the anomalies from the normal data. Experiments on three publicly available datasets show that SA2E-AD outperforms the advanced baseline methods in detection performance and demonstrate the effectiveness of each part of the process in our method.}
}


@article{DBLP:journals/tkdd/ZhuangCL24,
	author = {Wen{-}Ming Zhuang and
                  Chih{-}Yao Chen and
                  Cheng{-}Te Li},
	title = {Towards Robust Rumor Detection with Graph Contrastive and Curriculum
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {175},
	year = {2024},
	url = {https://doi.org/10.1145/3653023},
	doi = {10.1145/3653023},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuangCL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Establishing a robust rumor detection model is vital in safeguarding the veracity of information on social media platforms. However, existing approaches to stopping rumor from spreading rely on abundant and clean training data, which is rarely available in real-world scenarios. In this work, we aim to develop a trustworthy rumor detection model that can handle inadequate and noisy labeled data. Our work addresses robust rumor detection, including classic and early detection, as well as five types of robustness issues: noisy and incomplete propagation, label scarcity and noise, and user disappearance. We propose a novel method, Robustness-Enhanced Rumor Detection (RERD), which mainly leverages the information propagation graphs of source tweets, along with user profiles and retweeting knowledge, for model learning. The novelty of RERD is four-fold. First, we jointly exploit the propagation structures of non-text and text retweets to learn the representation of a source tweet. Second, we simultaneously utilize the top-down and bottom-up information flows with relational propagations for graph representation learning. Third, to have effective early and robust detection, we implement contrastive learning on graphs with early and complete views of information propagation so that small snapshots can foresee their future shapes. Last, we use curriculum pseudo-labeling to mitigate the impact of label scarcity and noisy labels, and to correct representations learned from corrupted data. Experimental results on three benchmark datasets demonstrate that RERD consistently outperforms competitors in classic, early, and robust rumor detection scenarios. To the best of our knowledge, we are the first to simultaneously cope with early and five robust detections of rumors.}
}


@article{DBLP:journals/tkdd/ZhangFLYYZC24,
	author = {Lei Zhang and
                  Lele Fu and
                  Chen Liu and
                  Zhao Yang and
                  Jinghua Yang and
                  Zibin Zheng and
                  Chuan Chen},
	title = {Toward Few-Label Vertical Federated Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {176},
	year = {2024},
	url = {https://doi.org/10.1145/3656344},
	doi = {10.1145/3656344},
	timestamp = {Thu, 22 Aug 2024 20:24:26 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangFLYYZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) provides a novel paradigm for privacy-preserving machine learning, enabling multiple clients to collaborate on model training without sharing private data. To handle multi-source heterogeneous data, Vertical Federated Learning (VFL) has been extensively investigated. However, in the context of VFL, the label information tends to be kept in one authoritative client and is very limited. This poses two challenges for model training in the VFL scenario. On the one hand, a small number of labels cannot guarantee to train a well VFL model with informative network parameters, resulting in unclear boundaries for classification decisions. On the other hand, the large amount of unlabeled data is dominant and should not be discounted, and it is worthwhile to focus on how to leverage them to improve representation modeling capabilities. To address the preceding two challenges, we first introduce supervised contrastive loss to enhance the intra-class aggregation and inter-class estrangement, which is to deeply explore label information and improve the effectiveness of downstream classification tasks. Then, for unlabeled data, we introduce a pseudo-label-guided consistency mechanism to induce the classification results coherent across clients, which allows the representations learned by local networks to absorb the knowledge from other clients, and alleviates the disagreement between different clients for classification tasks. We conduct sufficient experiments on four commonly used datasets, and the experimental results demonstrate that our method is superior to the state-of-the-art methods, especially in the low-label rate scenario, and the improvement becomes more significant.}
}


@article{DBLP:journals/tkdd/LiuHZLSZ24,
	author = {Xinru Liu and
                  Yongjing Hao and
                  Lei Zhao and
                  Guanfeng Liu and
                  Victor S. Sheng and
                  Pengpeng Zhao},
	title = {{LMACL:} Improving Graph Collaborative Filtering with Learnable Model
                  Augmentation Contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {177},
	year = {2024},
	url = {https://doi.org/10.1145/3657302},
	doi = {10.1145/3657302},
	timestamp = {Thu, 30 Jan 2025 15:05:44 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuHZLSZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph collaborative filtering (GCF) has achieved exciting recommendation performance with its ability to aggregate high-order graph structure information. Recently, contrastive learning (CL) has been incorporated into GCF to alleviate data sparsity and noise issues. However, most of the existing methods employ random or manual augmentation to produce contrastive views that may destroy the original topology and amplify the noisy effects. We argue that such augmentation is insufficient to produce the optimal contrastive view, leading to suboptimal recommendation results. In this article, we proposed a Learnable Model Augmentation Contrastive Learning (LMACL) framework for recommendation, which effectively combines graph-level and node-level collaborative relations to enhance the expressiveness of collaborative filtering (CF) paradigm. Specifically, we first use the graph convolution network (GCN) as a backbone encoder to incorporate multi-hop neighbors into graph-level original node representations by leveraging the high-order connectivity in user-item interaction graphs. At the same time, we treat the multi-head graph attention network (GAT) as an augmentation view generator to adaptively generate high-quality node-level augmented views. Finally, joint learning endows the end-to-end training fashion. In this case, the mutual supervision and collaborative cooperation of GCN and GAT achieves learnable model augmentation. Extensive experiments on several benchmark datasets demonstrate that LMACL provides a significant improvement over the strongest baseline in terms of Recall and NDCG by 2.5%–3.8% and 1.6%–4.0%, respectively. Our model implementation code is available at https://github.com/LiuHsinx/LMACL.}
}


@article{DBLP:journals/tkdd/SchlieperLKSEZ24,
	author = {Philipp Schlieper and
                  Hermann Luft and
                  Kai Klede and
                  Christoph Strohmeyer and
                  Bj{\"{o}}rn M. Eskofier and
                  Dario Zanca},
	title = {Enhancing Unsupervised Outlier Model Selection: {A} Study on {IREOS}
                  Algorithms},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {178},
	year = {2024},
	url = {https://doi.org/10.1145/3653719},
	doi = {10.1145/3653719},
	timestamp = {Thu, 22 Aug 2024 20:24:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SchlieperLKSEZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier detection stands as a critical cornerstone in the field of data mining, with a wide range of applications spanning from fraud detection to network security. However, real-world scenarios often lack labeled data for training, necessitating unsupervised outlier detection methods. This study centers on Unsupervised Outlier Model Selection (UOMS), with a specific focus on the family of Internal, Relative Evaluation of Outlier Solutions (IREOS) algorithms. IREOS measures outlier candidate separability by evaluating multiple maximum-margin classifiers and, while effective, it is constrained by its high computational demands. We investigate the impact of several different separation methods in UOMS in terms of ranking quality and runtime. Surprisingly, our findings indicate that different separability measures have minimal impact on IREOS’ effectiveness. However, using linear separation methods within IREOS significantly reduces its computation time. These insights hold significance for real-world applications where efficient outlier detection is critical. In the context of this work, we provide the code for the IREOS algorithm and our separability techniques.}
}


@article{DBLP:journals/tkdd/SuiYZHZL24,
	author = {Hongjie Sui and
                  Huan Yan and
                  Tianyi Zheng and
                  Wenzhen Huang and
                  Yunlin Zhuang and
                  Yong Li},
	title = {Congestion-aware Spatio-Temporal Graph Convolutional Network-based
                  A* Search Algorithm for Fastest Route Search},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {179},
	year = {2024},
	url = {https://doi.org/10.1145/3657640},
	doi = {10.1145/3657640},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SuiYZHZL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The fastest route search, which is to find a path with the shortest travel time when the user initiates a query, has become one of the most important services in many map applications. To enhance the user experience of travel, it is necessary to achieve accurate and real-time route search. However, traffic conditions are changing dynamically, and the frequent occurrence of traffic congestion may greatly increase travel time. Thus, it is challenging to achieve the above goal. To deal with it, we present a congestion-aware spatio-temporal graph convolutional network-based A* search algorithm for the task of fastest route search. We first identify a sequence of consecutive congested traffic conditions as a traffic congestion event. Then, we propose a spatio-temporal graph convolutional network that jointly models the congestion events and changing travel time to capture their complex spatio-temporal correlations, which can predict the future travel-time information of each road segment as the basis of route planning. Further, we design a path-aided neural network to achieve effective origin-destination (OD) shortest travel-time estimation by encoding the complex relationships between OD pairs and their corresponding fastest paths. Finally, the cost function in the A* algorithm is set by fusing the output results of the two components, which is used to guide the route search. Our experimental results on the two real-world datasets show the superior performance of the proposed method.}
}


@article{DBLP:journals/tkdd/BicegoC24,
	author = {Manuele Bicego and
                  Ferdinando Cicalese},
	title = {Computing Random Forest-distances in the presence of missing data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {180},
	year = {2024},
	url = {https://doi.org/10.1145/3656345},
	doi = {10.1145/3656345},
	timestamp = {Thu, 22 Aug 2024 20:24:27 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BicegoC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we study the problem of computing Random Forest-distances in the presence of missing data. We present a general framework which avoids pre-imputation and uses in an agnostic way the information contained in the input points. We centre our investigation on RatioRF, an RF-based distance recently introduced in the context of clustering and shown to outperform most known RF-based distance measures. We also show that the same framework can be applied to several other state-of-the-art RF-based measures and provide their extensions to the missing data case. We provide significant empirical evidence of the effectiveness of the proposed framework, showing extensive experiments with RatioRF on 15 datasets. Finally, we also positively compare our method with many alternative literature distances, which can be computed with missing values.}
}


@article{DBLP:journals/tkdd/DengHZ24,
	author = {Jiayi Deng and
                  Danyang Huang and
                  Bo Zhang},
	title = {Distributed Pseudo-Likelihood Method for Community Detection in Large-Scale
                  Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {181},
	year = {2024},
	url = {https://doi.org/10.1145/3657300},
	doi = {10.1145/3657300},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/DengHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a distributed pseudo-likelihood method (DPL) to conveniently identify the community structure of large-scale networks. Specifically, we first propose a block-wise splitting method to divide large-scale network data into several subnetworks and distribute them among multiple workers. For simplicity, we assume the classical stochastic block model. Then, the DPL algorithm is iteratively implemented for the distributed optimization of the sum of the local pseudo-likelihood functions. At each iteration, the worker updates its local community labels and communicates with the master. The master then broadcasts the combined estimator to each worker for the new iterative steps. Based on the distributed system, DPL significantly reduces the computational complexity of the traditional pseudo-likelihood method using a single machine. Furthermore, to ensure statistical accuracy, we theoretically discuss the requirements of the worker sample size. Moreover, we extend the DPL method to estimate degree-corrected stochastic block models. The superior performance of the proposed distributed algorithm is demonstrated through extensive numerical studies and real data analysis.}
}


@article{DBLP:journals/tkdd/XiaRGZ24,
	author = {Bolun (Namir) Xia and
                  Vipula Rawte and
                  Aparna Gupta and
                  Mohammed J. Zaki},
	title = {{FETILDA:} Evaluation Framework for Effective Representations of Long
                  Financial Documents},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {7},
	pages = {182},
	year = {2024},
	url = {https://doi.org/10.1145/3657299},
	doi = {10.1145/3657299},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XiaRGZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.}
}


@article{DBLP:journals/tkdd/YuBTL24,
	author = {Penghang Yu and
                  Bing{-}Kun Bao and
                  Zhiyi Tan and
                  Guanming Lu},
	title = {Improving Graph Collaborative Filtering with Directional Behavior
                  Enhanced Contrastive Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {183:1--183:20},
	year = {2024},
	url = {https://doi.org/10.1145/3663574},
	doi = {10.1145/3663574},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YuBTL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Collaborative Filtering is a widely adopted approach for recommendation, which captures similar behavior features through Graph Neural Network (GNN). Recently, Contrastive Learning (CL) has been demonstrated as an effective method to enhance the performance of graph collaborative filtering. Typically, CL-based methods first perturb users’ history behavior data (e.g., drop clicked items), then construct a self-discriminating task for behavior representations under different random perturbations. However, for widely existing inactive users, random perturbation makes their sparse behavior information more incomplete, thereby harming the behavior feature extraction. To tackle the above issue, we design a novel directional perturbation-based CL method to improve the graph collaborative filtering performance. The idea is to perturb node representations through directionally enhancing behavior features. To do so, we propose a simple yet effective feedback mechanism, which fuses the representations of nodes based on behavior similarity. Then, to avoid irrelevant behavior preferences introduced by the feedback mechanism, we construct a behavior self-contrast task before and after feedback, to align the node representations between the final output and the first layer of GNN. Different from the widely adopted self-discriminating task, the behavior self-contrast task avoids complex message propagation on different perturbed graphs, which is more efficient than previous methods. Extensive experiments on three public datasets demonstrate that the proposed method has distinct advantages over other CL methods on recommendation accuracy.}
}


@article{DBLP:journals/tkdd/LuN24,
	author = {Fengcheng Lu and
                  Michael Kwok{-}Po Ng},
	title = {FastHGNN: {A} New Sampling Technique for Learning with Hypergraph
                  Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {184:1--184:26},
	year = {2024},
	url = {https://doi.org/10.1145/3663670},
	doi = {10.1145/3663670},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LuN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hypergraphs can represent higher-order relations among objects. Traditional hypergraph neural networks involve node-edge-node transform, leading to high computational cost and timing. The main aim of this article is to propose a new sampling technique for learning with hypergraph neural networks. The core idea is to design a layer-wise sampling scheme for nodes and hyperedges to approximate the original hypergraph convolution. We rewrite hypergraph convolution in the form of double integral and leverage Monte Carlo to achieve a discrete and consistent estimator. In addition, we use importance sampling and finally derive feasible probability mass functions for both nodes and hyperedges in consideration of variance reduction, based on some assumptions. Notably, the proposed sampling technique allows us to handle large-scale hypergraph learning, which is not feasible with traditional hypergraph neural networks. Experiment results demonstrate that our proposed model keeps a good balance between running time and prediction accuracy.}
}


@article{DBLP:journals/tkdd/HuangGSLWZ24,
	author = {Yinqiu Huang and
                  Min Gao and
                  Kai Shu and
                  Chenghua Lin and
                  Jia Wang and
                  Wei Zhou},
	title = {{EML:} Emotion-Aware Meta Learning for Cross-Event False Information
                  Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {185:1--185:25},
	year = {2024},
	url = {https://doi.org/10.1145/3661485},
	doi = {10.1145/3661485},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangGSLWZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern social media’s development has dramatically changed how people obtain information. However, the wide dissemination of various false information has severe detrimental effects. Accordingly, many deep learning-based methods have been proposed to detect false information and achieve promising results. However, these methods are unsuitable for new events due to the extremely limited labeled data and their discrepant data distribution to existing events. Domain adaptation methods have been proposed to mitigate these problems. However, their performance is suboptimal because they are not sensitive to new events due to they aim to align the domain information between existing events, and they hardly capture the fine-grained difference between real and fake claims by only using semantic information. Therefore, we propose a novel Emotion-aware Meta Learning (EML) approach for cross-event false information early detection, which deeply integrates emotions in meta learning to find event-sensitive initialization parameters that quickly adapt to new events. EML is non-trivial and faces three challenges: (1) How to effectively model semantic and emotional features to capture fine-grained differences? (2) How to reduce the impact of noise in meta learning based on semantic and emotional features? (3) How to detect the false information in a zero-shot detection scenario, i.e., no labeled data for new events? To tackle these challenges, firstly, we construct the emotion-aware meta tasks by selecting claims with similar and opposite emotions to the target claim other than usually used random sampling. Secondly, we propose a task weighting method and event-adaptation meta tasks to further improve the model’s robustness and generalization ability for detecting new events. Finally, we propose a weak label annotation method to extend EML to zero-shot detection according to the calculated labels’ confidence. Extensive experiments on real-world datasets show that the EML achieves superior performances on false information detection for new events.}
}


@article{DBLP:journals/tkdd/QianZZZ24,
	author = {Yuyang Qian and
                  Zhen{-}Yu Zhang and
                  Peng Zhao and
                  Zhi{-}Hua Zhou},
	title = {Learning with Asynchronous Labels},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {186:1--186:27},
	year = {2024},
	url = {https://doi.org/10.1145/3662186},
	doi = {10.1145/3662186},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/QianZZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning with data streams has attracted much attention in recent decades. Conventional approaches typically assume that the feature and label of a data item can be timely observed at each round. In many real-world tasks, however, it often occurs that either the feature or the label is observed firstly while the other arrives with delay. For instance, in distributed learning systems, a central processor collects training data from different sub-processors to train a learning model, whereas the feature and label of certain data items can arrive asynchronously due to network latency. The problem of learning with asynchronous feature or label in streams encompasses many applications but still lacks sound solutions. In this article, we formulate the problem and propose a new approach to alleviate the negative effect of asynchronicity and mining asynchronous data streams. Our approach carefully exploits the timely arrived information and builds an online ensemble structure to adaptively reuse historical models and instances. We provide the theoretical guarantees of our approach and conduct extensive experiments to validate its effectiveness.}
}


@article{DBLP:journals/tkdd/HeBDWGZZ24,
	author = {Yifan He and
                  Yatao Bian and
                  Xi Ding and
                  Bingzhe Wu and
                  Jihong Guan and
                  Ji Zhang and
                  Shuigeng Zhou},
	title = {Variate Associated Domain Adaptation for Unsupervised Multivariate
                  Time Series Anomaly Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {187:1--187:24},
	year = {2024},
	url = {https://doi.org/10.1145/3663573},
	doi = {10.1145/3663573},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HeBDWGZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multivariate Time Series Anomaly Detection (MTS-AD) is crucial for the effective management and maintenance of devices in complex systems, such as server clusters, spacecrafts, and financial systems, and so on. However, upgrade or cross-platform deployment of these devices will introduce the issue of cross-domain distribution shift, which leads to the prototypical problem of domain adaptation for MTS-AD. Compared with general domain adaptation problems, MTS-AD domain adaptation presents two peculiar challenges: (1) the dimensions of data from the source domain and the target domain are usually different, so alignment without losing any information is necessary; and (2) the association between different variates plays a vital role in the MTS-AD task, which is overlooked by traditional domain adaptation approaches. Aiming at addressing the above issues, we propose a Variate Associated Domain Adaptation Method Combined with a Graph Deviation Network (VANDA) for MTS-AD, which includes two major contributions. First, we characterize the intra-domain variate associations of the source domain by a graph deviation network (GDN), which can share parameters across domains without dimension alignment. Second, we propose a sliding similarity to measure the inter-domain variate associations and perform joint training by minimizing the optimal transport distance between source and target data for transferring variate associations across domains. VANDA achieves domain adaptation by transferring both variate associations and GDN parameters from the source domain to the target domain. We construct two pairs of MTS-AD datasets from existing MTS-AD data and combine three domain adaptation strategies with six MTS-AD backbones as the benchmark methods for experimental evaluation and comparison. Extensive experiments demonstrate the effectiveness of our approach, which outperforms the benchmark methods, and significantly improves the AD performance of the target domain by effectively utilizing the source domain knowledge.}
}


@article{DBLP:journals/tkdd/LiangSWCZC24,
	author = {Tianhai Liang and
                  Qiangqiang Shen and
                  Shuqin Wang and
                  Yongyong Chen and
                  Guokai Zhang and
                  Junxin Chen},
	title = {Data Completion-Guided Unified Graph Learning for Incomplete Multi-View
                  Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {188:1--188:23},
	year = {2024},
	url = {https://doi.org/10.1145/3664290},
	doi = {10.1145/3664290},
	timestamp = {Tue, 15 Oct 2024 17:43:32 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiangSWCZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to its heterogeneous property, multi-view data has been widely concerned over single-view data for performance improvement. Unfortunately, some instances may be with partially available information because of some uncontrollable factors, for which the incomplete multi-view clustering (IMVC) problem is raised. IMVC aims to partition unlabeled incomplete multi-view data into their clusters by exploiting the heterogeneity of multi-view data and overcoming the difficulty of data loss. However, most existing IMVC methods like BSV, MIC, OMVC, and IVC tend to conduct basic completion processing on the input data, without taking advantage of the correlation between samples and information redundancy. To overcome the above issue, we propose one novel IMVC method named data completion-guided unified graph learning (DCUGL), which could complete the data of missing views and fuse multiple learned view-specific similarity matrices into one unified graph. Specifically, we first reduce the dimension of the input data to learn multiple view-specific similarity matrices. By stacking all view-specific similarity matrices, DCUGL constructs a third-order tensor with the low-rank constraint, such that sample correlation within and between views can be well explored. Finally, by dividing the original data into observed data and unobserved data, DCUGL can infer and complete the missing data according to the view-specific similarity matrices, and obtain a unified graph, which can be directly used for clustering. To solve the proposed model, we design an iterative algorithm, which is based on the alternating direction method of multipliers framework. The proposed model proves to be superior by benchmarking on six challenging datasets compared with state-of-the-art IMVC methods.}
}


@article{DBLP:journals/tkdd/LiHZWZHJL24,
	author = {Tong Li and
                  Shuodi Hui and
                  Shiyuan Zhang and
                  Huandong Wang and
                  Yuheng Zhang and
                  Pan Hui and
                  Depeng Jin and
                  Yong Li},
	title = {Mobile User Traffic Generation Via Multi-Scale Hierarchical {GAN}},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {189:1--189:19},
	year = {2024},
	url = {https://doi.org/10.1145/3664655},
	doi = {10.1145/3664655},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiHZWZHJL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile user traffic facilitates diverse applications, including network planning and optimization, whereas large-scale mobile user traffic is hardly available due to privacy concerns. One alternative solution is to generate mobile user traffic data for downstream applications. However, existing generation models cannot simulate the multi-scale temporal dynamics in mobile user traffic on individual and aggregate levels. In this work, we propose a multi-scale hierarchical generative adversarial network (MSH-GAN) containing multiple generators and a multi-class discriminator. Specifically, the mobile traffic usage behavior exhibits a mixture of multiple behavior patterns, which are called micro-scale behavior patterns and are modeled by different pattern generators in our model. Moreover, the traffic usage behavior of different users exhibits strong clustering characteristics, with the co-existence of users with similar and different traffic usage behaviors. Thus, we model each cluster of users as a class in the discriminator’s output, referred to as macro-scale user clusters. Then, the gap between micro-scale behavior patterns and macro-scale user clusters is bridged by introducing the switch mode generators, which describe the traffic usage behavior in switching between different patterns. All users share the pattern generators. In contrast, the switch mode generators are only shared by a specific cluster of users, which models the multi-scale hierarchical structure of the traffic usage behavior of massive users. Finally, we urge MSH-GAN to learn the multi-scale temporal dynamics via a combined loss function, including adversarial loss, clustering loss, aggregated loss, and regularity terms. Extensive experiment results demonstrate that MSH-GAN outperforms state-of-art baselines by at least 118.17% in critical data fidelity and usability metrics. Moreover, observations show that MSH-GAN can simulate traffic patterns and pattern switch behaviors.}
}


@article{DBLP:journals/tkdd/NicolaDL24,
	author = {Victor Gomes De Oliveira Martins Nicola and
                  Karina Valdivia Delgado and
                  Marcelo de Souza Lauretto},
	title = {Imbalance-Robust Multi-Label Self-Adjusting kNN},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {190:1--190:30},
	year = {2024},
	url = {https://doi.org/10.1145/3663575},
	doi = {10.1145/3663575},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NicolaDL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the task of multi-label classification in data streams, instances arriving in real-time need to be associated with multiple labels simultaneously. Various methods based on the k Nearest Neighbors algorithm have been proposed to address this task. However, these methods face limitations when dealing with imbalanced data streams, a problem that has received limited attention in existing works. To approach this gap, this article introduces the Imbalance-Robust Multi-Label Self-Adjusting kNN (IRMLSAkNN), designed to tackle multi-label imbalanced data streams. IRMLSAkNN’s strength relies on maintaining relevant instances with imbalance labels by using a discarding mechanism that considers the imbalance ratio per label. On the other hand, it evaluates subwindows with an imbalance-aware measure to discard older instances that are lacking performance. We conducted statistical experiments on 32 benchmark data streams, evaluating IRMLSAkNN against eight multi-label classification algorithms using common accuracy-aware and imbalance-aware measures. The obtained results demonstrate that IRMLSAkNN consistently outperforms these algorithms in terms of predictive capacity and time cost across various levels of imbalance.}
}


@article{DBLP:journals/tkdd/LiW24,
	author = {Xiangyu Li and
                  Hua Wang},
	title = {On Mean-Optimal Robust Linear Discriminant Analysis},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {191:1--191:27},
	year = {2024},
	url = {https://doi.org/10.1145/3665500},
	doi = {10.1145/3665500},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Linear discriminant analysis (LDA) is widely used for dimensionality reduction under supervised learning settings. Traditional LDA objective aims to minimize the ratio of the squared Euclidean distances that may not perform optimally on noisy datasets. Multiple robust LDA objectives have been proposed to address this problem, but their implementations have two major limitations. One is that their mean calculations use the squared \\(\\ell_{2}\\)-norm distance to center the data, which is not valid when the objective depends on other distance functions. The second problem is that there is no generalized optimization algorithm to solve different robust LDA objectives. In addition, most existing algorithms can only guarantee that the solution is locally optimal rather than globally optimal. In this article, we review multiple robust loss functions and propose a new and generalized robust objective for LDA. Besides, to remove the mean value within data better, our objective uses an optimal way to center the data through learning. As one important algorithmic contribution, we derive an efficient iterative algorithm to optimize the resulting non-smooth and non-convex objective function. We theoretically prove that our solution algorithm guarantees that both the objective and the solution sequences converge to globally optimal solutions at a sub-linear convergence rate. The results of comprehensive experimental evaluations demonstrate the effectiveness of our new method, achieving significant improvements compared to the other competing methods.}
}


@article{DBLP:journals/tkdd/EkleE24,
	author = {Ocheme Anthony Ekle and
                  William Eberle},
	title = {Anomaly Detection in Dynamic Graphs: {A} Comprehensive Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {192:1--192:44},
	year = {2024},
	url = {https://doi.org/10.1145/3669906},
	doi = {10.1145/3669906},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/EkleE24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This survey article presents a comprehensive and conceptual overview of anomaly detection (AD) using dynamic graphs. We focus on existing graph-based AD techniques and their applications to dynamic networks. The contributions of this survey article include the following: (i) a comparative study of existing surveys on AD; (ii) a Dynamic Graph-based anomaly detection (DGAD) review framework in which approaches for detecting anomalies in dynamic graphs are grouped based on traditional machine learning models, matrix transformations, probabilistic approaches, and deep learning approaches; (iii) a discussion of graphically representing both discrete and dynamic networks; and (iv) a discussion of the advantages of graph-based techniques for capturing the relational structure and complex interactions in dynamic graph data. Finally, this work identifies the potential challenges and future directions for detecting anomalies in dynamic networks. This DGAD survey approach aims to provide a valuable resource for researchers and practitioners by summarizing the strengths and limitations of each approach, highlighting current research trends, and identifying open challenges. In doing so, it can guide future research efforts and promote advancements in AD in dynamic graphs.}
}


@article{DBLP:journals/tkdd/XiLDTLZY24,
	author = {Yunjia Xi and
                  Weiwen Liu and
                  Xinyi Dai and
                  Ruiming Tang and
                  Qing Liu and
                  Weinan Zhang and
                  Yong Yu},
	title = {Utility-Oriented Reranking with Counterfactual Context},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {193:1--193:22},
	year = {2024},
	url = {https://doi.org/10.1145/3671004},
	doi = {10.1145/3671004},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/XiLDTLZY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a critical task for large-scale commercial recommender systems, reranking rearranges items in the initial ranking lists from the previous ranking stage to better meet users’ demands. Foundational work in reranking has shown the potential of improving recommendation results by uncovering mutual influence among items. However, rather than considering the context of initial lists as most existing methods do, an ideal reranking algorithm should consider the counterfactual context—the position and the alignment of the items in the reranked lists. In this work, we propose a novel pairwise reranking framework, Utility-oriented Reranking with Counterfactual Context (URCC), which maximizes the overall utility after reranking efficiently. Specifically, we first design a utility-oriented evaluator, which applies Bi-LSTM and graph attention mechanism to estimate the listwise utility via the counterfactual context modeling. Then, under the guidance of the evaluator, we propose a pairwise reranker model to find the most suitable position for each item by swapping misplaced item pairs. Extensive experiments on two benchmark datasets and a proprietary real-world dataset demonstrate that URCC significantly outperforms the state-of-the-art models in terms of both relevance-based metrics and utility-based metrics.}
}


@article{DBLP:journals/tkdd/YinHWCMZ24,
	author = {Jiao Yin and
                  Wei Hong and
                  Hua Wang and
                  Jinli Cao and
                  Yuan Miao and
                  Yanchun Zhang},
	title = {A Compact Vulnerability Knowledge Graph for Risk Assessment},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {194:1--194:17},
	year = {2024},
	url = {https://doi.org/10.1145/3671005},
	doi = {10.1145/3671005},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YinHWCMZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software vulnerabilities, also known as flaws, bugs or weaknesses, are common in modern information systems, putting critical data of organizations and individuals at cyber risk. Due to the scarcity of resources, initial risk assessment is becoming a necessary step to prioritize vulnerabilities and make better decisions on remediation, mitigation, and patching. Datasets containing historical vulnerability information are crucial digital assets to enable AI-based risk assessments. However, existing datasets focus on collecting information on individual vulnerabilities while simply storing them in relational databases, disregarding their structural connections. This article constructs a compact vulnerability knowledge graph, VulKG, containing over 276 K nodes and 1 M relationships to represent the connections between vulnerabilities, exploits, affected products, vendors, referred domain names, and more. We provide a detailed analysis of VulKG modeling and construction, demonstrating VulKG-based query and reasoning, and providing a use case of applying VulKG to a vulnerability risk assessment task, i.e., co-exploitation behavior discovery. Experimental results demonstrate the value of graph connections in vulnerability risk assessment tasks. VulKG offers exciting opportunities for more novel and significant research in areas related to vulnerability risk assessment. The data and codes of this article are available at https://github.com/happyResearcher/VulKG.git.}
}


@article{DBLP:journals/tkdd/YangMDKB24,
	author = {Lingkai Yang and
                  Sally I. McClean and
                  Mark P. Donnelly and
                  Kashaf Khan and
                  Kevin Burke},
	title = {Detecting Process Duration Drift Using Gamma Mixture Models in a Left-Truncated
                  and Right-Censored Environment},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {195:1--195:24},
	year = {2024},
	url = {https://doi.org/10.1145/3669942},
	doi = {10.1145/3669942},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangMDKB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the realm of business context, process duration signifies time spent by customers between successive activities. This temporal perspective offers important insight to customer behavior, highlighting potential bottlenecks, and influencing business management decisions. The distribution of these process duration often changes over time due to factors such as seasonality, emerging legislation, changes to supply chains, and customer demand. Referred to as concept drift, these variations pose challenges for robust process modeling, understanding, and refinement. Subsequently, gamma mixture models are widely employed to model durations. These source data can, however, become left-truncated and right-censored within any specific observation window thereby necessitating a (well-known) modification to the likelihood function. The approach reported in this article leveraged this adapted likelihood across a series of observation windows, applying the likelihood ratio test to identify duration changes/concept drift. Due to its flexibility in modelling any duration distribution, the gamma mixture model was used with Nelder–Mead optimized likelihood for the left-truncated and right-censored data. The number of gamma components was determined by the Bayesian information criterion. The proposed framework underwent validation through simulated exponential samples, leading to recommendations for its practical application. Subsequently, we applied the methodology to three real-life event logs exhibiting diverse characteristics. Experimental results showcase the effectiveness of our approach in terms of data fitting, as compared to Kaplan–Meier curves, and in detecting instances of drift. This comprehensive validation underscores the practical utility and reliability of our framework for dynamic business scenarios.}
}


@article{DBLP:journals/tkdd/ZhangYCWGZZ24,
	author = {Yang Zhang and
                  Ting Yu and
                  Shengqiang Chi and
                  Zhen Wang and
                  Yue Gao and
                  Ji Zhang and
                  Tianshu Zhou},
	title = {Attribute Diversity Aware Community Detection on Attributed Graphs
                  Using Three-View Graph Attention Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {196:1--196:24},
	year = {2024},
	url = {https://doi.org/10.1145/3672081},
	doi = {10.1145/3672081},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangYCWGZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection is a fundamental yet important task for characterizing and understanding the structure of attributed graphs. Existing methods mainly focus on the structural tightness and attribute similarity among nodes in a community. However, grouping numerous semantically homogeneous nodes will result in information cocoons and thus reduce the robustness of community structure and the efficiency of node collaboration in real-world applications, such as recommendation systems and collaboration networks. Since nodes with closer connections tend to be more similar, finding communities with dense structures and diverse attributes poses great challenges to mining latent relationships between the graph structure and attribute distribution. To our best knowledge, very little research has been conducted to address this challenge. In this article, we propose a novel three-view graph attention neural networks (TvGANN) model to formally address the attribute diversity aware community detection problem. TvGANN reveals correlations between the graph structure and attributes distribution from the perspective of node organization, attribute co-occurrence, and the node-attribute interaction. It effectively captures structural features and attributes distribution by feeding a structural network and an attribute co-occurrence network into graph attention modules through the encoder–decoder framework. It also learns heterogeneous information by feeding a network into a meta-node attention module. Then, it fuzes the three modules and clusters the embedding representations through a Student's t-distribution approach, which iteratively refines the clustering results. The experiments show that our method not only improves the quality in dense community detection but also performs efficiently for attributed graphs.}
}


@article{DBLP:journals/tkdd/LiLLZWW24,
	author = {Munan Li and
                  Kai Liu and
                  Hongbo Liu and
                  Zheng Zhao and
                  Tomas E. Ward and
                  Xindong Wu},
	title = {Heterogeneous Meta-Path Graph Learning for Higher-Order Social Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {197:1--197:25},
	year = {2024},
	url = {https://doi.org/10.1145/3673658},
	doi = {10.1145/3673658},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLLZWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation systems have become an indispensable part of daily life. Social recommendation systems, which utilize social relationships and past behaviors to infer users’ preferences, have gained popularity in recent years. Exploring the inherent characteristics implied by higher-order relationships offers a new approach to social recommendation. However, it is challenging due to sparse social networks, influence heterogeneity, and noisy feedback. In this article, we propose a Heterogeneous Meta-path Graph Learning model for Higher-order Social Recommendation (HEAL). Within HEAL, we introduce a heterogeneous graph in social recommendation and utilize a meta-path-guided random walk to generate higher-order relationships. By encoding higher-order structures and semantics along different meta-graphs, HEAL can mitigate the limitation of data sparsity. Moreover, HEAL exploits aspect-aware and semantic-aware attentions to adaptively propagate and aggregate useful features from different meta-neighbors and higher-order relations. These attention-based aggregation layers allow HEAL to suppress the heterogeneity of social influences. Furthermore, HEAL adopts contrastive learning as a supplemental task to the recommendation task by maximizing the consistency between the self-discriminating objectives. This auxiliary task enables the model to learn more differentiated representations, further reducing its sensitivity to noisy feedback. We evaluate the performance of HEAL through extensive experiments on public datasets. The results demonstrate that leveraging higher-order relations can enhance the quality of social recommendations by better capturing the complexity and diversity of users’ preferences and interactions.}
}


@article{DBLP:journals/tkdd/LiangYGCJW24,
	author = {Yuliang Liang and
                  Enneng Yang and
                  Guibing Guo and
                  Wei Cai and
                  Linying Jiang and
                  Xingwei Wang},
	title = {Deconfounding User Preference in Recommendation Systems through Implicit
                  and Explicit Feedback},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {198:1--198:18},
	year = {2024},
	url = {https://doi.org/10.1145/3673762},
	doi = {10.1145/3673762},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiangYGCJW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems are influenced by many confounding factors (i.e., confounders) which result in various biases (e.g., popularity biases) and inaccurate user preference. Existing approaches try to eliminate these biases by inference with causal graphs. However, they assume all confounding factors can be observed and no hidden confounders exist. We argue that many confounding factors (e.g., season) may not be observable from user–item interaction data, resulting inaccurate user preference. In this article, we propose a deconfounded recommender considering unobservable confounders. Specifically, we propose a new causal graph with explicit and implicit feedback, which can better model user preference. Then, we realize a deconfounded estimator by the front-door adjustment, which is able to eliminate the effect of unobserved confounders. Finally, we conduct a series of experiments on two real-world datasets, and the results show that our approach performs better than other counterparts in terms of recommendation accuracy.}
}


@article{DBLP:journals/tkdd/ZhaoBXCMJWK24,
	author = {Ziyu Zhao and
                  Yuqi Bai and
                  Ruoxuan Xiong and
                  Qingyu Cao and
                  Chao Ma and
                  Ning Jiang and
                  Fei Wu and
                  Kun Kuang},
	title = {Learning Individual Treatment Effects under Heterogeneous Interference
                  in Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {199:1--199:21},
	year = {2024},
	url = {https://doi.org/10.1145/3673761},
	doi = {10.1145/3673761},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoBXCMJWK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating individual treatment effects in networked observational data is a crucial and increasingly recognized problem. One major challenge of this problem is violating the stable unit treatment value assumption (SUTVA), which posits that a unit’s outcome is independent of others’ treatment assignments. However, in network data, a unit’s outcome is influenced not only by its treatment (i.e., direct effect) but also by the treatments of others (i.e., spillover effect) since the presence of interference. Moreover, the interference from other units is always heterogeneous (e.g., friends with similar interests have a different influence than those with different interests). In this article, we focus on the problem of estimating individual treatment effects (including direct effect and spillover effect) under heterogeneous interference in networks. To address this problem, we propose a novel dual weighting regression (DWR) algorithm by simultaneously learning attention weights to capture the heterogeneous interference from neighbors and sample weights to eliminate the complex confounding bias in networks. We formulate the learning process as a bi-level optimization problem. Theoretically, we give a generalization error bound for the expected estimation error of the individual treatment effects. Extensive experiments on four benchmark datasets demonstrate that the proposed DWR algorithm outperforms the state-of-the-art methods in estimating individual treatment effects under heterogeneous network interference.}
}


@article{DBLP:journals/tkdd/MaW24,
	author = {Fei Ma and
                  Ping Wang},
	title = {Structural Properties on Scale-Free Tree Network with an Ultra-Large
                  Diameter},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {200:1--200:26},
	year = {2024},
	url = {https://doi.org/10.1145/3674146},
	doi = {10.1145/3674146},
	timestamp = {Mon, 11 Nov 2024 17:30:26 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/MaW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scale-free networks are prevalently observed in a great variety of complex systems, which triggers various researches relevant to networked models of such type. In this work, we propose a family of growth tree networks \\(\\mathcal{T}_{t}\\), which turn out to be scale-free, in an iterative manner. As opposed to most of published tree models with scale-free feature, our tree networks have the power-law exponent \\(\\gamma=1{ + }\\ln 5/\\ln 2\\) that is obviously larger than \\(3\\). At the same time, “small-world” property can not be found particularly because models \\(\\mathcal{T}_{t}\\) have an ultra-large diameter \\(D_{t}\\) (i.e., \\(D_{t}\\sim|\\mathcal{T}_{t}|^{\\ln 3/\\ln 5}\\)) and a greater average shortest path length \\(\\langle\\mathcal{W}_{t}\\rangle\\) (namely, \\(\\langle\\mathcal{W}_{t}\\rangle\\sim|\\mathcal{T}_{t}|^{\\ln 3/\\ln 5}\\)) where \\(|\\mathcal{T}_{t}|\\) represents vertex number. Next, we determine Pearson correlation coefficient and verify that networks \\(\\mathcal{T}_{t}\\) display disassortative mixing structure. In addition, we study random walks on tree networks \\(\\mathcal{T}_{t}\\) and derive exact solution to mean hitting time \\(\\langle\\mathcal{H}_{t}\\rangle\\). The results suggest that the analytic formula for quantity \\(\\langle\\mathcal{H}_{t}\\rangle\\) as a function of vertex number \\(|\\mathcal{T}_{t}|\\) shows a power-law form, i.e., \\(\\langle\\mathcal{H}_{t}\\rangle\\sim|\\mathcal{T}_{t}|^{1+\\ln 3/\\ln 5}\\). Accordingly, we execute extensive experimental simulations, and demonstrate that empirical analysis is in strong agreement with theoretical results. Lastly, we provide a guide to extend the proposed iterative manner in order to generate more general scale-free tree networks with large diameter.}
}


@article{DBLP:journals/tkdd/LiSZLLYZ24,
	author = {Jiaye Li and
                  Jinjing Shi and
                  Jian Zhang and
                  Yuhu Lu and
                  Qin Li and
                  Chunlin Yu and
                  Shichao Zhang},
	title = {Quantum Nearest Neighbor Collaborative Filtering Algorithm for Recommendation
                  System},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {201:1--201:28},
	year = {2024},
	url = {https://doi.org/10.1145/3674982},
	doi = {10.1145/3674982},
	timestamp = {Mon, 10 Feb 2025 11:14:27 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiSZLLYZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommendation has become especially crucial during the COVID-19 pandemic as a significant number of people rely on online shopping from home. Existing recommendation algorithms, designed to address issues like cold start and data sparsity, often overlook the time constraints of users. Specifically, users expect to receive recommendations for products of interest in the shortest possible time. To address this challenge, we propose a novel collaborative filtering recommendation algorithm that leverages the advantages of quantum computing circuits based on data reconstruction. This approach allows for the rapid identification of users similar to the target user, thereby improving recommendation speed. In our method, we utilize the information of known users to linearly reconstruct that of the target users, forming a relational matrix. Subsequently, we employ \\(l_{2,1}-\\)norm and \\(l_{1}-\\)norm to sparsely constrain the relationship matrix, deducing the weight of each known user. The final step involves providing similar recommendations to target users based on these weights. Furthermore, we implement the proposed algorithm using a quantum circuit, enabling exponential acceleration. The final weight matrix is derived from the quantum state outputted by the circuit. The speed of this process is theoretically demonstrated in detail. Experimental results indicate that our algorithm outperforms state-of-the-art methods in terms of root mean squared error (RMSE), mean absolute error (MAE) and normalized discounted cumulative gain (NDCG). Compared to state-of-the-art comparison algorithms, the proposed algorithm achieves the fastest recommendation speed across eight public datasets.}
}


@article{DBLP:journals/tkdd/DingWDXGLW24,
	author = {Shifei Ding and
                  Benyu Wu and
                  Ling Ding and
                  Xiao Xu and
                  Lili Guo and
                  Hongmei Liao and
                  Xindong Wu},
	title = {Towards Faster Deep Graph Clustering via Efficient Graph Auto-Encoder},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {202:1--202:23},
	year = {2024},
	url = {https://doi.org/10.1145/3674983},
	doi = {10.1145/3674983},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DingWDXGLW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep graph clustering (DGC) has been a promising method for clustering graph data in recent years. However, existing research primarily focuses on optimizing clustering outcomes by improving the quality of embedded representations, resulting in slow-speed complex models. Additionally, these methods do not consider changes in node similarity and corresponding adjustments in the original structure during the iterative optimization process after updating node embeddings, which easily falls into the representation collapse issue. We introduce an Efficient Graph Auto-Encoder (EGAE) and a dynamic graph weight updating strategy to address these issues, forming the basis for our proposed Fast DGC (FastDGC) network. Specifically, we significantly reduce feature dimensions using a linear transformation that preserves the original node similarity. We then employ a single-layer graph convolutional filtering approximation to replace multiple layers of graph convolutional neural network, reducing computational complexity and parameter count. During iteration, we calculate the similarity between nodes using the linearly transformed features and periodically update the original graph structure to reduce edges with low similarity, thereby enhancing the learning of discriminative and cohesive representations. Theoretical analysis confirms that EGAE has lower computational complexity. Extensive experiments on standard datasets demonstrate that our proposed method improves clustering performance and achieves a speedup of 2–3 orders of magnitude compared to state-of-the-art methods, showcasing outstanding performance. The code for our model is available at https://github.com/Marigoldwu/FastDGC. Furthermore, we have organized a portion of the DGC code into a unified framework, available at https://github.com/Marigoldwu/A-Unified-Framework-for-Deep-Attribute-Graph-Clustering.}
}


@article{DBLP:journals/tkdd/SunLWW24,
	author = {Mingchen Sun and
                  Yingji Li and
                  Ying Wang and
                  Xin Wang},
	title = {Towards Domain-Aware Stable Meta Learning for Out-of-Distribution
                  Generalization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {203:1--203:24},
	year = {2024},
	url = {https://doi.org/10.1145/3676558},
	doi = {10.1145/3676558},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/SunLWW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning models are often trained on datasets that are limited in size and distribution, which may not fully represent the entire range of data encountered in practice. Thus, making deep learning models generalize to out-of-distribution data has received a significant amount of attention in recent studies due to the critical importance of this ability in real-world applications. Meta learning as an effective knowledge transfer paradigm, which learns a base model with high generalization ability to adapt to new data distributions by minimizing domain shifts across tasks during meta-training. However, most existing meta learning methods assume that the base model can access the labels of different domains, and this assumption is demanding in many real application scenarios. In addition, these methods focus on narrowing data-level domain shifts, while ignoring task-level domain shifts, which may lead to inadequate or even negative transfer. Inspired by human learners who use induction to learn and master new tasks, we propose a novel domain-aware meta learning framework for out-of-distribution generalization, termed SMLG. This framework enables the base model to generalize effectively to unseen domains without relying on domain-specific labels. Specifically, we develop a domain-aware transformation module to obtain meta representation and pseudo domain labels. As a result, the base model can be trained robustly without the need for direct domain label input. Furthermore, to investigate the impact of domain shifts at different levels, we introduce a joint loss function that combines cross-entropy with a domain alignment constraint. Extensive experiments on benchmark datasets demonstrate the efficacy of our framework.}
}


@article{DBLP:journals/tkdd/WeiWZDWQHS24,
	author = {Xiangyu Wei and
                  Wei Wang and
                  Chongsheng Zhang and
                  Weiping Ding and
                  Bin Wang and
                  Yaguan Qian and
                  Zhen Han and
                  Chunhua Su},
	title = {Neighbor-Enhanced Representation Learning for Link Prediction in Dynamic
                  Heterogeneous Attributed Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {204:1--204:25},
	year = {2024},
	url = {https://doi.org/10.1145/3676559},
	doi = {10.1145/3676559},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WeiWZDWQHS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamic link prediction aims to predict future connections among unconnected nodes in a network. It can be applied for friend recommendations, link completion, and other tasks. Network representation learning algorithms have demonstrated considerable effectiveness in various prediction tasks. However, most network representation learning algorithms are based on homogeneous networks and static networks for link prediction that do not consider rich semantic and dynamic information. Additionally, existing dynamic network representation learning methods neglect the neighborhood interaction structure of the node. In this work, we design a neighbor-enhanced dynamic heterogeneous attributed network embedding method (NeiDyHNE) for link prediction. In light of the impressive achievements of the heuristic methods, we learn the information of common neighbors and neighbors’ interaction in heterogeneous networks to preserve the neighbors proximity and common neighbors proximity. NeiDyHNE encodes the attributes and neighborhood structure of nodes as well as the evolutionary features of the dynamic network. More specifically, NeiDyHNE consists of the hierarchical structure attention module and the convolutional temporal attention module. The hierarchical structure attention module captures the rich features and semantic structure of nodes. The convolutional temporal attention module captures the evolutionary features of the network over time in dynamic heterogeneous networks. We evaluate our method and various baseline methods on the dynamic link prediction task. Experimental results demonstrate that our method is superior to baseline methods in terms of accuracy.}
}


@article{DBLP:journals/tkdd/LiZSY24,
	author = {Hongyu Li and
                  Lefei Zhang and
                  Kehua Su and
                  Wei Yu},
	title = {{MICCF:} {A} Mutual Information Constrained Clustering Framework for
                  Learning Clustering-Oriented Feature Representations},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {205:1--205:22},
	year = {2024},
	url = {https://doi.org/10.1145/3672402},
	doi = {10.1145/3672402},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZSY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep clustering is a crucial task in machine learning and data mining that focuses on acquiring feature representations conducive to clustering. Previous research relies on self-supervised representation learning for general feature representations, such features may not be optimally suited for downstream clustering tasks. In this article, we introduce MICCF, a framework designed to bridge this gap and enhance clustering performance. MICCF enhances feature representations by combining mutual information constraints at different levels and employs an auxiliary alignment mutual information module for learning clustering-oriented features. To be specific, we propose a dual mutual information constraints module, incorporating minimal mutual information constraints at the feature level and maximal mutual information constraints at the instance level. This reduction in feature redundancy encourages the neural network to extract more discriminative features, while maximization ensures more unbiased and robust representations. To obtain clustering-oriented representations, the auxiliary alignment mutual information module utilizes pseudo-labels to maximize mutual information through a multi-classifier network, aligning features with the clustering task. The main network and the auxiliary module work in synergy to jointly optimize feature representations that are well-suited for the clustering task. We validate the effectiveness of our method through extensive experiments on six benchmark datasets. The results indicate that our method performs well in most scenarios, particularly on fine-grained datasets, where our approach effectively distinguishes subtle differences between closely related categories. Notably, our approach achieved a remarkable accuracy of 96.4% on the ImageNet-10 dataset, surpassing other comparison methods. The code is available at https://github.com/Li-Hyn/MICCF.git.}
}


@article{DBLP:journals/tkdd/LiaoCZGL24,
	author = {Chengwu Liao and
                  Chao Chen and
                  Wanyi Zhang and
                  Suiming Guo and
                  Chao Liu},
	title = {{AGENDA:} Predicting Trip Purposes with {A} New Graph Embedding Network
                  and Active Domain Adaptation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {206:1--206:25},
	year = {2024},
	url = {https://doi.org/10.1145/3677020},
	doi = {10.1145/3677020},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiaoCZGL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Trip purpose is a meaningful aspect of travel behaviour for the understanding of urban mobility. However, it is non-trivial to automatically obtain trip purposes. On one hand, trip purposes are naturally diverse and complicated, but the available predictive data sources are limited in real-world scenarios. On the other hand, since trip purpose labeling is costly and the development levels of cities are unbalanced, it is infeasible to access large-scale labeled data in less developed cities to train advanced prediction models. To narrow the gaps, this article presents A new Graph Embedding Network and active Domain Adaptation based framework (AGENDA) that only requires open data sources and is capable of predicting in both label-rich cities and label-scarce cities. Specifically, in label-rich source cities, we first use the vehicle’s GPS trajectory and open POI check-ins to augment trip contexts. Then we establish a supervised graph embedding network with two attention mechanisms to extract the passenger’s latent activity semantics and a classifier to predict trip purpose. To enable the prediction in label-scarce target cities, we further devise an active domain adaptation framework, in which adversarial domain adaptation is used to transfer the source-learned knowledge, and active learning is used to integrate human intelligence in the model training. A group of experiments are conducted with real-world datasets in Beijing and Shanghai. Evaluation results demonstrate that the proposed framework significantly outperforms existing trip purpose prediction algorithms, and could make accurate trip purpose prediction in label-scarce cities with much fewer labeling efforts.}
}


@article{DBLP:journals/tkdd/PengCPN24,
	author = {Bo Peng and
                  Ziqi Chen and
                  Srinivasan Parthasarathy and
                  Xia Ning},
	title = {Modeling Sequences as Star Graphs to Address Over-Smoothing in Self-Attentive
                  Sequential Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {207:1--207:24},
	year = {2024},
	url = {https://doi.org/10.1145/3676560},
	doi = {10.1145/3676560},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/PengCPN24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-attention (SA) mechanisms have been widely used in developing sequential recommendation (SR) methods, and demonstrated state-of-the-art performance. However, in this article, we show that self-attentive SR methods substantially suffer from the over-smoothing issue that item embeddings within a sequence become increasingly similar across attention blocks. As widely demonstrated in the literature, this issue could lead to a loss of information in individual items, and significantly degrade models’ scalability and performance. To address the over-smoothing issue, in this article, we view items within a sequence constituting a star graph and develop a method, denoted as \\(\\mathop{\\mathtt{MSSG}}\\limits\\), for SR. Different from existing self-attentive methods, \\(\\mathop{\\mathtt{MSSG}}\\limits\\) introduces an additional internal node to specifically capture the global information within the sequence, and does not require information propagation among items. This design fundamentally addresses the over-smoothing issue and enables \\(\\mathop{\\mathtt{MSSG}}\\limits\\) a linear time complexity with respect to the sequence length. We compare \\(\\mathop{\\mathtt{MSSG}}\\limits\\) with eleven state-of-the-art baseline methods on six public benchmark datasets. Our experimental results demonstrate that \\(\\mathop{\\mathtt{MSSG}}\\limits\\) significantly outperforms the baseline methods, with an improvement of as much as 10.10%. Our analysis shows the superior scalability of \\(\\mathop{\\mathtt{MSSG}}\\limits\\) over the state-of-the-art self-attentive methods. Our complexity analysis and runtime performance comparison together show that \\(\\mathop{\\mathtt{MSSG}}\\limits\\) is both theoretically and practically more efficient than self-attentive methods. Our analysis of the attention weights learned in SA-based methods indicates that on sparse recommendation data, modeling dependencies in all item pairs using the SA mechanism yields limited information gain, and thus, might not benefit the recommendation performance. Our source code and data are publicly accessible through GitHub.}
}


@article{DBLP:journals/tkdd/WangSW24,
	author = {Chunnan Wang and
                  Xiangyu Shi and
                  Hongzhi Wang},
	title = {Fair Federated Learning with Multi-Objective Hyperparameter Optimization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {208:1--208:13},
	year = {2024},
	url = {https://doi.org/10.1145/3676968},
	doi = {10.1145/3676968},
	timestamp = {Thu, 03 Oct 2024 00:45:35 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangSW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is an attractive paradigm for privacy-aware distributed machine learning, which enables clients to collaboratively learn a global model without sharing clients’ data. Recently, many strategies have been proposed to improve the generality of the global model and thus improve FL effect. However, existing strategies either ignore the fairness among clients or sacrifice performance for fairness. They cannot ensure that the gap among clients is as small as possible without sacrificing federated performance. To address this issue, we propose ParetoFed, a new local information aggregation method dedicated to obtaining better federated performance with smaller gap among clients. Specifically, we propose to use multi-objective hyperparameter optimization (HPO) algorithm to gain global models that are both fair and effective. Then, we send Pareto Optimal global models to each client, allowing them to choose the most suitable one as the base to optimize their local model. ParetoFed not only make the global models more fair but also make the selection of local models more personalized, which can further improve the federated performance. Extensive experiments show that ParetoFed outperforms existing FL methods in terms of fairness, and even achieves better federated performance, which demonstrates the significance of our method.}
}


@article{DBLP:journals/tkdd/ZhouGYYZW24,
	author = {Peng Zhou and
                  Yufeng Guo and
                  Haoran Yu and
                  Yuanting Yan and
                  Yanping Zhang and
                  Xindong Wu},
	title = {Concept Evolution Detecting over Feature Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {209:1--209:32},
	year = {2024},
	url = {https://doi.org/10.1145/3678012},
	doi = {10.1145/3678012},
	timestamp = {Thu, 12 Dec 2024 17:57:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhouGYYZW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosion of data volume has gradually transformed big data processing from the static batch mode to the online streaming model. Streaming data can be divided into instance streams (feature space remains fixed while instances increase over time), feature streams (instance space is fixed while features arrive over time), or both. Generally, online streaming data learning has two main challenges: infinite length and concept changing. Recently, feature stream learning has received much attention. However, existing feature stream learning methods focus on feature selection or classification but ignore the concept changing over time. To the best of our knowledge, this is the first work that studies concept evolution detection over feature streams. Specifically, we first give the formal definition of concept evolution over feature streams, which include three different types: concept emerging, concept drift, and concept forgetting. Then, we design a novel framework to detect the concept evolution over feature streams that consists of a sliding window, an improved density peak-based clustering algorithm, and a weighted bipartite graph-based concept detecting method. Extensive experiments have been conducted on several synthetic and high-dimensional datasets to indicate our new method’s ability to cluster and detect concept evolution over feature streams.}
}


@article{DBLP:journals/tkdd/MengZCLLH24,
	author = {Siyuan Meng and
                  Jie Zhou and
                  Xuxin Chen and
                  Yufei Liu and
                  Fengyuan Lu and
                  Xinli Huang},
	title = {Structure-Information-Based Reasoning over the Knowledge Graph: {A}
                  Survey of Methods and Applications},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {8},
	pages = {210:1--210:42},
	year = {2024},
	url = {https://doi.org/10.1145/3671148},
	doi = {10.1145/3671148},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/MengZCLLH24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The knowledge graph (KG) is an efficient form of knowledge organization and expression, providing prior knowledge support for various downstream tasks, and has received extensive attention in natural language processing. However, existing large-scale KGs have many hidden facts that need to be discovered. How to effectively use the structure information of KG is an important research direction of knowledge reasoning. Structure-Information-based reasoning over the KG is a technique used to find the missing facts by the structure information of KG. This survey summarizes the methods and applications of Structure-Information-based reasoning and hopes to be helpful to the research in this field. First, we introduced the definition of knowledge reasoning and the conceptual description of related tasks. Then, we reviewed the methods of Structure-Information-based reasoning. Specifically, we categorized them into four representative classes: PRA-based reasoning, Path-Embedding-based reasoning, RL-based reasoning, and GNN-based reasoning. We compared the motivations and details between practices in the same category. After that, we described the application of Structure-Information-based knowledge reasoning in the KG Completion, Question Answering System, Recommendation System, and other fields. Finally, we discussed the future research directions of Structure-Information-based reasoning.}
}


@article{DBLP:journals/tkdd/FuHZYZZC24,
	author = {Lele Fu and
                  Sheng Huang and
                  Lei Zhang and
                  Jinghua Yang and
                  Zibin Zheng and
                  Chuanfu Zhang and
                  Chuan Chen},
	title = {Subspace-Contrastive Multi-View Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {211:1--211:35},
	year = {2024},
	url = {https://doi.org/10.1145/3674839},
	doi = {10.1145/3674839},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/FuHZYZZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most multi-view clustering methods based on shallow models are limited in sound nonlinear information perception capability, or fail to effectively exploit complementary information hidden in different views. To tackle these issues, we propose a novel Subspace-Contrastive Multi-View Clustering (SCMC) approach. Specifically, SCMC utilizes a set of view-specific auto-encoders to map the original multi-view data into compact features capturing its nonlinear structures. Considering the large semantic gap of data from different modalities, we project multiple heterogeneous features into a joint semantic space, namely the embedded compact features are passed through the self-expression layers to learn the subspace representations, respectively. In order to enhance the discriminability and efficiently excavate the complementarity of various subspace representations, we use the contrastive strategy to maximize the similarity between positive pairs while differentiate negative pairs. Thus, the graph regularization is employed to encode the local geometric structure within varying subspaces for optimizing the consistent affinity matrix. Furthermore, to endow the proposed SCMC with the ability of handling the multi-view out-of-samples, we develop a consistent sparse representation (CSR) learning mechanism over the in-samples. To demonstrate the effectiveness of the proposed model, we conduct a large number of comparative experiments on ten challenging datasets, and the experimental results show that SCMC outperforms existing shallow and deep multi-view clustering methods. In addition, the experimental results on out-of-samples illustrate the effectiveness of the proposed CSR.}
}


@article{DBLP:journals/tkdd/LiuLLHGLFG24,
	author = {Yonghao Liu and
                  Mengyu Li and
                  Ximing Li and
                  Lan Huang and
                  Fausto Giunchiglia and
                  Yanchun Liang and
                  Xiaoyue Feng and
                  Renchu Guan},
	title = {Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning
                  and Self-Training},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {212:1--212:30},
	year = {2024},
	url = {https://doi.org/10.1145/3679018},
	doi = {10.1145/3679018},
	timestamp = {Fri, 07 Feb 2025 15:03:40 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuLLHGLFG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural networks to solve few-shot node classification on graphs. Despite their promising performance, some limitations remain. First, they employ the node encoding mechanism of homophilic graphs to learn node embeddings, even in heterophilic graphs. Second, existing models based on meta-learning ignore the interference of randomness in the learning process. Third, they are trained using only limited labeled nodes within the specific task, without explicitly utilizing numerous unlabeled nodes. Finally, they treat almost all sampled tasks equally without customizing them for their uniqueness. To address these issues, we propose a novel framework for few-shot node classification called Meta-GPS\\(++\\). Specifically, we first adopt an efficient method to learn discriminative node representations on homophilic and heterophilic graphs. Then, we leverage a prototype-based approach to initialize parameters and contrastive learning for regularizing the distribution of node embeddings. Moreover, we apply self-training to extract valuable information from unlabeled nodes. Additionally, we adopt S\\({}^{2}\\) (scaling and shifting) transformation to learn transferable knowledge from diverse tasks. The results on real-world datasets show the superiority of Meta-GPS\\(++\\). Our code is available here.}
}


@article{DBLP:journals/tkdd/KumarMC24,
	author = {Rahul Kumar and
                  Jo{\~{a}}o Mendes{-}Moreira and
                  Joydeep Chandra},
	title = {Spatio-Temporal Parallel Transformer Based Model for Traffic Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {213:1--213:25},
	year = {2024},
	url = {https://doi.org/10.1145/3679017},
	doi = {10.1145/3679017},
	timestamp = {Sat, 25 Jan 2025 23:48:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/KumarMC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic forecasting problems involve jointly modeling the non-linear spatio-temporal dependencies at different scales. While graph neural network models have been effectively used to capture the non-linear spatial dependencies, capturing the dynamic spatial dependencies between the locations remains a major challenge. The errors in capturing such dependencies propagate in modeling the temporal dependencies between the locations, thereby severely affecting the performance of long-term predictions. While transformer-based mechanisms have been recently proposed for capturing the dynamic spatial dependencies, these methods are susceptible to fluctuations in data brought on by unforeseen events like traffic congestion and accidents. To mitigate these issues we propose an improvised spatio-temporal parallel transformer (STPT) based model for traffic prediction that uses multiple adjacency graphs passed through a pair of coupled graph transformer-convolution network units, operating in parallel, to generate more noise-resilient embeddings. We conduct extensive experiments on 4 real-world traffic datasets and compare the performance of STPT with several state-of-the-art baselines, in terms of measures like RMSE, MAE, and MAPE. We find that using STPT improves the performance by around \\(10-34\\%\\) as compared to the baselines. We also investigate the applicability of the model on other spatio-temporal data in other domains. We use a Covid-19 dataset to predict the number of future occurrences in different regions from a given set of historical occurrences. The results demonstrate the superiority of our model for such datasets.}
}


@article{DBLP:journals/tkdd/Voevodski24,
	author = {Konstantin Voevodski},
	title = {Large-Scale K-Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {214:1--214:23},
	year = {2024},
	url = {https://doi.org/10.1145/3674508},
	doi = {10.1145/3674508},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Voevodski24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale learning algorithms are essential for modern data collections that may have billions of data points. Here, we study the design of parallel \\(k\\)-clustering algorithms, which include the \\(k\\)-median, \\(k\\)-medoids, and \\(k\\)-means clustering problems. We design efficient parallel algorithms for these problems and prove that they still compute constant-factor approximations to the optimal solution for stable clustering instances. In addition to our theoretic results, we present computational experiments that show that our \\(k\\)-median and \\(k\\)-means algorithms work well in practice—we are able to find better clusterings than state-of-the-art coreset constructions using samples of the same size.}
}


@article{DBLP:journals/tkdd/YuWLJLZF24,
	author = {Qiaohong Yu and
                  Huandong Wang and
                  Yu Liu and
                  Depeng Jin and
                  Yong Li and
                  Lin Zhu and
                  Junlan Feng},
	title = {Mobility Prediction via Rule-enhanced Knowledge Graph},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {215:1--215:21},
	year = {2024},
	url = {https://doi.org/10.1145/3677019},
	doi = {10.1145/3677019},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YuWLJLZF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of location acquisition technologies, massive mobile trajectories have been collected and made available to us, which support a fantastic way of understanding and modeling individuals’ mobility. However, existing data-driven methods either fail to capture the long-range dependency or suffer from a high computational cost. To overcome these issues, we propose a knowledge-driven framework for mobility prediction, which leverages knowledge graphs (KG) to formulate the mobility prediction task into the KG completion problem through integrating the structured “knowledge” from the mobility data. However, most related mobility prediction works only focus on the structured information encoded in existing triples, which ignores the rich semantic information of relation paths composed of multiple relation triples. In this article, we apply a dedicated module to extract the supplementary semantic structure of paths in KG, which contributes to the interpretability and accuracy of our model. Specifically, the extracted rules are applied to capture the dependencies between relational facts. Moreover, by incorporating user information in the entity-relation space with the corresponding hyperplane, our method could capture diverse user mobility patterns and model the personal characteristics of users to improve the accuracy of mobility prediction. Extensive evaluations illustrate that our proposed model beats state-of-the-art mobility prediction algorithms, which verifies the superiority of utilizing logical rules and user hyperplanes. Our implementation code is available at https://github.com/tsinghua-fib-lab/RulekG-MobiPre.git}
}


@article{DBLP:journals/tkdd/YangCLDZWXK24,
	author = {Jiacheng Yang and
                  Miaoxin Chen and
                  Cao Liu and
                  Boqi Dai and
                  Hai{-}Tao Zheng and
                  Hui Wang and
                  Rui Xie and
                  Hong{-}Gee Kim},
	title = {A Segment Augmentation and Prediction Consistency Framework for Multi-label
                  Unknown Intent Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {216:1--216:18},
	year = {2024},
	url = {https://doi.org/10.1145/3680286},
	doi = {10.1145/3680286},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YangCLDZWXK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label unknown intent detection is a challenging task where each utterance may contain not only multiple known but also unknown intents. To tackle this challenge, pioneers proposed to predict the intent number of the utterance first, then compare it with the results of known intent matching to decide whether the utterence contains unknown intent(s). Though they have made remarkable progress on this task, their methods still suffer from two important issues: (1) It is inadequate to extract multiple intents using only utterance encoding; (2) Optimizing two sub-tasks (intent number prediction and known intent matching) independently leads to inconsistent predictions. In this article, we propose to incorporate segment augmentation rather than only use utterance encoding to better detect multiple intents. We also design a prediction consistency module to bridge the gap between the two sub-tasks. Empirical results on MultiWOZ2.3 and MixSNIPS datasets show that our method achieves state-of-the-art performance and significantly improves the best baseline.}
}


@article{DBLP:journals/tkdd/Burkhardt24,
	author = {Paul Burkhardt},
	title = {Triangle Centrality},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {217:1--217:34},
	year = {2024},
	url = {https://doi.org/10.1145/3685677},
	doi = {10.1145/3685677},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/Burkhardt24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Triangle centrality is introduced for finding important vertices in a graph based on the concentration of triangles surrounding each vertex. It has the distinct feature of allowing a vertex to be central if it is in many triangles or none at all. Given a simple, undirected graph \\(G=(V,E)\\) with \\(n=|V|\\) vertices and \\(m=|E|\\) edges, let \\(\\triangle(v)\\) and \\(\\triangle(G)\\) denote the respective triangle counts of \\(v\\) and \\(G\\). Let \\(N(v)\\) be the neighborhood set of \\(v\\). Respectively, \\(N_{\\triangle}(v)\\) and \\(N_{\\triangle}[v]=\\{v\\}\\cup N_{\\triangle}(v)\\) denote the set of neighbors that are in triangles with \\(v\\) and the closed set including \\(v\\). Then the triangle centrality for a vertex \\(v\\) is \\begin{align*}TC(v)=\\frac{\\frac{1}{3}\\sum_{u\\in N_{\\triangle}[v]}\\triangle(u)+\\sum_{w\\in\\{N(v)\\setminus N_{\\triangle}(v)\\}}\\triangle(w)}{\\triangle(G)}.\\end{align*}  We show experimentally that triangle centrality is broadly applicable to many different types of networks. Our empirical results demonstrate that 30% of the time triangle centrality identified central vertices that differed with those found by five well-known centrality measures, which suggests novelty without being overly specialized. It is also asymptotically faster to compute on sparse graphs than all but the most trivial of these other measures. We introduce optimal algorithms that compute triangle centrality in \\(O(m\\overline{\\delta})\\) time and \\(O(m+n)\\) space, where \\(\\overline{\\delta}\\leq O(\\sqrt{m})\\) is the average degeneracy introduced by Burkhardt, Faber, and Harris (2020). In practical applications, \\(\\overline{\\delta}\\) is much smaller than \\(\\sqrt{m}\\) so triangle centrality can be computed in nearly linear time. On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a near work-optimal parallel algorithm that takes \\(O(\\log n)\\) time using \\(O(m\\sqrt{m})\\) CREW PRAM processors. In MapReduce, we show it takes four rounds using \\(O(m\\sqrt{m})\\) communication bits and is therefore optimal. We also derive a linear algebraic formulation of triangle centrality which can be computed in \\(O(m\\overline{\\delta})\\) time on sparse graphs.}
}


@article{DBLP:journals/tkdd/ZhangWLY24,
	author = {Qianru Zhang and
                  Zheng Wang and
                  Cheng Long and
                  Siu{-}Ming Yiu},
	title = {Billiards Sports Analytics: Datasets and Tasks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {218:1--218:27},
	year = {2024},
	url = {https://doi.org/10.1145/3686804},
	doi = {10.1145/3686804},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangWLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, it becomes a common practice to capture some data of sports games with devices such as GPS sensors and cameras and then use the data to perform various analyses on sports games, including tactics discovery, similar game retrieval, performance study, and so forth. While this practice has been conducted to many sports such as basketball and soccer, it remains largely unexplored on the billiards sports, which is mainly due to the lack of publicly available datasets. Motivated by this, we collect a dataset of billiards sports, which includes the layouts (i.e., locations) of billiards balls after performing break shots, called break shot layouts, the traces of the balls as a result of strikes (in the form of trajectories), and detailed statistics and performance indicators. We then study and develop techniques for three tasks on the collected dataset, including (1) prediction and (2) generation on the layouts data, and (3) similar billiards layout retrieval on the layouts data, which can serve different users such as coaches, players and fans. We conduct extensive experiments on the collected dataset and the results show that our methods perform effectively and efficiently.}
}


@article{DBLP:journals/tkdd/KimLKK24,
	author = {Min{-}Jeong Kim and
                  Yeon{-}Chang Lee and
                  David Y. Kang and
                  Sang{-}Wook Kim},
	title = {Trustworthiness-Driven Graph Convolutional Networks for Signed Network
                  Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {219:1--219:26},
	year = {2024},
	url = {https://doi.org/10.1145/3685279},
	doi = {10.1145/3685279},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/KimLKK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCNs) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node’s extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Specifically, TrustSGCN leverages topological information to measure trustworthiness on edge sign for high-order relationships inferred by balance theory. It then considers structural properties inherent to an input network, such as the ratio of triads, to correct for incorrect embedding propagation. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known social theories, i.e., balance and status, to jointly preserve the edge sign and direction between nodes connected by existing edges in the embedding space. The experiments on six real-world signed network datasets demonstrate that TrustSGCN consistently outperforms six state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN.}
}


@article{DBLP:journals/tkdd/GongCF24,
	author = {Yan Gong and
                  Georgina Cosma and
                  Axel Finke},
	title = {{VITR:} Augmenting Vision Transformers with Relation-Focused Learning
                  for Cross-modal Information Retrieval},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {220:1--220:21},
	year = {2024},
	url = {https://doi.org/10.1145/3686805},
	doi = {10.1145/3686805},
	timestamp = {Sun, 19 Jan 2025 14:58:34 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/GongCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The relations expressed in user queries are vital for cross-modal information retrieval. Relation-focused cross-modal retrieval aims to retrieve information that corresponds to these relations, enabling effective retrieval across different modalities. Pre-trained networks, such as Contrastive Language-Image Pre-training networks, have gained significant attention and acclaim for their exceptional performance in various cross-modal learning tasks. However, the Vision Transformer (ViT) used in these networks is limited in its ability to focus on image region relations. Specifically, ViT is trained to match images with relevant descriptions at the global level, without considering the alignment between image regions and descriptions. This article introduces VITR, a novel network that enhances ViT by extracting and reasoning about image region relations based on a local encoder. VITR is comprised of two key components. Firstly, it extends the capabilities of ViT-based cross-modal networks by enabling them to extract and reason with region relations present in images. Secondly, VITR incorporates a fusion module that combines the reasoned results with global knowledge to predict similarity scores between images and descriptions. The proposed VITR network was evaluated through experiments on the tasks of relation-focused cross-modal information retrieval. The results derived from the analysis of the Flickr30K, MS-COCO, RefCOCOg, and CLEVR datasets demonstrated that the proposed VITR network consistently outperforms state-of-the-art networks in image-to-text and text-to-image retrieval.}
}


@article{DBLP:journals/tkdd/YingWCF24,
	author = {Wangyang Ying and
                  Dongjie Wang and
                  Haifeng Chen and
                  Yanjie Fu},
	title = {Feature Selection as Deep Sequential Generative Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {221:1--221:21},
	year = {2024},
	url = {https://doi.org/10.1145/3687485},
	doi = {10.1145/3687485},
	timestamp = {Sun, 19 Jan 2025 14:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YingWCF24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., LASSO) methods have hyperparameters (e.g., top-k, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding; (3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters. The code is available at http://tinyurl.com/FSDSGL.}
}


@article{DBLP:journals/tkdd/LiuULZYLY24,
	author = {Wei Liu and
                  Leong Hou U and
                  Shangsong Liang and
                  Huaijie Zhu and
                  Jianxing Yu and
                  Yubao Liu and
                  Jian Yin},
	title = {VAE*: {A} Novel Variational Autoencoder via Revisiting Positive and
                  Negative Samples for Top-\emph{N} Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {222:1--222:24},
	year = {2024},
	url = {https://doi.org/10.1145/3680552},
	doi = {10.1145/3680552},
	timestamp = {Sun, 02 Feb 2025 15:06:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuULZYLY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the easy access, implicit feedback is often used for recommender systems. Compared with point-wise learning and pair-wise learning methods, list-wise rank learning methods have superior performance for top-\\(N\\) recommendation. Recent solutions, especially the list-wise methods, simply treat all interacted items of a user as equally important positives and annotate all no-interaction items of a user as negatives. For the list-wise approaches, we argue that this annotation scheme of implicit feedback is over-simplified due to the sparsity and missing fine-grained labels of the feedback data. To overcome this issue, we revisit the so-called positive and negative samples. First, considering the loss function of list-wise ranking, we analyze the impact of false positives and negatives theoretically. Second, based on the observation, we propose a self-adjusting credibility weight mechanism to re-weigh the positive samples and exploit the higher-order relation based on item–item matrix to sample the critical negative samples. In order to prevent the introduction of noise, we design a pruning strategy for critical negatives. Besides, to combine the reconstruction loss function for the positive samples and critical negative samples, we develop a simple yet effective VAEs framework with linear structure, which abandons the complex non-linear structure. Extensive experiments are conducted on six public real-world datasets. The results demonstrate that, our VAE* outperforms other VAE-based models by a large margin. Besides, we also verify the effect of denoising positives and exploring critical negatives by ablation study.}
}


@article{DBLP:journals/tkdd/WangHZ24,
	author = {Zhu Wang and
                  Fengxia Han and
                  Shengjie Zhao},
	title = {A Survey on Knowledge Graph Related Research in Smart City Domain},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {223:1--223:31},
	year = {2024},
	url = {https://doi.org/10.1145/3672615},
	doi = {10.1145/3672615},
	timestamp = {Sat, 25 Jan 2025 23:48:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangHZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge graph employs the specific graph structure to store knowledge in the form of entities, relations, attributes, and so forth, which can effectively represent correlations among data and has been applied in many fields, including search engine optimization, intelligent question answering, and recommendation systems. In this article, we mainly focus on the research and application of the domain-specific knowledge graph in the field of the smart city, which has not been fully paid attention to. Currently, the major problem faced by the smart city lies in data mining and proper application. On the one hand, data are usually stored by government management departments, which creates challenges such as high data storing overhead and inefficient data usage. On the other hand, data cannot be coordinated and collaborated between different city management systems, because data silos exist. By constructing the corresponding knowledge graph, the data of urban traffic, services, and public resources are integrated to provide help for city builders and managers to make important decisions. Therefore, we will review the related literature on the knowledge graph existing in the smart city domain to expore reasearch scopes. Specifically, we will analyze and summarize knowledge graph construction research in the field of smart cities from four perspectives, i.e., smart city ontology, urban data processing, urban knowledge graph construction, and their application. Finally, the research limitations and prospects of the urban knowledge graph are provided.}
}


@article{DBLP:journals/tkdd/WangSCLLL24,
	author = {Hongjun Wang and
                  Yi Song and
                  Wei Chen and
                  Zhipeng Luo and
                  Chongshou Li and
                  Tianrui Li},
	title = {A Survey of Co-Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {224:1--224:28},
	year = {2024},
	url = {https://doi.org/10.1145/3681793},
	doi = {10.1145/3681793},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangSCLLL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Co-clustering is to cluster samples and features simultaneously, which can also reveal the relationship between row clusters and column clusters. Therefore, lots of scientists have drawn much attention to conduct extensive research on it, and co-clustering is widely used in recommendation systems, gene analysis, medical data analysis, natural language processing, image analysis, and social network analysis. In this article, we survey the entire research aspect of co-clustering, especially the latest advances in co-clustering, and discover the current research challenges and future directions. First, due to different views from researchers on the definition of co-clustering, this article summarizes the definition of co-clustering and its extended definitions, as well as related issues, based on the perspectives of various scientists. Second, existing co-clustering techniques are approximately categorized into four classes: information-theory-based, graph-theory-based, matrix-factorization-based, and other theories-based. Third, co-clustering is applied in various aspects such as recommendation systems, medical data analysis, natural language processing, image analysis, and social network analysis. Furthermore, 10 popular co-clustering algorithms are empirically studied on 10 benchmark datasets with 4 metrics—accuracy, purity, block discriminant index, and running time, and their results are objectively reported. Finally, future work is provided to get insights into the research challenges of co-clustering.}
}


@article{DBLP:journals/tkdd/ChengARWS24,
	author = {Kewei Cheng and
                  Nesreen K. Ahmed and
                  Ryan A. Rossi and
                  Theodore L. Willke and
                  Yizhou Sun},
	title = {Neural-Symbolic Methods for Knowledge Graph Reasoning: {A} Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {225:1--225:44},
	year = {2024},
	url = {https://doi.org/10.1145/3686806},
	doi = {10.1145/3686806},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ChengARWS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neural symbolic knowledge graph (KG) reasoning offers a promising approach that combines the expressive power of symbolic reasoning with the learning capabilities inherent in neural networks. This survey provides a comprehensive overview of advancements, techniques, and challenges in the field of neural symbolic KG reasoning. The survey introduces the fundamental concepts of KGs and symbolic logic, followed by an exploration of three significant KG reasoning tasks: KG completion, complex query answering, and logical rule learning. For each task, we thoroughly discuss three distinct categories of methods: pure symbolic methods, pure neural approaches, and the integration of neural networks and symbolic reasoning methods known as neural-symbolic. We carefully analyze and compare the strengths and limitations of each category of methods to provide a comprehensive understanding. By synthesizing recent research contributions and identifying open research directions, this survey aims to equip researchers and practitioners with a comprehensive understanding of the state-of-the-art in neural symbolic KG reasoning, fostering future advancements in this interdisciplinary domain.}
}


@article{DBLP:journals/tkdd/TanXSS24,
	author = {Tao Tan and
                  Hong Xie and
                  Xiaoyu Shi and
                  Mingsheng Shang},
	title = {A Meta-Learning Approach to Mitigating the Estimation Bias of Q-Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {226:1--226:23},
	year = {2024},
	url = {https://doi.org/10.1145/3688849},
	doi = {10.1145/3688849},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/TanXSS24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is a longstanding problem that Q-learning suffers from the overestimation bias. This issue originates from the fact that Q-learning uses the expectation of maximum Q-value to approximate the maximum expected Q-value. A number of algorithms, such as Double Q-learning, were proposed to address this problem by reducing the estimation of maximum Q-value, but this may lead to an underestimation bias. Note that this underestimation bias may have a larger performance penalty than the overestimation bias. Different from previous algorithms, this article studies this issue from a fresh perspective, i.e., meta-learning view, which leads to our Meta-Debias Q-learning. The main idea is to extract the maximum expected Q-value with meta-learning over multiple tasks to remove the estimation bias of maximum Q-value and help the agent choose the optimal action more accurately. However, there are two challenges: (1) How to automatically select suitable training tasks? (2) How to positively transfer the meta-knowledge from selected tasks to remove the estimation bias of maximum Q-value? To address the two challenges mentioned above, we quantify the similarity between the training tasks and the test task. This similarity enables us to select appropriate “partial” training tasks and helps the agent extract the maximum expected Q-value to remove the estimation bias. Extensive experiment results show that our Meta-Debias Q-learning outperforms SOTA baselines drastically in three evaluation indicators, i.e., maximum Q-value, policy, and reward. More specifically, our Meta-Debias Q-learning only underestimates \\(1.2*10^{-3}\\) than the maximum expected Q-value in the multi-armed bandit environment and only differs \\(5.04\\%-5\\%=0.04\\%\\) than the optimal policy in the two states MDP environment. In addition, we compare the uniform weight and our similarity weight. Experiment results reveal fundamental insights into why our proposed algorithm outperforms in the maximum Q-value, policy, and reward.}
}


@article{DBLP:journals/tkdd/TanQG24,
	author = {Jun Tan and
                  Zhifeng Qiu and
                  Ning Gui},
	title = {Graph Representation Learning Enhanced Semi-Supervised Feature Selection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {227:1--227:20},
	year = {2024},
	url = {https://doi.org/10.1145/3689428},
	doi = {10.1145/3689428},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/TanQG24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature selection is a key step in machine learning by eliminating features that are not related to the modeling target to create reliable and interpretable models. By exploring the potential complex correlations among features of unlabeled data, recently introduced self-supervision-enhanced feature selection greatly reduces the reliance on the labeled samples. However, they are generally based on the autoencoder with sample-wise self-supervision, which can hardly exploit the relations among samples. To address this limitation, this article proposes graph representation learning enhanced semi-supervised feature selection (G-FS) which performs feature selection based on the discovery and exploitation of the non-Euclidean relations among features and samples by translating unlabeled “plain” tabular data into a bipartite graph. A self-supervised edge prediction task is designed to distill rich information on the graph into low-dimensional embeddings, which remove redundant features and noise. Guided by the condensed graph representation, we propose a batch attention feature weight generation mechanism that generates more robust weights according to batch-based selection patterns rather than individual samples. The results show that G-FS achieves significant performance edges in 14 datasets compared to twelve state-of-the-art baselines, including two recent self-supervised baselines. The source code is public available at https://github.com/Icannotnamemyselff/G-FS_Graph_enhacned_feature_selection.}
}


@article{DBLP:journals/tkdd/LiYSBM24,
	author = {Keyi Li and
                  Sen Yang and
                  Travis M. Sullivan and
                  Randall S. Burd and
                  Ivan Marsic},
	title = {ProcessGAN: Generating Privacy-Preserving Time-Aware Process Data
                  with Conditional Generative Adversarial Nets},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {228:1--228:31},
	year = {2024},
	url = {https://doi.org/10.1145/3687464},
	doi = {10.1145/3687464},
	timestamp = {Sun, 19 Jan 2025 14:58:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiYSBM24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Process data constructed from event logs provides valuable insights into procedural dynamics over time. The confidential information in process data, together with the data’s intricate nature, makes the datasets not sharable and challenging to collect. Consequently, research is limited using process data and analytics in the process mining domain. In this study, we introduced a synthetic process data generation task to address the limitation of sharable process data. We introduced a generative adversarial network, called ProcessGAN, to generate process data with activity sequences and corresponding timestamps. ProcessGAN consists of a transformer-based network as the generator, and a time-aware self-attention network as the discriminator. It can generate privacy-preserving process data from random noise. ProcessGAN considers the duration of the process and time intervals between activities to generate realistic activity sequences with timestamps. We evaluated ProcessGAN on five real-world datasets, two that are public and three collected in medical domains that are private. To evaluate the synthetic data, in addition to statistical metrics, we trained a supervised model to score the synthetic processes. We also used process mining to discover workflows for synthetic medical processes and had domain experts evaluate the clinical applicability of the synthetic workflows. ProcessGAN outperformed the existing generative models in generating complex processes with valid parallel pathways. The synthetic process data generated by ProcessGAN better represented the long-range dependencies between activities, a feature relevant to complicated medical and other processes. The timestamps generated by the ProcessGAN model showed similar distributions with the authentic timestamps. In addition, we trained a transformer-based network to generate synthetic contexts (e.g., patient demographics) that were associated with the synthetic processes. The synthetic contexts generated by our model outperformed the baseline models, with the distributions similar to the authentic contexts. We conclude that ProcessGAN can generate sharable synthetic process data indistinguishable from authentic data. Our source code is available in https://github.com/raaachli/ProcessGAN.}
}


@article{DBLP:journals/tkdd/RenPJHWGYY24,
	author = {Jiaqian Ren and
                  Hao Peng and
                  Lei Jiang and
                  Zhifeng Hao and
                  Jia Wu and
                  Shengxiang Gao and
                  Zhengtao Yu and
                  Qiang Yang},
	title = {Toward Cross-Lingual Social Event Detection with Hybrid Knowledge
                  Distillation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {229:1--229:36},
	year = {2024},
	url = {https://doi.org/10.1145/3689948},
	doi = {10.1145/3689948},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/RenPJHWGYY24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently published graph neural networks (GNNs) show promising performance at social event detection tasks. However, most studies are oriented toward monolingual data in languages with abundant training samples. This has left the common lesser-spoken languages relatively unexplored. Thus, in this work, we present a GNN-based framework that integrates cross-lingual word embeddings into the process of graph knowledge distillation for detecting events in low-resource language data streams. To achieve this, a novel cross-lingual knowledge distillation framework, called CLKD, exploits prior knowledge learned from similar threads in English to make up for the paucity of annotated data. Specifically, to extract sufficient useful knowledge, we propose a hybrid distillation method that consists of both feature-wise and relation-wise information. To transfer both kinds of knowledge in an effective way, we add a cross-lingual module in the feature-wise distillation to eliminate the language gap and selectively choose beneficial relations in the relation-wise distillation to avoid distraction caused by teachers’ misjudgments. Our proposed CLKD framework also adopts different configurations to suit both offline and online situations. Experiments on real-world datasets show that the framework is highly effective at detection in languages where training samples are scarce.}
}


@article{DBLP:journals/tkdd/YinSCHL24,
	author = {Nan Yin and
                  Li Shen and
                  Chong Chen and
                  XianSheng Hua and
                  Xiao Luo},
	title = {{SPORT:} {A} Subgraph Perspective on Graph Classification with Label
                  Noise},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {230:1--230:20},
	year = {2024},
	url = {https://doi.org/10.1145/3687468},
	doi = {10.1145/3687468},
	timestamp = {Sun, 19 Jan 2025 14:58:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YinSCHL24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have achieved great success recently on graph classification tasks using supervised end-to-end training. Unfortunately, extensive noisy graph labels could exist in the real world because of the complicated processes of manual graph data annotations, which may significantly degrade the performance of GNNs. Therefore, we investigate the problem of graph classification with label noise, which is demanding because of the complex graph representation learning issue and serious memorization of noisy samples. In this work, we present a novel approach called Subgraph Set Network with Sample Selection and Consistency Learning (SPORT) for this problem. To release the overfitting of GNNs, SPORT proposes to characterize each graph as a set of subgraphs generated by certain predefined stratagems, which can be viewed as samples from its underlying semantic distribution in graph space. Then we develop an equivariant network to encode the subgraph set with the consideration of the symmetry group. To further release the influences of noisy examples, we leverage the predictions of subgraphs to measure the likelihood of a sample being clean or noisy, followed by effective label updating. In addition, we propose a joint loss to advance the model generalizability by introducing consistency regularization. Comprehensive experiments on a wide range of graph classification datasets demonstrate the effectiveness of our SPORT. Specifically, SPORT outperforms the most competing baseline by up to 6.4%.}
}


@article{DBLP:journals/tkdd/YuXCLCT24,
	author = {Shuo Yu and
                  Feng Xia and
                  Honglong Chen and
                  Ivan Lee and
                  Lianhua Chi and
                  Hanghang Tong},
	title = {Heterogeneous Network Motif Coding, Counting, and Profiling},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {231:1--231:21},
	year = {2024},
	url = {https://doi.org/10.1145/3687465},
	doi = {10.1145/3687465},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/YuXCLCT24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network motifs, as a fundamental higher-order structure in large-scale networks, have received significant attention over recent years. Particularly in heterogeneous networks, motifs offer a higher capacity to uncover diverse information compared to homogeneous networks. However, the structural complexity and heterogeneity pose challenges in coding, counting, and profiling heterogeneous motifs. This work addresses these challenges by first introducing a novel heterogeneous motif coding method, adaptable to homogeneous motifs as well. Building upon this coding framework, we then propose GIFT, a heterogeneous network motif counting algorithm. GIFT effectively leverages combined structures of heterogeneous motifs through three key procedures: neighborhood searching, motif combination, and redundant motif filtering. We apply GIFT to count three-order and four-order motifs across eight distinct heterogeneous networks. Subsequently, we profile these detected motifs using four classical motif-based indicators. Experimental results demonstrate that by appropriately selecting motifs tailored to specific networks, heterogeneous motifs emerge as significant features in characterizing the underlying network structure.}
}


@article{DBLP:journals/tkdd/FengLZZ24,
	author = {Jiahui Feng and
                  Hefu Liu and
                  Jingmei Zhou and
                  Yang Zhou},
	title = {A Spatial-Temporal Aggregated Graph Neural Network for Docked Bike-sharing
                  Demand Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {232:1--232:27},
	year = {2024},
	url = {https://doi.org/10.1145/3690388},
	doi = {10.1145/3690388},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/FengLZZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting the number of rented and returned bikes at each station is crucial for operators to proactively manage shared bike relocation. Although existing research has proposed spatial-temporal prediction models that significantly advance traffic prediction, these models often neglect the unique characteristics of shared bike systems (BSS). Spatially, the entire bike-sharing system (BSS) experiences peak activity during morning and evening rush hours, whereas, during other periods, activity is localized to local stations, with some recording no rides, highlighting the need to distinguish between global and local spatial information across different times. Temporally, the historical riding records for each station exhibit non-stationary patterns, necessitating the analysis of both global trends and local fluctuations. Existing Graph Neural Network (GNN) approaches to predicting shared bike demand primarily capture static spatial-temporal data and fail to account for the dynamic nature of bike flows. Moreover, these studies focus on global spatial-temporal information without considering local nuances, making it challenging to capture spatiotemporal dynamics in fluctuating BSS. To address these challenges, we introduce the Spatial-Temporal Aggregated Graph Neural Network (STAGNN). Our model first constructs a dynamic adjacent matrix to describe the evolving connections between stations, followed by local and global information layers to capture spatial-temporal information from large-scale shared bike networks accurately. Our methodology has been validated through experiments on four real-world datasets, comparing it against benchmark models to demonstrate superior prediction accuracy. Additionally, we conduct extended experiments on four datasets during the morning and evening rush hours, and the results also affirm the efficacy of the STAGNN in enhancing prediction performance.}
}


@article{DBLP:journals/tkdd/WangDXW24,
	author = {Yidong Wang and
                  Meng Ding and
                  Jinhui Xu and
                  Di Wang},
	title = {Fair Single Index Model},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {233:1--233:33},
	year = {2024},
	url = {https://doi.org/10.1145/3690646},
	doi = {10.1145/3690646},
	timestamp = {Sun, 19 Jan 2025 14:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangDXW24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Single index models (SIMs) have been widely used in various applications due to their simplicity and interpretability. However, despite the potential for SIMs to result in discriminatory outcomes based on sensitive attributes like gender, race, or ethnicity, the issue of fairness has not been thoroughly examined in recent studies on the topic. This article aims to address these fairness concerns by proposing methods for building fair SIMs. Specifically, based on the definition of equal opportunity, we first provide a fairness definition for SIM. Next, we develop a unified fair SIM model and propose an efficient method to solve the fair SIM. Theoretically, we also show that our output is consistent in fairness. Finally, we conduct comprehensive experimental studies over eleven benchmark datasets and demonstrate that our fair SIM outperforms the other eight baseline methods.}
}


@article{DBLP:journals/tkdd/CribeiroRamalloAB24,
	author = {Jose Cribeiro{-}Ramallo and
                  Vadim Arzamasov and
                  Klemens B{\"{o}}hm},
	title = {Efficient Generation of Hidden Outliers for Improved Outlier Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {234:1--234:21},
	year = {2024},
	url = {https://doi.org/10.1145/3690827},
	doi = {10.1145/3690827},
	timestamp = {Sat, 25 Jan 2025 23:48:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/CribeiroRamalloAB24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Outlier generation is a popular technique used to solve important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the “multiple views” property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose Bisect, a new outlier generation method that creates realistic outliers mimicking said property. To do so, Bisect employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current method for recreating “multiple views.” We use the synthetic outliers generated by Bisect to effectively enhance outlier detection in diverse datasets for multiple use cases. For instance, oversampling with Bisect reduced the error by up to 3 times when compared with the baselines.}
}


@article{DBLP:journals/tkdd/HuangHLC24,
	author = {Longji Huang and
                  Jianbin Huang and
                  He Li and
                  Jiangtao Cui},
	title = {{LSTGCN:} Inductive Spatial Temporal Imputation Using Long Short-Term
                  Dependencies},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {235:1--235:25},
	year = {2024},
	url = {https://doi.org/10.1145/3690645},
	doi = {10.1145/3690645},
	timestamp = {Tue, 04 Feb 2025 20:33:10 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangHLC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial temporal forecasting of urban sensors is essentially important for many urban systems, such as intelligent transportation and smart cities. However, due to the problem of hardware failure or network failure, there are some missing values or missing monitoring sensors that need to be interpolated. Recent research on deep learning has made substantial progress on imputation problem, especially temporal aspect (i.e., time series imputation), while little attention has been paid to spatial aspect (both dynamic and static) and long-term temporal dependencies. In this article, we proposed a spatial temporal imputation model, named Long Short-Term Graph Convolution Networks (LSTGCN), which includes gated temporal extraction (GTE) module, multi-head attention-based temporal capture (MHAT) module, long-term periodic temporal encoding (LPTE) module, and bidirectional spatial graph convolution (BSGC) module. The GTE adopts a gated mechanism to filter short-term temporal information, while the MHAT utilizes position encoding to enhance the difference of each timestamps, then use multi-head attention to capture short-term temporal dependency. The BSGC is adopted to handle with spatial relationships between sensor nodes. And we design a periodic encoding technique to process long-term temporal dependencies. The BSGC handles spatial relationships between sensor nodes, and a periodic encoding technique is used to process long-term temporal dependencies. Our experimental analysis includes completion and forecasting tasks, as well as transfer and ablation analyses. The results show that our proposed model outperforms state-of-the-art baselines on real-world datasets.}
}


@article{DBLP:journals/tkdd/LiZCWXZ24,
	author = {Shucheng Li and
                  Jingzhou Zhu and
                  Boyu Chang and
                  Hao Wu and
                  Fengyuan Xu and
                  Sheng Zhong},
	title = {Multi-Label and Evolvable Dataset Preparation for Web-Based Object
                  Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {236:1--236:21},
	year = {2024},
	url = {https://doi.org/10.1145/3695465},
	doi = {10.1145/3695465},
	timestamp = {Sun, 19 Jan 2025 14:58:35 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZCWXZ24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we focus on the emerging field of web-based object detection, which has gained considerable attention due to its ability to utilize large amounts of web data for training, thus eliminating the need for labor-intensive manual annotations. However, the noisy and ever-evolving nature of web data poses challenges in preparing high-quality datasets for web-based object detection. To address these challenges, we propose a fully automatic dataset preparation method in this article. Our proposed method incorporates a hierarchical clustering module that assigns multiple precise labels to each image. This module is based on our observation that web image data exhibits different distributions at varying granularities. Furthermore, an evolutionary relabeling module ensures the adaptability of both the prepared dataset and trained detection models to the ever-evolving web data. Extensive experiments demonstrate that our method outperforms other web-based methods, and achieves a comparable performance to those manually labeled benchmark datasets.}
}


@article{DBLP:journals/tkdd/OhUMK24,
	author = {Sejoon Oh and
                  Berk Ustun and
                  Julian J. McAuley and
                  Srijan Kumar},
	title = {{FINEST:} Stabilizing Recommendations by Rank-Preserving Fine-Tuning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {237:1--237:22},
	year = {2024},
	url = {https://doi.org/10.1145/3695256},
	doi = {10.1145/3695256},
	timestamp = {Wed, 08 Jan 2025 21:11:38 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/OhUMK24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a “reference” rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FINEST can ensure that recommender models output stable recommendations under a wide range of different perturbations without compromising next-item prediction accuracy.}
}


@article{DBLP:journals/tkdd/LiQWZC24,
	author = {Yifan Li and
                  Shuhan Qi and
                  Xuan Wang and
                  Jiajia Zhang and
                  Lei Cui},
	title = {A Novel Tree-Based Method for Interpretable Reinforcement Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {18},
	number = {9},
	pages = {238:1--238:22},
	year = {2024},
	url = {https://doi.org/10.1145/3695464},
	doi = {10.1145/3695464},
	timestamp = {Mon, 03 Mar 2025 22:25:33 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiQWZC24.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep reinforcement learning (DRL) has garnered remarkable success across various domains, propelled by advancements in deep learning (DL) technologies. However, the opacity of DL presents significant challenges, limiting the application of DRL in critical systems. In response, decision tree (DT)-based methods, known for their transparent decision-making mechanisms, have shown promise in making interpretable policies for decision-making problems. Existing methods often employ differential DTs to model RL policies and discretize them to conventional DTs for higher interpretability. Yet, this method leads to discrepancies between the trained differential DTs and the discretized DTs. To address this issue, we introduce Generative Consistent Trees (GCTs), a novel solution that circumvents the information loss typically associated with the argmax operation in prior research. By implementing a reparameterization technique to approximate the categorical distribution, GCTs ensure the consistencies between trained GCTs and discretized counterparts. Moreover, we have developed an imitation learning-based framework for interpretable reinforcement learning. This framework is designed to train GCTs by efficiently mimicking expert policies. Our extensive experiments across multiple environments have validated the effectiveness of this approach, highlighting the potential of GCTs in enhancing the interpretability and applicability of DRL.}
}
