@article{DBLP:journals/tkdd/WangCHH23,
	author = {Ting{-}Yun Wang and
                  Chiao{-}Ting Chen and
                  Ju{-}Chun Huang and
                  Szu{-}Hao Huang},
	title = {Modeling Cross-session Information with Multi-interest Graph Neural
                  Networks for the Next-item Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {1:1--1:28},
	year = {2023},
	url = {https://doi.org/10.1145/3532192},
	doi = {10.1145/3532192},
	timestamp = {Mon, 01 May 2023 13:01:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangCHH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-item recommendation involves predicting the next item of interest of a given user from their past behavior. Users tend to browse and purchase various items on e-commerce websites according to their varied interests and needs, as reflected in their purchasing history. Most existing next-item recommendation methods aim at extracting the main point of interest in each browsing session and encapsulate it in a single representation. However, past behavior sequences reflect the multiple interests of a single user, which cannot be captured by methods that focus on single-interest contexts. Indeed, multiple interests cannot be captured in a single representation, and doing so results in missing information. Therefore, we propose a model with a multi-interest structure for capturing the various interests of users from their behavior sequence. Moreover, we adopted a method based on a graph neural network to construct interest graphs based on the historical and current behavior sequences of users. These graphs can capture complex item transition patterns related to different interests. In experiments, the proposed method outperforms state-of-the-art session-based recommendation systems on three real-world datasets, achieving 4% improvement of Recall over the SOTAs on Jdata dataset.}
}


@article{DBLP:journals/tkdd/WangCLFZZ23,
	author = {Yu Wang and
                  Chuan Chen and
                  Jinrong Lai and
                  Lele Fu and
                  Yuren Zhou and
                  Zibin Zheng},
	title = {A Self-Representation Method with Local Similarity Preserving for
                  Fast Multi-View Outlier Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {2:1--2:20},
	year = {2023},
	url = {https://doi.org/10.1145/3532191},
	doi = {10.1145/3532191},
	timestamp = {Sun, 16 Apr 2023 20:31:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangCLFZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapidly growing attention to multi-view data in recent years, multi-view outlier detection has become a rising field with intense research. These researches have made some success, but still exist some issues that need to be solved. First, many multi-view outlier detection methods can only handle datasets that conform to the cluster structure but are powerless for complex data distributions such as manifold structures. This overly restrictive data assumption limits the applicability of these methods. In addition, almost the majority of multi-view outlier detection algorithms cannot solve the online detection problem of multi-view outliers. To address these issues, we propose a new detection method based on the local similarity relation and data reconstruction, i.e., the Self-Representation Method with Local Similarity Preserving for fast multi-view outlier detection (SRLSP). By using the local similarity structure, the proposed method fully utilizes the characteristics of outliers and detects outliers with an applicable objective function. Besides, a well-designed optimization algorithm is proposed, which completes each iteration with linear time complexity and can calculate each instance parallelly. Also, the optimization algorithm can be easily extended to the online version, which is more suitable for practical production environments. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method on both performance and time complexity.}
}


@article{DBLP:journals/tkdd/WangZLZW23,
	author = {Ke Wang and
                  Yanmin Zhu and
                  Haobing Liu and
                  Tianzi Zang and
                  Chunyang Wang},
	title = {Learning Aspect-Aware High-Order Representations from Ratings and
                  Reviews for Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {3:1--3:22},
	year = {2023},
	url = {https://doi.org/10.1145/3532188},
	doi = {10.1145/3532188},
	timestamp = {Tue, 17 Dec 2024 16:16:32 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZLZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Textual reviews contain rich semantic information that is useful for making better recommendation, as such semantic information may indicate more fine-grained preferences of users. Recent efforts make considerable improvement on recommendation by integrating textual reviews in rating-based recommendations. However, there still exist major challenges on integrating textual reviews for recommendation. On the one hand, most existing works focus on learning a single representation from reviews but ignoring complex relations between users (or items) and reviews, which may fail to capture user preferences and item attributes together. On the other hand, these works independently learn latent representations from ratings and reviews while omitting correlations between rating-based features and review-based features, which may harm recommendation performance. In this article, we capture the aspect-aware relations by constructing heterogeneous graphs from reviews. Furthermore, we propose a new recommendation model, namely AHOR, to jointly distill rating-based features and review-based features, which are derived from ratings and reviews, respectively. To explore the multi-hop connectivity information between users, items, and aspects, a novel graph neural network is introduced to learn aspect-aware high-order representations. Experiments based on public datasets show that our approach outperforms state-of-the-art methods. We also provide detailed analysis on the high-order signals and the aspect importance to show the interpretability of our proposed model.}
}


@article{DBLP:journals/tkdd/ZhangLLW23,
	author = {Zan Zhang and
                  Lin Liu and
                  Jiuyong Li and
                  Xindong Wu},
	title = {Integrating Global and Local Feature Selection for Multi-Label Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {4:1--4:37},
	year = {2023},
	url = {https://doi.org/10.1145/3532190},
	doi = {10.1145/3532190},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangLLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-label learning deals with the problem where an instance is associated with multiple labels simultaneously. Multi-label data is often of high dimensionality and has many noisy, irrelevant, and redundant features. As an important machine learning task, multi-label feature selection has received considerable attention in recent years due to its promising performance in dealing with high-dimensional multi-label data. Existing multi-label feature selection methods typically select the global features which are shared by all instances in a dataset. However, these multi-label feature selection methods may be suboptimal since they do not consider the specific characteristics of instances. In this paper, we propose a novel algorithm that integrates Global and Local Feature Selection (GLFS) to exploit both the global features and a subset of discriminative features shared only locally by a subgroup of instances in a multi-label dataset. Specifically, GLFS employs linear regression and ℓ2,1-norm on the regression parameters to achieve simultaneous global and local feature selection. Moreover, the proposed algorithm has an effective mechanism for utilizing label correlations to improve the feature selection. Experiments on real-world multi-label datasets show the superiority of GLFS over the state-of-the-art multi-label feature selection methods.}
}


@article{DBLP:journals/tkdd/ChenG23,
	author = {Xinye Chen and
                  Stefan G{\"{u}}ttel},
	title = {An Efficient Aggregation Method for the Symbolic Representation of
                  Temporal Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {5:1--5:22},
	year = {2023},
	url = {https://doi.org/10.1145/3532622},
	doi = {10.1145/3532622},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Symbolic representations are a useful tool for the dimension reduction of temporal data, allowing for the efficient storage of and information retrieval from time series. They can also enhance the training of machine learning algorithms on time series data through noise reduction and reduced sensitivity to hyperparameters. The adaptive Brownian bridge-based aggregation (ABBA) method is one such effective and robust symbolic representation, demonstrated to accurately capture important trends and shapes in time series. However, in its current form, the method struggles to process very large time series. Here, we present a new variant of the ABBA method, called fABBA. This variant utilizes a new aggregation approach tailored to the piecewise representation of time series. By replacing the k-means clustering used in ABBA with a sorting-based aggregation technique, and thereby avoiding repeated sum-of-squares error computations, the computational complexity is significantly reduced. In contrast to the original method, the new approach does not require the number of time series symbols to be specified in advance. Through extensive tests, we demonstrate that the new method significantly outperforms ABBA with a considerable reduction in runtime while also outperforming the popular SAX and 1d-SAX representations in terms of reconstruction accuracy. We further demonstrate that fABBA can compress other data types such as images.}
}


@article{DBLP:journals/tkdd/LiYTCW23,
	author = {Lei Li and
                  Mengjiao Yan and
                  Zhenchao Tao and
                  Huanhuan Chen and
                  Xindong Wu},
	title = {Semi-Supervised Graph Pattern Matching and Rematching for Expert Community
                  Location},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {6:1--6:26},
	year = {2023},
	url = {https://doi.org/10.1145/3532623},
	doi = {10.1145/3532623},
	timestamp = {Sun, 16 Apr 2023 20:31:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiYTCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph pattern matching (GPM) is widely used in social network analysis, such as expert finding, social group query, and social position detection. Technically, GPM is to find matched subgraphs that meet the requirements of pattern graphs in big social networks. In the application of expert community location, the nodes in the pattern graph and data graph represent expert entities, and the edges represent previous cooperations between them. However, the existing GPM methods focus on shortening the matching time and without considering the preference of the decision maker (DM), which makes it difficult for the DM to find ideal teams from numerous matches to complete the assigned task. In this article, as for the process of graph pattern matching and rematching, with a preferred expert set, i.e., the DM hopes that one or more experts in this set will appear in matched subgraphs, we propose a Dual Simulation-based Edge Sequencing-oriented Semi-Supervised GPM method (DsEs-ssGPM). In addition, considering a preferred expert set and a dispreferred expert set together, the DM hopes that experts in the dispreferred expert set will not appear in final matches, so we have the DsEs-ssGPM+ method. Technically, these DsEs-ssGPM methods conduct the matching process from the preferred expert set during dual simulation-based edge sequencing, and based on the edge sequence, these edges are searched recursively. Especially, as for the rematching process, when the preferred and/or the dispreferred expert sets change continuously, to process the GPM again is unnecessary and it is possible to revise the previous matched results partially with DsEs-ssGPM methods. Experiments on four large datasets demonstrate the effectiveness, efficiency and stability of our proposed DsEs-ssGPM methods, and the necessity of introducing an edge sequencing mechanism.}
}


@article{DBLP:journals/tkdd/GuF23,
	author = {Zhibin Gu and
                  Songhe Feng},
	title = {Individuality Meets Commonality: {A} Unified Graph Learning Framework
                  for Multi-View Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {7:1--7:21},
	year = {2023},
	url = {https://doi.org/10.1145/3532612},
	doi = {10.1145/3532612},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GuF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-view clustering, which aims at boosting the clustering performance by leveraging the individual information and the common information of multi-view data, has gained extensive consideration in recent years. However, most existing multi-view clustering algorithms either focus on extracting the multi-view individuality or emphasize on exploring the multi-view commonality, neither of which can fully utilize the comprehensive information from multiple views. To this end, we propose a novel algorithm named View-specific and Consensus Graph Alignment (VCGA) for multi-view clustering, which simultaneously formulates the multi-view individuality and the multi-view commonality into a unified framework to effectively partition data points. To be specific, the VCGA model constructs the view-specific graphs and the shared graph from original multi-view data and hidden latent representation, respectively. Furthermore, the view-specific graphs of different views and the consensus graph are aligned into an informative target graph, which is employed as a crucial input to the standard spectral clustering method for clustering. Extensive experimental results on six benchmark datasets demonstrate the superiority of our method against other state-of-the-art clustering algorithms.}
}


@article{DBLP:journals/tkdd/ChakrabortyDC23,
	author = {Roshni Chakraborty and
                  Ritwika Das and
                  Joydeep Chandra},
	title = {SigGAN: Adversarial Model for Learning Signed Relationships in Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {8:1--8:20},
	year = {2023},
	url = {https://doi.org/10.1145/3532610},
	doi = {10.1145/3532610},
	timestamp = {Sun, 16 Apr 2023 20:31:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChakrabortyDC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Signed link prediction in graphs is an important problem that has applications in diverse domains. It is a binary classification problem that predicts whether an edge between a pair of nodes is positive or negative. Existing approaches for link prediction in unsigned networks cannot be directly applied for signed link prediction due to their inherent differences. Furthermore, signed link prediction must consider the inherent characteristics of signed networks, such as structural balance theory. Recent signed link prediction approaches generate node representations using either generative models or discriminative models. Inspired by the recent success of Generative Adversarial Network (GAN) based models in several applications, we propose a GAN based model for signed networks, SigGAN. It considers the inherent characteristics of signed networks, such as integration of information from negative edges, high imbalance in number of positive and negative edges, and structural balance theory. Comparing the performance with state-of-the-art techniques on five real-world datasets validates the effectiveness of SigGAN.}
}


@article{DBLP:journals/tkdd/LiFYJYSJL23,
	author = {Fuxian Li and
                  Jie Feng and
                  Huan Yan and
                  Guangyin Jin and
                  Fan Yang and
                  Funing Sun and
                  Depeng Jin and
                  Yong Li},
	title = {Dynamic Graph Convolutional Recurrent Network for Traffic Prediction:
                  Benchmark and Solution},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {9:1--9:21},
	year = {2023},
	url = {https://doi.org/10.1145/3532611},
	doi = {10.1145/3532611},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiFYJYSJL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph. As far as we know, we are first to employ a generation method to model fine topology of dynamic graph at each time step. Furthermore, to enhance efficiency and performance, we employ a training strategy for DGCRN by restricting the iteration number of decoder during forward and backward propagation. Finally, a reproducible standardized benchmark and a brand new representative traffic dataset are opened for fair comparison and further research. Extensive experiments on three datasets demonstrate that our model outperforms 15 baselines consistently. Source codes are available at https://github.com/tsinghua-fib-lab/Traffic-Benchmark.}
}


@article{DBLP:journals/tkdd/HuangGWY23,
	author = {Gengsen Huang and
                  Wensheng Gan and
                  Jian Weng and
                  Philip S. Yu},
	title = {US-Rule: Discovering Utility-driven Sequential Rules},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {10:1--10:22},
	year = {2023},
	url = {https://doi.org/10.1145/3532613},
	doi = {10.1145/3532613},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangGWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Utility-driven mining is an important task in data science and has many applications in real life. High-utility sequential pattern mining (HUSPM) is one kind of utility-driven mining. It aims at discovering all sequential patterns with high utility. However, the existing algorithms of HUSPM can not provide a relatively accurate probability to deal with some scenarios for prediction or recommendation. High-utility sequential rule mining (HUSRM) is proposed to discover all sequential rules with high utility and high confidence. There is only one algorithm proposed for HUSRM, which is not efficient enough. In this article, we propose a faster algorithm called US-Rule, to efficiently mine high-utility sequential rules. It utilizes the rule estimated utility co-occurrence pruning strategy (REUCP) to avoid meaningless computations. Moreover, to improve its efficiency on dense and long sequence datasets, four tighter upper bounds (LEEU, REEU, LERSU, and RERSU) and corresponding pruning strategies (LEEUP, REEUP, LERSUP, and RERSUP) are designed. US-Rule also proposes the rule estimated utility recomputing pruning strategy (REURP) to deal with sparse datasets. Finally, a large number of experiments on different datasets compared to the state-of-the-art algorithm demonstrate that US-Rule can achieve better performance in terms of execution time, memory consumption, and scalability.}
}


@article{DBLP:journals/tkdd/WangWGHY23,
	author = {Jiapu Wang and
                  Boyue Wang and
                  Junbin Gao and
                  Yongli Hu and
                  Baocai Yin},
	title = {Multi-Concept Representation Learning for Knowledge Graph Completion},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {11:1--11:19},
	year = {2023},
	url = {https://doi.org/10.1145/3533017},
	doi = {10.1145/3533017},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangWGHY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge Graph Completion (KGC) aims at inferring missing entities or relations by embedding them in a low-dimensional space. However, most existing KGC methods generally fail to handle the complex concepts hidden in triplets, so the learned embeddings of entities or relations may deviate from the true situation. In this article, we propose a novel Multi-concept Representation Learning (McRL) method for the KGC task, which mainly consists of a multi-concept representation module, a deep residual attention module, and an interaction embedding module. Specifically, instead of the single-feature representation, the multi-concept representation module projects each entity or relation to multiple vectors to capture the complex conceptual information hidden in them. The deep residual attention module simultaneously explores the inter- and intra-connection between entities and relations to enhance the entity and relation embeddings corresponding to the current contextual situation. Moreover, the interaction embedding module further weakens the noise and ambiguity to obtain the optimal and robust embeddings. We conduct the link prediction experiment to evaluate the proposed method on several standard datasets, and experimental results show that the proposed method outperforms existing state-of-the-art KGC methods.}
}


@article{DBLP:journals/tkdd/KirielleCR23,
	author = {Nishadi Kirielle and
                  Peter Christen and
                  Thilina Ranbaduge},
	title = {Unsupervised Graph-Based Entity Resolution for Complex Entities},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {12:1--12:30},
	year = {2023},
	url = {https://doi.org/10.1145/3533016},
	doi = {10.1145/3533016},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KirielleCR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entity resolution (ER) is the process of linking records that refer to the same entity. Traditionally, this process compares attribute values of records to calculate similarities and then classifies pairs of records as referring to the same entity or not based on these similarities. Recently developed graph-based ER approaches combine relationships between records with attribute similarities to improve linkage quality. Most of these approaches only consider databases containing basic entities that have static attribute values and static relationships, such as publications in bibliographic databases. In contrast, temporal record linkage addresses the problem where attribute values of entities can change over time. However, neither existing graph-based ER nor temporal record linkage can achieve high linkage quality on databases with complex entities, where an entity (such as a person) can change its attribute values over time while having different relationships with other entities at different points in time. In this article, we propose an unsupervised graph-based ER framework that is aimed at linking records of complex entities. Our framework provides five key contributions. First, we propagate positive evidence encountered when linking records to use in subsequent links by propagating attribute values that have changed. Second, we employ negative evidence by applying temporal and link constraints to restrict which candidate record pairs to consider for linking. Third, we leverage the ambiguity of attribute values to disambiguate similar records that, however, belong to different entities. Fourth, we adaptively exploit the structure of relationships to link records that have different relationships. Fifth, using graph measures, we refine matched clusters of records by removing likely wrong links between records. We conduct extensive experiments on seven real-world datasets from different domains showing that on average our unsupervised graph-based ER framework can improve precision by up to 25% and recall by up to 29% compared to several state-of-the-art ER techniques.}
}


@article{DBLP:journals/tkdd/KwonL23,
	author = {Soonki Kwon and
                  Younghoon Lee},
	title = {Explainability-Based Mix-Up Approach for Text Data Augmentation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {13:1--13:14},
	year = {2023},
	url = {https://doi.org/10.1145/3533048},
	doi = {10.1145/3533048},
	timestamp = {Sun, 16 Apr 2023 20:31:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KwonL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Text augmentation is a strategy for increasing the diversity of training examples without explicitly collecting new data. Owing to the efficiency and effectiveness of text augmentation, numerous augmentation methodologies have been proposed. Among them, the method based on modification, particularly the mix-up method of swapping words between two or more sentences, is widely used because it can be applied simply and shows good levels of performance. However, the existing mix-up approaches are limited; they do not reflect the importance of the manipulated word. That is, even if a word that has a critical effect on the classification result is manipulated, it is not considered significant in labeling the augmented data. Therefore, in this study, we propose an effective text augmentation technique that explicitly derives the importance of manipulated words and reflects this importance in the labeling of augmented data. The importance of each word, in other words, explainability, is calculated, and this is explicitly reflected in the labeling process of the augmented data. The results of the experiment confirmed that when the importance of the manipulated word was reflected in the labeling, the performance was significantly higher than that of the existing methods.}
}


@article{DBLP:journals/tkdd/LiWWX23,
	author = {Qian Li and
                  Xiangmeng Wang and
                  Zhichao Wang and
                  Guandong Xu},
	title = {Be Causal: De-Biasing Social Network Confounding in Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {14:1--14:23},
	year = {2023},
	url = {https://doi.org/10.1145/3533725},
	doi = {10.1145/3533725},
	timestamp = {Mon, 01 Jul 2024 14:32:36 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiWWX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recommendation systems, the existence of the missing-not-at-random (MNAR) problem results in the selection bias issue, degrading the recommendation performance ultimately. A common practice to address MNAR is to treat missing entries from the so-called “exposure” perspective, i.e., modeling how an item is exposed (provided) to a user. Most of the existing approaches use heuristic models or re-weighting strategy on observed ratings to mimic the missing-at-random setting. However, little research has been done to reveal how the ratings are missing from a causal perspective. To bridge the gap, we propose an unbiased and robust method called DENC (De-Bias Network Confounding in Recommendation), inspired by confounder analysis in causal inference. In general, DENC provides a causal analysis on MNAR from both the inherent factors (e.g., latent user or item factors) and auxiliary network’s perspective. Particularly, the proposed exposure model in DENC can control the social network confounder meanwhile preserve the observed exposure information. We also develop a deconfounding model through the balanced representation learning to retain the primary user and item features, which enables DENC generalize well on the rating prediction. Extensive experiments on three datasets validate that our proposed model outperforms the state-of-the-art baselines.}
}


@article{DBLP:journals/tkdd/LuoWWCDHH23,
	author = {Xiao Luo and
                  Haixin Wang and
                  Daqing Wu and
                  Chong Chen and
                  Minghua Deng and
                  Jianqiang Huang and
                  Xian{-}Sheng Hua},
	title = {A Survey on Deep Hashing Methods},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {1},
	pages = {15:1--15:50},
	year = {2023},
	url = {https://doi.org/10.1145/3532624},
	doi = {10.1145/3532624},
	timestamp = {Sat, 29 Apr 2023 19:27:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LuoWWCDHH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.}
}


@article{DBLP:journals/tkdd/NguyenLA23,
	author = {Hung T. Nguyen and
                  Pierre J. Liang and
                  Leman Akoglu},
	title = {Detecting Anomalous Graphs in Labeled Multi-Graph Databases},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {16:1--16:25},
	year = {2023},
	url = {https://doi.org/10.1145/3533770},
	doi = {10.1145/3533770},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NguyenLA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within a large database 𝒢 containing graphs with labeled nodes and directed, multi-edges; how can we detect the anomalous graphs? Most existing work are designed for plain (unlabeled) and/or simple (unweighted) graphs. We introduce CODEtect, the first approach that addresses the anomaly detection task for graph databases with such complex nature. To this end, it identifies a small representative set 𝒮 of structural patterns (i.e., node-labeled network motifs) that losslessly compress database 𝒢 as concisely as possible. Graphs that do not compress well are flagged as anomalous. CODEtect exhibits two novel building blocks: (i) a motif-based lossless graph encoding scheme, and (ii) fast memory-efficient search algorithms for 𝒮. We show the effectiveness of CODEtect on transaction graph databases from three different corporations and statistically similar synthetic datasets, where existing baselines adjusted for the task fall behind significantly, across different types of anomalies and performance metrics.}
}


@article{DBLP:journals/tkdd/LuF23,
	author = {Xun Lu and
                  Songhe Feng},
	title = {Structure Diversity-Induced Anchor Graph Fusion for Multi-View Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {17:1--17:18},
	year = {2023},
	url = {https://doi.org/10.1145/3534931},
	doi = {10.1145/3534931},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LuF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The anchor graph structure has been widely used to speed up large-scale multi-view clustering and exhibited promising performance. How to effectively integrate the anchor graphs on multiple views to achieve enhanced clustering performance still remains a challenging task. Existing fusing strategies ignore the structure diversity among anchor graphs and restrict the anchor generation to be same on different views, which degenerates the representation ability of corresponding fused consensus graph. To overcome these drawbacks, we propose a novel structural fusion framework to integrate the multi-view anchor graphs for clustering. Different from traditional integration strategies, we merge the anchors and edges of all the view-specific anchor graphs into a single graph for the structural optimal graph learning. Benefiting from the structural fusion strategy, the anchor generation of each view is not forced to be same, which greatly improves the representation capability of the target structural optimal graph, since the anchors of each view capture the diverse structure of different views. By leveraging the potential structural consistency among each anchor graph, a connectivity constraint is imposed on the target graph to indicate clusters directly without any post-processing such as k-means in classical spectral clustering. Substantial experiments on real-world datasets are conducted to verify the superiority of the proposed method, as compared with the state-of-the-arts over the clustering performance and time expenditure.}
}


@article{DBLP:journals/tkdd/WangDLHHCF23,
	author = {Lichen Wang and
                  Zhengming Ding and
                  Kasey Lee and
                  Seungju Han and
                  Jae{-}Joon Han and
                  Changkyu Choi and
                  Yun Fu},
	title = {Generative Multi-Label Correlation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {18:1--18:19},
	year = {2023},
	url = {https://doi.org/10.1145/3538708},
	doi = {10.1145/3538708},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangDLHHCF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world applications, a single instance could have more than one label. To solve this task, multi-label learning methods emerged in recent years. It is a more challenging problem for many reasons, such as complex label correlation, long-tail label distribution, and data shortage. In general, overcoming these challenges and bettering learning performance could be achieved by utilizing more training samples and including label correlations. However, these solutions are expensive and inflexible. Large-scale, well-labeled datasets are difficult to obtain, and building label correlation maps requires task-specific semantic information as prior knowledge. To address these limitations, we propose a general and compact Multi-Label Correlation Learning (MUCO) framework. MUCO explicitly and effectively learns the latent label correlations by updating a label correlation tensor, which provides highly accurate and interpretable prediction results. In addition, a multi-label generative strategy is deployed to handle the long-tail label distribution challenge. It borrows the visual clues from limited samples and synthesizes more diverse samples. All networks in our model are optimized simultaneously. Extensive experiments illustrate the effectiveness and efficiency of MUCO. Ablation studies further prove the effectiveness of all the modules.}
}


@article{DBLP:journals/tkdd/LuFLJL23,
	author = {Xun Lu and
                  Songhe Feng and
                  Gengyu Lyu and
                  Yi Jin and
                  Congyan Lang},
	title = {Distance-Preserving Embedding Adaptive Bipartite Graph Multi-View
                  Learning with Application to Multi-Label Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {19:1--19:21},
	year = {2023},
	url = {https://doi.org/10.1145/3537900},
	doi = {10.1145/3537900},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LuFLJL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based multi-view learning has attracted much attention due to the efficacy of fusing the information from different views. However, most of them exhibit high computational complexity. We propose an anchor-based bipartite graph embedding approach to accelerate the learning process. Specifically, different from existing anchor-based methods where anchors are obtained from key samples by clustering or weighted averaging strategies, in this article, the anchors are learned in a principled fashion which aims at constructing a distance-preserving embedding for each view from samples to their representations, whose elements are the weights of the edges linking corresponding samples and anchors. In addition, the consistency among different views can be explored by imposing a low-rank constraint on the concatenated embedding representations. We further design a concise yet effective feature collinearity guided feature selection scheme to learn tight multi-label classifiers. The objective function is optimized in an alternating optimization fashion. Both theoretical analysis and experimental results on different multi-label image datasets verify the effectiveness and efficiency of the proposed method.}
}


@article{DBLP:journals/tkdd/WangGCYL23,
	author = {Qianru Wang and
                  Bin Guo and
                  Lu Cheng and
                  Zhiwen Yu and
                  Huan Liu},
	title = {CausalSE: Understanding Varied Spatial Effects with Missing Data Toward
                  Adding New Bike-sharing Stations},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {20:1--20:24},
	year = {2023},
	url = {https://doi.org/10.1145/3536427},
	doi = {10.1145/3536427},
	timestamp = {Mon, 28 Aug 2023 21:37:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangGCYL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the growing bike-sharing demands and make people’s travel convenient, the companies need to add new stations at locations where demands exceed supply. Before making reliable decisions on adding new stations, it is required to understand the spatial effects of new stations on the station network. In this article, we study the deployment of the new station by estimating its varied causal effects on the demands of nearby stations, e.g., how does adding a new station (treatment) causally influence the demands (outcome) of nearby stations? When working with observational data, we should control hidden confounders, which cause spurious relations between treatments and outcomes. However, previous studies use historical data of the individual unit (e.g., the station’s historical demands) to approximate its hidden confounders, which cannot deal with the lack of historical data for new stations. And the conventional methods overlook the differences between units, which cannot be applied to our problem. To overcome the challenges, we propose a novel model (CausalSE) to estimate the varied effects of new stations on nearby stations, which uses the shared knowledge (i.e., similar traveling patterns among stations) to approximate hidden confounders. Experimental results on real-world datasets show that CausalSE outperforms six state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/JerezZS23,
	author = {Carlos Ivan Jerez and
                  Jun Zhang and
                  Marcia R. Silva},
	title = {On Equivalence of Anomaly Detection Algorithms},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {21:1--21:26},
	year = {2023},
	url = {https://doi.org/10.1145/3536428},
	doi = {10.1145/3536428},
	timestamp = {Sat, 27 May 2023 15:23:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JerezZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In most domains, anomaly detection is typically cast as an unsupervised learning problem because of the infeasibility of labeling large datasets. In this setup, the evaluation and comparison of different anomaly detection algorithms is difficult. Although some work has been published in this field, they fail to account that different algorithms can detect different kinds of anomalies. More precisely, the literature on this topic has focused on defining criteria to determine which algorithm is better, while ignoring the fact that such criteria are meaningful only if the algorithms being compared are detecting the same kind of anomalies. Therefore, in this article, we propose an equivalence criterion for anomaly detection algorithms that measures to what degree two anomaly detection algorithms detect the same kind of anomalies. First, we lay out a set of desirable properties that such an equivalence criterion should have and why; second, we propose Gaussian Equivalence Criterion (GEC) as equivalence criterion and show mathematically that it has the desirable properties previously mentioned. Finally, we empirically validate these properties using a simulated and a real-world dataset. For the real-world dataset, we show how GEC can provide insight about the anomaly detection algorithms as well as the dataset.}
}


@article{DBLP:journals/tkdd/FrerisAV23,
	author = {Nikolaos M. Freris and
                  Ahmad Ajalloeian and
                  Michalis Vlachos},
	title = {Interpretable Embedding and Visualization of Compressed Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {22:1--22:22},
	year = {2023},
	url = {https://doi.org/10.1145/3537901},
	doi = {10.1145/3537901},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/FrerisAV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional embedding methodologies, also known as dimensionality reduction techniques, assume the availability of exact pairwise distances between the high-dimensional objects that will be embedded in a lower dimensionality. In this article, we propose an embedding that overcomes this limitation and can operate on pairwise distances that are represented as a range of lower and upper bounds. Such bounds are typically estimated when objects are compressed in a lossy manner, so our approach is highly applicable in the case of big compressed datasets. Our methodology can preserve multiple aspects of the original data relationships: distances, correlations, and object scores/ranks, whereas existing techniques typically preserve only distances. Comparative experiments with prevalent embedding methodologies (ISOMAP, t-SNE, MDS, UMAP) illustrate that our approach can provide fidelitous preservation of multiple object relationships, even in the presence of inexact distance information. Our visualization method is also easily interpretable.}
}


@article{DBLP:journals/tkdd/WangPW23,
	author = {Shaokang Wang and
                  Li Pan and
                  Yu Wu},
	title = {Meta-Information Fusion of Hierarchical Semantics Dependency and Graph
                  Structure for Structured Text Classification},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {23:1--23:18},
	year = {2023},
	url = {https://doi.org/10.1145/3537971},
	doi = {10.1145/3537971},
	timestamp = {Mon, 28 Aug 2023 21:37:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangPW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Structured text with plentiful hierarchical structure information is an important part in real-world complex texts. Structured text classification is attracting more attention in natural language processing due to the increasing complexity of application scenarios. Most existing methods treat structured text from a local hierarchy perspective, focusing on the semantics dependency and the graph structure of the structured text independently. However, structured text has global hierarchical structures with sophisticated dependency when compared to unstructured text. According to the variety of structured texts, it is not appropriate to use the existing methods directly. The function of distinction information within semantics dependency and graph structure for structured text, referred to as meta-information, should be stated more precisely. In this article, we propose HGMETA, a novel meta-information embedding frame network for structured text classification, to obtain the fusion embedding of hierarchical semantics dependency and graph structure in a structured text, and to distill the meta-information from fusion characteristics. To integrate the global hierarchical features with fused structured text information, we design a hierarchical LDA module and a structured text embedding module. Specially, we employ a multi-hop message passing mechanism to explicitly incorporate complex dependency into a meta-graph. The meta-information is constructed from meta-graph via neighborhood-based propagation to distill redundant information. Furthermore, using an attention-based network, we investigate the complementarity of semantics dependency and graph structure based on global hierarchical characteristics and meta-information. Finally, the fusion embedding and the meta-information can be straightforwardly incorporated for structured text classification. Experiments conducted on three real-world datasets show the effectiveness of meta-information and demonstrate the superiority of our method.}
}


@article{DBLP:journals/tkdd/ZhuHSL23,
	author = {Xuliang Zhu and
                  Xin Huang and
                  Longxu Sun and
                  Jiming Liu},
	title = {A Novel Graph Indexing Approach for Uncovering Potential {COVID-19}
                  Transmission Clusters},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {24:1--24:24},
	year = {2023},
	url = {https://doi.org/10.1145/3538492},
	doi = {10.1145/3538492},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuHSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The COVID-19 pandemic has caused the society lockdowns and a large number of deaths in many countries. Potential transmission cluster discovery is to find all suspected users with infections, which is greatly needed to fast discover virus transmission chains so as to prevent an outbreak of COVID-19 as early as possible. In this article, we study the problem of potential transmission cluster discovery based on the spatio-temporal logs. Given a query of patient user q and a timestamp of confirmed infection tq, the problem is to find all potential infected users who have close social contacts to user q before time tq. We motivate and formulate the potential transmission cluster model, equipped with a detailed analysis of transmission cluster property and particular model usability. To identify potential clusters, one straightforward method is to compute all close contacts on-the-fly, which is simple but inefficient caused by scanning spatio-temporal logs many times. To accelerate the efficiency, we propose two indexing algorithms by constructing a multigraph index and an advanced BCG-index. Leveraging two well-designed techniques of spatio-temporal compression and graph partition on bipartite contact graphs, our BCG-index approach achieves a good balance of index construction and online query processing to fast discover potential transmission cluster. We theoretically analyze and compare the algorithm complexity of three proposed approaches. Extensive experiments on real-world check-in datasets and COVID-19 confirmed cases in the United States validate the effectiveness and efficiency of our potential transmission cluster model and algorithms.}
}


@article{DBLP:journals/tkdd/AtyabiSJFBKLVC23,
	author = {Adham Atyabi and
                  Frederick Shic and
                  Jiajun Jiang and
                  Claire E. Foster and
                  Erin Barney and
                  Minah Kim and
                  Beibin Li and
                  Pamela Ventola and
                  Chung{-}Hao Chen},
	title = {Stratification of Children with Autism Spectrum Disorder Through Fusion
                  of Temporal Information in Eye-gaze Scan-Paths},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {25:1--25:20},
	year = {2023},
	url = {https://doi.org/10.1145/3539226},
	doi = {10.1145/3539226},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/AtyabiSJFBKLVC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Background: Looking pattern differences are shown to separate individuals with Autism Spectrum Disorder (ASD) and Typically Developing (TD) controls. Recent studies have shown that, in children with ASD, these patterns change with intellectual and social impairments, suggesting that patterns of social attention provide indices of clinically meaningful variation in ASD. Method: We conducted a naturalistic study of children with ASD (n = 55) and typical development (TD, n = 32). A battery of eye-tracking video stimuli was used in the study, including Activity Monitoring (AM), Social Referencing (SR), Theory of Mind (ToM), and Dyadic Bid (DB) tasks. This work reports on the feasibility of spatial and spatiotemporal scanpaths generated from eye-gaze patterns of these paradigms in stratifying ASD and TD groups. Algorithm: This article presents an approach for automatically identifying clinically meaningful information contained within the raw eye-tracking data of children with ASD and TD. The proposed mechanism utilizes combinations of eye-gaze scan-paths (spatial information), fused with temporal information and pupil velocity data and Convolutional Neural Network (CNN) for stratification of diagnosis (ASD or TD). Results: Spatial eye-gaze representations in the form of scanpaths in stratifying ASD and TD (ASD vs. TD: DNN: 74.4%) are feasible. These spatial eye-gaze features, e.g., scan-paths, are shown to be sensitive to factors mediating heterogeneity in ASD: age (ASD: 2–4 y/old vs. 10–17 y/old CNN: 80.5%), gender (Male vs. Female ASD: DNN: 78.0%) and the mixture of age and gender (5–9 y/old Male vs. 5–9 y/old Female ASD: DNN:98.8%). Limiting scan-path representations temporally increased variance in stratification performance, attesting to the importance of the temporal dimension of eye-gaze data. Spatio-Temporal scan-paths that incorporate velocity of eye movement in their images of eye-gaze are shown to outperform other feature representation methods achieving classification accuracy of 80.25%. Conclusion: The results indicate the feasibility of scan-path images to stratify ASD and TD diagnosis in children of varying ages and gender. Infusion of temporal information and velocity data improves the classification performance of our deep learning models. Such novel velocity fused spatio-temporal scan-path features are shown to be able to capture eye gaze patterns that reflect age, gender, and the mixed effect of age and gender, factors that are associated with heterogeneity in ASD and difficulty in identifying robust biomarkers for ASD.}
}


@article{DBLP:journals/tkdd/ChenRMKE23,
	author = {Hongjie Chen and
                  Ryan A. Rossi and
                  Kanak Mahadik and
                  Sungchul Kim and
                  Hoda Eldardiry},
	title = {Graph Deep Factors for Probabilistic Time-series Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {26:1--26:30},
	year = {2023},
	url = {https://doi.org/10.1145/3543511},
	doi = {10.1145/3543511},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenRMKE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Effective time-series forecasting methods are of significant importance to solve a broad spectrum of research problems. Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, a relational global model learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, a relational local model not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5% on average. Furthermore, we target addressing the common nature of many time-series forecasting applications where time-series are provided in a streaming version; however, most methods fail to leverage the newly incoming time-series values and result in worse performance over time. In this article, we propose an online incremental learning framework for probabilistic forecasting. The framework is theoretically proven to have lower time and space complexity. The framework can be universally applied to many other machine learning-based methods.}
}


@article{DBLP:journals/tkdd/JhaRCSR23,
	author = {Akshita Jha and
                  Vineeth Rakesh and
                  Jaideep Chandrashekar and
                  Adithya Samavedhi and
                  Chandan K. Reddy},
	title = {Supervised Contrastive Learning for Interpretable Long-Form Document
                  Matching},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {27:1--27:17},
	year = {2023},
	url = {https://doi.org/10.1145/3542822},
	doi = {10.1145/3542822},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JhaRCSR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of “similarity”), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE: Contrastive Long Document Encoder—a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE.}
}


@article{DBLP:journals/tkdd/VajiacLKLPOJRF23,
	author = {Catalina Vajiac and
                  Meng{-}Chieh Lee and
                  Aayushi Kulshrestha and
                  Sacha Levy and
                  Namyong Park and
                  Andreas M. Olligschlaeger and
                  Cara Jones and
                  Reihaneh Rabbany and
                  Christos Faloutsos},
	title = {DeltaShield: Information Theory for Human- Trafficking Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {28:1--28:27},
	year = {2023},
	url = {https://doi.org/10.1145/3563040},
	doi = {10.1145/3563040},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/VajiacLKLPOJRF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a million escort advertisements, how can we spot near-duplicates? Such micro-clusters of ads are usually signals of human trafficking (HT). How can we summarize them to convince law enforcement to act? Spotting micro-clusters of near-duplicate documents is useful in multiple, additional settings, including spam-bot detection in Twitter ads, plagiarism, and more. We present InfoShield, which makes the following contributions: practical, being scalable and effective on real data; parameter-free and principled, requiring no user-defined parameters; interpretable, finding a document to be the cluster representative, highlighting all the common phrases, and automatically detecting “slots” (i.e., phrases that differ in every document); and generalizable, beating or matching domain-specific methods in Twitter bot detection and HT detection, respectively, as well as being language independent. Interpretability is particularly important for the anti-HT domain, where law enforcement must visually inspect ads. Our experiments on real data show that InfoShield correctly identifies Twitter bots with an F1 score over 90% and detects HT ads with 84% precision. Moreover, it is scalable, requiring about 8 hours for 4 million documents on a stock laptop. Our incremental version, DeltaShield, allows for fast, incremental updates, with minor loss of accuracy.}
}


@article{DBLP:journals/tkdd/SunYXZ23,
	author = {Jianhui Sun and
                  Ying Yang and
                  Guangxu Xun and
                  Aidong Zhang},
	title = {Scheduling Hyperparameters to Improve Generalization: From Centralized
                  {SGD} to Asynchronous {SGD}},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {29:1--29:37},
	year = {2023},
	url = {https://doi.org/10.1145/3544782},
	doi = {10.1145/3544782},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunYXZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article1 studies how to schedule hyperparameters to improve generalization of both centralized single-machine stochastic gradient descent (SGD) and distributed asynchronous SGD (ASGD). SGD augmented with momentum variants (e.g., heavy ball momentum (SHB) and Nesterov’s accelerated gradient (NAG)) has been the default optimizer for many tasks, in both centralized and distributed environments. However, many advanced momentum variants, despite empirical advantage over classical SHB/NAG, introduce extra hyperparameters to tune. The error-prone tuning is the main barrier for AutoML. Centralized SGD: We first focus on centralized single-machine SGD and show how to efficiently schedule the hyperparameters of a large class of momentum variants to improve generalization. We propose a unified framework called multistage quasi-hyperbolic momentum (Multistage QHM), which covers a large family of momentum variants as its special cases (e.g., vanilla SGD/SHB/NAG). Existing works mainly focus on only scheduling learning rate α’s decay, while multistage QHM allows additional varying hyperparameters (e.g., momentum factor), and demonstrates better generalization than only tuning α. We show the convergence of multistage QHM for general non-convex objectives. Distributed SGD: We then extend our theory to distributed asynchronous SGD (ASGD), in which a parameter server distributes data batches to several worker machines and updates parameters via aggregating batch gradients from workers. We quantify the asynchrony between different workers (i.e., gradient staleness), model the dynamics of asynchronous iterations via a stochastic differential equation (SDE), and then derive a PAC-Bayesian generalization bound for ASGD. As a byproduct, we show how a moderately large learning rate helps ASGD to generalize better. Our tuning strategies have rigorous justifications rather than a blind trial-and-error as we theoretically prove why our tuning strategies could decrease our derived generalization errors in both cases. Our strategies simplify the tuning process and beat competitive optimizers in test accuracy empirically. Our codes are publicly available https://github.com/jsycsjh/centralized-asynchronous-tuning.}
}


@article{DBLP:journals/tkdd/MiaoPGZY23,
	author = {Xiaoye Miao and
                  Huanhuan Peng and
                  Yunjun Gao and
                  Zongfu Zhang and
                  Jianwei Yin},
	title = {On Dynamically Pricing Crowdsourcing Tasks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {2},
	pages = {30:1--30:27},
	year = {2023},
	url = {https://doi.org/10.1145/3544018},
	doi = {10.1145/3544018},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/MiaoPGZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers’ quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester’s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.}
}


@article{DBLP:journals/tkdd/KamhouaZMCLH23,
	author = {Barakeel Fanseu Kamhoua and
                  Lin Zhang and
                  Kaili Ma and
                  James Cheng and
                  Bo Li and
                  Bo Han},
	title = {{GRACE:} {A} General Graph Convolution Framework for Attributed Graph
                  Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {31:1--31:31},
	year = {2023},
	url = {https://doi.org/10.1145/3544977},
	doi = {10.1145/3544977},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KamhouaZMCLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Attributed graph clustering (AGC) is an important problem in graph mining as more and more complex data in real-world have been represented in graphs with attributed nodes. While it is a common practice to leverage both attribute and structure information for improved clustering performance, most existing AGC algorithms consider only a specific type of relations, which hinders their applicability to integrate various complex relations into node attributes for AGC. In this article, we propose GRACE, an extended graph convolution framework for AGC tasks. Our framework provides a general and interpretative solution for clustering many different types of attributed graphs, including undirected, directed, heterogeneous and hyper attributed graphs. By building suitable graph Laplacians for each of the aforementioned graph types, GRACE can seamlessly perform graph convolution on node attributes to fuse all available information for clustering. We conduct extensive experiments on 14 real-world datasets of four different graph types. The experimental results show that GRACE outperforms the state-of-the-art AGC methods on the different graph types in terms of clustering quality, time, and memory usage.}
}


@article{DBLP:journals/tkdd/ZhaiZ23,
	author = {Penglong Zhai and
                  Shihua Zhang},
	title = {Learnable Graph-Regularization for Matrix Decomposition},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {32:1--32:20},
	year = {2023},
	url = {https://doi.org/10.1145/3544781},
	doi = {10.1145/3544781},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaiZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-rank approximation models of data matrices have become important machine learning and data mining tools in many fields, including computer vision, text mining, bioinformatics, and many others. They allow for embedding high-dimensional data into low-dimensional spaces, which mitigates the effects of noise and uncovers latent relations. In order to make the learned representations inherit the structures in the original data, graph-regularization terms are often added to the loss function. However, the prior graph construction often fails to reflect the true network connectivity and the intrinsic relationships. In addition, many graph-regularized methods fail to take the dual spaces into account. Probabilistic models are often used to model the distribution of the representations, but most of previous methods often assume that the hidden variables are independent and identically distributed for simplicity. To this end, we propose a learnable graph-regularization model for matrix decomposition (LGMD), which builds a bridge between graph-regularized methods and probabilistic matrix decomposition models for the first time. LGMD incorporates two graphical structures (i.e., two precision matrices) learned in an iterative manner via sparse precision matrix estimation and is more robust to noise and missing entries. Extensive numerical results and comparison with competing methods demonstrate its effectiveness.}
}


@article{DBLP:journals/tkdd/ChenZZYJL23,
	author = {Jinwei Chen and
                  Zefang Zong and
                  Yunlin Zhuang and
                  Huan Yan and
                  Depeng Jin and
                  Yong Li},
	title = {Reinforcement Learning for Practical Express Systems with Mixed Deliveries
                  and Pickups},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {33:1--33:19},
	year = {2023},
	url = {https://doi.org/10.1145/3546952},
	doi = {10.1145/3546952},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenZZYJL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In real-world express systems, couriers need to satisfy not only the delivery demands but also the pick-up demands of customers. Delivery and pickup tasks are usually mixed together within integrated routing plans. Such a mixed routing problem can be abstracted and formulated as Vehicle Routing Problem with Mixed Delivery and Pickup (VRPMDP), which is an NP-hard combinatorial optimization problem. To solve VRPMDP, there are three major challenges as below. (a) Even though successive pickup and delivery tasks are independent to accomplish, the inter-influence between choosing pickup task or delivery task to deal with still exists. (b) Due to the two-way flow of goods between the depot and customers, the loading rate of vehicles leaving the depot affects routing decisions. (c) The proportion of deliveries and pickups will change due to the complex demand situation in real-world scenarios, which requires robustness of the algorithm. To solve the challenges above, we design an encoder-decoder based framework to generate high-quality and robust VRPMDP solutions. First, we consider a VRPMDP instance as a graph and utilize a GNN encoder to extract the feature of the instance effectively. The detailed routing solutions are further decoded as a sequence by the decoder with attention mechanism. Second, we propose a Coordinated Decision of Loading and Routing (CDLR) mechanism to determine the loading rate dynamically after the vehicle returns to the depot, thus avoiding the influence of improper loading rate settings. Finally, the model equipped with a GNN encoder and CDLR simultaneously can adapt to the changes in the proportion of deliveries and pickups. We conduct the experiments to demonstrate the effectiveness of our model. The experiments show that our method achieves desirable results and generalization ability.}
}


@article{DBLP:journals/tkdd/FengSXL23,
	author = {Tao Feng and
                  Sirui Song and
                  Tong Xia and
                  Yong Li},
	title = {Contact Tracing and Epidemic Intervention via Deep Reinforcement Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {34:1--34:24},
	year = {2023},
	url = {https://doi.org/10.1145/3546870},
	doi = {10.1145/3546870},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/FengSXL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent outbreak of COVID-19 poses a serious threat to people’s lives. Epidemic control strategies have also caused damage to the economy by cutting off humans’ daily commute. In this article, we develop an Individual-based Reinforcement Learning Epidemic Control Agent (IDRLECA) to search for smart epidemic control strategies that can simultaneously minimize infections and the cost of mobility intervention. IDRLECA first hires an infection probability model to calculate the current infection probability of each individual. Then, the infection probabilities together with individuals’ health status and movement information are fed to a novel GNN to estimate the spread of the virus through human contacts. The estimated risks are used to further support an RL agent to select individual-level epidemic-control actions. The training of IDRLECA is guided by a specially designed reward function considering both the cost of mobility intervention and the effectiveness of epidemic control. Moreover, we design a constraint for control-action selection that eases its difficulty and further improve exploring efficiency. Extensive experimental results demonstrate that IDRLECA can suppress infections at a very low level and retain more than 95% of human mobility.}
}


@article{DBLP:journals/tkdd/WanZLZ23,
	author = {Mingyang Wan and
                  Daochen Zha and
                  Ninghao Liu and
                  Na Zou},
	title = {In-Processing Modeling Techniques for Machine Learning Fairness: {A}
                  Survey},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {35:1--35:27},
	year = {2023},
	url = {https://doi.org/10.1145/3551390},
	doi = {10.1145/3551390},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WanZLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.}
}


@article{DBLP:journals/tkdd/LongBTSW23,
	author = {Qiang Long and
                  Adil M. Bagirov and
                  Sona Taheri and
                  Nargiz Sultanova and
                  Xue Wu},
	title = {Methods and Applications of Clusterwise Linear Regression: {A} Survey
                  and Comparison},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {36:1--36:54},
	year = {2023},
	url = {https://doi.org/10.1145/3550074},
	doi = {10.1145/3550074},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LongBTSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clusterwise linear regression (CLR) is a well-known technique for approximating a data using more than one linear function. It is based on the combination of clustering and multiple linear regression methods. This article provides a comprehensive survey and comparative assessments of CLR including model formulations, description of algorithms, and their performance on small to large-scale synthetic and real-world datasets. Some applications of the CLR algorithms and possible future research directions are also discussed.}
}


@article{DBLP:journals/tkdd/WuCLLLLW23,
	author = {Youxi Wu and
                  Mingjie Chen and
                  Yan Li and
                  Jing Liu and
                  Zhao Li and
                  Jinyan Li and
                  Xindong Wu},
	title = {ONP-Miner: One-off Negative Sequential Pattern Mining},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {37:1--37:24},
	year = {2023},
	url = {https://doi.org/10.1145/3549940},
	doi = {10.1145/3549940},
	timestamp = {Mon, 16 Oct 2023 19:24:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuCLLLLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this article discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this article proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic.}
}


@article{DBLP:journals/tkdd/CarchioloGLMM23,
	author = {Vincenza Carchiolo and
                  Marco Grassia and
                  Alessandro Longheu and
                  Michele Malgeri and
                  Giuseppe Mangioni},
	title = {Efficient Node PageRank Improvement via Link-building using Geometric
                  Deep Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {38:1--38:22},
	year = {2023},
	url = {https://doi.org/10.1145/3551642},
	doi = {10.1145/3551642},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CarchioloGLMM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Centrality is a relevant topic in the field of network research, due to its various theoretical and practical implications. In general, all centrality metrics aim at measuring the importance of nodes (according to some definition of importance), and such importance scores are used to rank the nodes in the network, therefore the rank improvement is a strictly related topic. In a given network, the rank improvement is achieved by establishing new links, therefore the question shifts to which and how many links should be collected to get a desired rank. This problem, also known as link-building has been shown to be NP-hard, and most heuristics developed failed in obtaining good performance with acceptable computational complexity. In this article, we present LB–GDM, a novel approach that leverages Geometric Deep Learning to tackle the link-building problem. To validate our proposal, 31 real-world networks were considered; tests show that LB–GDM performs significantly better than the state-of-the-art heuristics, while having a comparable or even lower computational complexity, which allows it to scale well even to large networks.}
}


@article{DBLP:journals/tkdd/JiangCC23,
	author = {Linli Jiang and
                  Chaoxiong Chen and
                  Chao Chen},
	title = {{L2MM:} Learning to Map Matching with Deep Models for Low-Quality
                  {GPS} Trajectory Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {39:1--39:25},
	year = {2023},
	url = {https://doi.org/10.1145/3550486},
	doi = {10.1145/3550486},
	timestamp = {Mon, 28 Aug 2023 21:37:08 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiangCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Map matching is a fundamental research topic with the objective of aligning GPS trajectories to paths on the road network. However, existing models fail to achieve satisfactory performance for low-quality (i.e., noisy, low-frequency, and non-uniform) trajectory data. To this end, we propose a general and robust deep learning-based model, L2MM, to tackle these issues at all. First, high-quality representations of low-quality trajectories are learned by two representation enhancement methods, i.e., enhancement with high-frequency trajectories and enhancement with the data distribution. The former employs high-frequency trajectories to enhance the expressive capability of representations, while the latter regularizes the representation distribution over the latent space to improve the generalization ability of representations. Secondly, to embrace more heuristic clues, typical mobility patterns are recognized in the latent space and further incorporated into the map matching task. Finally, based on the available representations and patterns, a mapping from trajectories to corresponding paths is constructed through a joint optimization method. Extensive experiments are conducted based on a range of datasets, which demonstrate the superiority of L2MM and validate the significance of high-quality representations as well as mobility patterns.}
}


@article{DBLP:journals/tkdd/ZhangH23,
	author = {Yihong Zhang and
                  Takahiro Hara},
	title = {Explainable Integration of Social Media Background in a Dynamic Neural
                  Recommender},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {40:1--40:14},
	year = {2023},
	url = {https://doi.org/10.1145/3550279},
	doi = {10.1145/3550279},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems nowadays are commonly deployed in e-commerce platforms to help customers making purchase decisions. Dynamic recommender considers not only static user-item interaction data, but the temporal information at the time of recommendation. Previous researches have suggested to incorporate social media as the temporal information in dynamic neural recommenders after transforming them into embeddings. While such an approach can potentially improve recommendation performance, the effectiveness is difficult to explain. In this article, we propose an explainable method to integrate social media in a dynamic neural recommender. Our method applies association rule mining, which can generate human-understandable behavior patterns from social media and e-commerce platforms. With real-world social media and e-commerce data, we show that the integration can improve accuracy by up to 14% while using the same data. Moreover, we can explain the positive cases by examining relevant association rules.}
}


@article{DBLP:journals/tkdd/WangWZL23,
	author = {Yashen Wang and
                  Zhaoyu Wang and
                  Huanhuan Zhang and
                  Zhirun Liu},
	title = {Microblog Retrieval Based on Concept-Enhanced Pre-Training Model},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {41:1--41:32},
	year = {2023},
	url = {https://doi.org/10.1145/3552311},
	doi = {10.1145/3552311},
	timestamp = {Fri, 19 May 2023 17:30:15 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangWZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to conventional ad-hoc retrieval tasks over web pages and newswire articles. This article proposes a concept-enhanced pre-training model for microblog retrieval task, leveraging Semantic Matching Model (SMM) objective and Concept Correlation Model (CCM) objective. The proposed model is a novel neural ranking model specifically designed for ranking short-text microblog, which could merge the advantage of pre-training methodology for generating valid contextualized embedding with the superiority of the prior lexical knowledge (e.g., concept knowledge) for understanding short-text language semantic. We conduct experiments on widely used real-world datasets, and the experimental results demonstrate the efficiency of the proposed model, even compared with latest state-of-the-art neural-based models and pre-training based models.}
}


@article{DBLP:journals/tkdd/WeiLSJTY23,
	author = {Xuemei Wei and
                  Ye{-}Zheng Liu and
                  Jianshan Sun and
                  Yuanchun Jiang and
                  Qifeng Tang and
                  Kun Yuan},
	title = {Dual Subgraph-Based Graph Neural Network for Friendship Prediction
                  in Location-Based Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {42:1--42:28},
	year = {2023},
	url = {https://doi.org/10.1145/3554981},
	doi = {10.1145/3554981},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WeiLSJTY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the wide use of Location-Based Social Networks (LBSNs), predicting user friendship from online social relations and offline trajectory data is of great value to improve the platform service quality and user satisfaction. Existing methods mainly focus on some hand-crafted features or graph embedding models based on the user-location bipartite graph, which cannot precisely capture the latent mobility similarity for the majority of users who have no explicit co-visit behaviors and also fail to balance the tradeoff between social features and mobility features for friendship prediction. In this regard, we propose a dual subgraph-based pairwise graph neural network (DSGNN) for friendship prediction in LBSNs, which extracts a pairwise social subgraph and a trajectory subgraph to model the social proximity and mobility similarity, respectively. Specifically, to overcome the co-visit data sparsity, we design an entropy-based random walk to construct a location graph that captures the high-level correlation between locations. Based on this, we characterize the pairwise mobility similarity from trajectory level instead of location level, which is modeled by a graph neural network (GNN) on a labeled trajectory subgraph composed of the two trajectories of the target user pair. Besides, we also utilize another GNN to extract social proximity based on social subgraph of the target user pair. Finally, we propose a gate layer to adaptively balance the fusion of the social and mobility features for friendship prediction. We conduct extensive experiments on the real-world datasets and demonstrate the superiority of our approach, which outperforms other state-of-the-art methods. In particular, the comparative experiments on the trajectory level mobility similarity further validate the effectiveness of the designed trajectory subgraph-based method, which can extract predictive mobility features.}
}


@article{DBLP:journals/tkdd/JiangYHLWW23,
	author = {Xin Jiang and
                  Zhengxin Yu and
                  Chao Hai and
                  Hongbo Liu and
                  Xindong Wu and
                  Tom{\'{a}}s Ward},
	title = {DNformer: Temporal Link Prediction with Transfer Learning in Dynamic
                  Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {43:1--43:21},
	year = {2023},
	url = {https://doi.org/10.1145/3551892},
	doi = {10.1145/3551892},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiangYHLWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Temporal link prediction (TLP) is among the most important graph learning tasks, capable of predicting dynamic, time-varying links within networks. The key problem of TLP is how to explore potential link-evolving tendency from the increasing number of links over time. There exist three major challenges toward solving this problem: temporal nonlinear sparsity, weak serial correlation, and discontinuous structural dynamics. In this article, we propose a novel transfer learning model, called DNformer, to predict temporal link sequence in dynamic networks. The structural dynamic evolution is sequenced into consecutive links one by one over time to inhibit temporal nonlinear sparsity. The self-attention of the model is used to capture the serial correlation between the input and output link sequences. Moreover, our structural encoding is designed to obtain changing structures from the consecutive links and to learn the mapping between link sequences. This structural encoding consists of two parts: the node clustering encoding of each link and the link similarity encoding between links. These encodings enable the model to perceive the importance and correlation of links. Furthermore, we introduce a measurement of structural similarity in the loss function for the structural differences of link sequences. The experimental results demonstrate that our model outperforms other state-of-the-art TLP methods such as Transformer, TGAT, and EvolveGCN. It achieves the three highest AUC and four highest precision scores in five different representative dynamic networks problems.}
}


@article{DBLP:journals/tkdd/AleryaniBWI23,
	author = {Aliya Aleryani and
                  Aaron Bostrom and
                  Wenjia Wang and
                  Beatriz de la Iglesia},
	title = {Multiple Imputation Ensembles for Time Series {(MIE-TS)}},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {44:1--44:28},
	year = {2023},
	url = {https://doi.org/10.1145/3551643},
	doi = {10.1145/3551643},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/AleryaniBWI23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time series classification has become an interesting field of research, thanks to the extensive studies conducted in the past two decades. Time series may have missing data, which may affect both the representation and also modeling of time series. Thus, recovering missing data using appropriate time series-based imputation methods is an essential step. Multiple imputation is a data recovery method where it produced multiple imputed data. The method proves its usefulness in terms of reflecting the uncertainty inherit in missing data; however, it is under-researched in time series problems. In this article, we propose two multiple imputation approaches for time series. The first is a multiple imputation method based on interpolation. The second is a multiple imputation and ensemble method. First, we simulate missing consecutive sub-sequences under a Missing Completely at Random mechanism; then, we use single/multiple imputation methods. The imputed data are used to build bagging and stacking ensembles. We build ensembles using standard classification algorithms as well as time series classifiers. The standard classifiers involve Random Forest, Support Vector Machines, K-Nearest Neighbour, C4.5, and PART while TSCHIEF, Proximity Forest, Time Series Forest, RISE, and BOSS are chosen as time series classifiers. Our findings show that the combination of multiple imputation and ensemble improves the performance of the majority of classifiers tested in this study, often above the performance obtained from the complete data, even under increasing missing data scenarios. This may be because the diversity injected by multiple imputation has a very favourable and stabilising effect on the classifier performance, which is a very important finding.}
}


@article{DBLP:journals/tkdd/LiLL23,
	author = {Dongjie Li and
                  Dong Li and
                  Guang Lian},
	title = {Variational Graph Autoencoder with Adversarial Mutual Information
                  Learning for Network Representation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {3},
	pages = {45:1--45:18},
	year = {2023},
	url = {https://doi.org/10.1145/3555809},
	doi = {10.1145/3555809},
	timestamp = {Tue, 16 May 2023 13:36:21 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the success of Graph Neural Network (GNN) in network data, some GNN-based representation learning methods for networks have emerged recently. Variational Graph Autoencoder (VGAE) is a basic GNN framework for network representation. Its purpose is to well preserve the topology and node attribute information of the network to learn node representation, but it only reconstructs network topology, and does not consider the reconstruction of node features. This strategy will make node representation can not well reserve node features information, impairing the ability of the VGAE method to learn higher quality representations. To solve this problem, we arise a new network representation method to improve the VGAE method for well retaining both node features and network structure information. The method utilizes adversarial mutual information learning to maximize the mutual information (MI) of node features and node representations during the encoding process of the variational autoencoder, which forces the variational encoder to get the representation containing the most informative node features. The method consists of three parts: a variational graph autoencoder includes a variational encoder (MI generator (G)) and a decoder, a positive MI sample module (maximizing MI module), and an MI discriminator (D). Furthermore, we explain why maximizing MI between node features and node representation can reconstruct node attributes. Finally, we conduct experiments on seven public representative datasets for nodes classification, nodes clustering, and graph visualization tasks. Experimental results demonstrate that the proposed algorithm significantly outperforms current popular network representation algorithms on these tasks. The best improvement is 17.13% than the VGAE method.}
}


@article{DBLP:journals/tkdd/WuZXLBW23,
	author = {Gongqing Wu and
                  Liangzhu Zhou and
                  Jiazhu Xia and
                  Lei Li and
                  Xianyu Bao and
                  Xindong Wu},
	title = {Crowdsourcing Truth Inference Based on Label Confidence Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {46:1--46:20},
	year = {2023},
	url = {https://doi.org/10.1145/3556545},
	doi = {10.1145/3556545},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuZXLBW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy.}
}


@article{DBLP:journals/tkdd/SehnanGMJGC23,
	author = {Dhruv Sehnan and
                  Vasu Goel and
                  Sarah Masud and
                  Chhavi Jain and
                  Vikram Goyal and
                  Tanmoy Chakraborty},
	title = {DiVA: {A} Scalable, Interactive and Customizable Visual Analytics
                  Platform for Information Diffusion on Large Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {47:1--47:33},
	year = {2023},
	url = {https://doi.org/10.1145/3558771},
	doi = {10.1145/3558771},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SehnanGMJGC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With an increasing outreach of digital platforms in our lives, researchers have taken a keen interest in studying different facets of social interactions. Analyzing the spread of information (aka diffusion) has brought forth multiple research areas such as modelling user engagement, determining emerging topics, forecasting the virality of online posts and predicting information cascades. Despite such ever-increasing interest, there remains a vacuum among easy-to-use interfaces for large-scale visualization of diffusion models. In this article, we introduce DiVA—Diffusion Visualization and Analysis, a tool that provides a scalable web interface and extendable APIs to analyze various diffusion trends on networks. DiVA uniquely offers support for simultaneous comparison of two competing diffusion models and even the comparison with the ground-truth results, which help develop a coherent understanding of real-world scenarios. Along with performing an exhaustive feature comparison and system evaluation of DiVA against publicly-available web interfaces for information diffusion, we conducted a user study to understand the strengths and limitations of DiVA. We noticed that evaluators had a seamless user experience, especially when analyzing diffusion on large networks.}
}


@article{DBLP:journals/tkdd/ELH23,
	author = {Jinlong E and
                  Mo Li and
                  Jianqiang Huang},
	title = {CrowdAtlas: Estimating Crowd Distribution within the Urban Rail Transit
                  System},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {48:1--48:24},
	year = {2023},
	url = {https://doi.org/10.1145/3558521},
	doi = {10.1145/3558521},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ELH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While urban rail transit systems are playing an increasingly important role in meeting the transportation demands of people, precise awareness of how the human crowd is distributed within such a system is highly necessary, which serves a range of important applications including emergency response, transit recommendation, and commercial valuation. Most rail transit systems are closed systems where once entered the passengers are free to move around all stations and are difficult to track. In this article, we attempt to estimate the crowd distribution based only on the tap-in and tap-out records of all the rail riders. Specifically, we study Singapore MRT (Mass Rapid Transit) as a vehicle and leverage EZ-Link transit card records to estimate the crowd distribution. Guided by a key observation that the passenger inflows and arrival flows at different MRT stations and time are spatio-temporally correlated due to behavioral consistency of MRT riders, we design and implement a machine learning-based solution, CrowdAtlas, that captures MRT riders’ transition probabilities among stations and across time, and based on that accurately estimates the crowd distribution within the MRT system. Our comprehensive performance evaluations with both trace-driven studies and real-world experiments in MRT disruption cases demonstrate the effectiveness of CrowdAtlas.}
}


@article{DBLP:journals/tkdd/YangHLC23,
	author = {Lei Yang and
                  Jiaming Huang and
                  Wanyu Lin and
                  Jiannong Cao},
	title = {Personalized Federated Learning on Non-IID Data via Group-based Meta-learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {49:1--49:20},
	year = {2023},
	url = {https://doi.org/10.1145/3558005},
	doi = {10.1145/3558005},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YangHLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML, which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve “personalization” at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15% relative to the state-of-the-art federated meta-learning.}
}


@article{DBLP:journals/tkdd/HermannsSTMKNBMK23,
	author = {Judith Hermanns and
                  Konstantinos Skitsas and
                  Anton Tsitsulin and
                  Marina Munkhoeva and
                  Alexander Frederiksen Kyster and
                  Simon Nielsen and
                  Alexander M. Bronstein and
                  Davide Mottin and
                  Panagiotis Karras},
	title = {{GRASP:} Scalable Graph Alignment by Spectral Corresponding Functions},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {50:1--50:26},
	year = {2023},
	url = {https://doi.org/10.1145/3561058},
	doi = {10.1145/3561058},
	timestamp = {Mon, 28 Aug 2023 21:37:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HermannsSTMKNBMK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {What is the best way to match the nodes of two graphs? This graph alignment problem generalizes graph isomorphism and arises in applications from social network analysis to bioinformatics. Some solutions assume that auxiliary information on known matches or node or edge attributes is available, or utilize arbitrary graph features. Such methods fare poorly in the pure form of the problem, in which only graph structures are given. Other proposals translate the problem to one of aligning node embeddings, yet, by doing so, provide only a single-scale view of the graph. In this article, we transfer the shape-analysis concept of functional maps from the continuous to the discrete case, and treat the graph alignment problem as a special case of the problem of finding a mapping between functions on graphs. We present GRASP, a method that first establishes a correspondence between functions derived from Laplacian matrix eigenvectors, which capture multiscale structural characteristics, and then exploits this correspondence to align nodes. We enhance the basic form of GRASP by altering two of its components, namely the embedding method and the assignment procedure it employs, leveraging its modular, hence adaptable design. Our experimental study, featuring noise levels higher than anything used in previous studies, shows that the enhanced form of GRASP outperforms scalable state-of-the-art methods for graph alignment across noise levels and graph types, and performs competitively with respect to the best non-scalable ones. We include in our study another modular graph alignment algorithm, CONE, which is also adaptable thanks to its modular nature, and show it can manage graphs with skewed power-law degree distributions.}
}


@article{DBLP:journals/tkdd/NakajimaS23,
	author = {Kazuki Nakajima and
                  Kazuyuki Shudo},
	title = {Random Walk Sampling in Social Networks Involving Private Nodes},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {51:1--51:28},
	year = {2023},
	url = {https://doi.org/10.1145/3561388},
	doi = {10.1145/3561388},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NakajimaS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analysis of social networks with limited data access is challenging for third parties. To address this challenge, a number of studies have developed algorithms that estimate properties of social networks via a simple random walk. However, most existing algorithms do not assume private nodes that do not publish their neighbors’ data when they are queried in empirical social networks. Here we propose a practical framework for estimating properties via random walk-based sampling in social networks involving private nodes. First, we develop a sampling algorithm by extending a simple random walk to the case of social networks involving private nodes. Then, we propose estimators with reduced biases induced by private nodes for the network size, average degree, and density of the node label. Our results show that the proposed estimators reduce biases induced by private nodes in the existing estimators by up to 92.6% on social network datasets involving private nodes.}
}


@article{DBLP:journals/tkdd/ShuiWHWWWG23,
	author = {Changjian Shui and
                  William Wei Wang and
                  Ihsen Hedhli and
                  Chi Man Wong and
                  Feng Wan and
                  Boyu Wang and
                  Christian Gagn{\'{e}}},
	title = {Lifelong Online Learning from Accumulated Knowledge},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {52:1--52:23},
	year = {2023},
	url = {https://doi.org/10.1145/3563947},
	doi = {10.1145/3563947},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ShuiWHWWWG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we formulate lifelong learning as an online transfer learning procedure over consecutive tasks, where learning a given task depends on the accumulated knowledge. We propose a novel theoretical principled framework, lifelong online learning, where the learning process for each task is in an incremental manner. Specifically, our framework is composed of two-level predictions: the prediction information that is solely from the current task; and the prediction from the knowledge base by previous tasks. Moreover, this article tackled several fundamental challenges: arbitrary or even non-stationary task generation process, an unknown number of instances in each task, and constructing an efficient accumulated knowledge base. Notably, we provide a provable bound of the proposed algorithm, which offers insights on the how the accumulated knowledge improves the predictions. Finally, empirical evaluations on both synthetic and real datasets validate the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/tkdd/DaiWHYD23,
	author = {Shaojie Dai and
                  Jinshuai Wang and
                  Chao Huang and
                  Yanwei Yu and
                  Junyu Dong},
	title = {Dynamic Multi-View Graph Neural Networks for Citywide Traffic Inference},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {53:1--53:22},
	year = {2023},
	url = {https://doi.org/10.1145/3564754},
	doi = {10.1145/3564754},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/DaiWHYD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate citywide traffic inference is critical for improving intelligent transportation systems with smart city applications. However, this task is very challenging given the limited training data, due to the high cost of sensor installment and maintenance across the entire urban space. A more practical scenario to study the citywide traffic inference is effectively modeling the spatial and temporal traffic patterns with limited historical traffic observations. In this work, we propose a dynamic multi-view graph neural network for citywide traffic inference with the method CTVI+. Specifically, for the temporal dimension, we propose a temporal self-attention mechanism that is capable of learning the dynamics of traffic data with the time-evolving traffic volume variations. For spatial dimension, we build a multi-view graph neural network, employing the road-wise message passing scheme to capture the region dependencies. With the designed spatial-temporal learning paradigms, we enable our traffic inference model to encode the dynamism from both spatial and temporal traffic patterns, which is reflective of intra- and inter-road traffic correlations. In our evaluation, CTVI+ achieves consistent better performance compared with different baselines on real-world traffic volume datasets. Further ablation study validates the effectiveness of key components in CTVI+. We release the model implementation at https://github.com/dsj96/TKDD.}
}


@article{DBLP:journals/tkdd/LingYCZH23,
	author = {Shuai Ling and
                  Zhe Yu and
                  Shaosheng Cao and
                  Haipeng Zhang and
                  Simon Hu},
	title = {{STHAN:} Transportation Demand Forecasting with Compound Spatio-Temporal
                  Relationships},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {54:1--54:23},
	year = {2023},
	url = {https://doi.org/10.1145/3565578},
	doi = {10.1145/3565578},
	timestamp = {Sat, 27 May 2023 15:23:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LingYCZH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Transportation demand forecasting is a critical precondition of optimal online transportation dispatch, which will greatly reduce drivers’ wasted mileage and customers’ waiting time, contributing to economic and environmental sustainability. Though various methods have been developed, the core spatio-temporal complexity remains challenging from three perspectives: (1) Compound spatial relationships. According to our empirical analysis, these relationships widely exist. Previous studies focus on capturing different spatial relationships using multi-homogeneous graphs. However, the information flow across various spatial relationships is not modeled explicitly. (2) Heterogeneity in spatial relationships. A region’s neighbors under the same spatial relationship may have different weights for this region. Meanwhile, different relationships may also weigh differently. (3) Synchronicity between compound spatial relationships and temporal relationships. Previous research considers synchronous influences from spatial and temporal relationships in a homogeneous fashion while compound spatial relationships are not captured for this synchronicity. To address the aforementioned perspectives, we propose the Spatio-Temporal Heterogeneous graph Attention Network (STHAN), where the key intuition is capturing the compound spatial relationships via meta-paths explicitly. We first construct a spatio-temporal heterogeneous graph including multiple spatial relationships and temporal relationships and use meta-paths to depict compound spatial relationships. To capture the heterogeneity, we use hierarchical attention, which contains node level attention and meta-path level attention. The synchronicity between temporal relationships and spatial relationships, including compound ones, is modeled in meta-path-level attention. Our framework outperforms state-of-the-art models by reducing 6.58%, 4.57%, and 4.20% of WMAPE in experiments on three real-world datasets, respectively.}
}


@article{DBLP:journals/tkdd/LiuXRXPC23,
	author = {Jiaying Liu and
                  Feng Xia and
                  Jing Ren and
                  Bo Xu and
                  Guansong Pang and
                  Lianhua Chi},
	title = {{MIRROR:} Mining Implicit Relationships via Structure-Enhanced Graph
                  Convolutional Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {55:1--55:24},
	year = {2023},
	url = {https://doi.org/10.1145/3564531},
	doi = {10.1145/3564531},
	timestamp = {Tue, 05 Sep 2023 13:50:42 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuXRXPC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data explosion in the information society drives people to develop more effective ways to extract meaningful information. Extracting semantic information and relational information has emerged as a key mining primitive in a wide variety of practical applications. Existing research on relation mining has primarily focused on explicit connections and ignored underlying information, e.g., the latent entity relations. Exploring such information (defined as implicit relationships in this article) provides an opportunity to reveal connotative knowledge and potential rules. In this article, we propose a novel research topic, i.e., how to identify implicit relationships across heterogeneous networks. Specially, we first give a clear and generic definition of implicit relationships. Then, we formalize the problem and propose an efficient solution, namely MIRROR, a graph convolutional network (GCN) model to infer implicit ties under explicit connections. MIRROR captures rich information in learning node-level representations by incorporating attributes from heterogeneous neighbors. Furthermore, MIRROR is tolerant of missing node attribute information because it is able to utilize network structure. We empirically evaluate MIRROR on four different genres of networks, achieving state-of-the-art performance for target relations mining. The underlying information revealed by MIRROR contributes to enriching existing knowledge and leading to novel domain insights.}
}


@article{DBLP:journals/tkdd/LiuCXBZSK23,
	author = {Zhi Liu and
                  Yang Chen and
                  Feng Xia and
                  Jixin Bian and
                  Bing Zhu and
                  Guojiang Shen and
                  Xiangjie Kong},
	title = {{TAP:} Traffic Accident Profiling via Multi-Task Spatio-Temporal Graph
                  Representation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {56:1--56:25},
	year = {2023},
	url = {https://doi.org/10.1145/3564594},
	doi = {10.1145/3564594},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuCXBZSK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Predicting traffic accidents can help traffic management departments respond to sudden traffic situations promptly, improve drivers’ vigilance, and reduce losses caused by traffic accidents. However, the causality of traffic accidents is complex and difficult to analyze. Most existing traffic accident prediction methods do not consider the dynamic spatio-temporal correlation of traffic data, which leads to unsatisfactory prediction accuracy. To address this issue, we propose a multi-task learning framework (TAP) based on the Spatio-temporal Variational Graph Auto-Encoders (ST-VGAE) for traffic accident profiling. We firstly capture the dynamic spatio-temporal correlation of traffic conditions through a spatio-temporal graph convolutional encoder and embed it as a low-latitude vector. Then, we use a multi-task learning scheme to combine external factors to generate the traffic accident profiling. Furthermore, we propose a traffic accident profiling application framework based on edge computing. This method increases the speed of calculation by offloading the calculation of traffic accident profiling to edge nodes. Finally, the experimental results on real datasets demonstrate that TAP outperforms other state-of-the-art baselines.}
}


@article{DBLP:journals/tkdd/ChenCTW23,
	author = {Lei Chen and
                  Jie Cao and
                  Haicheng Tao and
                  Jia Wu},
	title = {Trip Reinforcement Recommendation with Graph-based Representation
                  Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {57:1--57:20},
	year = {2023},
	url = {https://doi.org/10.1145/3564609},
	doi = {10.1145/3564609},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenCTW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tourism is an important industry and a popular leisure activity involving billions of tourists per annum. One challenging problem tourists face is identifying attractive Places-of-Interest (POIs) and planning the personalized trip with time constraints. Most of the existing trip recommendation methods mainly consider POI popularity and user preferences, and focus on the last visited POI when choosing the next POI. However, the visit patterns and their asymmetry property have not been fully exploited. To this end, in this article, we present a GRM-RTrip (short for Graph-based Representation Method for Reinforce Trip Recommendation) framework. GRM-RTrip learns POI representations from incoming and outgoing views to obtain asymmetric POI-POI transition probability via POI-POI graph networks, and then fuses the trained POI representation into a user-POI graph network to estimate user preferences. Finally, after formulating the personalized trip recommendation as a Markov Decision Process (MDP), we utilize a reinforcement learning algorithm for generating a personalized trip with maximal user travel experience. Extensive experiments are performed on the public datasets and the results demonstrate the superiority of GRM-RTrip compared with the state-of-the-art trip recommendation methods.}
}


@article{DBLP:journals/tkdd/ChenWYLJY23,
	author = {Huiming Chen and
                  Huandong Wang and
                  Quanming Yao and
                  Yong Li and
                  Depeng Jin and
                  Qiang Yang},
	title = {LoSAC: An Efficient Local Stochastic Average Control Method for Federated
                  Optimization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {58:1--58:28},
	year = {2023},
	url = {https://doi.org/10.1145/3566128},
	doi = {10.1145/3566128},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenWYLJY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated optimization (FedOpt), which targets at collaboratively training a learning model across a large number of distributed clients, is vital for federated learning. The primary concerns in FedOpt can be attributed to the model divergence and communication efficiency, which significantly affect the performance. In this article, we propose a new method, i.e., LoSAC, to learn from heterogeneous distributed data more efficiently. Its key algorithmic insight is to locally update the estimate for the global full gradient after each regular local model update. Thus, LoSAC can keep clients’ information refreshed in a more compact way. In particular, we have studied the convergence result for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information leakage from the recent technique Deep Leakage Gradients (DLG). Finally, experiments have verified the superiority of LoSAC comparing with state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves communication efficiency by more than 100% on average, mitigates the model divergence problem, and equips with the defense ability against DLG.}
}


@article{DBLP:journals/tkdd/JingZXLZWY23,
	author = {Mengyuan Jing and
                  Yanmin Zhu and
                  Yanan Xu and
                  Haobing Liu and
                  Tianzi Zang and
                  Chunyang Wang and
                  Jiadi Yu},
	title = {Learning Shared Representations for Recommendation with Dynamic Heterogeneous
                  Graph Convolutional Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {59:1--59:23},
	year = {2023},
	url = {https://doi.org/10.1145/3565575},
	doi = {10.1145/3565575},
	timestamp = {Tue, 17 Dec 2024 16:16:32 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/JingZXLZWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Convolutional Networks (GCNs) have been widely used for collaborative filtering, due to their effectiveness in exploiting high-order collaborative signals. However, two issues have not been well addressed by existing studies. First, usually only one kind of information is utilized, i.e., user preference in user-item graphs or item dependency in item-item graphs. Second, they usually adopt static graphs, which cannot retain the temporal evolution of the information. These can limit the recommendation quality. To address these limitations, we propose to mine three kinds of information (user preference, item dependency, and user behavior similarity) and their temporal evolution by constructing multiple discrete dynamic heterogeneous graphs (i.e., a user-item dynamic graph, an item-item dynamic graph, and a user-subseq dynamic graph) from interaction data. A novel network (PDGCN) is proposed to learn the representations of users and items in these dynamic graphs. Moreover, we designed a structural neighbor aggregation module with novel pooling and convolution operations to aggregate the features of structural neighbors. We also design a temporal neighbor aggregation module based on self-attention mechanism to aggregate the features of temporal neighbors. We conduct extensive experiments on four real-world datasets. The results indicate that our approach outperforms several competing methods in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Dynamic graphs are also shown to be effective in improving recommendation performance.}
}


@article{DBLP:journals/tkdd/WangZRZM23,
	author = {Yizong Wang and
                  Dong Zhao and
                  Yajie Ren and
                  Desheng Zhang and
                  Huadong Ma},
	title = {{SPAP:} Simultaneous Demand Prediction and Planning for Electric Vehicle
                  Chargers in a New City},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {4},
	pages = {60:1--60:25},
	year = {2023},
	url = {https://doi.org/10.1145/3565577},
	doi = {10.1145/3565577},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZRZM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For a new city that is committed to promoting Electric Vehicles (EVs), it is significant to plan the public charging infrastructure where charging demands are high. However, it is difficult to predict charging demands before the actual deployment of EV chargers for lack of operational data, resulting in a deadlock. A direct idea is to leverage the urban transfer learning paradigm to learn the knowledge from a source city, then exploit it to predict charging demands, and meanwhile determine locations and amounts of slow/fast chargers for charging stations in the target city. However, the demand prediction and charger planning depend on each other, and it is required to re-train the prediction model to eliminate the negative transfer between cities for each varied charger plan, leading to the unacceptable time complexity. To this end, we design an effective solution of Simultaneous Demand Prediction And Planning (SPAP): discriminative features are extracted from multi-source data, and fed into an Attention-based Spatial-Temporal City Domain Adaptation Network (AST-CDAN) for cross-city demand prediction; a novel Transfer Iterative Optimization (TIO) algorithm is designed for charger planning by iteratively utilizing AST-CDAN and a charger plan fine-tuning algorithm. Extensive experiments on real-world datasets collected from three cities in China validate the effectiveness and efficiency of SPAP. Specially, SPAP improves at most 72.5% revenue compared with the real-world charger deployment.}
}


@article{DBLP:journals/tkdd/LinWW23,
	author = {Dandan Lin and
                  Victor Junqiu Wei and
                  Raymond Chi{-}Wing Wong},
	title = {Effective and Scalable Manifold Ranking-Based Image Retrieval with
                  Output Bound},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {61:1--61:31},
	year = {2023},
	url = {https://doi.org/10.1145/3565574},
	doi = {10.1145/3565574},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LinWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Image retrieval keeps attracting a lot of attention from both academic and industry over past years due to its variety of useful applications. Due to the rapid growth of deep learning approaches, more better feature vectors of images could be discovered for improving image retrieval. However, most (if not all) existing deep learning approaches consider the similarity between two images locally without considering the similarity among a group of similar images globally, and thus could not return accurate results. In this article, we study the image retrieval with manifold ranking (MR) which considers both the local similarity and the global similarity, which could give more accurate results. However, existing best-known algorithms have one of the following issues: (1) they require to build a bulky index, (2) some of them do not have any theoretical bound on the output, and (3) some of them are time-consuming. Motivated by this, we propose two algorithms, namely Monte Carlo-based MR (MCMR) and MCMR+, for image retrieval, which do not have the above issues. We are the first one to propose an index-free manifold ranking image retrieval with the output theoretical bound. More importantly, our algorithms give the first best-known time complexity result of \\(O(n \\log n)\\) where \\(n\\) is the total number of images in the database compared with the existing best-known result of \\(O(n^2)\\) in the literature of computing the exact top-\\(k\\) results with quality guarantee. Lastly, our experimental result shows that MCMR+ outperforms existing algorithms by up to four orders of magnitude in terms of query time.}
}


@article{DBLP:journals/tkdd/ZhouLDL23,
	author = {Peng Zhou and
                  Xinwang Liu and
                  Liang Du and
                  Xuejun Li},
	title = {Self-paced Adaptive Bipartite Graph Learning for Consensus Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {62:1--62:35},
	year = {2023},
	url = {https://doi.org/10.1145/3564701},
	doi = {10.1145/3564701},
	timestamp = {Tue, 13 Feb 2024 16:32:42 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhouLDL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Consensus clustering provides an elegant framework to aggregate multiple weak clustering results to learn a consensus one that is more robust and stable than a single result. However, most of the existing methods usually use all data for consensus learning, whereas ignoring the side effects caused by some unreliable or difficult data. To address this issue, in this article, we propose a novel self-paced consensus clustering method with adaptive bipartite graph learning to gradually involve data from more reliable to less reliable ones in consensus learning. At first, we construct an initial bipartite graph from the base results, where the nodes represent the clusters and instances, and the edges indicate that an instance belongs to a cluster. Then, we adaptively learn a structured bipartite graph from this initial one by self-paced learning, i.e., we automatically determine the reliability of each edge with adaptive cluster similarity measuring and involve the edges in bipartite graph learning in order of their reliability. At last, we obtain the final consensus result from the learned structured bipartite graph. We conduct extensive experiments on both toy and benchmark datasets, and the results show the effectiveness and superiority of our method. The codes of this article are released in http://Doctor-Nobody.github.io/codes/code_SCCABG.zip.}
}


@article{DBLP:journals/tkdd/LiZLZY23,
	author = {Mengran Li and
                  Yong Zhang and
                  Xiaoyong Li and
                  Yuchen Zhang and
                  Baocai Yin},
	title = {Hypergraph Transformer Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {63:1--63:22},
	year = {2023},
	url = {https://doi.org/10.1145/3565028},
	doi = {10.1145/3565028},
	timestamp = {Sat, 27 May 2023 15:23:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiZLZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph neural networks (GNNs) have been widely used for graph structure learning and achieved excellent performance in tasks such as node classification and link prediction. Real-world graph networks imply complex and various semantic information and are often referred to as heterogeneous information networks (HINs). Previous GNNs have laboriously modeled heterogeneous graph networks with pairwise relations, in which the semantic information representation for learning is incomplete and severely hinders node embedded learning. Therefore, the conventional graph structure cannot satisfy the demand for information discovery in HINs. In this article, we propose an end-to-end hypergraph transformer neural network (HGTN) that exploits the communication abilities between different types of nodes and hyperedges to learn higher-order relations and discover semantic information. Specifically, attention mechanisms weigh the importance of semantic information hidden in original HINs to generate useful meta-paths. Meanwhile, our method develops a multi-scale attention module to aggregate node embeddings in higher-order neighborhoods. We evaluate the proposed model with node classification tasks on six datasets: DBLP, ACM, IBDM, Reuters, STUD-BJUT, and Citeseer. Experiments on a large number of benchmarks show the advantages of HGTN.}
}


@article{DBLP:journals/tkdd/LiLLXWSS23,
	author = {Haoran Li and
                  Zhiqiang Lv and
                  Jianbo Li and
                  Zhihao Xu and
                  Yue Wang and
                  Haokai Sun and
                  Zhaoyu Sheng},
	title = {Traffic Flow Forecasting in the {COVID-19:} {A} Deep Spatial-temporal
                  Model Based on Discrete Wavelet Transformation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {64:1--64:28},
	year = {2023},
	url = {https://doi.org/10.1145/3564753},
	doi = {10.1145/3564753},
	timestamp = {Mon, 28 Aug 2023 21:37:07 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLLXWSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic flow prediction has always been the focus of research in the field of Intelligent Transportation Systems, which is conducive to the more reasonable allocation of basic transportation resources and formulation of transportation policies. The spread of COVID-19 has seriously affected the normal order in the transportation sector. With the increase in the number of infected people and the government's anti-epidemic policy, human outgoing activities have gradually decreased, resulting in increasingly obvious discreteness and irregularities in traffic flow data. This article proposes a deep-space time traffic flow prediction model based on discrete wavelet transform (DSTM-DWT) to overcome the highly discrete and irregular nature of the new crown epidemic. First, DSTM-DWT decomposes traffic flow into discrete attributes, such as flow trend, discrete amplitude, and discrete baseline. Second, we design the spatial relationship of the transportation network as a graph and integrate the new crown pneumonia epidemic data into the characteristics of each transportation node. Then, we use the graph convolutional network to calculate the spatial correlation of each node, and the temporal convolutional network to calculate the temporal correlation of the data. In order to solve the problem of high discreteness of traffic flow data during the epidemic, this article proposes a graph memory network (GMN), which is used to convert discrete magnitudes separated by discrete wavelet transform into high-dimensional discrete features. Finally, use DWT to segment the predicted traffic data, and then perform the inverse discrete wavelet transform between the newly segmented traffic trend and discrete baseline and the discrete model predicted by GMN to obtain the final traffic flow prediction result. In simulation experiments, this work was compared with the existing advanced baselines to verify the superiority of DSTM-DWT.}
}


@article{DBLP:journals/tkdd/WuZBHHW23,
	author = {Gongqing Wu and
                  Xingrui Zhuo and
                  Xianyu Bao and
                  Xuegang Hu and
                  Richang Hong and
                  Xindong Wu},
	title = {Crowdsourcing Truth Inference via Reliability-Driven Multi-View Graph
                  Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {65:1--65:26},
	year = {2023},
	url = {https://doi.org/10.1145/3565576},
	doi = {10.1145/3565576},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuZBHHW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers’ reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Reliability-driven Multi-view Graph Embedding framework for Truth inference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.}
}


@article{DBLP:journals/tkdd/JangK23,
	author = {Jun{-}Gi Jang and
                  U Kang},
	title = {Static and Streaming Tucker Decomposition for Dense Tensors},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {66:1--66:34},
	year = {2023},
	url = {https://doi.org/10.1145/3568682},
	doi = {10.1145/3568682},
	timestamp = {Fri, 02 Jun 2023 21:23:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JangK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Given a dense tensor, how can we efficiently discover hidden relations and patterns in static and online streaming settings? Tucker decomposition is a fundamental tool to analyze multidimensional arrays in the form of tensors. However, existing Tucker decomposition methods in both static and online streaming settings have limitations of efficiency since they directly deal with large dense tensors for the result of Tucker decomposition. In a static setting, although few static methods have tried to reduce their time cost by sampling tensors, sketching tensors, and efficient matrix operations, there remains a need for an efficient method. Moreover, streaming versions of Tucker decomposition are still time-consuming to deal with newly arrived tensors. We propose D-Tucker and D-TuckerO, efficient Tucker decomposition methods for large dense tensors in static and online streaming settings, respectively. By decomposing a given large dense tensor with randomized singular value decomposition, avoiding the reconstruction from SVD results, and carefully determining the order of operations, D-Tucker and D-TuckerO efficiently obtain factor matrices and core tensor. Experimental results show that D-Tucker achieves up to 38.4 × faster running times, and requires up to 17.2 × less space than existing methods while having similar accuracy. Furthermore, D-TuckerO is up to 6.1× faster than existing streaming methods for each newly arrived tensor while its running time is proportional to the size of the newly arrived tensor, not the accumulated tensor.}
}


@article{DBLP:journals/tkdd/WangLHH23,
	author = {Meng Wang and
                  Boyu Li and
                  Kun He and
                  John E. Hopcroft},
	title = {Uncovering the Local Hidden Community Structure in Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {67:1--67:25},
	year = {2023},
	url = {https://doi.org/10.1145/3567597},
	doi = {10.1145/3567597},
	timestamp = {Fri, 02 Jun 2023 16:15:09 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLHH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hidden community is a useful concept proposed recently for social network analysis. Hidden communities indicate some weak communities whose most members also belong to other stronger dominant communities. Dominant communities could form a layer that partitions all the individuals of a network, and hidden communities could form other layer(s) underneath. These layers could be natural structures in the real-world networks like students grouped by major, minor, hometown, and so on. To handle the rapid growth of network scale, in this work, we explore the detection of hidden communities from the local perspective, and propose a new method that detects and boosts each layer iteratively on a subgraph sampled from the original network. We first expand the seed set from a single seed node based on our modified local spectral method and detect an initial dominant local community. Then we temporarily remove the members of this community as well as their connections to other nodes, and detect all the neighborhood communities in the remaining subgraph, including some “broken communities” that only contain a fraction of members in the original network. The local community and neighborhood communities form a dominant layer, and by reducing the edge weights inside these communities, we weaken this layer’s structure to reveal the hidden layers. Eventually, we repeat the whole process, and all communities containing the seed node can be detected and boosted iteratively. We theoretically show that our method can avoid some situations that a broken community and the local community are regarded as one community in the subgraph, leading to the inaccuracy of detection which can be caused by global hidden community detection methods. Extensive experiments show that our method could significantly outperform the state-of-the-art baselines designed for either global hidden community detection or multiple local community detection.}
}


@article{DBLP:journals/tkdd/LiuGZFZMX23,
	author = {Hao Liu and
                  Qingyu Guo and
                  Hengshu Zhu and
                  Yanjie Fu and
                  Fuzhen Zhuang and
                  Xiaojuan Ma and
                  Hui Xiong},
	title = {Characterizing and Forecasting Urban Vibrancy Evolution: {A} Multi-View
                  Graph Mining Perspective},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {68:1--68:24},
	year = {2023},
	url = {https://doi.org/10.1145/3568683},
	doi = {10.1145/3568683},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuGZFZMX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Urban vibrancy describes the prosperity, diversity, and accessibility of urban areas, which is vital to a city’s socio-economic development and sustainability. While many efforts have been made for statically measuring and evaluating urban vibrancy, there are few studies on the evolutionary process of urban vibrancy, yet we know little about the relationship between urban vibrancy evolution and sophisticated spatiotemporal dynamics. In this article, we make use of multi-sourced urban data to develop a data-driven framework, U-Evolve, to investigate urban vibrancy evolution. Specifically, we first exploit the spatiotemporal characteristics of urban areas to create multi-view time-dependent graphs. Then, we analyze the contextual features and graph patterns of multi-view time-dependent graphs in terms of informing future urban vibrancy variations. Our analysis validates the informativeness of multi-view time-dependent graphs for characterizing and informing future urban vibrancy evolution. After that, we construct a feature based model to forecast future urban vibrancy evolution and quantify each feature’s importance. Moreover, to further enhance the forecasting effectiveness, we propose a graph learning based model to capture spatiotemporal autocorrelation of urban areas based on multi-view time-dependent graphs in an end-to-end manner. Finally, extensive experiments on two metropolises, Beijing and Shanghai, demonstrate the effectiveness of our forecasting models. The U-Evolve framework has also been deployed in the production environment to deliver real-world urban development and planning insights for various cities in China.}
}


@article{DBLP:journals/tkdd/RenZYFCWCLZ23,
	author = {Yuyang Ren and
                  Haonan Zhang and
                  Peng Yu and
                  Luoyi Fu and
                  Xinde Cao and
                  Xinbing Wang and
                  Guihai Chen and
                  Fei Long and
                  Chenghu Zhou},
	title = {Ada-MIP: Adaptive Self-supervised Graph Representation Learning via
                  Mutual Information and Proximity Optimization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {69:1--69:23},
	year = {2023},
	url = {https://doi.org/10.1145/3568165},
	doi = {10.1145/3568165},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RenZYFCWCLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Self-supervised graph-level representation learning has recently received considerable attention. Given varied input distributions, jointly learning graphs’ unique and common features is vital to downstream tasks. Inspired by graph contrastive learning (GCL), which targets maximizing the agreement between graph representations from different views, we propose an Adaptive self-supervised framework, Ada-MIP, considering both Mutual Information between views (unique features) and inter-graph Proximity (common features). Specifically, Ada-MIP learns graphs’ unique information through a learnable and probably injective augmenter, which can acquire more adaptive views compared to the augmentation strategies applied by existing GCL methods; to learn graphs’ common information, we employ graph kernels to calculate graphs’ proximity and learn graph representations among which the precomputed proximity is preserved. By sharing a global encoder, graphs’ unique and common information can be well integrated into the graph representations learned by Ada-MIP. Ada-MIP is also extendable to semi-supervised scenarios, with our experiments confirming its superior performance in both unsupervised and semi-supervised tasks.}
}


@article{DBLP:journals/tkdd/GuFHL23,
	author = {Zhibin Gu and
                  Songhe Feng and
                  Ruiting Hu and
                  Gengyu Lyu},
	title = {{ONION:} Joint Unsupervised Feature Selection and Robust Subspace
                  Extraction for Graph-based Multi-View Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {70:1--70:23},
	year = {2023},
	url = {https://doi.org/10.1145/3568684},
	doi = {10.1145/3568684},
	timestamp = {Fri, 02 Jun 2023 21:23:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/GuFHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-based Multi-View Clustering (GMVC) has received extensive attention due to its ability to capture the neighborhood relationship among data points from diverse views. However, most existing approaches construct similarity graphs from the original multi-view data, the accuracy of which heavily and implicitly relies on the quality of the original multiple features. Moreover, previous methods either focus on mining the multi-view commonality or emphasize on exploring the multi-view individuality, making the rich information contained in multiple features cannot be effectively exploited. In this work, we design a novel GMVC framework via cOmmoNality and Individuality discOvering in lateNt subspace (ONION), seeking for a robust and discriminative subspace representation compatible across multiple features for GMVC. To be specific, our method simultaneously formulates the unsupervised sparse feature selection and the robust subspace extraction, as well as the target graph learning in a unified optimization model, which can help the learning of the discriminative subspace representation and the target graph in a mutual reinforcement manner. Meanwhile, we manipulate the target graph by an explicit structural penalty, rendering the connected components in the graph directly reveal clusters. Experimental results on seven benchmark datasets demonstrate the effectiveness of our proposed method.}
}


@article{DBLP:journals/tkdd/ZhangLDZLHGL23,
	author = {Zhijie Zhang and
                  Wenzhong Li and
                  Wangxiang Ding and
                  Linming Zhang and
                  Qingning Lu and
                  Peng Hu and
                  Tong Gui and
                  Sanglu Lu},
	title = {{STAD-GAN:} Unsupervised Anomaly Detection on Multivariate Time Series
                  with Self-training Generative Adversarial Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {71:1--71:18},
	year = {2023},
	url = {https://doi.org/10.1145/3572780},
	doi = {10.1145/3572780},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangLDZLHGL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection on multivariate time series (MTS) is an important research topic in data mining, which has a wide range of applications in information technology, financial management, manufacturing system, and so on. However, the state-of-the-art unsupervised deep learning models for MTS anomaly detection are vulnerable to noise and have poor performance on the training data containing anomalies. In this article, we propose a novel Self-Training based Anomaly Detection with Generative Adversarial Network (GAN) model called STAD-GAN to address the practical challenge. The STAD-GAN model consists of a generator-discriminator structure for adversarial learning and a neural network classifier for anomaly classification. The generator is learned to capture the normal data distribution, and the discriminator is learned to amplify the reconstruction error of abnormal data for better recognition. The proposed model is optimized with a self-training teacher-student framework, where a teacher model generates reliable high-quality pseudo-labels to train a student model iteratively with a refined dataset so that the performance of the anomaly classifier can be gradually improved. Extensive experiments based on six open MTS datasets show that STAD-GAN is robust to noise and achieves significant performance improvement compared to the state-of-the-art.}
}


@article{DBLP:journals/tkdd/WuHCLZ23,
	author = {Hongxin Wu and
                  Meng Han and
                  Zhiqiang Chen and
                  Muhang Li and
                  Xilong Zhang},
	title = {A Weighted Ensemble Classification Algorithm Based on Nearest Neighbors
                  for Multi-Label Data Stream},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {72:1--72:21},
	year = {2023},
	url = {https://doi.org/10.1145/3570960},
	doi = {10.1145/3570960},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuHCLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of data stream, multi-label algorithms for mining dynamic data become more and more important. At the same time, when data distribution changes, concept drift will occur, which will make the existing classification models lose effectiveness. Ensemble methods have been used for multi-label classification, but few methods consider both the accuracy and diversity of base classifiers. To address the above-mentioned problem, a Weighted Ensemble classification algorithm based on Nearest Neighbors for Multi-Label data stream (WENNML) is proposed. WENNML uses data blocks to train Active candidate Ensemble Classifiers (AEC) and Passive candidate Ensemble Classifiers (PEC). The base classifiers of AEC and PEC are dynamically updated using geometric and diversity weighting methods. When the difference value between the number of current instances and the number of warning instances reaches the passive warning value, the algorithm selects the optimal base classifiers from AEC and PEC according to the subset accuracy and hamming score and puts them into the predictive ensemble classifiers. Experiments are carried out on 12 kinds of datasets with 9 comparison algorithms. The results show that WENNML achieves the best average rankings among the four evaluation metrics.}
}


@article{DBLP:journals/tkdd/WangZWC23,
	author = {Chunnan Wang and
                  Kaixin Zhang and
                  Hongzhi Wang and
                  Bozhou Chen},
	title = {Auto-STGCN: Autonomous Spatial-Temporal Graph Convolutional Network
                  Search},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {73:1--73:21},
	year = {2023},
	url = {https://doi.org/10.1145/3571285},
	doi = {10.1145/3571285},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, many spatial-temporal graph convolutional network (STGCN) models are proposed to deal with the spatial-temporal network data forecasting problem. These STGCN models have their own advantages, i.e., each of them puts forward many effective operations and achieves good prediction results in the real applications. If users can effectively utilize and combine these excellent operations integrating the advantages of existing models, then they may obtain more effective STGCN models thus create greater value using existing work. However, they fail to do so due to the lack of domain knowledge, and there is lack of automated system to help users to achieve this goal. In this article, we fill this gap and propose Auto-STGCN algorithm, which makes use of existing models to automatically explore high-performance STGCN model for specific scenarios. Specifically, we design Unified-STGCN framework, which summarizes the operations of existing architectures, and use parameters to control the usage and characteristic attributes of each operation, so as to realize the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. Then, we present Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN, and generate optimal STGCN models automatically. Extensive experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models used for search space construction, which demonstrates the effectiveness of our proposed method.}
}


@article{DBLP:journals/tkdd/ChenRCLXLY23,
	author = {Yufu Chen and
                  Yanghui Rao and
                  Shurui Chen and
                  Zhiqi Lei and
                  Haoran Xie and
                  Raymond Y. K. Lau and
                  Jian Yin},
	title = {Semi-Supervised Sentiment Classification and Emotion Distribution
                  Learning Across Domains},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {74:1--74:30},
	year = {2023},
	url = {https://doi.org/10.1145/3571736},
	doi = {10.1145/3571736},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenRCLXLY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this study, sentiment classification and emotion distribution learning across domains are both formulated as a semi-supervised domain adaptation problem, which utilizes a small amount of labeled documents in the target domain for model training. By introducing a shared matrix that captures the stable association between document clusters and word clusters, non-negative matrix tri-factorization (NMTF) is robust to the labeled target domain data and has shown remarkable performance in cross-domain text classification. However, the existing NMTF-based models ignore the incompatible relationship of sentiment polarities and the relatedness among emotions. Besides, their applications on large-scale datasets are limited by the high computation complexity. To address these issues, we propose a semi-supervised NMTF framework for sentiment classification and emotion distribution learning across domains. Based on a many-to-many mapping between document clusters and sentiment polarities (or emotions), we first incorporate the prior information of label dependency to improve the model performance. Then, we develop a parallel algorithm based on message passing interface (MPI) to further enhance the model scalability. Extensive experiments on real-world datasets validate the effectiveness of our method.}
}


@article{DBLP:journals/tkdd/TangLGZWZL23,
	author = {Hui Tang and
                  Xun Liang and
                  Yuhui Guo and
                  Xiangping Zheng and
                  Bo Wu and
                  Sensen Zhang and
                  Zhiying Li},
	title = {Diffuse and Smooth: Beyond Truncated Receptive Field for Scalable
                  and Adaptive Graph Representation Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {5},
	pages = {75:1--75:25},
	year = {2023},
	url = {https://doi.org/10.1145/3572781},
	doi = {10.1145/3572781},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/TangLGZWZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the scope of receptive field and the depth of Graph Neural Networks (GNNs) are two completely orthogonal aspects for graph learning, existing GNNs often have shallow layers with truncated-receptive field and far from achieving satisfactory performance. In this article, we follow the idea of decoupling graph convolution into propagation and transformation processes, which generates representations over a sequence of increasingly larger neighborhoods. Though this manner can enlarge the receptive field, it has two critical problems unsolved: how to find the suitable receptive field to avoid under-smoothing or over-smoothing? and how to balance different diffusion operators for better capturing the local and global dependencies? We tackle these challenges and propose a Scalable, Adaptive Graph Convolutional Networks (SAGCN) with Transformer architecture. Concretely, we propose a novel non-heuristic metric method that quickly finds the suitable number of diffusing iterations and produces smoothed local embeddings that enable the truncated receptive field to become scalable and independent of prior experience. Furthermore, we devise smooth2seq and diffusion-based position schemes introduced into Transformer architecture for better capturing local and global information among embeddings. Experimental results show that SAGCN enjoys high accuracy, scalability and efficiency on various open benchmarks and is competitive with other state-of-the-art competitors.}
}


@article{DBLP:journals/tkdd/LiuGSYMLA23,
	author = {Xiao Liu and
                  Bonan Gao and
                  Basem Suleiman and
                  Han You and
                  Zisu Ma and
                  Yu Liu and
                  Ali Anaissi},
	title = {Privacy-Preserving Personalized Fitness Recommender System \emph{P\({}^{\mbox{3}}\)FitRec}:
                  {A} Multi-level Deep Learning Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {76:1--76:24},
	year = {2023},
	url = {https://doi.org/10.1145/3572899},
	doi = {10.1145/3572899},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuGSYMLA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users’ privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies. Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users’ sensitive information. Thus, it preserves the users’ privacy.}
}


@article{DBLP:journals/tkdd/YangWRCYS23,
	author = {Jie Yang and
                  Zhixiao Wang and
                  Xiaobin Rui and
                  Yahui Chai and
                  Philip S. Yu and
                  Lichao Sun},
	title = {Triadic Closure Sensitive Influence Maximization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {77:1--77:26},
	year = {2023},
	url = {https://doi.org/10.1145/3573011},
	doi = {10.1145/3573011},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YangWRCYS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The influence are not linked to any footnote in the text. Please check and suggest. maximization problem aims at selecting the k most influential nodes (i.e., seed nodes) from a social network, where the nodes can maximize the number of influenced nodes activated by a certain propagation model. However, the widely used Independent Cascade model shares the same propagation probability among substantial adjacent node pairs, which is too idealistic and unreasonable in practice. In addition, most heuristic algorithms for influence maximization need to update the expected influence of the remaining nodes in the seed selection process, resulting in high computation cost. To address these non-trivial problems, we propose a novel edge propagation probability calculation method. The method first utilizes the triadic closure structure of social networks to precisely measure the closeness between nodes and assigns different propagation probabilities to each edge, deriving a Triadic Closure-based Independent Cascade (TC-IC) model. Then, we further propose a heuristic influence maximization algorithm named Triadic Closure-based Influence Maximization (TC-IM). The algorithm evaluates the expected influence of a node by integrating the triadic closure weighted propagation probability and the triadic closure weighted degree. Especially, in the seed selection process, only the most influential node that has not been updated in the current round needs to be updated, which significantly improves the efficiency. Besides, we further provide theoretical proofs to guarantee the correctness of this updating strategy. Experimental results on nine real datasets and three propagation models demonstrate that: (1) The TC-IC model can set a proper propagation probability for each node pair, where the IM algorithms could easily identify influential nodes; (2) The TC-IM algorithm can significantly reduce the complexity through an efficient updating strategy with a comparable influence spread to the approximation IM algorithms; (3) Besides, the TC-IM algorithm also exhibits stable performance under other IC models including UIC and WIC, exhibiting good stability and generality.}
}


@article{DBLP:journals/tkdd/CousinsWR23,
	author = {Cyrus Cousins and
                  Chloe Wohlgemuth and
                  Matteo Riondato},
	title = {Bavarian: Betweenness Centrality Approximation with Variance-aware
                  Rademacher Averages},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {78:1--78:47},
	year = {2023},
	url = {https://doi.org/10.1145/3577021},
	doi = {10.1145/3577021},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CousinsWR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {“[A]llain Gersten, Hopfen, und Wasser” — 1516 Reinheitsgebot We present Bavarian, a collection of sampling-based algorithms for approximating the Betweenness Centrality (BC) of all vertices in a graph. Our algorithms use Monte-Carlo Empirical Rademacher Averages (MCERAs), a concept from statistical learning theory, to efficiently compute tight bounds on the maximum deviation of the estimates from the exact values. The MCERAs provide a sample-dependent approximation guarantee much stronger than the state-of-the-art, thanks to its use of variance-aware probabilistic tail bounds. The flexibility of the MCERAs allows us to introduce a unifying framework that can be instantiated with existing sampling-based estimators of BC, thus allowing a fair comparison between them, decoupled from the sample-complexity results with which they were originally introduced. Additionally, we prove novel sample-complexity results showing that, for all estimators, the sample size sufficient to achieve a desired approximation guarantee depends on the vertex-diameter of the graph, an easy-to-bound characteristic quantity. We also show progressive-sampling algorithms and extensions to other centrality measures, such as percolation centrality. Our extensive experimental evaluation of Bavarian shows the improvement over the state-of-the-art made possible by the MCERAs (2–4× reduction in the error bound), and it allows us to assess the different trade-offs between sample size and accuracy guarantees offered by the different estimators.}
}


@article{DBLP:journals/tkdd/JalaliCSWKRS23,
	author = {Zeinab S. Jalali and
                  Qilan Chen and
                  Shwetha Koushik Manchinahalli Srikanta and
                  Weixiang Wang and
                  Myunghwan Kim and
                  Hema Raghavan and
                  Sucheta Soundarajan},
	title = {Fairness of Information Flow in Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {79:1--79:26},
	year = {2023},
	url = {https://doi.org/10.1145/3578268},
	doi = {10.1145/3578268},
	timestamp = {Sat, 17 Aug 2024 15:18:17 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JalaliCSWKRS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social networks form a major parts of people’s lives, and individuals often make important life decisions based on information that spreads through these networks. For this reason, it is important to know whether individuals from different protected groups have equal access to information flowing through a network. In this article, we define the Information Unfairness (IUF) metric, which quantifies inequality in access to information across protected groups. We then introduce MinIUF, an algorithm for reducing inequalities in information flow by adding edges to the network. Finally, we provide an in-depth analysis of information flow with respect to an attribute of interest, such as gender, across different types of networks to evaluate whether the structure of these networks allows groups to equally access information flowing in the network. Moreover, we investigate the causes of unfairness in such networks and how it can be improved.}
}


@article{DBLP:journals/tkdd/LiHZLLSZ23,
	author = {Yunyi Li and
                  Yongjing Hao and
                  Pengpeng Zhao and
                  Guanfeng Liu and
                  Yanchi Liu and
                  Victor S. Sheng and
                  Xiaofang Zhou},
	title = {Edge-enhanced Global Disentangled Graph Neural Network for Sequential
                  Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {80:1--80:22},
	year = {2023},
	url = {https://doi.org/10.1145/3577928},
	doi = {10.1145/3577928},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiHZLLSZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sequential recommendation has been a widely popular topic of recommender systems. Existing works have contributed to enhancing the prediction ability of sequential recommendation systems based on various methods, such as recurrent networks and self-attention mechanisms. However, they fail to discover and distinguish various relationships between items, which could be underlying factors which motivate user behaviors. In this article, we propose an Edge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to capture the relation information between items for global item representation and local user intention learning. At the global level, we build a global-link graph over all sequences to model item relationships. Then a channel-aware disentangled learning layer is designed to decompose edge information into different channels, which can be aggregated to represent the target item from its neighbors. At the local level, we apply a variational auto-encoder framework to learn user intention over the current sequence. We evaluate our proposed method on three real-world datasets. Experimental results show that our model can get a crucial improvement over state-of-the-art baselines and is able to distinguish item features.}
}


@article{DBLP:journals/tkdd/FengLC23,
	author = {Wenjie Feng and
                  Shenghua Liu and
                  Xueqi Cheng},
	title = {Hierarchical Dense Pattern Detection in Tensors},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {81:1--81:29},
	year = {2023},
	url = {https://doi.org/10.1145/3577022},
	doi = {10.1145/3577022},
	timestamp = {Fri, 02 Jun 2023 21:23:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/FengLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dense subtensor detection gains remarkable success in spotting anomalies and fraudulent behaviors for multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors flatly and separately, with the underlying assumption that those subtensors are exclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure and dynamic communities in networks. It is also unexplored how to fuse the prior knowledge into dense pattern detection to capture the local behavior. In this article, we propose CatchCore, a novel framework to efficiently find the hierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the proposed metric, CatchCore detects hierarchical dense subtensors through the hierarchy-wise alternative optimization and finds local dense patterns concerning some items in a query manner. Finally, we utilize the minimum description length principle to measure the quality of detection results and select the optimal hierarchical dense subtensors. Extensive experiments on synthetic and real-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns, like network attacks. Additionally, CatchCore successfully identifies a hierarchical researcher co-authorship group with intense interactions in the DBLP dataset; it can also capture core collaboration and multi-hop relations around some query objects. Meanwhile, CatchCore also scales linearly with all aspects of tensors.}
}


@article{DBLP:journals/tkdd/HuangCYOW23,
	author = {Jingmin Huang and
                  Bowei Chen and
                  Zhi Yan and
                  Iadh Ounis and
                  Jun Wang},
	title = {{GEO:} {A} Computational Design Framework for Automotive Exterior
                  Facelift},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {82:1--82:20},
	year = {2023},
	url = {https://doi.org/10.1145/3578521},
	doi = {10.1145/3578521},
	timestamp = {Wed, 07 Jun 2023 14:24:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HuangCYOW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exterior facelift has become an effective method for automakers to boost the consumers’ interest in an existing car model before it is redesigned. To support the automotive facelift design process, this study develops a novel computational framework\xa0– Generator, Evaluator, Optimiser (GEO), which comprises three components: a StyleGAN2-based design generator that creates different facelift designs; a convolutional neural network (CNN)-based evaluator that assesses designs from the aesthetics perspective; and a recurrent neural network (RNN)-based decision optimiser that selects designs to maximise the predicted profit of the targeted car model over time. We validate the GEO framework in experiments with real-world datasets and describe some resulting managerial implications for automotive facelift. Our study makes both methodological and application contributions. First, the generator’s mapping network and projection methods are carefully tailored to facelift where only minor changes are performed without affecting the family signature of the automobile brands. Second, two evaluation metrics are proposed to assess the generated designs. Third, profit maximisation is taken into account in the design selection. From a high-level perspective, our study contributes to the recent use of machine learning and data mining in marketing and design studies. To the best of our knowledge, this is the first study that uses deep generative models for automotive regional design upgrading and that provides an end-to-end decision-support solution for automakers and designers.}
}


@article{DBLP:journals/tkdd/SinghLLFC23,
	author = {Karandeep Singh and
                  SeungEon Lee and
                  Giuseppe (Joe) Labianca and
                  Jesse Michael Fagan and
                  Meeyoung Cha},
	title = {Multi-Stage Machine Learning Model for Hierarchical Tie Valence Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {83:1--83:20},
	year = {2023},
	url = {https://doi.org/10.1145/3579096},
	doi = {10.1145/3579096},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SinghLLFC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Individuals interacting in organizational settings involving varying levels of formal hierarchy naturally form a complex network of social ties having different tie valences (e.g., positive and negative connections). Social ties critically affect employees’ satisfaction, behaviors, cognition, and outcomes—yet identifying them solely through survey data is challenging because of the large size of some organizations or the often hidden nature of these ties and their valences. We present a novel deep learning model encompassing NLP and graph neural network techniques that identifies positive and negative ties in a hierarchical network. The proposed model uses human resource attributes as node information and web-logged work conversation data as link information. Our findings suggest that the presence of conversation data improves the tie valence classification by 8.91% compared to employing user attributes alone. This gain came from accurately distinguishing positive ties, particularly for male, non-minority, and older employee groups. We also show a substantial difference in conversation patterns for positive and negative ties with positive ties being associated with more messages exchanged on weekends, and lower use of words related to anger and sadness. These findings have broad implications for facilitating collaboration and managing conflict within organizational and other social networks.}
}


@article{DBLP:journals/tkdd/SuGCH23,
	author = {Sixing Su and
                  Jiewen Guan and
                  Bilian Chen and
                  Xin Huang},
	title = {Nonnegative Matrix Factorization Based on Node Centrality for Community
                  Detection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {84:1--84:21},
	year = {2023},
	url = {https://doi.org/10.1145/3578520},
	doi = {10.1145/3578520},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SuGCH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection is an important topic in network analysis, and recently many community detection methods have been developed on top of the Nonnegative Matrix Factorization (NMF) technique. Most NMF-based community detection methods only utilize the first-order proximity information in the adjacency matrix, which has some limitations. Besides, many NMF-based community detection methods involve sparse regularizations to promote clearer community memberships. However, in most of these regularizations, different nodes are treated equally, which seems unreasonable. To dismiss the above limitations, this article proposes a community detection method based on node centrality under the framework of NMF. Specifically, we design a new similarity measure which considers the proximity of higher-order neighbors to form a more informative graph regularization mechanism, so as to better refine the detected communities. Besides, we introduce the node centrality and Gini impurity to measure the importance of nodes and sparseness of the community memberships, respectively. Then, we propose a novel sparse regularization mechanism which forces nodes with higher node centrality to have smaller Gini impurity. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over thirteen state-of-the-art methods.}
}


@article{DBLP:journals/tkdd/WangLXKWG23,
	author = {Yuxiang Wang and
                  Jun Liu and
                  Xiaoliang Xu and
                  Xiangyu Ke and
                  Tianxing Wu and
                  Xiaoxuan Gou},
	title = {Efficient and Effective Academic Expert Finding on Heterogeneous Graphs
                  through (\emph{k}, {\unicode{119979}})-Core based Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {85:1--85:35},
	year = {2023},
	url = {https://doi.org/10.1145/3578365},
	doi = {10.1145/3578365},
	timestamp = {Fri, 02 Jun 2023 21:23:14 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangLXKWG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Expert finding is crucial for a wealth of applications in both academia and industry. Given a user query and trove of academic papers, expert finding aims at retrieving the most relevant experts for the query, from the academic papers. Existing studies focus on embedding-based solutions that consider academic papers’ textual semantic similarities to a query via document representation and extract the top-n experts from the most similar papers. Beyond implicit textual semantics, however, papers’ explicit relationships (e.g., co-authorship) in a heterogeneous graph (e.g., DBLP) are critical for expert finding, because they help improve the representation quality. Despite their importance, the explicit relationships of papers generally have been ignored in the literature. In this article, we study expert finding on heterogeneous graphs by considering both the explicit relationships and implicit textual semantics of papers in one model. Specifically, we define the cohesive (k, 𝒫)-core community of papers w.r.t. a meta-path 𝒫 (i.e., relationship) and propose a (k, 𝒫)-core based document embedding model to enhance the representation quality. Based on this, we design a proximity graph-based index (PG-Index) of papers and present a threshold algorithm (TA)-based method to efficiently extract top-n experts from papers returned by PG-Index. We further optimize our approach in two ways: (1) we boost effectiveness by considering the (k, 𝒫)-core community of experts and the diversity of experts’ research interests, to achieve high-quality expert representation from paper representation; and (2) we streamline expert finding, going from “extract top-n experts from top-m (m> n) semantically similar papers” to “directly return top-n experts”. The process of returning a large number of top-m papers as intermediate data is avoided, thereby improving the efficiency. Extensive experiments using real-world datasets demonstrate our approach’s superiority.}
}


@article{DBLP:journals/tkdd/WangWLM23,
	author = {Yongjie Wang and
                  Ke Wang and
                  Cheng Long and
                  Chunyan Miao},
	title = {Summarizing User-item Matrix By Group Utility Maximization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {86:1--86:22},
	year = {2023},
	url = {https://doi.org/10.1145/3578586},
	doi = {10.1145/3578586},
	timestamp = {Tue, 22 Oct 2024 20:38:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WangWLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A user-item utility matrix represents the utility (or preference) associated with each (user, item) pair, such as citation counts, rating/vote on items or locations, and clicks on items. A high utility value indicates a strong association of the pair. In this work, we consider the problem of summarizing strong association for a large user-item matrix using a small summary size. Traditional techniques fail to distinguish user groups associated with different items (such as top-l item selection) or fail to focus on high utility (such as similarity- based subspace clustering and biclustering). We formulate a new problem, called Group Utility Maximization (GUM), to summarize the entire user population through k user groups and l items for each group; the goal is to maximize the total utility of selected items over all groups collectively. We show this problem is NP-hard even for l=1. We present two algorithms. One greedily finds the next group, called Greedy algorithm, and the other iteratively refines existing k groups, called k-max algorithm. Greedy algorithm provides the \\((1-\\frac{1}{e})\\) approximation guarantee for a nonnegative utility matrix, whereas k-max algorithm is more efficient for large datasets. We evaluate these algorithms on real-life datasets.}
}


@article{DBLP:journals/tkdd/NiZW23,
	author = {Peikun Ni and
                  Jianming Zhu and
                  Guoqing Wang},
	title = {Misinformation Blocking Problem in Virtual and Real Interconversion
                  Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {87:1--87:25},
	year = {2023},
	url = {https://doi.org/10.1145/3578936},
	doi = {10.1145/3578936},
	timestamp = {Wed, 17 May 2023 21:56:20 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/NiZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the in-depth development of intelligent media technology, online and offline fusion, reality and virtual entanglement, information content generalization, the boundary between positive and negative information is blurred, all kinds of misinformation in the social network fission spread, and cyberspace governance has become a global consensus. In this article, we comprehensively consider the spread of misinformation in location-based interpersonal social network and online social network, and systematically tackle the novel problem of minimizing the influence of misinformation under individual protection strategies. We first analyze the complexity and modularity of the problem. Then, we leverage the Lovász extension to devise a nonsubmodular set function continuity approximate convex relaxation method, and develop an approximate projected subgradient procedure to obtain a solution with a factor approximate guarantee. Finally, experiments on three assembled real-world datasets demonstrate the effectiveness and feasibility of our designed method and developed the algorithm.}
}


@article{DBLP:journals/tkdd/RenWD23,
	author = {Jinjun Ren and
                  Yuping Wang and
                  Xiyan Deng},
	title = {Slack-Factor-Based Fuzzy Support Vector Machine for Class Imbalance
                  Problems},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {88:1--88:26},
	year = {2023},
	url = {https://doi.org/10.1145/3579050},
	doi = {10.1145/3579050},
	timestamp = {Sat, 27 May 2023 15:23:45 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RenWD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Class imbalance and noisy data widely exist in real-world problems, and the support vector machine (SVM) is hard to construct good classifiers on these data. Fuzzy SVMs (FSVMs), as variants of SVM, use a fuzzy membership function both to reflect the samples’ importance and to remove the impact of noises, and employ cost-sensitive technology to address the class imbalance. They can handle the noise and class imbalance problems in many cases; however, the fuzzy membership functions are often affected by the class imbalance data, leading to inaccurate measures for samples’ performance and affecting the performance of FSVMs. To solve this problem, we design a new fuzzy membership function and combine it with cost-sensitive learning to deal with the class imbalance problem with noisy data, named Slack-Factor-based FSVM (SFFSVM). In SFFSVM, the relative distances between samples and an estimated hyperplane, called slack factors, are used to define the fuzzy membership function. To eliminate the impact of class imbalance on the function and gain more accurate samples’ importance, we rectify the importance according to the positional relationship between the estimated hyperplane and the optimal hyperplane of the problem, and the slack factors of samples. Comprehensive experiments on artificial and real-world datasets demonstrate that SFFSVM outperforms other comparative methods on F1, MCC, and AUC-PR metrics.}
}


@article{DBLP:journals/tkdd/LiLZCW23,
	author = {Lei Li and
                  Zhiyuan Liu and
                  Zan Zhang and
                  Huanhuan Chen and
                  Xindong Wu},
	title = {Three-way Preference Completion via Preference Graph},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {89:1--89:19},
	year = {2023},
	url = {https://doi.org/10.1145/3580368},
	doi = {10.1145/3580368},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiLZCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the personal partial rankings from agents over a subset of alternatives, the goal of preference completion is to infer the agent’s personalized preference over all alternatives including those the agent has not yet handled from uncertain preference of third parties. By combining the partial rankings of the target agent and the partial rankings from third parties to settle some disagreement with three-way preference completion, which includes a general strategy, an optimal strategy, and a pessimistic strategy, it forms the weighted preference graph. Technically, to settle the disagreement and obtain the completed preference of the target agent in the weighted preference graph, maximum likelihood estimation (MLE) under Mallows is proposed and validated theoretically by removing edges with the minimum weight in the weighted preference graph. However, it is not easy to locate the edges with the minimum weight efficiently in a big graph. Hence, an optimal MLE algorithm and three greedy MLE algorithms are proposed to process the MLE. Furthermore, these proposed algorithms are experimentally validated and compared with each other by both the synthetic dataset and the Flixter dataset.}
}


@article{DBLP:journals/tkdd/KhokharFIAH23,
	author = {Rashid Hussain Khokhar and
                  Benjamin C. M. Fung and
                  Farkhund Iqbal and
                  Khalil Al{-}Hussaeni and
                  Mohammed Hussain},
	title = {Differentially Private Release of Heterogeneous Network for Managing
                  Healthcare Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {6},
	pages = {90:1--90:30},
	year = {2023},
	url = {https://doi.org/10.1145/3580367},
	doi = {10.1145/3580367},
	timestamp = {Wed, 17 May 2023 21:56:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/KhokharFIAH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing adoption of digital health platforms through mobile apps and online services, people have greater flexibility connecting with medical practitioners, pharmacists, and laboratories and accessing resources to manage their own health-related concerns. Many healthcare institutions are connecting with each other to facilitate the exchange of healthcare data, with the goal of effective healthcare data management. The contents generated over these platforms are often shared with third parties for a variety of purposes. However, sharing healthcare data comes with the potential risk of exposing patients’ sensitive information to privacy threats. In this article, we address the challenge of sharing healthcare data while protecting patients’ privacy. We first model a complex healthcare dataset using a heterogeneous information network that consists of multi-type entities and their relationships. We then propose DiffHetNet, an edge-based differentially private algorithm, to protect the sensitive links of patients from inbound and outbound attacks in the heterogeneous health network. We evaluate the performance of our proposed method in terms of information utility and efficiency on different types of real-life datasets that can be modeled as networks. Experimental results suggest that DiffHetNet generally yields less information loss and is significantly more efficient in terms of runtime in comparison with existing network anonymization methods. Furthermore, DiffHetNet is scalable to large network datasets.}
}


@article{DBLP:journals/tkdd/ZhangP23,
	author = {Mimi Zhang and
                  Andrew Parnell},
	title = {Review of Clustering Methods for Functional Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {91:1--91:34},
	year = {2023},
	url = {https://doi.org/10.1145/3581789},
	doi = {10.1145/3581789},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.}
}


@article{DBLP:journals/tkdd/0001LYC23,
	author = {Ling Chen and
                  Dandan Lyu and
                  Shanshan Yu and
                  Gencai Chen},
	title = {Multi-Level Visual Similarity Based Personalized Tourist Attraction
                  Recommendation Using Geo-Tagged Photos},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {92:1--92:18},
	year = {2023},
	url = {https://doi.org/10.1145/3582015},
	doi = {10.1145/3582015},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/0001LYC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geo-tagged photo-based tourist attraction recommendation can discover users’ travel preferences from their taken photos, so as to recommend suitable tourist attractions to them. However, existing visual content-based methods cannot fully exploit the user and tourist attraction information of photos to extract visual features, and do not differentiate the significance of different photos. In this article, we propose multi-level visual similarity-based personalized tourist attraction recommendation using geo-tagged photos (MEAL). MEAL utilizes the visual contents of photos and interaction behavior data to obtain the final embeddings of users and tourist attractions, which are then used to predict the visit probabilities. Specifically, by crossing the user and tourist attraction information of photos, we define four visual similarity levels and introduce a corresponding quintuplet loss to embed the visual contents of photos. In addition, to capture the significance of different photos, we exploit the self-attention mechanism to obtain the visual representations of users and tourist attractions. We conducted experiments on two datasets crawled from Flickr, and the experimental results proved the advantage of this method.}
}


@article{DBLP:journals/tkdd/XuZ23,
	author = {Wanyue Xu and
                  Zhongzhi Zhang},
	title = {Optimal Scale-Free Small-World Graphs with Minimum Scaling of Cover
                  Time},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {93:1--93:19},
	year = {2023},
	url = {https://doi.org/10.1145/3583691},
	doi = {10.1145/3583691},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/XuZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cover time of random walks on a graph has found wide practical applications in different fields of computer science, such as crawling and searching on the World Wide Web and query processing in sensor networks, with the application effects dependent on the behavior of the cover time: the smaller the cover time, the better the application performance. It was proved that over all graphs with N nodes, complete graphs have the minimum cover time N log N. However, complete graphs cannot mimic real-world networks with small average degree and scale-free small-world properties, for which the cover time has not been examined carefully, and its behavior is still not well understood. In this article, we first experimentally evaluate the cover time for various real-world networks with scale-free small-world properties, which scales as N log N. To better understand the behavior of the cover time for real-world networks, we then study the cover time of three scale-free small-world model networks by using the connection between cover time and resistance diameter. For all the three networks, their cover time also behaves as N log N. This work indicates that sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of cover time. Our results deepen understanding the behavior of cover time in real-world networks with scale-free small-world structure, and have potential implications in the design of efficient algorithms related to cover time.}
}


@article{DBLP:journals/tkdd/ShethG00C23,
	author = {Paras Sheth and
                  Ruocheng Guo and
                  Lu Cheng and
                  Huan Liu and
                  Kasim Sel{\c{c}}uk Candan},
	title = {Causal Disentanglement for Implicit Recommendations with Network Information},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {94:1--94:18},
	year = {2023},
	url = {https://doi.org/10.1145/3582435},
	doi = {10.1145/3582435},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ShethG00C23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Online user engagement is highly influenced by various machine learning models, such as recommender systems. These systems recommend new items to the user based on the user’s historical interactions. Implicit recommender systems reflect a binary setting showing whether a user interacted (e.g., clicked on) with an item or not. However, the observed clicks may be due to various causes such as user’s interest, item’s popularity, and social influence factors. Traditional recommender systems consider these causes under a unified representation, which may lead to the emergence and amplification of various biases in recommendations. However, recent work indicates that by disentangling the unified representations, one can mitigate bias (e.g., popularity bias) in recommender systems and help improve recommendation performance. Yet, prior work in causal disentanglement in recommendations does not consider a crucial factor, that is, social influence. Social theories such as homophily and social influence provide evidence that a user’s decision can be highly influenced by the user’s social relations. Thus, accounting for the social relations while disentangling leads to less biased recommendations. To this end, we identify three separate causes behind an effect (e.g., clicks): (a) user’s interest, (b) item’s popularity, and (c) user’s social influence. Our approach seeks to causally disentangle the user and item latent features to mitigate popularity bias in implicit feedback–based social recommender systems. To achieve this goal, we draw from causal inference theories and social network theories and propose a causality-aware disentanglement method that leverages both the user–item interaction network and auxiliary social network information. Experiments on real-world datasets against various state-of-the-art baselines validate the effectiveness of the proposed model for mitigating popularity bias and generating de-biased recommendations.}
}


@article{DBLP:journals/tkdd/0001FH23,
	author = {Yihong Zhang and
                  Xiu Susie Fang and
                  Takahiro Hara},
	title = {Evolving Social Media Background Representation with Frequency Weights
                  and Co-Occurrence Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {95:1--95:17},
	year = {2023},
	url = {https://doi.org/10.1145/3585389},
	doi = {10.1145/3585389},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/0001FH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Social media as a background information source has been utilized in many practical computational tasks, such as stock price prediction, epidemic tracking, and product recommendation. However, proper representation of an evolving social media background is still in an early research stage. In this article, we propose a representation method that considers temporal novelties as well as the fine details of word inter-dependencies. Our method is based on the tf-idf and graph embedding techniques. The proposed method has superiority over other representation methods because it takes the advantage of both the temporal aspect of tf-idf and the semantic aspect of graph embeddings. We compare our method with a variety of baselines in two practical application scenarios using real-world data. In tweet popularity prediction, our representation achieves 5.7% less error and 12.8% higher correlation compared to the best baseline. In e-commerce product recommendation, our representation achieves 17% higher hit-rate and 20% higher NDCG compared to the best baseline.}
}


@article{DBLP:journals/tkdd/LiJX23,
	author = {Huiru Li and
                  Liangxiao Jiang and
                  Siqing Xue},
	title = {Neighborhood Weighted Voting-Based Noise Correction for Crowdsourcing},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {96:1--96:18},
	year = {2023},
	url = {https://doi.org/10.1145/3586998},
	doi = {10.1145/3586998},
	timestamp = {Fri, 20 Dec 2024 08:07:17 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiJX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance’s own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance’s neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.}
}


@article{DBLP:journals/tkdd/0006JLHMCHQY23,
	author = {He Li and
                  Duo Jin and
                  Xuejiao Li and
                  Jianbin Huang and
                  Xiaoke Ma and
                  Jiangtao Cui and
                  Deshuang Huang and
                  Shaojie Qiao and
                  Jaesoo Yoo},
	title = {DMGF-Net: An Efficient Dynamic Multi-Graph Fusion Network for Traffic
                  Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {97:1--97:19},
	year = {2023},
	url = {https://doi.org/10.1145/3586164},
	doi = {10.1145/3586164},
	timestamp = {Wed, 11 Dec 2024 17:20:53 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/0006JLHMCHQY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic prediction is the core task of intelligent transportation system (ITS) and accurate traffic prediction can greatly improve the utilization of public resources. Dynamic interaction of multiple spatial relationships will influence the accuracy of traffic prediction. However, many existing methods only consider static spatial relationships, which restricts the accuracy of the prediction. To address the above problem, in this article, we propose the Dynamic Multi-Graph Fusion Network (DMGF-Net) to model the spatial-temporal correlations in traffic network. In the DMGF-Net, the fusion graph is designed to leverage and extract the various spatial correlations between different regions by fusing spatial graph, semantic graph, and spatial-semantic graph. Further, to dynamically learn the importance of different neighbors, we design the Dynamic Spatial-Temporal Unit (DSTU), which can adjust the aggregation weights of different neighbors by combining the convolution operation and the attention mechanism. It can selectively aggregate spatial-temporal features from different neighbors. Extensive experiments on three datasets demonstrate that effectiveness of our model, especially on PEMS08, our model achieves an increase of about 8.55% and 7.55% in terms of MAE and RMSE than the static model STGCN.}
}


@article{DBLP:journals/tkdd/ZhaoWC0LG23,
	author = {Boxiang Zhao and
                  Shuliang Wang and
                  Lianhua Chi and
                  Qi Li and
                  Xiaojia Liu and
                  Jing Geng},
	title = {Causal Discovery via Causal Star Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {98:1--98:24},
	year = {2023},
	url = {https://doi.org/10.1145/3586997},
	doi = {10.1145/3586997},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhaoWC0LG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discovering causal relationships among observed variables is an important research focus in data mining. Existing causal discovery approaches are mainly based on constraint-based methods and functional causal models (FCMs). However, the constraint-based method cannot identify the Markov equivalence class and the functional causal models cannot identify the complex interrelationships when multiple variables affect one variable. To address the two aforementioned problems, we propose a new graph structure Causal Star Graph (CSG) and a corresponding framework Causal Discovery via Causal Star Graphs (CD-CSG) to divide a causal directed acyclic graph into multiple CSGs for causal discovery. In this framework, we also propose a generalized learning in CSGs based on a variational approach to learn the representative intermediate variable of CSG’s non-central variables. Through the generalized learning in CSGs, the asymmetry in the forward and backward model of CD-CSG can be found to identify the causal directions in the directed acyclic graphs. We further divide the CSGs into three categories and provide the causal identification principle under each category in our proposed framework. Experiments using synthetic data show that the causal relationships between variables can be effectively identified with CD-CSG and the accuracy of CD-CSG is higher than the best existing model. By applying CD-CSG to real-world data, our proposed method can greatly augment the applicability and effectiveness of causal discovery.}
}


@article{DBLP:journals/tkdd/Wang0DZHZL23,
	author = {Dexian Wang and
                  Tianrui Li and
                  Ping Deng and
                  Fan Zhang and
                  Wei Huang and
                  Pengfei Zhang and
                  Jia Liu},
	title = {A Generalized Deep Learning Clustering Algorithm Based on Non-Negative
                  Matrix Factorization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {99:1--99:20},
	year = {2023},
	url = {https://doi.org/10.1145/3584862},
	doi = {10.1145/3584862},
	timestamp = {Mon, 07 Oct 2024 21:54:25 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Wang0DZHZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Clustering is a popular research topic in the field of data mining, in which the clustering method based on non-negative matrix factorization (NMF) has been widely employed. However, in the update process of NMF, there is no learning rate to guide the update as well as the update depends on the data itself, which leads to slow convergence and low clustering accuracy. To solve these problems, a generalized deep learning clustering (GDLC) algorithm based on NMF is proposed in this article. Firstly, a nonlinear constrained NMF (NNMF) algorithm is constructed to achieve sequential updates of the elements in the matrix guided by the learning rate. Then, the gradient values corresponding to the element update are transformed into generalized weights and generalized biases, by inputting the elements as well as their corresponding generalized weights and generalized biases into the nonlinear activation function to construct the GDLC algorithm. In addition, for improving the understanding of the GDLC algorithm, its detailed inference procedure and algorithm design are provided. Finally, the experimental results on eight datasets show that the GDLC algorithm has efficient performance.}
}


@article{DBLP:journals/tkdd/BiswasAC23,
	author = {Tarun Kumer Biswas and
                  Alireza Abbasi and
                  Ripon Kumar Chakrabortty},
	title = {Robust Influence Maximization Under Both Aleatory and Epistemic Uncertainty},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {100:1--100:27},
	year = {2023},
	url = {https://doi.org/10.1145/3587100},
	doi = {10.1145/3587100},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/BiswasAC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncertainty is ubiquitous in almost every real-life optimization problem, which must be effectively managed to get a robust outcome. This is also true for the Influence Maximization (IM) problem, which entails locating a set of influential users within a social network. However, most of the existing IM approaches have overlooked the uncertain factors in finding the optimal solution, which often leads to subpar performance in reality. A few recent studies have considered only the epistemic uncertainty (i.e., arises from the imprecise data), while ignoring completely the aleatory uncertainty (i.e., arises from natural or physical variability). In this article, we propose a formulation and a novel algorithm for the Robust Influence Maximization (RIM) problem under both types of uncertainties. First, we develop a robust influence spread function under aleatory uncertainty that, in contrast to the existing IM theory, is no longer monotone and submodular. Thereafter, we expand our RIM formulation to incorporate epistemic uncertainty aiming to maximize the robust ratio between the selected worst-case solution and the best-case optimal solution, adopting a conservative approach. Furthermore, using a chance-constraint-based method, we investigated feasibility robustness by accounting for the uncertainties related to constraint functions. Finally, an Evolutionary Algorithm\xa0(named EA-RIM) is designed to solve the proposed formulation of the RIM problem. Experimental evaluation results on four empirical datasets show that our proposed formulation and algorithm are more effective in dealing with uncertainties and finding an optimal solution for the RIM problem.}
}


@article{DBLP:journals/tkdd/Guo0WZ023,
	author = {Yuhui Guo and
                  Xun Liang and
                  Bo Wu and
                  Xiangping Zheng and
                  Xuan Zhang},
	title = {Dual-aware Domain Mining and Cross-aware Supervision for Weakly-supervised
                  Semantic Segmentation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {101:1--101:19},
	year = {2023},
	url = {https://doi.org/10.1145/3589343},
	doi = {10.1145/3589343},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Guo0WZ023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Weakly Supervised Semantic Segmentation with image-level annotation uses localization maps from the classifier to generate pseudo labels. However, such localization maps focus only on sparse salient object regions, it is difficult to generate high-quality segmentation labels, which deviates from the requirement of semantic segmentation. To address this issue, we propose a dual-aware domain mining and cross-aware supervision (DDMCAS) method for weakly-supervised semantic segmentation. Specifically, we propose a dual-aware domain mining (DDM) module consisting of graph-based global reasoning unit and salient-region extension controller, which produces dense localization maps by exploring object features in salient regions and adjacent non-salient regions simultaneously. In order to further bridge the gap between salient regions and adjacent non-salient regions to generate more refined localization maps, we propose a cross-aware supervision (CAS) strategy to recover missing parts of the target objects and enhance weak attention in adjacent non-salient regions, leading to pseudo labels of higher quality for training the segmentation network. Based on the generated pseudo-labels, extensive experiments on PASCAL VOC 2012 dataset demonstrate that our method outperforms state-of-the-art methods using image-level labels for weakly supervised semantic segmentation.}
}


@article{DBLP:journals/tkdd/ChengHZ23,
	author = {Jiezhu Cheng and
                  Kaizhu Huang and
                  Zibin Zheng},
	title = {Fitting Imbalanced Uncertainties in Multi-output Time Series Forecasting},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {102:1--102:23},
	year = {2023},
	url = {https://doi.org/10.1145/3584704},
	doi = {10.1145/3584704},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChengHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We focus on multi-step ahead time series forecasting with the multi-output strategy. From the perspective of multi-task learning (MTL), we recognize imbalanced uncertainties between prediction tasks of different future time steps. Unexpectedly, trained by the standard summed Mean Squared Error (MSE) loss, existing multi-output forecasting models may suffer from performance drops due to the inconsistency between the loss function and the imbalance structure. To address this problem, we reformulate each prediction task as a distinct Gaussian Mixture Model (GMM) and derive a multi-level Gaussian mixture loss function to better fit imbalanced uncertainties in multi-output time series forecasting. Instead of using the two-step Expectation-Maximization (EM) algorithm, we apply the self-attention mechanism on the task-specific parameters to learn the correlations between different prediction tasks and generate the weight distribution for each GMM component. In this way, our method jointly optimizes the parameters of the forecasting model and the mixture model simultaneously in an end-to-end fashion, avoiding the need of two-step optimization. Experiments on three real-world datasets demonstrate the effectiveness of our multi-level Gaussian mixture loss compared to models trained with the standard summed MSE loss function. All the experimental data and source code are available at https://github.com/smallGum/GMM-FNN.}
}


@article{DBLP:journals/tkdd/ZhangSCZHS23,
	author = {Xuanqi Zhang and
                  Qiangqiang Shen and
                  Yongyong Chen and
                  Guokai Zhang and
                  Zhongyun Hua and
                  Jingyong Su},
	title = {Multi-view Ensemble Clustering via Low-rank and Sparse Decomposition:
                  From Matrix to Tensor},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {103:1--103:19},
	year = {2023},
	url = {https://doi.org/10.1145/3589768},
	doi = {10.1145/3589768},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangSCZHS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a significant extension of classical clustering methods, ensemble clustering first generates multiple basic clusterings and then fuses them into one consensus partition by solving a problem concerning graph partition with respect to the co-association matrix. Although the collaborative cluster structure among basic clusterings can be well discovered by ensemble clustering, most advanced ensemble clustering utilizes the self-representation strategy with the constraint of low-rank to explore a shared consensus representation matrix in multiple views. However, they still encounter two challenges: (1) high computational cost caused by both the matrix inversion operation and singular value decomposition of large-scale square matrices; (2) less considerable attention on high-order correlation attributed to the pursue of the two-dimensional pair-wise relationship matrix. In this article, based on low-rank and sparse decomposition from both matrix and tensor perspectives, we propose two novel multi-view ensemble clustering methods, which tangibly decrease computational complexity. Specifically, our first method utilizes low-rank and sparse matrix decomposition to learn one common co-association matrix, while our last method constructs all co-association matrices into one third-order tensor to investigate the high-order correlation among multiple views by low-rank and sparse tensor decomposition. We adopt the alternating direction method of multipliers to solve two convex models by dividing them into several subproblems with closed-form solution. Experimental results on ten real-world datasets prove the effectiveness and efficiency of the proposed two multi-view ensemble clustering methods by comparing them with other advanced ensemble clustering methods.}
}


@article{DBLP:journals/tkdd/Zhang0TZ0M23,
	author = {Sensen Zhang and
                  Xun Liang and
                  Hui Tang and
                  Xiangping Zheng and
                  Alex X. Zhang and
                  Yuefeng Ma},
	title = {DuCape: Dual Quaternion and Capsule Network-Based Temporal Knowledge
                  Graph Embedding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {104:1--104:19},
	year = {2023},
	url = {https://doi.org/10.1145/3589644},
	doi = {10.1145/3589644},
	timestamp = {Fri, 29 Sep 2023 15:51:58 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Zhang0TZ0M23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, with the development of temporal knowledge graph technology, more and more Temporal Knowledge Graph Embedded (TKGE) models have been developed. The effectiveness of TKGE largely depends on the ability to model intrinsic relation patterns and capture specific information about entities and relations. However, existing approaches can capture only some of them with insufficient modeling capacity, and none has a “deep” architecture for modeling the entries in a quadruple at the same dimension. In this article, we propose a more powerful KGE framework named DuCape, which combines a dual quaternion and capsule network in modeling for the first time to make up for the defects of existing TKGE models. In dual quaternion vector space, the head entity learns a k-dimensional rigid transformation parametrized by relation and time, falling near its corresponding tail entity. Further, we employ the embeddings of entities, relations, and time trained from dual quaternion vector space as the input to capsule networks. Experimental results on several basic datasets show that the DuCape model constructed in this article is superior to existing state-of-the-art models.}
}


@article{DBLP:journals/tkdd/Wang0LD023,
	author = {Hao Wang and
                  Bin Guo and
                  Jiaqi Liu and
                  Yasan Ding and
                  Zhiwen Yu},
	title = {Towards Informative and Diverse Dialogue Systems Over Hierarchical
                  Crowd Intelligence Knowledge Graph},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {7},
	pages = {105:1--105:25},
	year = {2023},
	url = {https://doi.org/10.1145/3583758},
	doi = {10.1145/3583758},
	timestamp = {Tue, 16 Apr 2024 15:32:13 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Wang0LD023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge-enhanced dialogue systems aim at generating factually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of “crowd intelligence knowledge” extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.}
}


@article{DBLP:journals/tkdd/0002ETFC00023,
	author = {Mohamed Ragab and
                  Emadeldeen Eldele and
                  Wee Ling Tan and
                  Chuan{-}Sheng Foo and
                  Zhenghua Chen and
                  Min Wu and
                  Chee Keong Kwoh and
                  Xiaoli Li},
	title = {{ADATIME:} {A} Benchmarking Suite for Domain Adaptation on Time Series
                  Data},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {106:1--106:18},
	year = {2023},
	url = {https://doi.org/10.1145/3587937},
	doi = {10.1145/3587937},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/0002ETFC00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised domain adaptation methods aim at generalizing well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite (AdaTime) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting state-of-the-art visual domain adaptation methods to time series data as well as the recent methods specifically developed for time series data. We conduct extensive experiments to evaluate 11 state-of-the-art methods on five representative datasets spanning 50 cross-domain scenarios. Our results suggest that with careful selection of hyper-parameters, visual domain adaptation methods are competitive with methods proposed for time series domain adaptation. In addition, we find that hyper-parameters could be selected based on realistic model selection approaches. Our work unveils practical insights for applying domain adaptation methods on time series data and builds a solid foundation for future works in the field. The code is available at github.com/emadeldeen24/AdaTime.}
}


@article{DBLP:journals/tkdd/HalsteadKRPB23,
	author = {Ben Halstead and
                  Yun Sing Koh and
                  Patricia Riddle and
                  Mykola Pechenizkiy and
                  Albert Bifet},
	title = {Combining Diverse Meta-Features to Accurately Identify Recurring Concept
                  Drift in Data Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {107:1--107:36},
	year = {2023},
	url = {https://doi.org/10.1145/3587098},
	doi = {10.1145/3587098},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HalsteadKRPB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept, is currently present in order to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation being key to accurate identification. Existing concept representations are based on meta-features, efficient univariate summaries of a concept. However, no single meta-feature can fully represent a concept, leading to severe accuracy loss when existing representations cannot describe concept drift. To avoid these failure cases, we propose the first general framework for combining a diverse range of meta-features into a single representation. We solve two main challenges, first presenting a method of efficiently computing, storing, and querying an arbitrary set of meta-features as a single representation, showing that a combination of meta-features may successfully avoid failure cases seen with existing methods. Second, we present the first method for dynamically learning which meta-features distinguish concepts in any given dataset, significantly improving performance. Our proposed approach enables state-of-the-art feature selection methods, such as mutual information, to be applied to concept representation meta-features for the first time. We investigate tradeoffs between memory budget and classification performance, observing accuracy increases of up to 16% by dynamically weighting the contribution of each meta-feature.}
}


@article{DBLP:journals/tkdd/Shi0FXLW23,
	author = {Linrui Shi and
                  Zheng Zhang and
                  Zizhu Fan and
                  Chao Xi and
                  Zhengming Li and
                  Gaochang Wu},
	title = {Kernel Fisher Dictionary Transfer Learning},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {108:1--108:17},
	year = {2023},
	url = {https://doi.org/10.1145/3588575},
	doi = {10.1145/3588575},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Shi0FXLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dictionary learning is an efficient knowledge representation method that can learn the essential features of data. Traditional dictionary learning methods are difficult to obtain nonlinear information when processing large-scale and high-dimensional datasets. While most dictionary learning algorithms are based on the assumption that the training data and test data have the same feature distribution, which is not always true in practical applications. To address the above problems, we propose the Kernel Fisher Dictionary Transfer Learning (KFDTL) algorithm. First, we map each sample to high-dimensional space through kernel mapping and use any dictionary learning algorithm to learn the essential features. Then, the feature-based transfer learning method is performed to predict the labels of the target samples. This method includes three main contributions: (1) KFDTL constructs a discriminative Fisher embedding model to make the same class samples have similar coding coefficients; (2) Based on the relationship between profiles and atoms, KFDTL constructs an adaptive model that adapts source domain samples to target domain samples; (3) The kernel method is used to efficiently solve nonlinear problems. Experiments on a large number of public image datasets have proved the effectiveness of the proposed method. The source code of the proposed method is available at https://github.com/zzfan3/KFDTL.}
}


@article{DBLP:journals/tkdd/SunSLZHJC23,
	author = {Heli Sun and
                  Miaomiao Sun and
                  Xuechun Liu and
                  Linlin Zhu and
                  Liang He and
                  Xiaolin Jia and
                  Yuan Chen},
	title = {Graph Neural Networks with Motisf-aware for Tenuous Subgraph Finding},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {109:1--109:19},
	year = {2023},
	url = {https://doi.org/10.1145/3589643},
	doi = {10.1145/3589643},
	timestamp = {Mon, 04 Sep 2023 20:40:38 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SunSLZHJC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tenuous subgraph finding aims to detect a subgraph with few social interactions and weak relationships among nodes. Despite significant efforts made on this task, they are mostly carried out in view of graph-structured data. These methods depend on calculating the shortest path and need to enumerate all the paths between nodes, which suffer the combinatorial explosion. Moreover, they all lack the integration of neighborhood information. To this end, we propose a novel model named Graph Neural Network with Motif-aware for tenuous subgraph finding (GNNM), a neighborhood aggregation-based GNN framework that can capture the latent relationship between nodes. We design a GNN module to project nodes into a low-dimensional vector combining the higher-order correlation within nodes based on a motif-aware module. Then we design greedy algorithms in vector space to obtain a tenuous subgraph whose size is greater than a specified constraint. Particularly, considering that existing evaluation indicators cannot capture the latent friendship between nodes, we introduce a novel Potential Friend concept to measure the tenuity of a graph from a new perspective. Experimental results on the real-world and synthetic datasets demonstrate that our proposed method GNNM outperforms existing algorithms in efficiency and subgraph quality.}
}


@article{DBLP:journals/tkdd/WuZLH0C23,
	author = {Likang Wu and
                  Hongke Zhao and
                  Zhi Li and
                  Zhenya Huang and
                  Qi Liu and
                  Enhong Chen},
	title = {Learning the Explainable Semantic Relations via Unified Graph Topic-Disentangled
                  Neural Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {110:1--110:23},
	year = {2023},
	url = {https://doi.org/10.1145/3589964},
	doi = {10.1145/3589964},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/WuZLH0C23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph Neural Networks (GNNs) such as Graph Convolutional Networks (GCNs) can effectively learn node representations via aggregating neighbors based on the relation graph. However, despite a few exceptions, most of the previous work in this line does not consider the topical semantics underlying the edges, making the node representations less effective and the learned relation between nodes hard to explain. For instance, the current GNNs make us usually don’t know what is the reason for the connection of network nodes, such as the specific research topics cited in this article and the concerns among friends on social platforms. Some methods have begun to explore the extraction of relation semantics in recent related literature, but existing studies generally face two bottlenecks, i.e., either being unable to explain the mined latent relations to ensure their reasonableness and independence, or demanding the textual content of edges which is unavailable in most real-world datasets. Actually, these two issues are both crucial in practical use. In our work, we propose a novel Topic-Disentangled Graph Neural Network (TDG) to address the above two issues at the same time, which explores the relation topics from the perspective of node contents. We design an optimized graph topic module to handle node features to construct independent and explainable semantic subspaces, then the reasonable relation topics that correspond to these subspaces are assigned to each graph relation via a neighborhood routing mechanism. Our proposed model can be easily combined with related graph tasks to form an end-to-end model, to avoid the risk of deviation between node representation space and task space. To evaluate the efficiency of our model, sufficient node-related tasks are conducted on three public datasets in the experimental section. The results show the obvious superiority of TDG compared with the state-of-the-art models.}
}


@article{DBLP:journals/tkdd/Jia0QZDZCM23,
	author = {Bohan Jia and
                  Jian Cao and
                  Shiyou Qian and
                  Nengjun Zhu and
                  Xin Dong and
                  Liang Zhang and
                  Lei Cheng and
                  Linjian Mo},
	title = {{SMONE:} {A} Session-based Recommendation Model Based on Neighbor
                  Sessions with Similar Probabilistic Intentions},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {111:1--111:22},
	year = {2023},
	url = {https://doi.org/10.1145/3587099},
	doi = {10.1145/3587099},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Jia0QZDZCM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A session-based recommendation system (SRS) tries to predict the next possible choice of anonymous users. In recent years, graph neural network (GNN) models have been successfully applied to SRSs and have achieved great success. Using GNN models in SRSs, each session graph is processed successively to obtain the embedding of the node (i.e, each action on an item), which is then imported into the prediction module to generate recommendation results. However, solely depending on the session graph to obtain the node embeddings is not sufficient because each session only involves a few items. Therefore, neighbor sessions have been used to extend the session graph to learn more informative node representations. In this paper, we introduce a Session-based recommendation MOdel based on Neighbor sessions with similar probabilistic int Entions(SMONE). SMONE models the intentions behind sessions in a probabilistic way and retrieves the neighbor sessions with similar intentions. After the neighbor sessions are found, the target session and its neighbor sessions are modeled as a hypyergraph to learn the contextualized embeddings, which are combined with item embeddings through GNN to produce the final item recommendations. Experiments on real-world datasets prove the effectiveness and superiority of SMONE.}
}


@article{DBLP:journals/tkdd/HassanAKSA23,
	author = {Zohair Raza Hassan and
                  Sarwan Ali and
                  Imdadullah Khan and
                  Mudassir Shabbir and
                  Waseem Abbas},
	title = {Computing Graph Descriptors on Edge Streams},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {112:1--112:25},
	year = {2023},
	url = {https://doi.org/10.1145/3591468},
	doi = {10.1145/3591468},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/HassanAKSA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Feature extraction is an essential task in graph analytics. These feature vectors, called graph descriptors, are used in downstream vector-space-based graph analysis models. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy. However, known algorithms to compute meaningful descriptors do not scale to large graphs since: (1) they require storing the entire graph in memory, and (2) the end-user has no control over the algorithm’s runtime. In this article, we present streaming algorithms to approximately compute three different graph descriptors capturing the essential structure of graphs. Operating on edge streams allows us to avoid storing the entire graph in memory, and controlling the sample size enables us to keep the runtime of our algorithms within desired bounds. We demonstrate the efficacy of the proposed descriptors by analyzing the approximation error and classification accuracy. Our scalable algorithms compute descriptors of graphs with millions of edges within minutes. Moreover, these descriptors yield predictive accuracy comparable to the state-of-the-art methods but can be computed using only 25% as much memory.}
}


@article{DBLP:journals/tkdd/CanforaMS23,
	author = {Gerardo Canfora and
                  Francesco Mercaldo and
                  Antonella Santone},
	title = {A Novel Classification Technique based on Formal Methods},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {113:1--113:30},
	year = {2023},
	url = {https://doi.org/10.1145/3592796},
	doi = {10.1145/3592796},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/CanforaMS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.}
}


@article{DBLP:journals/tkdd/LinLGXY23,
	author = {Bei Lin and
                  You Li and
                  Ning Gui and
                  Zhuopeng Xu and
                  Zhiwu Yu},
	title = {Multi-view Graph Representation Learning Beyond Homophily},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {114:1--114:21},
	year = {2023},
	url = {https://doi.org/10.1145/3592858},
	doi = {10.1145/3592858},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LinLGXY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised graph representation learning (GRL) aims at distilling diverse graph information into task-agnostic embeddings without label supervision. Due to a lack of support from labels, recent representation learning methods usually adopt self-supervised learning, and embeddings are learned by solving a handcrafted auxiliary task (so-called pretext task). However, partially due to the irregular non-Euclidean data in graphs, the pretext tasks are generally designed under homophily assumptions and cornered in the low-frequency signals, which results in significant loss of other signals, especially high-frequency signals widespread in graphs with heterophily. Motivated by this limitation, we propose a multi-view perspective and the usage of diverse pretext tasks to capture different signals in graphs into embeddings. A novel framework, denoted as Multi-view Graph Encoder (MVGE), is proposed, and a set of key designs are identified. More specifically, a set of new pretext tasks are designed to encode different types of signals, and a straightforward operation is proposed to maintain both the commodity and personalization in both the attribute and the structural levels. Extensive experiments on synthetic and real-world network datasets show that the node representations learned with MVGE achieve significant performance improvements in three different downstream tasks, especially on graphs with heterophily.}
}


@article{DBLP:journals/tkdd/TajeunaBW23,
	author = {Etienne Gael Tajeuna and
                  Mohamed Bouguessa and
                  Shengrui Wang},
	title = {Modeling Regime Shifts in Multiple Time Series},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {115:1--115:31},
	year = {2023},
	url = {https://doi.org/10.1145/3592857},
	doi = {10.1145/3592857},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/TajeunaBW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a mapping grid. By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work.}
}


@article{DBLP:journals/tkdd/Shi0DS0023,
	author = {Dan Shi and
                  Lei Zhu and
                  Xiao Dong and
                  Xuemeng Song and
                  Jingjing Li and
                  Zhiyong Cheng},
	title = {Adaptive Collaborative Soft Label Learning for Unsupervised Multi-View
                  Feature Selection},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {116:1--116:25},
	year = {2023},
	url = {https://doi.org/10.1145/3591467},
	doi = {10.1145/3591467},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/Shi0DS0023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unsupervised multi-view feature selection aims to select informative features with multi-view features and unsupervised learning. It is a challenging problem due to the absence of explicit semantic supervision. Recently, graph theory and hard pseudo-label learning have been adopted to solve multi-view feature selection problems under the unsupervised learning paradigm. However, graph-based methods are difficult to support large-scale real scenarios due to the high computational complexity of graph construction. Moreover, existing methods based on hard pseudo-label learning generally result in significant information loss. In this article, we propose an Adaptive Collaborative Soft Label Learning (ACSLL) model for unsupervised multi-view feature selection. In this model, collaborative soft label learning and multi-view feature selection are integrated into a unified framework. Specifically, we learn the pseudo soft labels from each view feature by a simple and efficient method and fuse them with an adaptive weighting strategy into a joint soft label matrix. This matrix is further used for guiding the feature selection process to identify valuable features. An effective optimization strategy guaranteed with proven convergence is derived to iteratively solve this problem. Experiments demonstrate the superiority of the proposed method in both feature selection accuracy and efficiency.}
}


@article{DBLP:journals/tkdd/0079X0ZG23,
	author = {Hao Zhang and
                  Yewei Xia and
                  Kun Zhang and
                  Shuigeng Zhou and
                  Jihong Guan},
	title = {Conditional Independence Test Based on Residual Similarity},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {117:1--117:18},
	year = {2023},
	url = {https://doi.org/10.1145/3593810},
	doi = {10.1145/3593810},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/0079X0ZG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, many regression-based conditional independence (CI) test methods have been proposed to solve the problem of causal discovery. These methods provide alternatives to test CI of x,y given Z by first removing the information of the controlling set Z from x and y, and then testing the independence between the two residuals Rx,Z and Ry,Z. When the residuals are linearly uncorrelated, the independence test between them is nontrivial. With the ability to calculate inner product in high-dimensional space, kernel-based methods are usually used to achieve this goal, but they are considerably time-consuming. In this paper, we test the independence between two linear combinations under linear structural equation model. We show that the dependence between the two residuals can be captured by the difference between the similarity of Rx,Z and Ry,Z and that of Rx,Z and Rr (Rr is an independent copy of Ry,Z) in high-dimensional space. With this result, we provide a new way to test CI based on the similarity between residuals, which is called SCIT — the abbreviation of Similarity-based CI Testing. Furthermore, we develop two versions of the proposal, called Kernel-SCIT and Neural-SCIT, respectively. Kernel-SCIT calculates the similarity by using kernel functions, while Neural-SCIT approximates the upper bound of the similarity by using deep neural networks. In both algorithms, random permutation tests are performed to control Type I error rate. The proposed tests are evaluated on (conditional) independence test and causal discovery with both synthetic and real datasets. Experimental results show that Kernel-SCIT is simpler yet more efficient and effective than the typical existing kernel-based methods HSIC and KCIT in the cases of small sample size, and Neural-SCIT can significantly boost the performance of CI testing when sufficient samples are available. The source code is available at https://github.com/xyw5vplus1/SCIT.}
}


@article{DBLP:journals/tkdd/YuanMXGL0LK23,
	author = {Junkun Yuan and
                  Xu Ma and
                  Ruoxuan Xiong and
                  Mingming Gong and
                  Xiangyu Liu and
                  Fei Wu and
                  Lanfen Lin and
                  Kun Kuang},
	title = {Instrumental Variable-Driven Domain Generalization with Unobserved
                  Confounders},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {118:1--118:21},
	year = {2023},
	url = {https://doi.org/10.1145/3595380},
	doi = {10.1145/3595380},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YuanMXGL0LK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.}
}


@article{DBLP:journals/tkdd/QinZL23,
	author = {Xi Qin and
                  Cheng Zhong and
                  Hai{-}Xiang Lin},
	title = {Community-Based Influence Maximization Using Network Embedding in
                  Dynamic Heterogeneous Social Networks},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {119:1--119:21},
	year = {2023},
	url = {https://doi.org/10.1145/3594544},
	doi = {10.1145/3594544},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/QinZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence maximization (IM) is a very important issue in social network diffusion analysis. The topology of real social network is large-scale, dynamic, and heterogeneous. The heterogeneity, and continuous expansion and evolution of social network pose a challenge to find influential users. Existing IM algorithms usually assume that social networks are static or dynamic but homogeneous to simplify the complexity of the IM problem. We propose a community-based influence maximization algorithm using network embedding in dynamic heterogeneous social networks. We use DyHATR algorithm to obtain the propagation feature vectors of network nodes, and execute k-means cluster algorithm to transform the original network into a coarse granularity network (CGN). On CGN, we propose a community-based three-hop independent cascade model and construct the objective function of IM problem. We design a greedy heuristics algorithm to solve the IM problem with \\((1-\\frac{1}{e})-\\)approximation guarantee and use community structure to quickly identify seed users and estimate their influence value. Experimental results on real social networks demonstrated that compared with existing IM algorithms, our proposed algorithm had better comprehensive performance with respect to the influence value, more less execution time and memory consumption, and better scalability.}
}


@article{DBLP:journals/tkdd/ZhuangM0S23,
	author = {Jiabo Zhuang and
                  Shunmei Meng and
                  Jing Zhang and
                  Victor S. Sheng},
	title = {Contrastive Learning Based Graph Convolution Network for Social Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {8},
	pages = {120:1--120:21},
	year = {2023},
	url = {https://doi.org/10.1145/3587268},
	doi = {10.1145/3587268},
	timestamp = {Fri, 21 Jul 2023 22:26:48 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhuangM0S23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Exploiting social networks is expected to enhance the performance of recommender systems when interaction information is sparse. Existing social recommendation models focus on modeling multi-graph structures and then aggregating the information from these multiple graphs to learn potential user preferences. However, these methods often employ complex models and redundant parameters to get a slight performance improvement. Contrastive learning has been widely researched as an effective paradigm in the area of recommendation. Most existing contrastive learning-based models usually focus on constructing multi-graph structures to perform graph augmentation for contrastive learning. However, the effect of graph augmentation on contrastive learning is inconclusive. In view of these challenges, in this work, we propose a contrastive learning based graph convolution network for social recommendation (CLSR), which integrates information from both the social graph and the interaction graph. First, we propose a fusion-simplified method to combine the social graph and the interaction graph. Technically, on the basis of exploring users’ interests by interaction graph, we further exploit social connections to alleviate data sparsity. By combining the user embeddings learned through two graphs in a certain proportion, we can obtain user representation at a finer granularity. Meanwhile, we introduce a contrastive learning framework for multi-graph network modeling, where we explore the feasibility of constructing positive and negative samples of contrastive learning by conducting data augmentation on embedding representations. Extensive experiments verify the superiority of CLSR’s contrastive learning framework and fusion-simplified method of integrating social relations.}
}


@article{DBLP:journals/tkdd/ZhangL23,
	author = {Liang Zhang and
                  Cheng Long},
	title = {Road Network Representation Learning: {A} Dual Graph-based Approach},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {121:1--121:25},
	year = {2023},
	url = {https://doi.org/10.1145/3592859},
	doi = {10.1145/3592859},
	timestamp = {Tue, 22 Oct 2024 20:38:19 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named road network representation learning (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a hypergraph, where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the high-order relationships among roads with hyperedges. We then allow information propagation via both the edges in the simple graph and the hyperedges in the hypergraph in a graph neural network context. In addition, we introduce different pretext tasks based on both the simple graph (i.e., graph reconstruction) and the hypergraph (including hypergraph reconstruction and hyperedge classification) for optimizing the representations of roads. The graph reconstruction and hypergraph reconstruction tasks are conventional ones and can capture structural information. The hyperedge classification task can capture long-range relationships between pairs of roads that belong to hyperedges with the same label. We call the resulting model HyperRoad. We further extend HyperRoad to problem settings when additional inputs of road attributes and/or trajectories that are generated on the roads are available. We conduct extensive experiments on two real datasets, for five downstream tasks, and under four problem settings, which demonstrate that our model achieves impressive improvements compared with existing baselines across datasets, tasks, problem settings, and performance metrics. CCS Concepts: •\xa0 Information systems → Data mining; •\xa0 Urban computing; •\xa0 Spatial-temporal systems;}
}


@article{DBLP:journals/tkdd/SyedM23,
	author = {Tahir Syed and
                  Behroz Mirza},
	title = {Self-supervision for Tabular Data by Learning to Predict Additive
                  Homoskedastic Gaussian Noise as Pretext},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {122:1--122:17},
	year = {2023},
	url = {https://doi.org/10.1145/3594720},
	doi = {10.1145/3594720},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/SyedM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The lack of scalability of data annotation translates to the need to decrease dependency on labels. Self-supervision offers a solution with data training themselves. However, it has received relatively less attention on tabular data, data that drive a large proportion of business and application domains. This work, which we name the Statistical Self-Supervisor (SSS), proposes a method for self-supervision on tabular data by defining a continuous perturbation as pretext. It enables a neural network to learn representations by learning to predict the level of additive isotropic Gaussian noise added to inputs. The choice of the pretext transformation is motivated by intrinsic characteristics of a neural network fundamentally performing linear fits under the widely adopted assumption of Gaussianity in its fitting error and the preservation of locality of a data example on the data manifold in the presence of small random perturbations. The transform condenses information in the generated representations, making them better employable for further task-specific prediction as evidenced by performance improvement of the downstream classifier. To evaluate the persistence of performance under low-annotation settings, SSS is evaluated against different levels of label availability to the downstream classifier (1% to 100%) and benchmarked against self- and semi-supervised methods. At the most label-constrained, 1% setting, we report a maximum increase of at least 2.5% against the next-best semi-supervised competing method. We report an increase of more than 1.5% against self-supervised state of the art. Ablation studies also reveal that increasing label availability from 0% to 1% results in a maximum increase of up to 50% on either of the five performance metrics and up to 15% thereafter, indicating diminishing returns in additional annotation.}
}


@article{DBLP:journals/tkdd/LiWCGY23,
	author = {Xiaona Li and
                  Zhu Wang and
                  Xindong Chen and
                  Bin Guo and
                  Zhiwen Yu},
	title = {A Hybrid Continuous-Time Dynamic Graph Representation Learning Model
                  by Exploring Both Temporal and Repetitive Information},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {123:1--123:22},
	year = {2023},
	url = {https://doi.org/10.1145/3596447},
	doi = {10.1145/3596447},
	timestamp = {Wed, 01 Nov 2023 08:59:06 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiWCGY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, dynamic graph representation learning has attracted more and more attention from both academic and industrial communities due to its capabilities of capturing different real-world phenomena. For a dynamic graph represented as a sequence of timestamped events, there are two kinds of evolutionary essences: temporal and repetitive information. At present, the temporal information of interactions (e.g., timestamps) have been deeply explored. However, as another vital nature of dynamic graphs, the repetitive information of interactions between two nodes is neglected, which may lead to inaccurate node representation. To address this issue, we propose a novel continuous-time dynamic graph representation learning model, which consists of a node-level-memory based module, a historical high-order neighborhood based vertical aggregation module and a repetitive-topological information based horizontal aggregation module. In particular, to characterize the evolving pattern of the repetitive information of interactions between a pair of nodes, we put forward a repetitive-interaction based attention mechanism to integrate the two key attributes (i.e., the content and the number of interactions) of repetitive interactions at different moments, based on the insight that the repetitive behaviors of nodes are widespread and essential. We conduct extensive experiments including future link prediction tasks (for transductive and inductive learning) and dynamic node classification task, and results on three real-life dynamic graph datasets demonstrate that the proposed method significantly outperforms state-of-the-art baselines, for both observed nodes and new ones.}
}


@article{DBLP:journals/tkdd/OuJWJWZ23,
	author = {Junjie Ou and
                  Haiming Jin and
                  Xiaocheng Wang and
                  Hao Jiang and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {{STA-TCN:} Spatial-temporal Attention over Temporal Convolutional
                  Network for Next Point-of-interest Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {124:1--124:19},
	year = {2023},
	url = {https://doi.org/10.1145/3596497},
	doi = {10.1145/3596497},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/OuJWJWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed a vastly increasing popularity of location-based social networks (LBSNs), which facilitates studies on the next Point-of-Interest (POI) recommendation problem. A user’s POI visiting behavior shows the sequential transition correlation with previous successive check-ins and the global spatial-temporal correlation with those check-ins that happened a long time ago at a similar time of day and in geographically close areas. Although previous POI recommendation methods attempted to capture these two correlations, several limitations remain to be solved: (1) RNNs are widely adopted to capture the sequential transition correlation, whereas training an RNN is rather time-consuming given the long input check-in sequence. (2) The pairwise proximities on time of day and geographical area of check-ins are crucial for global spatial-temporal correlation learning, but have not been comprehensively considered by previous methods. To tackle these issues, we propose a novel next POI recommendation framework named STA-TCN. Specifically, instead of RNNs, STA-TCN augments the Temporal Convolutional Network with gated input injection to learn sequential transition correlation. Furthermore, STA-TCN fuses two novel grid-difference and time-sensitivity learning mechanisms with attention network to learn the pairwise spatial-temporal proximities among a user’s check-ins. Extensive experiments are conducted on two large-scale real-world LBSN datasets, and the results show that STA-TCN outperforms the best state-of-the-art baseline with an average improvement of 9.71% and 7.88% on hit rate and normalized discounted cumulative gain, respectively.}
}


@article{DBLP:journals/tkdd/JiangHCXL23,
	author = {Shaowei Jiang and
                  Wei He and
                  Lizhen Cui and
                  Yonghui Xu and
                  Lei Liu},
	title = {Modeling Long- and Short-Term User Preferences via Self-Supervised
                  Learning for Next {POI} Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {125:1--125:20},
	year = {2023},
	url = {https://doi.org/10.1145/3597211},
	doi = {10.1145/3597211},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JiangHCXL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the accumulation of check-in data from location-based services, next Point-of-Interest (POI) recommendations are gaining increasing attention. It is well known that the spatio-temporal contextual information of user check-in behavior plays a crucial role in handling vital and inherent challenges in next POI recommendation, including capture of user dynamic preferences and the sparsity problem of check-in data. However, many studies either ignore or simply stack the context features with the embedding of POIs while relying only on POI recommendation loss to optimize the entire model, therefore failing to take full advantage of the potential information in contexts. Additionally, users’ interests are usually unstable and evolve over time, and accordingly recent studies have proposed various approaches to predict users’ next POIs by incorporating contextual information and modeling both their long- and short-term preferences, respectively. Yet many studies overemphasize the final POI recommendation performance, and the association between POI sequences and contextual information is not well embodied in data representations. In this article, we focus on the preceding problems and propose a unified attention framework for next POI recommendation by modeling users’ Long- and Short-term Preferences via Self-supervised Learning (LSPSL). Specifically, based on the self-attention network and two self-supervised optimization objectives, LSPSL first deeply exploits the intrinsic correlations between POI sequences and contextual information through pre-training, which strengthens data representations. Then, supported by pre-trained contextualized embeddings, LSPSL models and fuses users’ complex long- and short-term preferences in a unified way. Extensive experiments on real-world datasets demonstrate the superiority of our model compared with other state-of-the-art approaches.}
}


@article{DBLP:journals/tkdd/JangSELPCK23,
	author = {Jun{-}Gi Jang and
                  Sooyeon Shim and
                  Vladimir Egay and
                  Jeeyong Lee and
                  Jongmin Park and
                  Suhyun Chae and
                  U Kang},
	title = {Accurate Open-Set Recognition for Memory Workload},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {126:1--126:14},
	year = {2023},
	url = {https://doi.org/10.1145/3597027},
	doi = {10.1145/3597027},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/JangSELPCK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How can we accurately identify new memory workloads while classifying known memory workloads? Verifying DRAM (Dynamic Random Access Memory) using various workloads is an important task to guarantee the quality of DRAM. A crucial component in the process is open-set recognition which aims to detect new workloads not seen in the training phase. Despite its importance, however, existing open-set recognition methods are unsatisfactory in terms of accuracy since they fail to exploit the characteristics of workload sequences. In this article, we propose Acorn, an accurate open-set recognition method capturing the characteristics of workload sequences. Acorn extracts two types of feature vectors to capture sequential patterns and spatial locality patterns in memory access. Acorn then uses the feature vectors to accurately classify a subsequence into one of the known classes or identify it as the unknown class. Experiments show that Acorn achieves state-of-the-art accuracy, giving up to 37% points higher unknown class detection accuracy while achieving comparable known class classification accuracy than existing methods.}
}


@article{DBLP:journals/tkdd/QinZBZY23,
	author = {Meng Qin and
                  Chaorui Zhang and
                  Bo Bai and
                  Gong Zhang and
                  Dit{-}Yan Yeung},
	title = {Towards a Better Tradeoff between Quality and Efficiency of Community
                  Detection: An Inductive Embedding Method across Graphs},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {127:1--127:34},
	year = {2023},
	url = {https://doi.org/10.1145/3596605},
	doi = {10.1145/3596605},
	timestamp = {Fri, 24 Nov 2023 13:29:37 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/QinZBZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many network applications can be formulated as NP-hard combinatorial optimization problems of community detection (CD) that partitions nodes of a graph into several groups with dense linkage. Most existing CD methods are transductive, which independently optimized their models for each single graph, and can only ensure either high quality or efficiency of CD by respectively using advanced machine learning techniques or fast heuristic approximation. In this study, we consider the CD task and aims to alleviate its NP-hard challenge. Motivated by the efficient inductive inference of graph neural networks (GNNs), we explore the possibility to achieve a better tradeoff between the quality and efficiency of CD via an inductive embedding scheme across multiple graphs of a system and propose a novel inductive community detection (ICD) method. Concretely, ICD first conducts the offline training of an adversarial dual GNN structure on historical graphs to capture key properties of a system. The trained model is then directly generalized to new graphs of the same system for online CD without additional optimization, where a better tradeoff between quality and efficiency can be achieved. Compared with existing inductive approaches, we develop a novel feature extraction module based on graph coarsening, which can efficiently extract informative feature inputs for GNNs. Moreover, our original designs of adversarial dual GNN and clustering regularization loss further enable ICD to capture permutation-invariant community labels in the offline training and help derive community-preserved embedding to support the high-quality online CD. Experiments on a set of benchmarks demonstrate that ICD can achieve a significant tradeoff between quality and efficiency over various baselines.}
}


@article{DBLP:journals/tkdd/ChenCTLW23,
	author = {Haoran Chen and
                  Xu Chen and
                  Hongwei Tao and
                  Zuhe Li and
                  Xiao Wang},
	title = {Low-rank Representation with Adaptive Dimensionality Reduction via
                  Manifold Optimization for Clustering},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {128:1--128:18},
	year = {2023},
	url = {https://doi.org/10.1145/3589767},
	doi = {10.1145/3589767},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ChenCTLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dimensionality reduction techniques are often used to reduce data dimensionality for computational efficiency or other purposes in existing low-rank representation (LRR)-based methods. However, the two steps of dimensionality reduction and learning low-rank representation coefficients are implemented in an independent way; thus, the adaptability of representation coefficients to the original data space may not be guaranteed. This article proposes a novel model, i.e., low-rank representation with adaptive dimensionality reduction (LRRARD) via manifold optimization for clustering, where dimensionality reduction and learning low-rank representation coefficients are integrated into a unified framework. This model introduces a low-dimensional projection matrix to find the projection that best fits the original data space. And the low-dimensional projection matrix and the low-rank representation coefficients interact with each other to simultaneously obtain the best projection matrix and representation coefficients. In addition, a manifold optimization method is employed to obtain the optimal projection matrix, which is an unconstrained optimization method in a constrained search space. The experimental results on several real datasets demonstrate the superiority of our proposed method.}
}


@article{DBLP:journals/tkdd/LiuWHWNMLC23,
	author = {Ye Liu and
                  Han Wu and
                  Zhenya Huang and
                  Hao Wang and
                  Yuting Ning and
                  Jianhui Ma and
                  Qi Liu and
                  Enhong Chen},
	title = {TechPat: Technical Phrase Extraction for Patent Mining},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {129:1--129:31},
	year = {2023},
	url = {https://doi.org/10.1145/3596603},
	doi = {10.1145/3596603},
	timestamp = {Mon, 12 Feb 2024 16:07:18 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/LiuWHWNMLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, due to the explosive growth of patent applications, patent mining has drawn extensive attention and interest. An important issue of patent mining is that of recognizing the technologies contained in patents, which serves as a fundamental preparation for deeper analysis. To this end, in this article, we make a focused study on constructing a technology portrait for each patent, i.e., to recognize technical phrases concerned in it, which can summarize and represent patents from a technical perspective. Along this line, a critical challenge is how to analyze the unique characteristics of technical phrases and illustrate them with definite descriptions. Therefore, we first generate the detailed descriptions about the technical phrases existing in extensive patents based on different criteria, including various previous works, practical experience, and statistical analyses. Then, considering the unique characteristics of technical phrases and the complex structure of patent documents, such as multi-aspect semantics and multi-level relevances, we further propose a novel unsupervised model, namely TechPat, which can not only automatically recognize technical phrases from massive patents but also avoid the need for expensive human labeling. After that, we evaluate the extraction results from various aspects. Specifically, we propose a novel evaluation metric called Information Retrieval Efficiency (IRE) to quantify the performance of extracted technical phrases from a new perspective. Extensive experiments on real-world patent data demonstrate that the TechPat model can effectively discriminate technical phrases in patents and greatly outperform existing methods. We further apply extracted technical phrases to two practical application tasks, namely patent search and patent classification, where the experimental results confirm the wide application prospects of technical phrases. Finally, we discuss the generalization ability of our proposed methods.}
}


@article{DBLP:journals/tkdd/WangZLZWY23,
	author = {Chunyang Wang and
                  Yanmin Zhu and
                  Haobing Liu and
                  Tianzi Zang and
                  Ke Wang and
                  Jiadi Yu},
	title = {Multifaceted Relation-aware Meta-learning with Dual Customization
                  for User Cold-start Recommendation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {130:1--130:27},
	year = {2023},
	url = {https://doi.org/10.1145/3597458},
	doi = {10.1145/3597458},
	timestamp = {Tue, 17 Dec 2024 16:16:32 +0100},
	biburl = {https://dblp.org/rec/journals/tkdd/WangZLZWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User cold-start scenarios pose great challenges to recommendation systems in accurately capturing user preferences with sparse interaction records. Besides incorporating auxiliary information to enrich user/item representations, recent studies under the schema of meta-learning focus on quickly adapting personalized recommendation models based on cold-start users’ scarce interactions. The majority of meta-learning based recommendation methods follow a bi-level optimization paradigm and learn globally shared initialization across all cold-start recommendation tasks. In addition, to further facilitate the ability of fast adaptation, existing methods have made efforts to tailor task-specific prior knowledge by identifying the individual characteristics of each task. However, we argue that multi-view commonalities between existing users and cold-start users are also essential for precisely distinguishing new tasks, but not comprehensively modeled in previous studies. In this article, we propose a multifaceted relation-aware meta-learning approach namely MeCM for user cold-start recommendation, which enhances task-adaptive initialization customization by extracting multiple views of task relevance. We design a dual customization framework consisting of two successive phases including cluster-level customization and task-level customization. Specifically, MeCM first extracts multifaceted semantic relations between tasks and refines task commonalities into task clusters maintained with memory networks (MNs). Globally learned fast weights corresponding to task clusters are queried to perform cluster-level customization. Then task-level customization is triggered based on contextual information of the target task via interaction-wise encoding. Extensive experiments on real-world datasets demonstrate the superior performance of our model over state-of-the-art meta-learning-based recommendation methods.}
}


@article{DBLP:journals/tkdd/YuDWCXWG23,
	author = {Zhiwen Yu and
                  Minling Dang and
                  Qilong Wu and
                  Liming Chen and
                  Yujin Xie and
                  Yu Wang and
                  Bin Guo},
	title = {An Information Theory Based Method for Quantifying the Predictability
                  of Human Mobility},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {131:1--131:19},
	year = {2023},
	url = {https://doi.org/10.1145/3597500},
	doi = {10.1145/3597500},
	timestamp = {Tue, 13 Aug 2024 08:01:17 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/YuDWCXWG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Research on human mobility drives the development of economy and society. How to predict when and where one will go accurately is one of the core research questions. Existing work is mainly concerned with performance of mobility prediction models. Since accuracy of predict models does not indicate whether or not one’s mobility is inherently easy to predict, there has not been a definite conclusion about that to what extent can our predictions of human mobility be accurate. To help solve this problem, we describe the formalized definition of predictability of human mobility, propose a model based on additive Markov chain to measure the probability of exploration, and further develop an information theory based method for quantifying the predictability considering exploration of human mobility. Then, we extend our method by using mutual information in order to measure the predictability considering external influencing factors, which has not been studied before. Experiments on simulation data and three real-world datasets show that our method yields a tighter upper bound on predictability of human mobility than previous work, and that predictability increased slightly when considering external factors such as weather and temperature.}
}


@article{DBLP:journals/tkdd/RamezaniABR23,
	author = {Maryam Ramezani and
                  Aryan Ahadinia and
                  Amirmohammad Ziaei Bideh and
                  Hamid R. Rabiee},
	title = {Joint Inference of Diffusion and Structure in Partially Observed Social
                  Networks Using Coupled Matrix Factorization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {132:1--132:28},
	year = {2023},
	url = {https://doi.org/10.1145/3599237},
	doi = {10.1145/3599237},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RamezaniABR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called “DiffStru.” The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated independent cascades over LFR networks and real datasets, including Twitter and Memetracker. Experiments on these synthetic and real-world datasets show that the proposed method successfully detects invisible social behaviors, predicts links, and identifies latent features.}
}


@article{DBLP:journals/tkdd/LiGGGW23,
	author = {Yandi Li and
                  Haobo Gao and
                  Yunxuan Gao and
                  Jianxiong Guo and
                  Weili Wu},
	title = {A Survey on Influence Maximization: From an ML-Based Combinatorial
                  Optimization},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {133:1--133:50},
	year = {2023},
	url = {https://doi.org/10.1145/3604559},
	doi = {10.1145/3604559},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/LiGGGW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Influence Maximization (IM) is a classical combinatorial optimization problem, which can be widely used in mobile networks, social computing, and recommendation systems. It aims at selecting a small number of users such that maximizing the influence spread across the online social network. Because of its potential commercial and academic value, there are a lot of researchers focusing on studying the IM problem from different perspectives. The main challenge comes from the NP-hardness of the IM problem and #P-hardness of estimating the influence spread, thus traditional algorithms for overcoming them can be categorized into two classes: heuristic algorithms and approximation algorithms. However, there is no theoretical guarantee for heuristic algorithms, and the theoretical design is close to the limit. Therefore, it is almost impossible to further optimize and improve their performance. With the rapid development of artificial intelligence, technologies based on Machine Learning (ML) have achieved remarkable achievements in many fields. In view of this, in recent years, a number of new methods have emerged to solve combinatorial optimization problems by using ML-based techniques. These methods have the advantages of fast solving speed and strong generalization ability to unknown graphs, which provide a brand-new direction for solving combinatorial optimization problems. Therefore, we abandon the traditional algorithms based on iterative search and review the recent development of ML-based methods, especially Deep Reinforcement Learning, to solve the IM problem and other variants in social networks. We focus on summarizing the relevant background knowledge, basic principles, common methods, and applied research. Finally, the challenges that need to be solved urgently in future IM research are pointed out.}
}


@article{DBLP:journals/tkdd/ZhangZYLLWW23,
	author = {Zan Zhang and
                  Zhe Zhang and
                  Jialu Yao and
                  Lin Liu and
                  Jiuyong Li and
                  Gongqing Wu and
                  Xindong Wu},
	title = {Multi-Label Feature Selection Via Adaptive Label Correlation Estimation},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {134:1--134:28},
	year = {2023},
	url = {https://doi.org/10.1145/3604560},
	doi = {10.1145/3604560},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/ZhangZYLLWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-label learning, each instance is associated with multiple labels simultaneously. Multi-label data often have noisy, irrelevant, and redundant features of high dimensionality. Multi-label feature selection has received considerable attention as an effective means for dealing with high-dimensional multi-label data. Many multi-label feature selection methods exploit label correlations to help select features. However, finding label correlations and selecting features in existing multi-label feature selection methods are often two separate processes, the existence of noises and outliers in training data makes the label correlations exploited from label space less reliable. Therefore, the learned label correlations may mislead the feature selection process and result in the selection of less informative features. This article proposes a novel algorithm named ROAD, i.e., multi-label featuRe selectiOn via ADaptive label correlation estimation. ROAD jointly performs adaptive label correlation exploration and feature selection with alternating optimization to obtain reliable estimation of label correlations, which can more effectively reveal the intrinsic manifold structure among labels and lead to the selection of a more proper feature subset. Comprehensive experiments on several frequently used datasets validate the superiority of ROAD against the state-of-the-art multi-label feature selection algorithms.}
}


@article{DBLP:journals/tkdd/RenGLWWY23,
	author = {Siyuan Ren and
                  Bin Guo and
                  Ke Li and
                  Qianru Wang and
                  Qinfen Wang and
                  Zhiwen Yu},
	title = {CoupledGT: Coupled Geospatial-temporal Data Modeling for Air Quality
                  Prediction},
	journal = {{ACM} Trans. Knowl. Discov. Data},
	volume = {17},
	number = {9},
	pages = {135:1--135:21},
	year = {2023},
	url = {https://doi.org/10.1145/3604616},
	doi = {10.1145/3604616},
	timestamp = {Fri, 27 Oct 2023 20:39:59 +0200},
	biburl = {https://dblp.org/rec/journals/tkdd/RenGLWWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Air pollution seriously affects public health, while effective air quality prediction remains a challenging problem since the complex spatial-temporal couplings exist in multi-area monitoring data of the city. Current approaches rarely consider relative geographical locations when capturing spatial-temporal relations, instead the latent inter-dependencies (i.e., implicit spatial relations) of data as a replacement. However, such relations cannot necessarily reflect the diffusion of air pollutants in the real world, and genuine location-related information could be lost during the implicit relation learning process. In this article, we introduce a new concept, geospatial-temporal data, and propose a novel deep neural network architecture, CoupledGT, to learn the geospatial-temporal couplings within data for air quality prediction. Specifically, the asymmetric diffusion relation of air quality data between two areas is first explicitly represented by the newly developed planar Gaussian diffusion (PGD) equation. And then, a geospatial couplings diffuser (GCD) is designed to parameterize the PGD equation and learn multi-areas diffusion mutually affected geospatial couplings. Besides, the RNN is employed to capture temporal couplings of each area, and incorporated with GCD to learn both shared and unique characteristics of the geospatial-temporal data simultaneously, which empowers the generalization and efficiency of the model. Extensive experiments on two real-world datasets demonstrate our method is robust and outperforms existing baseline methods in air quality prediction tasks.}
}
