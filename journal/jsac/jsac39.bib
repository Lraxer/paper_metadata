@article{DBLP:journals/jsac/LiSOKQHHGE21,
	author = {Geoffrey Ye Li and
                  Walid Saad and
                  Ayfer {\"{O}}zg{\"{u}}r and
                  Peter Kairouz and
                  Zhijin Qin and
                  Jakob Hoydis and
                  Zhu Han and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Jaafar M. H. Elmirghani},
	title = {Series Editorial: Inauguration Issue of the Series on Machine Learning
                  in Communications and Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {1--3},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036785},
	doi = {10.1109/JSAC.2020.3036785},
	timestamp = {Wed, 05 Jan 2022 14:30:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiSOKQHHGE21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the era of the new generation of communication systems, data traffic is expected to continuously strain the capacity of future communication networks. Along with the remarkable growth in data traffic, new applications, such as wearable devices, autonomous systems, and the Internet of Things (IoT), continue to emerge and generate even more data traffic with vastly different requirements. This growth in the application domain brings forward an inevitable need for more intelligent processing, operation, and optimization of future communication networks.}
}


@article{DBLP:journals/jsac/ChenZJAZ21,
	author = {Wei Chen and
                  Bowen Zhang and
                  Shi Jin and
                  Bo Ai and
                  Zhangdui Zhong},
	title = {Solving Sparse Linear Inverse Problems in Communication Systems: {A}
                  Deep Learning Approach With Adaptive Depth},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {4--17},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036959},
	doi = {10.1109/JSAC.2020.3036959},
	timestamp = {Tue, 27 Apr 2021 15:12:54 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenZJAZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sparse signal recovery problems from noisy linear measurements appear in many areas of wireless communications. In recent years, deep learning (DL) based approaches have attracted interests of researchers to solve the sparse linear inverse problem by unfolding iterative algorithms as neural networks. Typically, research concerning DL assume a fixed number of network layers. However, it ignores a key character in traditional iterative algorithms, where the number of iterations required for convergence changes with varying sparsity levels. By investigating on the projected gradient descent, we unveil the drawbacks of the existing DL methods with fixed depth. Then we propose an end-to-end trainable DL architecture, which involves an extra halting score at each layer. Therefore, the proposed method learns how many layers to execute to emit an output, and the network depth is dynamically adjusted for each task in the inference phase. We conduct experiments using both synthetic data and applications including random access in massive MTC and massive MIMO channel estimation, and the results demonstrate the improved efficiency for the proposed approach.}
}


@article{DBLP:journals/jsac/BaleviDJDA21,
	author = {Eren Balevi and
                  Akash S. Doshi and
                  Ajil Jalal and
                  Alexandros G. Dimakis and
                  Jeffrey G. Andrews},
	title = {High Dimensional Channel Estimation Using Deep Generative Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {18--30},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036947},
	doi = {10.1109/JSAC.2020.3036947},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BaleviDJDA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a novel compressed sensing (CS) approach to high dimensional wireless channel estimation by optimizing the input to a deep generative network. Channel estimation using generative networks relies on the assumption that the reconstructed channel lies in the range of a generative model. Channel reconstruction using generative priors outperforms conventional CS techniques and requires fewer pilots. It also eliminates the need of a priori knowledge of the sparsifying basis, instead using the structure captured by the deep generative model as a prior. Using this prior, we also perform channel estimation from one-bit quantized pilot measurements, and propose a novel optimization objective function that attempts to maximize the correlation between the received signal and the generator's channel estimate while minimizing the rank of the channel estimate. Our approach significantly outperforms sparse signal recovery methods such as Orthogonal Matching Pursuit (OMP) and Approximate Message Passing (AMP) algorithms such as EM-GM-AMP for narrowband mmWave channel reconstruction, and its execution time is not noticeably affected by the increase in the number of received pilot symbols.}
}


@article{DBLP:journals/jsac/WangJW21,
	author = {Hui{-}Ming Wang and
                  Jia{-}Cheng Jiang and
                  Yu{-}Ning Wang},
	title = {Model Refinement Learning and an Example on Channel Estimation With
                  Universal Noise Model},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {31--46},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036964},
	doi = {10.1109/JSAC.2020.3036964},
	timestamp = {Thu, 16 Feb 2023 16:21:17 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangJW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Model-based method and data-based method are two basic approaches for the design of wireless communication systems. Model-based methods suffer from inaccurate modeling assumptions due to excessively complex environment. Recently, data-based methods have achieved remarkable performances in the communication system design without the knowledge of accurate model but encounter some challenges such as, lack of available labelled training data and explainability. In this paper, we propose a novel hybrid idea to integrate the strengths of both data and model-based methods, named model refinement learning , which is training affordable, theoretically interpretable and self-adapting. To show the idea more concretely, a novel channel estimation algorithm is proposed in the multiple-input single-output (MISO) system in the case where the noise model is unknown. In particular, we utilize a universal mixture of Gaussian (MoG) model, which can adaptively adjust the involved parameters to fit the true noise distribution by using observed data. We propose a novel variational inference framework to achieve automatical noise model refinement and design the corresponding online channel estimator. To reduce the online algorithm overhead, we propose a decoupled variational Bayesian method to achieve linear computational complexity. Simulations show that our proposed method outperforms both the model-based and data-based counterparts.}
}


@article{DBLP:journals/jsac/SunWST21,
	author = {Li Sun and
                  Yuwei Wang and
                  A. Lee Swindlehurst and
                  Xiao Tang},
	title = {Generative-Adversarial-Network Enabled Signal Detection for Communication
                  Systems With Unknown Channel Models},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {47--60},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036954},
	doi = {10.1109/JSAC.2020.3036954},
	timestamp = {Thu, 02 Nov 2023 16:27:12 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunWST21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Viterbi algorithm is widely adopted in digital communication systems because of its capability of realizing maximum-likelihood signal sequence detection. However, implementation of the Viterbi algorithm requires instantaneous channel state information (CSI) to be available at the receiver. This is difficult to satisfy in some emerging communication systems such as molecular communications, underwater optical communications, etc, where the underlying channel models are highly complex or completely unknown. ViterbiNet, developed in the prior literature, is a promising framework to cope with this challenge, where deep learning (DL) techniques are combined with the Viterbi Algorithm to enable near-optimal signal detection without CSI. This paper offers a non-trivial variation of ViterbiNet based on generative adversarial networks (GAN). Specifically, a novel architecture using GAN is designed to directly learn the channel transition probability (CTP) from receiver observations, which is the only part of the Viterbi algorithm that is channel-dependent. With the learned CTP, the classical Viterbi algorithm can be implemented without modifications. To make the proposed architecture applicable to time-varying channels, we further develop two methods to fine-tune the learned CTP online. In the first method, pilots within each frame are exploited to update the CTP learning network; In the second method, a decision-directed approach is devised to generate training data in real-time, which is utilized to re-train the learning network. By combining these two approaches, the receiver is able to track the dynamic channel conditions without being trained from scratch. Numerical simulations demonstrate the superiority of the proposed design compared to existing methods.}
}


@article{DBLP:journals/jsac/LuCCMLV21,
	author = {Yuxin Lu and
                  Peng Cheng and
                  Zhuo Chen and
                  Wai Ho Mow and
                  Yonghui Li and
                  Branka Vucetic},
	title = {Deep Multi-Task Learning for Cooperative {NOMA:} System Design and
                  Principles},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {61--78},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036943},
	doi = {10.1109/JSAC.2020.3036943},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LuCCMLV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Envisioned as a promising component of the future wireless Internet-of-Things (IoT) networks, the non-orthogonal multiple access (NOMA) technique can support massive connectivity with a significantly increased spectral efficiency. Cooperative NOMA is able to further improve the communication reliability of users under poor channel conditions. However, the conventional system design suffers from several inherent limitations and is not optimized from the bit error rate (BER) perspective. In this article, we develop a novel deep cooperative NOMA scheme, drawing upon the recent advances in deep learning (DL). We develop a novel hybrid-cascaded deep neural network (DNN) architecture such that the entire system can be optimized in a holistic manner. On this basis, we construct multiple loss functions to quantify the BER performance and propose a novel multi-task oriented two-stage training method to solve the end-to-end training problem in a self-supervised manner. The learning mechanism of each DNN module is then analyzed based on information theory, offering insights into the explainable DNN architecture and its corresponding training method. We also adapt the proposed scheme to handle the power allocation (PA) mismatch between training and inference and incorporate it with channel coding to combat signal deterioration. Simulation results verify its advantages over orthogonal multiple access (OMA) and the conventional cooperative NOMA scheme in various scenarios.}
}


@article{DBLP:journals/jsac/CaciularuRRGB21,
	author = {Avi Caciularu and
                  Nir Raviv and
                  Tomer Raviv and
                  Jacob Goldberger and
                  Yair Be'ery},
	title = {perm2vec: Attentive Graph Permutation Selection for Decoding of Error
                  Correction Codes},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {79--88},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036951},
	doi = {10.1109/JSAC.2020.3036951},
	timestamp = {Thu, 21 Jan 2021 17:36:21 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CaciularuRRGB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Error correction codes are an integral part of communication applications, boosting the reliability of transmission. The optimal decoding of transmitted codewords is the maximum likelihood rule, which is NP-hard due to the curse of dimensionality. For practical realizations, sub-optimal decoding algorithms are employed; yet limited theoretical insights prevent one from exploiting the full potential of these algorithms. One such insight is the choice of permutation in permutation decoding. We present a data-driven framework for permutation selection, combining domain knowledge with machine learning concepts such as node embedding and self-attention. Significant and consistent improvements in the bit error rate are introduced for all simulated codes, over the baseline decoders. To the best of the authors' knowledge, this work is the first to leverage the benefits of the neural Transformer networks in physical layer communication systems.}
}


@article{DBLP:journals/jsac/JankowskiGM21,
	author = {Mikolaj Jankowski and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Krystian Mikolajczyk},
	title = {Wireless Image Retrieval at the Edge},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {89--100},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036955},
	doi = {10.1109/JSAC.2020.3036955},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JankowskiGM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the image retrieval problem at the wireless edge, where an edge device captures an image, which is then used to retrieve similar images from an edge server. These can be images of the same person or a vehicle taken from other cameras at different times and locations. Our goal is to maximize the accuracy of the retrieval task under power and bandwidth constraints over the wireless link. Due to the stringent delay constraint of the underlying application, sending the whole image at a sufficient quality is not possible. We propose two alternative schemes based on digital and analog communications, respectively. In the digital approach, we first propose a deep neural network (DNN) aided retrieval-oriented image compression scheme, whose output bit sequence is transmitted over the channel using conventional channel codes. In the analog joint source and channel coding (JSCC) approach, the feature vectors are directly mapped into channel symbols. We evaluate both schemes on image based re-identification (re-ID) tasks under different channel conditions, including both static and fading channels. We show that the JSCC scheme significantly increases the end-to-end accuracy, speeds up the encoding process, and provides graceful degradation with channel conditions. The proposed architecture is evaluated through extensive simulations on different datasets and channel conditions, as well as through ablation studies.}
}


@article{DBLP:journals/jsac/ShenSZL21,
	author = {Yifei Shen and
                  Yuanming Shi and
                  Jun Zhang and
                  Khaled B. Letaief},
	title = {Graph Neural Networks for Scalable Radio Resource Management: Architecture
                  Design and Theoretical Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {101--115},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036965},
	doi = {10.1109/JSAC.2020.3036965},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ShenSZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning has recently emerged as a disruptive technology to solve challenging radio resource management problems in wireless networks. However, the neural network architectures adopted by existing works suffer from poor scalability and generalization, and lack of interpretability. A long-standing approach to improve scalability and generalization is to incorporate the structures of the target task into the neural network architecture. In this paper, we propose to apply graph neural networks (GNNs) to solve large-scale radio resource management problems, supported by effective neural network architecture design and theoretical analysis. Specifically, we first demonstrate that radio resource management problems can be formulated as graph optimization problems that enjoy a universal permutation equivariance property. We then identify a family of neural networks, named message passing graph neural networks (MPGNNs). It is demonstrated that they not only satisfy the permutation equivariance property, but also can generalize to large-scale problems, while enjoying a high computational efficiency. For interpretablity and theoretical guarantees, we prove the equivalence between MPGNNs and a family of distributed optimization algorithms, which is then used to analyze the performance and generalization of MPGNN-based methods. Extensive simulations, with power control and beamforming as two examples, demonstrate that the proposed method, trained in an unsupervised manner with unlabeled samples, matches or even outperforms classic optimization-based algorithms without domain-specific knowledge. Remarkably, the proposed method is highly scalable and can solve the beamforming problem in an interference channel with 1000 transceiver pairs within 6 milliseconds on a single GPU.}
}


@article{DBLP:journals/jsac/LiuWMBAS21,
	author = {Yanan Liu and
                  Xianbin Wang and
                  Jie Mei and
                  Gary Boudreau and
                  Hatem Abou{-}zeid and
                  Akram Bin Sediq},
	title = {Situation-Aware Resource Allocation for Multi-Dimensional Intelligent
                  Multiple Access: {A} Proactive Deep Learning Framework},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {116--130},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036969},
	doi = {10.1109/JSAC.2020.3036969},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuWMBAS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the ever-increasing communication services with diverse requirements, situation-aware intelligent utilization of multi-dimensional communication resources is becoming essential. In this paper, considering a time-division-duplex downlink cellular scenario, a deep learning-based framework for multi-dimensional intelligent multiple access (MD-IMA) scheme is developed for beyond 5G and 6G wireless networks to meet the real-time and diverse quality of service (QoS) requirements by fully utilizing the available radio resources in heterogeneous domains. To achieve intelligent operation of MD-IMA, the proposed deep learning scheme is achieved based on the convergence of long short term memory (LSTM) and deep reinforcement learning (DRL). Specifically, an LSTM neural network is used to predict the long-term network dynamics and inference changes in QoS requirements of the MD-IMA. Meanwhile, a deterministic policy gradient (DDPG) algorithm, a model-free DRL technique, is adopted to optimize the multi-dimensional radio resource allocation in real-time by dynamically following the fluctuations of the network situation. With the aid of the DDPG algorithm, radio resource management for MD-IMA can be achieved efficiently with reduced processing latency as compared to the conventional model-based approaches. Furthermore, the effectiveness of our proposed deep learning framework for MD-IMA is validated through real-world cellular traffic data-sets. The experimental results demonstrate that the proposed scheme can outperform state-of-the-art algorithms.}
}


@article{DBLP:journals/jsac/PengS21,
	author = {Hai{-}xia Peng and
                  Xuemin Shen},
	title = {Multi-Agent Reinforcement Learning Based Resource Management in {MEC-}
                  and UAV-Assisted Vehicular Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {131--141},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036962},
	doi = {10.1109/JSAC.2020.3036962},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/PengS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate multi-dimensional resource management for unmanned aerial vehicles (UAVs) assisted vehicular networks. To efficiently provide on-demand resource access, the macro eNodeB and UAV, both mounted with multi-access edge computing (MEC) servers, cooperatively make association decisions and allocate proper amounts of resources to vehicles. Since there is no central controller, we formulate the resource allocation at the MEC servers as a distributive optimization problem to maximize the number of offloaded tasks while satisfying their heterogeneous quality-of-service (QoS) requirements, and then solve it with a multi-agent deep deterministic policy gradient (MADDPG)-based method. Through centrally training the MADDPG model offline, the MEC servers, acting as learning agents, then can rapidly make vehicle association and resource allocation decisions during the online execution stage. From our simulation results, the MADDPG-based method can converge within 200 training episodes, comparable to the single-agent DDPG (SADDPG)-based one. Moreover, the proposed MADDPG-based resource management scheme can achieve higher delay/QoS satisfaction ratios than the SADDPG-based and random schemes.}
}


@article{DBLP:journals/jsac/XieQ21,
	author = {Huiqiang Xie and
                  Zhijin Qin},
	title = {A Lite Distributed Semantic Communication System for Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {142--153},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036968},
	doi = {10.1109/JSAC.2020.3036968},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XieQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of deep learning (DL) and widespread applications of Internet-of-Things (IoT) have made the devices smarter than before, and enabled them to perform more intelligent tasks. However, it is challenging for any IoT device to train and run DL models independently due to its limited computing capability. In this paper, we consider an IoT network where the cloud/edge platform performs the DL based semantic communication (DeepSC) model training and updating while IoT devices perform data collection and transmission based on the trained model. To make it affordable for IoT devices, we propose a lite distributed semantic communication system based on DL, named L-DeepSC, for text transmission with low complexity, where the data transmission from the IoT devices to the cloud/edge works at the semantic level to improve transmission efficiency. Particularly, by pruning the model redundancy and lowering the weight resolution, the L-DeepSC becomes affordable for IoT devices and the bandwidth required for model weight transmission between IoT devices and the cloud/edge is reduced significantly. Through analyzing the effects of fading channels in forward-propagation and back-propagation during the training of L-DeepSC, we develop a channel state information (CSI) aided training processing to decrease the effects of fading channels on transmission. Meanwhile, we tailor the semantic constellation to make it implementable on capacity-limited IoT devices. Simulation demonstrates that the proposed L-DeepSC achieves competitive performance compared with traditional methods, especially in the low signal-to-noise (SNR) region. In particular, while it can reach as large as 40\\times compression ratio without performance degradation.}
}


@article{DBLP:journals/jsac/WangLWLTL21,
	author = {Xiaofei Wang and
                  Ruibin Li and
                  Chenyang Wang and
                  Xiuhua Li and
                  Tarik Taleb and
                  Victor C. M. Leung},
	title = {Attention-Weighted Federated Deep Reinforcement Learning for Device-to-Device
                  Assisted Heterogeneous Collaborative Edge Caching},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {154--169},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036946},
	doi = {10.1109/JSAC.2020.3036946},
	timestamp = {Wed, 07 Dec 2022 23:03:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangLWLTL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to meet the growing demands for multimedia service access and release the pressure of the core network, edge caching and device-to-device (D2D) communication have been regarded as two promising techniques in next generation mobile networks and beyond. However, most existing related studies lack consideration of effective cooperation and adaptability to the dynamic network environments. In this article, based on the flexible trilateral cooperation among user equipment, edge base stations and a cloud server, we propose a D2D-assisted heterogeneous collaborative edge caching framework by jointly optimizing the node selection and cache replacement in mobile networks. We formulate the joint optimization problem as a Markov decision process, and use a deep Q-learning network to solve the long-term mixed integer linear programming problem. We further design an attention-weighted federated deep reinforcement learning (AWFDRL) model that uses federated learning to improve the training efficiency of the Q-learning network by considering the limited computing and storage capacity, and incorporates an attention mechanism to optimize the aggregation weights to avoid the imbalance of local model quality. We prove the convergence of the corresponding algorithm, and present simulation results to show the effectiveness of the proposed AWFDRL framework in reducing average delay of content access, improving hit rate and offloading traffic.}
}


@article{DBLP:journals/jsac/LiuS21,
	author = {Dongzhu Liu and
                  Osvaldo Simeone},
	title = {Privacy for Free: Wireless Federated Learning via Uncoded Transmission
                  With Adaptive Power Control},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {170--185},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036948},
	doi = {10.1109/JSAC.2020.3036948},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) refers to distributed protocols that avoid direct raw data exchange among the participating devices while training for a common learning task. This way, FL can potentially reduce the information on the local data sets that is leaked via communications. In order to provide formal privacy guarantees, however, it is generally necessary to put in place additional masking mechanisms. When FL is implemented in wireless systems via uncoded transmission, the channel noise can directly act as a privacy-inducing mechanism. This paper demonstrates that, as long as the privacy constraint level, measured via differential privacy (DP), is below a threshold that decreases with the signal-to-noise ratio (SNR), uncoded transmission achieves privacy “for free”, i.e., without affecting the learning performance. More generally, this work studies adaptive power allocation (PA) for distributed gradient descent in wireless FL with the aim of minimizing the learning optimality gap under privacy and power constraints. Both orthogonal multiple access (OMA) and non-orthogonal multiple access (NOMA) transmission with “over-the-air-computing” are studied, and solutions are obtained in closed form for an offline optimization setting. Furthermore, heuristic online methods are proposed that leverage iterative one-step-ahead optimization. The importance of dynamic PA and the potential benefits of NOMA versus OMA are demonstrated through extensive simulations.}
}


@article{DBLP:journals/jsac/DingFH21,
	author = {Ningning Ding and
                  Zhixuan Fang and
                  Jianwei Huang},
	title = {Optimal Contract Design for Efficient Federated Learning With Multi-Dimensional
                  Private Information},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {186--200},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036944},
	doi = {10.1109/JSAC.2020.3036944},
	timestamp = {Wed, 02 Feb 2022 07:55:15 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DingFH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an emerging machine learning technique, federated learning has received significant attention recently due to its promising performance in mitigating privacy risks and costs. While most of the existing work of federated learning focused on designing learning algorithm to improve training performance, the incentive issue for encouraging users' participation is still under-explored. This paper presents an analytical study on the server's optimal incentive mechanism design, in the presence of users' multi-dimensional private information (e.g., training cost and communication delay). Specifically, we consider a multi-dimensional contract-theoretic approach, with a key contribution of summarizing users' multi-dimensional private information into a one-dimensional criterion that allows a complete order of users. We further perform the analysis in three information scenarios to reveal the impact of information asymmetry levels on server's optimal strategy and minimum cost. We show that weakly incomplete information does not increase the server's cost (comparing with the complete information scenario) when training data is IID, but it in general does when data is non-IID. Furthermore, the optimal mechanism design under strongly incomplete information is much more challenging, and it is not always optimal for the server to incentivize the group of users with the lowest training cost and delay to participate.}
}


@article{DBLP:journals/jsac/NguyenSHBCP21,
	author = {Hung T. Nguyen and
                  Vikash Sehwag and
                  Seyyedali Hosseinalipour and
                  Christopher G. Brinton and
                  Mung Chiang and
                  H. Vincent Poor},
	title = {Fast-Convergent Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {201--218},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036952},
	doi = {10.1109/JSAC.2020.3036952},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/NguyenSHBCP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has emerged recently as a promising solution for distributing machine learning tasks through modern networks of mobile devices. Recent studies have obtained lower bounds on the expected decrease in model loss that is achieved through each round of federated learning. However, convergence generally requires a large number of communication rounds, which induces delay in model training and is costly in terms of network resources. In this paper, we propose a fast-convergent federated learning algorithm, called \\mathsf {FOLB}\n, which performs intelligent sampling of devices in each round of model training to optimize the expected convergence speed. We first theoretically characterize a lower bound on improvement that can be obtained in each round if devices are selected according to the expected improvement their local models will provide to the current global model. Then, we show that \\mathsf {FOLB}\nobtains this bound through uniform sampling by weighting device updates according to their gradient information. \\mathsf {FOLB}\nis able to handle both communication and computation heterogeneity of devices by adapting the aggregations according to estimates of device’s capabilities of contributing to the updates. We evaluate \\mathsf {FOLB}\nin comparison with existing federated learning algorithms and experimentally show its improvement in trained model accuracy, convergence speed, and/or model stability across various machine learning tasks and datasets.}
}


@article{DBLP:journals/jsac/RenYD21,
	author = {Jinke Ren and
                  Guanding Yu and
                  Guangyao Ding},
	title = {Accelerating {DNN} Training in Wireless Federated Edge Learning Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {219--232},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036971},
	doi = {10.1109/JSAC.2020.3036971},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/RenYD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Training task in classical machine learning models, such as deep neural networks, is generally implemented at a remote cloud center for centralized learning, which is typically time-consuming and resource-hungry. It also incurs serious privacy issue and long communication latency since a large amount of data are transmitted to the centralized node. To overcome these shortcomings, we consider a newly-emerged framework, namely federated edge learning, to aggregate local learning updates at the network edge in lieu of users' raw data. Aiming at accelerating the training process, we first define a novel performance evaluation criterion, called learning efficiency. We then formulate a training acceleration optimization problem in the CPU scenario, where each user device is equipped with CPU. The closed-form expressions for joint batchsize selection and communication resource allocation are developed and some insightful results are highlighted. Further, we extend our learning framework to the GPU scenario. The optimal solution in this scenario is manifested to have the similar structure as that of the CPU scenario, recommending that our proposed algorithm is applicable in more general systems. Finally, extensive experiments validate the theoretical analysis and demonstrate that the proposed algorithm can reduce the training time and improve the learning accuracy simultaneously.}
}


@article{DBLP:journals/jsac/PrakashDAYTAH21,
	author = {Saurav Prakash and
                  Sagar Dhakal and
                  Mustafa Riza Akdeniz and
                  Yair Yona and
                  Shilpa Talwar and
                  Salman Avestimehr and
                  Nageen Himayat},
	title = {Coded Computing for Low-Latency Federated Learning Over Wireless Edge
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {233--250},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036961},
	doi = {10.1109/JSAC.2020.3036961},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/PrakashDAYTAH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning enables training a global model from data located at the client nodes, without data sharing and moving client data to a centralized server. Performance of federated learning in a multi-access edge computing (MEC) network suffers from slow convergence due to heterogeneity and stochastic fluctuations in compute power and communication link qualities across clients. We propose a novel coded computing framework, CodedFedL, that injects structured coding redundancy into federated learning for mitigating stragglers and speeding up the training procedure. CodedFedL enables coded computing for non-linear federated learning by efficiently exploiting distributed kernel embedding via random Fourier features that transforms the training task into computationally favourable distributed linear regression. Furthermore, clients generate local parity datasets by coding over their local datasets, while the server combines them to obtain the global parity dataset. Gradient from the global parity dataset compensates for straggling gradients during training, and thereby speeds up convergence. For minimizing the epoch deadline time at the MEC server, we provide a tractable approach for finding the amount of coding redundancy and the number of local data points that a client processes during training, by exploiting the statistical properties of compute as well as communication delays. We also characterize the leakage in data privacy when clients share their local parity datasets with the server. Additionally, we analyze the convergence rate and iteration complexity of CodedFedL under simplifying assumptions, by treating CodedFedL as a stochastic gradient descent algorithm. Finally, for demonstrating gains that CodedFedL can achieve in practice, we conduct numerical experiments using practical network parameters and benchmark datasets, in which CodedFedL speeds up the overall training time by up to 15× in comparison to the benchmark schemes.}
}


@article{DBLP:journals/jsac/ChenX21,
	author = {Lixing Chen and
                  Jie Xu},
	title = {Seek Common While Shelving Differences: Orchestrating Deep Neural
                  Networks for Edge Service Provisioning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {251--264},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036953},
	doi = {10.1109/JSAC.2020.3036953},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing (EC) platforms, which enable Application Service Providers (ASPs) to deploy applications in close proximity to users, are providing ultra-low latency and location-awareness to a rich portfolio of services. As monetary costs are incurred for renting computing resources on edge servers to enable service provisioning, ASP has to cautiously decide where to deploy the application and how much resources would be needed to deliver satisfactory performance. However, the service provisioning problem exhibits complex correlations with multifarious factors in EC systems, ranging from user behavior to computation offloading, which are difficult to be fully captured by mathematical modeling and also put off traditional machine learning techniques due to the induction of high-dimension state space. The recent success of deep learning (DL) underpins new tools for addressing our problem. While previous works provide valuable insights on applying DL techniques, e.g., distributed DL, deep reinforcement learning (DRL), and multi-agent DL, in EC systems, these techniques cannot solely handle the distributed and heterogeneous nature of EC systems. To address these limitations, we propose a novel framework based on multi-agent DRL, distributed neural network orchestration (N 2 O), and knowledge distilling. The multi-agent DRL enables edge servers to learn deep neural networks that shelve distinct features learned from local edge sites and hence caters to the heterogeneity of EC systems. N 2 O coordinates edge servers in a fully distributed manner toward a common goal of maximizing ASP’s reward. It requires only local communications during execution and provides provable performance guarantees. The knowledge distilling is further utilized to distill the N 2 O policy for reducing the communication overhead and stabilizing the decision-making. We also carry out systematic experiments to show the advantages of our method over state-of-the-art alternatives.}
}


@article{DBLP:journals/jsac/AbbaslooYC21,
	author = {Soheil Abbasloo and
                  Chen{-}Yu Yen and
                  H. Jonathan Chao},
	title = {Wanna Make Your {TCP} Scheme Great for Cellular Networks? Let Machines
                  Do It for You!},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {265--279},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036958},
	doi = {10.1109/JSAC.2020.3036958},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AbbaslooYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Can we instead of designing yet another new TCP algorithm, design a TCP plug-in that can enable machines to automatically boost the performance of the existing/future TCP designs in cellular networks? We answer this question by introducing DeepCC. DeepCC leverages advanced deep reinforcement learning (DRL) techniques to let machines automatically learn how to steer throughput-oriented TCP algorithms toward achieving applications' desired delays in a highly dynamic network such as the cellular network. We used DeepCC plug-in to boost the performance of various old and new TCP schemes including TCP Cubic, Google's BBR, TCP Westwood, and TCP Illinois in cellular networks. Through both extensive trace-based evaluations and real-world experiments, we show that not only DeepCC can significantly improve the performance of TCP schemes, but also after accompanied by DeepCC, these schemes can outperform state-of-the-art TCP protocols including new clean-slate machine learning-based designs and the ones designed solely for cellular networks.}
}


@article{DBLP:journals/jsac/HagerP21,
	author = {Christian H{\"{a}}ger and
                  Henry D. Pfister},
	title = {Physics-Based Deep Learning for Fiber-Optic Communication Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {1},
	pages = {280--294},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3036950},
	doi = {10.1109/JSAC.2020.3036950},
	timestamp = {Sat, 09 Jan 2021 14:14:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HagerP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a new machine-learning approach for fiber-optic communication systems whose signal propagation is governed by the nonlinear Schrödinger equation (NLSE). Our main observation is that the popular split-step method (SSM) for numerically solving the NLSE has essentially the same functional form as a deep multi-layer neural network; in both cases, one alternates linear steps and pointwise nonlinearities. We exploit this connection by parameterizing the SSM and viewing the linear steps as general linear functions, similar to the weight matrices in a neural network. The resulting physics-based machine-learning model has several advantages over “black-box” function approximators. For example, it allows us to examine and interpret the learned solutions in order to understand why they perform well. As an application, low-complexity nonlinear equalization is considered, where the task is to efficiently invert the NLSE. This is commonly referred to as digital backpropagation (DBP). Rather than employing neural networks, the proposed algorithm, dubbed learned DBP (LDBP), uses the physics-based model with trainable filters in each step and its complexity is reduced by progressively pruning filter taps during gradient descent. Our main finding is that the filters can be pruned to remarkably short lengths-as few as 3 taps/step-without sacrificing performance. As a result, the complexity can be reduced by orders of magnitude in comparison to prior work. By inspecting the filter responses, an additional theoretical justification for the learned parameter configurations is provided. Our work illustrates that combining data-driven optimization with existing domain knowledge can generate new insights into old communications problems.}
}


@article{DBLP:journals/jsac/RodriguesWFPC21,
	author = {Joel J. P. C. Rodrigues and
                  Honggang Wang and
                  Simon James Fong and
                  Nada Y. Philip and
                  Jia Chen},
	title = {Guest Editorial: Internet of Things for In-Home Health Monitoring},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {295--299},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3042420},
	doi = {10.1109/JSAC.2020.3042420},
	timestamp = {Sun, 25 Jul 2021 11:42:58 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RodriguesWFPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Under the pressure of the growing millennial population and senior citizens are aging, which is one of the top societal priorities in many countries, the provision of healthcare needs to evolve and improve. A roadmap paved by World Health Organization (WHO) in March 2019 called Global Strategy on Digital Health 2020-2024, specified a grand vision of promoting healthy lives and well-beings for everyone, everywhere, at all ages [1] . WHO urges all nations to work hand in hand in developing and delivering Digital Health initiatives supported by robust government strategies that amalgamate financial, organizational, human and technological resources [2] . In particular, there are some niches areas in the strategies such as the adoption of distributed sensors and assisted living emerging in recent years. To this end, a lot of efforts both from the research community and industrial providers are anticipated to put forth in the coming decade, in implementing the concept of assisted living using hardware devices into meaningful solutions for fulfilling the growing needs of assisted living.}
}


@article{DBLP:journals/jsac/PhilipRWFC21,
	author = {Nada Y. Philip and
                  Joel J. P. C. Rodrigues and
                  Honggang Wang and
                  Simon James Fong and
                  Jia Chen},
	title = {Internet of Things for In-Home Health Monitoring Systems: Current
                  Advances, Challenges and Future Directions},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {300--310},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3042421},
	doi = {10.1109/JSAC.2020.3042421},
	timestamp = {Sun, 25 Jul 2021 11:42:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PhilipRWFC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things has been one of the catalysts in revolutionizing conventional healthcare services. With the growing society, traditional healthcare systems reach their capacity in providing sufficient and high-quality services. The world is facing the aging population and the inherent need for assisted-living environments for senior citizens. There is also a commitment by national healthcare organizations to increase support for personalized, integrated care to prevent and manage chronic conditions. Many applications related to In-Home Health Monitoring have been introduced over the last few decades, thanks to the advances in mobile and Internet of Things technologies and services. Such advances include improvements in optimized network architecture, indoor networks coverage, increased device reliability and performance, ultra-low device cost, low device power consumption, and improved device and network security and privacy. Current studies of in-home health monitoring systems presented many benefits including improved safety, quality of life and reduction in hospitalization and cost. However, many challenges of such a paradigm shift still exist, that need to be addressed to support scale-up and wide uptake of such systems, including technology acceptance and adoption by patients, healthcare providers and policymakers. The aim of this paper is three folds: First, review of key factors that drove the adoption and growth of the IoT-based in-home remote monitoring; Second, present the latest advances of IoT based in-home remote monitoring system architecture and key building blocks; Third, discuss future outlook and our recommendations of the in-home remote monitoring applications going forward.}
}


@article{DBLP:journals/jsac/CaiFCHCS21,
	author = {Guofa Cai and
                  Yi Fang and
                  Pingping Chen and
                  Guojun Han and
                  Guoen Cai and
                  Yang Song},
	title = {Design of an MISO-SWIPT-Aided Code-Index Modulated Multi-Carrier {M-DCSK}
                  System for e-Health IoT},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {311--324},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020603},
	doi = {10.1109/JSAC.2020.3020603},
	timestamp = {Fri, 29 Apr 2022 16:37:06 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaiFCHCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Code index modulated multi-carrier M-ary differential chaos shift keying (CIM-MC-M-DCSK) system not only inherits low-power and low-complexity advantages of the conventional DCSK system, but also significantly increases the transmission rate. This feature is of particular importance to Internet of Things (IoT) with trillions of low-cost devices. In particular, for e-health IoT applications, an efficient transmission scheme is designed to solve the challenge of the limited battery capacity for numerous user equipments served by one base station. In this paper, a new multiple-input-single-output simultaneous wireless information and power transfer (MISO-SWIPT) scheme for CIM-MC-M-DCSK system is proposed by utilizing orthogonal characteristic of chaotic signals with different initial values. The proposed system adopts power splitting mode, which is very promising for simultaneously providing energy and transmitting information of the user equipments without any external power supply. In particular, the new system can achieve desirable anti-multipath-fading capability without using channel estimator. Moreover, the analytical bit-error-rate expression of the proposed system is derived over multipath Rayleigh fading channels. Furthermore, the spectral efficiency and energy efficiency of the proposed system are analyzed. Simulation results not only validate the analytical expressions, but also demonstrate the superiority of the proposed system.}
}


@article{DBLP:journals/jsac/ZhangYDSZZ21,
	author = {Xuewan Zhang and
                  Liuqing Yang and
                  Zhiguo Ding and
                  Jian Song and
                  Yunkai Zhai and
                  Di Zhang},
	title = {Sparse Vector Coding-Based Multi-Carrier {NOMA} for In-Home Health
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {325--337},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020679},
	doi = {10.1109/JSAC.2020.3020679},
	timestamp = {Mon, 20 Dec 2021 16:58:27 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangYDSZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-home health networks greatly rely on the massive connected monitoring devices. Compared to orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA) can connect more monitoring devices and enhance the spectrum efficiency (SE) performance, which makes it an ideal solution to in-home health networks. However, conventional NOMA (C-NOMA) is mostly constrained to single-carrier scenario. The problem of multi-carrier NOMA lies in the inter-carrier interference (ICI) from neighboring carriers. In this article, we propose a sparse vector coding-based NOMA (SVC-NOMA) to suppress the ICI. We give closed-form expressions of capacity and symbol error rate (SER) performances for both C-NOMA and SVC-NOMA within the considered multi-carrier scenario. Simulation results demonstrate that compared to C-NOMA, SVC-NOMA has better capacity and SER performances. In addition, we find from our results that there is a trade-off between SVC-NOMA’s ICI suppression ability and the system capacity performance.}
}


@article{DBLP:journals/jsac/MisraRRM21,
	author = {Sudip Misra and
                  Arijit Roy and
                  Chandana Roy and
                  Anandarup Mukherjee},
	title = {{DROPS:} Dynamic Radio Protocol Selection for Energy-Constrained Wearable
                  IoT Healthcare},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {338--345},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020678},
	doi = {10.1109/JSAC.2020.3020678},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MisraRRM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose “DROPS”, a scheme which dynamically selects radio protocols in an energy-constrained wearable IoT healthcare system. We consider the use of multiple radio protocols, which are capable of transmitting a patient's sensed physiological parameters to the server through Local Processing Units (LPUs). As the health parameters are non-stationary and temporally fluctuating, especially for critical patients, the selection of an appropriate radio protocol is essential to maintain the accuracy and timely delivery of data from the patient to the server. Additionally, the mobility of patients through various locations within the hospital mandates the selection of the best radio protocol among the multiple available ones for each location, to enable data to offload to the remote server. We use single-leader-multiple-follower Stackelberg non-cooperative game to map the strategic interactions between a patient's LPU and the hospital's server. “DROPS” dynamically selects the appropriate radio protocol, based on the criticality index of a patient, the reputation of the radio, the Euclidean distance between the radios and the LPU, and the load on the protocol. Results on real-life data and their large-scale emulation show that the data rate increases by almost 78% and throughput by approximately 7%, as compared to existing schemes.}
}


@article{DBLP:journals/jsac/DeebakA21,
	author = {Bakkiam David Deebak and
                  Fadi Al{-}Turjman},
	title = {Smart Mutual Authentication Protocol for Cloud Based Medical Healthcare
                  Systems Using Internet of Medical Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {346--360},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020599},
	doi = {10.1109/JSAC.2020.3020599},
	timestamp = {Thu, 25 Apr 2024 15:20:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DeebakA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technological development expands the computation process of smart devices that adopt the telecare medical information system (TMIS) to fulfill the demands of the healthcare organization. It provides better medical identification to claim the features namely trustworthy, efficient, and resourceful. Moreover, the telecare services automate the remote healthcare monitoring process to ease professional workloads. Importantly, it is conceived to be more timesaving, economical, and easy healthcare access. Cloud-Based Medical Healthcare (CBMH) system is a standard platform that gives its support to the patients for emergency treatment from the medical experts over Internet communication. Since the medical records are very sensitive, security protection is much necessitated. In addition, patient anonymity should be well preserved. In 2016, Chiou et al. proposed a mutual authentication protocol for the Telecare Medical Information System (TMIS) using Cloud Environment (CE). They claim that their protocol satisfies patient anonymity. However, this paper proves that the Chiou et al. scheme is not only completely insecure against the patient anonymity, health-report revelation, health-report forgery, report confidentiality, and non-repudiation but also fails to validate the service access against verifiability, undeniability and unforgeability. In order to provide better mutual authenticity, this paper suggests the framework of smart service authentication to cross-examine the common secret session key among the communication entities. In order to examine the security properties, formal and informal verification was carried out. Lastly, to prove the security and performance efficiency of a system, the proposed SSA framework was implemented using FPGA and Moteiv TMote Sky-Mote. A proposed smart service authentication (SSA) framework is presented to ensure better data security between the patients and the physicians. The formal and informal security analysis proves the significance of the SSA framework model to withstand the security attacks such as health-report forgery, health-report revelation, server-spoofing etc. As a result, it is claimed that it can be well suited for TMIS.}
}


@article{DBLP:journals/jsac/AlladiCN21,
	author = {Tejasvi Alladi and
                  Vinay Chamola and
                  Naren Naren},
	title = {{HARCI:} {A} Two-Way Authentication Protocol for Three Entity Healthcare
                  IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {361--369},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020605},
	doi = {10.1109/JSAC.2020.3020605},
	timestamp = {Thu, 25 Apr 2024 15:20:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AlladiCN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the recent use of IoT in the field of healthcare, a lot of patient data is being transmitted and made available online. This necessitates sufficient security measures to be put in place to prevent the possibilities of cyberattacks. In this regard, several authentication techniques have been designed in recent times to mitigate these challenges, but the physical security of the healthcare IoT devices against node tampering and node replacement attacks, in particular, is not addressed sufficiently in the literature. To address these challenges, a two-way two-stage authentication protocol using hardware security primitives called Physical Unclonable Functions (PUFs) is presented in this paper. Considering the memory and energy constraints of healthcare IoT devices, this protocol is made very lightweight. A formal security evaluation of this protocol is done to prove its validity. We also compare it with relevant protocols in the healthcare IoT scenario in terms of computation time and security to show its suitability and robustness.}
}


@article{DBLP:journals/jsac/LuHSDMDG21,
	author = {Di Lu and
                  Ruidong Han and
                  Yulong Shen and
                  Xuewen Dong and
                  Jianfeng Ma and
                  Xiaojiang Du and
                  Mohsen Guizani},
	title = {xTSeH: {A} Trusted Platform Module Sharing Scheme Towards Smart IoT-eHealth
                  Devices},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {370--383},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020658},
	doi = {10.1109/JSAC.2020.3020658},
	timestamp = {Tue, 07 May 2024 20:20:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LuHSDMDG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IoT based eHealth system brings a revolution to healthcare industry, with which the old healthcare systems can be updated into smarter and more personalized ones. The practitioners can continue monitoring the physical status of the patients at anytime and anywhere, and develop more precise treatment plans by analyzing the collected data, such as heart rate, blood pressure, blood glucose. Actually, these smart sensors used in eHealth system are smart embedded devices (SED). Due to the limitations on hardware capabilities, these inter-connected SEDs lack of security considerations in design and implementation, and face the threats from the network. To prevent the malicious users (or programs) from tampering with the SEDs, trusted platform module (TPM) is adopted, which can guarantee the system integrity via detecting unauthorized modifications to data and system environment. However, due to the limited scalability and insufficient system resources, not all SEDs can be deployed with TPM chips. To address this issue, in this paper, a TPM extension scheme (xTSeH) is proposed. In xTSeH, we have extended the functions of a TPM deployed in a SED (TSED) to those non-TPM-protected SEDs (N-TSED) via network. A shadow TPM in the form of a kernel module is designed as the trust base for the N-TSED, which is the representative of the TPM in TSED. Then, three protocols are proposed to implement the integrity verification and inter-SED authentication. Finally, a Raspberry Pi based prototype system is designed and implemented. The feasibility and usability of our scheme are proved by the analysis of the experimental results of system performance.}
}


@article{DBLP:journals/jsac/ZhangHOVH21,
	author = {Yudi Zhang and
                  Debiao He and
                  Mohammad S. Obaidat and
                  Pandi Vijayakumar and
                  Kuei{-}Fang Hsiao},
	title = {Efficient Identity-Based Distributed Decryption Scheme for Electronic
                  Personal Health Record Sharing System},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {384--395},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020656},
	doi = {10.1109/JSAC.2020.3020656},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangHOVH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of the Internet of Things (IoT) has led to the emergence of more and more novel applications in recent years. One of them is the e-health system, which can provide people with high-quality and convenient health care. Meanwhile, it is a key issue and challenge to protect the privacy and security of the user's personal health record. Some cryptographic methods have been proposed such as encrypt user's data before sharing it. However, it is complicated to share the data with multiple parties (doctors, health departments, etc.), due to the fact that data should be encrypted under each recipient's keys. Although several (t, n) threshold secret sharing schemes can share the data only need one encryption operation, there is a limitation that the decryption private key has to be reconstructed by one party. To offset this shortcoming, in this paper, we propose an efficient identity-based distributed decryption scheme for personal health record sharing system. It is convenient to share their data with multiple parties and does not require to reconstruct the decryption private key. We prove that our scheme is secure under chosen-ciphertext attack (CCA). Moreover, we implement our scheme by using the Java pairing-based cryptography (JPBC) library on a laptop and an Android phone. The experimental results show that our system is practical and effective in the electronic personal health record system.}
}


@article{DBLP:journals/jsac/ZhouWYLMOF21,
	author = {Zhenyu Zhou and
                  Zhao Wang and
                  Haijun Yu and
                  Haijun Liao and
                  Shahid Mumtaz and
                  Lu{\'{\i}}s M. L. Oliveira and
                  Valerio Frascolla},
	title = {Learning-Based URLLC-Aware Task Offloading for Internet of Health
                  Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {396--410},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020680},
	doi = {10.1109/JSAC.2020.3020680},
	timestamp = {Fri, 19 Feb 2021 11:42:05 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouWYLMOF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Internet of Health Things (IoHT)-based e-Health paradigm, a large number of computational-intensive tasks have to be offloaded from resource-limited IoHT devices to proximal powerful edge servers to reduce latency and improve energy efficiency. However, the lack of global state information (GSI), the adversarial competition among multiple IoHT devices, and the ultra reliable and low latency communication (URLLC) constraints have imposed new challenges for task offloading optimization. In this article, we formulate the task offloading problem as an adversarial multi-armed bandit (MAB) problem. In addition to the average-based performance metrics, bound violation probability, occurrence probability of extreme events, and statistical properties of excess values are employed to characterize URLLC constraints. Then, we propose a URLLC-aware Task Offloading scheme based on the exponential-weight algorithm for exploration and exploitation (EXP3) named UTO-EXP3. URLLC awareness is achieved by dynamically balancing the URLLC constraint deficits and energy consumption through online learning. We provide a rigorous theoretical analysis to show that guaranteed performance with a bounded deviation can be achieved by UTO-EXP3 based on only local information. Finally, the effectiveness and reliability of UTO-EXP3 are validated through simulation results.}
}


@article{DBLP:journals/jsac/ZhangCLPF21,
	author = {Long Zhang and
                  Bin Cao and
                  Yun Li and
                  Mugen Peng and
                  Gang Feng},
	title = {A Multi-Stage Stochastic Programming-Based Offloading Policy for Fog
                  Enabled IoT-eHealth},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {411--425},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020659},
	doi = {10.1109/JSAC.2020.3020659},
	timestamp = {Tue, 21 Mar 2023 18:15:25 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangCLPF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet low latency and real-time monitoring demands of IoT-eHealth, fog computing is envisioned as a key technology to offer elastic computing resource at the edge of networks. In this context, eHealth devices can offload collected healthcare data or computational expensive tasks to a nearby fog server. However, the mobility of the eHealth devices may make the connection between them to fog servers uncertain, resulting in possible migration between fog servers. In order to evaluate the impact of this uncertainty on decision-making for offloading and resource allocation, we formulate the task offloading problem as a Multi-Stage Stochastic Programming (MSSP), with aim of minimizing the total latency of offloading to determine whether to offload or not, how much workload to offload, how much computing resource to allocate, as well as whether to migrate or not. Different from the previous MSSP based work focusing on the workload assignment only, the proposed MSSP examines joint decisions of offloading, resource allocation, and migration, advancing the understanding of the interactions among these decisions. Furthermore, to reduce the computational complexity of MSSP, we design an efficient sub-optimal offloading policy based on Sample Average Approximation, called SAA-MSSP. We conduct extensive simulation experiments to validate the effectiveness of SAA-MSSP. The results show that SAA-MSSP can converge to a near-optimal solution quickly.}
}


@article{DBLP:journals/jsac/ZhouWWC21,
	author = {Liang Zhou and
                  Dan Wu and
                  Xin Wei and
                  Jianxin Chen},
	title = {Cross-Modal Stream Scheduling for eHealth},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {426--437},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021543},
	doi = {10.1109/JSAC.2020.3021543},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouWWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cross-modal applications that elaborately integrate audio, video, and haptic streams will become the mainstream of the eHealth systems. However, existing stream schedulers usually fail to simultaneously meet the cross-modal transmission requests in terms of low latency, high reliability, high throughput, and low complexity. To circumvent this dilemma, this article proposes a general cross-modal stream scheduling scheme by fully taking advantage of the characteristics of different modal streams and their underlying temporal, spatial, and semantic relevance. Specifically, we first propose a hierarchical stream category framework, in which the transmission priority of the modal stream instead of the data flow can be flexibly settled. Next, we design a series of modal-aware stream scheduling schemes by jointly making use of the network slice and mobile edge computing to achieve the tradeoff among the various metrics. Importantly, the transmission strategy can be adjusted adaptively to realize the optimal resource allocation. Subsequently, we analyze the relationship among the user experience, multi-modal impact, and stream scheduling through investigating the interacted impacts among the different modal streams, then develop a user experience based scheduling switch strategy to improve the application generality and reduce the performance fluctuation. Numerical objective and subjective results demonstrate the efficiency of the proposed cross-modal scheduling scheme.}
}


@article{DBLP:journals/jsac/Sanabria-RussoS21,
	author = {Luis Sanabria{-}Russo and
                  Jordi Serra and
                  David Pubill and
                  Christos V. Verikoukis},
	title = {{CURATE:} On-Demand Orchestration of Services for Health Emergencies
                  Prediction and Mitigation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {438--445},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021570},
	doi = {10.1109/JSAC.2020.3021570},
	timestamp = {Fri, 19 Feb 2021 11:42:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/Sanabria-RussoS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Telemedicine, or the ability granted to doctors to remotely assist patients has been greatly benefited by advances in IoT, network communications, Machine Learning and Edge/Cloud computing. With the impeding arrival of 5G, virtualized infrastructures and cloud-native approaches enable the execution of unprecedented procedures during such patient/doctor interactions, allowing medical professionals to e.g. request higher granularity metrics from patients' telemetry equipment, or perform on-demand data mining/processing of patient's stored data in order to provide a more educated diagnostic or prediction. In this work we coalesce the virtues of virtualized infrastructures and IoT into a solution able to satisfy increasing data processing demands for eHealth, e.g. for telemedicine applications, remote assistance or patient pre-screening procedures. The proposed platform, CURATE, leverages Network Functions Virtualisation Management and Orchestration (NFV MANO) for the on-demand instantiation of the required virtual resources on the operator's infrastructure, as well as the concept of 5G Network Slices to guarantee efficient resource allocation and tenant isolation. Results show the proposed platform is able to efficiently make use of the available hardware resources via Network Slices, as well as provide cost-effective service guarantees employing dynamic scaling operations.}
}


@article{DBLP:journals/jsac/AloiFGPS21,
	author = {Gianluca Aloi and
                  Giancarlo Fortino and
                  Raffaele Gravina and
                  Pasquale Pace and
                  Claudio Savaglio},
	title = {Simulation-Driven Platform for Edge-Based {AAL} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {446--462},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021544},
	doi = {10.1109/JSAC.2020.3021544},
	timestamp = {Mon, 03 Jan 2022 22:12:06 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AloiFGPS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-growing aging of the population has emphasized the importance of in-home AAL (Ambient Assisted Living) services for monitoring and improving its well-being and health, especially in the context of care facilities (retirement villages, clinics, senior neighborhood, etc). The paper proposes a novel simulation-driven platform named E-ALPHA (Edge-based Assisted Living Platform for Home cAre) which supports both Edge and Cloud Computing paradigm to develop innovative AAL services in scenarios of different scales. E-ALPHA flexibly combines Edge, Cloud or Edge/Cloud deployments, supports different communication protocols, and fosters the interoperability with other IoT platforms. Moreover, the simulation-based design helps in preliminary assessing (i) the expected performance of the service to be deployed according to the infrastructural characteristics of each specific small, medium and large scenario; and (ii) the most appropriate applications/platform configuration for a real deployment (kind and number of involved devices, Edge- or Cloud-based deployment, required connectivity type, etc). In this direction, two different use cases modeled according to realistic input (coming from past experience involving real testbed) are shown in order to demonstrate the potentials of the proposed simulation-driven AAL platform.}
}


@article{DBLP:journals/jsac/NingDWHGHGQK21,
	author = {Zhaolong Ning and
                  Peiran Dong and
                  Xiaojie Wang and
                  Xiping Hu and
                  Lei Guo and
                  Bin Hu and
                  Yi Guo and
                  Tie Qiu and
                  Ricky Yu{-}Kwong Kwok},
	title = {Mobile Edge Computing Enabled 5G Health Monitoring for Internet of
                  Medical Things: {A} Decentralized Game Theoretic Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {463--478},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020645},
	doi = {10.1109/JSAC.2020.3020645},
	timestamp = {Wed, 07 Dec 2022 23:03:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/NingDWHGHGQK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The prompt evolution of Internet of Medical Things (IoMT) promotes pervasive in-home health monitoring networks. However, excessive requirements of patients result in insufficient spectrum resources and communication overload. Mobile Edge Computing (MEC) enabled 5G health monitoring is conceived as a favorable paradigm to tackle such an obstacle. In this paper, we construct a cost-efficient in-home health monitoring system for IoMT by dividing it into two sub-networks, i.e., intra-Wireless Body Area Networks (WBANs) and beyond-WBANs. Highlighting the characteristics of IoMT, the cost of patients depends on medical criticality, Age of Information (AoI) and energy consumption. For intra-WBANs, a cooperative game is formulated to allocate the wireless channel resources. While for beyond-WBANs, considering the individual rationality and potential selfishness, a decentralized non-cooperative game is proposed to minimize the system-wide cost in IoMT. We prove that the proposed algorithm can reach a Nash equilibrium. In addition, the upper bound of the algorithm time complexity and the number of patients benefiting from MEC is theoretically derived. Performance evaluations demonstrate the effectiveness of our proposed algorithm with respect to the system-wide cost and the number of patients benefiting from MEC.}
}


@article{DBLP:journals/jsac/WuHYW21,
	author = {Dapeng Wu and
                  Xiaojuan Han and
                  Zhigang Yang and
                  Ruyan Wang},
	title = {Exploiting Transfer Learning for Emotion Recognition Under Cloud-Edge-Client
                  Collaborations},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {479--490},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020677},
	doi = {10.1109/JSAC.2020.3020677},
	timestamp = {Wed, 05 Jun 2024 17:31:54 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuHYW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging virtual reality/augmented reality games and self-driving cars necessitate accurate/responsive/private emotion recognition. Usually, traditional emotion recognition models are deployed at central servers, which results in the lack of abilities in generalization and covering the individual variation of clients. This paper proposes a responsive, localized, and private transfer learning based emotion recognition framework under the cloud-edge-client collaborations. Additionally, a 3-dimensional channel mapping method is designed to aggregate features extracted from electroencephalogram (EEG) signals for the generic emotion recognition model, which is further localized and personalized using transfer learning. Simulation results validate the performance of the proposed TLER framework in reducing model training time and improving emotion recognition accuracy.}
}


@article{DBLP:journals/jsac/AujlaJ21,
	author = {Gagangeet Singh Aujla and
                  Anish Jindal},
	title = {A Decoupled Blockchain Approach for Edge-Envisioned IoT-Based Healthcare
                  Monitoring},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {491--499},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020655},
	doi = {10.1109/JSAC.2020.3020655},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AujlaJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The in-house health monitoring sensors form a large network of Internet of things (IoT) that continuously monitors and sends the data to the nearby devices or server. However, the connectivity of these IoT-based sensors with different entities leads to security loopholes wherein the adversary can exploit the vulnerabilities due to the openness of the data. This is a major concern especially in the healthcare sector where the change in data values from sensors can change the course of diagnosis which can cause severe health issues. Therefore, in order to prevent the data tempering and preserve the privacy of patients, we present a decoupled blockchain-based approach in the edge-envisioned ecosystem. This approach leverages the nearby edge devices to create the decoupled blocks in blockchain so as to securely transmit the healthcare data from sensors to the edge nodes. The edge nodes then transmit and store the data at the cloud using the incremental tensor-based scheme. This helps to reduce the data duplication of the huge amount of data transmitted in the large IoT healthcare network. The results show the effectiveness of the proposed approach in terms of the block preparation time, header generation time, tensor reduction ratio, and approximation error.}
}


@article{DBLP:journals/jsac/YangWM21,
	author = {Chao Yang and
                  Xuyu Wang and
                  Shiwen Mao},
	title = {Respiration Monitoring With {RFID} in Driving Environments},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {500--512},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020606},
	doi = {10.1109/JSAC.2020.3020606},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YangWM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve driving safety and avoid accidents caused by driving fatigue, drowsiness detection aims to alarm the driver before he/she falls asleep. Since breathing rate is a key indicator of the drowsy state, respiration monitoring in the noisy driving environment is critical for developing an effective driving fatigue detection system. In this paper, we propose, for the first time, an RFID based respiration monitoring system for driving environments. The system estimates the respiration rate of a driver based on phase values sampled from multiple RFID tags attached to the seat belt, while exploiting the tag diversity to combat the strong noise in the driving environment. Both tensor completion and tensor Canonical Polyadic Decomposition (CPD) are applied to process the phase values, to overcome the influence of frequency hopping, random sampling, vehicle vibration, and other environmental movements. The proposed system is analyzed and implemented with commodity RFID devices. Its accurate and robust performance is demonstrated with extensive experiments conducted in a real driving car.}
}


@article{DBLP:journals/jsac/ZhangLLDGW21,
	author = {Shigeng Zhang and
                  Xuan Liu and
                  Yangyang Liu and
                  Bo Ding and
                  Song Guo and
                  Jianxin Wang},
	title = {Accurate Respiration Monitoring for Mobile Users With Commercial {RFID}
                  Devices},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {513--525},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020604},
	doi = {10.1109/JSAC.2020.3020604},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangLLDGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vital signs (e.g., respiration rate or heartbeat rate) sensing is of great importance to implement pervasive in-home healthcare. Traditional vital signs monitoring approaches usually require users to wear some dedicated sensors. These approaches are intrusive and inconvenient to use, especially for elderly people. Some non-intrusive vital signs monitoring approaches based on wireless sensing have been proposed in recent years. However, these approaches require the target user to be in situ during the monitoring process, which greatly limits their utilization in practical scenarios where the target users usually move around. In this paper, we propose RF-RMM, an RFID-based approach to accurate and continuous respiration monitoring for mobile users. The major challenge in respiration monitoring for moving people is that the tiny body displacement caused by the user’s respiration is overwhelmed by the user’s entire body movement. To address this issue, we propose a novel approach that uses a pair of tags to eliminate the effect of the user’s body movement. We fuse the data from the paired tags to cancel the effect of the user’s entire body movement and retain only the displacement caused by the user’s respiration. Another challenging issue in implementing RF-RMM is how to resolve the phase ambiguity problem when the target user moves around, which becomes more serious than in the static case. We propose a distance tracking algorithm to track the phase transition during the user’s movement, according to which the phase ambiguity problem can be well handled. We implement RF-RMM on commercial RFID devices and conduct extensive real-world experiments to evaluate its performance. The results show that RF-RMM achieves accurate respiration rate monitoring with an average error of 0.54 BPM in estimating different users’ respiration rate and an average relative error of less than 13% in estimating the user’s individual breath length.}
}


@article{DBLP:journals/jsac/SalemAMB21,
	author = {Osman Salem and
                  Khalid Alsubhi and
                  Ahmed Mehaoua and
                  Raouf Boutaba},
	title = {Markov Models for Anomaly Detection in Wireless Body Area Networks
                  for Secure Health Monitoring},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {526--540},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020602},
	doi = {10.1109/JSAC.2020.3020602},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SalemAMB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of Wireless Body Area Networks (WBANs) in healthcare for pervasive monitoring enhances the lives of patients and allows them to fulfill their daily life activities while being monitored. Various non-invasive sensors are placed on the skin to monitor several physiological attributes, and the measured data are transmitted wirelessly to a centralized processing unit to detect changes in the health of the monitored patient. However, the transferred data are vulnerable to various sources of interference, sensor faults, measurement faults, injection and alteration by malicious attackers, etc. In this article, we propose a change point detection model based on a Markov chain for centralized anomaly detection in WBANs. The model is derived from the Root Mean Square Error (RMSE) between the forecasted and measured values for whole attributes. The RMSE transforms the monitored attributes into a univariate times series which is divided into overlapping sliding window. The joint probability of the sequence of RMSE values in each sliding window is calculated to decide whether a change has occurred or not. When an effective change is detected over k consecutive windows, the number of deviated attributes is used to distinguish faulty measurements from a health emergency. We apply our proposed approach on real physiological data from the Physionet database and compare it with existing approaches. Our experimental results prove the effectiveness of our proposed approach, as it achieves high detection accuracy with a low false alarm rate (5.2%).}
}


@article{DBLP:journals/jsac/JuniorSNFMA21,
	author = {Carlos M. J. M. Dourado J{\'{u}}nior and
                  Suane Pires P. da Silva and
                  Raul Victor Medeiros da N{\'{o}}brega and
                  Pedro Pedrosa Rebou{\c{c}}as Filho and
                  Khan Muhammad and
                  Victor Hugo C. de Albuquerque},
	title = {An Open IoHT-Based Deep Learning Framework for Online Medical Image
                  Recognition},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {541--548},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020598},
	doi = {10.1109/JSAC.2020.3020598},
	timestamp = {Mon, 24 Jul 2023 12:33:29 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JuniorSNFMA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Systems developed to work with computational intelligence have become very efficient, and in some cases obtain more accurate results than evaluations by humans. Hence, this work proposes a new online approach based on deep learning tools according to the concept of transfer learning to generate a computational intelligence framework for use with the Internet of Health Things (IoHT) devices. This framework allows the user to add their images and perform platform training almost as easily as creating folders and placing files in regular cloud storage services. The trials carried out with the tool showed that even people with no programming and image processing knowledge were able to set up projects in a few minutes. The proposed approach is validated using three medical databases, which include cerebral vascular accident images for stroke type classification, lung nodule images for malignant classification, and skin images for the classification of melanocytic lesions. The results show the efficiency and reliability of the framework, which reached 91.6% Accuracy in the stroke images and lung nodules databases, and 92% Accuracy in the skin images databases. This prove the immense contribution that this work can bring to assist medical professionals in analyzing complex examinations quickly and accurately, allowing a large medical examination database through a consolidated collaborative IoT platform.}
}


@article{DBLP:journals/jsac/YuanHG21,
	author = {Weiwei Yuan and
                  Guangjie Han and
                  Donghai Guan},
	title = {Learning From Mislabeled Training Data Through Ambiguous Learning
                  for In-Home Health Monitoring},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {549--561},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021572},
	doi = {10.1109/JSAC.2020.3021572},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YuanHG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data are widely collected via the IoT for machine learning tasks in in-home health monitoring applications and mislabeled training data lead to unreliable machine learning models in in-home health monitoring. Researchers have proposed a wide arrangement of algorithms to deal with mislabeled training data, in which one straightforward and effective solution is to directly filter noise from training data so that the negative effects of mislabeled data can be minimized. In essence, noise filtering might be a suboptimal solution because the mislabeled data are not completely useless. The features and distributions of mislabeled data are still useful for learning, especially when training data are insufficient. In this work, we propose a novel framework to learn from mislabeled training data through ambiguous learning (LeMAL). LeMAL mainly consists of two parts. First, it converts the original training data to ambiguous data. Second, an ambiguous learning algorithm is applied to the ambiguous data. In this work, we propose a novel distance-based ambiguous learning algorithm so that the ambiguous data can be used in a better way. Finally, we demonstrate that LeMAL can effectively improve learning performance over existing noise filtering methods.}
}


@article{DBLP:journals/jsac/PostolacheHAGGK21,
	author = {Octavian Postolache and
                  D. Jude Hemanth and
                  Ricardo Alexandre and
                  Deepak Gupta and
                  Oana Geman and
                  Ashish Khanna},
	title = {Remote Monitoring of Physical Rehabilitation of Stroke Patients Using
                  IoT and Virtual Reality},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {562--573},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020600},
	doi = {10.1109/JSAC.2020.3020600},
	timestamp = {Wed, 07 Dec 2022 23:03:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/PostolacheHAGGK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The statistics highlights that physical rehabilitation are required nowadays by increased number of people that are affected by motor impairments caused by accidents or aging. Among the most common causes of disability in adults are strokes or cerebral palsy. To reduce the costs preserving the quality of services new solutions based on current technologies in the area of physiotherapy are emerging. The remote monitoring of physical training sessions could facilitate for physicians and physical therapists' information about training outcome that may be useful to personalize the exercises helping the patients to achieve better rehabilitation results in short period of time process. This research work aims to apply physical rehabilitation monitoring combining Virtual Reality serious games and Wearable Sensor Network to improve the patient engagement during physical rehabilitation and evaluate their evolution. Serious games based on different scenarios of Virtual Reality, allows a patient with motor difficulties to perform exercises in a highly interactive and non-intrusive way, using a set of wearable devices, contributing to their motivational process of rehabilitation. The system implementation, system validation and experimental results are included in the paper.}
}


@article{DBLP:journals/jsac/TongLSYHBZ21,
	author = {Chao Tong and
                  Baoyu Liang and
                  Qiang Su and
                  Mengbo Yu and
                  Jiexuan Hu and
                  Ali Kashif Bashir and
                  Zhigao Zheng},
	title = {Pulmonary Nodule Classification Based on Heterogeneous Features Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {574--581},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020657},
	doi = {10.1109/JSAC.2020.3020657},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TongLSYHBZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Pulmonary cancer is one of the most dangerous cancers with a high incidence and mortality. An early accurate diagnosis and treatment of pulmonary cancer can observably increase the survival rates, where computer-aided diagnosis systems can largely improve the efficiency of radiologists. In this article, we propose a deep automated lung nodule diagnosis system based on three-dimensional convolutional neural network (3D-CNN) and support vector machine (SVM) with multiple kernel learning (MKL) algorithms. The system not only explores the computed tomography (CT) scans, but also the clinical information of patients like age, smoking history and cancer history. To extract deeper image features, a 34-layers 3D Residual Network (3D-ResNet) is employed. Heterogeneous features including the extracted image features and the clinical data are learned with MKL. The experimental results prove the effectiveness of the proposed image feature extractor and the combination of heterogeneous features in the task of lung nodule diagnosis.}
}


@article{DBLP:journals/jsac/SciarroneBGLSK21,
	author = {Andrea Sciarrone and
                  Igor Bisio and
                  Chiara Garibotto and
                  Fabio Lavagetto and
                  Gerhard H. Staude and
                  Andreas Knopp},
	title = {Leveraging IoT Wearable Technology Towards Early Diagnosis of Neurological
                  Diseases},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {582--592},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021573},
	doi = {10.1109/JSAC.2020.3021573},
	timestamp = {Tue, 01 Jun 2021 15:21:09 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SciarroneBGLSK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The leading trends in the framework of the Internet of Things are driving the research community to provide smart systems and solutions aimed at revolutionizing medical sciences and healthcare. One of the major opportunities offered by IoT lies in the ubiquitous connectivity, thus enabling smart services such as remote patient monitoring, in-home therapy/rehabilitation, and assisted living platforms. In this paper we present a prototype of wearable smart glasses able to monitor the Eye Blinks (EBs) through ElectroOculoGram (EOG) signal in a transparent way with respect to the final user. We propose a novel pre-filtering scheme to reduce EOG noise along with an analytical derivation of a matched filter to detect and count EBs. We have carried out an in-depth experimental campaign in order to validate the robustness of our approach with respect to the main solutions available in the literature. Furthermore, we have compared the performances obtained with out wearable prototype versus the results achievable with professional medical equipments. Results show that our solution is able to achieve very high accuracy in EB detection, obtaining comparable performance with respect to professional medical desktop equipment, with the additional benefit of portability, comfort and easiness of use for the patients.}
}


@article{DBLP:journals/jsac/RazaASIH21,
	author = {Mohsin Raza and
                  Muhammad Awais and
                  Nishant Singh and
                  Muhammad Imran and
                  Sajjad Hussain},
	title = {Intelligent IoT Framework for Indoor Healthcare Monitoring of Parkinson's
                  Disease Patient},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {593--602},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3021571},
	doi = {10.1109/JSAC.2020.3021571},
	timestamp = {Tue, 11 Jan 2022 16:58:16 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/RazaASIH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Parkinson's disease is associated with high treatment costs, primarily attributed to the needs of hospitalization and frequent care services. A study reveals annual per-person healthcare costs for Parkinson's patients to be\n21,482,withanadditional\n29,695 burden to society. Due to the high stakes and rapidly rising Parkinson's patients' count, it is imperative to introduce intelligent monitoring and analysis systems. In this paper, an Internet of Things (IoT) based framework is proposed to enable remote monitoring, administration, and analysis of patient's conditions in a typical indoor environment. The proposed infrastructure offers both static and dynamic routing, along with delay analysis and priority enabled communications. The scheme also introduces machine learning techniques to detect the progression of Parkinson's over six months using auditory inputs. The proposed IoT infrastructure and machine learning algorithm are thoroughly evaluated and a detailed analysis is performed. The results show that the proposed scheme offers efficient communication scheduling, facilitating a high number of users with low latency. The proposed machine learning scheme also outperforms state-of-the-art techniques in accurately predicting the Parkinson's progression.}
}


@article{DBLP:journals/jsac/MuhammadHK21,
	author = {Ghulam Muhammad and
                  M. Shamim Hossain and
                  Neeraj Kumar},
	title = {EEG-Based Pathology Detection for Home Health Monitoring},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {2},
	pages = {603--610},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020654},
	doi = {10.1109/JSAC.2020.3020654},
	timestamp = {Tue, 26 Jan 2021 08:45:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MuhammadHK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An electroencephalogram (EEG)-based remote pathology detection system is proposed in this study. The system uses a deep convolutional network consisting of 1D and 2D convolutions. Features from different convolutional layers are fused using a fusion network. Various types of networks are investigated; the types include a multilayer perceptron (MLP) with a varying number of hidden layers, and an autoencoder. Experiments are done using a publicly available EEG signal database that contains two classes: normal and abnormal. The experimental results demonstrate that the proposed system achieves greater than 89% accuracy using the convolutional network followed by the MLP with two hidden layers. The proposed system is also evaluated in a cloud-based framework, and its performance is found to be comparable with the performance obtained using only a local server.}
}


@article{DBLP:journals/jsac/ChenNYLAS21,
	author = {Xiaoming Chen and
                  Derrick Wing Kwan Ng and
                  Wei Yu and
                  Erik G. Larsson and
                  Naofal Al{-}Dhahir and
                  Robert Schober},
	title = {Guest Editorial Massive Access for 5G and Beyond - Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {611--614},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018975},
	doi = {10.1109/JSAC.2020.3018975},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenNYLAS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive access, also known as massive connectivity or massive machine-type communication (mMTC), is one of the main use cases of the fifth-generation (5G) and beyond 5G (B5G) wireless networks. In the past few years, it has received considerable attention in academia and industry. This Special Issue (SI) of the IEEE Journal on Selected Areas in Communications (JSAC) on Massive Access for 5G and Beyond contains the latest results of researchers, industry practitioners, and individuals working on related research problems. Due to the extremely high response to the Call for Papers, this SI is split into two parts. The first part includes a guest editor-authored survey paper and 17 technical papers focusing on access models and access protocols, while the second part contains 18 papers focusing on access techniques and coverage enhancement approaches. We sincerely thank the authors, reviewers, JSAC staffs, and the Senior Editor, Prof. Wayne Stark, for their effort and time in preparing this SI.}
}


@article{DBLP:journals/jsac/ChenNYLAS21a,
	author = {Xiaoming Chen and
                  Derrick Wing Kwan Ng and
                  Wei Yu and
                  Erik G. Larsson and
                  Naofal Al{-}Dhahir and
                  Robert Schober},
	title = {Massive Access for 5G and Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {615--637},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3019724},
	doi = {10.1109/JSAC.2020.3019724},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenNYLAS21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive access, also known as massive connectivity or massive machine-type communication (mMTC), is one of the main use cases of the fifth-generation (5G) and beyond 5G (B5G) wireless networks. A typical application of massive access is the cellular Internet of Things (IoT). Different from conventional human-type communication, massive access aims at realizing efficient and reliable communications for a massive number of IoT devices. Hence, the main characteristics of massive access include low power, massive connectivity, and broad coverage, which require new concepts, theories, and paradigms for the design of next-generation cellular networks. This paper presents a comprehensive survey of massive access design for B5G wireless networks. Specifically, we provide a detailed review of massive access from the perspectives of theory, protocols, techniques, coverage, energy, and security. Furthermore, several future research directions and challenges are identified.}
}


@article{DBLP:journals/jsac/ZhongMGZ21,
	author = {Yi Zhong and
                  Guoqiang Mao and
                  Xiaohu Ge and
                  Fu{-}Chun Zheng},
	title = {Spatio-temporal Modeling for Massive and Sporadic Access},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {638--651},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018776},
	doi = {10.1109/JSAC.2020.3018776},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhongMGZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The vision for smart city imperiously appeals to the implementation of Internet-of-Things (IoT), some features of which, such as massive access and bursty short packet transmissions, require new methods to enable the cellular system to seamlessly support its integration. Rigorous theoretical analysis is indispensable to obtain constructive insight for the networking design of massive access. In this paper, we propose and define the notion of massive and sporadic access (MSA) to quantitatively describe the massive access of IoT devices. We evaluate the temporal correlation of interference and successful transmission events, and verify that such correlation is negligible in the scenario of MSA. In view of this, in order to resolve the difficulty in any precise spatio-temporal analysis where complex interactions persist among the queues, we propose an approximation that all nodes are moving so fast that their locations are independent at different time slots. Furthermore, we compare the original static network and the equivalent network with high mobility to demonstrate the effectiveness of the proposed approximation approach. The proposed approach is promising for providing a convenient and general solution to evaluate and design the IoT network with massive and sporadic access.}
}


@article{DBLP:journals/jsac/ZhangWP21,
	author = {Xi Zhang and
                  Jingqing Wang and
                  H. Vincent Poor},
	title = {Statistical Delay and Error-Rate Bounded QoS Provisioning for mURLLC
                  Over 6G {CF} {M-MIMO} Mobile Networks in the Finite Blocklength Regime},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {652--667},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020109},
	doi = {10.1109/JSAC.2020.3020109},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangWP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In supporting the new 6G standard traffic services-massive ultra-reliable low-latency communications (mURLLC), several advanced techniques, including statistical delay-bounded quality-of-service (QoS) provisioning theory and finite blocklength coding (FBC), have been developed to upper-bound both delay and error-rate for time-sensitive multimedia applications. On the other hand, cell-free (CF) massive multi-input multioutput (m-MIMO), where a large number of distributed access points (APs) jointly serve a massive number of mobile devices using the same time-frequency resources, has emerged as one of the 6G key promising techniques to significantly improve various QoS performances for supporting mURLLC. However, it is challenging to statistically guarantee stringent mURLLC QoS-requirements for transmitting multimedia traffics over CF m-MIMO and FBC based 6G wireless networks. To overcome these problems, we develop analytical models to precisely characterize the delay and error-rate bounded QoS performances while considering non-vanishing decode-error probability for CF m-MIMO based schemes. In particular, we develop FBC based system models and apply the Mellin transform to characterize arrival/service processes for our proposed CF m-MIMO modeling schemes. Then, we formulate and solve the delay violation probability minimization problem and obtain the closed-form solution of the optimal rate adaptation policy for each mobile user over 6G CF m-MIMO mobile wireless networks in the finite blocklength regime. Our simulation results validate and evaluate our proposed schemes for statistical delay and error-rate bounded QoS provisioning.}
}


@article{DBLP:journals/jsac/JiangDNY21,
	author = {Nan Jiang and
                  Yansha Deng and
                  Arumugam Nallanathan and
                  Jinhong Yuan},
	title = {A Decoupled Learning Strategy for Massive Access Optimization in Cellular
                  IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {668--685},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018806},
	doi = {10.1109/JSAC.2020.3018806},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JiangDNY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular-based networks are expected to offer connectivity for massive Internet of Things (mIoT) systems. However, their Random Access CHannel (RACH) procedure suffers from unreliability, due to the collision from the simultaneous massive access. Despite that this collision problem has been treated in existing RACH schemes, these schemes usually organize IoT devices' transmission and re-transmission along with fixed parameters, thus can hardly adapt to time-varying traffic patterns. Without adaptation, the RACH procedure easily suffers from high access delay, high energy consumption, or even access unavailability. With the goal of improving the RACH procedure, this paper targets to optimize the RACH procedure in real-time by maximizing a long-term hybrid multi-objective function, which consists of the number of access success devices, the average energy consumption, and the average access delay. To do so, we first optimize the long-term objective in the number of access success devices by using Deep Reinforcement Learning (DRL) algorithms for different RACH schemes, including Access Class Barring (ACB), Back-Off (BO), and Distributed Queuing (DQ). The converging capability and efficiency of different DRL algorithms including Policy Gradient (PG), Actor-Critic (AC), Deep Q-Network (DQN), and Deep Deterministic Policy Gradient (DDPG) are compared. Inspired by the results from this comparison, a decoupled learning strategy is developed to jointly and dynamically adapt the access control factors of those three access schemes. This decoupled strategy integrates predicted traffic into the learning process to improve training efficiency, where a Recurrent Neural Network (RNN) model is first employed to predict the real-time traffic values of the network environment, and then multiple DRL agents are employed to cooperatively configure parameters of each RACH scheme. Our results demonstrate that the decoupled strategy remarkably accelerate the training speedy.}
}


@article{DBLP:journals/jsac/BaiLYCZ21,
	author = {Lin Bai and
                  Jiexun Liu and
                  Quan Yu and
                  Jinho Choi and
                  Wei Zhang},
	title = {A Collision Resolution Protocol for Random Access in Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {686--699},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018798},
	doi = {10.1109/JSAC.2020.3018798},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/BaiLYCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 5G and beyond wireless scenarios, it is expected to support tremendous amount of communicating machines. With an exponential increase of machine-to-machine (M2M) system deployments, efficient approaches to massive access by a large number of devices are to be studied. In this paper, we propose a massive multiple-input multiple-output (MIMO) based grant-free random access (RA) with resolution of preamble collision for massive access. Based on the channel hardening and favorable propagation characteristics of massive MIMO, collided signals processed at the base station (BS) can be viewed as a variation of superposition modulation, and are to be recovered by successive interference cancellation (SIC) techniques. Besides, taking into consideration the effect of pass loss and fractional power control (FPC), analytic expressions of success probability of the proposed collision resolution with conjugate beamforming (CB) and zero-forcing beamforming (ZFB) are derived. With simulation results, we verify the analyses and show that the proposed protocol can resolve most preamble collisions.}
}


@article{DBLP:journals/jsac/LiuAQJ21,
	author = {Jie Liu and
                  Mamta Agiwal and
                  Miao Qu and
                  Hu Jin},
	title = {Online Control of Preamble Groups With Priority in Massive IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {700--713},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018964},
	doi = {10.1109/JSAC.2020.3018964},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuAQJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) imbued with several heterogeneous services and applications is integral to the ultimate realization of connected living. However, massive connectivity would pose huge challenges not only due to immense scalability but also due to diverse characteristics of periodicity, delay-criticality, and quality-of-service (QoS) requirement. One important trend of this diversity is that the connected devices would manifest different priorities. Random access procedure (RAP) is the first step by which most devices establish connection to the base station. A major transformation is urgently needed in RAP for cellular IoT as the volume of connected devices would far exceeds human oriented connections of the legacy networks. To prioritize the RAP for IoT devices, we propose novel online control algorithm that enables the most likely successful preamble transmission for the devices that manifest higher priority requirements. In the proposed algorithm, the number of active devices in each priority is recursively estimated based on Bayesian rule and then the preambles to each priority are accordingly allocated. We extend our proposal by adopting access class baring (ACB) to optimize the algorithm and subsequently, enhance it further by incorporating access delay requirements. Extensive simulations show the effectiveness of proposed algorithms over multiple priorities and confirm that the proposed algorithms are able to resolve congestions for massive activation of IoT devices.}
}


@article{DBLP:journals/jsac/YuZCLZWA21,
	author = {Jihong Yu and
                  Pengfei Zhang and
                  Lin Chen and
                  Jiangchuan Liu and
                  Rongrong Zhang and
                  Kehao Wang and
                  Jianping An},
	title = {Stabilizing Frame Slotted Aloha-Based IoT Systems: {A} Geometric Ergodicity
                  Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {714--725},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018795},
	doi = {10.1109/JSAC.2020.3018795},
	timestamp = {Mon, 06 Dec 2021 13:35:03 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YuZCLZWA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosive deployment of the Internet of Things (IoT) brings a massive number of light-weight and energy-limited IoT devices, challenging stable wireless access. Energy-efficient, Frame Slotted Aloha (FSA) recently emerged as a promising MAC protocol for large-scale IoT systems such as Machine to Machine (M2M) and Radio Frequency Identification (RFID). Yet the stability of FSA and how to stabilize it, despite of its fundamental importance on the effective operation in practical systems, have not been systematically addressed. In order to bridge this gap, we devote this paper to designing stable FSA-based access protocol (SFP) to stabilize IoT systems. We first design an additive active node population estimation scheme and use the estimate to set frame size and participation probability for throughput optimization. We then carry out theoretical analysis demonstrating the stability of SFP in the sense of geometric ergodicity of Markov chain derived from dynamics of the active node population and its estimate. Our central theoretical result is a set of closed-form conditions on the stability of SFP. We further conduct extensive simulations whose results confirm our theoretical analysis and demonstrate the effectiveness of SFP.}
}


@article{DBLP:journals/jsac/ChengLP21,
	author = {Yiyao Cheng and
                  Lei Liu and
                  Li Ping},
	title = {Orthogonal {AMP} for Massive Access in Channels With Spatial and Temporal
                  Correlations},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {726--740},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018799},
	doi = {10.1109/JSAC.2020.3018799},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChengLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the joint device activity detection and channel estimation (JACE) problem in a massive MIMO connectivity scenario in which a large number of mobile devices are connected to a base station (BS), while only a small portion are active at any given time. The main objective is to provide an efficient transmission and detection scheme with both spatial and temporal correlations. We formulate JACE as a multiple measurement vector (MMV) problem with correlated entries in the vectors to be estimated. We propose an MMV form of the orthogonal approximate message passing algorithm (OAMP-MMV). We derive a group Gram-Schmidt orthogonalization (GGSO) procedure for the realization of OAMP-MMV. We outline a state evolution (SE) procedure for OAMP-MMV and examine its accuracy using numerical results. We also compare OAMP-MMV with existing alternatives, including AMP-MMV and GTurbo-MMV. We show that OAMP-MMV outperforms AMP-MMV when pilot sequences are generated using Hadamard pilot matrices. Such a pilot design is attractive due to the low-cost signal processing technique using the fast Hadamard transform (FHT). We also show that OAMP-MMV outperforms GTurbo-MMV in correlated channels.}
}


@article{DBLP:journals/jsac/LiuDENK21,
	author = {Yan Liu and
                  Yansha Deng and
                  Maged Elkashlan and
                  Arumugam Nallanathan and
                  George K. Karagiannidis},
	title = {Analyzing Grant-Free Access for {URLLC} Service},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {741--755},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018822},
	doi = {10.1109/JSAC.2020.3018822},
	timestamp = {Mon, 28 Aug 2023 21:38:10 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuDENK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G New Radio (NR) is expected to support new ultra-reliable low-latency communication (URLLC) service targeting at supporting the small packets transmissions with very stringent latency and reliability requirements. Current Long Term Evolution (LTE) system has been designed based on grant-based (GB) (i.e., dynamic grant) random access, which can hardly support the URLLC requirements. Grant-free (GF) (i.e., configured grant) access is proposed as a feasible and promising technology to meet such requirements, especially for uplink transmissions, which effectively saves the time of requesting/waiting for a grant. While some basic GF access features have been proposed and standardized in NR Release-15, there is still much space to improve. Being proposed as 3GPP study items, three GF access schemes with Hybrid Automatic Repeat reQuest (HARQ) retransmissions including Reactive, K-repetition, and Proactive, are analyzed in this article. Specifically, we present a spatio-temporal analytical framework for the contention-based GF access analysis. Based on this framework, we define the latent access failure probability to characterize URLLC reliability and latency performances. We propose a tractable approach to derive and analyze the latent access failure probability of the typical UE under three GF HARQ schemes. Our results show that under shorter latency constraints, the Proactive scheme provides the lowest latent access failure probability, whereas, under longer latency constraints, the K-repetition scheme achieves the lowest latent access failure probability, which depends on K. If K is overestimated, the Proactive scheme provides lower latent access failure probability than the K-repetition scheme.}
}


@article{DBLP:journals/jsac/KeGWGW21,
	author = {Malong Ke and
                  Zhen Gao and
                  Yongpeng Wu and
                  Xiqi Gao and
                  Kai{-}Kit Wong},
	title = {Massive Access in Cell-Free Massive MIMO-Based Internet of Things:
                  Cloud Computing and Edge Computing Paradigms},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {756--772},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018807},
	doi = {10.1109/JSAC.2020.3018807},
	timestamp = {Mon, 31 Jul 2023 09:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KeGWGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article studies massive access in cell-free massive multi-input multi-output (MIMO)-based Internet of Things and solves the challenging active user detection (AUD) and channel estimation (CE) problems. For the uplink transmission, we propose an advanced frame structure design to reduce the access latency. Moreover, by considering the cooperation of all access points (APs), we investigate two processing paradigms at the receiver for massive access: cloud computing and edge computing. For cloud computing, all APs are connected to a centralized processing unit (CPU), and the signals received at all APs are centrally processed at the CPU. While for edge computing, the central processing is offloaded to part of APs equipped with distributed processing units, so that the AUD and CE can be performed in a distributed processing strategy. Furthermore, by leveraging the structured sparsity of the channel matrix, we develop a structured sparsity-based generalized approximated message passing (SS-GAMP) algorithm for reliable joint AUD and CE, where the quantization accuracy of the processed signals is taken into account. Based on the SS-GAMP algorithm, a successive interference cancellation-based AUD and CE scheme is further developed under two paradigms for reduced access latency. Simulation results validate the superiority of the proposed approach over the state-of-the-art baseline schemes. Besides, the results reveal that the edge computing can achieve the similar massive access performance as the cloud computing, and the edge computing is capable of alleviating the burden on CPU, having a faster access response, and supporting more flexible AP cooperation.}
}


@article{DBLP:journals/jsac/AbebeK21,
	author = {Ameha Tsegaye Abebe and
                  Chung Gu Kang},
	title = {MIMO-Based Reliable Grant-Free Massive Access With QoS Differentiation
                  for 5G and Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {773--787},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018963},
	doi = {10.1109/JSAC.2020.3018963},
	timestamp = {Mon, 25 Sep 2023 20:37:22 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AbebeK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Grant-free (GF) access has been one of the enablers for the various use cases in 5th generation (5G) mobile system, especially for time-critical massive machine-type communication (mMTC). However, these use cases have diverse quality of service (QoS) requirements, which can be measured in terms of an access success rate from a GF random access perspective. Consequently, a GF scheme that enables supporting of diverse QoS is highly sought. This article proposes a GF access scheme in which high-QoS users superpose multiple preambles to improve their access success rate as a result of the diversity in access collision and multiple access interference seen by multiple preambles. We further show that in the presence of multiple antennas in the base station (BS), a low-complexity receiver can correctly detect active preambles with a significantly high probability, even under severe multiple access interference caused by non-orthogonal preamble transmission. A theoritical performance analysis is conducted by modeling the preamble reception as a multiple measurement vector-based compressive sensing problem. The preamble misdetection probability is shown to decrease exponentially as the number antennas at BS increases. Numerical results demonstrate multiple-order improvement in terms of the access success rate for critical-QoS users, even under severe noise and multiple access contamination.}
}


@article{DBLP:journals/jsac/CuiLZ21,
	author = {Ying Cui and
                  Shuaichao Li and
                  Wanqing Zhang},
	title = {Jointly Sparse Signal Recovery and Support Recovery via Deep Learning
                  With Applications in MIMO-Based Grant-Free Random Access},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {788--803},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018802},
	doi = {10.1109/JSAC.2020.3018802},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CuiLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we investigate jointly sparse signal recovery and jointly sparse support recovery in Multiple Measurement Vector (MMV) models for complex signals, which arise in many applications in communications and signal processing. Recent key applications include channel estimation and device activity detection in MIMO-based grant-free random access which is proposed to support massive machine-type communications (mMTC) for Internet of Things (IoT). Utilizing techniques in compressive sensing, optimization and deep learning, we propose two model-driven approaches, based on the standard auto-encoder structure for real numbers. One is to jointly design the common measurement matrix and jointly sparse signal recovery method, and the other aims to jointly design the common measurement matrix and jointly sparse support recovery method. The proposed model-driven approaches can effectively utilize features of sparsity patterns in designing common measurement matrices and adjusting model-driven decoders, and can greatly benefit from the underlying state-of-the-art recovery methods with theoretical guarantee. Hence, the obtained common measurement matrices and recovery methods can significantly outperform the underlying advanced recovery methods. We conduct extensive numerical results on channel estimation and device activity detection in MIMO-based grant-free random access. The numerical results show that the proposed approaches provide pilot sequences and channel estimation or device activity detection methods which can achieve higher estimation or detection accuracy with shorter computation time than existing ones. Furthermore, the numerical results explain how such gains are achieved via the proposed approaches.}
}


@article{DBLP:journals/jsac/KuaiYYL21,
	author = {Xiaoyan Kuai and
                  Xiaojun Yuan and
                  Wenjing Yan and
                  Ying{-}Chang Liang},
	title = {Coexistence of Human-Type and Machine-Type Communications in Uplink
                  Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {804--819},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018796},
	doi = {10.1109/JSAC.2020.3018796},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/KuaiYYL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we study the receiver design for the uplink transmission of a human-type communications (HTC) and machine-type communications (MTC) (H&M) coexisted massive MIMO system. We first establish a probability model to characterize the crucial system features including channel sparsity of massive MIMO and signal sparsity of MTC packets. With the probability model, we propose to conduct joint device activity identification, channel estimation, and signal detection. We develop a message-passing-based statistical interference framework to systematically and efficiently solve the joint estimation problem for the H&M coexisted massive MIMO system. Specifically, we propose two receiver schemes based on time-slotted and non-time-slotted grant-free random access for massive machine-type device connectivity. We show that, by exploiting the channel and signal sparsity, our proposed message-passing-based algorithms significantly outperform the conventional training-based approaches in which the device activity state and the channel are estimated by sending pilots prior to data transmission, and are able to approach the genie bound with known signal support in the relatively high signal-to-noise (SNR) regime. Last but not least, we show that there exists a significant gain in terms of the number of admissible devices in the system by allowing H&M coexistence, as compared to orthogonal transmission approaches in which different time/frequency slots are assigned to HTC and MTC services.}
}


@article{DBLP:journals/jsac/ShyianovBMH21,
	author = {Volodymyr Shyianov and
                  Faouzi Bellili and
                  Amine Mezghani and
                  Ekram Hossain},
	title = {Massive Unsourced Random Access Based on Uncoupled Compressive Sensing:
                  Another Blessing of Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {820--834},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3019722},
	doi = {10.1109/JSAC.2020.3019722},
	timestamp = {Tue, 23 Aug 2022 09:19:57 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShyianovBMH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We put forward a new algorithmic solution to the massive unsourced random access (URA) problem, by leveraging the rich spatial dimensionality offered by large-scale antenna arrays. This paper makes an observation that spatial signature is key to URA in massive connectivity setups. The proposed scheme relies on a slotted transmission framework but eliminates the need for concatenated coding that was introduced in the context of the coupled compressive sensing (CCS) paradigm. Indeed, all existing works on CCS-based URA rely on an inner/outer tree-based encoder/decoder to stitch the slot-wise recovered sequences. This paper takes a different path by harnessing the nature-provided correlations between the slot-wise reconstructed channels of each user in order to put together its decoded sequences. The required slot-wise channel estimates and decoded sequences are first obtained through the hybrid generalized approximate message passing (HyGAMP) algorithm which systematically accommodates the multiantenna-induced group sparsity. Then, a channel correlation-aware clustering framework based on the expectation-maximization (EM) concept is used together with the Hungarian algorithm to find the slot-wise optimal assignment matrices by enforcing two clustering constraints that are very specific to the problem at hand. Stitching is then accomplished by associating the decoded sequences to their respective users according to the ensuing assignment matrices. Exhaustive computer simulations reveal that the proposed scheme can bring performance improvements, at high spectral efficiencies, as compared to a state-of-the-art technique that investigates the use of large-scale antenna arrays in the context of massive URA.}
}


@article{DBLP:journals/jsac/TangCL21,
	author = {Minjie Tang and
                  Songfu Cai and
                  Vincent K. N. Lau},
	title = {Remote State Estimation With Asynchronous Mission-Critical IoT Sensors},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {835--850},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018800},
	doi = {10.1109/JSAC.2020.3018800},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TangCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider a mission-critical remote state estimation system with asynchronous massive access of the IoT sensors. We focus on remote state estimation stability of the system in the presence of asynchronous access of the sensors. Exploiting the sparsity in the observation matrix induced by the asynchronous access, we propose a low complexity 2-D message passing state estimation algorithm, where the cyclic loops in the 2-D factor graphs are removed based on the Gaussian-elimination-based quasi-diagonalization of the oversampled aggregated channel matrix of the IoT sensors. As a result, the proposed state estimation scheme is of low complexity and can achieve exact MAP estimation. Using Lyapunov drift analysis, we derive closed-form necessary and sufficient conditions for stability of the mission-critical remote state estimation system. We show that our proposed scheme can achieve significant performance gain over various state-of-the-art baselines for the large-scale system under asynchronous massive access.}
}


@article{DBLP:journals/jsac/YuCW21,
	author = {Baoquan Yu and
                  Yueming Cai and
                  Dan Wu},
	title = {Joint Access Control and Resource Allocation for Short-Packet-Based
                  mMTC in Status Update Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {851--865},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018801},
	doi = {10.1109/JSAC.2020.3018801},
	timestamp = {Tue, 02 Mar 2021 11:26:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YuCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we investigate the performance of massive machine type communications (mMTC) in status update systems, where massive machine type communication devices (MTCDs) send status packets to the BS for system monitoring. However, massive MTCDs sending status packets to the BS will cause severe packet collisions, which will have a negative impact on status update performance. In this case, it is necessary to carry out reasonable access control and resource allocation scheme to improve the status update performance for mMTC. In this article, taking the features of mMTC into consideration, we first analyze access control, packet collisions and packet errors in mMTC respectively, and derive the closed-form expression of the average age of information for all MTCDs as the performance metric, and then propose a joint access control, frame division and subchannel allocation scheme to improve the overall status update performance. Simulation and numerical results verify the correctness of theoretical results and show that our proposed scheme can achieve almost the same performance as the exhaustive search method and outperforms benchmark schemes.}
}


@article{DBLP:journals/jsac/HattabPC21,
	author = {Ghaith Hattab and
                  Petar Popovski and
                  Danijela Cabric},
	title = {Spectrum Sharing for Massive Access in Ultra-Narrowband IoT Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {866--880},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018797},
	doi = {10.1109/JSAC.2020.3018797},
	timestamp = {Thu, 14 Oct 2021 09:41:32 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HattabPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-narrowband (UNB) communications has become a signature feature for many emerging low-power wide-area (LPWA) networks. Specifically, using extremely narrowband signals helps the network connect more Internet-of-things (IoT) devices within a given band. It also improves robustness to interference, extending the coverage of the network. In this article, we study the coexistence capability of UNB networks and their scalability to enable massive access. To this end, we develop a stochastic geometry framework to analyze and model UNB networks on a large scale. The framework captures the unique characteristics of UNB communications, including the asynchronous time-frequency access, signal repetition, and the absence of base station (BS) association. Closed-form expressions of the transmission success probability and network connection density are presented for several UNB protocols. We further discuss multiband access for UNB networks, proposing a low-complexity protocol. Our analysis reveals several insights on the geographical diversity achieved when devices do not connect to a single BS, the optimal number of signal repetitions, and how to utilize multiple bands without increasing the complexity of BSs. Simulation results are provided to validate the analysis, and they show that UNB communications enables a single BS to connect thousands of devices even when the spectrum is shared with other networks.}
}


@article{DBLP:journals/jsac/QianZMYYS21,
	author = {Bo Qian and
                  Haibo Zhou and
                  Ting Ma and
                  Kai Yu and
                  Quan Yuan and
                  Xuemin Shen},
	title = {Multi-Operator Spectrum Sharing for Massive IoT Coexisting in 5G/B5G
                  Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {3},
	pages = {881--895},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018803},
	doi = {10.1109/JSAC.2020.3018803},
	timestamp = {Wed, 22 Mar 2023 21:18:25 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QianZMYYS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With a massive number of Internet-of-Things (IoT) devices connecting with the Internet via 5G or beyond 5G (B5G) wireless networks, how to support massive access for coexisting cellular users and IoT devices with quality-of-service (QoS) guarantees over limited radio spectrum is one of the main challenges. In this paper, we investigate the multi-operator dynamic spectrum sharing problem to support the coexistence of rate guaranteed cellular users and massive IoT devices. For the spectrum sharing among mobile network operators (MNOs), we introduce a wireless spectrum provider (WSP) to make spectrum trading with MNOs through the Stackelberg pricing game. This framework is inspired by the active radio access network (RAN) sharing architecture of 3GPP, which is regarded as a promising solution for MNOs to improve the resource utilization and reduce deployment and operation cost. For the coexistence of cellular users and IoT devices under each MNO, we propose the coexisting access rules to ensure their QoS and the priority of cellular users. In particular, we prove the uniqueness of the Stackelberg equilibrium (SE) solution, which can maximize the payoffs of MNOs and WSP simultaneously. Moreover, we propose an iterative algorithm for the Stackelberg pricing game, which is proved to achieve the unique SE solution. Extensive numerical simulations demonstrate that, the payoffs of WSP and MNOs are maximized and the SE solution can be reached. Meanwhile, the proposed multi-operator dynamic spectrum sharing algorithm can support more than almost 40% IoT devices compared with the existing no-sharing method, and the gap is less than about 10% compared with the exhaustive method.}
}


@article{DBLP:journals/jsac/ChenNYLAS21b,
	author = {Xiaoming Chen and
                  Derrick Wing Kwan Ng and
                  Wei Yu and
                  Erik G. Larsson and
                  Naofal Al{-}Dhahir and
                  Robert Schober},
	title = {Guest Editorial Massive Access for 5G and Beyond - Part {II}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {899--902},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018841},
	doi = {10.1109/JSAC.2020.3018841},
	timestamp = {Wed, 07 Apr 2021 16:01:26 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenNYLAS21b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This second part of the two-part Special Issue (SI) on massive access for 5G and beyond starts with several papers on massive access techniques, then switches to coverage enhancement approaches, and finishes with a paper on the application of massive access in industrial Internet-of-Things (IoT).}
}


@article{DBLP:journals/jsac/LiZGFD21,
	author = {Muye Li and
                  Shun Zhang and
                  Feifei Gao and
                  Pingzhi Fan and
                  Octavia A. Dobre},
	title = {A New Path Division Multiple Access for the Massive {MIMO-OTFS} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {903--918},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018826},
	doi = {10.1109/JSAC.2020.3018826},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiZGFD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article focuses on a new path division multiple access (PDMA) for both uplink (UL) and downlink (DL) massive multiple-input multiple-output network over a high mobility scenario, where the orthogonal time frequency space (OTFS) is adopted. First, the 3D UL channel model and the received signal model in the angle-delay-Doppler domain are studied. Secondly, the 3D-Newtonized orthogonal matching pursuit algorithm is utilized for the extraction of the UL channel parameters, including channel gains, directions of arrival, delays, and Doppler frequencies, over the antenna-time-frequency domain. Thirdly, we carefully analyze energy dispersion and power leakage of the 3D angle-delay-Doppler channels. Then, along UL, we design a path scheduling algorithm to properly assign angle-domain resources at user sides and to assure that the observation regions for different users do not overlap over the 3D cubic area, i.e., angle-delay-Doppler domain. After scheduling, different users can map their respective data to the scheduled delay-Doppler domain grids, and simultaneously send the data to base station (BS) without inter-user interference in the same OTFS block. Correspondingly, the signals at desired grids within the 3D resource space of BS are separately collected to implement the 3D channel estimation and maximal ratio combining-based data recovery over the angle-delay-Doppler domain. Then, we construct a low complexity beamforming scheme over the angle-delay-Doppler domain to achieve inter-user interference free DL communication. Simulation results are provided to demonstrate the validity of our proposed unified UL/DL PDMA scheme.}
}


@article{DBLP:journals/jsac/LinLNZL21,
	author = {Zhipeng Lin and
                  Tiejun Lv and
                  Wei Ni and
                  J. Andrew Zhang and
                  Ren Ping Liu},
	title = {Nested Hybrid Cylindrical Array Design and DoA Estimation for Massive
                  IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {919--933},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018833},
	doi = {10.1109/JSAC.2020.3018833},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LinLNZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reducing cost and power consumption while maintaining high network access capability is a key physical-layer requirement of massive Internet of Things (mIoT) networks. Deploying a hybrid array is a cost- and energy-efficient way to meet the requirement, but would penalize system degree of freedom (DoF) and channel estimation accuracy. This is because signals from multiple antennas are combined by a radio frequency (RF) network of the hybrid array. This article presents a novel hybrid uniform circular cylindrical array (UCyA) for mIoT networks. We design a nested hybrid beamforming structure based on sparse array techniques and propose the corresponding channel estimation method based on the second-order channel statistics. As a result, only a small number of RF chains are required to preserve the DoF of the UCyA. We also propose a new tensor-based two-dimensional (2-D) direction-of-arrival (DoA) estimation algorithm tailored for the proposed hybrid array. The algorithm suppresses the noise components in all tensor modes and operates on the signal data model directly, hence improving estimation accuracy with an affordable computational complexity. Corroborated by a Cramér-Rao lower bound (CRLB) analysis, simulation results show that the proposed hybrid UCyA array and the DoA estimation algorithm can accurately estimate the 2-D DoAs of a large number of IoT devices.}
}


@article{DBLP:journals/jsac/MengZWWWCW21,
	author = {Xiangming Meng and
                  Lei Zhang and
                  Chao Wang and
                  Lei Wang and
                  Yiqun Wu and
                  Yan Chen and
                  Wenjin Wang},
	title = {Advanced {NOMA} Receivers From a Unified Variational Inference Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {934--948},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018834},
	doi = {10.1109/JSAC.2020.3018834},
	timestamp = {Wed, 16 Nov 2022 21:12:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MengZWWWCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-orthogonal multiple access (NOMA) on shared resources has been identified as a promising technology in 5G to improve resource efficiency and support massive access in all kinds of transmission modes. Power domain and code domain NOMA have been extensively studied and evaluated in both literatures and 3GPP standardization, especially for the uplink where large number of users would like to send their messages to the base station. Though different in the transmitter side design, power domain NOMA and code domain NOMA share the same need of the advanced multi-user detection (MUD) design at the receiver side. Various multi-user detection algorithms have been proposed, balancing performance and complexity in different ways, which is important for the implementation of NOMA in practical networks. In this paper, we introduce a unified variational inference (VI) perspective on various universal NOMA MUD algorithms such as belief propagation (BP), expectation propagation (EP), vector EP (VEP), approximate message passing (AMP) and vector AMP (VAMP), demonstrating how they could be derived from and adapted to each other within the VI framework. Moreover, we unveil and prove that conventional elementary signal estimator (ESE) and linear minimum mean square error (LMMSE) receivers are special cases of EP and VEP, respectively, thus bridging the gap between classic linear receivers and message passing based nonlinear receivers. Such a unified perspective would not only help the design and adaptation of NOMA receivers, but also open a door for the systematic design of joint active user detection and multi-user decoding for sporadic grant-free transmission.}
}


@article{DBLP:journals/jsac/TanWWX21,
	author = {Fangqing Tan and
                  Peiran Wu and
                  Yik{-}Chung Wu and
                  Minghua Xia},
	title = {Energy-Efficient Non-Orthogonal Multicast and Unicast Transmission
                  of Cell-Free Massive {MIMO} Systems With {SWIPT}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {949--968},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3020110},
	doi = {10.1109/JSAC.2020.3020110},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TanWWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work investigates the energy-efficient resource allocation for layered-division multiplexing (LDM) based non-orthogonal multicast and unicast transmission in cell-free massive multiple-input multiple-output (MIMO) systems, where each user equipment (UE) performs wireless information and power transfer simultaneously. To begin with, the achievable data rates for multicast and unicast services are derived in closed form, as well as the received radio frequency (RF) power at each UE. Based on the analytical results, a nonsmooth and nonconvex optimization problem for energy efficiency (EE) maximization is formulated, which is however a challenging fractional programming problem with complex constraints. To suit the massive access setting, a first-order algorithm is developed to find both initial feasible point and the nearly optimal solution. Moreover, an accelerated algorithm is designed to improve the convergence speed. Numerical results demonstrate that the proposed first-order algorithms can achieve almost the same EE as that of second-order approaches yet with much lower computational complexity, which provides insight into the superiority of the proposed algorithms for massive access in cell-free massive MIMO systems.}
}


@article{DBLP:journals/jsac/ChenZZY21,
	author = {Weichao Chen and
                  Shengjie Zhao and
                  Rongqing Zhang and
                  Liuqing Yang},
	title = {Generalized User Grouping in {NOMA} Based on Overlapping Coalition
                  Formation Game},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {969--981},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018832},
	doi = {10.1109/JSAC.2020.3018832},
	timestamp = {Fri, 14 May 2021 08:28:36 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenZZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-orthogonal multiple access (NOMA) is regarded as a promising technology to provide high spectral efficiency and support massive connectivity in 5G systems. In most existing NOMA user grouping approaches, users are grouped into disjoint groups, which may lead to a waste of power resources within each NOMA group. Motivated by this, in this paper we propose a novel generalized user grouping (GuG) concept for NOMA from an overlapping perspective, which allows each user to participate in multiple groups but subject to individual maximum power constraint. In order to achieve effective GuG and maximize the system sum rate, we formulate a joint power control and GuG optimization problem. Then, we address this problem by exploiting the overlapping coalition formation (OCF) game framework, and we further propose an OCF-based algorithm in which each user can be self-organized into a desirable overlapping coalition structure. Simulation results verify the efficiency of GuG in NOMA systems and indicate that compared with traditional NOMA user grouping schemes, our proposed OCF-based GuG NOMA scheme achieves significant performance gains in terms of system sum rate.}
}


@article{DBLP:journals/jsac/KimCLSP21,
	author = {Heasung Kim and
                  Taehyun Cho and
                  Jungwoo Lee and
                  Wonjae Shin and
                  H. Vincent Poor},
	title = {Optimized Shallow Neural Networks for Sum-Rate Maximization in Energy
                  Harvesting Downlink Multiuser {NOMA} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {982--997},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018827},
	doi = {10.1109/JSAC.2020.3018827},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KimCLSP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article considers a power allocation problem in energy harvesting downlink non-orthogonal multiple access (NOMA) systems in which a transmitter sends desired messages to their respective receivers by using harvested energy. To tackle this problem, we make use of a reinforcement learning approach based on a shallow neural network structure. We prove that the optimal power allocation policy and the optimal action-value function depend monotonically on some of their input variables and the shallow neural network structure is designed based on properties revealed in the proof. Different from inefficient deep learning methods that tend to require tremendous computational resources, this structure is capable of fully capturing the characteristics of the desired function with a single hidden layer. The optimized structure also allows learning agents to be robust and highly reliable in learning about randomly occurring data. Furthermore, we provide comprehensive experimental results in harsh environments where various arbitrary factors are assumed in order to demonstrate the robustness of the proposed learning approach compared with deep neural networks without proper grounds. It is also shown that the proposed learning process converges to a policy that outperforms existing power allocation algorithms.}
}


@article{DBLP:journals/jsac/LiXWW21,
	author = {Zongze Li and
                  Minghua Xia and
                  Miaowen Wen and
                  Yik{-}Chung Wu},
	title = {Massive Access in Secure {NOMA} Under Imperfect {CSI:} Security Guaranteed
                  Sum-Rate Maximization With First-Order Algorithm},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {998--1014},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018805},
	doi = {10.1109/JSAC.2020.3018805},
	timestamp = {Mon, 28 Aug 2023 21:38:10 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiXWW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Non-orthogonal multiple access (NOMA) is a promising solution for secure transmission under massive access. However, in addition to the uncertain channel state information (CSI) of the eavesdroppers due to their passive nature, the CSI of the legitimate users may also be imperfect at the base station due to the limited feedback. Under both channel uncertainties, the optimal power allocation and transmission rate design for a secure NOMA scheme is currently not known due to the difficulty of handling the probabilistic constraints. This article fills this gap by proposing novel transformation of the probabilistic constraints and variable decoupling so that the security guaranteed sum-rate maximization problem can be solved by alternatively executing branch-and-bound method and difference of convex programming. To scale the solution to a truly massive access scenario, a first-order algorithm with very low complexity is further proposed. Simulation results show that the proposed first-order algorithm achieves identical performance to the conventional method but saves at least two orders of magnitude in computation time. Moreover, the resultant transmission scheme significantly improves the security guaranteed sum-rate compared to the orthogonal multiple access transmission and NOMA ignoring CSI uncertainty.}
}


@article{DBLP:journals/jsac/LiuLP21,
	author = {Binghong Liu and
                  Chenxi Liu and
                  Mugen Peng},
	title = {Resource Allocation for Energy-Efficient {MEC} in NOMA-Enabled Massive
                  IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1015--1027},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018809},
	doi = {10.1109/JSAC.2020.3018809},
	timestamp = {Mon, 15 May 2023 16:24:41 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating mobile edge computing (MEC) into the Internet of Things (IoT) enables the IoT devices of limited computation capabilities and energy to offload their computation-intensive and delay-sensitive tasks to the network edge, thereby providing high quality of service to the devices. In this article, we apply non-orthogonal multiple access (NOMA) technique to enable massive connectivity and investigate how it can be exploited to achieve energy-efficient MEC in IoT networks. In order to maximize the energy efficiency for offloading, while simultaneously satisfying the maximum tolerable delay constraints of IoT devices, a joint radio and computation resource allocation problem is formulated, which takes both intra- and inter-cell interference into consideration. To tackle this intractable mixed integer non-convex problem, we first decouple it into separated radio and computation resource allocation problems. Then, the radio resource allocation problem is further decomposed into a subchannel allocation problem and a power allocation problem, which can be solved by matching and sequential convex programming algorithms, respectively. Based on the obtained radio resource allocation solution, the computation resource allocation problem can be solved by utilizing the Knapsack method. Numerical results validate our analysis and show that our proposed scheme can significantly improve the energy efficiency of NOMA-enabled MEC in IoT networks compared to the existing baselines.}
}


@article{DBLP:journals/jsac/Al-EryaniAH21,
	author = {Yasser F. Al{-}Eryani and
                  Mohamed Akrout and
                  Ekram Hossain},
	title = {Multiple Access in Cell-Free Networks: Outage Performance, Dynamic
                  Clustering, and Deep Reinforcement Learning-Based Design},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1028--1042},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018825},
	doi = {10.1109/JSAC.2020.3018825},
	timestamp = {Tue, 23 Aug 2022 09:19:56 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/Al-EryaniAH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In future cell-free (or cell-less) wireless networks, a large number of devices in a geographical area will be served simultaneously in non-orthogonal multiple access scenarios by a large number of distributed access points (APs), which coordinate with a centralized processing pool. For such a centralized cell-free network with static predefined beamforming design, we first derive a closed-form expression of uplink outage probability for a user/device. To reduce the complexity of joint processing of received signals in presence of a large number of devices and APs, we propose a novel dynamic cell-free network architecture. In this architecture, the distributed APs are clustered (i.e. partitioned) among a set of subgroups with each subgroup acting as a virtual AP in a distributed antenna system (DAS). The conventional static cell-free network is a special case of this dynamic cell-free network when the cluster size is one. For this dynamic cell-free network, we propose a successive interference cancellation (SIC)-enabled signal detection method and an inter-user-interference (IUI)-aware receive diversity combining scheme. We then formulate the general problem of clustering the APs and designing the beamforming vectors with an objective such as maximizing the sum rate or maximizing the minimum rate. To this end, we propose a hybrid deep reinforcement learning (DRL) model, namely, a deep deterministic policy gradient (DDPG)-deep double Q-network (DDQN) model to solve the optimization problem for online implementation with low complexity. The DRL model for sum-rate optimization significantly outperforms that for maximizing the minimum rate in terms of average per-user rate performance. Also, in our system setting, the proposed DDPG-DDQN scheme is found to achieve around 78% of the rate achievable through an exhaustive search-based design.}
}


@article{DBLP:journals/jsac/KishkA21,
	author = {Mustafa A. Kishk and
                  Mohamed{-}Slim Alouini},
	title = {Exploiting Randomly Located Blockages for Large-Scale Deployment of
                  Intelligent Surfaces},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1043--1056},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018808},
	doi = {10.1109/JSAC.2020.3018808},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KishkA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the promising technologies for the next generation wireless networks is the reconfigurable intelligent surfaces (RISs). This technology provides planar surfaces the capability to manipulate the reflected waves of impinging signals, which leads to a more controllable wireless environment. One potential use case of such technology is providing indirect line-of-sight (LoS) links between mobile users and base stations (BSs) which do not have direct LoS channels. Objects that act as blockages for the communication links, such as buildings or trees, can be equipped with RISs to enhance the coverage probability of the cellular network through providing extra indirect LoS-links. In this article, we use tools from stochastic geometry to study the effect of large-scale deployment of RISs on the performance of cellular networks. In particular, we model the blockages using the line Boolean model. For this setup, we study how equipping a subset of the blockages with RISs will enhance the performance of the cellular network. We first derive the ratio of the blind-spots to the total area. Next, we derive the probability that a typical mobile user associates with a BS using an RIS. Finally, we derive the probability distribution of the path-loss between the typical user and its associated BS. We draw multiple useful system-level insights from the proposed analysis. For instance, we show that deployment of RISs highly improves the coverage regions of the BSs. Furthermore, we show that to ensure that the ratio of blind-spots to the total area is below 10 -5 , the required density of RISs increases from just 6 RISs/km 2 when the density of the blockages is 300 blockage/km 2 to 490 RISs/km 2 when the density of the blockages is 700 blockage/km 2 .}
}


@article{DBLP:journals/jsac/LiuLCP21,
	author = {Xiao Liu and
                  Yuanwei Liu and
                  Yue Chen and
                  H. Vincent Poor},
	title = {{RIS} Enhanced Massive Non-Orthogonal Multiple Access Networks: Deployment
                  and Passive Beamforming Design},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1057--1071},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018823},
	doi = {10.1109/JSAC.2020.3018823},
	timestamp = {Wed, 07 Apr 2021 16:01:26 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuLCP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A novel framework is proposed for the deployment and passive beamforming design of a reconfigurable intelligent surface (RIS) with the aid of non-orthogonal multiple access (NOMA) technology. The problem of joint deployment, phase shift design, as well as power allocation in the multiple-input-single-output (MISO) NOMA network is formulated for maximizing the energy efficiency with considering users particular data requirements. To tackle this pertinent problem, machine learning approaches are adopted in two steps. Firstly, a novel long short-term memory (LSTM) based echo state network (ESN) algorithm is proposed to predict users' tele-traffic demand by leveraging a real dataset. Secondly, a decaying double deep Q-network (D 3 QN) based position-acquisition and phase-control algorithm is proposed to solve the joint problem of deployment and design of the RIS. In the proposed algorithm, the base station, which controls the RIS by a controller, acts as an agent. The agent periodically observes the state of the RIS-enhanced system for attaining the optimal deployment and design policies of the RIS by learning from its mistakes and the feedback of users. Additionally, it is proved that the proposed D 3 QN based deployment and design algorithm is capable of converging within mild conditions. Simulation results are provided for illustrating that the proposed LSTM-based ESN algorithm is capable of striking a tradeoff between the prediction accuracy and computational complexity. Finally, it is demonstrated that the proposed D 3 QN based algorithm outperforms the benchmarks, while the NOMA-enhanced RIS system is capable of achieving higher energy efficiency than orthogonal multiple access (OMA) enabled RIS system.}
}


@article{DBLP:journals/jsac/MursiaSGCCG21,
	author = {Placido Mursia and
                  Vincenzo Sciancalepore and
                  Andres Garcia{-}Saavedra and
                  Laura Cottatellucci and
                  Xavier P{\'{e}}rez Costa and
                  David Gesbert},
	title = {{RISMA:} Reconfigurable Intelligent Surfaces Enabling Beamforming
                  for IoT Massive Access},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1072--1085},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018829},
	doi = {10.1109/JSAC.2020.3018829},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MursiaSGCCG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive access for Internet-of-Things (IoT) in beyond 5G networks represents a daunting challenge for conventional bandwidth-limited technologies. Millimeter-wave technologies (mmWave)-which provide large chunks of bandwidth at the cost of more complex wireless processors in harsher radio environments-is a promising alternative to accommodate massive IoT but its cost and power requirements are an obstacle for wide adoption in practice. In this context, meta-materials arise as a key innovation enabler to address this challenge by Re-configurable Intelligent Surfaces (RISs). In this article we take on the challenge and study a beyond 5G scenario consisting of a multi-antenna base station (BS) serving a large set of single-antenna user equipments (UEs) with the aid of RISs to cope with non-line-of-sight paths. Specifically, we build a mathematical framework to jointly optimize the precoding strategy of the BS and the RIS parameters in order to minimize the system sum mean squared error (SMSE). This novel approach reveals convenient properties used to design two algorithms, RISMA and Lo- RISMA, which are able to either find simple and efficient solutions to our problem (the former) or accommodate practical constraints with low-resolution RISs (the latter). Numerical results show that our algorithms outperform conventional benchmarks that do not employ RIS (even with low-resolution meta-surfaces) with gains that span from 20% to 120% in sum rate performance.}
}


@article{DBLP:journals/jsac/ChenZBZA21,
	author = {Shuaifei Chen and
                  Jiayi Zhang and
                  Emil Bj{\"{o}}rnson and
                  Jing Zhang and
                  Bo Ai},
	title = {Structured Massive Access for Scalable Cell-Free Massive {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1086--1100},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018836},
	doi = {10.1109/JSAC.2020.3018836},
	timestamp = {Mon, 21 Aug 2023 15:51:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenZBZA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to meet the demand for increasing number of users, higher data rates, and stringent quality-of-service (QoS) in the beyond fifth-generation (B5G) networks? Cell-free massive multiple-input multiple-output (MIMO) is considered as a promising solution, in which many wireless access points cooperate to jointly serve the users by exploiting coherent signal processing. However, there are still many unsolved practical issues in cell-free massive MIMO systems, whereof scalable massive access implementation is one of the most vital. In this paper, we propose a new framework for structured massive access in cell-free massive MIMO systems, which comprises one initial access algorithm, a partial large-scale fading decoding (P-LSFD) strategy, two pilot assignment schemes, and one fractional power control policy. New closed-form spectral efficiency (SE) expressions with maximum ratio (MR) combining are derived. The simulation results show that our proposed framework provides high SE when using local partial minimum mean-square error (LP-MMSE) and MR combining. Specifically, the proposed initial access algorithm and pilot assignment schemes outperform their corresponding benchmarks, P-LSFD achieves scalability with a negligible performance loss compared to the conventional optimal large-scale fading decoding (LSFD), and scalable fractional power control provides a controllable trade-off between user fairness and the average SE.}
}


@article{DBLP:journals/jsac/KhairyBCC21,
	author = {Sami Khairy and
                  Prasanna Balaprakash and
                  Lin X. Cai and
                  Yu Cheng},
	title = {Constrained Deep Reinforcement Learning for Energy Sustainable Multi-UAV
                  Based Random Access IoT Networks With {NOMA}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1101--1115},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018804},
	doi = {10.1109/JSAC.2020.3018804},
	timestamp = {Wed, 07 Apr 2021 16:01:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KhairyBCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we apply the Non-Orthogonal Multiple Access (NOMA) technique to improve the massive channel access of a wireless IoT network where solar-powered Unmanned Aerial Vehicles (UAVs) relay data from IoT devices to remote servers. Specifically, IoT devices contend for accessing the shared wireless channel using an adaptive p-persistent slotted Aloha protocol; and the solar-powered UAVs adopt Successive Interference Cancellation (SIC) to decode multiple received data from IoT devices to improve access efficiency. To enable an energy-sustainable capacity-optimal network, we study the joint problem of dynamic multi-UAV altitude control and multi-cell wireless channel access management of IoT devices as a stochastic control problem with multiple energy constraints. We first formulate this problem as a Constrained Markov Decision Process (CMDP), and propose an online model-free Constrained Deep Reinforcement Learning (CDRL) algorithm based on Lagrangian primal-dual policy optimization to solve the CMDP. Extensive simulations demonstrate that our proposed algorithm learns a cooperative policy in which the altitude of UAVs and channel access probability of IoT devices are dynamically controlled to attain the maximal long-term network capacity while ensuring energy sustainability of UAVs, outperforming baseline schemes. The proposed CDRL agent can be trained on a small network, yet the learned policy can efficiently manage networks with a massive number of IoT devices and varying initial states, which can amortize the cost of training the CDRL agent.}
}


@article{DBLP:journals/jsac/LiuFCWG21,
	author = {Chengxiao Liu and
                  Wei Feng and
                  Yunfei Chen and
                  Cheng{-}Xiang Wang and
                  Ning Ge},
	title = {Cell-Free Satellite-UAV Networks for 6G Wide-Area Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1116--1131},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018837},
	doi = {10.1109/JSAC.2020.3018837},
	timestamp = {Tue, 18 May 2021 15:57:53 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuFCWG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In fifth generation (5G) and beyond Internet of Things (IoT), it becomes increasingly important to serve a massive number of IoT devices outside the coverage of terrestrial cellular networks. Due to their own limitations, unmanned aerial vehicles (UAVs) and satellites need to coordinate with each other in the coverage holes of 5G, leading to a cognitive satellite-UAV network (CSUN). In this paper, we investigate multi-domain resource allocation for CSUNs consisting of a satellite and a swarm of UAVs, so as to improve the efficiency of massive access in wide areas. Particularly, the cell-free on-demand coverage is established to overcome the cost-ineffectiveness of conventional cellular architecture. Opportunistic spectrum sharing is also implemented to cope with the spectrum scarcity problem. To this end, a process-oriented optimization framework is proposed for jointly allocating subchannels, transmit power and hovering times, which considers the whole flight process of UAVs and uses only the slowly-varying large-scale channel state information (CSI). Under the on-board energy constraints of UAVs and interference temperature constraints from UAV swarm to satellite users, we present iterative multi-domain resource allocation algorithms to improve network efficiency with guaranteed user fairness. Simulation results demonstrate the superiority of the proposed algorithms. Moreover, the adaptive cell-free coverage pattern is observed, which implies a promising way to efficiently serve wide-area IoT devices in the upcoming sixth generation (6G) era.}
}


@article{DBLP:journals/jsac/SunWYZ21,
	author = {Zhuo Sun and
                  Zhiqiang Wei and
                  Nan Yang and
                  Xiangyun Zhou},
	title = {Two-Tier Communication for UAV-Enabled Massive IoT Systems: Performance
                  Analysis and Joint Design of Trajectory and Resource Allocation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1132--1146},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018855},
	doi = {10.1109/JSAC.2020.3018855},
	timestamp = {Tue, 15 Mar 2022 16:14:16 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunWYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, we propose a two-tier communication strategy to facilitate data collection in unmanned aerial vehicle (UAV)-enabled massive Internet of Things (IoT) systems through introducing ground access points (APs) to serve between the UAV and IoT devices. In the first tier of our proposed strategy, all IoT devices transmit their packets to their local APs via a multi-channel ALOHA-based random access scheme, while in the second tier, APs deliver their aggregated data to the UAV through coordinated time division multiple access. Thus, our introduced APs not only liberate the UAV from the potential massive IoT congestion but also facilitate the design of UAV's trajectory based on the location of APs. To examine the performance of our strategy, we propose a tractable framework to analyze the average system throughput. We reveal that the average two-tier throughput of each AP monotonically increases with its maximum achievable throughput in the second tier, while the increasing slope becomes steeper with a higher traffic load mean in the first tier. Then, we formulate the joint design of UAV's trajectory and resource allocation as a non-convex optimization problem to maximize the average system throughput while considering the heterogeneous quality of service requirement of each AP. To solve this problem, a low-complexity iterative algorithm is devised based on successive convex approximation. Numerical results demonstrate the substantial average system throughput gain achieved by our proposed strategy and design in the context of massive access, compared to the baseline schemes in the literature.}
}


@article{DBLP:journals/jsac/JiaSLZH21,
	author = {Ziye Jia and
                  Min Sheng and
                  Jiandong Li and
                  Di Zhou and
                  Zhu Han},
	title = {Joint {HAP} Access and {LEO} Satellite Backhaul in 6G: Matching Game-Based
                  Approaches},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1147--1159},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018824},
	doi = {10.1109/JSAC.2020.3018824},
	timestamp = {Wed, 05 Jan 2022 14:30:57 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JiaSLZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space-air-ground networks play important roles in both fifth generation (5G) and sixth generation (6G) techniques. Low earth orbit (LEO) satellites and high altitude platforms (HAPs) are key components in space-air-ground networks to provide access services for the massive mobile and Internet of Things (IoT) users, especially in remote areas short of ground base station coverage. LEO satellite networks provide global coverage, while HAPs provide terrestrial users with closer, stable massive access service. In this work, we consider the cooperation of LEO satellites and HAPs for the massive access and data backhaul of remote area users. The problem is formulated to maximize the revenue in LEO satellites, which is in the form of mixed integer nonlinear programming. Since finding the optimal solution by exhaustive search is extremely complicated with a large scale of network, we propose a satellite-oriented restricted three-sided matching algorithm to deal with the matching among users, HAPs, and satellites. Furthermore, to tackle the dynamic connections between satellites and HAPs caused by the periodic motion of satellites, we present a two-tier matching algorithm, composed of the Gale-Shapley-based matching algorithm between users and HAPs, and the random path to pairwise-stable matching algorithm between HAPs and satellites. Numerical results show the effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/jsac/JiaCW21,
	author = {Wen{-}Kang Jia and
                  Yaw{-}Chung Chen and
                  Xufang Wang},
	title = {UMUcast: {A} Framework for Massive Small-Data Delivering in Industrial
                  Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {4},
	pages = {1160--1176},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3018828},
	doi = {10.1109/JSAC.2020.3018828},
	timestamp = {Tue, 21 May 2024 13:17:06 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JiaCW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a key infrastructure of Industry 4.0, Industrial Internet-of-Things (IIoT) promises the opportunity to build powerful industrial environments by leveraging the growing ubiquity of wired and wireless communication technologies. Designing a data delivery scheme in the future IIoT networks is undoubtedly a challenging task, as it should satisfy several conflicting requirements: massive-scale, data-intensive, and mission-critical, the requirements of which have motivated the desired need for feasible IIoT network architecture. In particular, the traffic characteristics of certain IIoT applications feature small-data patterns especially in typical automation control scenarios such as robot control on the downlink. In order to reduce the huge overhead associated with each individual unicast transmission of the small-data message, we propose a novel “multipoint-to-multipoint” and/or “point-to-multipoint with different contents” communication paradigm-Uni-Multi-Unicast (UMUcast), which is based-on traditional 4G technologies such as evolved Multimedia Broadcast Multicast Service (eMBMS) and Group Communication System Enablers (GCSE), and novel 5G technologies such as Multi-Access Edge Computing (MEC). For the downlink, the UMUcast transmitter can jointly encode multiple-sources' small-data messages into a single chunk at MEC equipment in conjoint with a gNB, where chunk can be one-off transmission to multiple receivers simultaneously through an eMBMS frame, whereas the chunk is decoded into multiple small-data by each individual IIoT devices separately and respectively. The simulation results show that UMUcast has remarkable improvements over conventional point-to-point unicasting in handling multi-source, multi-destination, and massive small-data delivery with characteristics of low-overhead, high-throughput, and ultra-low-latency for future 5G-based IIoT networks.}
}


@article{DBLP:journals/jsac/YatesSBKMU21,
	author = {Roy D. Yates and
                  Yin Sun and
                  D. Richard Brown and
                  Sanjit K. Kaul and
                  Eytan H. Modiano and
                  Sennur Ulukus},
	title = {Guest Editorial Age of Information},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1179--1182},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3066816},
	doi = {10.1109/JSAC.2021.3066816},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YatesSBKMU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the emergence of real-time cyberphysical systems, status updates are becoming an important form of communication. Vehicular messaging, security reports from homes and offices, surveillance video, and video feedback from remote-controlled systems all represent status updates at networked monitors that need to be as timely as possible. However, this is typically constrained by limited network resources. This tension has led to the introduction of new performance metrics based on the Age of Information (AoI) that capture how timely is one’s knowledge of an entity or process. Optimization based on AoI metrics of both the networks and the senders’ updating policies has yielded new and even surprising results.}
}


@article{DBLP:journals/jsac/YatesSBKMU21a,
	author = {Roy D. Yates and
                  Yin Sun and
                  D. Richard Brown and
                  Sanjit K. Kaul and
                  Eytan H. Modiano and
                  Sennur Ulukus},
	title = {Age of Information: An Introduction and Survey},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1183--1210},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065072},
	doi = {10.1109/JSAC.2021.3065072},
	timestamp = {Sat, 30 Sep 2023 10:20:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YatesSBKMU21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We summarize recent contributions in the broad area of age of information (AoI). In particular, we describe the current state of the art in the design and optimization of low-latency cyberphysical systems and applications in which sources send time-stamped status updates to interested recipients. These applications desire status updates at the recipients to be as timely as possible; however, this is typically constrained by limited system resources. We describe AoI timeliness metrics and present general methods of AoI evaluation analysis that are applicable to a wide variety of sources and systems. Starting from elementary single-server queues, we apply these AoI methods to a range of increasingly complex systems, including energy harvesting sensors transmitting over noisy channels, parallel server systems, queueing networks, and various single-hop and multi-hop wireless networks. We also explore how update age is related to MMSE methods of sampling, estimation and control of stochastic processes. The paper concludes with a review of efforts to employ age optimization in cyberphysical applications.}
}


@article{DBLP:journals/jsac/ZhangAHP21,
	author = {Meng Zhang and
                  Ahmed Arafa and
                  Jianwei Huang and
                  H. Vincent Poor},
	title = {Pricing Fresh Data},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1211--1225},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065088},
	doi = {10.1109/JSAC.2021.3065088},
	timestamp = {Mon, 08 Aug 2022 11:28:42 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangAHP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce the concept of fresh data trading, in which a destination user requests, and pays for, fresh data updates from a source provider, and data freshness is captured by the age of information (AoI) metric. Keeping data fresh relies on costly frequent data updates by the source, which motivates the source to price fresh data. In this work, the destination incurs an age-related cost, modeled as a general increasing function of the AoI. The source designs a pricing mechanism to maximize its profit, while the destination chooses a data update schedule to trade off its payments to the source and its age-related cost. Depending on different real-time applications and scenarios, we study both a finite-horizon model and an infinite-horizon model with time discounting. The key challenge of designing the optimal pricing scheme lies in the destination's time-interdependent valuations, due to the nature of AoI, and the infinite-dimensional dynamic optimization. To this end, we exploit three different dimensions in designing pricing by studying three pricing schemes: a time-dependent pricing scheme, in which the price for each update depends on when it is requested; a quantity-based pricing scheme, in which the price of each update depends on how many updates have been previously requested; and a simple subscription-based pricing scheme, in which the price per update is constant but the source charges an additional subscription fee. Our analysis reveals that (1) the optimal subscription-based pricing maximizes the source's profit among all possible pricing schemes under both finite-horizon and infinite-horizon models; (2) the optimal quantity-based pricing scheme is only optimal with a finite horizon; and (3) the time-dependent pricing scheme, under the infinite-horizon model with significant time discounting, is asymptotically optimal. Numerical results show that the profit-maximizing pricing schemes can also lead to significant reductions in AoI and social costs, and that a moderate degree of time discounting is enough to achieve a close-to-optimal time-dependent pricing scheme.}
}


@article{DBLP:journals/jsac/ZhangAWB21,
	author = {Meng Zhang and
                  Ahmed Arafa and
                  Ermin Wei and
                  Randall Berry},
	title = {Optimal and Quantized Mechanism Design for Fresh Data Acquisition},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1226--1239},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065090},
	doi = {10.1109/JSAC.2021.3065090},
	timestamp = {Mon, 08 Aug 2022 11:28:42 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangAWB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of real-time applications has spurred much interest in data freshness, captured by the age-of-information (AoI) metric. When strategic data sources have private market information, a fundamental economic challenge is how to incentivize them to acquire fresh data and optimize the age-related performance. In this work, we consider an information update system in which a destination acquires, and pays for, fresh data updates from multiple sources. The destination incurs an age-related cost, modeled as a general increasing function of the AoI. Each source is strategic and incurs a sampling cost, which is its private information and may not be truthfully reported to the destination. The destination decides on the price of updates, when to get them, and who should generate them, based on the sources' reported sampling costs. We show that a benchmark that naively trusts the sources' reports can lead to an arbitrarily bad outcome compared to the case where sources truthfully report. To tackle this issue, we design an optimal (economic) mechanism for timely information acquisition following Myerson's seminal work. To this end, our proposed optimal mechanism minimizes the sum of the destination's age-related cost and its payment to the sources, while ensuring that the sources truthfully report their private information and will voluntarily participate in the mechanism. However, finding the optimal mechanisms may suffer from prohibitively expensive computational overheads as it involves solving a nonlinear infinite-dimensional optimization problem. We further propose a quantized version of the optimal mechanism that achieves asymptotic optimality, maintains the other economic properties, and enables one to tradeoff between optimality and computational overheads. Our analytical and numerical studies show that (i) both the optimal and quantized mechanisms can lead to an unbounded benefit under some distributions of the source costs compared against a benchmark; (ii) the optimal and quantized mechanisms are most beneficial when there are few sources with heterogeneous sampling costs.}
}


@article{DBLP:journals/jsac/SauravV21,
	author = {Kumar Saurav and
                  Rahul Vaze},
	title = {Game of Ages in a Distributed Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1240--1249},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065053},
	doi = {10.1109/JSAC.2021.3065053},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SauravV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a distributed IoT network, where each node wants to minimize its own age of information and there is a cost to make any transmission. A collision model is considered, where any transmission is successful from a node to a common monitor if no other node transmits in the same slot. Nodes cannot coordinate their transmission, and can learn about the network only via binary collision information. Under this distributed competition model, the objective of this paper is to find a distributed transmission strategy for each node that converges to an equilibrium that only depends on the past observations seen by each node and does not require network information, e.g., the number of other nodes, or their strategies. A simple update strategy is shown to converge to an equilibrium for any number of nodes that are unknown to the update strategy. The equilibrium achieved is in fact a Nash equilibrium for a suitable utility function, that captures all the right tradeoffs for each node.}
}


@article{DBLP:journals/jsac/FerdowsiASD21,
	author = {Aidin Ferdowsi and
                  Mohamed A. Abd{-}Elmagid and
                  Walid Saad and
                  Harpreet S. Dhillon},
	title = {Neural Combinatorial Deep Reinforcement Learning for Age-Optimal Joint
                  Trajectory and Scheduling Design in UAV-Assisted Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1250--1265},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065049},
	doi = {10.1109/JSAC.2021.3065049},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/FerdowsiASD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this article, an unmanned aerial vehicle (UAV)-assisted wireless network is considered in which a battery-constrained UAV is assumed to move towards energy-constrained ground nodes to receive status updates about their observed processes. The UAV's flight trajectory and scheduling of status updates are jointly optimized with the objective of minimizing the normalized weighted sum of Age of Information (NWAoI) values for different physical processes at the UAV. The problem is first formulated as a mixed-integer program. Then, for a given scheduling policy, a convex optimization-based solution is proposed to derive the UAV's optimal flight trajectory and time instants on updates. However, finding the optimal scheduling policy is challenging due to the combinatorial nature of the formulated problem. Therefore, to complement the proposed convex optimization-based solution, a finite-horizon Markov decision process (MDP) is used to find the optimal scheduling policy. Since the state space of the MDP is extremely large, a novel neural combinatorial-based deep reinforcement learning (NCRL) algorithm using deep Q-network (DQN) is proposed to obtain the optimal policy. However, for large-scale scenarios with numerous nodes, the DQN architecture cannot efficiently learn the optimal scheduling policy anymore. Motivated by this, a long short-term memory (LSTM)-based autoencoder is proposed to map the state space to a fixed-size vector representation in such large-scale scenarios while capturing the spatio-temporal interdependence between the update locations and time instants. A lower bound on the minimum NWAoI is analytically derived which provides system design guidelines on the appropriate choice of importance weights for different nodes. Furthermore, an upper bound on the UAV's minimum speed is obtained to achieve this lower bound value. The numerical results also demonstrate that the proposed NCRL approach can significantly improve the achievable NWAoI per process compared to the baseline policies, such as weight-based and discretized state DQN policies.}
}


@article{DBLP:journals/jsac/LiL21,
	author = {Bin Li and
                  Jia Liu},
	title = {Achieving Information Freshness With Selfish and Rational Users in
                  Mobile Crowd-Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1266--1276},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065092},
	doi = {10.1109/JSAC.2021.3065092},
	timestamp = {Mon, 28 Aug 2023 21:38:10 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of smart mobile devices has spurred an explosive growth of mobile crowd-learning services, where service providers rely on the user community to voluntarily collect, report, and share real-time information for a collection of scattered points of interest (PoI). A critical factor affecting the future large-scale adoption of such mobile crowd-learning applications is the freshness of the crowd-learned information, which can be measured by a metric termed "age-of-information" (AoI). However, we show that the AoI of mobile crowd-learning could be arbitrarily bad under selfish and rational users\' behaviors if the system is poorly designed. This motivates us to design efficient reward mechanisms to incentivize mobile users to report information in time, with the goal to keep the AoI and congestion level of each PoI low. Toward this end, we consider a simple linear AoI-based reward mechanism and analyze its AoI and congestion performances in terms of price of anarchy (PoA), which characterizes the degradation of the system efficiency due to selfish and rational behavior of users. In this paper, we consider both average maximum age and average weighted sum of age. Remarkably, we show that the proposed mechanism achieves the optimal AoI performance in terms of average maximum age asymptotically in a deterministic scenario, i.e., the corresponding PoA decreases to 0 asymptotically. Moreover, the PoA in terms of average total age under our proposed mechanism can be upper-bounded by 1/2 asymptotically. Further, we prove that the proposed mechanism achieves a bounded PoA in general stochastic cases, and the bound only depends on system parameters. Particularly, when the service rates of PoIs are symmetric in stochastic cases, the achieved PoA is upper-bounded by 1/2 asymptotically. Collectively, this work advances our understanding of information freshness in mobile crowd-learning systems.}
}


@article{DBLP:journals/jsac/LvZWC21,
	author = {Hongtao Lv and
                  Zhenzhe Zheng and
                  Fan Wu and
                  Guihai Chen},
	title = {Strategy-Proof Online Mechanisms for Weighted AoI Minimization in
                  Edge Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1277--1292},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065078},
	doi = {10.1109/JSAC.2021.3065078},
	timestamp = {Sat, 09 Apr 2022 12:32:42 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LvZWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time information processing is critical to the success of diverse applications from many areas. Age of Information (AoI), as a new metric, has received considerable attention to evaluate the performance of real-time information processing systems. In recent years, edge computing is becoming an efficient paradigm to reduce the AoI and to provide the real-time services. Considering the substantial deployment cost and the resulting resource limitation in edge computing, a proper pricing mechanism is highly necessary to fully utilize edge resources and then minimize the overall AoI of the whole system. However, there are two challenges to design this mechanism: 1) the priorities (or values) of the real-time computing tasks, critical to the efficient resource allocation, are usually private information of users and may be manipulated by selfish users for their own interests; 2) due to the time-varying property of AoI, the values of the tasks discount with time, making the traditional pricing mechanisms infeasible. In this paper, we extend the classical Myerson Theorem to the online setting with time discounting tasks values, and accordingly propose an online auction mechanism, called PreDisc, including an allocation rule and a payment rule. We leverage dynamic programming to greedily allocate resources in each time slot, and charge the winning user with a new critical price, extended from the classical Myerson payment rule. A preemption factor is further employed to make a trade-off between the newly arrived tasks and ongoing tasks. We prove that PreDisc guarantees the economic property of strategy-proofness and achieves a constant competitive ratio. We conduct extensive simulations and the results demonstrate that PreDisc outperforms the traditional mechanisms, in terms of both weighted AoI and revenue of edge service providers. Compared with the optimal solution in offline VCG mechanism, PreDisc has much lower computation complexity with only a slight performance loss.}
}


@article{DBLP:journals/jsac/InoueK21,
	author = {Yoshiaki Inoue and
                  Tomotaka Kimura},
	title = {Age-Effective Information Updating Over Intermittently Connected MANETs},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1293--1308},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065031},
	doi = {10.1109/JSAC.2021.3065031},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/InoueK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Immediately after the occurrence of a natural disaster, communication infrastructures used in daily life become temporarily unavailable. Under such a situation, intermittently connected mobile ad hoc networks (MANETs) play an important role in providing post-disaster networking. While previous studies on such networks have mainly focused on one-to-one messaging applications, the importance of monitoring applications has become increasingly important in recent years. For monitoring applications, the key performance measure is given by the freshness of the information, rather than the traditional delay characteristics. In this paper, we present a mathematical analysis of the age of information (AoI) for intermittently connected MANETs, which captures the information freshness of monitoring applications. We further investigate basic principles in the network design based on the analytical results obtained. In particular, we discuss the AoI-energy tradeoff from different perspectives of source and relay nodes.}
}


@article{DBLP:journals/jsac/HuC21,
	author = {Shaoling Hu and
                  Wei Chen},
	title = {Monitoring Real-Time Status of Analog Sources: {A} Cross-Layer Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1309--1324},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065076},
	doi = {10.1109/JSAC.2021.3065076},
	timestamp = {Thu, 29 Apr 2021 15:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time status updating or monitoring plays a critical role in emerging applications including Industrial Internet of Things (IIoT) and Vehicular-to-Everything (V2X) systems. In these applications, the real-time status is mostly characterized by analog signal samples desiring lossy compression before digital transmissions. However, how to ensure the data freshness while reducing the distortion due to lossy compression still remains open. In this paper, we are interested in a cross-layer framework aiming at achieving low Age-of-Information (AoI) and compression distortion concurrently in real-time monitoring over fading channels. More specifically, a cross-layer optimization is formulated to jointly control the lossy compression in the application layer and data transmission in the physical layer. We present a hierarchical solution method by decomposing the cross-layer optimization into inner and outer problems. The objective of the inner problem, solved by convex optimization, is to minimize an age-weighted distortion function and strike the optimal tradeoff between the instantaneous AoI and compression loss. The objective of the outer problem, solved by dimension-reduced Constrained Markov Decision Process (CMDP), is to acquire the optimal scheduling policy reducing the average AoI and distortion. We demonstrate the structural results of the optimal cross-layer design, which substantially reduces the protocol complexity in practice.}
}


@article{DBLP:journals/jsac/RajaramanVR21,
	author = {Nived Rajaraman and
                  Rahul Vaze and
                  Goonwanth Reddy},
	title = {Not Just Age but Age and Quality of Information},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1325--1338},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065061},
	doi = {10.1109/JSAC.2021.3065061},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RajaramanVR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A versatile scheduling problem to model a three-way tradeoff between age of information (AoI), quality/distortion, and energy is considered. The considered problem called the age and quality of information (AQI) is to select which packets to transmit at each time slot to minimize a linear combination of the utility driven by quality, the AoI, and the energy transmission cost in an online fashion. AQI problem combines tradeoffs from some important distinct problems, such as AoI with multiple sources, the remote sampling problem with sampling constraint, the classical speed scaling problem among others. The arbitrary/adversarial case input model is considered in the online setting, where the performance metric is the competitive ratio. A greedy algorithm is proposed that is shown to be 2-competitive, independent of all parameters of the problem. For the special case of AQI problem, a maximum weight matching based algorithm is also shown to be 2-competitive.}
}


@article{DBLP:journals/jsac/GuoH21,
	author = {Daojing Guo and
                  I{-}Hong Hou},
	title = {Scheduling Real-Time Information-Update Flows for the Optimal Confidence
                  in Estimation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1339--1351},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065093},
	doi = {10.1109/JSAC.2021.3065093},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuoH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers a wireless network where multiple flows are delivering status updates about their respective information sources. An end-user aims to make accurate real-time estimations about the status of each information source using its received packets. As the accuracy of estimation is most impacted by events in the recent past, we propose to measure the Confidence-in-Estimation by the number of timely deliveries in a window of the recent past, and say that a flow suffers from a Loss-of-Confidence (LoC) if this number is insufficient for the end user to make a reliable estimation with small confidence intervals. We then study the problem of minimizing the system-wide LoC in wireless networks where each flow has a different requirement and link quality. We show that the problem of minimizing the system-wide LoC requires the control of the temporal variance of timely deliveries for each flow. This feature makes our problem significantly different from other optimization problems that only involve the average of control variables. Surprisingly, we show that there exists a simple online scheduling algorithm that is near-optimal. Simulation results show that our proposed algorithm is significantly better than other state-of-the-art policies. The practical value of this work is further evaluated by a case study of the real-time estimation problem of linear Gaussian processes, where we show that, under the optimal estimate algorithm, our scheduling policy results in better estimate accuracy, both in terms of the average mean square error and 95-percentile of mean square error, than other policies, including one that aims to optimize Age-of-Information, another performance metric for the application of real-time estimation.}
}


@article{DBLP:journals/jsac/KostaPEA21,
	author = {Antzela Kosta and
                  Nikolaos Pappas and
                  Anthony Ephremides and
                  Vangelis Angelakis},
	title = {The Age of Information in a Discrete Time Queue: Stationary Distribution
                  and Non-Linear Age Mean Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1352--1364},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065045},
	doi = {10.1109/JSAC.2021.3065045},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KostaPEA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we investigate information freshness in a status update communication system consisting of a source-destination link. Initially, we study the properties of a sample path of the age of information (AoI) process at the destination. We obtain a general formula of the stationary distribution of the AoI, under the assumption of ergodicity. We relate this result to a discrete time queueing system and provide a general expression of the generating function of AoI in relation with the system time and the peak age of information (PAoI) metric. Furthermore, we consider three different single-server system models and we obtain closed-form expressions of the generating functions and the stationary distributions of the AoI and the PAoI. The first model is a first-come-first-served (FCFS) queue, the second model is a preemptive last-come-first-served (LCFS) queue, and the last model is a bufferless system with packet dropping. We build upon these results to provide a methodology for analyzing general non-linear age functions for this type of systems, using representations of functions as power series.}
}


@article{DBLP:journals/jsac/ChampatiAOG21,
	author = {Jaya Prakash Champati and
                  Ramana R. Avula and
                  Tobias J. Oechtering and
                  James Gross},
	title = {Minimum Achievable Peak Age of Information Under Service Preemptions
                  and Request Delay},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1365--1379},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065047},
	doi = {10.1109/JSAC.2021.3065047},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChampatiAOG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is a growing interest in analysing freshness of data in networked systems. Age of Information (AoI) has emerged as a relevant metric to quantify this freshness at a receiver, and minimizing this metric for different system models has received significant research attention. However, a fundamental question remains: what is the minimum achievable AoI in any single-server-single-source queuing system for a given service-time distribution? We address this question for the average peak AoI (PAoI) statistic by considering generate-at-will source model, service preemptions, and request delays. Our main result is on the characterization of the minimum achievable average PAoI, and we show that it is achieved by a fixed-threshold policy among the set of all causal policies. We use the characterization to provide necessary and sufficient condition for preemptions to be beneficial for a given service-time distribution. Our numerical results, obtained using well-known distributions, demonstrate that the heavier the tail of a distribution the higher the performance gains of using preemptions.}
}


@article{DBLP:journals/jsac/GuWCLV21,
	author = {Yifan Gu and
                  Qian Wang and
                  He Chen and
                  Yonghui Li and
                  Branka Vucetic},
	title = {Optimizing Information Freshness in Two-Hop Status Update Systems
                  Under a Resource Constraint},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1380--1392},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065060},
	doi = {10.1109/JSAC.2021.3065060},
	timestamp = {Mon, 26 Jun 2023 20:51:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuWCLV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the age minimization problem for a two-hop relay system, under a resource constraint on the average number of forwarding operations at the relay. We first design an optimal policy by modelling the considered scheduling problem as a constrained Markov decision process (CMDP) problem. Based on the observed multi-threshold structure of the optimal policy, we then devise a low-complexity double threshold relaying (DTR) policy with only two thresholds, one for relay’s AoI and the other one for the age gain between destination and relay. We derive approximate closed-form expressions of the average AoI at the destination, and the average number of forwarding operations at the relay for the DTR policy, by modelling the tangled evolution of age at relay and destination as a Markov chain (MC). Numerical results validate all the theoretical analysis, and show that the low-complexity DTR policy can achieve near optimal performance compared with the optimal CMDP-based policy. Moreover, the relay should always consider the threshold for its local age to maintain a low age at the destination. When the resource constraint is relatively tight, it further needs to consider the threshold on the age gain to ensure that only those packets that can decrease destination’s age dramatically will be forwarded.}
}


@article{DBLP:journals/jsac/ZhengXFZL21,
	author = {Haina Zheng and
                  Ke Xiong and
                  Pingyi Fan and
                  Zhangdui Zhong and
                  Khaled Ben Letaief},
	title = {Age of Information-Based Wireless Powered Communication Networks With
                  Selfish Charging Nodes},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1393--1411},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065038},
	doi = {10.1109/JSAC.2021.3065038},
	timestamp = {Thu, 29 Apr 2021 15:14:58 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengXFZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a multi-node wireless powered communication network (WPCN), where a hybrid access point (HAP) first charges an Internet of Thing (IoT) device wirelessly with the assistance of multiple selfish wireless nodes (WNs), and then the IoT device uses the harvested energy to transmit real-time status updates to the HAP. Two incentive schemes, i.e., the energy-incentive scheme and the price-incentive scheme, are designed to overcome the selfishness of the WNs and enhance the per-packet AoI performance. For the energy-incentive scheme, an AoI-energy utility function is defined and an optimization problem is formulated to maximize the AoI-energy utility value of the HAP-IoT device pair. By using equality constraint elimination and Lagrangian method, the problem is solved and some closed-form solutions are derived to obtain the optimal solution. For the price-incentive scheme, an AoI-price utility function is defined and a Stackelberg game is established to maximize the utility of the HAP-IoT device pair. By using function transformation and Lagrange method, some semi-closed-form solutions are derived to maximize their own profits of the HAP and WNs in a distributed way. Numerical results show that our proposed two incentive mechanisms are able to achieve higher network utility values than the benchmark scheme. The more the number of WNs, the lower the AoI of each status update packet and the higher utility value of the HAP. It also shows that by positioning WNs closer to the IoT device, the better per-packet AoI performance can be achieved by both incentive mechanisms. Additionally, for the energy-incentive mechanism, its achieved AoI gain and energy gain decrease with the increment of the distance between the HAP and the IoT device. But for the price-incentive mechanism, the opposite phenomenon is observed.}
}


@article{DBLP:journals/jsac/CeranGG21,
	author = {Elif Tug{\c{c}}e Ceran and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Andr{\'{a}}s Gy{\"{o}}rgy},
	title = {A Reinforcement Learning Approach to Age of Information in Multi-User
                  Networks With {HARQ}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1412--1426},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065057},
	doi = {10.1109/JSAC.2021.3065057},
	timestamp = {Sun, 16 May 2021 00:14:23 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CeranGG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scheduling the transmission of time-sensitive information from a source node to multiple users over error-prone communication channels is studied with the goal of minimizing the long-term average age of information (AoI) at the users. A long-term average resource constraint is imposed on the source, which limits the average number of transmissions. The source can transmit only to a single user at each time slot, and after each transmission, it receives an instantaneous ACK/NACK feedback from the intended receiver, and decides when and to which user to transmit the next update. Assuming the channel statistics are known, the optimal scheduling policy is studied for both the standard automatic repeat request (ARQ) and hybrid ARQ (HARQ) protocols. Then, a reinforcement learning (RL) approach is introduced to find a near-optimal policy, which does not assume any a priori information on the random processes governing the channel states. Different RL methods including average-cost SARSA with linear function approximation (LFA), upper confidence reinforcement learning (UCRL2), and deep Q-network (DQN) are applied and compared through numerical simulations.}
}


@article{DBLP:journals/jsac/PanLLLL21,
	author = {Haoyuan Pan and
                  Soung Chang Liew and
                  Jiaxin Liang and
                  Victor C. M. Leung and
                  Jianqiang Li},
	title = {Coding of Multi-Source Information Streams With Age of Information
                  Requirements},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1427--1440},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065079},
	doi = {10.1109/JSAC.2021.3065079},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PanLLLL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This article puts forth a new channel coding paradigm for multi-source information streams with Age of Information (AoI) requirements. The recently introduced AoI metric characterizes the freshness of information, defined as the time elapsed since the generation of the last successfully received update. We study a setup in which a large number of sensors want to send update information to a common monitor with the help of aggregators. Specifically, an aggregator collects update packets from sensors and forwards them to the monitor. Conventional block codes (such as LDPC codes) that encode and decode each update packet separately do not perform well in such an information aggregation and update scenario. When update packets suffer from packet loss, we show that block codes lead to high instantaneous AoI because a sensor waits for a long time for the next update opportunity. This article investigates stream-based codes to tackle this problem. A distinguishing feature of stream-based codes is the joint encoding of update packets from different sensors, and a series of coded packets are sent continuously like a stream. Different update packets are then jointly decoded using multiple coded packets from the stream. A key challenge with AoI requirements is the joint design of error corrections of old packets and fast decodings of new packets. We design a practical encoding-decoding scheme and a sliding decoding window mechanism to control the decoding complexity. We evaluate two AoI metrics, average AoI and bounded AoI. In particular, bounded AoI corresponds to an AoI threshold that the instantaneous AoI is below a large percentage of the time. Experimental results on software-defined radio show that stream-based codes significantly outperform block codes in both average AoI and bounded AoI under varying channel conditions. Overall, stream-based codes provide a viable channel coding solution to multi-source information streams with timely update requirements.}
}


@article{DBLP:journals/jsac/SahaSM21,
	author = {Subham Saha and
                  Vineeth Bala Sukumaran and
                  Chandra R. Murthy},
	title = {On the Minimum Average Age of Information in {IRSA} for Grant-Free
                  mMTC},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1441--1455},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065065},
	doi = {10.1109/JSAC.2021.3065065},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SahaSM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the optimal design of the frame-based irregular repetition slotted ALOHA (IRSA) protocol for minimizing the average age of information (AAoI) in grant-free massive machine-type communications (mMTC). To this end, we first characterize the AAoI as a function of the number of user elements (UEs), the frame duration, and the repetition distribution of IRSA. We present this characterization for IRSA schemes with packet recovery at the end of frame and packet generation either at the beginning of the frame or just in time before first transmission in a frame. We also propose and characterize the AAoI of a novel early packet recovery method which further reduces the average age of information. In all cases, the analysis reveals that, as a function of normalized channel traffic (defined as the ratio of number of UEs to frame duration), the AAoI first decreases linearly due to more frequent updates received from the UEs, and increases sharply beyond a critical point due to packet recovery failures caused by collisions. We then consider the problem of minimizing AAoI by optimizing over the normalized channel traffic and repetition distribution for all the proposed sampling and recovery schemes. The optimization problem is challenging since the objective function is semi-analytical and can only be completely characterized using simulations. In an asymptotic regime where the number of UEs as well as the frame size is large, we characterize the AAoI using upper and lower bounds. We also obtain a locally optimal normalized channel traffic and repetition distribution using differential evolution. Based on the insights obtained from the asymptotic analysis, we also propose a pragmatic approach to obtain a normalized channel traffic and repetition distribution for AAoI reduction in the non-asymptotic case. Finally, we empirically show that our AAoI minimizing schemes outperform conventional throughput optimal schemes.}
}


@article{DBLP:journals/jsac/YavascanU21,
	author = {Orhan Tahir Yavascan and
                  Elif Uysal},
	title = {Analysis of Slotted {ALOHA} With an Age Threshold},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1456--1470},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065043},
	doi = {10.1109/JSAC.2021.3065043},
	timestamp = {Tue, 21 Mar 2023 21:08:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YavascanU21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a comprehensive steady-state analysis of threshold-ALOHA, a distributed age-aware modification of slotted ALOHA proposed in recent literature. In threshold-ALOHA, each terminal suspends its transmissions until the Age of Information (AoI) of the status update flow it is sending reaches a certain threshold Γ. Once the age exceeds Γ, the terminal attempts transmission with constant probability τ in each slot, as in standard slotted ALOHA. We analyze the time-average expected AoI attained by this policy, and explore its scaling with network size, n. We derive the probability distribution of the number of active users at steady state, and show that as network size increases the policy converges to one that runs slotted ALOHA with fewer sources: on average about one fifth of the users is active at any time. We obtain an expression for steady-state expected AoI and use this to optimize the parameters Γ and τ, resolving the conjectures in previous literature by confirming that the optimal age threshold and transmission probability are 2.2n and 4.69/n, respectively. We find that the optimal AoI scales with the network size as 1.4169n, which is almost half the minimum AoI achievable with slotted ALOHA, while the loss from the maximum throughput of e -1 remains below 1%. We compare the performance of this rudimentary algorithm to that of the SAT policy [2] that dynamically adapts its transmission probabilities.}
}


@article{DBLP:journals/jsac/BhatVM21,
	author = {Rajshekhar Vishweshwar Bhat and
                  Rahul Vaze and
                  Mehul Motani},
	title = {Minimization of Age of Information in Fading Multiple Access Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1471--1484},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065048},
	doi = {10.1109/JSAC.2021.3065048},
	timestamp = {Thu, 29 Apr 2021 15:14:59 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BhatVM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Freshness of information is an important requirement in many real-time applications. It is measured by a metric called the age of information (AoI), defined as the time elapsed since the generation of the last successful update received by the destination. We consider M sources (users) updating their statuses to a base station (BS) over a block-fading multiple access channel (MAC). At the start of each fading block, the BS acquires perfect information about channel power gain realizations of all the users in the block. Using this information, a centralized scheduling policy at the BS decides, for each block, which users should transmit and with what powers. The objective is to minimize a long-term weighted average AoI across all users subject to a long-term average power constraint at each user. Under this setting, we first consider a simple time-division multiple access (TDMA) strategy, in which at most one user can transmit in a slot, and propose a simple age-independent stationary randomized policy (AI-SRP). The AI-SRP makes transmission decisions based on the channel power gain realizations, without considering the AoIs. We then consider a more general non-orthogonal multiple access (NOMA) strategy, in which any number of users can transmit in a slot subject to capacity constraints of the MAC and propose an AI-SRP. The AI-SRPs we propose are optimal solutions to appropriate optimization problems. We show that the minimum achievable weighted average AoIs across the users under the proposed AI-SRPs are at most two times those of the respective optimal policies under TDMA and NOMA strategies.}
}


@article{DBLP:journals/jsac/SunWJZN21,
	author = {Jingzhou Sun and
                  Lehan Wang and
                  Zhiyuan Jiang and
                  Sheng Zhou and
                  Zhisheng Niu},
	title = {Age-Optimal Scheduling for Heterogeneous Traffic With Timely Throughput
                  Constraints},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {5},
	pages = {1485--1498},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3065059},
	doi = {10.1109/JSAC.2021.3065059},
	timestamp = {Wed, 22 Dec 2021 17:17:01 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunWJZN21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a base station supporting two types of traffics, i.e., status update traffic and timely throughput traffic. The goal is to improve the information freshness of status update traffic while satisfying timely throughput constraints. Age of Information (AoI) is adopted as a metric for information freshness. We first propose an age-aware policy that makes scheduling decisions based on the current value of AoI directly. Given timely throughput constraint, an upper bound of the weighted average AoI under this policy is provided. To evaluate policy performance, it is important to obtain the minimum weighted average AoI achievable given timely throughput constraint. A low complexity method is proposed to estimate a lower bound of this value. Furthermore, inspired by the estimation procedure, we design an age-oblivious policy that does not rely on the current AoI to make scheduling decisions. Surprisingly, simulation results show that the weighted average AoI of the age-oblivious policy is comparable to that of the age-aware policy, and both are close to the lower bound.}
}


@article{DBLP:journals/jsac/AkyildizKGDB21,
	author = {Ian F. Akyildiz and
                  Tetsuya Kawanishi and
                  Wolfgang H. Gerstacker and
                  Xiaodai Dong and
                  Aydin Babakhani},
	title = {Guest Editorial Special Issue on "THz Communications and Networking"},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1499--1505},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071914},
	doi = {10.1109/JSAC.2021.3071914},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AkyildizKGDB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an ever-increasing demand for higher data traffic volume to be handled in wireless networks. Data-intensive applications such as high-resolution video and 3-D experience are emerging fast, and a variety and a high number of new devices with extended functionalities, sensing capabilities, and intelligence are introduced and adopted. As a consequence, next-generation wireless networks need to be able to provide data rates in the order of Terabit per second (Tb/s) with high reliability and possibly low latency.}
}


@article{DBLP:journals/jsac/LemicATSCAMF21,
	author = {Filip Lemic and
                  Sergi Abadal and
                  Wouter Tavernier and
                  Pieter Stroobant and
                  Didier Colle and
                  Eduard Alarc{\'{o}}n and
                  Johann M{\'{a}}rquez{-}Barja and
                  Jeroen Famaey},
	title = {Survey on Terahertz Nanocommunication and Networking: {A} Top-Down
                  Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1506--1543},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071837},
	doi = {10.1109/JSAC.2021.3071837},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LemicATSCAMF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent developments in nanotechnology herald nanometer-sized devices expected to bring light to a number of groundbreaking applications. Communication with and among nanodevices will be needed for unlocking the full potential of such applications. As the traditional communication approaches cannot be directly applied in nanocommunication, several alternative paradigms have emerged. Among them, electromagnetic nanocommunication in the terahertz (THz) frequency band is particularly promising, mainly due to the breakthrough of novel materials such as graphene. For this reason, numerous research efforts are nowadays targeting THz band nanocommunication and consequently nanonetworking. As it is expected that these trends will continue in the future, we see it beneficial to summarize the current status in these research domains. In this survey, we therefore aim to provide an overview of the current THz nanocommunication and nanonetworking research. Specifically, we discuss the applications envisioned to be supported by nanonetworks operating in the THz band, together with the requirements such applications pose on the underlying nanonetworks. Subsequently, we provide an overview of the current contributions on the different layers of the protocol stack, as well as the available channel models and experimentation tools. Finally, we identify a number of open research challenges and outline several future research directions.}
}


@article{DBLP:journals/jsac/ZhangWP21a,
	author = {Xi Zhang and
                  Jingqing Wang and
                  H. Vincent Poor},
	title = {Optimal Resource Allocations for Statistical QoS Provisioning to Support
                  mURLLC Over FBC-EH-Based 6G THz Wireless Nano-Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1544--1560},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071816},
	doi = {10.1109/JSAC.2021.3071816},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangWP21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of most important techniques for enabling the sixth-generation (6G) mobile wireless network lies in how to efficiently guarantee various stringent quality-of-service (QoS) performance-metrics to support the emerging massive Ultra-Reliable Low-Latency Communications (mURLLC) in 6G. Correspondingly, finite blocklength coding (FBC) has been developed as an effective technique to significantly improve various QoS indices for mURLLC through implementing short-packet communications. On the other hand, Terahertz (THz) band wireless nano-communications have been widely envisioned as a promising 6G technique to efficiently support utra-high data-rate (up to 1 Tbps). One of the major constraints over THz-band nano-networks is the severely limited energy that can be accessed by nano devices. Towards this end, various novel energy harvesting (EH) mechanisms have been proposed to remedy the energy scarcity problem. However, how to accurately characterize the relationships among THz wireless channels, energy consumption, and EH models for FBC based nano communications remains a challenging problem to support statistical delay and error-rate bounded QoS provisioning over FBC based 6G THz wireless nano-networks. To overcome these challenges, in this paper we propose optimal resource allocation policies to achieve the maximum ε-effective capacity in the THz band over FBC-EH-based nano-networks. Particularly, we establish nano-scale system models and characterize wireless channel models in the THz band using FBC. In order to support statistical delay and error-rate bounded QoS provisioning, we formulate and solve the ε-effective capacity maximization problem under several different EH constraints for our proposed schemes. Simulation results are included, which validate and evaluate our proposed schemes in the finite blocklength regime.}
}


@article{DBLP:journals/jsac/JuXKR21,
	author = {Shihao Ju and
                  Yunchou Xing and
                  Ojas Kanhere and
                  Theodore S. Rappaport},
	title = {Millimeter Wave and Sub-Terahertz Spatial Statistical Channel Model
                  for an Indoor Office Building},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1561--1575},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071844},
	doi = {10.1109/JSAC.2021.3071844},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JuXKR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Millimeter-wave (mmWave) and sub-Terahertz (THz) frequencies are expected to play a vital role in 6G wireless systems and beyond due to the vast available bandwidth of many tens of GHz. This paper presents an indoor 3-D spatial statistical channel model for mmWave and sub-THz frequencies based on extensive radio propagation measurements at 28 and 140 GHz conducted in an indoor office environment from 2014 to 2020. Omnidirectional and directional path loss models and channel statistics such as the number of time clusters, cluster delays, and cluster powers were derived from over 15,000 measured power delay profiles. The resulting channel statistics show that the number of time clusters follows a Poisson distribution and the number of subpaths within each cluster follows a composite exponential distribution for both LOS and NLOS environments at 28 and 140 GHz. This paper proposes a unified indoor statistical channel model for mmWave and sub-Terahertz frequencies following the mathematical framework of the previous outdoor NYUSIM channel models. A corresponding indoor channel simulator is developed, which can recreate 3-D omnidirectional, directional, and multiple input multiple output (MIMO) channels for arbitrary mmWave and sub-THz carrier frequency up to 150 GHz, signal bandwidth, and antenna beamwidth. The presented statistical channel model and simulator will guide future air-interface, beamforming, and transceiver designs for 6G and beyond.}
}


@article{DBLP:journals/jsac/WangWHWG21,
	author = {Jue Wang and
                  Cheng{-}Xiang Wang and
                  Jie Huang and
                  Haiming Wang and
                  Xiqi Gao},
	title = {A General 3D Space-Time-Frequency Non-Stationary THz Channel Model
                  for 6G Ultra-Massive {MIMO} Wireless Communication Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1576--1589},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071850},
	doi = {10.1109/JSAC.2021.3071850},
	timestamp = {Mon, 31 Jul 2023 09:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangWHWG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a novel three-dimensional (3D) space-time-frequency (STF) non-stationary geometry-based stochastic model (GBSM) is proposed for the sixth generation (6G) terahertz (THz) wireless communication systems. The proposed THz channel model is very general having the capability to capture different channel characteristics in multiple THz application scenarios such as indoor scenarios, device-to-device (D2D) communications, ultra-massive multiple-input multiple-output (MIMO) communications, and long traveling paths of users. Also, the generality of the proposed channel model is demonstrated by the fact that it can easily be reduced to different simplified channel models to fit specific scenarios by properly adjusting model parameters. The proposed general channel model takes into consideration the non-stationarities in space, time, and frequency domains caused by ultra-massive MIMO, long traveling paths, and large bandwidths of THz communications, respectively. Statistical properties of the proposed general THz channel model are investigated. The accuracy and generality of the proposed channel model are verified by comparing the simulation results of the relative angle spread and root mean square (RMS) delay spread with corresponding channel measurements.}
}


@article{DBLP:journals/jsac/EckhardtPMKK21,
	author = {Johannes M. Eckhardt and
                  Vitaly Petrov and
                  Dmitri Moltchanov and
                  Yevgeni Koucheryavy and
                  Thomas K{\"{u}}rner},
	title = {Channel Measurements and Modeling for Low-Terahertz Band Vehicular
                  Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1590--1603},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071843},
	doi = {10.1109/JSAC.2021.3071843},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/EckhardtPMKK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communications in the low terahertz band (0.1 THz-1 THz) is a promising candidate to enable ultra-high-rate vehicular networks beyond 5G. The successful design and adoption of such systems require a deep understanding of the low THz channel specifics in complex vehicular scenarios. In this paper, a comprehensive measurement campaign is reported with the aim of analyzing the wave propagation at 300 GHz in typical vehicular deployments. Following a modular approach, the generic vehicular scenario is decomposed into basic propagation setups that are further analyzed in detail. The obtained measurement data are then applied to derive the mathematical approximations that characterize the low THz band channel properties for each scenario. Finally, the combination of measurement and modeling results is used to identify the critical propagation effects that has to be accounted for in the applied studies. The presented approach, raw and processed data, as well as the contributed analysis, serve as building blocks for future analytical and simulation tools to model prospective vehicular communication systems in the low THz band.}
}


@article{DBLP:journals/jsac/DovelosMNB21,
	author = {Konstantinos Dovelos and
                  Michail Matthaiou and
                  Hien Quoc Ngo and
                  Boris Bellalta},
	title = {Channel Estimation and Hybrid Combining for Wideband Terahertz Massive
                  {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1604--1620},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071851},
	doi = {10.1109/JSAC.2021.3071851},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DovelosMNB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Terahertz (THz) communication is widely considered as a key enabler for future 6G wireless systems. However, THz links are subject to high propagation losses and inter-symbol interference due to the frequency selectivity of the channel. Massive multiple-input multiple-output (MIMO) along with orthogonal frequency division multiplexing (OFDM) can be used to deal with these problems. Nevertheless, when the propagation delay across the base station (BS) antenna array exceeds the symbol period, the spatial response of the BS array varies over the OFDM subcarriers. This phenomenon, known as beam squint, renders narrowband combining approaches ineffective. Additionally, channel estimation becomes challenging in the absence of combining gain during the training stage. In this work, we address the channel estimation and hybrid combining problems in wideband THz massive MIMO with uniform planar arrays. Specifically, we first introduce a low-complexity beam squint mitigation scheme based on true-time-delay. Next, we propose a novel variant of the popular orthogonal matching pursuit (OMP) algorithm to accurately estimate the channel with low training overhead. Our channel estimation and hybrid combining schemes are analyzed both theoretically and numerically. Moreover, the proposed schemes are extended to the multi-antenna user case. Simulation results are provided showcasing the performance gains offered by our design compared to standard narrowband combining and OMP-based channel estimation.}
}


@article{DBLP:journals/jsac/ShaW21,
	author = {Ziyuan Sha and
                  Zhaocheng Wang},
	title = {Channel Estimation and Equalization for Terahertz Receiver With {RF}
                  Impairments},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1621--1635},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071824},
	doi = {10.1109/JSAC.2021.3071824},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShaW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The radio frequency (RF) impairments of analog devices have been regarded as an important factor degrading the performance of Terahertz (THz) communications, where in-phase/quadrature (IQ) imbalance and phase noise (PN) are the two typical RF impairments at THz transceiver. In this paper, we investigate the channel estimation (CE) and equalization for THz receiver in the presence of wideband IQ imbalance and PN, where single-carrier frequency-domain equalization is used. Since PN is inserted between channel impulse response (CIR) and wideband IQ imbalance at the receiver, the CIR and IQ imbalance cannot be treated together as an effective channel. Therefore, a novel two-stage CE method is derived to separately estimate the CIR and wideband IQ imbalance parameters, and its corresponding channel equalization method is designed to compensate both wideband IQ imbalance and PN, and equalizes the CIR. Theoretical and simulation results validate the efficiency of our proposed CE, and the proposed equalization method is shown to outperform the state-of-the-art methods.}
}


@article{DBLP:journals/jsac/BouhlelSB21,
	author = {Nizar Bouhlel and
                  Majed Saad and
                  Faouzi Bader},
	title = {Sub-Terahertz Wireless System Using Dual-Polarized Generalized Spatial
                  Modulation With {RF} Impairments},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1636--1650},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071828},
	doi = {10.1109/JSAC.2021.3071828},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BouhlelSB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we address a multiple-input multiple-output (MIMO) communication system based on generalized spatial modulation (GSM) and dual-polarized (DP) antennas in sub-TeraHertz (sub-THz) bands to improve the Spectral Efficiency (SE) while reducing the spatial correlation effect and the space occupancy. Moreover, the joint Maximum Likelihood (ML) detector and a Modified Ordered Block Minimum Mean-Squared Error (MOB-MMSE) detector algorithm are proposed to detect jointly the complex symbols and the virtual bits conveyed by the activated polarization and the transmit antenna combination index. MOB-MMSE can achieve near-ML performance with low complexity. The performance of DP-GSM system over correlated Rayleigh/Rician fading and indoor sub-THz channels is studied. Provided results show that DP-GSM system is robust to the spatial correlation effects under Rician fading channel. In addition, an average bit-error probability (ABEP) upper bounding is derived and analyzed for the DP-GSM system over correlated Rayleigh/Rician fading channels. Since the sub-THz band suffers from many technological limitations and severe RF-impairments such as low transmit output power and important phase noise (PN), the DP-GSM system is studied by considering the sub-THz impairments. Moreover, the Dual Polarized Spatial Multiplexing (DP-SMX) and the Uni-Polarized GSM (UP-GSM) are compared with the DP-GSM system. The results reveal that DP-GSM with low order modulation schemes as QPSK outperforms the UP-GSM. Besides, DP-GSM and DP-SMX provide a good performance up to medium PN, which is interesting for sub-THz bands, but DP-GSM can reduce the transceiver cost by using a lower number of RF chains when the used transmitter architecture is based on RF-switching.}
}


@article{DBLP:journals/jsac/MaoW21,
	author = {Tianqi Mao and
                  Zhaocheng Wang},
	title = {Terahertz Wireless Communications With Flexible Index Modulation Aided
                  Pilot Design},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1651--1662},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071841},
	doi = {10.1109/JSAC.2021.3071841},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MaoW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Terahertz (THz) wireless communication is envisioned as a promising technology, which is capable of providing ultra-high-rate transmission up to Terabit per second. However, some hardware imperfections, which are generally neglected in the existing literature concerning lower data rates and traditional operating frequencies, cannot be overlooked in the THz systems. Hardware imperfections usually consist of phase noise, in-phase/quadrature imbalance, and nonlinearity of power amplifier. Due to the time-variant characteristic of phase noise, frequent pilot insertion is required, leading to decreased spectral efficiency. In this paper, to address this issue, a novel pilot design strategy is proposed based on index modulation (IM), where the positions of pilots are flexibly changed in the data frame, and additional information bits can be conveyed by indices of pilots. Furthermore, a turbo receiving algorithm is developed, which jointly performs the detection of pilot indices and channel estimation in an iterative manner. It is shown that the proposed turbo receiver works well even under the situation where the prior knowledge of channel state information is outdated. Analytical and simulation results validate that the proposed schemes achieve significant enhancement of bit-error rate performance and channel estimation accuracy, whilst attaining higher spectral efficiency in comparison with its classical counterpart.}
}


@article{DBLP:journals/jsac/HuangYAXWYZD21,
	author = {Chongwen Huang and
                  Zhaohui Yang and
                  George C. Alexandropoulos and
                  Kai Xiong and
                  Li Wei and
                  Chau Yuen and
                  Zhaoyang Zhang and
                  M{\'{e}}rouane Debbah},
	title = {Multi-Hop RIS-Empowered Terahertz Communications: {A} DRL-Based Hybrid
                  Beamforming Design},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1663--1677},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071836},
	doi = {10.1109/JSAC.2021.3071836},
	timestamp = {Tue, 22 Jun 2021 11:43:26 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuangYAXWYZD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communication in the TeraHertz band (0.1-10 THz) is envisioned as one of the key enabling technologies for the future sixth generation (6G) wireless communication systems scaled up beyond massive multiple input multiple output (Massive-MIMO) technology. However, very high propagation attenuations and molecular absorptions of THz frequencies often limit the signal transmission distance and coverage range. Benefited from the recent breakthrough on the reconfigurable intelligent surfaces (RIS) for realizing smart radio propagation environment, we propose a novel hybrid beamforming scheme for the multi-hop RIS-assisted communication networks to improve the coverage range at THz-band frequencies. Particularly, multiple passive and controllable RISs are deployed to assist the transmissions between the base station (BS) and multiple single-antenna users. We investigate the joint design of digital beamforming matrix at the BS and analog beamforming matrices at the RISs, by leveraging the recent advances in deep reinforcement learning (DRL) to combat the propagation loss. To improve the convergence of the proposed DRL-based algorithm, two algorithms are then designed to initialize the digital beamforming and the analog beamforming matrices utilizing the alternating optimization technique. Simulation results show that our proposed scheme is able to improve 50% more coverage range of THz communications compared with the benchmarks. Furthermore, it is also shown that our proposed DRL-based method is a state-of-the-art method to solve the NP-hard beamforming problem, especially when the signals at RIS-assisted THz communication networks experience multiple hops.}
}


@article{DBLP:journals/jsac/ZhaiTPW21,
	author = {Bangzhao Zhai and
                  Aimin Tang and
                  Chen Peng and
                  Xudong Wang},
	title = {{SS-OFDMA:} Spatial-Spread Orthogonal Frequency Division Multiple
                  Access for Terahertz Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1678--1692},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071818},
	doi = {10.1109/JSAC.2021.3071818},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaiTPW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to achieve efficient multiple access for Terahertz (THz) networks is still an open problem. The key obstacle is the extremely narrow angular coverage by a highly directional THz beam. Thus, a hybrid THzPrism beamforming (HTB) architecture is designed in this paper to greatly enlarge angular coverage by spreading frequency subcarriers to different directions. Based on the HTB architecture, a spatial-spread orthogonal frequency division multiple access (SS-OFDMA) scheme is developed. To serve users dispersed in a large angular range, a user grouping mechanism is first designed for SS-OFDMA to utilize SDMA and suppress the inter-group interference. To improve the spectrum and energy efficiency for sporadic users within a group, a non-uniform beam spreading mechanism is then developed for SS-OFDMA by joint design of digital and analog beamforming. Finally, a resource allocation algorithm is designed to minimize the transmit power of the base station by optimizing the subarray allocation among groups as well as the subcarrier and power allocation for each user within a group. Compared with the existing schemes, SS-OFDMA increases the achievable data rate by up to 124%, reduces the power consumption by up to 71%, and increases the average number of concurrently served users by up to 147%.}
}


@article{DBLP:journals/jsac/TanD21,
	author = {Jingbo Tan and
                  Linglong Dai},
	title = {Wideband Beam Tracking in THz Massive {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1693--1710},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071817},
	doi = {10.1109/JSAC.2021.3071817},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TanD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Terahertz (THz) massive multiple-input multiple-output (MIMO) has been considered as one of the promising technologies for future 6G wireless communications. It is essential to obtain channel information by beam tracking scheme to track mobile users in THz massive MIMO systems. However, the existing beam tracking schemes designed for narrowband systems with the traditional hybrid precoding structure suffer from a severe performance loss caused by the beam split effect, and thus cannot be directly applied to wideband THz massive MIMO systems. To solve this problem, in this paper we propose a beam zooming based beam tracking scheme by considering the recently proposed delay-phase precoding structure for THz massive MIMO. Specifically, we firstly prove the beam zooming mechanism to flexibly control the angular coverage of frequency-dependent beams over the whole bandwidth, i.e., the degree of the beam split effect, which can be realized by the elaborate design of time delays in the delay-phase precoding structure. Then, based on this beam zooming mechanism, we propose to track multiple user physical directions simultaneously in each time slot by generating multiple beams. The angular coverage of these beams is flexibly zoomed to adapt to the potential variation range of the user physical direction. After several time slots, the base station is able to obtain the exact user physical direction by finding out the beam with the largest user received power. Unlike traditional schemes where only one frequency-independent beam can be usually generated by one radio-frequency chain, the proposed beam zooming based beam tracking scheme can simultaneously track multiple user physical directions by using multiple frequency-dependent beams generated by one radio-frequency chain. Theoretical analysis shows that the proposed scheme can achieve the near-optimal achievable sum-rate performance with low beam training overhead, which is also verified by extensive simulation results.}
}


@article{DBLP:journals/jsac/ShahAR21,
	author = {Syed Hashim Ali Shah and
                  Sundar Aditya and
                  Sundeep Rangan},
	title = {Power-Efficient Beam Tracking During Connected Mode {DRX} in mmWave
                  and Sub-THz Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1711--1724},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071791},
	doi = {10.1109/JSAC.2021.3071791},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShahAR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discontinuous reception (DRX), wherein a user equipment (UE) temporarily disables its receiver, is a critical power saving feature in modern cellular systems. DRX is likely to be aggressively used at mmWave and sub-THz frequencies due to the high front-end power consumption. A key challenge for DRX at these frequencies is blockage-induced link outages: a UE will likely need to track many directional links to ensure reliable multi-connectivity, thereby increasing the power consumption. In this paper, we explore bandit algorithms for link tracking in connected mode DRX that reduce power consumption by tracking only a fraction of the available links, but without adversely affecting the outage and throughput performance. Through detailed, system level simulations at 28 GHz (5G) and 140 GHz (6G), we observe that even sub-optimal link tracking policies can achieve considerable power savings with relatively little degradation in outage and throughput performance, especially with digital beamforming at the UE. In particular, we show that it is feasible to reduce power consumption by 75% and still achieve up to 95% (80%) of the maximum throughput using digital beamforming at 28 GHz (140 GHz), subject to an outage probability of at most 1%.}
}


@article{DBLP:journals/jsac/GaoWXAL21,
	author = {Feifei Gao and
                  Bolei Wang and
                  Chengwen Xing and
                  Jianping An and
                  Geoffrey Ye Li},
	title = {Wideband Beamforming for Hybrid Massive {MIMO} Terahertz Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1725--1740},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071822},
	doi = {10.1109/JSAC.2021.3071822},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GaoWXAL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The combination of large bandwidth at terahertz (THz) and the large number of antennas in massive MIMO results in the non-negligible spatial wideband effect in time domain or the corresponding beam squint issue in frequency domain, which will cause severe performance degradation if not properly treated. In particular, for a phased array based hybrid transceiver, there exists a contradiction between the requirement of mitigating the beam squint issue and the hardware implementation of the analog beamformer/combiner, which makes the accurate beamforming an enormous challenge. In this paper, we propose two wideband hybrid beamforming approaches, based on the virtual sub-array and the true-time-delay (TTD) lines, respectively, to eliminate the impact of beam squint. The former one divides the whole array into several virtual sub-arrays to generate a wider beam and provides an evenly distributed array gain across the whole operating frequency band. To further enhance the beamforming performance and thoroughly address the aforementioned contradiction, the latter one introduces the TTD lines and propose a new hardware implementation of analog beamformer/combiner. This TTD-aided hybrid implementation enables the wideband beamforming and achieves the near-optimal performance close to full-digital transceivers. Analytical and numerical results demonstrate the effectiveness of two proposed wideband beamforming approaches.}
}


@article{DBLP:journals/jsac/LiaoGWWYNA21,
	author = {Anwen Liao and
                  Zhen Gao and
                  Dongming Wang and
                  Hua Wang and
                  Hao Yin and
                  Derrick Wing Kwan Ng and
                  Mohamed{-}Slim Alouini},
	title = {Terahertz Ultra-Massive MIMO-Based Aeronautical Communications in
                  Space-Air-Ground Integrated Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1741--1767},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071834},
	doi = {10.1109/JSAC.2021.3071834},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiaoGWWYNA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging space-air-ground integrated network has attracted intensive research and necessitates reliable and efficient aeronautical communications. This paper investigates terahertz Ultra-Massive (UM)-MIMO-based aeronautical communications and proposes an effective channel estimation and tracking scheme, which can solve the performance degradation problem caused by the unique triple delay-beam-Doppler squint effects of aeronautical terahertz UM-MIMO channels. Specifically, based on the rough angle estimates acquired from navigation information, an initial aeronautical link is established, where the delay-beam squint at transceiver can be significantly mitigated by employing a Grouping True-Time Delay Unit (GTTDU) module (e.g., the designed Rotman lens -based GTTDU module). According to the proposed prior-aided iterative angle estimation algorithm, azimuth/elevation angles can be estimated, and these angles are adopted to achieve precise beam-alignment and refine GTTDU module for further eliminating delay-beam squint. Doppler shifts can be subsequently estimated using the proposed prior-aided iterative Doppler shift estimation algorithm. On this basis, path delays and channel gains can be estimated accurately, where the Doppler squint can be effectively attenuated via compensation process. For data transmission, a data-aided decision-directed based channel tracking algorithm is developed to track the beam-aligned effective channels. When the data-aided channel tracking is invalid, angles will be re-estimated at the pilot-aided channel tracking stage with an equivalent sparse digital array, where angle ambiguity can be resolved based on the previously estimated angles. The simulation results and the derived Cramér-Rao lower bounds verify the effectiveness of our solution.}
}


@article{DBLP:journals/jsac/SaeedGBA21,
	author = {Akhtar Saeed and
                  {\"{O}}zg{\"{u}}r G{\"{u}}rb{\"{u}}z and
                  Ahmet Ozan Bicen and
                  Mustafa Alper Akkas},
	title = {Variable-Bandwidth Model and Capacity Analysis for Aerial Communications
                  in the Terahertz Band},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1768--1784},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071831},
	doi = {10.1109/JSAC.2021.3071831},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SaeedGBA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, 0.75-10 THz band is explored for non-terrestrial communications, by leveraging the improved atmospheric conditions at various altitudes. A channel model for aerial communications at THz band is proposed to calculate the common flat bands for frequency-selective path gain and the colored noise spectrums, both of which are highly affected by the atmospheric conditions. With capacity computation based on common flat bands, we consider aerial vehicles at different altitudes and distances, under realistic weather and channel fading conditions, due to beam misalignment and also multi path fading. An extensive capacity analysis is presented, considering equal and water-filling power allocation. It is shown that, when there is no fading, capacity for aerial links is several orders of magnitude larger than the sea-level capacity. When ergodic capacity is computed for the fading scenarios, it is shown that the impact of fading vanishes with altitude. Sea-level ergodic capacity is increased by an order of magnitude for drone-to-drone communications, providing several Tbps at 10 m range, while 10s of Tbps is achievable among jet planes and UAVs, and several 100s of Tbps is possible for satellites/cubesats at 1 km under fading, suggesting that THz band is a promising alternative for aerial communications.}
}


@article{DBLP:journals/jsac/YiKSKKJKL21,
	author = {Changhwan Yi and
                  Dongkyo Kim and
                  Sourabh Solanki and
                  Jae{-}Hong Kwon and
                  Moonil Kim and
                  Sanggeun Jeon and
                  Young{-}Chai Ko and
                  Inkyu Lee},
	title = {Design and Performance Analysis of THz Wireless Communication Systems
                  for Chip-to-Chip and Personal Area Networks Applications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1785--1796},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071849},
	doi = {10.1109/JSAC.2021.3071849},
	timestamp = {Tue, 01 Jun 2021 08:36:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YiKSKKJKL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Terahertz (THz) communication is a promising technique for chip-to-chip communication and wireless personal area networks. In this paper, we present an experimental study and design to realize such THz communication systems. We develop two different chip sets for on-off-keying (OOK) modulation based THz transceivers which include carrier generators, modulators, THz amplifiers, and baseband amplifiers. Specifically, the first chip set integrates the circuit blocks for the OOK modulation without the THz amplifier for short-range communication. In addition, the second chip set design includes the THz amplifier modules to extend the coverage of transmission. For these two chip sets, we experimentally demonstrate the feasibility of the wireless communication at THz frequency bands and assess performance using the bit error rate (BER) analysis. We estimate the BER by calculating the signal-to-noise ratio (SNR) based on the eye diagram and compare with actual BER measurements and Monte Carlo simulations. We also address the impact of the distance, the transmit power, and the data rate for the proposed THz transceivers based on the link budget analysis, and confirm the accuracy of the derived BER expression.}
}


@article{DBLP:journals/jsac/AmlashiKSXCT21,
	author = {Salman Behboudi Amlashi and
                  Mohsen Khalily and
                  Vikrant Singh and
                  Pei Xiao and
                  David J. Carey and
                  Rahim Tafazolli},
	title = {Surface Electromagnetic Performance Analysis of a Graphene-Based Terahertz
                  Sensor Using a Novel Spectroscopy Technique},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1797--1816},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071835},
	doi = {10.1109/JSAC.2021.3071835},
	timestamp = {Wed, 07 Dec 2022 23:03:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AmlashiKSXCT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a novel terahertz (THz) spectroscopy technique and a new graphene-based sensor is proposed. The proposed sensor consists of a graphene-based metasurface (MS) that operates in reflection mode over a broad range of frequency band (0.2 - 6 THz) and can detect relative permittivity of up to 4 with a resolution of 0.1 and a thickness ranging from 5 μm to 600 μm with a resolution of 0.5 μm. To the best of author's knowledge, such a THz sensor with such capabilities has not been reported yet. Additionally, an equivalent circuit of the novel unit cell is derived and compared with two conventional grooved structures to showcase the superiority of the proposed unit cell. The proposed spectroscopy technique utilizes some unique spectral features of a broadband reflection wave including Accumulated Spectral power (ASP) and Averaged Group Delay (AGD), which are independent to resonance frequencies and can operate over a broad range of spectrum. ASP and AGD can be combined to analyse the magnitude and phase of the reflection diagram as a coherent technique for sensing purposes. This enables the capability to distinguish between different analytes with high precision which, to the best of author's knowledge, has been accomplished for the first time.}
}


@article{DBLP:journals/jsac/ShafieYDZHJ21,
	author = {Akram Shafie and
                  Nan Yang and
                  Salman Durrani and
                  Xiangyun Zhou and
                  Chong Han and
                  Markku J. Juntti},
	title = {Coverage Analysis for 3D Terahertz Communication Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1817--1832},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071840},
	doi = {10.1109/JSAC.2021.3071840},
	timestamp = {Tue, 01 Jun 2021 08:36:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShafieYDZHJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We conduct novel coverage probability analysis of downlink transmission in a three-dimensional (3D) terahertz (THz) communication (THzCom) system. In this system, we address the unique propagation properties in THz band, e.g., absorption loss, super-narrow directional beams, and high vulnerability towards blockage, which are fundamentally different from those at lower frequencies. Different from existing studies, we characterize the performance while considering the effect of 3D directional antennas at both access points (APs) and user equipments (UEs), and the joint impact of the blockage caused by the user itself, moving humans, and wall blockers in a 3D environment. Under such consideration, we develop a tractable analytical framework to derive a new expression for the coverage probability by examining the regions where dominant interferers (i.e., those can cause outage by themselves) can exist, and the average number of interferers existing in these regions. Aided by numerical results, we validate our analysis and reveal that ignoring the impact of the vertical heights of THz devices in the analysis leads to a substantial underestimation of the coverage probability. We also show that it is more worthwhile to increase the antenna directivity at the APs than at the UEs, to produce a more reliable THzCom system.}
}


@article{DBLP:journals/jsac/OzkocKKLP21,
	author = {Mustafa F. Ozkoc and
                  Athanasios Koutsaftis and
                  Rajeev Kumar and
                  Pei Liu and
                  Shivendra S. Panwar},
	title = {The Impact of Multi-Connectivity and Handover Constraints on Millimeter
                  Wave and Terahertz Cellular Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {6},
	pages = {1833--1853},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3071852},
	doi = {10.1109/JSAC.2021.3071852},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/OzkocKKLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communication over terahertz (THz) frequency bands is envisioned as the key enabler of many applications and services offered in 6G networks. The abundantly available bandwidth in THz frequencies can satisfy the ultra-high user throughput requirements and accommodate a massive number of connected devices. However, poor propagation characteristics, shadowing, and blockages may result in sudden outages and necessitate frequent handovers. Therefore, an inefficient handover procedure will impose severe challenges in meeting the ultra-high reliability and low latency requirements of emerging applications. In blockage driven mmWave and THz networks, a higher multi-connectivity degree and efficient handover procedures are needed to reduce the data plane interruptions and to achieve high reliability. We present an analytical model to study the impact of handover procedures and multi-connectivity degree on the latency and reliability of blockage driven wireless networks. From the network protocol design perspective, our study offers a quick and accurate way to envisage how network architecture and protocols should evolve in terms of multi-connectivity degrees and handover procedural efficiency. Our results suggest that, for THz systems, coverage range should be increased even if it comes at the cost of increased initial access and base station discovery times.}
}


@article{DBLP:journals/jsac/LiSOKQHHGE21a,
	author = {Geoffrey Ye Li and
                  Walid Saad and
                  Ayfer {\"{O}}zg{\"{u}}r and
                  Peter Kairouz and
                  Zhijin Qin and
                  Jakob Hoydis and
                  Zhu Han and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Jaafar Mohamed Hashim Elmirghani},
	title = {Series Editorial: The Second Issue of the Series on Machine Learning
                  in Communications and Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1855--1857},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078790},
	doi = {10.1109/JSAC.2021.3078790},
	timestamp = {Wed, 05 Jan 2022 14:30:57 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiSOKQHHGE21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Second Call for Papers of the Series on Machine Learning in Communications and Networks has continued to receive a great number of high-quality papers covering various aspects of intelligent communication systems. In addition to 23 original contributions in response to the first call for papers, we include in this issue 5 articles submitted to the second call for papers. In the following, we provide a brief review of key contributions of papers in this issue according to their topics.}
}


@article{DBLP:journals/jsac/MaggiVH21,
	author = {Lorenzo Maggi and
                  Alvaro Valcarce and
                  Jakob Hoydis},
	title = {Bayesian Optimization for Radio Resource Management: Open Loop Power
                  Control},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1858--1871},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078490},
	doi = {10.1109/JSAC.2021.3078490},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MaggiVH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We provide the reader with an accessible yet rigorous introduction to Bayesian optimisation with Gaussian processes (BOGP) for the purpose of solving a wide variety of radio resource management (RRM) problems. We believe that BOGP is a powerful tool that has been somewhat overlooked in RRM research, although it elegantly addresses pressing requirements for fast convergence, safe exploration, and interpretability. BOGP also provides a natural way to exploit prior knowledge during optimization. After explaining the nuts and bolts of BOGP, we delve into more advanced topics, such as the choice of the acquisition function and the optimization of dynamic performance functions. Finally, we put the theory into practice for the RRM problem of uplink open-loop power control (OLPC) in 5G cellular networks, for which BOGP is able to converge to almost optimal solutions in tens of iterations without significant performance drops during exploration.}
}


@article{DBLP:journals/jsac/GuoWJ21,
	author = {Jiajia Guo and
                  Chao{-}Kai Wen and
                  Shi Jin},
	title = {Deep Learning-Based {CSI} Feedback for Beamforming in Single- and
                  Multi-Cell Massive {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1872--1884},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041397},
	doi = {10.1109/JSAC.2020.3041397},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuoWJ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The potentials of massive multiple-input multiple-output (MIMO) are all based on the available instantaneous channel state information (CSI) at the base station (BS). Therefore, the user in frequency-division duplexing (FDD) systems has to keep on feeding back the CSI to the BS, thereby occupying large uplink transmission resources. Recently, deep learning (DL) has achieved great success in the CSI feedback. However, the existing works just focus on improving the feedback accuracy and ignore the effects on the following modules, e.g., beamforming (BF). In this paper, we propose a DL-based CSI feedback framework for BF design, called CsiFBnet. The key idea of the CsiFBnet is to maximize the BF performance gain rather than the feedback accuracy. We apply it to two representative scenarios: single- and multi-cell systems. The CsiFBnet-s in the single-cell system is based on the autoencoder architecture, where the encoder at the user compresses the CSI and the decoder at the BS generates the BF vector. The CsiFBnet-m in the multi-cell system has to feed back two kinds of CSI: the desired and the interfering CSI. The entire neural networks are trained by an unsupervised learning strategy. Simulation results show the great performance improvement and complexity reduction of the CsiFBnet compared with the conventional DL-based CSI feedback methods.}
}


@article{DBLP:journals/jsac/YangGXAA21,
	author = {Yuwen Yang and
                  Feifei Gao and
                  Chengwen Xing and
                  Jianping An and
                  Ahmed Alkhateeb},
	title = {Deep Multimodal Learning: Merging Sensory Data for Massive {MIMO}
                  Channel Prediction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1885--1898},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041383},
	doi = {10.1109/JSAC.2020.3041383},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangGXAA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing work in intelligent communications has recently made preliminary attempts to utilize multi-source sensing information (MSI) to improve the system performance. However, the research on MSI aided intelligent communications has not yet explored how to integrate and fuse the multimodal sensory data, which motivates us to develop a systematic framework for wireless communications based on deep multimodal learning (DML). In this paper, we first present complete descriptions and heuristic understandings on the framework of DML based wireless communications, where core design choices are analyzed in the view of communications. Then, we develop several DML based architectures for channel prediction in massive multiple-input multiple-output (MIMO) systems that leverage various modality combinations and fusion levels. The case study of massive MIMO channel prediction offers an important example that can be followed in developing other DML based communication technologies. Simulation results demonstrate that the proposed DML framework can effectively exploit the constructive and complementary information of multimodal sensory data to assist the current wireless communications.}
}


@article{DBLP:journals/jsac/ShaoCQZZ21,
	author = {Xiaodan Shao and
                  Xiaoming Chen and
                  Yiyang Qiang and
                  Caijun Zhong and
                  Zhaoyang Zhang},
	title = {Feature-Aided Adaptive-Tuning Deep Learning for Massive Device Detection},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1899--1914},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078500},
	doi = {10.1109/JSAC.2021.3078500},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShaoCQZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing development of Internet of Things (IoT), the upcoming sixth-generation (6G) wireless network is required to support grant-free random access of a massive number of sporadic traffic devices. In particular, at the beginning of each time slot, the base station (BS) performs joint activity detection and channel estimation (JADCE) based on the received pilot sequences sent from active devices. Due to the deployment of a large-scale antenna array and the existence of a massive number of IoT devices, conventional JADCE approaches usually have high computational complexity and need long pilot sequences. To solve these challenges, this paper proposes a novel deep learning framework for JADCE in 6G wireless networks, which contains a dimension reduction module, a deep learning network module, an active device detection module, and a channel estimation module. Then, prior-feature learning followed by an adaptive-tuning strategy is proposed, where an inner network composed of the Expectation-maximization (EM) and back-propagation is introduced to jointly tune the precision and learn the distribution parameters of the device state matrix. Finally, by designing the inner layer-by-layer and outer layer-by-layer training method, a feature-aided adaptive-tuning deep learning network is built. Both theoretical analysis and simulation results confirm that the proposed deep learning framework has low computational complexity and needs short pilot sequences in practical scenarios.}
}


@article{DBLP:journals/jsac/WuYZWYG21,
	author = {Chi Wu and
                  Xinping Yi and
                  Yiming Zhu and
                  Wenjin Wang and
                  Li You and
                  Xiqi Gao},
	title = {Channel Prediction in High-Mobility Massive {MIMO:} From Spatio-Temporal
                  Autoregression to Deep Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1915--1930},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078503},
	doi = {10.1109/JSAC.2021.3078503},
	timestamp = {Mon, 31 Jul 2023 09:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuYZWYG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While massive multiple-input multiple-output (MIMO) has achieved tremendous success in both theory and practice, it faces a crisis of sharp performance degradation in moderate or high-mobility scenarios (e.g., 30 km/h), due to the breach of uplink-downlink channel duality. Such a “curse of mobility” has spurred the research on channel prediction in high-mobility scenarios. Instead of predicting channel response matrix in the space-frequency domain, we investigate it in the angle-delay domain by utilizing the high angle-delay resolution of wideband massive MIMO systems. Specifically, we study the general angle-delay domain channel characterization and obtain that: 1) the correlations between the angle-delay domain channel response matrix (ADCRM) elements are decoupled significantly; 2) when the number of antennas and bandwidth are limited, the decoupling is insufficient and residual correlations between the neighboring ADCRM elements exist. Then focusing on the ADCRM, we propose two channel prediction methods: a spatio-temporal autoregressive (ST-AR) model-driven unsupervised-learning method and a deep learning (DL) based data-driven supervised-learning method. While the model-driven method provides a principled way for channel prediction, the data-driven method is generalizable to various channel scenarios. In particular, ST-AR exploits the residual spatio-temporal correlations of the channel element with its most neighboring elements, and DL realizes element-wise angle-delay domain channel prediction utilizing a complex-valued neural network (CVNN). Simulation results under the 3GPP non-line-of-sight (NLOS) scenarios indicate that, compared to the state-of-the-art Prony-based angular-delay domain (PAD) prediction method, both the proposed ST-AR and the CVNN-based channel prediction methods can enhance the channel prediction accuracy.}
}


@article{DBLP:journals/jsac/JiangCY21,
	author = {Tao Jiang and
                  Hei Victor Cheng and
                  Wei Yu},
	title = {Learning to Reflect and to Beamform for Intelligent Reflecting Surface
                  With Implicit Channel Estimation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1931--1945},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078502},
	doi = {10.1109/JSAC.2021.3078502},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JiangCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS), which consists of a large number of tunable reflective elements, is capable of enhancing the wireless propagation environment in a cellular network by intelligently reflecting the electromagnetic waves from the base-station (BS) toward the users. The optimal tuning of the phase shifters at the IRS is, however, a challenging problem, because due to the passive nature of reflective elements, it is difficult to directly measure the channels between the IRS, the BS, and the users. Instead of following the traditional paradigm of first estimating the channels then optimizing the system parameters, this paper advocates a machine learning approach capable of directly optimizing both the beamformers at the BS and the reflective coefficients at the IRS based on a system objective. This is achieved by using a deep neural network to parameterize the mapping from the received pilots (plus any additional information, such as the user locations) to an optimized system configuration, and by adopting a permutation invariant/equivariant graph neural network (GNN) architecture to capture the interactions among the different users in the cellular network. Simulation results show that the proposed implicit channel estimation based approach is generalizable, can be interpreted, and can efficiently learn to maximize a sum-rate or minimum-rate objective from a much fewer number of pilots than the traditional explicit channel estimation based approaches.}
}


@article{DBLP:journals/jsac/RenMBL21,
	author = {Zhenwen Ren and
                  Mithun Mukherjee and
                  Mehdi Bennis and
                  Jaime Lloret},
	title = {Multikernel Clustering via Non-Negative Matrix Factorization Tailored
                  Graph Tensor Over Distributed Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1946--1956},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041396},
	doi = {10.1109/JSAC.2020.3041396},
	timestamp = {Mon, 28 Aug 2023 21:38:11 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RenMBL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next-generation wireless networks are witnessing an increasing number of clustering applications, and produce a large amount of non-linear and unlabeled data. In some degree, single kernel methods face the challenging problem of kernel choice. To overcome this problem for non-linear data clustering, multiple kernel graph-based clustering (MKGC) has attracted intense attention in recent years. However, existing MKGC methods suffer from two common problems: (1) they mainly aim to learn a consensus kernel from multiple candidate kernels, slight affinity graph learning, such that cannot fully exploit the underlying graph structure of non-linear data; (2) they disregard the high-order correlations between all base kernels, which cannot fully capture the consistent and complementary information of all kernels. In this paper, we propose a novel non-negative matrix factorization (NMF) tailored graph tensor MKGC method for non-linear data clustering, namely TMKGC. Specifically, TMKGC integrates NMF and graph learning together in kernel space so as to learn multiple candidate affinity graphs. Afterwards, the high-order structure information of all candidate graphs is captured in a 3-order tensor kernel space by introducing tensor singular value decomposition based tensor nuclear norm, such that an optimal affinity graph can be obtained subsequently. Based on the alternating direction method of multipliers, the effective local and distributed solvers are elaborated to solve the proposed objective function. Extensive experiments have demonstrated the superiority of TMKGC compared to the state-of-the-art MKGC methods.}
}


@article{DBLP:journals/jsac/BuchbergerHPSA21,
	author = {Andreas Buchberger and
                  Christian H{\"{a}}ger and
                  Henry D. Pfister and
                  Laurent Schmalen and
                  Alexandre Graell i Amat},
	title = {Pruning and Quantizing Neural Belief Propagation Decoders},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1957--1966},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041392},
	doi = {10.1109/JSAC.2020.3041392},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BuchbergerHPSA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider near maximum-likelihood (ML) decoding of short linear block codes. In particular, we propose a novel decoding approach based on neural belief propagation (NBP) decoding recently introduced by Nachmani et al. in which we allow a different parity-check matrix in each iteration of the algorithm. The key idea is to consider NBP decoding over an overcomplete parity-check matrix and use the weights of NBP as a measure of the importance of the check nodes (CNs) to decoding. The unimportant CNs are then pruned. In contrast to NBP, which performs decoding on a given fixed parity-check matrix, the proposed pruning-based neural belief propagation (PB-NBP) typically results in a different parity-check matrix in each iteration. For a given complexity in terms of CN evaluations, we show that PB-NBP yields significant performance improvements with respect to NBP. We apply the proposed decoder to the decoding of a Reed-Muller code, a short low-density parity-check (LDPC) code, and a polar code. PB-NBP outperforms NBP decoding over an overcomplete parity-check matrix by 0.27-0.31 dB while reducing the number of required CN evaluations by up to 97%. For the LDPC code, PB-NBP outperforms conventional belief propagation with the same number of CN evaluations by 0.52 dB. We further extend the pruning concept to offset min-sum decoding and introduce a pruning-based neural offset min-sum (PB-NOMS) decoder, for which we jointly optimize the offsets and the quantization of the messages and offsets. We demonstrate performance 0.5 dB from ML decoding with 5-bit quantization for the Reed-Muller code.}
}


@article{DBLP:journals/jsac/ZhengLS21,
	author = {Simeng Zheng and
                  Yi Liu and
                  Paul H. Siegel},
	title = {{PR-NN:} RNN-Based Detection for Coded Partial-Response Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1967--1982},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041377},
	doi = {10.1109/JSAC.2020.3041377},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the use of recurrent neural network (RNN)-based detection of magnetic recording channels with inter-symbol interference (ISI). We refer to the proposed detection method, which is intended for recording channels with partial-response equalization, as Partial-Response Neural Network (PR-NN). We train bi-directional gated recurrent units (bi-GRUs) to recover the ISI channel inputs from noisy channel output sequences and evaluate the network performance when applied to continuous, streaming data. The computational complexity of PR-NN during the evaluation process is comparable to that of a Viterbi detector. The recording system on which the experiments were conducted uses a rate-2/3, (1,7) runlength-limited (RLL) code with an E 2 PR4 partial-response channel target. Experimental results with ideal PR signals show that the performance of PR-NN detection approaches that of Viterbi detection in additive white gaussian noise (AWGN). Moreover, the PR-NN detector outperforms Viterbi detection and achieves the performance of Noise-Predictive Maximum Likelihood (NPML) detection in additive colored noise (ACN) at different channel densities. A PR-NN detector trained with both AWGN and ACN maintains the performance observed under separate training. Similarly, when trained with ACN corresponding to two different channel densities, PR-NN maintains its performance at both densities. Experiments confirm that this robustness is consistent over a wide range of signal-to-noise ratios (SNRs). Finally, PR-NN displays robust performance when applied to a more realistic magnetic recording channel with MMSE-equalized Lorentzian signals.}
}


@article{DBLP:journals/jsac/DaiTSNCPC21,
	author = {Jincheng Dai and
                  Kailin Tan and
                  Zhongwei Si and
                  Kai Niu and
                  Mingzhe Chen and
                  H. Vincent Poor and
                  Shuguang Cui},
	title = {Learning to Decode Protograph {LDPC} Codes},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {1983--1999},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078488},
	doi = {10.1109/JSAC.2021.3078488},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DaiTSNCPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent development of deep learning methods provides a new approach to optimize the belief propagation (BP) decoding of linear codes. However, the limitation of existing works is that the scale of neural networks increases rapidly with the codelength, thus they can only support short to moderate codelengths. From the point view of practicality, we propose a high-performance neural min-sum (MS) decoding method that makes full use of the lifting structure of protograph low-density parity-check (LDPC) codes. By this means, the size of the parameter array of each layer in the neural decoder only equals the number of edge-types for arbitrary codelengths. In particular, for protograph LDPC codes, the proposed neural MS decoder is constructed in a special way such that identical parameters are shared by a bundle of edges derived from the same edge-type. To reduce the complexity and overcome the vanishing gradient problem in training the proposed neural MS decoder, an iteration-by-iteration (i.e., layer-by-layer in neural networks) greedy training method is proposed. With this, the proposed neural MS decoder tends to be optimized with faster convergence, which is aligned with the early termination mechanism widely used in practice. To further enhance the generalization ability of the proposed neural MS decoder, a codelength/rate compatible training method is proposed, which randomly selects samples from a set of codes lifted from the same base code. As a theoretical performance evaluation tool, a trajectory-based extrinsic information transfer (T-EXIT) chart is developed for various decoders. Both T-EXIT and simulation results show that the optimized MS decoding can provide faster convergence and up to 1dB gain compared with the plain MS decoding and its variants with only slightly increased complexity. In addition, it can even outperform the sum-product algorithm for some short codes.}
}


@article{DBLP:journals/jsac/SaiduttaAF21,
	author = {Yashas Malur Saidutta and
                  Afshin Abdi and
                  Faramarz Fekri},
	title = {Joint Source-Channel Coding Over Additive Noise Analog Channels Using
                  Mixture of Variational Autoencoders},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2000--2013},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078489},
	doi = {10.1109/JSAC.2021.3078489},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SaiduttaAF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a learning scheme for Joint Source-Channel Coding (JSCC) over analog independent additive noise channels. We formulate the learning problem by showing that the minimization loss function from rate-distortion theory, is upper bounded by the loss function of the Variational Autoencoder (VAE). We show that when the source dimension is greater than the channel dimension, the encoding of two source samples in the neighborhood of each other need not be near each other. Such discontinuous projection needs to be accounted for by using multiple encoders and selecting an encoder to encode samples on a particular side of the discontinuity. We explore two selection methodologies, one based on an intuitive rule and the other where it is posed as a learning task in a Mixture-of-Experts (MoE) setup. We analyze the gradients of these methods and reason why the latter is better at avoiding local optima. We show the efficacy of the proposed methodology by simulating the performance of the system for JSCC of Gaussian sources over AWGN channels and showing that the learned solutions are close to or better than the ones proposed earlier. The proposed methodology is also naturally capable of generalizing to other source distributions which we showcase by simulating for Laplace sources. The learned systems are also robust to changes in channel conditions. Further, a single system can be trained to generalize over a range of channel conditions provided the channel conditions are known at both the transmitter and the receiver. Finally, we evaluate our proposed methodology on three different image datasets and showcase consistent improvement over existing methods due to the VAE formulation.}
}


@article{DBLP:journals/jsac/GuSHLMEV21,
	author = {Zhouyou Gu and
                  Changyang She and
                  Wibowo Hardjawana and
                  Simon Lumb and
                  David McKechnie and
                  Todd Essery and
                  Branka Vucetic},
	title = {Knowledge-Assisted Deep Reinforcement Learning in 5G Scheduler Design:
                  From Theoretical Framework to Implementation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2014--2028},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078498},
	doi = {10.1109/JSAC.2021.3078498},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuSHLMEV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we develop a knowledge-assisted deep reinforcement learning (DRL) algorithm to design wireless schedulers in the fifth-generation (5G) cellular networks with time-sensitive traffic. Since the scheduling policy is a deterministic mapping from channel and queue states to scheduling actions, it can be optimized by using deep deterministic policy gradient (DDPG). We show that a straightforward implementation of DDPG converges slowly, has a poor quality-of-service (QoS) performance, and cannot be implemented in real-world 5G systems, which are non-stationary in general. To address these issues, we propose a theoretical DRL framework, where theoretical models from wireless communications are used to formulate a Markov decision process in DRL. To reduce the convergence time and improve the QoS of each user, we design a knowledge-assisted DDPG (K-DDPG) that exploits expert knowledge of the scheduler design problem, such as the knowledge of the QoS, the target scheduling policy, and the importance of each training sample, determined by the approximation error of the value function and the number of packet losses. Furthermore, we develop an architecture for online training and inference, where K-DDPG initializes the scheduler off-line and then fine-tunes the scheduler online to handle the mismatch between off-line simulations and non-stationary real-world systems. Simulation results show that our approach reduces the convergence time of DDPG significantly and achieves better QoS than existing schedulers (reducing 30% ~ 50% packet losses). Experimental results show that with off-line initialization, our approach achieves better initial QoS than random initialization and the online fine-tuning converges in few minutes.}
}


@article{DBLP:journals/jsac/LiuSZK21,
	author = {Jiajia Liu and
                  Zhenjiang Shi and
                  Shangwei Zhang and
                  Nei Kato},
	title = {Distributed Q-Learning Aided Uplink Grant-Free {NOMA} for Massive
                  Machine-Type Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2029--2041},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078496},
	doi = {10.1109/JSAC.2021.3078496},
	timestamp = {Tue, 21 Mar 2023 21:08:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuSZK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The explosive growth of machine-type communications (MTC) devices poses critical challenges to the existing cellular networks. Therefore, how to support massive MTC devices with limited resources is an urgent problem to be solved. Bursty traffic is an important characteristic of MTC devices, which makes it difficult for agents to learn useful experience and has a negative impact on model convergence. However, most existing reinforcement learning-based literatures assume that devices have saturate data. Towards this end, we propose two distributed Q-learning aided uplink grant-free non-orthogonal multiple access (NOMA) schemes (including all-devices distributed Q-learning (ADDQ) scheme and portion-devices distributed Q-learning (PDDQ) scheme) to maximize the number of accessible devices, where the bursty traffic of massive MTC devices is carefully considered. In order to reduce the dimension of scheduling space and mitigate the impact of bursty traffic, the idea of grouping devices as well as transmission resources and the intermittent learning mode are adopted in our schemes. Extensive numerical results demonstrate the advantages of proposed schemes from multiple perspectives.}
}


@article{DBLP:journals/jsac/LiuLC21,
	author = {Xiao Liu and
                  Yuanwei Liu and
                  Yue Chen},
	title = {Machine Learning Empowered Trajectory and Passive Beamforming Design
                  in {UAV-RIS} Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2042--2055},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041401},
	doi = {10.1109/JSAC.2020.3041401},
	timestamp = {Thu, 02 Dec 2021 17:27:17 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A novel framework is proposed for integrating reconfigurable intelligent surfaces (RIS) in unmanned aerial vehicle (UAV) enabled wireless networks, where an RIS is deployed for enhancing the service quality of the UAV. Non-orthogonal multiple access (NOMA) technique is invoked to further improve the spectrum efficiency of the network, while mobile users (MUs) are considered as roaming continuously. The energy consumption minimizing problem is formulated by jointly designing the movement of the UAV, phase shifts of the RIS, power allocation policy from the UAV to MUs, as well as determining the dynamic decoding order. A decaying deep Q-network (D-DQN) based algorithm is proposed for tackling this pertinent problem. In the proposed D-DQN based algorithm, the central controller is selected as an agent for periodically observing the state of UAV-enabled wireless network and for carrying out actions to adapt to the dynamic environment. In contrast to the conventional DQN algorithm, the decaying learning rate is leveraged in the proposed D-DQN based algorithm for attaining a tradeoff between accelerating training speed and converging to the local optimal. Numerical results demonstrate that: 1) In contrast to the conventional Q-learning algorithm, which cannot converge when being adopted for solving the formulated problem, the proposed D-DQN based algorithm is capable of converging with minor constraints; 2) The energy dissipation of the UAV can be significantly reduced by integrating RISs in UAV-enabled wireless networks; 3) By designing the dynamic decoding order and power allocation policy, the RIS-NOMA case consumes 11.7% less energy than the RIS-OMA case.}
}


@article{DBLP:journals/jsac/PhanNB21,
	author = {Trung V. Phan and
                  Tri Gia Nguyen and
                  Thomas Bauschert},
	title = {DeepMatch: Fine-Grained Traffic Flow Measurement in {SDN} With Deep
                  Dueling Neural Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2056--2075},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041406},
	doi = {10.1109/JSAC.2020.3041406},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PhanNB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel flow rule matching framework, DeepMatch, in Software-Defined Networking (SDN) to provide a fine-grained traffic flow measurement capability. Specifically, the flow rule matching control at a particular SDN switch is examined to maximize the traffic flow granularity degree while proactively protecting the flow-table in the switch from being overflowed. This control process is supervised by a control module referred to as DeepMatch instance. Regarding this instance, an optimization problem is formulated based on a Markov decision process (MDP) and a Partially Observable Markov decision process (POMDP), respectively. We develop a deep dueling neural network based flow rule matching control algorithm to solve the optimization problem, thereby quickly attaining a significant traffic flow granularity level and eliminating the switch flow-table overflow problem. Furthermore, we propose an experience data sharing (EDS) mechanism that enables a new instance to learn faster about the flow rule matching control. The results of our performance evaluation show that, by applying the DeepMatch framework in a highly dynamic traffic scenario, the traffic flow granularity degree at the access and the core switches increases by 24.0% and 31.63%, respectively, compared to the FlowStat method. DeepMatch is also highly outperforming the ReWiFlow, SDN-Mon, and Exact-Match approaches. In addition, by employing the EDS mechanism, a new instance can reduce its learning time up to 46.42% for supervising an access switch and up to 37.50% for supervising a core switch.}
}


@article{DBLP:journals/jsac/WuCZLSZL21,
	author = {Wen Wu and
                  Nan Chen and
                  Conghao Zhou and
                  Mushu Li and
                  Xuemin Shen and
                  Weihua Zhuang and
                  Xu Li},
	title = {Dynamic {RAN} Slicing for Service-Oriented Vehicular Networks via
                  Constrained Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2076--2089},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041405},
	doi = {10.1109/JSAC.2020.3041405},
	timestamp = {Fri, 13 Aug 2021 14:56:50 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuCZLSZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate a radio access network (RAN) slicing problem for Internet of vehicles (IoV) services with different quality of service (QoS) requirements, in which multiple logically-isolated slices are constructed on a common roadside network infrastructure. A dynamic RAN slicing framework is presented to dynamically allocate radio spectrum and computing resource, and distribute computation workloads for the slices. To obtain an optimal RAN slicing policy for accommodating the spatial-temporal dynamics of vehicle traffic density, we first formulate a constrained RAN slicing problem with the objective to minimize long-term system cost. This problem cannot be directly solved by traditional reinforcement learning (RL) algorithms due to complicated coupled constraints among decisions. Therefore, we decouple the problem into a resource allocation subproblem and a workload distribution subproblem, and propose a two-layer constrained RL algorithm, named R esource A llocation and W orkload di S tribution (RAWS) to solve them. Specifically, an outer layer first makes the resource allocation decision via an RL algorithm, and then an inner layer makes the workload distribution decision via an optimization subroutine. Extensive trace-driven simulations show that the RAWS effectively reduces the system cost while satisfying QoS requirements with a high probability, as compared with benchmarks.}
}


@article{DBLP:journals/jsac/DingFDH21,
	author = {Ningning Ding and
                  Zhixuan Fang and
                  Lingjie Duan and
                  Jianwei Huang},
	title = {Optimal Incentive and Load Design for Distributed Coded Machine Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2090--2104},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078494},
	doi = {10.1109/JSAC.2021.3078494},
	timestamp = {Mon, 05 Feb 2024 20:23:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DingFDH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A distributed machine learning platform needs to recruit many heterogeneous worker nodes to finish computation simultaneously. As a result, the overall performance may be degraded due to straggling workers. By introducing redundancy into computation, coded machine learning can effectively improve the runtime performance by recovering the final computation result through the first k (out of the total n) workers who finish computation. While existing studies focus on designing efficient coding schemes, the issue of designing proper incentives to encourage worker participation is still under-explored. This paper studies the platform's optimal incentive mechanism for motivating proper workers' participation in coded machine learning, despite the multi-dimensional incomplete information about heterogeneous workers' computation performances and costs. A key contribution of this work is to summarize workers' multi-dimensional heterogeneity as a one-dimensional metric, which guides the platform's efficient selection of workers under incomplete information with a linear computation complexity. Although the exact overall runtime is intractable, we characterize the platform's (asymptotically) optimal load assignment to heterogeneous workers in coded machine learning. When the platform has incomplete information about workers' costs, it is optimal to assign loads only based on workers' computation performances; when the platform further lacks workers' computation performance information, it is optimal to design the loads to be cost-dependent and performance-dependent.}
}


@article{DBLP:journals/jsac/ChoLX21,
	author = {Myung Cho and
                  Lifeng Lai and
                  Weiyu Xu},
	title = {Distributed Dual Coordinate Ascent in General Tree Networks and Communication
                  Network Effect on Synchronous Machine Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2105--2119},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078495},
	doi = {10.1109/JSAC.2021.3078495},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChoLX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the big size of data and limited data storage volume of a single computer or a single server, data are often stored in a distributed manner. Thus, performing large-scale machine learning operations with the distributed datasets through communication networks is often required. In this paper, we study the convergence rate of the distributed dual coordinate ascent for distributed machine learning problems in a general tree-structured network. Since a tree network model can be understood as the generalization of a star network, our algorithm can be thought of as the generalization of the distributed dual coordinate ascent in a star network. We provide the convergence rate of the distributed dual coordinate ascent over a general tree network in a recursive manner and analyze the network effect on the convergence rate. Secondly, by considering network communication delays, we optimize the distributed dual coordinate ascent algorithm to maximize its convergence speed. From our analytical result, we can choose the optimal number of local iterations depending on the communication delay severity to achieve the fastest convergence speed. In numerical experiments, we consider machine learning scenarios over communication networks, where local workers cannot directly reach to a central node due to constraints in communication, and demonstrate that the usability of our distributed dual coordinate ascent algorithm in tree networks.}
}


@article{DBLP:journals/jsac/SongK21,
	author = {Jaeyoung Song and
                  Marios Kountouris},
	title = {Wireless Distributed Edge Learning: How Many Edge Devices Do We Need?},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2120--2134},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041379},
	doi = {10.1109/JSAC.2020.3041379},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SongK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider distributed machine learning at the wireless edge, where a parameter server builds a global model with the help of multiple wireless edge devices that perform computations on local dataset partitions. Edge devices transmit the result of their computations (updates of current global model) to the server using a fixed rate and orthogonal multiple access over an error prone wireless channel. In case of a transmission error, the undelivered packet is retransmitted until successfully decoded at the receiver. Leveraging on the fundamental tradeoff between computation and communication in distributed systems, our aim is to derive how many edge devices are needed to minimize the average completion time while guaranteeing convergence. We provide upper and lower bounds for the average completion and we find a necessary condition for adding edge devices in two asymptotic regimes, namely the large dataset and the high accuracy regime. Conducted experiments on real datasets and numerical results confirm our analysis and substantiate our claim that the number of edge devices should be carefully selected for timely distributed edge learning.}
}


@article{DBLP:journals/jsac/ChenWCYZ21,
	author = {Qi Chen and
                  Wei Wang and
                  Wei Chen and
                  F. Richard Yu and
                  Zhaoyang Zhang},
	title = {Cache-Enabled Multicast Content Pushing With Structured Deep Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2135--2149},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078493},
	doi = {10.1109/JSAC.2021.3078493},
	timestamp = {Tue, 07 May 2024 20:20:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenWCYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The cache-enabled multicast content pushing, which multicasts the content items to multiple users and caches them until requested, is a promising technique to alleviate the heavy network load by enhancing the traffic offloading. This, in turn, has called for the optimization of content pushing strategy while considering both the transmission and caching resources, which jointly result in the complicated coupling among pushing decisions and lead to high computational complexity. Unlike most existing approaches which simplify the pushing problem via bypassing the complicated coupling, in this paper, we propose a multicast content pushing strategy to maximize the offloaded traffic with the cost on content caching based on structured deep learning. Specifically, we design the convolution stage to extract the spatio-temporal correlations of one content item between different pushing decisions, and construct the fully-connected stage to capture the spatial coupling among the decisions of pushing different content items to different user devices. Moreover, to address the absence of the ground truth on multicast content pushing, we relax the transmission constraint to derive a performance upper bound for guiding the training direction. This relaxed problem is solved based on dynamic programming in a bottom-up manner. Compared to the state-of-the-art baselines including both the traditional model-based and the general neural network-based strategies, the proposed pushing strategy achieves significant performance gain in both the random-generated dataset and the real LastFM dataset. In addition, it is also shown that the proposed strategy is robust to the uncertainty of user request information.}
}


@article{DBLP:journals/jsac/ZhengSC21,
	author = {Sihui Zheng and
                  Cong Shen and
                  Xiang Chen},
	title = {Design and Analysis of Uplink and Downlink Communications for Federated
                  Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2150--2167},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041388},
	doi = {10.1109/JSAC.2020.3041388},
	timestamp = {Tue, 27 Sep 2022 14:40:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication has been known to be one of the primary bottlenecks of federated learning (FL), and yet existing studies have not addressed the efficient communication design, particularly in wireless FL where both uplink and downlink communications have to be considered. In this paper, we focus on the design and analysis of physical layer quantization and transmission methods for wireless FL. We answer the question of what and how to communicate between clients and the parameter server and evaluate the impact of the various quantization and transmission options of the updated model on the learning performance. We provide new convergence analysis of the well-known FED AVG under non-i.i.d. dataset distributions, partial clients participation, and finite-precision quantization in uplink and downlink communications. These analyses reveal that, in order to achieve an O (1/T) convergence rate with quantization, transmitting the weight requires increasing the quantization level at a logarithmic rate, while transmitting the weight differential can keep a constant quantization level. Comprehensive numerical evaluation on various real-world datasets reveals that the benefit of a FL-tailored uplink and downlink communication design is enormous - a carefully designed quantization and transmission achieves more than 98% of the floating-point baseline accuracy with fewer than 10% of the baseline bandwidth, for majority of the experiments on both i.i.d. and non-i.i.d. datasets. In particular, 1-bit quantization (3.1% of the floating-point baseline bandwidth) achieves 99.8% of the floating-point baseline accuracy at almost the same convergence rate on MNIST, representing the best known bandwidth-accuracy tradeoff to the best of the authors' knowledge.}
}


@article{DBLP:journals/jsac/SoGA21,
	author = {Jinhyun So and
                  Basak G{\"{u}}ler and
                  Amir Salman Avestimehr},
	title = {Byzantine-Resilient Secure Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2168--2181},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041404},
	doi = {10.1109/JSAC.2020.3041404},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SoGA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure federated learning is a privacy-preserving framework to improve machine learning models by training over large volumes of data collected by mobile users. This is achieved through an iterative process where, at each iteration, users update a global model using their local datasets. Each user then masks its local update via random keys, and the masked models are aggregated at a central server to compute the global model for the next iteration. As the local updates are protected by random masks, the server cannot observe their true values. This presents a major challenge for the resilience of the model against adversarial (Byzantine) users, who can manipulate the global model by modifying their local updates or datasets. Towards addressing this challenge, this paper presents the first single-server Byzantine-resilient secure aggregation framework (BREA) for secure federated learning. BREA is based on an integrated stochastic quantization, verifiable outlier detection, and secure model aggregation approach to guarantee Byzantine-resilience, privacy, and convergence simultaneously. We provide theoretical convergence and privacy guarantees and characterize the fundamental trade-offs in terms of the network size, user dropouts, and privacy protection. Our experiments demonstrate convergence in the presence of Byzantine users, and comparable accuracy to conventional federated learning benchmarks.}
}


@article{DBLP:journals/jsac/HuZBRHS21,
	author = {Jingzhi Hu and
                  Hongliang Zhang and
                  Kaigui Bian and
                  Marco Di Renzo and
                  Zhu Han and
                  Lingyang Song},
	title = {MetaSensing: Intelligent Metasurface Assisted {RF} 3D Sensing by Deep
                  Reinforcement Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2182--2197},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078492},
	doi = {10.1109/JSAC.2021.3078492},
	timestamp = {Mon, 12 Feb 2024 16:07:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuZBRHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Using RF signals for wireless sensing has gained increasing attention. However, due to the unwanted multi-path fading in uncontrollable radio environments, the accuracy of RF sensing is limited. Instead of passively adapting to the environment, in this paper, we consider the scenario where an intelligent metasurface is deployed for sensing the existence and locations of 3D objects. By programming its beamformer patterns, the metasurface can provide desirable propagation properties. However, achieving a high sensing accuracy is challenging, since it requires the joint optimization of the beamformer patterns and mapping of the received signals to the sensed outcome. To tackle this challenge, we formulate an optimization problem for minimizing the cross-entropy loss of the sensing outcome, and propose a deep reinforcement learning algorithm to jointly compute the optimal beamformer patterns and the mapping of the received signals. Simulation results verify the effectiveness of the proposed algorithm and show how the size of the metasurface and the target space influence the sensing accuracy.}
}


@article{DBLP:journals/jsac/TroiaSVM21,
	author = {Sebastian Troia and
                  Federico Sapienza and
                  Leonardo Var{\'{e}} and
                  Guido Maier},
	title = {On Deep Reinforcement Learning for Traffic Engineering in {SD-WAN}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2198--2212},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2020.3041385},
	doi = {10.1109/JSAC.2020.3041385},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TroiaSVM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The demand for reliable and efficient Wide Area Networks (WANs) from business customers is continuously increasing. Companies and enterprises use WANs to exchange critical data between headquarters, far-off business branches and cloud data centers. Many WANs solutions have been proposed over the years, such as: leased lines, Frame Relay, Multi-Protocol Label Switching (MPLS), Virtual Private Networks (VPN). Each solution positions differently in the trade-off between reliability, Quality of Service (QoS) and cost. Today, the emerging technology for WAN is Software-Defined Wide Area Networking (SD-WAN) that introduces the Software-Defined Networking (SDN) paradigm into the enterprise-network market. SD-WAN can support differentiated services over public WAN by dynamically reconfiguring in real-time network devices at the edge of the network according to network measurements and service requirements. On the one hand, SD-WAN reduces the high costs of guaranteed QoS WAN solutions (as MPLS), without giving away reliability in practical scenarios. On the other, it brings numerous technical challenges, such as the implementation of Traffic Engineering (TE) methods. TE is critically important for enterprises not only to efficiently orchestrate network traffic among the edge devices, but also to keep their services always available. In this work, we develop different kind of TE algorithms with the aim of improving the performance of an SD-WAN based network in terms of service availability. We first evaluate the performance of baseline TE algorithms. Then, we implement different deep Reinforcement Learning (deep-RL) algorithms to overcome the limitations of the baseline approaches. Specifically, we implement three kinds of deep-RL algorithms, which are: policy gradient, TD- λ and deep Q-learning. Results show that a deep-RL algorithm with a well-designed reward function is capable of increasing the overall network availability and guaranteeing network protection and restoration in SD-WAN.}
}


@article{DBLP:journals/jsac/WangLJH21,
	author = {Huandong Wang and
                  Yong Li and
                  Depeng Jin and
                  Zhu Han},
	title = {Attentional Markov Model for Human Mobility Prediction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2213--2225},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078499},
	doi = {10.1109/JSAC.2021.3078499},
	timestamp = {Wed, 05 Jan 2022 14:30:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangLJH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate human mobility prediction is important for many applications in wireless networks, including intelligent content caching and prefetching, network optimization, etc. However, modeling mobility patterns has been a challenging problem due to the complicated human mobility patterns influenced by the long-term correlation with historical trajectories and context information, and the long time interval between consecutive mobility records. In this paper, we integrate the novel attention technique into the Markov model to predict future locations. This model allows us to consider the time context of users' mobility behavior, which overcomes the disadvantage of the traditional Markov model in modeling the periodicity of trajectories. We conducted extensive evaluations using two different mobility datasets, which involve over 20,000 users. Our evaluations show that our proposed solution outperforms the state-of-the-art algorithms by over 6.6% on average. In addition, compared with the method based on deep recurrent neural networks achieving the same performance, our proposed model runs significantly faster, i.e., reducing the computation time by over 170 times, demonstrating the effeteness of our proposed model.}
}


@article{DBLP:journals/jsac/FanWHW21,
	author = {Shukai Fan and
                  Yongzhi Wu and
                  Chong Han and
                  Xudong Wang},
	title = {{SIABR:} {A} Structured Intra-Attention Bidirectional Recurrent Deep
                  Learning Method for Ultra-Accurate Terahertz Indoor Localization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2226--2240},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078491},
	doi = {10.1109/JSAC.2021.3078491},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/FanWHW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-accuracy localization technology has gained increasing attention in gesture and motion control and many diverse applications. Due to multi-path fading and blockage effects in indoor propagation, 0.1m-level precise localization is still challenging. Promising for 6G wireless communications, the Terahertz (THz) spectrum provides multi-GHz ultra-broad bandwidth. Applying the THz spectrum to indoor localization, the channel state information (CSI) of THz signals, including angle of arrival (AoA), received power, and delay, has unprecedented resolution that can be explored for positioning. In this paper, a Structured Intra-Attention Bidirectional Recurrent (SIABR) deep learning method is proposed to solve the CSI-based three-dimensional (3D) THz indoor localization problem with significantly improved accuracy. As a two-level structure, the features of individual multi-path rays are first analyzed in the recurrent neural network with the attention mechanism at the lower level. Furthermore, the upper-level residual network (ResNet) of the constructed SIABR network extracts hidden information to output the geometric coordinates. Simulation results demonstrate that the 3D localization accuracy in the metric of mean distance error is within 0.25m. The developed SIABR network has very fast convergence and is robust against THz indoor line-of-sight blockage, multi-path fading, channel sparsity and CSI estimation error.}
}


@article{DBLP:journals/jsac/AddadDTF21,
	author = {Rami Akrem Addad and
                  Diego Leonel Cadette Dutra and
                  Tarik Taleb and
                  Hannu Flinck},
	title = {Toward Using Reinforcement Learning for Trigger Selection in Network
                  Slice Mobility},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2241--2253},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078501},
	doi = {10.1109/JSAC.2021.3078501},
	timestamp = {Sun, 25 Jul 2021 11:42:58 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AddadDTF21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent 5G trials have demonstrated the usefulness of the Network Slicing concept that delivers customizable services to new and under-serviced industry sectors. However, user mobility's impact on the optimal resource allocation within and between slices deserves more attention. Slices and their dedicated resources should be offered where the services are to be consumed to minimize network latency and associated overheads and costs. Different mobility patterns lead to different resource re-allocation triggers, leading eventually to slice mobility when enough resources are to be migrated. The selection of the proper triggers for resource re-allocation and related slice mobility patterns is challenging due to triggers' multiplicity and overlapping nature. In this paper, we investigate the applicability of two Deep Reinforcement Learning based algorithms for allowing a fine-grained selection of mobility triggers that may instantiate slice and resource mobility actions. While the first proposed algorithm relies on a value-based learning method, the second one exploits a hybrid approach to optimize the action selection process. We present an enhanced ETSI Network Function Virtualization edge computing architecture that incorporates the studied mechanisms to implement service and slice migration. We evaluate the proposed methods' efficiency in a simulated environment and compare their performance in terms of training stability, learning time, and scalability. Finally, we identify and quantify the applicability aspects of the respective approaches.}
}


@article{DBLP:journals/jsac/LiRT21,
	author = {Zhida Li and
                  Ana Laura Gonzalez Rios and
                  Ljiljana Trajkovic},
	title = {Machine Learning for Detecting Anomalies and Intrusions in Communication
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {7},
	pages = {2254--2264},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3078497},
	doi = {10.1109/JSAC.2021.3078497},
	timestamp = {Tue, 13 Jul 2021 13:27:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiRT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cyber attacks are becoming more sophisticated and, hence, more difficult to detect. Using efficient and effective machine learning techniques to detect network anomalies and intrusions is an important aspect of cyber security. A variety of machine learning models have been employed to help detect malicious intentions of network users. In this paper, we evaluate performance of recurrent neural networks (Long Short-Term Memory and Gated Recurrent Unit) and Broad Learning System with its extensions to classify known network intrusions. We propose two BLS-based algorithms with and without incremental learning. The algorithms may be used to develop generalized models by using various subsets of input data and expanding the network structure. The models are trained and tested using Border Gateway Protocol routing records as well as network connection records from the NSL-KDD and Canadian Institute of Cybersecurity datasets. Performance of the models is evaluated based on selected features, accuracy, F-Score, and training time.}
}


@article{DBLP:journals/jsac/LiSOKQHHGE21b,
	author = {Geoffrey Ye Li and
                  Walid Saad and
                  Ayfer {\"{O}}zg{\"{u}}r and
                  Peter Kairouz and
                  Zhijin Qin and
                  Jakob Hoydis and
                  Zhu Han and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Jaafar M. H. Elmirghani},
	title = {Series Editorial: The Third Issue of the Series on Machine Learning
                  in Communications and Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2267--2270},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087366},
	doi = {10.1109/JSAC.2021.3087366},
	timestamp = {Wed, 05 Jan 2022 14:30:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiSOKQHHGE21b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Second Call for Papers of the Series on Machine Learning in Communications and Networks has continued to receive a great number of high-quality papers covering various aspects of intelligent communication systems. In addition to those already published, we include in this issue 27 articles that have been submitted to the call. In the following, we provide a brief review of key contributions of papers in this issue according to their topics.}
}


@article{DBLP:journals/jsac/WangTHJLWCC21,
	author = {Jinghe Wang and
                  Wankai Tang and
                  Yu Han and
                  Shi Jin and
                  Xiao Li and
                  Chao{-}Kai Wen and
                  Qiang Cheng and
                  Tie Jun Cui},
	title = {Interplay Between {RIS} and {AI} in Wireless Communications: Fundamentals,
                  Architectures, Applications, and Open Research Problems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2271--2288},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087259},
	doi = {10.1109/JSAC.2021.3087259},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangTHJLWCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future wireless communication networks are expected to fulfill the unprecedented performance requirements to support our highly digitized and globally data-driven society. Various technological challenges must be overcome to achieve our goal. Among many potential technologies, reconfigurable intelligent surface (RIS) and artificial intelligence (AI) have attracted extensive attention, thereby leading to a proliferation of studies for utilizing them in wireless communication systems. The RIS-based wireless communication frameworks and AI-enabled technologies, two of the promising technologies for the sixth-generation networks, interact and promote with each other, striving to collaboratively create a controllable, intelligent, reconfigurable, and programmable wireless propagation environment. This paper explores the road to implementing the combination of RIS and AI, more specifically, integrating AI-enabled technologies into RIS-based frameworks for maximizing the practicality of RIS to facilitate the realization of smart radio propagation environments, elaborated from shallow to deep insights. We begin with the basic concept and fundamental characteristics of RIS, followed by the overview of the research status of RIS. Then, we analyze the inevitable trend of RIS to be combined with AI. In particular, we focus on recent research about RIS-based architectures embedded with AI, elucidating from the intelligent structures and systems of metamaterials to the AI-embedded RIS-assisted wireless communication systems. Finally, the challenges and potential of the topic are discussed.}
}


@article{DBLP:journals/jsac/HuLCYD21,
	author = {Qiyu Hu and
                  Yanzhen Liu and
                  Yunlong Cai and
                  Guanding Yu and
                  Zhi Ding},
	title = {Joint Deep Reinforcement Learning and Unfolding: Beam Selection and
                  Precoding for mmWave Multiuser {MIMO} With Lens Arrays},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2289--2304},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087233},
	doi = {10.1109/JSAC.2021.3087233},
	timestamp = {Thu, 29 Jul 2021 13:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuLCYD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The millimeter wave (mmWave) multiuser multiple-input multiple-output (MU-MIMO) systems with discrete lens arrays (DLA) have received great attention due to their simple hardware implementation and excellent performance. In this work, we investigate the joint design of beam selection and digital precoding matrices for mmWave MU-MIMO systems with DLA to maximize the sum-rate subject to the transmit power constraint and the constraints of the selection matrix structure. The investigated non-convex problem with discrete variables and coupled constraints is challenging to solve and an efficient framework of joint neural network (NN) design is proposed to tackle it. Specifically, the proposed framework consists of a deep reinforcement learning (DRL)-based NN and a deep-unfolding NN, which are employed to optimize the beam selection and digital precoding matrices, respectively. As for the DRL-based NN, we formulate the beam selection problem as a Markov decision process and a double deep Q-network algorithm is developed to solve it. The base station is considered to be an agent, where the state, action, and reward function are carefully designed. Regarding the design of the digital precoding matrix, we develop an iterative weighted minimum mean-square error algorithm induced deep-unfolding NN, which unfolds this algorithm into a layer-wise structure with introduced trainable parameters. Simulation results verify that this jointly trained NN remarkably outperforms the existing iterative algorithms with reduced complexity and stronger robustness.}
}


@article{DBLP:journals/jsac/WangGGODP21,
	author = {Yu Wang and
                  Guan Gui and
                  Haris Gacanin and
                  Tomoaki Ohtsuki and
                  Octavia A. Dobre and
                  H. Vincent Poor},
	title = {An Efficient Specific Emitter Identification Method Based on Complex-Valued
                  Neural Networks and Network Compression},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2305--2317},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087243},
	doi = {10.1109/JSAC.2021.3087243},
	timestamp = {Tue, 12 Mar 2024 14:33:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangGGODP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Specific emitter identification (SEI) is a promising technology to discriminate the individual emitter and enhance the security of various wireless communication systems. SEI is generally based on radio frequency fingerprinting (RFF) originated from the imperfection of emitter's hardware, which is difficult to forge. SEI is generally modeled as a classification task and deep learning (DL), which exhibits powerful classification capability, has been introduced into SEI for better identification performance. In the recent years, a novel DL model, named as complex-valued neural network (CVNN), has been applied into SEI methods for directly processing complex baseband signal and improving identification performance, but it also brings high model complexity and large model size, which is not conducive to the deployment of SEI, especially in Internet-of-things (IoT) scenarios. Thus, we propose an efficient SEI method based on CVNN and network compression, and the former is for performance improvement, while the latter is to reduce model complexity and size with ensuring satisfactory identification performance. Simulation results demonstrated that our proposed CVNN-based SEI method is superior to the existing DL-based methods in both identification performance and convergence speed, and the identification accuracy of CVNN can reach up to nearly 100% at high signal-to-noise ratios (SNRs). In addition, SlimCVNN just has 10% ~ 30% model sizes of the basic CVNN, and its computing complexity has different degrees of decline at different SNRs; there is almost no performance gap between SlimCVNN and CVNN. These results demonstrated the feasibility and potential of CVNN and model compression.}
}


@article{DBLP:journals/jsac/GoutayAHG21,
	author = {Mathieu Goutay and
                  Fay{\c{c}}al Ait Aoudia and
                  Jakob Hoydis and
                  Jean{-}Marie Gorce},
	title = {Machine Learning for {MU-MIMO} Receive Processing in {OFDM} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2318--2332},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087224},
	doi = {10.1109/JSAC.2021.3087224},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GoutayAHG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) starts to be widely used to enhance the performance of multi-user multiple-input multiple-output (MU-MIMO) receivers. However, it is still unclear if such methods are truly competitive with respect to conventional methods in realistic scenarios and under practical constraints. In addition to enabling accurate signal reconstruction on realistic channel models, MU-MIMO receive algorithms must allow for easy adaptation to a varying number of users without the need for retraining. In contrast to existing work, we propose an machine learning (ML)-enhanced MU-MIMO receiver that builds on top of a conventional linear minimum mean squared error (LMMSE) architecture. It preserves the interpretability and scalability of the LMMSE receiver, while improving its accuracy in two ways. First, convolutional neural networks (CNNs) are used to compute an approximation of the second-order statistics of the channel estimation error which are required for accurate equalization. Second, a CNN-based demapper jointly processes a large number of orthogonal frequency-division multiplexing (OFDM) symbols and subcarriers, which allows it to compute better log likelihood ratios (LLRs) by compensating for channel aging. The resulting architecture can be used in the up- and downlink and is trained in an end-to-end manner, removing the need for hard-to-get perfect channel state information (CSI) during the training phase. Simulation results demonstrate consistent performance improvements over the baseline which are especially pronounced in high mobility scenarios.}
}


@article{DBLP:journals/jsac/SangdehZ21,
	author = {Pedram Kheirkhah Sangdeh and
                  Huacheng Zeng},
	title = {DeepMux: Deep-Learning-Based Channel Sounding and Resource Allocation
                  for {IEEE} 802.11ax},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2333--2346},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087246},
	doi = {10.1109/JSAC.2021.3087246},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SangdehZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {MU-MIMO and OFDMA are two key techniques in IEEE 802.11ax standard. Although these two techniques have been intensively studied in cellular networks, their joint optimization in Wi-Fi networks has been rarely explored as OFDMA was introduced to Wi-Fi networks for the first time in 802.11ax. The marriage of these two techniques in Wi-Fi networks creates both opportunities and challenges in the practical design of MAC-layer protocols and algorithms to optimize airtime overhead, spectral efficiency, and computational complexity. In this paper, we present DeepMux, a deep-learning-based MU-MIMO-OFDMA transmission scheme for 802.11ax networks. DeepMux mainly comprises two components: deep-learning-based channel sounding (DLCS) and deep-learning-based resource allocation (DLRA), both of which reside in access points (APs) and impose no computational/communication burden on Wi-Fi clients. DLCS reduces the airtime overhead of 802.11 protocols by leveraging the deep neural networks (DNNs). It uses uplink channels to train the DNNs for downlink channels, making the training process easy to implement. DLRA employs a DNN to solve the mixed-integer resource allocation problem, enabling an AP to obtain a near-optimal solution in polynomial time. We have built a wireless testbed to examine the performance of DeepMux in real-world environments. Our experimental results show that DeepMux reduces the sounding overhead by 62.0% ~ 90.5% and increases the network throughput by 26.3% ~ 43.6%.}
}


@article{DBLP:journals/jsac/SohrabiCY21,
	author = {Foad Sohrabi and
                  Zhilin Chen and
                  Wei Yu},
	title = {Deep Active Learning Approach to Adaptive Beamforming for mmWave Initial
                  Alignment},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2347--2360},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087234},
	doi = {10.1109/JSAC.2021.3087234},
	timestamp = {Thu, 29 Jul 2021 13:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SohrabiCY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a deep learning approach to the adaptive and sequential beamforming design problem for the initial access phase in a mmWave environment with a single-path channel. For a single-user scenario where the problem is equivalent to designing the sequence of sensing beamformers to learn the angle of arrival (AoA) of the dominant path, we propose a novel deep neural network (DNN) that designs the adaptive sensing vectors sequentially based on the available information so far at the base station (BS). By recognizing that the AoA posterior distribution is a sufficient statistic for solving the initial access problem, we use the posterior distribution as the input to the proposed DNN for designing the adaptive sensing strategy. However, computing the posterior distribution can be computationally challenging when the channel fading coefficient is unknown. To address this issue, this paper proposes to use an estimate of the fading coefficient to compute an approximation of the posterior distribution. Further, this paper shows that the proposed DNN can deal with practical beamforming constraints such as the constant modulus constraint. Numerical results demonstrate that compared to the existing adaptive and non-adaptive beamforming schemes, the proposed DNN-based adaptive sensing strategy achieves a significantly better AoA acquisition performance.}
}


@article{DBLP:journals/jsac/FerrandDOG21,
	author = {Paul Ferrand and
                  Alexis Decurninge and
                  Luis Garcia Ord{\'{o}}{\~{n}}ez and
                  Maxime Guillaud},
	title = {Triplet-Based Wireless Channel Charting: Architecture and Experiments},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2361--2373},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087251},
	doi = {10.1109/JSAC.2021.3087251},
	timestamp = {Mon, 03 Jan 2022 22:12:06 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FerrandDOG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Channel charting is a data-driven baseband processing technique consisting in applying self-supervised machine learning techniques to channel state information (CSI), with the objective of reducing the dimension of the data and extracting the fundamental parameters governing its distribution. We introduce a novel channel charting approach based on triplets of samples. The proposed algorithm learns a meaningful similarity metric between CSI samples on the basis of proximity in their respective acquisition times, and simultaneously performs dimensionality reduction. We present an extensive experimental validation of the proposed approach on data obtained from a commercial Massive MIMO system; in particular, we evaluate to which extent the obtained channel chart is similar to the user location information, although it is not supervised by any geographical data. Finally, we propose and evaluate variations in the channel charting process, including the partially supervised case where some labels are available for part of the dataset.}
}


@article{DBLP:journals/jsac/JhaL21,
	author = {Nilesh Kumar Jha and
                  Vincent K. N. Lau},
	title = {Online Downlink Multi-User Channel Estimation for mmWave Systems Using
                  Bayesian Neural Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2374--2387},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087249},
	doi = {10.1109/JSAC.2021.3087249},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JhaL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a Bayesian deep learning framework for model driven online sparse channel estimation task in Multi-user MIMO systems. Tools from Bayesian neural network and stochastic variational Bayesian Inference are utilized to capture aleatoric and epistemic uncertainty estimates. We treat the network prediction as an auxiliary variable to allow inference performance to be unaffected by the stage of training of the network. In addition to providing uncertainty estimates, being Bayesian, the framework enables us the possibility to marginalize over penalty parameters and is well suited for online scenario with changing environments. Our simulations show that the framework is robust to model mismatch, and efficiently captures uncertainty in the predictions.}
}


@article{DBLP:journals/jsac/MaGGR21,
	author = {Xisuo Ma and
                  Zhen Gao and
                  Feifei Gao and
                  Marco Di Renzo},
	title = {Model-Driven Deep Learning Based Channel Estimation and Feedback for
                  Millimeter-Wave Massive Hybrid {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2388--2406},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087269},
	doi = {10.1109/JSAC.2021.3087269},
	timestamp = {Thu, 29 Jul 2021 13:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MaGGR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a model-driven deep learning (MDDL)-based channel estimation and feedback scheme for wideband millimeter-wave (mmWave) massive hybrid multiple-input multiple-output (MIMO) systems, where the angle-delay domain channels' sparsity is exploited for reducing the overhead. First, we consider the uplink channel estimation for time-division duplexing systems. To reduce the uplink pilot overhead for estimating high-dimensional channels from a limited number of radio frequency (RF) chains at the base station (BS), we propose to jointly train the phase shift network and the channel estimator as an auto-encoder. Particularly, by exploiting the channels' structured sparsity from an a priori model and learning the integrated trainable parameters from the data samples, the proposed multiple-measurement-vectors learned approximate message passing (MMV-LAMP) network with the devised redundant dictionary can jointly recover multiple subcarriers' channels with significantly enhanced performance. Moreover, we consider the downlink channel estimation and feedback for frequency-division duplexing systems. Similarly, the pilots at the BS and channel estimator at the users can be jointly trained as an encoder and a decoder, respectively. Besides, to further reduce the channel feedback overhead, only the received pilots on part of the subcarriers are fed back to the BS, which can exploit the MMV-LAMP network to reconstruct the spatial-frequency channel matrix. Numerical results show that the proposed MDDL-based channel estimation and feedback scheme outperforms state-of-the-art approaches.}
}


@article{DBLP:journals/jsac/ZhaoVGS21,
	author = {Zhongyuan Zhao and
                  Mehmet Can Vuran and
                  Fujuan Guo and
                  Stephen D. Scott},
	title = {Deep-Waveform: {A} Learned {OFDM} Receiver Based on Deep Complex-Valued
                  Convolutional Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2407--2420},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087241},
	doi = {10.1109/JSAC.2021.3087241},
	timestamp = {Wed, 20 Jul 2022 10:55:48 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoVGS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The (inverse) discrete Fourier transform (DFT/ IDFT) is often perceived as essential to orthogonal frequency-division multiplexing (OFDM) systems. In this paper, a deep complex-valued convolutional network (DCCN) is developed to recover bits from time-domain OFDM signals without relying on any explicit DFT/IDFT. The DCCN can exploit the cyclic prefix (CP) of OFDM waveform for increased SNR by replacing DFT with a learned linear transform, and has the advantage of combining CP-exploitation, channel estimation, and intersymbol interference (ISI) mitigation, with a complexity of O (N 2 ). Numerical tests show that the DCCN receiver can outperform the legacy channel estimators based on ideal and approximate linear minimum mean square error (LMMSE) estimation and a conventional CP-enhanced technique in Rayleigh fading channels with various delay spreads and mobility. The proposed approach benefits from the expressive nature of complex-valued neural networks, which, however, currently lack support from popular deep learning platforms. In response, guidelines of exact and approximate implementations of a complex-valued convolutional layer are provided for the design and analysis of convolutional networks for wireless PHY. Furthermore, a suite of novel training techniques are developed to improve the convergence and generalizability of the trained model in fading channels. This work demonstrates the capability of deep neural networks in processing OFDM waveforms and the results suggest that the FFT processor in OFDM receivers can be replaced by a hardware AI accelerator.}
}


@article{DBLP:journals/jsac/ShenHXYC21,
	author = {Boxiao Shen and
                  Chuan Huang and
                  Wenjun Xu and
                  Tingting Yang and
                  Shuguang Cui},
	title = {Blind Channel Codes Recognition via Deep Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2421--2433},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087252},
	doi = {10.1109/JSAC.2021.3087252},
	timestamp = {Mon, 06 Sep 2021 18:46:01 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShenHXYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers the blind recognition of the type and the encoding parameters of channel codes from the Gaussian noisy signals. Specifically, based on the recurrent neural network (RNN), the attention mechanism, and the residual neural network (ResNet), three universal recognizers are proposed to identify the type, rate, and length of the target channel codes, with a training set generated by a small portion of all the possible code parameters. The proposed architectures need near zero a priori knowledge about the target channel code, and only require the length of the received signal to be dozen times of the codeword length. Numerical experiments show that the proposed deep learning methods own strong generalization to identify channel codes from the testing samples not generated by the encoding parameters utilized for the training set.}
}


@article{DBLP:journals/jsac/WengQ21,
	author = {Zhenzi Weng and
                  Zhijin Qin},
	title = {Semantic Communication Systems for Speech Transmission},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2434--2444},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087240},
	doi = {10.1109/JSAC.2021.3087240},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WengQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communications could improve the transmission efficiency significantly by exploring the semantic information. In this paper, we make an effort to recover the transmitted speech signals in the semantic communication systems, which minimizes the error at the semantic level rather than the bit or symbol level. Particularly, we design a deep learning (DL)-enabled semantic communication system for speech signals, named DeepSC-S. In order to improve the recovery accuracy of speech signals, especially for the essential information, DeepSC-S is developed based on an attention mechanism by utilizing a squeeze-and-excitation (SE) network. The motivation behind the attention mechanism is to identify the essential speech information by providing higher weights to them when training the neural network. Moreover, in order to facilitate the proposed DeepSC-S for dynamic channel environments, we find a general model to cope with various channel conditions without retraining. Furthermore, we investigate DeepSC-S in telephone systems as well as multimedia transmission systems to verify the model adaptation in practice. The simulation results demonstrate that our proposed DeepSC-S outperforms the traditional communications in both cases in terms of the speech signals metrics, such as signal-to-distortion ration and perceptual evaluation of speech distortion. Besides, DeepSC-S is more robust to channel variations, especially in the low signal-to-noise (SNR) regime.}
}


@article{DBLP:journals/jsac/ZhouWZW21,
	author = {Huan Zhou and
                  Tong Wu and
                  Haijun Zhang and
                  Jie Wu},
	title = {Incentive-Driven Deep Reinforcement Learning for Content Caching and
                  {D2D} Offloading},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2445--2460},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087232},
	doi = {10.1109/JSAC.2021.3087232},
	timestamp = {Tue, 21 Mar 2023 21:09:00 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouWZW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offloading cellular traffic via Device-to-Device communication (or D2D offloading) has been proved to be an effective way to ease the traffic burden of cellular networks. However, mobile nodes may not be willing to take part in D2D offloading without proper financial incentives since the data offloading process will incur a lot of resource consumption. Therefore, it is imminent to exploit effective incentive mechanisms to motivate nodes to participate in D2D offloading. Furthermore, the design of the content caching strategy is also crucial to the performance of D2D offloading. In this paper, considering these issues, a novel Incentive-driven and Deep Q Network (DQN) based Method, named IDQNM is proposed, in which the reverse auction is employed as the incentive mechanism. Then, the incentive-driven D2D offloading and content caching process is modeled as Integer Non-Linear Programming (INLP), aiming to maximize the saving cost of the Content Service Provider (CSP). To solve the optimization problem, the content caching method based on a Deep Reinforcement Learning (DRL) algorithm, named DQN is proposed to get the approximate optimal solution, and a standard Vickrey-Clarke-Groves (VCG)-based payment rule is proposed to compensate for mobile nodes' cost. Extensive real trace-driven simulation results demonstrate that the proposed IDQNM greatly outperforms other baseline methods in terms of the CSP's saving cost and the offloading rate in different scenarios.}
}


@article{DBLP:journals/jsac/LiGLXL21,
	author = {Wenzhong Li and
                  Shaohua Gao and
                  Xiang Li and
                  Yeting Xu and
                  Sanglu Lu},
	title = {TCP-NeuRoc: Neural Adaptive {TCP} Congestion Control With Online Changepoint
                  Detection},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2461--2475},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087247},
	doi = {10.1109/JSAC.2021.3087247},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiGLXL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion control is a fundamental mechanism for TCP protocol, which has been extensively studied in the past three decades. However, our experimental evaluations show that the state-of-art congestion control algorithms such as Cubic and BBR are far from optimal: they have unresolved issues such as insufficient usage of available bandwidth, inadaptable to dynamic bandwidth variants, and compromising on one or more performance dimensions. To address these challenges, we propose a novel congestion control mechanism called NeuRoc that coordinately uses online changepoint detection and deep reinforcement learning (DRL) technique to generate the optimal congestion control policy, which allows TCP operating at Kleinrock ’s optimal operation point to achieve fully bandwidth usage and low latency. To address the practical issues of deploying the deep learning based congestion control mechanism, we propose a cold-started training and deployment framework to reduce the cost of bootstrap. We implement NeuRoc on an emulation platform which connects to the Linux network protocol stack through virtual network interfaces. Extensive experiments show that NeuRoc achieves the best throughput-latency tradeoff compared with the state-of-the-arts in a variety of scenarios.}
}


@article{DBLP:journals/jsac/XuWZTWX21,
	author = {Zhiyuan Xu and
                  Kun Wu and
                  Weiyi Zhang and
                  Jian Tang and
                  Yanzhi Wang and
                  Guoliang Xue},
	title = {PnP-DRL: {A} Plug-and-Play Deep Reinforcement Learning Approach for
                  Experience-Driven Networking},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2476--2486},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087270},
	doi = {10.1109/JSAC.2021.3087270},
	timestamp = {Fri, 14 Jun 2024 11:24:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XuWZTWX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While Deep Reinforcement Learning has emerged as a de facto approach to many complex experience-driven networking problems, it remains challenging to deploy DRL into real systems. Due to the random exploration or half-trained deep neural networks during the online training process, the DRL agent may make unexpected decisions, which may lead to system performance degradation or even system crash. In this paper, we propose PnP-DRL, an offline-trained, plug and play DRL solution, to leverage the batch reinforcement learning approach to learn the best control policy from pre-collected transition samples without interacting with the system. After being trained without interaction with systems, our Plug and Play DRL agent will start working seamlessly, without additional exploration or possible disruption of the running systems. We implement and evaluate our PnP-DRL solution on a prevalent experience-driven networking problem, Dynamic Adaptive Streaming over HTTP (DASH). Extensive experimental results manifest that 1) The existing batch reinforcement learning method has its limits; 2) Our approach PnP-DRL significantly outperforms classical adaptive bitrate algorithms in average user Quality of Experience (QoE); 3) PnP-DRL, unlike the state-of-the-art online DRL methods, can be off and running without learning gaps, while achieving comparable performances.}
}


@article{DBLP:journals/jsac/AkbariAJPME21,
	author = {Mohammad Akbari and
                  Mohammad Reza Abedi and
                  Roghayeh Joda and
                  Mohsen Pourghasemian and
                  Nader Mokari and
                  Melike Erol{-}Kantarci},
	title = {Age of Information Aware {VNF} Scheduling in Industrial IoT Using
                  Deep Reinforcement Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2487--2500},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087264},
	doi = {10.1109/JSAC.2021.3087264},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AkbariAJPME21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In delay-sensitive industrial Internet of Things (IIoT) applications, the age of information (AoI) is employed to characterize the freshness of information. Meanwhile, the emerging network function virtualization provides flexibility and agility for service providers to deliver a given network service using a sequence of virtual network functions (VNFs). However, suitable VNF placement and scheduling in these schemes is NP-hard and finding a globally optimal solution by traditional approaches is complex. Recently, deep reinforcement learning (DRL) has appeared as a viable way to solve such problems. In this paper, we first utilize single agent low-complex compound action actor-critic RL to cover both discrete and continuous actions and jointly minimize VNF cost and AoI in terms of network resources under end-to-end Quality of Service constraints. To surmount the single-agent capacity limitation for learning, we then extend our solution to a multi-agent DRL scheme in which agents collaborate with each other. Simulation results demonstrate that single-agent schemes significantly outperform the greedy algorithm in terms of average network cost and AoI. Moreover, multi-agent solution decreases the average cost by dividing the tasks between the agents. However, it needs more iterations to be learned due to the requirement on the agents' collaboration.}
}


@article{DBLP:journals/jsac/ZhangDCCS21,
	author = {Minglong Zhang and
                  Yi Dou and
                  Peter Han Joo Chong and
                  Henry C. B. Chan and
                  Boon{-}Chong Seet},
	title = {Fuzzy Logic-Based Resource Allocation Algorithm for {V2X} Communications
                  in 5G Cellular Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2501--2513},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087244},
	doi = {10.1109/JSAC.2021.3087244},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangDCCS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we spotlight vehicle-to-everything (V2X) communications in 5G cellular networks. Cellular V2X (C-V2X) communications in 5G enable more advanced services with requirements of ultra-low latency and ultra-high reliability. How to make full use of the limited physical-layer resources is a key determinant to guarantee the quality of service (QoS). Therefore, resource allocation plays an essential role in exchanging information between vehicles, infrastructure, and other devices. In order to intelligently and reasonably allocate resources, a self-adaptive fuzzy logic-based strategy is developed in this paper. To evaluate the network performance for this adaptive strategy, a system model for V2X communications is built for urban areas, and typical safety and non-safety services are deployed in the network. Simulation results reveal that the proposed fuzzy logic-based algorithm can substantially improve resource utilization and satisfy the requirements of V2X services, compared with prior counterparts, which cannot provide guaranteed services due to low resource utilization.}
}


@article{DBLP:journals/jsac/HuangCGXHC21,
	author = {Chong Huang and
                  Gaojie Chen and
                  Yu Gong and
                  Peng Xu and
                  Zhu Han and
                  Jonathon A. Chambers},
	title = {Buffer-Aided Relay Selection for Cooperative Hybrid {NOMA/OMA} Networks
                  With Asynchronous Deep Reinforcement Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2514--2525},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087225},
	doi = {10.1109/JSAC.2021.3087225},
	timestamp = {Wed, 05 Jan 2022 14:30:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuangCGXHC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates asynchronous reinforcement learning algorithms for joint buffer-aided relay selection and power allocation in the non-orthogonal-multiple-access (NOMA) relay network. With the hybrid NOMA/OMA transmission, we investigate joint relay selection and power allocation to maximize the throughput with the delay constraint. To solve this complicated high-dimensional optimization problem, we propose two asynchronous reinforcement learning-based schemes: the asynchronous deep Q-Learning network (ADQN)-based scheme and the asynchronous advantage actor-critic (A3C)-based scheme, respectively. The A3C-based scheme achieves better performance and robustness when the action space is large, while the ADQN-based scheme converges faster with a small action space. Moreover, a-prior information is exploited to improve the convergence of the proposed schemes. The simulation results show that the proposed asynchronous learning-based schemes can learn from the environment and achieve good convergence.}
}


@article{DBLP:journals/jsac/DoshiYFYA21,
	author = {Akash S. Doshi and
                  Srinivas Yerramalli and
                  Lorenzo Ferrari and
                  Taesang Yoo and
                  Jeffrey G. Andrews},
	title = {A Deep Reinforcement Learning Framework for Contention-Based Spectrum
                  Sharing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2526--2540},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087254},
	doi = {10.1109/JSAC.2021.3087254},
	timestamp = {Thu, 29 Jul 2021 13:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DoshiYFYA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing number of wireless devices operating in unlicensed spectrum motivates the development of intelligent adaptive approaches to spectrum access. We consider decentralized contention-based medium access for base stations (BSs) operating on unlicensed shared spectrum, where each BS autonomously decides whether or not to transmit on a given resource. The contention decision attempts to maximize not its own downlink throughput, but rather a network-wide objective. We formulate this problem as a decentralized partially observable Markov decision process with a novel reward structure that provides long term proportional fairness in terms of throughput. We then introduce a two-stage Markov decision process in each time slot that uses information from spectrum sensing and reception quality to make a medium access decision. Finally, we incorporate these features into a distributed reinforcement learning framework for contention-based spectrum access. Our formulation provides decentralized inference, online adaptability and also caters to partial observability of the environment through recurrent Q-learning. Empirically, we find its maximization of the proportional fairness metric to be competitive with a genie-aided adaptive energy detection threshold, while being robust to channel fading and small contention windows.}
}


@article{DBLP:journals/jsac/WangQGWLZ21,
	author = {Haozhao Wang and
                  Zhihao Qu and
                  Song Guo and
                  Ningqi Wang and
                  Ruixuan Li and
                  Weihua Zhuang},
	title = {{LOSP:} Overlap Synchronization Parallel With Local Compensation for
                  Fast Distributed Training},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2541--2557},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087272},
	doi = {10.1109/JSAC.2021.3087272},
	timestamp = {Wed, 28 Jun 2023 15:57:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangQGWLZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {When running in Parameter Server (PS), the Distributed Stochastic Gradient Descent (D-SGD) incurs significant communication delays and huge communication overhead due to the model synchronization. Moreover, considering the heterogeneity of computational capability among workers, traditional synchronization modes incur under-utilization of computational resources because fast workers have to wait for slow ones finishing the computation. Although our previous work OSP can effectively solve these problems by overlapping the computation and communication procedures and allowing adaptive multiple local updates in distributed training, it causes the staleness problem brought by the overlap, yielding a performance degradation. In this paper, we propose a new method named LOSP by introducing local compensation to our previous synchronization mechanism, which mitigates adverse effects caused by the overlapping synchronization. We theoretically prove that LOSP (1) preserves the same convergence rate as the sequential SGD for non-convex problems, and (2) exhibits good scalability due to the linear speedup property with respect to both the number of workers and the average number of local updates. Evaluations show that LOSP significantly improves performance over the state-of-the-art ones in terms of both convergence accuracy and communication cost.}
}


@article{DBLP:journals/jsac/HuangZZMZMH21,
	author = {Haojun Huang and
                  Cheng Zeng and
                  Yangming Zhao and
                  Geyong Min and
                  Yingying Zhu and
                  Wang Miao and
                  Jia Hu},
	title = {Scalable Orchestration of Service Function Chains in NFV-Enabled Networks:
                  {A} Federated Reinforcement Learning Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2558--2571},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087227},
	doi = {10.1109/JSAC.2021.3087227},
	timestamp = {Thu, 16 Nov 2023 09:03:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuangZZMZMH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network function virtualization (NFV) is critical to the scalability and flexibility of various network services in the form of service function chains (SFCs), which refer to a set of Virtual Network Functions (VNFs) chained in a specific order. However, the NFV performance is hard to fulfill the ever-increasing requirements of network services mainly due to the static orchestrations of SFCs. To tackle this issue, a novel Scalable SFC Orchestration (SSCO) scheme is proposed in this paper for NFV-enabled networks via federated reinforcement learning. SSCO has three remarkable characteristics distinguishing from the previous work: (1) A federated-learning-based framework is designed to train a global learning model, with time-variant local model explorations, for scalable SFC orchestration, while avoiding data sharing among stakeholders; (2) SSCO allows for parameter update among local clients and the cloud server just at the first and last epochs of each episode to ensure that distributed clients can make model optimization at a low communication cost; (3) SSCO introduces an efficient deep reinforcement learning (DRL) approach, with the local learning knowledge of available resources and instantiation cost, to map VNFs into networks flexibly. Furthermore, a loss-weight-based mechanism is proposed to generate and exploit reference samples in replay buffers for future training, avoiding the strong relevance of samples. Simulation results obtained from different working scenarios demonstrate that SSCO can significantly reduce placement errors and improve resource utilization ratio to place time-variant VNFs compared with the state-of-the-art mechanisms. Furthermore, the results show that the proposed approach can achieve desirable scalability.}
}


@article{DBLP:journals/jsac/CuiSZP21,
	author = {Laizhong Cui and
                  Xiaoxin Su and
                  Yipeng Zhou and
                  Yi Pan},
	title = {Slashing Communication Traffic in Federated Learning by Transmitting
                  Clustered Model Updates},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2572--2589},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087262},
	doi = {10.1109/JSAC.2021.3087262},
	timestamp = {Thu, 29 Jul 2021 13:41:45 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CuiSZP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is an emerging decentralized learning framework through which multiple clients can collaboratively train a learning model. However, a major obstacle that impedes the wide deployment of FL lies in massive communication traffic. To train high dimensional machine learning models (such as CNN models), heavy communication traffic can be incurred by exchanging model updates via the Internet between clients and the parameter server (PS), implying that the network resource can be easily exhausted. Compressing model updates is an effective way to reduce the traffic amount. However, a flexible unbiased compression algorithm applicable for both uplink and downlink compression in FL is still absent from existing works. In this work, we devise the Model Update Compression by Soft Clustering (MUCSC) algorithm to compress model updates transmitted between clients and the PS. In MUCSC, it is only necessary to transmit cluster centroids and the cluster ID of each model update. Moreover, we prove that: 1) The compressed model updates are unbiased estimation of their original values so that the convergence rate by transmitting compressed model updates is unchanged; 2) MUCSC can guarantee that the influence of the compression error on the model accuracy is minimized. Then, we further propose the boosted MUCSC (B-MUCSC) algorithm, a biased compression algorithm that can achieve an extremely high compression rate by grouping insignificant model updates into a super cluster. B-MUCSC is suitable for scenarios with very scarce network resource. Ultimately, we conduct extensive experiments with the CIFAR-10 and FEMNIST datasets to demonstrate that our algorithms can not only substantially reduce the volume of communication traffic in FL, but also improve the training efficiency in practical networks.}
}


@article{DBLP:journals/jsac/TungKRG21,
	author = {Tze{-}Yang Tung and
                  Szymon Kobus and
                  Joan S. Pujol Roig and
                  Deniz G{\"{u}}nd{\"{u}}z},
	title = {Effective Communications: {A} Joint Learning and Communication Framework
                  for Multi-Agent Reinforcement Learning Over Noisy Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2590--2603},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087248},
	doi = {10.1109/JSAC.2021.3087248},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TungKRG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel formulation of the “effectiveness problem” in communications, put forth by Shannon and Weaver in their seminal work “The Mathematical Theory of Communication”, by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment, can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment, and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate “effectively” over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the “learning to communicate” framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.}
}


@article{DBLP:journals/jsac/ShenZMPW21,
	author = {Guanxiong Shen and
                  Junqing Zhang and
                  Alan Marshall and
                  Linning Peng and
                  Xianbin Wang},
	title = {Radio Frequency Fingerprint Identification for LoRa Using Deep Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2604--2616},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087250},
	doi = {10.1109/JSAC.2021.3087250},
	timestamp = {Thu, 29 Jul 2021 13:41:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShenZMPW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Radio frequency fingerprint identification (RFFI) is an emerging device authentication technique that relies on the intrinsic hardware characteristics of wireless devices. This paper designs a deep learning-based RFFI scheme for Long Range (LoRa) systems. Firstly, the instantaneous carrier frequency offset (CFO) is found to drift, which could result in misclassification and significantly compromise the stability of the deep learning-based RFFI system. CFO compensation is demonstrated to be effective mitigation. Secondly, three signal representations for deep learning-based RFFI are investigated in time, frequency, and time-frequency domains, namely in-phase and quadrature (IQ) samples, fast Fourier transform (FFT) results and spectrograms, respectively. For these signal representations, three deep learning models are implemented, i.e., multilayer perceptron (MLP), long short-term memory (LSTM) network and convolutional neural network (CNN), in order to explore an optimal framework. Finally, a hybrid classifier that can adjust the prediction of deep learning models with the estimated CFO is designed to further increase the classification accuracy. The CFO will not change dramatically over several continuous days, hence it can be used to correct predictions when the estimated CFO is much different from the reference one. Experimental evaluation is performed in real wireless environments involving 25 LoRa devices and a Universal Software Radio Peripheral (USRP) N210 platform. The spectrogram-CNN model is found to be optimal for classifying LoRa devices which can reach an accuracy of 96.40% with the least complexity and training time.}
}


@article{DBLP:journals/jsac/GongCWHMSZ21,
	author = {Xueluan Gong and
                  Yanjiao Chen and
                  Qian Wang and
                  Huayang Huang and
                  Lingshuo Meng and
                  Chao Shen and
                  Qian Zhang},
	title = {Defense-Resistant Backdoor Attacks Against Deep Neural Networks in
                  Outsourced Cloud Environment},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2617--2631},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087237},
	doi = {10.1109/JSAC.2021.3087237},
	timestamp = {Mon, 05 Feb 2024 20:23:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GongCWHMSZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The time and monetary costs of training sophisticated deep neural networks are exorbitant, which motivates resource-limited users to outsource the training process to the cloud. Concerning that an untrustworthy cloud service provider may inject backdoors to the returned model, the user can leverage state-of-the-art defense strategies to examine the model. In this paper, we aim to develop robust backdoor attacks (named RobNet) that can evade existing defense strategies from the standpoint of malicious cloud providers. The key rationale is to diversify the triggers and strengthen the model structure so that the backdoor is hard to be detected or removed. To attain this objective, we refine the trigger generation algorithm by selecting the neuron(s) with large weights and activations and then computing the triggers via gradient descent to maximize the value of the selected neuron(s). In stark contrast to existing works that fix the trigger location, we design a multi-location patching method to make the model less sensitive to mild displacement of triggers in real attacks. Furthermore, we extend the attack space by proposing multi-trigger backdoor attacks that can misclassify inputs with different triggers into the same or different target label(s). We evaluate the performance of RobNet on MNIST, GTSRB, and CIFAR-10 datasets, against four representative defense strategies Pruning, NeuralCleanse, Strip, and ABS. The comparison with two state-of-the-art baselines BadNets and Hidden Backdoors demonstrates that RobNet achieves higher attack success rate and is more resistant to potential defenses.}
}


@article{DBLP:journals/jsac/HanWZCYLSY21,
	author = {Dongqi Han and
                  Zhiliang Wang and
                  Ying Zhong and
                  Wenqi Chen and
                  Jiahai Yang and
                  Shuqiang Lu and
                  Xingang Shi and
                  Xia Yin},
	title = {Evaluating and Improving Adversarial Robustness of Machine Learning-Based
                  Network Intrusion Detectors},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2632--2647},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087242},
	doi = {10.1109/JSAC.2021.3087242},
	timestamp = {Tue, 18 Oct 2022 08:35:31 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HanWZCYLSY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical -the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic -the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable -we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).}
}


@article{DBLP:journals/jsac/LiLT21,
	author = {Yupeng Li and
                  Ben Liang and
                  Ali Tizghadam},
	title = {Robust Online Learning against Malicious Manipulation and Feedback
                  Delay With Application to Network Flow Classification},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {8},
	pages = {2648--2663},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3087268},
	doi = {10.1109/JSAC.2021.3087268},
	timestamp = {Tue, 13 Dec 2022 16:02:35 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiLT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Malicious data manipulation reduces the effectiveness of machine learning techniques, which rely on accurate knowledge of the input data. Motivated by real-world applications in network flow classification, we address the problem of robust online learning with delayed feedback in the presence of malicious data generators that attempt to gain favorable classification outcome by manipulating the data features. When the feedback delay is static, we propose online algorithms termed ROLC-NC and ROLC-C when the malicious data generators are non-clairvoyant and clairvoyant, respectively. We then consider the dynamic delay case, for which we propose online algorithms termed ROLC-NC-D and ROLC-C-D when the malicious data generators are non-clairvoyant and clairvoyant, respectively. We derive regret bounds for these four algorithms and show that they are sub-linear under mild conditions. We further evaluate the proposed algorithms in network flow classification via extensive experiments using real-world data traces. Our experimental results demonstrate that the proposed algorithms can approach the performance of an optimal static offline classifier that is not under attack, while outperforming the same offline classifier when tested with a mixture of normal and manipulated data.}
}


@article{DBLP:journals/jsac/TornatoreWZCBW21,
	author = {Massimo Tornatore and
                  Elaine Wong and
                  Zuqing Zhu and
                  Ramon Casellas and
                  Balagangadhar G. Bathula and
                  Lena Wosinska},
	title = {Guest Editorial Latest Advances in Optical Networks for 5G Communications
                  and Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2667--2671},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3094600},
	doi = {10.1109/JSAC.2021.3094600},
	timestamp = {Thu, 16 Sep 2021 18:06:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TornatoreWZCBW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This Special Issue contains a collection of outstanding papers covering several recent advances in optical networks for 5G communications and beyond. Papers are organized into four categories: network resource planning; optical access networks; optical fronthaul solutions; and autonomous and data-driven network management. In this introduction, a brief overview of the field is given, followed by a summary of the seventeen papers of this Special Issue, and a discussion of future directions in the field.}
}


@article{DBLP:journals/jsac/VassTHBHKOVR21,
	author = {Bal{\'{a}}zs Vass and
                  J{\'{a}}nos Tapolcai and
                  Zal{\'{a}}n Heszberger and
                  J{\'{o}}zsef B{\'{\i}}r{\'{o}} and
                  David Hay and
                  Fernando A. Kuipers and
                  Jorik Oostenbrink and
                  Alessandro Valentini and
                  Lajos R{\'{o}}nyai},
	title = {Probabilistic Shared Risk Link Groups Modeling Correlated Resource
                  Failures Caused by Disasters},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2672--2687},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064652},
	doi = {10.1109/JSAC.2021.3064652},
	timestamp = {Tue, 16 Aug 2022 23:06:38 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/VassTHBHKOVR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To evaluate the expected availability of a backbone network service, the administrator should consider all possible failure scenarios under the specific service availability model stipulated in the corresponding service-level agreement. Given the increase in natural disasters and malicious attacks with geographically extensive impact, considering only independent single component failures is often insufficient. This paper builds a stochastic model of geographically correlated link failures caused by disasters to estimate the hazards an optical backbone network may be prone to and to understand the complex correlation between possible link failures. We first consider link failures only and later extend our model also to capture node failures. With such a model, one can quickly extract essential information such as the probability of an arbitrary set of network resources to fail simultaneously, the probability of two nodes to be disconnected, the probability of a path to survive a disaster. Furthermore, we introduce standard data structures and a unified terminology on Probabilistic Shared Risk Link Groups (PSRLGs), along with a pre-computation process, which represents the failure probability of a set of resources succinctly. In particular, we generate a quasilinear-sized data structure in polynomial time, which allows the efficient computation of the cumulative failure probability of any set of network elements. Our evaluation is based on carefully pre-processed seismic hazard data matched to real-world optical backbone network topologies.}
}


@article{DBLP:journals/jsac/MoghaddamBS21,
	author = {Elham Ehsani Moghaddam and
                  Hamzeh Beyranvand and
                  Jawad A. Salehi},
	title = {Resource Allocation in Space Division Multiplexed Elastic Optical
                  Networks Secured With Quantum Key Distribution},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2688--2700},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064641},
	doi = {10.1109/JSAC.2021.3064641},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MoghaddamBS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Elastic Optical Network (EON) is a promising solution to address the high capacity, low latency, and flexibility requirements of the upcoming 5th-generation (5G) networks. Furthermore, Multi-Core Fibers (MCFs) and Space Division Multiplexing (SDM) technique can be utilized to overcome the capacity limitation of the conventional Single Mode Fibers (SMFs). On the other hand, Quantum Key Distribution (QKD) is an effective solution to address the security issues in 5G transport networks. In this paper, we investigate the performance of QKD over elastic optical networks with multi-core fibers and address the resource allocation problem for quantum and classical channels of QKD (QChs and CChs) and conventional data channels (DChs). To do so, we calculate the background noise caused by different noise sources and accordingly calculate the Secret Key Rate (SKR) in quantum channels. Then, we propose an Integer Linear Programming formulation and a heuristic algorithm to allocate network resources (spectrum, core, and links) to QChs, CChs, and DChs, with the objective of maximizing the secret key rate and minimizing the number of utilized frequency slots (FSs). Finally, we evaluate the proposed ILP and heuristic algorithm in terms of SKR and the number of utilized FSs. In our simulations, we consider core and metro topologies, fixed and distance adaptive launch power for classical signals, different fiber specifications, and different assumptions regarding the relative locations of quantum and classical channels in a multi-core fiber.}
}


@article{DBLP:journals/jsac/CaoZLLZC21,
	author = {Yuan Cao and
                  Yongli Zhao and
                  Jun Li and
                  Rui Lin and
                  Jie Zhang and
                  Jiajia Chen},
	title = {Hybrid Trusted/Untrusted Relay-Based Quantum Key Distribution Over
                  Optical Backbone Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2701--2718},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064662},
	doi = {10.1109/JSAC.2021.3064662},
	timestamp = {Mon, 18 Jul 2022 20:00:23 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaoZLLZC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum key distribution (QKD) has demonstrated a great potential to provide future-proofed security, especially for 5G and beyond communications. As the critical infrastructure for 5G and beyond communications, optical networks can offer a cost-effective solution to QKD deployment utilizing the existing fiber resources. In particular, measurement-device-independent QKD shows its ability to extend the secure distance with the aid of an untrusted relay. Compared to the trusted relay, the untrusted relay has obviously better security, since it does not rely on any assumption on measurement and even allows to be accessed by an eavesdropper. However, it cannot extend QKD to an arbitrary distance like the trusted relay, such that it is expected to be combined with the trusted relay for large-scale QKD deployment. In this work, we study the hybrid trusted/untrusted relay based QKD deployment over optical backbone networks and focus on cost optimization during the deployment phase. A new network architecture of hybrid trusted/untrusted relay based QKD over optical backbone networks is described, where the node structures of the trusted relay and untrusted relay are elaborated. The corresponding network, cost, and security models are formulated. To optimize the deployment cost, an integer linear programming model and a heuristic algorithm are designed. Numerical simulations verify that the cost-optimized design can significantly outperform the benchmark algorithm in terms of deployment cost and security level. Up to 25% cost saving can be achieved by deploying QKD with the hybrid trusted/untrusted relay scheme while keeping much higher security level relative to the conventional point-to-point QKD protocols that are only with the trusted relays.}
}


@article{DBLP:journals/jsac/ChatterjeeWO21,
	author = {Bijoy Chand Chatterjee and
                  Abdul Wadud and
                  Eiji Oki},
	title = {Proactive Fragmentation Management Scheme Based on Crosstalk-Avoided
                  Batch Processing for Spectrally-Spatially Elastic Optical Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2719--2733},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064594},
	doi = {10.1109/JSAC.2021.3064594},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChatterjeeWO21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fragmentation with crosstalks is the major obstacle in spectrally-spatially elastic optical networks, which suppresses resource utilization while degrading the quality-of-transmission. To overcome this issue, this article proposes, for the first time, a proactive fragmentation management scheme based on batch processing while satisfying both inter-core and inter-mode crosstalks to enhance resource utilization. The proposed scheme adopts a batch processing method to create batches of lightpath requests received within a time threshold to utilize spectrum resources effectively. In batch processing, lightpath requests are prioritized based on the number of links in their routes and required slots. To maintain fairness in batch processing, when any request is rejected, the proposed scheme triggers a procedure that gives an equal opportunity to all arriving requests within the threshold, irrespective of numbers of hops and requested capacities, for allocation. We formulate the static batch processing of lightpath requests (SBPLR) as an integer linear programming (ILP) problem. We prove that SBPLR is an NP-Complete problem. We introduce a heuristic solution when ILP is intractable. To serve lightpath requests in each batch while avoiding inter-core and inter-mode crosstalks, we develop a core-mode-spectrum allocation algorithm. We present a dynamic batch processing based fragmentation management approach. Numerical results indicate that the proposed scheme outperforms the benchmark schemes.}
}


@article{DBLP:journals/jsac/ShahriarZCTBMH21,
	author = {Nashid Shahriar and
                  Mubeen Zulfiqar and
                  Shihabur Rahman Chowdhury and
                  Sepehr Taeb and
                  Raouf Boutaba and
                  Jeebak Mitra and
                  Mahdi Hemmati},
	title = {Disruption Minimized Bandwidth Scaling in EON-Enabled Transport Network
                  Slices},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2734--2747},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064643},
	doi = {10.1109/JSAC.2021.3064643},
	timestamp = {Thu, 16 Sep 2021 18:06:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShahriarZCTBMH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Elastic Optical Networks (EONs) enable finer-grained resource allocation and tuning of transmission configurations for right-sized resource allocation. These features make EONs excellent choice for 5G transport networks supporting highly dynamic traffic with diverse Quality-of-Service (QoS) requirements. 5G network slices are expected to host applications with a dynamic nature ( e.g. , augmented/virtual reality broadcasting), which will result in slice resource requirement changing over time. The initial resource allocation to network slices has to be adapted to accommodate such changes without causing significant disruption to existing traffic and using minimal additional resources. In this paper, we address the problem of scaling bandwidth demand of network slices on an EON-enabled 5G transport network. In contrast to the state-of-the-art, we do not assume any specific technologies for minimizing disruption when accommodating the scaling request. Rather, we propose an Integer Linear Program (ILP) and a heuristic algorithm for accommodating scaling requests by choosing from a comprehensive set of reconfiguration actions. We carefully design a novel cost model for capturing traffic disruptions and additional resource usage by these different actions. Our extensive simulations using realistic network topologies shed light on the trade-off between additional resource usage and disruption while accommodating slice scaling requests by employing a comprehensive set of reconfiguration actions. Simulation results also show that our heuristic algorithm can find solutions that remain within 10% of ILP-based solutions, while executing several orders of magnitude faster than ILP.}
}


@article{DBLP:journals/jsac/LiuGYC21,
	author = {Yong Liu and
                  Huaxi Gu and
                  Fulong Yan and
                  Nicola Calabretta},
	title = {Highly-Efficient Switch Migration for Controller Load Balancing in
                  Elastic Optical Inter-Datacenter Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2748--2761},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064664},
	doi = {10.1109/JSAC.2021.3064664},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuGYC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In elastic optical inter-datacenter networks, multiple software-defined networking (SDN) controllers have been used to improve scalability and reliability. The static controller deployment faces the problem of controller load imbalance under dynamic changes in network traffic. In response to this problem, existing research works proposed to use dynamic switch migration to achieve the controller load balancing. However, the efficiency of the existing switch migration is low, because the load balancing performance of controllers does not improve significantly after the switch migration and the migration activities can also cause additional migration costs. Thus the efficiency of switch migration has not yet been well resolved. In this work, we propose a highly-efficient switch migration (HESM) for controller load balancing. The proposed HESM method defines multiple load metrics to measure the load of controllers, and selects the optimal target controller with the largest remaining resource, which improves the load balancing performance. HESM selects switches based on minimizing migration costs, reducing additional migration costs. In addition, HESM can handle the load of multiple controllers in parallel during a switch migration activity, which improves the efficiency of migration. The simulation results show that HESM significantly improves load balancing performance of controllers and reduces the migration cost compared to existing solutions.}
}


@article{DBLP:journals/jsac/BharA21,
	author = {Chayan Bhar and
                  Erik Agrell},
	title = {Energy- and Bandwidth-Efficient, QoS-Aware Edge Caching in Fog-Enhanced
                  Radio Access Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2762--2771},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064659},
	doi = {10.1109/JSAC.2021.3064659},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BharA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging video services are associated with stringent quality-of-service (QoS) requirements and place high bandwidth demands on the core networks. Edge caching can facilitate the stringent QoS demands while easing the bandwidth requirement from core networks. However, such schemes require on-field caching equipment, in which energy consumption is a function of cache utilization. Designing opportunistic caching strategies for energy efficiency is therefore essential in such schemes. This paper studies the possibilities for achieving high energy efficiency, QoS, and low bandwidth consumption from the core network, in an optically fronthauled fog-enhanced radio access network that implements edge caching. An analytical model for such a network has been derived to measure latency, bandwidth consumption, and cache utilization. It is deduced from the results that low latency (high QoS) and bandwidth consumption can be ensured in such schemes while reducing the energy consumption by up to 93%. The derived model allows to design caching strategies for addressing the trade-off between energy efficiency, QoS, and bandwidth efficiency.}
}


@article{DBLP:journals/jsac/DatsikaVRMMNV21,
	author = {Eftychia G. Datsika and
                  John S. Vardakas and
                  Kostas Ramantas and
                  Prodromos{-}Vasileios Mekikis and
                  Idelfonso Tafur Monroy and
                  Luiz Anet Neto and
                  Christos V. Verikoukis},
	title = {SDN-Enabled Resource Management for Converged Fi-Wi 5G Fronthaul},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2772--2788},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064651},
	doi = {10.1109/JSAC.2021.3064651},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DatsikaVRMMNV21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future mobile networks will offer high data rates based on high-capacity fronthaul. Current fronthaul design has two main components that communicate via the common public radio interface and fiber links, i.e., remote units (RUs) that implement simple signal processing and centralized baseband units (CBBUs) in high power-consuming data centers that perform complex network functions. Various functional splits between CBBUs and RUs are feasible, inducing trade-offs between centralization gains and bandwidth demands. This design lacks in capacity and flexibility, motivating the use of converged fiber-wireless (Fi-Wi) fronthaul with high-bandwidth fiber and millimeter-wave links, and splits that move functionalities to RUs reducing the delay demands. Further flexibility is offered by analog radio-over-fiber fronthaul that supports dynamic functional splitting via software-defined networking (SDN). Ensuring acceptable delay for all RUs, i.e., minimizing fronthaul grade-of-service (GoS), requires selection of CBBUs, channel bandwidth and functional splits of RUs. The split type affects fronthaul power consumption determining which fronthaul components are active and their processing power. Using a simulated annealing-based dynamic fronthaul resource allocation (DFRA) scheme, we jointly optimize GoS and power consumption in a novel SDN Fi-Wi fronthaul. Our results show that DFRA minimizes GoS and power consumption for all load levels outperforming baseline approaches.}
}


@article{DBLP:journals/jsac/PandeyCD21,
	author = {Gaurav Pandey and
                  Amol Choudhary and
                  Abhishek Dixit},
	title = {Wavelength Division Multiplexed Radio Over Fiber Links for 5G Fronthaul
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2789--2803},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064654},
	doi = {10.1109/JSAC.2021.3064654},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PandeyCD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-increasing demand for data rates by mobile users can be fulfilled by radio over fiber (RoF) combined with spectrally-efficient modulation techniques. Thus, RoF is critical to the design of the fronthaul for 5 th generation (5G) mobile communication networks. We propose and experimentally demonstrate a low-cost directly modulated laser (DML)-based wavelength division multiplexing (WDM)-RoF transmission system for use in next-generation 5G networks. The proposed architecture provides flexible resource allocation as well, under low and high traffic loads at the remote radio heads (RRHs). We also present robust analytical models of the proposed DML-based WDM-RoF network with spectrally-efficient M-ary-quadrature amplitude modulation ( M-QAM), beneficial for system designers. In OptiSystem software, we evaluate the error vector magnitudes (EVM) as a function of the optical link lengths, input optical power, and the received optical power with 4/16/64-QAM over the optical link. The EVMs of the link for 4/16/64-QAM are below the 3GPP standards for 5G. We find good agreement between the simulations and the analytical results. We also experimentally demonstrate a small scale model of this network with resource allocation. Such a flexible architecture is suitable for meeting the demands of femtocells in 5G fronthaul networks.}
}


@article{DBLP:journals/jsac/FabregaMNMMVMVG21,
	author = {Josep M. Fabrega and
                  Raul Mu{\~{n}}oz and
                  Laia Nadal and
                  Carlos Manso and
                  Michela Svaluto Moreolo and
                  Ricard Vilalta and
                  Ricardo Mart{\'{\i}}nez and
                  Francisco Javier Vilchez and
                  Diego Perez Galacho and
                  Salvador Sales and
                  Evangelos Grivas and
                  Jaroslaw P. Turkiewicz and
                  Simon Rommel and
                  Idelfonso Tafur Monroy},
	title = {Experimental Demonstration of Extended 5G Digital Fronthaul Over a
                  Partially-Disaggregated {WDM/SDM} Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2804--2815},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064645},
	doi = {10.1109/JSAC.2021.3064645},
	timestamp = {Mon, 08 May 2023 14:38:38 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/FabregaMNMMVMVG21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We experimentally demonstrate a 5G digital fronthaul network that relies on multi-adaptive bandwidth/bitrate variable transceivers (BVTs) and an autonomic software-defined networking (SDN) control system for partially-disaggregated wavelength division multiplexing (WDM)/space division multiplexing (SDM). Transmission of 256-QAM 760.32 MHz orthogonal frequency-division multiplexing (OFDM) radio signal is performed, with a total radio transmission capacity of 5.667 Gb/s. Digitized signal samples are carried as a 22.25 Gb/s digitized radio-over-fiber (DRoF) data stream and transmitted over a WDM/SDM infrastructure including 40-wavelength 100-GHz arrayed waveguide gratings (AWGs) and 19-core fiber. The autonomic SDN controller deploys a control loop for the multi-adaptive OFDM-based BVTs that monitors the per-subcarrier signal to noise ratio (SNR) and assigns the optimal constellation based on the actual signal degradation. An error vector magnitude (EVM) below the targeted 2.1% is achieved while setting up connections in less than 5 s.}
}


@article{DBLP:journals/jsac/TsakyridisRKODR21,
	author = {Apostolos Tsakyridis and
                  Eugenio Ruggeri and
                  George Kalfas and
                  Ruud M. Oldenbeuving and
                  Paul W. L. van Dijk and
                  Chris G. H. Roeloffzen and
                  Yigal Leiba and
                  Amalia N. Miliou and
                  Nikos Pleros and
                  Christos Vagionas},
	title = {Reconfigurable Fiber Wireless IFoF Fronthaul With 60 GHz Phased Array
                  Antenna and Silicon Photonic {ROADM} for 5G mmWave C-RANs},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2816--2826},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064649},
	doi = {10.1109/JSAC.2021.3064649},
	timestamp = {Thu, 16 Sep 2021 18:06:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TsakyridisRKODR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We demonstrate experimentally a bandwidth-reconfigurable mmWave Fiber Wireless (FiWi) fronthaul bus topology for spectrally efficient and flexibly reconfigurable 5G Centralized-Radio Access Networks (C-RAN). The proposed fronthaul architecture includes four 1 Gb/s Intermediate Frequency over Fiber (IFoF) channels that can be flexibly allocated among two in-series Reconfigurable Optical Add/Drop Multiplexer (ROADM) integrated nodes, supporting in total 8 V-band 32-element Phased Array Antenna (PAA) terminals. The ROADM was fabricated as an integrated photonic device exploiting the ultra-low loss Si 3 N 4 TriPleX waveguide integration platform and an architectural layout based on cascaded MZI interleavers. The device has flat top response of 32.5 GHz with a Free Spectral Range (FSR) of 100 GHz and fiber-to-fiber losses of 5 dB, while the V-band PAA supports analog RF beamsteering capabilities within a 90°-sector and 1m wireless distance. Each of the FiWi links carries a 250 MBd QAM16 waveform enabling a total of 1 Gb/s rate per end user beam, complying with the 5G Key Performance Indicator (KPI) user-rate requirement. Bandwidth-reconfigurability is experimentally demonstrated by selectively dropping channels either at the first or at the second ROADM node, allowing in this way the bandwidth allocation to be flexibly defined between two different network segments. Both uplink and downlink performance are experimentally validated for different ratios of bandwidth allocation among the two nodes, revealing Error Vector Magnitude (EVM) values that meet the respective 3GPP signal quality specifications. The two-stage FiWi IFoF/mmWave fronthaul bus topology, based on a miniaturized, integrated, low loss Si 3 N 4 ROADM and supporting high-capacity wireless beamsteering capability can form a promising roadmap towards flexible and reconfigurable 5G C-RAN architectures.}
}


@article{DBLP:journals/jsac/MilovancevVLZS21,
	author = {Dinka Milovancev and
                  Nemanja Vokic and
                  David L{\"{o}}schenbrand and
                  Thomas Zemen and
                  Bernhard Schrenk},
	title = {Analog Coherent-Optical Mobile Fronthaul With Integrated Photonic
                  Beamforming},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2827--2837},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064640},
	doi = {10.1109/JSAC.2021.3064640},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MilovancevVLZS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a mobile fronthaul methodology for the analogue optical transmission of native radio signals with integrated photonic true-time delay for the beam steering of phased-array antennas. Laser-based coherent homodyne detectors and the delay dissemination through ultra-dense wavelength division multiplexing ensure a low complexity at the optical layer. We experimentally demonstrate beamsteering for a linear phased-array antenna with a 1×3 configuration at 3.5 GHz carrier frequency and prove native radio signal transmission over the 14.3-km reach coherent optical fronthaul at a low end-to-end error vector magnitude of 3.3%. Experimental results are found to stand in good agreement with theoretical predictions.}
}


@article{DBLP:journals/jsac/TsaiWCCL21,
	author = {Cheng{-}Ting Tsai and
                  Huai{-}Yung Wang and
                  Yu{-}Chieh Chi and
                  Chih{-}Hsien Cheng and
                  Gong{-}Ru Lin},
	title = {Quad-Mode {VCSEL} Optical Carrier for Long-Reach K\({}_{\mbox{a}}\)-Band
                  Millimeter-Wave Over Fiber Link},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2838--2848},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064644},
	doi = {10.1109/JSAC.2021.3064644},
	timestamp = {Thu, 16 Sep 2021 18:06:19 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TsaiWCCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A quad-mode optical carrier generated by mode-deviated injection-locking a vertical-cavity surface-emitting laser (VCSEL) at a beating frequency of 40 GHz is demonstrated for establishing a long-reach millimeter-wave over fiber (MMWoF) link, in which only one mode of the quad-mode optical carriers is modulated by the 16-quadrature amplitude modulation (16-QAM) orthogonal frequency division multiplexing (OFDM) data stream to achieve an optical single-carrier modulation (OSCM). Such an OSCM operation reduces the long-reach single-mode-fiber (SMF) transmission induced RF power fading effect to significantly suppress the frequency notch in the modulated spectrum of the delivered QAM-OFDM data after remotely heterodyne detection. With destructively interfered beating the quad-mode optical carrier at the remote node, the peak power of the optically heterodyned MMW central carrier can be attenuated by 38.3 dB, which saves the gain of the photodetector and electrical amplifier to efficiently boost the carried QAM-OFDM data. This architecture ensures sufficient power of the 16-QAM OFDM data after optoelectrical conversion and facilitates the quad-mode optical carrier based MMWoF link to achieve 50-km wired SMF and 10-m wireless MMW transmissions with the error vector magnitude of 17.2%, signal-to-noise ratio of 15.28 dB, and bit error rate of 3.5×10 -3 at a data rate of 12 Gbps.}
}


@article{DBLP:journals/jsac/PellePSC21,
	author = {Istv{\'{a}}n Pelle and
                  Francesco Paolucci and
                  Bal{\'{a}}zs Sonkoly and
                  Filippo Cugini},
	title = {Latency-Sensitive Edge/Cloud Serverless Dynamic Deployment Over Telemetry-Based
                  Packet-Optical Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2849--2863},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064655},
	doi = {10.1109/JSAC.2021.3064655},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/PellePSC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The serverless technology, introduced for data center operation, represents an attractive technology for latency-sensitive applications operated at the edge, enabling a resource-aware deployment accounting for limited edge computing resources or end-to-end network congestion to the cloud. This paper presents and validates a framework for automated deployment and dynamic reconfiguration of serverless functions at either the edge or cloud. The framework relies on extensive telemetry data retrieved from both the computing and packet-optical network infrastructure and operates on diverse Amazon Web Services technologies, including Greengrass on the edge. Experimental demonstration with a latency-sensitive serverless application is then provided, showing fast dynamic reconfiguration capabilities, e.g., enabling even zero outage time under certain conditions.}
}


@article{DBLP:journals/jsac/VelascoBSFCCPNR21,
	author = {Luis Velasco and
                  Sima Barzegar and
                  Diogo Gon{\c{c}}alo Sequeira and
                  Alessio Ferrari and
                  Nelson Costa and
                  Vittorio Curri and
                  Jo{\~{a}}o Pedro and
                  Antonio Napoli and
                  Marc Ruiz},
	title = {Autonomous and Energy Efficient Lightpath Operation Based on Digital
                  Subcarrier Multiplexing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2864--2877},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064698},
	doi = {10.1109/JSAC.2021.3064698},
	timestamp = {Tue, 15 Aug 2023 17:02:43 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/VelascoBSFCCPNR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The massive deployment of 5G and beyond will require high capacity and low latency connectivity services, so network operators will have either to overprovision capacity in their transport networks or to upgrade the optical network controllers to make decisions nearly in real time; both solutions entail high capital and operational expenditures. A different approach could be to move the decision making toward the nodes and subsystems, so they can adapt dynamically the capacity to the actual needs and thus reduce operational costs in terms of energy consumption. To achieve this, several technological challenges need to be addressed. In this paper, we focus on the autonomous operation of Digital Subcarrier Multiplexing (DSCM) systems, which enable the transmission of multiple and independent subcarriers (SC). Herein, we present several solutions enabling the autonomous DSCM operation, including: i) SC quality of transmission estimation; ii) autonomous SC operation at the transmitter side and blind SC configuration recognition at the receiver side; and iii) intent-based capacity management implemented through Reinforcement Learning. We provide useful guidelines for the application of autonomous SC management supported by the extensive results presented.}
}


@article{DBLP:journals/jsac/ChenPLY21,
	author = {Xiaoliang Chen and
                  Roberto Proietti and
                  Che{-}Yu Liu and
                  S. J. Ben Yoo},
	title = {A Multi-Task-Learning-Based Transfer Deep Reinforcement Learning Design
                  for Autonomic Optical Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2878--2889},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064657},
	doi = {10.1109/JSAC.2021.3064657},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenPLY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep reinforcement learning (DRL) enables autonomic optical networking by allowing the network control and management systems to self-learn successful networking policies from operational experiences. This paper proposes a transfer learning approach for effective and scalable DRL in optical networks. We first present a modular DRL agent design to facilitate retrieving and transferring relevant knowledge between tasks requiring different dimensions of network state data. In particular, we partition network state data into common states, which contain generic information critical to multiple tasks (e.g., the spectrum utilization on fiber links), and task-specific states that are only used by a specific task (e.g., the utilization of virtual network functions). Separate neural network blocks are employed to process different state data. Based on the modular agent design, a multi-task learning (MTL) aided knowledge transferring scheme is proposed. The proposed scheme trains an MTL agent that can master multiple tasks simultaneously and thus enables to learn and transfer better-generalized knowledge across tasks. We perform case studies on the proposed transfer DRL approach considering two scenarios, namely, knowledge transferring between routing, modulation and spectrum assignment (RMSA) tasks for different networks and knowledge transferring from RMSA tasks to anycast service provisioning tasks. The DRL designs for RMSA and anycast service provisioning, including the state, action, and reward formulations and the training mechanisms, are also elaborated. Performance evaluations under both scenarios show that the proposed approach can effectively expedite the training processes of the target tasks and improve the ultimate service throughput.}
}


@article{DBLP:journals/jsac/ArfaouiSTGASH21,
	author = {Mohamed Amine Arfaoui and
                  Mohammad Dehghani Soltani and
                  Iman Tavakkolnia and
                  Ali Ghrayeb and
                  Chadi M. Assi and
                  Majid Safari and
                  Harald Haas},
	title = {Invoking Deep Learning for Joint Estimation of Indoor LiFi User Position
                  and Orientation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {9},
	pages = {2890--2905},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3064637},
	doi = {10.1109/JSAC.2021.3064637},
	timestamp = {Thu, 16 Sep 2021 18:06:20 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ArfaouiSTGASH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Light-fidelity (LiFi) is a fully-networked bidirectional optical wireless communication (OWC) technology that is considered as a promising solution for high-speed indoor connectivity. In this paper, the joint estimation of user 3D position and user equipment (UE) orientation in indoor LiFi systems with unknown emission power is investigated. Existing solutions for this problem assume either ideal LiFi system settings or perfect knowledge of the UE states, rendering them unsuitable for realistic LiFi systems. In addition, these solutions consider the non-line-of-sight (NLOS) links of the LiFi channel gain as a source of deterioration for the estimation performance instead of harnessing these components in improving the position and the orientation estimation performance. This is mainly due to the lack of appropriate estimation techniques that can extract the position and orientation information hidden in these components. In this paper, and against the above limitations, the UE is assumed to be connected with at least one access point (AP), i.e., at least one active LiFi link. Fingerprinting is employed as an estimation technique and the received signal-to-noise ratio (SNR) is used as an estimation metric, where both the line-of-sight (LOS) and NLOS components of the LiFi channel are considered. Motivated by the success of deep learning techniques in solving several complex estimation and prediction problems, we employ two deep artificial neural network (ANN) models, one based on the multilayer perceptron (MLP) and the second on the convolutional neural network (CNN), that can map efficiently the instantaneous received SNR with the user 3D position and the UE orientation. Through numerous examples, we investigate the performance of the proposed schemes in terms of the average estimation error, precision, computational time, and the bit error rate. We also compare this performance to that of the k-nearest neighbours (KNN) scheme, which is widely used in solving wireless localization problems. It is demonstrated that the proposed schemes achieve significant gains and are superior to the KNN scheme.}
}


@article{DBLP:journals/jsac/WuXZNASS21,
	author = {Qingqing Wu and
                  Jie Xu and
                  Yong Zeng and
                  Derrick Wing Kwan Ng and
                  Naofal Al{-}Dhahir and
                  Robert Schober and
                  A. Lee Swindlehurst},
	title = {Guest Editorial Special Issue on {UAV} Communications in 5G and Beyond
                  Networks - Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2907--2911},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3090875},
	doi = {10.1109/JSAC.2021.3090875},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuXZNASS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communication is an essential technology to unlock the full potential of unmanned aerial vehicles (UAVs) in numerous applications and has thus received unprecedented attention recently. Although technologies such as direct link, WiFi, and satellite communications are still useful in some remote scenarios where cellular services are unavailable, it is believed that exploiting the thriving 5G and beyond cellular networks to support UAV communications is the most promising and cost-effective approach, especially when the number of UAVs grows dramatically. On the one hand, to guarantee safe and efficient flight operations of multiple UAVs, it is of paramount importance to provide secure and ultra-reliable communication links between the UAVs and their ground pilots or control stations for conveying command and control signals, especially in beyond-visual-line-of-sight (BVLOS) scenarios. On the other hand, because of advances in communication equipment miniaturization as well as UAV manufacturing, mounting compact and lightweight base stations (BSs) or relays on UAVs becomes increasingly feasible. This has led to two promising research paradigms for UAV communications, namely, UAV-assisted cellular communications and cellular-connected UAVs, where UAVs are integrated into cellular networks as aerial communication platforms and aerial users, respectively. As such, integrating UAVs into cellular networks is believed to be a win-win technology for both UAV-related industries and cellular network operators, which not only creates plenty of new business opportunities but also benefits the communication performance of 3-D wireless networks. In addition, UAV related sensing and computing are also helpful for achieving efficient and reliable communication (e.g., in avoiding coverage holes) as well as smart UAV coordination, positioning, and trajectory design. However, 5G and beyond wireless networks with UAVs significantly differs from traditional communication systems, because of the high altitude and high maneuverability of UAVs, the unique UAV-ground channels, the diversified quality of service (QoS) requirements for downlink command and control (C&C) and uplink mission-related data transmission, the stringent constraints imposed by the size, weight, and power (SWAP) limitations of UAVs, as well as the new design degrees of freedom enabled by joint UAV mobility control and communication resource allocation.}
}


@article{DBLP:journals/jsac/WuXZNASS21a,
	author = {Qingqing Wu and
                  Jie Xu and
                  Yong Zeng and
                  Derrick Wing Kwan Ng and
                  Naofal Al{-}Dhahir and
                  Robert Schober and
                  A. Lee Swindlehurst},
	title = {A Comprehensive Overview on 5G-and-Beyond Networks With UAVs: From
                  Communications to Sensing and Intelligence},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2912--2945},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088681},
	doi = {10.1109/JSAC.2021.3088681},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuXZNASS21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the advancements in cellular technologies and the dense deployment of cellular infrastructure, integrating unmanned aerial vehicles (UAVs) into the fifth-generation (5G) and beyond cellular networks is a promising solution to achieve safe UAV operation as well as enabling diversified applications with mission-specific payload data delivery. In particular, 5G networks need to support three typical usage scenarios, namely, enhanced mobile broadband (eMBB), ultra-reliable low-latency communications (URLLC), and massive machine-type communications (mMTC). On the one hand, UAVs can be leveraged as cost-effective aerial platforms to provide ground users with enhanced communication services by exploiting their high cruising altitude and controllable maneuverability in three-dimensional (3D) space. On the other hand, providing such communication services simultaneously for both UAV and ground users poses new challenges due to the need for ubiquitous 3D signal coverage as well as the strong air-ground network interference. Besides the requirement of high-performance wireless communications, the ability to support effective and efficient sensing as well as network intelligence is also essential for 5G-and-beyond 3D heterogeneous wireless networks with coexisting aerial and ground users. In this paper, we provide a comprehensive overview of the latest research efforts on integrating UAVs into cellular networks, with an emphasis on how to exploit advanced techniques (e.g., intelligent reflecting surface, short packet transmission, energy harvesting, joint communication and radar sensing, and edge intelligence) to meet the diversified service requirements of next-generation wireless systems. Moreover, we highlight important directions for further investigation in future work.}
}


@article{DBLP:journals/jsac/WangMLWLC21,
	author = {Jin{-}Yuan Wang and
                  Yang Ma and
                  Rong{-}Rong Lu and
                  Jun{-}Bo Wang and
                  Min Lin and
                  Julian Cheng},
	title = {Hovering UAV-Based {FSO} Communications: Channel Modelling, Performance
                  Analysis, and Parameter Optimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2946--2959},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088656},
	doi = {10.1109/JSAC.2021.3088656},
	timestamp = {Fri, 29 Jul 2022 15:16:51 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangMLWLC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Relay-assisted free-space optical (FSO) communication systems are exploited as a means to mitigate the limiting effects of the turbulence induced atmospheric scintillation. However, conventional ground relays are stationary, and their optimal placement is not always feasible. Due to their mobility and flexibility, unmanned aerial vehicles (UAVs) provide new opportunities for FSO relaying systems. In this paper, a hovering UAV-based serial FSO decode-and-forward relaying system is investigated. In the channel modelling for such a system, four types of impairments (i.e., atmospheric loss, atmospheric turbulence, pointing error, and link interruption due to angle-of-arrival fluctuation) are considered. Based on the proposed channel model, a tractable expression for the probability density function of the total channel gain is obtained. Closed-form expressions of the link outage probability and end-to-end outage probability are derived. Asymptotic outage performance bounds for each link and the overall system are also presented to reveal insights into the impacts of different impairments. To improve system performance, we optimize the beam width, field-of-view and UAVs' locations. Numerical results show that the derived theoretical expressions are accurate to evaluate the outage performance of the system. Moreover, the proposed optimization schemes are efficient and can improve performance significantly.}
}


@article{DBLP:journals/jsac/MaAHZY21,
	author = {Zhangfeng Ma and
                  Bo Ai and
                  Ruisi He and
                  Zhangdui Zhong and
                  Mi Yang},
	title = {A Non-Stationary Geometry-Based {MIMO} Channel Model for Millimeter-Wave
                  {UAV} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2960--2974},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088659},
	doi = {10.1109/JSAC.2021.3088659},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MaAHZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) communications are expected to play a major role in future space-air-ground integrated networks (SAGINs). In this paper, a geometric three-dimensional (3D) non-stationary channel model operating at millimeter-wave (mmWave) band is proposed for wideband UAV multiple-input multiple-output (MIMO) communications based on a multiple-layer cylinder reference model, where both stationary and moving clusters around transmitter (Tx) and receiver (Rx) are considered. Unlike the existing UAV-based GBSMs, the proposed model considers both local and far clusters in the propagation environments. On this basis, a continuous-time Markov model with two states is used to model the dynamic properties of clusters (i.e., clusters appear/disappear with time), and the closed-form expressions of the survival probabilities of clusters are derived. Furthermore, we derive and investigate some significant statistical properties, including space-time-frequency correlation function, quasi-stationary interval, and Doppler power spectrum. Numerical results show that the local mobile cluster (LMC) component leads to higher time correlation compared with the local stationary cluster (LSC) component. In addition, it is found that the LMC component leads to larger quasi-stationary interval compared with the LSC component. Finally, it is found that the motion of transceivers and clusters, and the changes of carrier frequency introduce significant fluctuations in Doppler power spectrum. These observations and conclusions can be considered as a guidance for mmWave UAV MIMO system design.}
}


@article{DBLP:journals/jsac/CaiIRKWTM21,
	author = {Xuesong Cai and
                  Tomasz Izydorczyk and
                  Jos{\'{e}} Rodr{\'{\i}}guez{-}Pi{\~{n}}eiro and
                  Istv{\'{a}}n Zsolt Kov{\'{a}}cs and
                  Jeroen Wigard and
                  Fernando M. L. Tavares and
                  Preben E. Mogensen},
	title = {Empirical Low-Altitude Air-to-Ground Spatial Channel Characterization
                  for Cellular Networks Connectivity},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2975--2991},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088715},
	doi = {10.1109/JSAC.2021.3088715},
	timestamp = {Wed, 15 Dec 2021 10:32:22 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CaiIRKWTM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cellular-connected unmanned aerial vehicles (UAVs) have recently attracted a surge of interest in both academia and industry. Understanding the air-to-ground (A2G) propagation channels is essential to enable reliable and/or high-throughput communications for UAVs and protect the ground user equipments (UEs). In this contribution, a recently conducted measurement campaign for the A2G channels is introduced. A uniform circular array (UCA) with 16 antenna elements was employed to collect the downlink signals of two different Long Term Evolution (LTE) networks, at the heights of 0-40m in three different, namely rural, urban and industrial scenarios. The channel impulse responses (CIRs) have been extracted from the received data, and the spatial, including angular, parameters of the multipath components in individual channels were estimated according to a high-resolution-parameter-estimation (HRPE) principle. Based on the HRPE results, clusters of multipath components were further identified. Finally, comprehensive spatial channel characteristics were investigated in the composite and cluster levels at different heights in the three scenarios.}
}


@article{DBLP:journals/jsac/LiuWCHB21,
	author = {Yu Liu and
                  Cheng{-}Xiang Wang and
                  Hengtai Chang and
                  Yubei He and
                  Ji Bian},
	title = {A Novel Non-Stationary 6G {UAV} Channel Model for Maritime Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {2992--3005},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088664},
	doi = {10.1109/JSAC.2021.3088664},
	timestamp = {Mon, 05 Feb 2024 20:23:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuWCHB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To achieve space-air-ground-sea integrated communication networks for future sixth generation (6G) communications, unmanned aerial vehicle (UAV) communications applying to maritime scenarios serving as mobile base stations have recently attracted more attentions. The UAV-to-ship channel modeling is the fundamental for the system design, testing, and performance evaluation of UAV communication systems in maritime scenarios. In this paper, a novel non-stationary multi-mobility UAV-to-ship channel model is proposed, consisting of three kinds of components, i.e., the line-of-sight (LoS) component, the single-bounce (SB) components resulting from the fluctuation of sea water, and multi-bounce (MB) components introduced by the waveguide effect over the sea surface. In the proposed model, the UAV as the transmitter (Tx), the ship as the receiver (Rx), and the clusters between the Tx and Rx, can be seen as moving with arbitrary velocities and arbitrary directions. Then, some typical statistical properties of the proposed UAV-to-ship channel model, including the temporal autocorrelation function (ACF), spatial cross-correlation function (CCF), Doppler power spectrum density (PSD), delay PSD, angular PSD, stationary interval, and root mean square (RMS) delay spread, are derived and investigated. Finally, by comparing with the available measurement data, the accuracy of proposed channel model is validated.}
}


@article{DBLP:journals/jsac/HannaKC21,
	author = {Samer S. Hanna and
                  Enes Krijestorac and
                  Danijela Cabric},
	title = {{UAV} Swarm Position Optimization for High Capacity {MIMO} Backhaul},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3006--3021},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088677},
	doi = {10.1109/JSAC.2021.3088677},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HannaKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A swarm of cooperating UAVs communicating with a distant multiantenna ground station can leverage MIMO spatial multiplexing to scale the capacity. Due to the line-of-sight propagation between the swarm and the ground station, the MIMO channel is highly correlated, leading to limited multiplexing gains. In this paper, we optimize the UAV positions to attain the maximum MIMO capacity given by the single user bound. An infinite set of UAV placements that attains the capacity bound is first derived. Given an initial swarm placement, we formulate the problem of minimizing the distance traveled by the UAVs to reach a placement within the capacity maximizing set of positions. An offline centralized solution to the problem using block coordinate descent is developed assuming known initial positions of UAVs. We also propose an online distributed algorithm, where the UAVs iteratively adjust their positions to maximize the capacity. Our proposed approaches are shown to significantly increase the capacity at the expense of a bounded translation from the initial UAV placements. This capacity increase persists when using a massive MIMO ground station. Using numerical simulations, we show the robustness of our approaches in a Rician channel under UAV motion disturbances.}
}


@article{DBLP:journals/jsac/GaoLJM21,
	author = {Ning Gao and
                  Xiao Li and
                  Shi Jin and
                  Michail Matthaiou},
	title = {3-D Deployment of {UAV} Swarm for Massive {MIMO} Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3022--3034},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088668},
	doi = {10.1109/JSAC.2021.3088668},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GaoLJM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the uplink transmission between a multi-antenna ground station and an unmanned aerial vehicle (UAV) swarm. The UAVs are assumed as intelligent agents, which can explore their optimal three dimensional (3-D) deployment to maximize the channel capacity of the multiple input multiple output (MIMO) system. Specifically, considering the limitations of each UAV in accessing the global information of the network, we focus on a decentralized control strategy by noting that each UAV in the swarm can only utilize the local information to achieve the optimal 3-D deployment. In this case, the optimization problem can be divided into several optimization sub-problems with respect to the rank function. Due to the non-convex nature of the rank function and the fact that the optimization sub-problems are coupled, the original problem is NP-hard and, thus, cannot be solved with standard convex optimization solvers. Interestingly, we can relax the constraint condition of each sub-problem and solve the optimization problem by a formulated UAVs channel capacity maximization game. We analyze such game according to the designed reward function and the potential function. Then, we discuss the existence of the pure Nash equilibrium in the game. To achieve the best Nash equilibrium of the MIMO system, we develop a decentralized learning algorithm, namely decentralized UAVs channel capacity learning. The details of the algorithm are provided, and then, the convergence, the effectiveness and the computational complexity are analyzed, respectively. Moreover, we give some insightful remarks based on the proofs and the theoretical analysis. Also, extensive simulations illustrate that the developed learning algorithm can achieve a high MIMO channel capacity by optimizing the 3-D UAV swarm deployment with the local information.}
}


@article{DBLP:journals/jsac/CaoYHYRNH21,
	author = {Xuelin Cao and
                  Bo Yang and
                  Chongwen Huang and
                  Chau Yuen and
                  Marco Di Renzo and
                  Dusit Niyato and
                  Zhu Han},
	title = {Reconfigurable Intelligent Surface-Assisted Aerial-Terrestrial Communications
                  via Multi-Task Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3035--3050},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088634},
	doi = {10.1109/JSAC.2021.3088634},
	timestamp = {Wed, 05 Jan 2022 14:30:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CaoYHYRNH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The aerial-terrestrial communication system constitutes an efficient paradigm for supporting and complementing terrestrial communications. However, the benefits of such a system cannot be fully exploited, especially when the line-of-sight (LoS) transmissions are prone to severe deterioration due to complex propagation environments in urban areas. The emerging technology of reconfigurable intelligent surfaces (RISs) has recently become a potential solution to mitigate propagation-induced impairments and improve wireless network coverage. Motivated by these considerations, in this paper, we address the coverage and link performance problems of the aerial-terrestrial communication system by proposing an RIS-assisted transmission strategy. In particular, we design an adaptive RIS-assisted transmission protocol, in which the channel estimation, transmission strategy, and data transmission are independently implemented in a frame. On this basis, we formulate an RIS-assisted transmission strategy optimization problem as a mixed-integer non-linear program (MINLP) to maximize the overall system throughput. We then employ multi-task learning to speed up the solution to the problem. Benefiting from multi-task learning, the computation time is reduced by about four orders of magnitude. Numerical results show that the proposed RIS-assisted transmission protocol significantly improves the system throughput and reduces the transmit power.}
}


@article{DBLP:journals/jsac/MuLGLP21,
	author = {Xidong Mu and
                  Yuanwei Liu and
                  Li Guo and
                  Jiaru Lin and
                  H. Vincent Poor},
	title = {Intelligent Reflecting Surface Enhanced Multi-UAV {NOMA} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3051--3066},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088679},
	doi = {10.1109/JSAC.2021.3088679},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MuLGLP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS) enhanced multi-unmanned aerial vehicle (UAV) non-orthogonal multiple access (NOMA) networks are investigated. A new transmission framework is proposed, where multiple UAV-mounted base stations employ NOMA to serve multiple groups of ground users with the aid of an IRS. The three-dimensional (3D) placement and transmit power of UAVs, the reflection matrix of the IRS, and the NOMA decoding orders among users are jointly optimized for maximization of the sum rate of considered networks. To tackle the formulated mixed-integer non-convex optimization problem with coupled variables, a block coordinate descent (BCD)-based iterative algorithm is developed. Specifically, the original problem is decomposed into three subproblems, which are alternately solved by exploiting the penalty-based method and the successive convex approximation technique. The proposed BCD-based algorithm is demonstrated to be able to obtain a stationary point of the original problem with polynomial time complexity. Numerical results show that: 1) the proposed NOMA-IRS scheme for multi-UAV networks achieves a higher sum rate compared to the benchmark schemes, i.e., orthogonal multiple access (OMA)-IRS and NOMA without IRS; 2) the use of IRS is capable of providing performance gain for multi-UAV networks by both enhancing channel qualities of UAVs to their served users and mitigating the inter-UAV interference; and 3) optimizing the UAV placement can make the sum rate gain brought by NOMA more distinct due to the flexible decoding order design.}
}


@article{DBLP:journals/jsac/NewLNSD21,
	author = {Wee{-}Kiat New and
                  Chee Yen Leow and
                  Keivan Navaie and
                  Yanshi Sun and
                  Zhiguo Ding},
	title = {Interference-Aware {NOMA} for Cellular-Connected UAVs: Stochastic
                  Geometry Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3067--3080},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088671},
	doi = {10.1109/JSAC.2021.3088671},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/NewLNSD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficiency of cellular-connected UAVs is challenged by spectrum inefficiency, limited number of concurrent connectivity, and strong interference. To overcome these issues, in this paper, we study the performance of downlink non-orthogonal multiple access for cellular-connected UAVs. We develop a novel framework based on stochastic geometry for the co-existence of aerial users (AUs) and terrestrial users (TUs), where the spatial distribution of the base stations (BSs) follows a Poisson Point Process. In our analysis, two user association policies and two types of receive antennas are considered while an inter-cell interference coordination (ICIC) technique is also in place. As the main performance measures, we then analytically derive the coverage probability and average rate of AUs and TUs. These derivations are then used to provide quantitative insights on the impact of different system parameters and settings including AU’s altitude, TU’s distance from the BS, power allocation, successive interference cancellation (SIC) constraints, user association policy, antenna beamwidth, and the number of coordinated BSs. Based on our analysis we then propose an interference-aware scheme based on maximum-SINR user association, directional antenna, and ICIC. A benchmark scheme based on minimum-distance user association, omni-directional antenna, and without ICIC is considered. Compared to the benchmark scheme, our proposed scheme improves the AU’s coverage probability by threefold and TU’s average rate by six-fold. Compared to the orthogonal multiple access, our proposed scheme trades off a slight reduction in the AU’s coverage probability (~1%) with a significant increase in the achieved rate of the TUs (603Kbps/resource block).}
}


@article{DBLP:journals/jsac/YuanHS21,
	author = {Xiaopeng Yuan and
                  Yulin Hu and
                  Anke Schmeink},
	title = {Joint Design of {UAV} Trajectory and Directional Antenna Orientation
                  in UAV-Enabled Wireless Power Transfer Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3081--3096},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088682},
	doi = {10.1109/JSAC.2021.3088682},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YuanHS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we investigate an unmanned aerial vehicle (UAV)-enabled wireless power transfer (WPT) network with multiple ground sensor nodes (SNs). A UAV is operated at a fixed altitude with a directional antenna array and is designed to wirelessly transfer energy to the SNs. We consider a non-linear energy harvesting (EH) model and a directional antenna structure of uniform linear array (ULA) where we apply an analog directional beamforming scheme. Taking the fairness issue into account, we consider a problem aiming at maximizing the minimum harvested energy among all SNs during a fixed time period by jointly optimizing the UAV trajectory and the orientation of the directional antenna on the UAV. However, the complex antenna pattern expression of analog directional beamforming and the implicit non-linear function in the EH model introduce significant difficulties in handling the non-convex problem of the joint design. To tackle these difficulties, we propose and adopt a modified approximate antenna pattern model, i.e., a modified cosine antenna pattern, and reformulate the original problem via quantizing the UAV trajectory in the time domain. Later, by employing a convex property in the EH model and a proved lemma, we successfully construct a tight convex approximation for the reformulated problem, based on which the problem can be solved via a proposed iterative algorithm and the objective converges to an efficient suboptimal solution. Finally, we provide numerical results to confirm the convergence of the proposed algorithm, examine the approximation error and evaluate the system performance. The results show the performance advantage of the directional antenna in UAV-enabled WPT networks than the omni-directional antenna case, and illustrate how the directional antenna of the UAV overcomes its coverage limitation}
}


@article{DBLP:journals/jsac/LiuXLNFL21,
	author = {Yuan Liu and
                  Ke Xiong and
                  Yang Lu and
                  Qiang Ni and
                  Pingyi Fan and
                  Khaled Ben Letaief},
	title = {UAV-Aided Wireless Power Transfer and Data Collection in Rician Fading},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3097--3113},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088693},
	doi = {10.1109/JSAC.2021.3088693},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuXLNFL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A UAV-aided wireless power transfer and data collection network is studied, where it is assumed that when the harvested energy at the sensor node (SN) cannot surpass its circuit activation threshold or the received data rate at UAV falls below a minimal required rate threshold, the information outage occurs. The closed-form expressions of energy outage probability and rate outage probability are derived at first, and then the overall outage probability and coverage performance of the system are analyzed. Based on which, an optimization problem is formulated to minimize the overall outage probability by optimizing UAV’s elevation angle and the time splitting (TS) factor. Since the problem is non-convex and has no known solution, an alternating optimization (AO)-based algorithm with Golden-section (GS) based linear search method is designed to find the global optimal solution. In order to explore the maximum coverage area of the UAV for a given tolerable outage probability, another optimization problem is also formulated to maximize the coverage range by optimizing UAV’s elevation angle. By using Karush-Kuhn-Tucker (KKT) conditions, the closed-form solution of the optimal elevation angle for maximizing the coverage area is derived. Monte Carlo simulations verify the accuracy of the derived closed-form expression of the overall outage probability and the semi-closed-form expressions of the optimum UAV’s elevation angle and TS factor. It shows that there exist a unique optimum elevation angle and the TS factor to achieve the minimum overall outage probability, and significant performance gain can be obtained by using our proposed optimization scheme. The developed theoretical results can be useful to the design of UAV-aided wireless communication systems with wireless power transfer.}
}


@article{DBLP:journals/jsac/ZhengZA21,
	author = {Jiakang Zheng and
                  Jiayi Zhang and
                  Bo Ai},
	title = {{UAV} Communications With WPT-Aided Cell-Free Massive {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3114--3128},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088632},
	doi = {10.1109/JSAC.2021.3088632},
	timestamp = {Mon, 21 Aug 2023 15:51:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengZA21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell-free (CF) massive multiple-input multiple-output (MIMO) is a promising solution to provide uniform good performance for unmanned aerial vehicle (UAV) communications. In this paper, we propose the UAV communication with wireless power transfer (WPT) aided CF massive MIMO systems, where the harvested energy (HE) from the downlink WPT is used to support both uplink data and pilot transmission. We derive novel closed-form downlink HE and uplink spectral efficiency (SE) expressions that take hardware impairments of UAV into account. UAV communications with current small cell (SC) and cellular massive MIMO enabled WPT systems are also considered for comparison. It is significant to show that CF massive MIMO achieves two and five times higher 95%-likely uplink SE than the ones of SC and cellular massive MIMO, respectively. Besides, the large-scale fading decoding receiver cooperation can reduce the interference of the terrestrial user. Moreover, the maximum SE can be achieved by changing the time-splitting fraction. We prove that the optimal time-splitting fraction for maximum SE is determined by the number of antennas, altitude and hardware quality factor of UAVs. Furthermore, we propose three UAV trajectory design schemes to improve the SE. It is interesting that the angle search scheme performs best than both AP search and line path schemes. Finally, simulation results are presented to validate the accuracy of our expressions.}
}


@article{DBLP:journals/jsac/HanBWLCZ21,
	author = {Rui Han and
                  Lin Bai and
                  Yongqing Wen and
                  Jianwei Liu and
                  Jinho Choi and
                  Wei Zhang},
	title = {UAV-Aided Backscatter Communications: Performance Analysis and Trajectory
                  Optimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3129--3143},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088676},
	doi = {10.1109/JSAC.2021.3088676},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HanBWLCZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In 5G massive machine-type communication (mMTC), power-limited or battery-free parasite devices such as radio frequency identification (RFID) tags, can use the transmitted signals from host devices as ambient signals for backscatter communications to send information to a base station (BS). Unmanned aerial vehicles (UAVs) can be employed as host devices to help transmissions of parasite devices due to the advantages of high mobility and low operating cost. In this paper, we propose a signal detection approach based on the central limit theorem to detect the presence of parasite devices and separate parasite signals from host signals. Then, closed-form expressions for the probability of error detection and the bit error rate (BER) are derived. Moreover, the trajectory planning of multiple UAVs is optimized with the consideration of minimizing the energy consumption of UAV swarms to serve parasite devices. Theoretical and simulation results show that our proposed method provides good detection performance for parasite devices. It also shows that the trajectory planning of multiple UAVs is optimized.}
}


@article{DBLP:journals/jsac/YangZXLSX21,
	author = {Helin Yang and
                  Jun Zhao and
                  Zehui Xiong and
                  Kwok{-}Yan Lam and
                  Sumei Sun and
                  Liang Xiao},
	title = {Privacy-Preserving Federated Learning for UAV-Enabled Networks: Learning-Based
                  Joint Scheduling and Resource Management},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3144--3159},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088655},
	doi = {10.1109/JSAC.2021.3088655},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangZXLSX21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are capable of serving as flying base stations (BSs) for supporting data collection, machine learning (ML) model training, and wireless communications. However, due to the privacy concerns of devices and limited computation or communication resource of UAVs, it is impractical to send raw data of devices to UAV servers for model training. Moreover, due to the dynamic channel condition and heterogeneous computing capacity of devices in UAV-enabled networks, the reliability and efficiency of data sharing require to be further improved. In this paper, we develop an asynchronous federated learning (AFL) framework for multi-UAV-enabled networks, which can provide asynchronous distributed computing by enabling model training locally without transmitting raw sensitive data to UAV servers. The device selection strategy is also introduced into the AFL framework to keep the low-quality devices from affecting the learning efficiency and accuracy. Moreover, we propose an asynchronous advantage actor-critic (A3C) based joint device selection, UAVs placement, and resource management algorithm to enhance the federated convergence speed and accuracy. Simulation results demonstrate that our proposed framework and algorithm achieve higher learning accuracy and faster federated execution time compared to other existing solutions.}
}


@article{DBLP:journals/jsac/MouZGWZH21,
	author = {Zhiyu Mou and
                  Yu Zhang and
                  Feifei Gao and
                  Huan{-}gang Wang and
                  Tao Zhang and
                  Zhu Han},
	title = {Deep Reinforcement Learning Based Three-Dimensional Area Coverage
                  With {UAV} Swarm},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3160--3176},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088718},
	doi = {10.1109/JSAC.2021.3088718},
	timestamp = {Wed, 05 Jan 2022 14:30:55 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MouZGWZH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) technology is recognized as a promising solution to area coverage problems (ACPs) and has been extensively studied recently. In this paper, we study the 3D irregular terrain surface coverage problem with a hierarchical UAV swarm. We first build the 3D model of a random irregular terrain and propose a geometric way to project the 3D terrain surface into many weighted 2D patches. Then we develop a two-level hierarchical UAV swarm architecture, including the low-level follower UAVs (FUAVs) and the high-level leader UAVs (LUAVs). For FUAVs, we design a coverage trajectory algorithm to carry out specific coverage tasks within patches based on the star communication topology. For LUAVs, we propose a swarm deep Q-learning (SDQN) reinforcement learning algorithm to select patches. Moreover, an observation history model based on convolutional neural networks (CNNs) and the mean embedding method is integrated into SDQN to address the communication limitation problems of LUAVs. The numerical results show that FUAVs can cover the entire area of each patch with little redundancies, and the total coverage time of the SDQN is less than that of existing methods, which demonstrates the effectiveness of the proposed algorithms.}
}


@article{DBLP:journals/jsac/HuCSPC21,
	author = {Ye Hu and
                  Mingzhe Chen and
                  Walid Saad and
                  H. Vincent Poor and
                  Shuguang Cui},
	title = {Distributed Multi-Agent Meta Learning for Trajectory Design in Wireless
                  Drone Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3177--3192},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088689},
	doi = {10.1109/JSAC.2021.3088689},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuCSPC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the problem of the trajectory design for a group of energy-constrained drones operating in dynamic wireless network environments is studied. In the considered model, a team of drone base stations (DBSs) is dispatched to cooperatively serve clusters of ground users that have dynamic and unpredictable uplink access demands. In this scenario, the DBSs must cooperatively navigate in the considered area to maximize coverage of the dynamic requests of the ground users. This trajectory design problem is posed as an optimization framework whose goal is to find optimal trajectories that maximize the fraction of users served by all DBSs. To find an optimal solution for this non-convex optimization problem under unpredictable environments, a value decomposition based reinforcement learning (VD-RL) solution coupled with a meta-training mechanism is proposed. This algorithm allows the DBSs to dynamically learn their trajectories while generalizing their learning to unseen environments. Analytical results show that, the proposed VD-RL algorithm is guaranteed to converge to a locally optimal solution of the non-convex optimization problem. Simulation results show that, even without meta-training, the proposed VD-RL algorithm can achieve a 53.2% improvement of the service coverage and a 30.6% improvement in terms of the convergence speed, compared to baseline multi-agent algorithms. Meanwhile, the use of the meta-training mechanism improves the convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must deal with a previously unseen task.}
}


@article{DBLP:journals/jsac/ZhaoLSTZL21,
	author = {Chenxi Zhao and
                  Junyu Liu and
                  Min Sheng and
                  Wei Teng and
                  Yang Zheng and
                  Jiandong Li},
	title = {Multi-UAV Trajectory Planning for Energy-Efficient Content Coverage:
                  {A} Decentralized Learning-Based Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3193--3207},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088669},
	doi = {10.1109/JSAC.2021.3088669},
	timestamp = {Thu, 02 Dec 2021 15:54:10 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoLSTZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In next-generation wireless networks, high-mobility unmanned aerial vehicles (UAVs) are promising to provide content coverage, where users can receive sufficient requested content within a given time. However, trajectory planning for multiple UAVs to provide content coverage is challenging since 1) UAVs cannot provide content coverage for all users due to the limited energy and caching storage, and 2) the trajectory planning of UAV is coupled with each other. Moreover, the complete information based trajectory planning methods are unusable since UAVs cannot obtain prior information on the rapidly changing environment. In this paper, we investigate the multi-UAV trajectory planning for energy-efficient content coverage. We first formulate an energy efficiency maximization problem considering recharging scheduling, which aims to reduce the total length of trajectories of UAVs under the quality of service (QoS) constraints. To settle environment uncertainty, the trajectory planning problem is modeled as two coupled multi-agent stochastic games, whose equilibrium constitute the optimal trajectory. To obtain the equilibrium, we propose a decentralized reinforcement learning algorithm, which can decouple the two games. We prove that the proposed algorithm can converge to the optimal solution of the Bellman equation with a higher rate compared to the centralized one. Moreover, simulation results show that the energy efficiency of the proposed algorithm is smaller than 5% compared the optimal, which is obtained with the prior information of environments.}
}


@article{DBLP:journals/jsac/ChaiL21,
	author = {Shuqi Chai and
                  Vincent K. N. Lau},
	title = {Multi-UAV Trajectory and Power Optimization for Cached {UAV} Wireless
                  Networks With Energy and Content Recharging-Demand Driven Deep Learning
                  Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3208--3224},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088694},
	doi = {10.1109/JSAC.2021.3088694},
	timestamp = {Tue, 05 Oct 2021 10:02:49 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChaiL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel joint trajectory and communication scheduling scheme for multiple unmanned aerial vehicles (UAVs) enabled wireless caching networks. To exploit the favorable propagation of air-to-ground channels, we consider an ultra dense UAVs enabled content-centric wireless transmission network, where massive UAVs are deployed to transmit cached contents to a group of random distributed ground users. We formulate the problem as an infinite horizon ergodic stochastic differential game (SDG) for optimizing the users' quality-of-experience (QoE). In particular, stochastic dynamics of channel states, UAVs' mobility, energy queues and content request queues are modeled in this game. To deal with the state coupling between the UAVs, we consider a limiting problem for large number of UAV based on mean field analysis. A reduced-complexity decentralized solution can be obtained through mean-field equilibrium analysis. To further reduce the solution complexity on each UAV, we propose a model-specific deep neural network (DNN) to learn the optimal control solution in an online manner. The DNN is not arbitrarily generated but tailored to the structural properties of the value function and stationary distribution based on the homotopy perturbation method analysis. Finally, simulation results are provided to show that the proposed solution can achieve significant gain over the existing baselines.}
}


@article{DBLP:journals/jsac/YangXGQCC21,
	author = {Peng Yang and
                  Xing Xi and
                  Kun Guo and
                  Tony Q. S. Quek and
                  Jingxuan Chen and
                  Xianbin Cao},
	title = {Proactive {UAV} Network Slicing for {URLLC} and Mobile Broadband Service
                  Multiplexing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {10},
	pages = {3225--3244},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088624},
	doi = {10.1109/JSAC.2021.3088624},
	timestamp = {Tue, 25 Apr 2023 22:58:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangXGQCC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The unmanned aerial vehicle (UAV) network that is convinced as a significant component of 5G and emerging 6G wireless networks is desired to accommodate multiple types of service requirements simultaneously. However, how to converge different types of services onto a common UAV network without deploying an individual network solution for each type of service is challenging. We tackle this challenge in this paper through slicing the UAV network, i.e., creating logical UAV networks customized for specific requirements. To this end, we formulate the UAV network slicing problem as a sequential decision problem to provide mobile broadband (MBB) services for ground mobile users while satisfying ultra-reliable and low-latency requirements of UAV control and non-payload signal delivery. This problem, however, is difficult to be directly solved mainly due to the sequence-dependent characteristic and the lack of accurate location information of mobile users and accurate and tractable channel gain models in practice. To overcome these difficulties, we propose a novel solution approach based on learning and optimization methods. Particularly, we develop a distributed learning method to predict mobile users’ locations, where partial user location information stored on each UAV is utilized to train user location prediction networks. To achieve accurate channel gain models, we design deep neural networks (DNNs) that are trained by signal measurements at each UAV. To cope with the challenging sequence-dependent characteristic of the problem, we develop a Lyapunov-based optimization framework with provable performance guarantees to decompose the original problem into a sequence of separate optimization subproblems based on the learned results. Finally, an iterative optimization scheme joint with a successive convex approximation technique is exploited to solve these subproblems. Simulation results demonstrate the accuracy of the learning methods as well as the effectiveness of the Lyapunov-based optimization framework.}
}


@article{DBLP:journals/jsac/WuXZNASS21b,
	author = {Qingqing Wu and
                  Jie Xu and
                  Yong Zeng and
                  Derrick Wing Kwan Ng and
                  Naofal Al{-}Dhahir and
                  Robert Schober and
                  A. Lee Swindlehurst},
	title = {Guest Editorial Special Issue on {UAV} Communications in 5G and Beyond
                  Networks - Part {II}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3247--3251},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3090897},
	doi = {10.1109/JSAC.2021.3090897},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuXZNASS21b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless communication is an essential technology to unlock the full potential of unmanned aerial vehicles (UAVs) in numerous applications and has thus received unprecedented attention recently. Although technologies such as direct link, WiFi, and satellite communications are still useful in some remote scenarios where cellular services are unavailable, it is believed that exploiting the thriving 5G and beyond cellular networks to support UAV communications is the most promising and cost-effective approach, especially when the number of UAVs grows dramatically. On the one hand, to guarantee safe and efficient flight operations of multiple UAVs, it is of paramount importance to provide secure and ultra-reliable communication links between the UAVs and their ground pilots or control stations for conveying command and control signals, especially in beyond-visual-line-of-sight (BVLOS) scenarios. On the other hand, because of advances in communication equipment miniaturization as well as UAV manufacturing, mounting compact and lightweight base stations (BSs) or relays on UAVs becomes increasingly feasible. This has led to two promising research paradigms for UAV communications, namely, UAV-assisted cellular communications and cellular-connected UAVs, where UAVs are integrated into cellular networks as aerial communication platforms and aerial users, respectively. As such, integrating UAVs into cellular networks is believed to be a win-win technology for both UAV-related industries and cellular network operators, which not only creates plenty of new business opportunities but also benefits the communication performance of 3-D wireless networks. In addition, UAV related sensing and computing are also helpful for achieving efficient and reliable communication (e.g., in avoiding coverage holes) as well as smart UAV coordination, positioning, and trajectory design. However, 5G and beyond wireless networks with UAVs significantly differs from traditional communication systems, because of the high altitude and high maneuverability of UAVs, the unique UAV-ground channels, the diversified quality of service (QoS) requirements for downlink command and control (C&C) and uplink mission-related data transmission, the stringent constraints imposed by the size, weight, and power (SWAP) limitations of UAVs, as well as the new design degrees of freedom enabled by joint UAV mobility control and communication resource allocation.}
}


@article{DBLP:journals/jsac/LiwangGW21,
	author = {Minghui Liwang and
                  Zhibin Gao and
                  Xianbin Wang},
	title = {Let's Trade in the Future! {A} Futures-Enabled Fast Resource Trading
                  Mechanism in Edge Computing-Assisted {UAV} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3252--3270},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088657},
	doi = {10.1109/JSAC.2021.3088657},
	timestamp = {Thu, 04 Nov 2021 13:41:52 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiwangGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) has emerged as one of the key technical aspects of the fifth-generation (5G) networks. The integration of MEC with resource-constrained unmanned aerial vehicles (UAVs) greatly enables flexible resource provisioning for supporting dynamic and computation-intensive UAV applications. Existing resource trading could facilitate this paradigm with proper incentives, which, however, may often incur unexpected negotiation latency and energy consumption, trading failures and unfair pricing, due to the unpredictable nature of the resource trading process. Motivated by these challenges, an efficient futures-enabled resource trading mechanism for edge computing-assisted UAV network is proposed, where a mutually beneficial and risk-tolerable forward contract is devised to promote resource trading between an MEC server (seller) and a UAV (buyer) with multiple tasks. Two key problems i.e. futures contract design before trading, and transmission power optimization during trading are studied. By analyzing historical statistics associated with future resource supply, demand, and air-to-ground communication quality, the contract design is formulated as a multi-objective optimization problem aiming to maximize both the seller’s and the buyer’s expected utilities, while estimating their acceptable risk tolerance. Accordingly, we propose an efficient bilateral negotiation scheme to help players reach a trading consensus on the amount of resources and the relevant price. For the power optimization problem, we develop a practical algorithm that enables the buyer to determine its optimal transmission power via convex optimization techniques. Comprehensive simulations demonstrate that the proposed mechanism offers mutually beneficial utilities to players, while achieving commendable performance on trading failures and fairness, negotiation latency and cost, comparing with baseline methods.}
}


@article{DBLP:journals/jsac/FengTZZWWC21,
	author = {Wanmei Feng and
                  Jie Tang and
                  Nan Zhao and
                  Xiuyin Zhang and
                  Xianbin Wang and
                  Kai{-}Kit Wong and
                  Jonathon A. Chambers},
	title = {Hybrid Beamforming Design and Resource Allocation for UAV-Aided Wireless-Powered
                  Mobile Edge Computing Networks With {NOMA}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3271--3286},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3091158},
	doi = {10.1109/JSAC.2021.3091158},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FengTZZWWC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Beamforming and non-orthogonal multiple access (NOMA) serve as two potential solutions for achieving spectral efficient communication in the fifth generation and beyond wireless networks. In this paper, we jointly apply a hybrid beamforming and NOMA techniques to an unmanned aerial vehicle (UAV)-carried wireless-powered mobile edge computing (MEC) system, within which the UAV is equipped with a wireless power charger and the MEC platform delivers energy and computing services to Internet of Things (IoT) devices. Our aim is to maximize the sum computation rate at all IoT devices whilst satisfying the constraint of energy harvesting and coverage. The resultant optimization problem is non-convex involving joint optimization of the UAV’s 3D placement and hybrid beamforming matrices as well as computation resource allocation in both partial and binary offloading patterns, and thus is quite difficult to tackle directly. By applying the polyhedral annexation method and the deep deterministic policy gradient (DDPG) algorithm, we develop an effective algorithm to derive the closed-form solution for the optimal 3D deployment of the UAV, and find the solution for the hybrid beamformer. Two resource allocation algorithms for partial and binary offloading patterns are thereby proposed. Simulation results verify that our designed algorithms achieve a significant computation performance enhancement as compared to the benchmark schemes.}
}


@article{DBLP:journals/jsac/QuDWDWGW21,
	author = {Yuben Qu and
                  Haipeng Dai and
                  Haichao Wang and
                  Chao Dong and
                  Fan Wu and
                  Song Guo and
                  Qihui Wu},
	title = {Service Provisioning for UAV-Enabled Mobile Edge Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3287--3305},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088660},
	doi = {10.1109/JSAC.2021.3088660},
	timestamp = {Fri, 04 Nov 2022 09:06:25 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QuDWDWGW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV)-enabled mobile edge computing has been recognized as a promising technology to flexibly and efficiently handle computation-intensive and latency-sensitive tasks in the era of fifth generation (5G) and beyond. In this paper, we study the problem of Service Provisioning for UAV-enabled mobile edge computiNg (SPUN). Specifically, under task latency requirements and various resource constraints, we jointly optimize the service placement, UAV movement trajectory, task scheduling, and computation resource allocation, to minimize the overall energy consumption of all terrestrial user equipments (UEs). Due to the non-convexity of the SPUN problem as well as complex coupling among mixed integer variables, it is a non-convex mixed integer nonlinear programming (MINLP) problem. To solve this challenging problem, we propose two alternating optimization-based suboptimal solutions with different time complexities. In the first solution with relatively high complexity in the worst case, the joint service placement and task scheduling subproblem, and UAV trajectory subproblem are iteratively solved by the Branch and Bound (BnB) method and successive convex approximation (SCA), respectively, while the optimal solution to the computation resource allocation subproblem is efficiently obtained in the closed form. To avoid the high complexity caused by BnB, in the second solution, we propose a novel approximation algorithm based on relaxation and randomized rounding techniques for the joint service placement and task scheduling subproblem, while the other two subproblems are solved in the same way as that of the first solution. Extensive simulations demonstrate that the proposed solutions achieve significantly lower energy consumption of UEs compared to three benchmarks.}
}


@article{DBLP:journals/jsac/NingDWWGKP21,
	author = {Zhaolong Ning and
                  Peiran Dong and
                  Miaowen Wen and
                  Xiaojie Wang and
                  Lei Guo and
                  Ricky Yu{-}Kwong Kwok and
                  H. Vincent Poor},
	title = {5G-Enabled UAV-to-Community Offloading: Joint Trajectory Design and
                  Task Scheduling},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3306--3320},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088663},
	doi = {10.1109/JSAC.2021.3088663},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/NingDWWGKP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to line-of-sight communication links and distributed deployment, Unmanned Aerial Vehicles (UAVs) have attracted substantial interest in agile Mobile Edge Computing (MEC) service provision. In this paper, by clustering multiple users into independent communities based on their geographic locations, we design a 5G-enabled UAV-to-community offloading system. A system throughput maximization problem is formulated, subjected to the transmission rate, atomicity of tasks and speed of UAVs. By relaxing the transmission rate constraint, the mixed integer non-linear program is transformed into two subproblems. We first develop an average throughput maximization-based auction algorithm to determine the trajectory of UAVs, where a community-based latency approximation algorithm is developed to regulate the designed auction bidding. Then, a dynamic task admission algorithm is proposed to solve the task scheduling subproblem within one community. Performance analyses demonstrate that our designed auction bidding can guarantee user truthfulness, and can be fulfilled in polynomial time. Extensive simulations based on real-world data in health monitoring and online YouTube video services show that our proposed algorithm is able to maximize the system throughput while guaranteeing the fraction of served users.}
}

@article{DBLP:journals/jsac/YanHC21,
	author       = {Shihao Yan and
					Stephen V. Hanly and
					Iain B. Collings},
	title        = {Optimal Transmit Power and Flying Location for {UAV} Covert Wireless
					Communications},
	journal      = {{IEEE} J. Sel. Areas Commun.},
	volume       = {39},
	number       = {11},
	pages        = {3321--3333},
	year         = {2021},
	url          = {https://doi.org/10.1109/JSAC.2021.3088667},
	doi          = {10.1109/JSAC.2021.3088667},
	timestamp    = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl       = {https://dblp.org/rec/journals/jsac/YanHC21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	abstract     = {This paper jointly optimizes the flying location and wireless communication transmit power for an unmanned aerial vehicle (UAV) conducting covert operations. This is motivated by application scenarios such as military ground surveillance from airborne platforms, where it is vital for a UAV’s signal transmission to be undetectable by those within the surveillance region. Specifically, we maximize the communication quality to a legitimate receiver, who is also a ground-user but outside the surveillance region, under specific constraints on communication covertness, maximum transmit power, and the UAV’s physical location related to the required surveillance quality. We provide an explicit solution to the optimization problem for one of the most practical constraint combinations. For other constraint combinations, we determine feasible regions for flight, that can then be searched to establish the UAV’s optimal location. In many cases, the 2-dimensional optimal location is achieved by a 1-dimensional search. We discuss two heuristic approaches to UAV placement, and show that in some cases they are able to achieve close to optimal, but that in other cases significant gains can be achieved by employing our developed solutions.}
}


@article{DBLP:journals/jsac/WangLWCFDD21,
	author = {Wei Wang and
                  Xinrui Li and
                  Rui Wang and
                  Kanapathippillai Cumanan and
                  Wei Feng and
                  Zhiguo Ding and
                  Octavia A. Dobre},
	title = {Robust 3D-Trajectory and Time Switching Optimization for Dual-UAV-Enabled
                  Secure Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3334--3347},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088628},
	doi = {10.1109/JSAC.2021.3088628},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangLWCFDD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a dual-unmanned aerial vehicle (UAV)-enabled secure communication system, in which, a UAV moves around to send confidential messages to a mobile user while another cooperative UAV transmits artificial noise signals to confuse malicious eavesdroppers. Both UAVs have energy constraints and the location information of eavesdroppers is imperfect. We consider a worst-case secrecy rate maximization problem of the mobile user over all time slots. This optimization problem is solved by jointly designing the three-dimensional (3D) trajectory of UAVs and the time allocation (recharging and service or jamming) under practical constraints including maximum UAV speed, UAV collision avoidance, UAV positioning error, and UAV energy harvesting. Specifically, we adopt a more practical UAV-ground channel model with both large-scale and small-scale fading components. Due to the non-convex feasible region constructed by the complicated constraints, directly finding the optimal solution of the original problem is intractable. To address this issue, we decouple the original optimization problem into three subproblems and develop an iterative algorithm to find its suboptimal solution by using the block coordinate descent technique. To solve each subproblem, certain advanced optimization tools, such as integer relaxation, S-procedure, and successive convex approximation techniques, are utilized. Numerical simulation results are provided to corroborate the theoretical derivations and to evaluate the performance of the proposed algorithm. Additionally, the numerical results assist to draw new insights on the 3D UAV trajectory by comparing the performance with conventional two-dimensional (2D) schemes.}
}


@article{DBLP:journals/jsac/ZhouYSCL21,
	author = {Xiaobo Zhou and
                  Shihao Yan and
                  Feng Shu and
                  Riqing Chen and
                  Jun Li},
	title = {UAV-Enabled Covert Wireless Data Collection},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3348--3362},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088688},
	doi = {10.1109/JSAC.2021.3088688},
	timestamp = {Thu, 11 Nov 2021 14:44:45 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouYSCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work considers unmanned aerial vehicle (UAV) networks for collecting data covertly from ground users. The full-duplex (FD) UAV intends to gather critical information from a scheduled user (SU) through wireless communication and generate artificial noise (AN) with random transmit power in order to ensure a negligible probability of the SU’s transmission being detected by the unscheduled users (USUs). To enhance the system performance, we jointly design the UAV’s trajectory and its maximum AN transmit power together with the user scheduling strategy subject to practical constraints, e.g., a covertness constraint, which is explicitly determined by analyzing each USU’s detection performance, and a binary constraint induced by user scheduling. The formulated design problem is a mixed-integer non-convex optimization problem, which is challenging to solve directly, but tackled by our developed penalty successive convex approximation (P-SCA) scheme. An efficient UAV trajectory initialization is also presented based on the successive hover-and-fly (SHAF) trajectory, which also serves as a benchmark scheme. Our examination shows the developed P-SCA scheme significantly outperforms the benchmark scheme in terms of achieving a higher max-min average transmission rate (ATR) from all the SUs to the UAV.}
}


@article{DBLP:journals/jsac/LiZL21,
	author = {Yabo Li and
                  Haijun Zhang and
                  Keping Long},
	title = {Joint Resource, Trajectory, and Artificial Noise Optimization in Secure
                  Driven 3-D UAVs With {NOMA} and Imperfect {CSI}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3363--3377},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088623},
	doi = {10.1109/JSAC.2021.3088623},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by the practicality of unmanned aerial vehicle (UAV), we consider a dual-UAV based non-orthogonal multiple access (NOMA) scenario, which consists of one communication UAV for services and one jamming UAV against eavesdropping. The goal is to maximize the secrecy energy efficiency through the successive convex approximation based communication resource, UAV trajectory, and artificial noise optimization. Considering the probabilistic constraint of outage probability from imperfect channel state information, we transform it to a non-probabilistic problem by Markov inequality and Marcum\nQ\n-function, then the problem is decomposed into three subproblems. We apply matching-swapping method to assign subchannel in non-orthogonal multiple access (NOMA) UAV networks before the joint process, then convert the power optimization problem to a standard convex optimization form by upper bound of the concave function. The communication UAV trajectory is studied under the constraints of flying energy consumption, maximum speed, and flying altitude. To track this NP-hard problem, Taylor expansion and various slack variables sets are introduced to transform the non-convex problem to convex one. For the artificial noise optimization problem, we use the lower bound to replace the convex term turning it into an easy-to-solve convex optimization problem. In the end, simulations results reveal that: 1) The reasonable jamming scheme can improve the secrecy energy efficiency of the NOMA UAV networks, even if it can cause interference for legitimate users; 2) UAV will fly to a place where the performance gain from users is high when flying energy consumption permits.}
}


@article{DBLP:journals/jsac/SongCKC21,
	author = {Sooeun Song and
                  Minsu Choi and
                  Da{-}Eun Ko and
                  Jong{-}Moon Chung},
	title = {Multi-UAV Trajectory Optimization Considering Collisions in {FSO}
                  Communication Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3378--3394},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088665},
	doi = {10.1109/JSAC.2021.3088665},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SongCKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a trajectory optimization algorithm for multiple unmanned aerial vehicles (UAVs) is investigated for free space optic (FSO) based wireless communication networks, which is composed of multiple UAVs and multiple ground terminals (GTs). Considering the FSO channel model and line of sight (LoS) probability, the altitude of the UAVs for FSO connection between each GT and UAV is decided by the predetermined radius of the communication area of the GT. A multi-UAV trajectory optimization (MUTO) scheme is proposed to maximize the service time. In the MUTO scheme, first, the network is divided into multiple sectors using graph partitioning, and assigned to each UAV, and then the traveling salesman problem (TSP) is used to determine the order of the GTs that the UAV passes through. The number of UAVs that maximizes the time for the UAV to stay within the communication area of the GT is derived. The UAV trajectories are determined by sequentially minimizing the energy consumed when moving between GTs and maximizing the service time by applying successive convex approximation. In addition, to assist operations that use multiple UAVs, two collision avoidance schemes are applied to the MUTO scheme. First is the additional constraints (AC) method, which uses a geographic formula to control the distance between the UAVs, and second is the initial delay (ID) method, which deliberately adds a delay to the UAV trajectories to provide sufficient UAV spacing. The simulation results show that under low density operation conditions, MUTO-ID and MUTO-AC are sufficient to avoid all UAV collisions, and in overly dense UAV operations with very close UAV trajectories, MUTO-ID can reduce collisions by approximately 80%, MUTO-AC can reduce collisions by approximately 85%, and MUTO-ACID can reduce collisions by approximately 95%, when compared to the MUTO scheme with no collision avoidance support applied.}
}


@article{DBLP:journals/jsac/YaoCZZ21,
	author = {Zhuohui Yao and
                  Wenchi Cheng and
                  Wei Zhang and
                  Hailin Zhang},
	title = {Resource Allocation for 5G-UAV-Based Emergency Wireless Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3395--3410},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088684},
	doi = {10.1109/JSAC.2021.3088684},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YaoCZZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For unforeseen natural disasters, such as earthquakes, hurricanes, and floods, etc., the traditional communication infrastructure is unavailable or seriously disrupted along with persistent secondary disasters. Under such circumstances, it is highly demanded to deploy emergency wireless communication (EWC) networks to restore connectivity in accident/ incident areas. The emerging fifth-generation (5G)/beyond-5G (B5G) wireless communication system, like unmanned aerial vehicle (UAV) assisted networks and intelligent reflecting surface (IRS) based communication systems, are expected to be designed or re-farmed for supporting temporary high quality communications in post-disaster areas. However, the channel characteristics of post-disaster areas quickly change as the secondary disaster resulted topographical changes, imposing new but critical challenges for EWC networks. In this paper, we propose a novel heterogeneous\nF\ncomposite fading channel model for EWC networks which accurately models and characterizes the composite fading channel with reflectors, path-loss exponent, fading, and shadowing parameters in 5G-UAV based EWC networks. Based on the model, we develop the optimal power allocation scheme with the simple closed-form expression and the numerical results based optimal joint bandwidth-power allocation scheme. We derive the corresponding capacities and compare the energy efficiency between IRS and traditional relay based 5G-UAVs. Numerical results show that the new heterogeneous Fisher-Snedecor\nF\ncomposite fading channel adapted resource allocation schemes can achieve higher capacity and energy efficiency than those of traditional channel model adapted resource allocation schemes, thus providing better communications service for post-disaster areas.}
}


@article{DBLP:journals/jsac/Do-DuyNDKC21,
	author = {Tan Do{-}Duy and
                  Long Dinh Nguyen and
                  Trung Q. Duong and
                  Saeed R. Khosravirad and
                  Holger Claussen},
	title = {Joint Optimisation of Real-Time Deployment and Resource Allocation
                  for UAV-Aided Disaster Emergency Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3411--3424},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088662},
	doi = {10.1109/JSAC.2021.3088662},
	timestamp = {Tue, 21 Mar 2023 14:59:32 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/Do-DuyNDKC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we consider a joint optimisation of real-time deployment and resource allocation scheme for UAV-aided relay systems in emergency scenarios such as disaster relief and public safety missions. In particular, to recover the network within a disaster area, we propose a fast K-means-based user clustering model and jointly optimal power and time transferring allocation which can be applied in the real system by using UAVs as flying base stations for real-time recovering and maintaining network connectivity during and after disasters. Under the stringent QoS constraints, we then provide centralised and distributed models to maximise the energy efficiency of the considered network. Numerical results are provided to illustrate the effectiveness of the proposed computational approaches in terms of network energy efficiency and execution time for solving the resource allocation problem in real-time scenarios. We demonstrate that our proposed algorithm outperforms other benchmark schemes.}
}


@article{DBLP:journals/jsac/ZhangWP21b,
	author = {Xi Zhang and
                  Jingqing Wang and
                  H. Vincent Poor},
	title = {AoI-Driven Statistical Delay and Error-Rate Bounded QoS Provisioning
                  for mURLLC Over UAV-Multimedia 6G Mobile Networks Using {FBC}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3425--3443},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088625},
	doi = {10.1109/JSAC.2021.3088625},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangWP21b.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Massive ultra-reliable and low latency communications (mURLLC) has emerged as new and dominating 6G-standard services to support statistical quality-of-services (QoS) provisioning for delay-sensitive data transmissions. To measure the freshness of updated information, age of information (AoI) has recently formed as the new dimension of QoS metric. Since status updates usually consist of a small number of information bits but warrant ultra-low latency, integrating AoI with finite blocklength coding (FBC) creates an alternative promising solution for mURLLC. On the other hand, to solve the massive connectivity issues imposed by mURLLC, unmanned aerial vehicle (UAV) has been developed to significantly enhance the line-of-sight (LOS) coverage while guaranteeing various QoS requirements. However, how to efficiently integrate the above new techniques for statistical delay and error-rate bounded QoS provisioning in UAV systems has been neither well understood nor thoroughly studied. To overcome these challenges, we propose FBC based statistical delay and error-rate bounded QoS provisioning schemes which leverage AoI as a key QoS provisioning technique for mURLLC over UAV mobile networks. First, we develop FBC based UAV system models. Second, we build up AoI-metric based modeling frameworks to upper-bound peak AoI violation probability using FBC. Third, we formulate and solve FBC based peak AoI violation probability minimization problem. Forth, we jointly optimize peak AoI violation probability and\nϵ\n-effective capacity and characterize their tradeoffs. Finally, our simulations validate and evaluate our developed schemes.}
}


@article{DBLP:journals/jsac/HuangLXW21,
	author = {Mingfeng Huang and
                  Anfeng Liu and
                  Neal N. Xiong and
                  Jie Wu},
	title = {A UAV-Assisted Ubiquitous Trust Communication System in 5G and Beyond
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3444--3458},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088675},
	doi = {10.1109/JSAC.2021.3088675},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuangLXW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {UAV-assisted wireless communications facilitate the applications of Internet of Things (IoT), which employ billions of devices to sense and collect data with an on-demand style. However, there are numerous malicious Mobile Data Collectors (MDCs) mixing into the network, stealing or tampering with data, which greatly damages IoT applications. So, it is urgent to build a ubiquitous trust communication system. In this paper, a UAV-assisted Ubiquitous Trust Evaluation (UUTE) framework is proposed, which combines the UAV-assisted global trust evaluation and the historical interaction based local trust evaluation. We first propose a global trust evaluation model for data collection platforms. It can accurately eliminate malicious MDCs and create a clean data collection environment, by dispatching UAVs to collect baseline data to validate the data submitted by MDCs. After that, a local trust evaluation model is proposed to help select credible MDCs for collaborative data collection. By letting UAVs distribute the data verification hash codes to MDCs, the MDCs can verify whether the exchanged data from the interacted MDCs is reliable. Extensive experiments conduct on a real-life dataset demonstrate that our UUTE system outperforms the existing trust evaluation systems in terms of accuracy and cost.}
}


@article{DBLP:journals/jsac/TuanNSPD21,
	author = {Hoang Duong Tuan and
                  Ali Arshad Nasir and
                  Andrey V. Savkin and
                  H. Vincent Poor and
                  Eryk Dutkiewicz},
	title = {MPC-Based {UAV} Navigation for Simultaneous Solar-Energy Harvesting
                  and Two-Way Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3459--3474},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088633},
	doi = {10.1109/JSAC.2021.3088633},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TuanNSPD21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The paper is the first work that considers a constrained feedback control strategy to navigate an unmanned aerial vehicle (UAV) from a given starting point to a given terminal point while harvesting solar energy and providing a wireless communication service for ground users. Wireless communication channels are stochastic and cannot be known off-line, making the problem of off-line UAV path planning for wireless communication as considered in most existing works less meaningful. We consider the problem of navigating a solar-powered UAV from a starting point to a terminal point to harvest solar energy while serving the two-way communication between multiple pairs of ground users in a complex terrain. The objective is to jointly optimize the UAV’s flight time and its flight path by trading-off between the harvested energy and power consumption subject to the ground users’ minimum throughput requirement. We develop a new model predictive control (MPC) technique to address this problem. Namely, based on the well-known statistics of the air-to-ground (A2G) and ground-to-air (G2A) wireless channels, a predictive control model is proposed at each time-instant, which leads to an optimization problem over a receding horizon for the control design. This problem is non-convex due to the involvement of various optimization variables, which is then solved via novel convex iterations. Simulation results show the merits of the proposed algorithm. The results obtained by the proposed algorithm match with the benchmark non-MPC and offline-MPC approaches.}
}


@article{DBLP:journals/jsac/WangFWQ21,
	author = {Yanmin Wang and
                  Wei Feng and
                  Jue Wang and
                  Tony Q. S. Quek},
	title = {Hybrid Satellite-UAV-Terrestrial Networks for 6G Ubiquitous Coverage:
                  {A} Maritime Communications Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3475--3490},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088692},
	doi = {10.1109/JSAC.2021.3088692},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangFWQ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the coming smart ocean era, reliable and efficient communications are crucial for promoting a variety of maritime activities. Current maritime communication networks (MCNs) mainly rely on marine satellites and on-shore base stations (BSs). The former generally provides limited transmission rate, while the latter lacks wide-area coverage capability. Due to these facts, the state-of-the-art MCN falls far behind terrestrial fifth-generation (5G) networks. To fill up the gap in the coming sixth-generation (6G) era, we explore the benefit of deployable BSs for maritime coverage enhancement. Both unmanned aerial vehicles (UAVs) and mobile vessels are used to configure deployable BSs. This leads to a hierarchical satellite-UAV-terrestrial network on the ocean. We address the joint link scheduling and rate adaptation problem for this hybrid network, to minimize the total energy consumption with quality of service (QoS) guarantees. Different from previous studies, we use only the large-scale channel state information (CSI), which is location-dependent and thus can be predicted through the position information of each UAV/vessel based on its specific trajectory/shipping lane. The problem is shown to be an NP-hard mixed integer nonlinear programming problem with a group of hidden non-linear equality constraints. We solve it suboptimally by using Min-Max transformation and iterative problem relaxation, leading to a process-oriented joint link scheduling and rate adaptation scheme. As observed by simulations, the scheme can provide agile on-demand coverage for all users with much reduced system overhead and a polynomial computation complexity. Moreover, it can achieve a prominent performance close to the optimal solution.}
}


@article{DBLP:journals/jsac/MaZQCSCB21,
	author = {Ting Ma and
                  Haibo Zhou and
                  Bo Qian and
                  Nan Cheng and
                  Xuemin Shen and
                  Xiang Chen and
                  Bo Bai},
	title = {{UAV-LEO} Integrated Backbone: {A} Ubiquitous Data Collection Approach
                  for {B5G} Internet of Remote Things Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3491--3505},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088626},
	doi = {10.1109/JSAC.2021.3088626},
	timestamp = {Wed, 22 Mar 2023 21:18:25 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MaZQCSCB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advance of unmanned aerial vehicles (UAVs) and low earth orbit (LEO) satellites, the integration of space, air and ground networks has become a potential solution to the beyond fifth generation (B5G) Internet of remote things (IoRT) networks. However, due to the network heterogeneity and the high mobility of UAVs and LEOs, how to design an efficient UAV-LEO integrated data collection scheme without infrastructure support is very challenging. In this paper, we investigate the resource allocation problem for a two-hop uplink UAV-LEO integrated data collection for the B5G IoRT networks, where numerous UAVs gather data from IoT devices and transmit the IoT data to LEO satellites. In order to maximize the data gathering efficiency in the IoT-UAV data gathering process, we study the bandwidth allocation of IoT devices and the 3-dimensional (3D) trajectory design of UAVs. In the UAV-LEO data transmission process, we jointly optimize the transmit powers of UAVs and the selections of LEO satellites for the total uploaded data amount and the energy consumption of UAVs. Considering the relay role and the cache capacity limitations of UAVs, we merge the optimizations of IoT-UAV data gathering and UAV-LEO data transmission into an integrated optimization problem, which is solved with the aid of the successive convex approximation (SCA) and the block coordinate descent (BCD) techniques. Simulation results demonstrate that the proposed scheme achieves better performance than the benchmark algorithms in terms of both energy consumption and total upload data amount.}
}


@article{DBLP:journals/jsac/GuoYYZ21,
	author = {Yijun Guo and
                  Changsheng You and
                  Changchuan Yin and
                  Rui Zhang},
	title = {{UAV} Trajectory and Communication Co-Design: Flexible Path Discretization
                  and Path Compression},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3506--3523},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088690},
	doi = {10.1109/JSAC.2021.3088690},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GuoYYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance optimization of UAV communication systems requires the joint design of UAV trajectory and communication efficiently. To tackle the challenge of infinite design variables arising from the continuous-time UAV trajectory optimization, a commonly adopted approach in the existing literature is by approximating the UAV trajectory with piecewise-linear path segments connected via a finite number of waypoints in three-dimensional (3D) space. However, this approach may still incur prohibitive computational complexity in practice when the UAV flight period/distance becomes long, as the distance between consecutive waypoints needs to be kept sufficiently small to retain high approximation accuracy. To resolve this fundamental issue, we propose in this paper a new and general framework for UAV trajectory and communication co-design with flexible number of waypoint optimization variables (called designable waypoints) or their sub-path representations. First, we propose a flexible path discretization scheme that optimizes only a number of selected waypoints (designable waypoints) along the UAV path for complexity reduction, while all the designable and non-designable waypoints are used in calculating the approximated communication utility along the UAV trajectory for ensuring high trajectory discretization accuracy. Next, we propose a novel path compression scheme, which treats the UAV trajectory as a signal and compresses its path representation based on the basis decomposition. Specifically, the UAV 3D path is first decomposed into three one-dimensional (1D) sub-paths and each sub-path is then approximated by superimposing a number of selected basis paths (which are generally less than the number of designable waypoints) weighted by their corresponding path coefficients, thus further reducing the path design complexity. Finally, we provide a case study on UAV trajectory design for aerial data harvesting from distributed sensors, and numerically show that the proposed flexible path discretization and path compression schemes can significantly reduce the UAV trajectory design complexity yet achieve favorable rate performance as compared to conventional path/time discretization schemes.}
}


@article{DBLP:journals/jsac/YuanHLS21,
	author = {Xiaopeng Yuan and
                  Yulin Hu and
                  Deshi Li and
                  Anke Schmeink},
	title = {Novel Optimal Trajectory Design in UAV-Assisted Networks: {A} Mechanical
                  Equivalence-Based Strategy},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3524--3541},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088627},
	doi = {10.1109/JSAC.2021.3088627},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YuanHLS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs), also known as drones, have already been widely implemented in wireless networks for promoting network performance and enabling new services. To efficiently explore the diversity introduced by the mobility of UAV, many efforts have been made in the design of the UAV trajectory under various wireless scenarios. However, the continuity of a UAV trajectory in both time and topology forces researchers to approximate the UAV trajectory by a discrete model, which always results in a sub-optimal solution. To tackle the difficulty and obtain the optimal trajectory, in this work we introduce an artificial potential field (APF) to reformulate the objective in trajectory design, with which the UAV trajectory problem can be completely equivalent to a mechanical problem. In such mechanical problem, the UAV trajectory is represented by an extremely soft and thin rope with variable density carrying UAV speed information, and the original objective of optimizing the system performance is transformed to minimizing the overall artificial potential energy on the rope. As a result, the rope in the optimal solution stays in a state of equilibrium and the UAV trajectory can be equivalently optimized by designing the shape of a rope under the APF via mechanical principles. We provide a case study to describe in detail the problem equivalence, i.e., taking a single-user network as an example in which the throughput between UAV and the user is considered as the objective performance. In particular, the optimal trajectory of a UAV is constructed based on mechanical principles, while the global optimality is also rigorously proved and further confirmed via simulations. Moreover, we also highlight that the novel strategy of constructing equivalent mechanical problem has the possibilities to be extended to various UAV trajectory problems under different scenarios with different performance optimization objectives.}
}


@article{DBLP:journals/jsac/MatarS21,
	author = {Amr S. Matar and
                  Xuemin Shen},
	title = {Joint Subchannel Allocation and Power Control in Licensed and Unlicensed
                  Spectrum for Multi-Cell UAV-Cellular Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3542--3554},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088672},
	doi = {10.1109/JSAC.2021.3088672},
	timestamp = {Wed, 03 Nov 2021 08:28:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MatarS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the resource and interference management problem in a novel scenario where multiple unmanned aerial vehicle base stations (UAV-BSs) provide cellular services to UAV users (UAV-UEs) by reusing both licensed and unlicensed spectrum. Considering the co-existence of terrestrial cellular, WiFi and UAV-BSs, a joint optimization problem is formulated for both subchannel allocation and power control of UAV-UEs over the licensed/unlicensed spectrum in order to maximize the uplink sum-rate of the multi-cell UAV-cellular network. Since the formulated problem is NP-hard, we decompose it into three sub-problems. Specifically, we first use the convex optimization and the Hungarian algorithm to obtain the global optimal of power and subchannel allocations in the licensed spectrum, respectively. Then, we propose a matching game with externalities and coalition game algorithms to obtain the Nash stable of the subchannel allocation in the unlicensed band. Local optimal power assignment in the unlicensed spectrum is obtained using the successive convex approximation (SCA) method. An iterative algorithm is thereby developed to solve the three sub-problems sequentially till reaching convergence. Simulation results show that the proposed algorithm can improve the network capacity by nearly two times than the Long Term Evolution-Advanced (LTE-A).}
}


@article{DBLP:journals/jsac/SunLLLK21,
	author = {Geng Sun and
                  Jiahui Li and
                  Yanheng Liu and
                  Shuang Liang and
                  Hui Kang},
	title = {Time and Energy Minimization Communications Based on Collaborative
                  Beamforming for {UAV} Networks: {A} Multi-Objective Optimization Method},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {11},
	pages = {3555--3572},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3088720},
	doi = {10.1109/JSAC.2021.3088720},
	timestamp = {Wed, 19 Jun 2024 17:14:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SunLLLK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) communications and networks are of utmost concern. However, they have challenges such as the limited on-board energy and restricted transmit power. In this paper, we study a UAV-enabled communication scenario that a set of UAVs perform a virtual antenna array (VAA) to communicate with different remote base stations (BSs) by using collaborative beamforming (CB). To achieve a better transmission performance, the UAV elements can fly to optimal positions by using optimal speeds and adjust to optimal excitation current weights for performing CB transmissions. However, there are some trade-offs between energy consumption and transmission performance. Thus, we formulate a time and energy minimization communication multi-objective optimization problem (TEMCMOP) of CB in UAV networks to simultaneously minimize the total transmission time, total performing time of VAAs and total motion and hovering energy consumptions of UAVs by jointly optimizing the positions, flight speeds and excitation current weights of UAVs, as well as the order of communicating with different BSs. Due to the complexity and NP-hardness of the formulated TEMCMOP, we propose an improved multi-objective ant lion optimization (IMOALO) algorithm with chaos-opposition based learning solution initialization and hybrid solution update operators to solve the problem. Simulation results verify that the proposed IMOALO can effectively solve the formulated TEMCMOP and it has better performance than some other benchmark approaches.}
}


@article{DBLP:journals/jsac/ChenGHSBFP21,
	author = {Mingzhe Chen and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Kaibin Huang and
                  Walid Saad and
                  Mehdi Bennis and
                  Aneta Vulgarakis Feljan and
                  H. Vincent Poor},
	title = {Guest Editorial Special Issue on Distributed Learning Over Wireless
                  Edge Networks - Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3575--3578},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118484},
	doi = {10.1109/JSAC.2021.3118484},
	timestamp = {Sat, 25 Dec 2021 15:52:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenGHSBFP21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Analyzing massive amounts of data using complex machine learning models requires significant computational resources. The conventional approach to such problems involves centralizing training data and inference processes in the cloud, i.e., in data centers. However, with the proliferation of mobile devices and increasing application of the Internet-of-Things (IoT) paradigm, very large amounts of data are collected at the edges of wireless networks, and due to privacy constraints and limited communication resources, it is undesirable or impractical to upload this data from mobile devices to the cloud for centralized learning. This problem can be solved by distributed learning at the network edge, by which edge devices collaboratively train a shared learning model using real-time mobile data. The avoidance of raw-data uploading not only helps to preserve privacy but may also alleviate network-traffic congestion and minimize latency. With that said, distributed training still requires a substantial amount of information exchange between devices and edge servers over wireless links. In the process, wireless impairments such as noise, interference, and imperfect knowledge of channel states can significantly slow down distributed learning (e.g., convergence speed) and degrades its performance (e.g., learning accuracy). This makes it crucial to optimize wireless network performance so as to support the efficient deployment of distributed learning algorithms. On the other hand, distributed learning algorithms provide a powerful tool-set for solving complex problems in wireless communication and networking. One important framework, called federated learning (FL), enables users to collaboratively learn a shared model while helping to preserve local data privacy. The application of FL can endow edge devices with capabilities of user behavior prediction, user identification, and wireless environment analysis. As another example, distributed reinforcement learning is capable of leveraging distributed computation power and data to solve complex optimization and control problems that arise in various use cases, such as network control, user clustering, resource management, and interference alignment. To cover this paradigm of distributed learning over wireless networks, this two-part Special Issue features papers dealing with two main research challenges: a) optimization of wireless network performance for efficient implementation of distributed learning in wireless networks, and b) distributed learning for solving communication problems and optimizing network performance.}
}


@article{DBLP:journals/jsac/ChenGHSBFP21a,
	author = {Mingzhe Chen and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Kaibin Huang and
                  Walid Saad and
                  Mehdi Bennis and
                  Aneta Vulgarakis Feljan and
                  H. Vincent Poor},
	title = {Distributed Learning in Wireless Networks: Recent Progress and Future
                  Challenges},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3579--3605},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118346},
	doi = {10.1109/JSAC.2021.3118346},
	timestamp = {Sat, 25 Dec 2021 15:52:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenGHSBFP21a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next-generation of wireless networks will enable many machine learning (ML) tools and applications to efficiently analyze various types of data collected by edge devices for inference, autonomy, and decision making purposes. However, due to resource constraints, delay limitations, and privacy challenges, edge devices cannot offload their entire collected datasets to a cloud server for centrally training their ML models or inference purposes. To overcome these challenges, distributed learning and inference techniques have been proposed as a means to enable edge devices to collaboratively train ML models without raw data exchanges, thus reducing the communication overhead and latency as well as improving data privacy. However, deploying distributed learning over wireless networks faces several challenges including the uncertain wireless environment (e.g., dynamic channel and interference), limited wireless resources (e.g., transmit power and radio spectrum), and hardware resources (e.g., computational power). This paper provides a comprehensive study of how distributed learning can be efficiently and effectively deployed over wireless edge networks. We present a detailed overview of several emerging distributed learning paradigms, including federated learning, federated distillation, distributed inference, and multi-agent reinforcement learning. For each learning framework, we first introduce the motivation for deploying it over wireless networks. Then, we present a detailed literature review on the use of communication techniques for its efficient deployment. We then introduce an illustrative example to show how to optimize wireless networks to improve its performance. Finally, we introduce future research opportunities. In a nutshell, this paper provides a holistic set of guidelines on how to deploy a broad range of distributed learning frameworks over real-world wireless communication networks.}
}


@article{DBLP:journals/jsac/LuoLWHT21,
	author = {Bing Luo and
                  Xiang Li and
                  Shiqiang Wang and
                  Jianwei Huang and
                  Leandros Tassiulas},
	title = {Cost-Effective Federated Learning in Mobile Edge Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3606--3621},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118436},
	doi = {10.1109/JSAC.2021.3118436},
	timestamp = {Tue, 21 Mar 2023 21:08:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LuoLWHT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a distributed learning paradigm that enables a large number of mobile devices to collaboratively learn a model under the coordination of a central server without sharing their raw data. Despite its practical efficiency and effectiveness, the iterative on-device learning process (e.g., local computations and global communications with the server) incurs a considerable cost in terms of learning time and energy consumption, which depends crucially on the number of selected clients and the number of local iterations in each training round. In this paper, we analyze how to design adaptive FL in mobile edge networks that optimally chooses these essential control variables to minimize the total cost while ensuring convergence. We establish the analytical relationship between the total cost and the control variables with the convergence upper bound. To efficiently solve the cost minimization problem, we develop a low-cost sampling-based algorithm to learn the convergence related unknown parameters. We derive important solution properties that effectively identify the design principles for different optimization metrics. Practically, we evaluate our theoretical results both in a simulated environment and on a hardware prototype. Experimental evidence verifies our derived properties and demonstrates that our proposed solution achieves near-optimal performance for different optimization metrics for various datasets and heterogeneous system and statistical settings.}
}


@article{DBLP:journals/jsac/WanLFSPL21,
	author = {Shuo Wan and
                  Jiaxun Lu and
                  Pingyi Fan and
                  Yunfeng Shao and
                  Chenghui Peng and
                  Khaled B. Letaief},
	title = {Convergence Analysis and System Design for Federated Learning Over
                  Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3622--3639},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118351},
	doi = {10.1109/JSAC.2021.3118351},
	timestamp = {Tue, 18 Jul 2023 16:34:29 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WanLFSPL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has recently emerged as an important and promising learning scheme in IoT, enabling devices to jointly learn a model without sharing their raw data sets. As FL does not collect and store the data centrally, it requires frequent model exchange through the wireless network. However, since the aggregation in FL can be partially participated with synchronized frequency, its communication pattern is different from the conventional network. Therein, limited bandwidth and package loss restrict interactions in training. Thus, the network scheduling could largely affect the FL convergence. To figure out the specific effects, we analyze the convergence rate of FL regarding the joint impact of communication and training. Combining it with the network model, we formulate the optimal scheduling problem for FL implementation. The theoretical results could guide the hyper-parameter design in the network and explain the principle of how the wireless communication could influence the FL training process.}
}


@article{DBLP:journals/jsac/LimNXNMK21,
	author = {Wei Yang Bryan Lim and
                  Jer Shyuan Ng and
                  Zehui Xiong and
                  Dusit Niyato and
                  Chunyan Miao and
                  Dong In Kim},
	title = {Dynamic Edge Association and Resource Allocation in Self-Organizing
                  Hierarchical Federated Learning Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3640--3653},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118401},
	doi = {10.1109/JSAC.2021.3118401},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LimNXNMK21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a promising privacy-preserving distributed machine learning paradigm. However, communication inefficiency remains the key bottleneck that impedes its large-scale implementation. Recently, hierarchical FL (HFL) has been proposed in which data owners, i.e., workers, can first transmit their updated model parameters to edge servers for intermediate aggregation. This reduces the instances of global communication and straggling workers. To enable efficient HFL, it is important to address the issues of edge association and resource allocation in the context of non-cooperative players, i.e., workers, edge servers, and model owner. However, the existing studies merely focus on static approaches and do not consider the dynamic interactions and bounded rationalities of the players. In this paper, we propose a hierarchical game framework to study the dynamics of edge association and resource allocation in self-organizing HFL networks. In the lower-level game, the edge association strategies of the workers are modelled using an evolutionary game. In the upper-level game, a Stackelberg differential game is adopted in which the model owner decides an optimal reward scheme given the expected bandwidth allocation control strategy of the edge server. Finally, we provide numerical results to validate that our proposed framework captures the HFL system dynamics under varying sources of network heterogeneity.}
}


@article{DBLP:journals/jsac/MaXXJHH21,
	author = {Qianpiao Ma and
                  Yang Xu and
                  Hongli Xu and
                  Zhida Jiang and
                  Liusheng Huang and
                  He Huang},
	title = {FedSA: {A} Semi-Asynchronous Federated Learning Mechanism in Heterogeneous
                  Edge Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3654--3672},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118435},
	doi = {10.1109/JSAC.2021.3118435},
	timestamp = {Mon, 24 Jan 2022 15:07:42 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MaXXJHH21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) involves training machine learning models over distributed edge nodes ( i.e. , workers) while facing three critical challenges, edge heterogeneity, Non-IID data and communication resource constraint. In the synchronous FL, the parameter server has to wait for the slowest workers, leading to significant waiting time due to edge heterogeneity. Though asynchronous FL can well tackle the edge heterogeneity, it requires frequent model transfers, resulting in massive communication resource consumption. Moreover, the different relative frequency of workers participating in asynchronous updating may seriously hurt training accuracy, especially on Non-IID data. In this paper, we propose a semi-asynchronous federated learning mechanism (FedSA), where the parameter server aggregates a certain number of local models by their arrival order in each round. We theoretically analyze the quantitative relationship between the convergence bound of FedSA and different factors, e.g. , the number of participating workers in each round, the degree of data Non-IID and edge heterogeneity. Based on the convergence bound, we present an efficient algorithm to determine the number of participating workers to minimize the training completion time. To further improve the training accuracy on Non-IID data, FedSA deploys adaptive learning rates for workers by their relative participation frequency. We extend our proposed mechanism to the dynamic and multiple learning tasks scenarios. Experimental results on the testbed show that our proposed mechanism and algorithms address the three challenges more effectively than the state-of-the-art solutions.}
}


@article{DBLP:journals/jsac/LeeL21,
	author = {Hyun{-}Suk Lee and
                  Jang{-}Won Lee},
	title = {Adaptive Transmission Scheduling in Wireless Networks for Asynchronous
                  Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3673--3687},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118353},
	doi = {10.1109/JSAC.2021.3118353},
	timestamp = {Thu, 03 Nov 2022 17:11:01 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LeeL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study asynchronous federated learning (FL) in a wireless distributed learning network (WDLN). To allow each edge device to use its local data more efficiently via asynchronous FL, transmission scheduling in the WDLN for asynchronous FL should be carefully determined considering system uncertainties, such as time-varying channel and stochastic data arrivals, and the scarce radio resources in the WDLN. To address this, we propose a metric, called an effectivity score, which represents the amount of learning from asynchronous FL. We then formulate an Asynchronous Learning-aware transmission Scheduling (ALS) problem to maximize the effectivity score and develop three ALS algorithms, called ALSA-PI, BALSA, and BALSA-PO, to solve it. If the statistical information about the uncertainties is known, the problem can be optimally and efficiently solved by ALSA-PI. Even if not, it can be still optimally solved by BALSA that learns the uncertainties based on a Bayesian approach using the state information reported from devices. BALSA-PO suboptimally solves the problem, but it addresses a more restrictive WDLN in practice, where the AP can observe a limited state information compared with the information used in BALSA. We show via simulations that the models trained by our ALS algorithms achieve performances close to that by an ideal benchmark and outperform those by other state-of-the-art baseline scheduling algorithms in terms of model accuracy, training loss, learning speed, and robustness of learning. These results demonstrate that the adaptive scheduling strategy in our ALS algorithms is effective to asynchronous FL.}
}


@article{DBLP:journals/jsac/ZhangYWPZZS21,
	author = {Weiting Zhang and
                  Dong Yang and
                  Wen Wu and
                  Haixia Peng and
                  Ning Zhang and
                  Hongke Zhang and
                  Xuemin Shen},
	title = {Optimizing Federated Learning in Distributed Industrial IoT: {A} Multi-Agent
                  Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3688--3703},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118352},
	doi = {10.1109/JSAC.2021.3118352},
	timestamp = {Mon, 03 Jan 2022 22:12:06 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangYWPZZS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we aim to make the best joint decision of device selection and computing and spectrum resource allocation for optimizing federated learning (FL) performance in distributed industrial Internet of Things (IIoT) networks. To implement efficient FL over geographically dispersed data, we introduce a three-layer collaborative FL architecture to support deep neural network (DNN) training. Specifically, using the data dispersed in IIoT devices, the industrial gateways locally train the DNN model and the local models can be aggregated by their associated edge servers every FL epoch or by a cloud server every a few FL epochs for obtaining the global model. To optimally select participating devices and allocate computing and spectrum resources for training and transmitting the model parameters, we formulate a stochastic optimization problem with the objective of minimizing FL evaluating loss while satisfying delay and long-term energy consumption requirements. Since the objective function of the FL evaluating loss is implicit and the energy consumption is temporally correlated, it is difficult to solve the problem via traditional optimization methods. Thus, we propose a “ Reinforcement on Federated ” (RoF) scheme, based on deep multi-agent reinforcement learning, to solve the problem. Specifically, the RoF scheme is executed decentralizedly at edge servers, which can cooperatively make the optimal device selection and resource allocation decisions. Moreover, a device refinement subroutine is embedded into the RoF scheme to accelerate convergence while effectively saving the on-device energy. Simulation results demonstrate that the RoF scheme can facilitate efficient FL and achieve better performance compared with state-of-the-art benchmarks.}
}


@article{DBLP:journals/jsac/JinJQZL21,
	author = {Yibo Jin and
                  Lei Jiao and
                  Zhuzhong Qian and
                  Sheng Zhang and
                  Sanglu Lu},
	title = {Budget-Aware Online Control of Edge Federated Learning on Streaming
                  Data With Stochastic Inputs},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3704--3722},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118421},
	doi = {10.1109/JSAC.2021.3118421},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JinJQZL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Performing federated learning continuously in edge networks while training data are dynamically and unpredictably streamed to the devices faces critical challenges, including the global model convergence, the long-term resource budget, and the uncertain stochastic network and execution environment. We formulate an integer program to capture all these challenges, which minimizes the cumulative total latency of stream learning on device and federated learning between devices and the edge server. We then decouple the problem, design an online learning algorithm for controlling the number of local model updates via a convex-concave reformulation and rectified gradient-descent steps, and design a bandit learning algorithm for selecting the edge server for global model aggregations by incorporating the budget information to strike the exploit-explore balance. We rigorously prove the sub-linear regret regarding the optimization objective and the sub-linear constraint violation regarding the maximal on-device load, while guaranteeing the convergence of the global model trained. Extensive evaluations with real-world training data and input traces confirm the empirical superiority of our approach over multiple state-of-the-art algorithms.}
}


@article{DBLP:journals/jsac/XingSB21,
	author = {Hong Xing and
                  Osvaldo Simeone and
                  Suzhi Bi},
	title = {Federated Learning Over Wireless Device-to-Device Networks: Algorithms
                  and Convergence Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3723--3741},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118400},
	doi = {10.1109/JSAC.2021.3118400},
	timestamp = {Wed, 15 Dec 2021 10:32:22 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XingSB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The proliferation of Internet-of-Things (IoT) devices and cloud-computing applications over siloed data centers is motivating renewed interest in the collaborative training of a shared model by multiple individual clients via federated learning (FL). To improve the communication efficiency of FL implementations in wireless systems, recent works have proposed compression and dimension reduction mechanisms, along with digital and analog transmission schemes that account for channel noise, fading, and interference. The prior art has mainly focused on star topologies consisting of distributed clients and a central server. In contrast, this paper studies FL over wireless device-to-device (D2D) networks by providing theoretical insights into the performance of digital and analog implementations of decentralized stochastic gradient descent (DSGD). First, we introduce generic digital and analog wireless implementations of communication-efficient DSGD algorithms, leveraging random linear coding (RLC) for compression and over-the-air computation (AirComp) for simultaneous analog transmissions. Next, under the assumptions of convexity and connectivity, we provide convergence bounds for both implementations. The results demonstrate the dependence of the optimality gap on the connectivity and on the signal-to-noise ratio (SNR) levels in the network. The analysis is corroborated by experiments on an image-classification task.}
}


@article{DBLP:journals/jsac/XuLYHW21,
	author = {Chunmei Xu and
                  Shengheng Liu and
                  Zhaohui Yang and
                  Yongming Huang and
                  Kai{-}Kit Wong},
	title = {Learning Rate Optimization for Federated Learning Exploiting Over-the-Air
                  Computation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3742--3756},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118402},
	doi = {10.1109/JSAC.2021.3118402},
	timestamp = {Mon, 03 Jan 2022 22:12:06 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XuLYHW21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) as a promising edge-learning framework can effectively address the latency and privacy issues by featuring distributed learning at the devices and model aggregation in the central server. In order to enable efficient wireless data aggregation, over-the-air computation (AirComp) has recently attracted great attention. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. In this paper, we propose a modified federated averaging (FedAvg) algorithm by introducing the local learning rates and present the convergence analysis. To combat the distortion, the local learning rate is optimized to adapt the fading channel, which is termed as dynamic learning rate (DLR). We begin our discussion by considering multiple-input-single-output (MISO) scenario, since the underlying optimization problem is convex and has a closed-form solution. Our studies are extended to a more general multiple-input-multiple-output (MIMO) case and an iterative method is derived. We also present the asymptotic analysis and give a near-optimal and closed-form receive beamforming solution when the number of antennas approaches infinity. Extensive simulation results demonstrate the effectiveness of the proposed DLR scheme in reducing the aggregate distortion and guaranteeing the testing accuracy on the MNIST and CIFAR10 datasets. In addition, the asymptotic analysis and the close-form solution are verified through numerical simulations.}
}


@article{DBLP:journals/jsac/FanYZ21,
	author = {Dian Fan and
                  Xiaojun Yuan and
                  Ying{-}Jun Angela Zhang},
	title = {Temporal-Structure-Assisted Gradient Aggregation for Over-the-Air
                  Federated Edge Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3757--3771},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118348},
	doi = {10.1109/JSAC.2021.3118348},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FanYZ21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate over-the-air model aggregation in a federated edge learning (FEEL) system. We introduce a Markovian probability model to characterize the intrinsic temporal structure of the model aggregation series. With this temporal probability model, we formulate the model aggregation problem as to infer the desired aggregated update given all the past observations from a Bayesian perspective. We develop a message passing based algorithm, termed temporal-structure-assisted gradient aggregation (TSA-GA), to fulfil this estimation task with low complexity and near-optimal performance. We further establish the state evolution (SE) analysis to characterize the behaviour of the proposed TSA-GA algorithm, and derive an explicit bound of the expected loss reduction of the FEEL system under certain standard regularity conditions. In addition, we develop an expectation maximization (EM) strategy to learn the unknown parameters in the Markovian model. We show that the proposed TSA-GA significantly outperforms the state-of-the-art analog compression scheme, and is able to achieve comparable learning performance as the error-free benchmark in terms of final test accuracy.}
}


@article{DBLP:journals/jsac/LiYGSWCL21,
	author = {Lintao Li and
                  Longwei Yang and
                  Xin Guo and
                  Yuanming Shi and
                  Haiming Wang and
                  Wei Chen and
                  Khaled B. Letaief},
	title = {Delay Analysis of Wireless Federated Learning Based on Saddle Point
                  Approximation and Large Deviation Theory},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3772--3789},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118431},
	doi = {10.1109/JSAC.2021.3118431},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiYGSWCL21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a collaborative machine learning paradigm, which enables deep learning model training over a large volume of decentralized data residing in mobile devices without accessing clients’ private data. Driven by the ever increasing demand for model training of mobile applications or devices, a vast majority of FL tasks are implemented over wireless fading channels. Due to the time-varying nature of wireless channels, however, random delay occurs in both the uplink and downlink transmissions of FL. How to analyze the overall time consumption of a wireless FL task, or more specifically, a FL’s delay distribution, becomes a challenging but important open problem, especially for delay-sensitive model training. In this paper, we present a unified framework to calculate the approximate delay distributions of FL over arbitrary fading channels. Specifically, saddle point approximation, extreme value theory (EVT), and large deviation theory (LDT) are jointly exploited to find the approximate delay distribution along with its tail distribution, which characterizes the quality-of-service of a wireless FL system. Simulation results will demonstrate that our approximation method achieves a small approximation error, which vanishes with the increase of training accuracy.}
}


@article{DBLP:journals/jsac/ZhangWB21,
	author = {Meng Zhang and
                  Ermin Wei and
                  Randall Berry},
	title = {Faithful Edge Federated Learning: Scalability and Privacy},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3790--3804},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118423},
	doi = {10.1109/JSAC.2021.3118423},
	timestamp = {Sat, 06 Aug 2022 22:05:46 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangWB21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning enables machine learning algorithms to be trained over decentralized edge devices without requiring the exchange of local datasets. Successfully deploying federated learning requires ensuring that agents (e.g., mobile devices) faithfully execute the intended algorithm, which has been largely overlooked in the literature. In this study, we first use risk bounds to analyze how the key feature of federated learning, unbalanced and non-i.i.d. data, affects agents’ incentives to voluntarily participate and obediently follow traditional federated learning algorithms. To be more specific, our analysis reveals that agents with less typical data distributions and relatively more samples are more likely to opt out of or tamper with federated learning algorithms. To this end, we formulate the first faithful implementation problem of federated learning and design two faithful federated learning mechanisms which satisfy economic properties, scalability, and privacy. First, we design a Faithful Federated Learning (FFL) mechanism which approximates the Vickrey–Clarke–Groves (VCG) payments via an incremental computation. We show that it achieves (probably approximate) optimality, faithful implementation, voluntary participation, and some other economic properties (such as budget balance). Further, the time complexity in the number of agents\nK\nis\nO(log(K))\n. Second, by partitioning agents into several clusters, we present a scalable VCG mechanism approximation. We further design a scalable and Differentially Private FFL (DP-FFL) mechanism , the first differentially private faithful mechanism, that maintains the economic properties. Our DP-FFL mechanism enables one to make three-way performance tradeoffs among privacy, the iterations needed, and payment accuracy loss.}
}


@article{DBLP:journals/jsac/SunCWWWWS21,
	author = {Peng Sun and
                  Haoxuan Che and
                  Zhibo Wang and
                  Yuwei Wang and
                  Tao Wang and
                  Liantao Wu and
                  Huajie Shao},
	title = {Pain-FL: Personalized Privacy-Preserving Incentive for Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3805--3820},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118354},
	doi = {10.1109/JSAC.2021.3118354},
	timestamp = {Tue, 21 Feb 2023 17:10:35 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunCWWWWS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a privacy-preserving distributed machine learning framework, which involves training statistical models over a number of mobile users (i.e., workers) while keeping data localized. However, recent works have demonstrated that workers engaged in FL are still susceptible to advanced inference attacks when sharing model updates or gradients, which would discourage them from participating. Most of the existing incentive mechanisms for FL mainly account for workers’ resource cost, while the cost incurred by potential privacy leakage resulting from inference attacks has rarely been incorporated. To address these issues, in this paper, we propose a contract-based personalized privacy-preserving incentive for FL, named Pain-FL, to provide customized payments for workers with different privacy preferences as compensation for privacy leakage cost while ensuring satisfactory convergence performance of FL models. The core idea of Pain-FL is that each worker agrees on a customized contract, which specifies a kind of privacy-preserving level (PPL) and the corresponding payment, with the server in each round of FL. Then, the worker perturbs her calculated stochastic gradients to be uploaded with that PPL in exchange for that payment. In particular, we respectively derive a set of optimal contracts analytically under both complete and incomplete information models, which could optimize the convergence performance of the finally learned global model, while bearing some desired economic properties, i.e., budget feasibility, individual rationality, and incentive compatibility. An exhaustive experimental evaluation of Pain-FL is conducted, and the results corroborate its practicability and effectiveness.}
}


@article{DBLP:journals/jsac/MohamedCT21,
	author = {Mohamed Seif Eldin Mohamed and
                  Wei{-}Ting Chang and
                  Ravi Tandon},
	title = {Privacy Amplification for Federated Learning via User Sampling and
                  Wireless Aggregation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3821--3835},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118408},
	doi = {10.1109/JSAC.2021.3118408},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MohamedCT21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of federated learning over a wireless channel with user sampling, modeled by a fading multiple access channel, subject to central and local differential privacy (DP/LDP) constraints. It has been shown that the superposition nature of the wireless channel provides a dual benefit of bandwidth efficient gradient aggregation, in conjunction with strong DP guarantees for the users. Specifically, the central DP privacy leakage has been shown to scale as\nO(1/\nK\n1/2\n)\n, where\nK\nis the number of users. It has also been shown that user sampling coupled with orthogonal transmission can enhance the central DP privacy leakage with the same scaling behavior. In this work, we show that, by jointly incorporating both wireless aggregation and user sampling, one can obtain even stronger privacy guarantees. We propose a private wireless gradient aggregation scheme, which relies on independently randomized participation decisions by each user. The central DP leakage of our proposed scheme scales as\nO(1/\nK\n3/4\n)\n. In addition, we show that LDP is also boosted by user sampling. We also present analysis for the convergence rate of the proposed scheme and study the tradeoffs between wireless resources, convergence, and privacy theoretically and empirically for two scenarios when the number of sampled participants are\n(a)\nknown, or\n(b)\nunknown at the parameter server.}
}


@article{DBLP:journals/jsac/YangZY21,
	author = {Yuzhi Yang and
                  Zhaoyang Zhang and
                  Qianqian Yang},
	title = {Communication-Efficient Federated Learning With Binary Neural Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3836--3850},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118415},
	doi = {10.1109/JSAC.2021.3118415},
	timestamp = {Fri, 06 Oct 2023 19:24:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangZY21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a privacy-preserving machine learning setting that enables many devices to jointly train a shared global model without the need to reveal their data to a central server. However, FL involves a frequent exchange of the parameters between all the clients and the server that coordinates the training. This introduces extensive communication overhead, which can be a major bottleneck in FL with limited communication links. In this paper, we consider training the binary neural networks (BNNs) in the FL setting instead of the typical real-valued neural networks to fulfill the stringent delay and efficiency requirement in wireless edge networks. We introduce a novel FL framework of training BNNs, where the clients only upload the binary parameters to the server. We also propose a novel parameter updating scheme based on the Maximum Likelihood (ML) estimation that preserves the performance of the BNN even without the availability of aggregated real-valued auxiliary parameters that are usually needed during the training of the BNN. Moreover, for the first time in the literature, we theoretically derive the conditions under which the training of BNN is converging. Numerical results show that the proposed FL framework significantly reduces the communication cost compared to the conventional neural networks with typical real-valued parameters, and the performance loss incurred by the binarization can be further compensated by a hybrid method.}
}


@article{DBLP:journals/jsac/LinHABM21,
	author = {Frank Po{-}Chen Lin and
                  Seyyedali Hosseinalipour and
                  Sheikh Shams Azam and
                  Christopher G. Brinton and
                  Nicol{\`{o}} Michelusi},
	title = {Semi-Decentralized Federated Learning With Cooperative {D2D} Local
                  Model Aggregations},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3851--3869},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118344},
	doi = {10.1109/JSAC.2021.3118344},
	timestamp = {Sat, 25 Dec 2021 15:52:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LinHABM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has emerged as a popular technique for distributing machine learning (ML) model training across the wireless edge. In this paper, we propose two timescale hybrid federated learning ( TT-HF ), a semi-decentralized learning architecture that combines the conventional device-to-server communication paradigm for federated learning with device-to-device (D2D) communications for model training. In TT-HF , during each global aggregation interval, devices (i) perform multiple stochastic gradient descent iterations on their individual datasets, and (ii) aperiodically engage in consensus procedure of their model parameters through cooperative, distributed D2D communications within local clusters. With a new general definition of gradient diversity, we formally study the convergence behavior of TT-HF , resulting in new convergence bounds for distributed ML. We leverage our convergence bounds to develop an adaptive control algorithm that tunes the step size, D2D communication rounds, and global aggregation period of TT-HF over time to target a sublinear convergence rate of\nO(1/t)\nwhile minimizing network resource utilization. Our subsequent experiments demonstrate that TT-HF significantly outperforms the current art in federated learning in terms of model accuracy and/or network energy consumption in different scenarios where local device datasets exhibit statistical heterogeneity. Finally, our numerical evaluations demonstrate robustness against outages caused by fading channels, as well favorable performance with non-convex loss functions.}
}


@article{DBLP:journals/jsac/HanCPM21,
	author = {Dong{-}Jun Han and
                  Minseok Choi and
                  Jungwuk Park and
                  Jaekyun Moon},
	title = {FedMes: Speeding Up Federated Learning With Multiple Edge Servers},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {39},
	number = {12},
	pages = {3870--3885},
	year = {2021},
	url = {https://doi.org/10.1109/JSAC.2021.3118422},
	doi = {10.1109/JSAC.2021.3118422},
	timestamp = {Wed, 15 Dec 2021 10:32:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HanCPM21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider federated learning (FL) with multiple wireless edge servers having their own local coverage. We focus on speeding up training in this increasingly practical setup. Our key idea is to utilize the clients located in the overlapping coverage areas among adjacent edge servers (ESs); in the model-downloading stage, the clients in the overlapping areas receive multiple models from different ESs, take the average of the received models, and then update the averaged model with their local data. These clients send their updated model to multiple ESs by broadcasting, which acts as bridges for sharing the trained models between servers. Even when some ESs are given biased datasets within their coverage regions, their training processes can be assisted by adjacent servers through the clients in their overlapping regions. As a result, the proposed scheme does not require costly communications with the central cloud server (located at the higher tier of edge servers) for model synchronization, significantly reducing the overall training time compared to the conventional cloud-based FL systems. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods. Our design targets latency-sensitive applications where edge-based FL is essential, e.g., when a number of connected cars/drones must cooperate (via FL) to quickly adapt to dynamically changing environments.}
}
