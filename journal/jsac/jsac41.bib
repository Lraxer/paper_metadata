@article{DBLP:journals/jsac/GunduzQADYYWC23,
	author = {Deniz G{\"{u}}nd{\"{u}}z and
                  Zhijin Qin and
                  Inaki Estella Aguerri and
                  Harpreet S. Dhillon and
                  Zhaohui Yang and
                  Aylin Yener and
                  Kai{-}Kit Wong and
                  Chan{-}Byoung Chae},
	title = {Guest Editorial Special Issue on Beyond Transmitting Bits: Context,
                  Semantics, and Task-Oriented Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {1--4},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221853},
	doi = {10.1109/JSAC.2022.3221853},
	timestamp = {Wed, 04 Jan 2023 17:50:27 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GunduzQADYYWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is our pleasure to share with you this Special Issue, which brings together a diverse set of articles dealing with various aspects of semantic and goal-oriented communications, providing a snapshot of research activities in this highly active research area. Wireless communications and networking research has traditionally focused on improving the capacity and throughput of the underlying wireless network. However, recent explosion in data-driven machine learning applications and their reliance on huge datasets collected by edge devices have raised legitimate concerns that the increasing data traffic might soon overwhelm the capacity of current networks despite ongoing efforts to increase their capacity and efficiency. Also, most of the edge intelligence applications impose stringent delay constraints, which cannot be met by naive forwarding of data samples for processing at the receiver end. This made it obvious to researchers in both academia and industry that it is essential to analyze the “value” or “relevance” of collected data, and filter and prioritize the delivery of data based on its value/relevance as well as the wireless channel and network conditions. In this context, data value will be closely connected to the underlying signals and processes that generate the data, e.g., text, image, video, or sensor data, and what the receiver intends to do with the received data. This subjectivity of data value makes semantic and goal-oriented communication a rather elusive research topic, which has led to both an increasingly rich and active area of investigation, but also a controversial one, mainly due to the lack of clear and widely agreed-upon definitions of some of the core concepts and formulations. Despite these disagreements, there is almost unanimous consensus on the importance and potential impact of this line of investigation for the design of future communication systems and networks.}
}


@article{DBLP:journals/jsac/GunduzQADYYWC23a,
	author = {Deniz G{\"{u}}nd{\"{u}}z and
                  Zhijin Qin and
                  Inaki Estella Aguerri and
                  Harpreet S. Dhillon and
                  Zhaohui Yang and
                  Aylin Yener and
                  Kai{-}Kit Wong and
                  Chan{-}Byoung Chae},
	title = {Beyond Transmitting Bits: Context, Semantics, and Task-Oriented Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {5--41},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3223408},
	doi = {10.1109/JSAC.2022.3223408},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GunduzQADYYWC23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Communication systems to date primarily aim at reliably communicating bit sequences. Such an approach provides efficient engineering designs that are agnostic to the meanings of the messages or to the goal that the message exchange aims to achieve. Next generation systems, however, can be potentially enriched by folding message semantics and goals of communication into their design. Further, these systems can be made cognizant of the context in which communication exchange takes place, thereby providing avenues for novel design insights. This tutorial summarizes the efforts to date, starting from its early adaptations, semantic-aware and task-oriented communications, covering the foundations, algorithms and potential implementations. The focus is on approaches that utilize information theory to provide the foundations, as well as the significant role of learning in semantics and task-aware communications.}
}


@article{DBLP:journals/jsac/ZouZLSP23,
	author = {Hang Zou and
                  Chao Zhang and
                  Samson Lasaulce and
                  Lucas Saludjian and
                  H. Vincent Poor},
	title = {Goal-Oriented Quantization: Analysis, Design, and Application to Resource
                  Allocation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {42--54},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221976},
	doi = {10.1109/JSAC.2022.3221976},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZouZLSP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the situation in which a receiver has to execute a task from a quantized version of the information source of interest is considered. The task is modeled by the minimization problem of a general goal function f(x;g) for which the decision x has to be taken from a quantized version of the parameters g . This problem is relevant in many applications, e.g., for radio resource allocation (RA), high spectral efficiency communications, controlled systems, or data clustering in the smart grid. By resorting to high resolution (HR) analysis, it is shown how to design a quantizer that minimizes the gap between the minimum of f (which would be reached by knowing g perfectly) and what is effectively reached with a quantized g . The conducted formal analysis both provides quantization strategies in the HR regime and insights for the general regime and allows a practical algorithm to be designed. The analysis also allows one to provide some elements to the new and fundamental problem of the relationship between the goal function regularity properties and the hardness to quantize its parameters. The derived results are discussed and supported by a rich numerical performance analysis in which known RA goal functions are studied and allows one to exhibit very significant improvements by tailoring the quantization operation to the final task.}
}


@article{DBLP:journals/jsac/HuangGTDL23,
	author = {Danlan Huang and
                  Feifei Gao and
                  Xiaoming Tao and
                  Qiyuan Du and
                  Jianhua Lu},
	title = {Toward Semantic Communications: Deep Learning-Based Image Semantic
                  Coding},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {55--71},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221999},
	doi = {10.1109/JSAC.2022.3221999},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuangGTDL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communications has received growing interest since it can remarkably reduce the amount of data to be transmitted without missing critical information. Most existing works explore the semantic encoding and transmission for text and apply techniques in Natural Language Processing (NLP) to interpret the meaning of the text. In this paper, we conceive the semantic communications for image data that is much more richer in semantics and bandwidth sensitive. We propose an reinforcement learning based adaptive semantic coding (RL-ASC) approach that encodes images beyond pixel level. Firstly, we define the semantic concept of image data that includes the category, spatial arrangement, and visual feature as the representation unit, and propose a convolutional semantic encoder to extract semantic concepts. Secondly, we propose the image reconstruction criterion that evolves from the traditional pixel similarity to semantic similarity and perceptual performance. Thirdly, we design a novel RL-based semantic bit allocation model, whose reward is the increase in rate-semantic-perceptual performance after encoding a certain semantic concept with adaptive quantization level. Thus, the task-related information is preserved and reconstructed properly while less important data is discarded. Finally, we propose the Generative Adversarial Nets (GANs) based semantic decoder that fuses both locally and globally features via an attention module. Experimental results demonstrate that the proposed RL-ASC is noise robust and could reconstruct visually pleasant and semantic consistent image in low bit rate condition.}
}


@article{DBLP:journals/jsac/LeiDHMSW23,
	author = {Zhongyue Lei and
                  Peng Duan and
                  Xuemin Hong and
                  Jo{\~{a}}o F. C. Mota and
                  Jianghong Shi and
                  Cheng{-}Xiang Wang},
	title = {Progressive Deep Image Compression for Hybrid Contexts of Image Classification
                  and Reconstruction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {72--89},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221998},
	doi = {10.1109/JSAC.2022.3221998},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LeiDHMSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Progressive deep image compression (DIC) with hybrid contexts is an under-investigated problem that aims to jointly maximize the utility of a compressed image for multiple contexts or tasks under variable rates. In this paper, we consider the contexts of image reconstruction and classification. We propose a DIC framework, called residual-enhanced mask-based progressive generative coding (RMPGC), designed for explicit control of the performance within the rate-distortion-classification-perception (RDCP) trade-off. Three independent mechanisms are introduced to yield a semantically structured latent representation that can support parameterized control of rate and context adaptation. Experimental results show that the proposed RMPGC outperforms a benchmark DIC scheme using the same generative adversarial nets (GANs) backbone in all six metrics related to classification, distortion, and perception. Moreover, RMPGC is a flexible framework that can be applied to different neural network backbones. Some typical implementations are given and shown to outperform the classic BPG codec and four state-of-the-art DIC schemes in classification and perception metrics, with a slight degradation in distortion metrics. Our proposal of a nonlinear-neural-coded and richly structured latent space makes the proposed DIC scheme well suited for image compression in wireless communications, multi-user broadcasting, and multi-tasking applications.}
}


@article{DBLP:journals/jsac/WangLSCHPYFJ23,
	author = {Hanling Wang and
                  Qing Li and
                  Heyang Sun and
                  Zuozhou Chen and
                  Yingqian Hao and
                  Junkun Peng and
                  Zhenhui Yuan and
                  Junsheng Fu and
                  Yong Jiang},
	title = {VaBUS: Edge-Cloud Real-Time Video Analytics via Background Understanding
                  and Subtraction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {90--106},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221995},
	doi = {10.1109/JSAC.2022.3221995},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangLSCHPYFJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-cloud collaborative video analytics is transforming the way data is being handled, processed, and transmitted from the ever-growing number of surveillance cameras around the world. To avoid wasting limited bandwidth on unrelated content transmission, existing video analytics solutions usually perform temporal or spatial filtering to realize aggressive compression of irrelevant pixels. However, most of them work in a context-agnostic way while being oblivious to the circumstances where the video content is happening and the context-dependent characteristics under the hood. In this work, we propose VaBUS, a real-time video analytics system that leverages the rich contextual information of surveillance cameras to reduce bandwidth consumption for semantic compression. As a task-oriented communication system, VaBUS dynamically maintains the background image of the video on the edge with minimal system overhead and sends only highly confident Region of Interests (RoIs) to the cloud through adaptive weighting and encoding. With a lightweight experience-driven learning module, VaBUS is able to achieve high offline inference accuracy even when network congestion occurs. Experimental results show that VaBUS reduces bandwidth consumption by 25.0%-76.9% while achieving 90.7% accuracy for both the object detection and human keypoint detection tasks.}
}


@article{DBLP:journals/jsac/TandonCPLMMWS23,
	author = {Pulkit Tandon and
                  Shubham Chandak and
                  Pat Pataranutaporn and
                  Yimeng Liu and
                  Anesu M. Mapuranga and
                  Pattie Maes and
                  Tsachy Weissman and
                  Misha Sra},
	title = {Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via
                  Text},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {107--118},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221953},
	doi = {10.1109/JSAC.2022.3221953},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TandonCPLMMWS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video represents the majority of internet traffic today, driving a continual race between the generation of higher quality content, transmission of larger file sizes, and the development of network infrastructure. In addition, the recent COVID-19 pandemic fueled a surge in the use of video conferencing tools. Since videos take up considerable bandwidth (\n∼100\nKbps to a few Mbps), improved video compression can have a substantial impact on network performance for live and pre-recorded content, providing broader access to multimedia content worldwide. We present a novel video compression pipeline, called Txt2Vid, which dramatically reduces data transmission rates by compressing webcam videos (“talking-head videos”) to a text transcript. The text is transmitted and decoded into a realistic reconstruction of the original video using recent advances in deep learning based voice cloning and lip syncing models. Our generative pipeline achieves two to three orders of magnitude reduction in the bitrate as compared to the standard audio-video codecs (encoders-decoders), while maintaining equivalent Quality-of-Experience based on a subjective evaluation by users (\nn=242\n) in an online study. The Txt2Vid framework opens up the potential for creating novel applications such as enabling audio-video communication during poor internet connectivity, or in remote terrains with limited bandwidth. The code for this work is available at https://github.com/tpulkit/txt2vid.git .}
}


@article{DBLP:journals/jsac/KalfaYAGGLTDA23,
	author = {Mert Kalfa and
                  Sadik Yagiz Yetim and
                  Arda Atalik and
                  Mehmetcan Gok and
                  Yiqun Ge and
                  Rong Li and
                  Wen Tong and
                  Tolga M. Duman and
                  Orhan Arikan},
	title = {Reliable Extraction of Semantic Information and Rate of Innovation
                  Estimation for Graph Signals},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {119--140},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221950},
	doi = {10.1109/JSAC.2022.3221950},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/KalfaYAGGLTDA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic signal processing and communications are poised to play a central part in developing the next generation of sensor devices and networks. A crucial component of a semantic system is the extraction of semantic signals from the raw input signals, which has become increasingly tractable with the recent advances in machine learning (ML) and artificial intelligence (AI) techniques. The accurate extraction of semantic signals using the aforementioned ML and AI methods, and the detection of semantic innovation for scheduling transmission and/or storage events are critical tasks for reliable semantic signal processing and communications. In this work, we propose a reliable semantic information extraction framework based on our previous work on semantic signal representations in a hierarchical graph-based structure. The proposed framework includes a time integration method to increase fidelity of ML outputs in a class-aware manner, a graph-edit-distance based metric to detect innovation events at the graph-level and filter out sporadic errors, and a Hidden Markov Model (HMM) to produce smooth and reliable graph signals. The proposed methods within the framework are demonstrated individually and collectively through simulations and case studies based on real-world computer vision examples.}
}


@article{DBLP:journals/jsac/KimSCV23,
	author = {Yongjune Kim and
                  Junyoung Shin and
                  Yuval Cassuto and
                  Lav R. Varshney},
	title = {Distributed Boosting Classification Over Noisy Communication Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {141--154},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221972},
	doi = {10.1109/JSAC.2022.3221972},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/KimSCV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the design of inference-oriented communication systems where multiple transmitters send partial inference values through noisy communication channels, and the receiver aggregates these channel outputs to obtain a reliable final inference. Since large data items are replaced by compact inference values, these systems lead to significant savings of communication resources. In particular, we present a principled framework to optimize communication-resource allocation for distributed boosting classifiers. Boosting classification algorithms make a final decision via a weighted vote from the outputs of multiple base classifiers. Since these base classifiers transmit their partial inference values over noisy channels, communication errors would degrade the final classification accuracy. We formulate communication resource allocation problems to maximize the final classification accuracy by taking into account the importance of base classifiers and the resource budget. To solve these problems rigorously, we formulate convex optimization problems to optimize: 1) transmit-power allocations and 2) transmit-rate allocations. This framework departs from classical communication-systems optimizations in seeking to maximize the classification accuracy rather than the reliability of the individual communicated bits. Results from numerical experiments demonstrate the benefits of our approach.}
}


@article{DBLP:journals/jsac/MuLGA23,
	author = {Xidong Mu and
                  Yuanwei Liu and
                  Li Guo and
                  Naofal Al{-}Dhahir},
	title = {Heterogeneous Semantic and Bit Communications: {A} Semi-NOMA Scheme},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {155--169},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3222000},
	doi = {10.1109/JSAC.2022.3222000},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MuLGA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multiple access (MA) design is investigated to facilitate the coexistence of the emerging semantic transmission and the conventional bit-based transmission in future networks. The semantic rate is adopted for measuring the performance of the semantic transmission. However, a key challenge is that there is no closed-form expression for a key parameter, namely the semantic similarity , which characterizes the sentence similarity between an original sentence and the corresponding recovered sentence. To overcome this challenge, we propose a data regression method, where the semantic similarity is approximated by a generalized logistic function . Using the obtained tractable function, we propose a heterogeneous semantic and bit communication framework, where an access point simultaneously sends the semantic and bit streams to one semantics-interested user (S-user) and one bit-interested user (B-user). To realize this heterogeneous semantic and bit transmission in multi-user networks, three MA schemes are proposed, namely orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), and semi-NOMA. More specifically, the bit stream in semi-NOMA is split into two streams, one is transmitted with the semantic stream over the shared frequency sub-band and the other is transmitted over the separate orthogonal frequency sub-band. To study the fundamental performance limits of the three proposed MA schemes, the semantic-versus-bit (SvB) rate region and the power region are defined. An optimal resource allocation procedure is then derived for characterizing the boundary of the SvB rate region and the power region achieved by each MA scheme. The structures of the derived solutions demonstrate that semi-NOMA is superior to both NOMA and OMA given its highly flexible transmission policy. Our numerical results: 1) confirm that the proposed semi-NOMA is the optimal MA scheme as compared to OMA and NOMA even under the symmetric channel case, and 2) reveal that the superiority of semi-NOMA is more prominent when the channel condition of the S-user is better than that of the B-user.}
}


@article{DBLP:journals/jsac/ZhangSTBL23,
	author = {Hongwei Zhang and
                  Shuo Shao and
                  Meixia Tao and
                  Xiaoyan Bi and
                  Khaled B. Letaief},
	title = {Deep Learning-Enabled Semantic Communication Systems With Task-Unaware
                  Transmitter and Dynamic Data},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {170--185},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221991},
	doi = {10.1109/JSAC.2022.3221991},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangSTBL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing deep learning-enabled semantic communication systems often rely on shared background knowledge between the transmitter and receiver that includes empirical data and their associated semantic information. In practice, the semantic information is defined by the pragmatic task of the receiver and cannot be known to the transmitter. The actual observable data at the transmitter can also have non-identical distribution with the empirical data in the shared background knowledge library. To address these practical issues, this paper proposes a new neural network-based semantic communication system for image transmission, where the task is unaware at the transmitter and the data environment is dynamic. The system consists of two main parts, namely the semantic coding (SC) network and the data adaptation (DA) network. The SC network learns how to extract and transmit the semantic information using a receiver-leading training process. By using the domain adaptation technique from transfer learning, the DA network learns how to convert the data observed into a similar form of the empirical data that the SC network can process without re-training. Numerical experiments show that the proposed method can be adaptive to observable datasets while keeping high performance in terms of both data recovery and task execution.}
}


@article{DBLP:journals/jsac/KangDLXMNL23,
	author = {Jiawen Kang and
                  Hongyang Du and
                  Zonghang Li and
                  Zehui Xiong and
                  Shiyao Ma and
                  Dusit Niyato and
                  Yuan Li},
	title = {Personalized Saliency in Task-Oriented Semantic Communications: Image
                  Transmission and Performance Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {186--201},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221990},
	doi = {10.1109/JSAC.2022.3221990},
	timestamp = {Fri, 28 Jun 2024 14:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KangDLXMNL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication, as a promising technology, has emerged to break through the Shannon limit, which is envisioned as the key enabler and fundamental paradigm for future 6G networks and applications, e.g., smart healthcare. In this paper, we focus on UAV image-sensing-driven task-oriented semantic communications scenarios. The majority of existing work has focused on designing advanced algorithms for high-performance semantic communication. However, the challenges, such as energy-hungry and efficiency-limited image retrieval manner, and semantic encoding without considering user personality, have not been explored yet. These challenges have hindered the widespread adoption of semantic communication. To address the above challenges, at the semantic level, we first design an energy-efficient task-oriented semantic communication framework with a triple-based scene graph for image information. We then design a new personalized semantic encoder based on user interests to meet the requirements of personalized saliency. Moreover, at the communication level, we study the effects of dynamic wireless fading channel on semantic transmission mathematically and thus design an optimal multi-user resource allocation scheme by using game theory. Numerical results based on real-world datasets clearly indicate that the proposed framework and schemes significantly enhance the personalization and anti-interference performance of semantic communication, and are also efficient to improve the communication quality of semantic communication services.}
}


@article{DBLP:journals/jsac/DongLXHWZ23,
	author = {Chen Dong and
                  Haotai Liang and
                  Xiaodong Xu and
                  Shujun Han and
                  Bizhu Wang and
                  Ping Zhang},
	title = {Semantic Communication System Based on Semantic Slice Models Propagation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {202--213},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221948},
	doi = {10.1109/JSAC.2022.3221948},
	timestamp = {Wed, 22 Feb 2023 21:12:12 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DongLXHWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional communication systems treat messages’ semantic aspects and meaning as irrelevant to communication, revealing its limitations in the era of artificial intelligence (AI), such as communication efficiency and intent-sharing among different entities. Through broadening the scope of the traditional communication system and the AI-based encoding techniques, in this manuscript, we present a novel semantic communication system, which involves the essential semantic information exploration, transmission and recovery for more efficient communications. Compared to other state-of-the-art semantic communication-related works, our proposed semantic communication system is characterized by the “flow of the intelligence” via the propagation of the model. Besides, the concept of semantic slice-models (SeSM) is proposed to enable flexible model-resembling under the different requirements of the model performance, channel situation and transmission goals. Specifically, a layer-based semantic communication system for images (LSCI) is built on the simulation platform to demonstrate the feasibility of the proposed system and a novel semantic metric called semantic service quality (SS) is proposed to evaluate the semantic communication systems. We evaluate the proposed system on Cityscapes and Open Images datasets, resulting in averaged 10% and 2% bit rate reduction over JPEG and JPEG2000, respectively. In comparison to LDPC, the proposed channel coding scheme can averagely save 2dB and 5dB in AWGN channel and Rayleigh fading channel, respectively.}
}


@article{DBLP:journals/jsac/WangDLNSDQZ23,
	author = {Sixian Wang and
                  Jincheng Dai and
                  Zijian Liang and
                  Kai Niu and
                  Zhongwei Si and
                  Chao Dong and
                  Xiaoqi Qin and
                  Ping Zhang},
	title = {Wireless Deep Video Semantic Transmission},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {214--229},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221977},
	doi = {10.1109/JSAC.2022.3221977},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangDLNSDQZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we design a new class of high-efficiency deep joint source-channel coding methods to achieve end-to-end video transmission over wireless channels. The proposed methods exploit nonlinear transform and conditional coding architecture to adaptively extract semantic features across video frames, and transmit semantic feature domain representations over wireless channels via deep joint source-channel coding. Our framework is collected under the name deep video semantic transmission (DVST). In particular, benefiting from the strong temporal prior provided by the feature domain context, the learned nonlinear transform function becomes temporally adaptive, resulting in a richer and more accurate entropy model guiding the transmission of current frame. Accordingly, a novel rate adaptive transmission mechanism is developed to customize deep joint source-channel coding for video sources. It learns to allocate the limited channel bandwidth within and among video frames to maximize the overall transmission performance. The whole DVST design is formulated as an optimization problem whose goal is to minimize the end-to-end transmission rate-distortion performance under perceptual quality metrics or machine vision task performance metrics. Across standard video source test sequences and various communication scenarios, experiments show that our DVST can generally surpass traditional wireless video coded transmission schemes. The proposed DVST framework can well support future semantic communications due to its video content-aware and machine vision task integration abilities.}
}


@article{DBLP:journals/jsac/JiangWJL23,
	author = {Peiwen Jiang and
                  Chao{-}Kai Wen and
                  Shi Jin and
                  Geoffrey Ye Li},
	title = {Wireless Semantic Communications for Video Conferencing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {230--244},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221968},
	doi = {10.1109/JSAC.2022.3221968},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JiangWJL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video conferencing has become a popular mode of meeting despite consuming considerable communication resources. Conventional video compression causes resolution reduction under a limited bandwidth. Semantic video conferencing (SVC) maintains a high resolution by transmitting some keypoints to represent the motions because the background is almost static, and the speakers do not change often. However, the study on the influence of transmission errors on keypoints is limited. In this paper, an SVC network based on keypoint transmission is established, which dramatically reduces transmission resources while only losing detailed expressions. Transmission errors in SVC only lead to a changed expression, whereas those in the conventional methods directly destroy pixels. However, the conventional error detector, such as cyclic redundancy check, cannot reflect the degree of expression changes. To overcome this issue, an incremental redundancy hybrid automatic repeat-request framework for varying channels (SVC-HARQ) incorporating a novel semantic error detector is developed. SVC-HARQ has flexibility in bit consumption and achieves a good performance. In addition, SVC-channel state information (CSI) is designed for CSI feedback to allocate the keypoint transmission and enhance the performance dramatically. Simulation shows that the proposed wireless semantic communication system can remarkably improve transmission efficiency.}
}


@article{DBLP:journals/jsac/HanYSHZ23,
	author = {Tianxiao Han and
                  Qianqian Yang and
                  Zhiguo Shi and
                  Shibo He and
                  Zhaoyang Zhang},
	title = {Semantic-Preserved Communication System for Highly Efficient Speech
                  Transmission},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {245--259},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221952},
	doi = {10.1109/JSAC.2022.3221952},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HanYSHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL) based semantic communication methods have been explored for the efficient transmission of images, text, and speech in recent years. In contrast to traditional wireless communication methods that focus on the transmission of abstract symbols, semantic communication approaches attempt to achieve better transmission efficiency by only sending the semantic-related information of the source data. In this paper, we consider semantic-oriented speech transmission which transmits only the semantic-relevant information over the channel for the speech recognition task, and a compact additional set of semantic-irrelevant information for the speech reconstruction task. We propose a novel end-to-end DL-based transceiver which extracts and encodes the semantic information from the input speech spectrums at the transmitter and outputs the corresponding transcriptions from the decoded semantic information at the receiver. In particular, we employ a soft alignment module and a redundancy removal module to extract only the text-related semantic features while dropping semantically redundant content, greatly reducing the amount of semantic redundancy compared to existing methods. We also propose a semantic correction module to further correct the predicted transcription with semantic knowledge by leveraging a pretrained language model. For the speech to speech transmission, we further include a CTC alignment module that extracts a small number of additional semantic-irrelevant but speech-related information, such as duration, pitch, power and speaker identification of the speech for the better reconstruction of the original speech signals at the receiver. We also introduce a two-stage training scheme which speeds up the training of the proposed DL model. The simulation results confirm that our proposed method outperforms current methods in terms of the accuracy of the predicted text for the speech to text transmission and the quality of the recovered speech signals for the speech to speech transmission, and significantly improves transmission efficiency. More specifically, the proposed method only sends 16% of the amount of the transmitted symbols required by the existing methods while achieving about a 10% reduction in WER for the speech to text transmission. For the speech to speech transmission, it results in an even more remarkable improvement in terms of transmission efficiency with only 0.2% of the amount of the transmitted symbols required by the existing method while preserving the comparable quality of the reconstructed speech signals.}
}


@article{DBLP:journals/jsac/XuAWC23,
	author = {Jialong Xu and
                  Bo Ai and
                  Ning Wang and
                  Wei Chen},
	title = {Deep Joint Source-Channel Coding for {CSI} Feedback: An End-to-End
                  Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {260--273},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221963},
	doi = {10.1109/JSAC.2022.3221963},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XuAWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increased throughput brought by MIMO technology relies on the knowledge of channel state information (CSI) acquired in the base station (BS). To make the CSI feedback overhead affordable for the evolution of MIMO technology (e.g., massive MIMO and ultra-massive MIMO), deep learning (DL) is introduced to deal with the CSI compression task. In traditional communication systems, the compressed CSI bits is treated equally and expected to be transmitted accurately over the noisy channel. While the errors occur due to the limited bandwidth or low signal-to-noise ratios (SNRs), the reconstruction performance of the CSI degrades drastically. As a branch of semantic communications, deep joint source-channel coding (DJSCC) scheme performs better than the separate source-channel coding (SSCC) scheme—the cornerstone of traditional communication systems—in the limited bandwidth and low SNRs. In this paper, we propose a DJSCC based framework for the CSI feedback task. In particular, the proposed method can simultaneously learn from the CSI source and the wireless channel. Instead of truncating CSI via Fourier transform in the delay domain in existing methods, we apply non-linear transform networks to compress the CSI. Furthermore, we adopt an SNR adaption mechanism to deal with wireless channel variations. The extensive experiments demonstrate the validity, adaptability, and generality of the proposed framework.}
}


@article{DBLP:journals/jsac/LinGH23,
	author = {Zhenyi Lin and
                  Yi Gong and
                  Kaibin Huang},
	title = {Distributed Over-the-Air Computing for Fast Distributed Optimization:
                  Beamforming Design and Convergence Analysis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {274--287},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3223661},
	doi = {10.1109/JSAC.2022.3223661},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LinGH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed optimization finds a wide range of applications ranging from machine learning to vehicle platooning. To overcome the bottleneck caused by the required extensive message exchange, we propose in this work the framework of distributed over-the-air computing (AirComp) to realize a one-step aggregation for distributed optimization. Equivalently, the technique superimposes multiple instances of conventional AirComp processes, giving rise to the challenge of jointly designing multicast beamforming at devices to rein in errors due to interference and channel distortion. We consider two design criteria. One is to minimize the sum AirComp error (i.e., sum mean-squared error (MSE)) with respect to the desired average-functional values. An efficient solution approach is proposed by transforming the non-convex beamforming problem into an equivalent concave-convex fractional program and solving it by nesting convex programming into a bisection search. The other one, called zero-forcing (ZF) multicast beamforming, is to force the received over-the-air aggregated signals at devices to be equal to the desired functional values, where the optimal beamforming admits closed form. Last, the convergence of a classic distributed optimization algorithm is analyzed. The distributed AirComp is found experimentally to accelerate convergence by dramatically reducing communication latency.}
}


@article{DBLP:journals/jsac/MengSZM23,
	author = {Zhen Meng and
                  Changyang She and
                  Guodong Zhao and
                  Daniele De Martini},
	title = {Sampling, Communication, and Prediction Co-Design for Synchronizing
                  the Real-World Device and Digital Model in Metaverse},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {1},
	pages = {288--300},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3221993},
	doi = {10.1109/JSAC.2022.3221993},
	timestamp = {Sun, 15 Jan 2023 18:31:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MengSZM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The metaverse has the potential to revolutionize the next generation of the Internet by supporting highly interactive services with satisfactory user experience. The synchronization between devices in the physical world and their digital models in the metaverse is crucial. This work proposes a sampling, communication and prediction co-design framework to minimize the communication load subject to a constraint on the tracking error. To optimize the sampling rate and the prediction horizon, we exploit expert knowledge and develop a constrained deep reinforcement learning algorithm. We validate our framework on a prototype composed of a real-world robotic arm and its digital model. The results show that our framework achieves a better trade-off between the average tracking error and the average communication load compared with a communication system without sampling and prediction. For example, the average communication load can be reduced up to 87% when the average track error constraint is 0.007°. In addition, our policy outperforms the benchmark with the static sampling rate and prediction horizon optimized by exhaustive search, in terms of the tail probability of the tracking error. Furthermore, with the assistance of expert knowledge, the proposed algorithm achieves better convergence time, stability, communication load, and average tacking error.}
}


@article{DBLP:journals/jsac/WangYJZNTJ23,
	author = {Kunlun Wang and
                  Yang Yang and
                  Jiong Jin and
                  Tao Zhang and
                  Arumugam Nallanathan and
                  Chintha Tellambura and
                  Bijan Jabbari},
	title = {Guest Editorial Multi-Tier Computing for Next Generation Wireless
                  Networks - Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {301--305},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3228093},
	doi = {10.1109/JSAC.2022.3228093},
	timestamp = {Sun, 12 Nov 2023 02:18:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangYJZNTJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-tier computing effectively enables flexible computation and communication resource sharing by offloading computation-intensive tasks to nearby servers along the cloud-to-thing continuum. In essence, multi-tier computing networks can distribute computing, storage, and communication functions anywhere between the cloud and the endpoint to take full advantage of the resources available along this continuum, thus extending the traditional cloud computing architecture to the edge of the network. With multi-tier computing, some application component processing, such as delay-sensitive components, can take place at the edge of the network, while other components, such as time-tolerant and computation-intensive components, can be performed in the cloud. To best meet user requirements, centralized cloud computing with extensive resources, secure environments, and powerful algorithms is still needed, but also must be complemented by distributed fog and edge computing with shared resources, accessible environments, and simple algorithms for real-time decision-making. Given heterogeneous computing resources and collaborative service architectures, future multi-tier computing networks will be capable of supporting a full range of computing and networking services for different environments and applications. This Special Issue aims to provide a forum for the latest advances in multi-tier computing for next-generation wireless network research, innovations, and applications. Multi-tier computing enables low-latency processing by allowing data to be processed at the network edge close to end devices. It also facilitates the distribution of fog/edge nodes to collect data from end devices. Therefore, multi-tier computing effectively complements the cloud computing architecture.}
}


@article{DBLP:journals/jsac/WangJYZNTJ23,
	author = {Kunlun Wang and
                  Jiong Jin and
                  Yang Yang and
                  Tao Zhang and
                  Arumugam Nallanathan and
                  Chintha Tellambura and
                  Bijan Jabbari},
	title = {Task Offloading With Multi-Tier Computing Resources in Next Generation
                  Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {306--319},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227102},
	doi = {10.1109/JSAC.2022.3227102},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangJYZNTJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of next-generation wireless networks, the Internet of Things (IoT) is evolving towards the intelligent IoT (iIoT), where intelligent applications usually have stringent delay and jitter requirements. In order to provide low-latency services to heterogeneous users in the emerging iIoT, multi-tier computing was proposed by effectively combining edge computing and fog computing. More specifically, multi-tier computing systems compensate for cloud computing through task offloading and dispersing computing tasks to multi-tier nodes along the continuum from the cloud to things. In this paper, we investigate key techniques and directions for wireless communications and resource allocation approaches to enable task offloading in multi-tier computing systems. A multi-tier computing model, with its main functionality and optimization methods, is presented in detail. We hope that this paper will serve as a valuable reference and guide to the theoretical, algorithmic, and systematic opportunities of multi-tier computing towards next-generation wireless networks.}
}


@article{DBLP:journals/jsac/XuLKD23,
	author = {Sai Xu and
                  Jiajia Liu and
                  Nei Kato and
                  Ya{-}Nan Du},
	title = {Intelligent Reflecting Surface Backscatter Enabled Multi-Tier Computing
                  for 6G Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {320--333},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3231861},
	doi = {10.1109/JSAC.2022.3231861},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XuLKD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a novel framework of intelligent reflecting surface (IRS) backscatter enabled multi-tier computing system. In such a hierarchical network, the data bits of computational task requested by each user equipment (UE) are broken up into three parts, which are respectively computed at tier-1 UEs, tier-2 access points (APs) and a tier-3 central server. Distinguished from conventional active antennas, IRS backscatter at the UEs is leveraged to offload data bits to the APs. Based on the established network framework, an optimization problem is formulated, which aims at maximizing the sum computational bits of system during the considered time block by jointly optimizing the active beamforming at the power beacon, the passive beamforming at the UEs, the active beamforming at the APs, the bandwidth and power allocation among all the UEs, as well as the time of local computing. To seek the optimal solution, the optimization problem is decomposed into two, namely the maximization of stage-1 sum computational bits and the minimization of stage-2 delay. By the objective function conversion and alternative optimization methods, the two problems are addressed. Extensive simulations are performed to confirm the feasibility of the proposed system and show the achievable performance in processing computational bits.}
}


@article{DBLP:journals/jsac/LiCLTL23,
	author = {Zhendong Li and
                  Wen Chen and
                  Ziwei Liu and
                  Hongying Tang and
                  Jianmin Lu},
	title = {Joint Communication and Computation Design in Transmissive {RMS} Transceiver
                  Enabled Multi-Tier Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {334--348},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3228553},
	doi = {10.1109/JSAC.2022.3228553},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiCLTL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a novel transmissive reconfigurable meta-surface (RMS) transceiver enabled multi-tier computing network architecture is proposed for improving computing capability, decreasing computing delay and reducing base station (BS) deployment cost, in which transmissive RMS equipped with a feed antenna can be regarded as a new type of multi-antenna system. We formulate a total energy consumption minimization problem by a joint optimization of subcarrier allocation, task input bits, time slot allocation, transmit power allocation and RMS transmissive coefficient while taking into account the constraints of communication resources and computing resources. This formulated problem is a non-convex optimization problem due to the high coupling of optimization variables, which is NP-hard to obtain its optimal solution. To address the above challenging problems, block coordinate descent (BCD) technique is employed to decouple the optimization variables to solve the problem. Specifically, the joint optimization problem of subcarrier allocation, task input bits, time slot allocation, transmit power allocation and RMS transmissive coefficient is divided into three subproblems to solve by applying BCD. Then, the decoupled three subproblems are optimized alternately by using successive convex approximation (SCA) and difference-convex (DC) programming until the convergence is achieved. Numerical results verify that our proposed algorithm is superior in reducing total energy consumption compared to other benchmarks.}
}


@article{DBLP:journals/jsac/ChenWLWF23,
	author = {Guangji Chen and
                  Qingqing Wu and
                  Ruiqi Liu and
                  Jingxian Wu and
                  Chao Fang},
	title = {{IRS} Aided {MEC} Systems With Binary Offloading: {A} Unified Framework
                  for Dynamic {IRS} Beamforming},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {349--365},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3228605},
	doi = {10.1109/JSAC.2022.3228605},
	timestamp = {Tue, 14 May 2024 17:02:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenWLWF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we develop a unified dynamic intelligent reflecting surface (IRS) beamforming framework to boost the sum computation rate of an IRS-aided mobile edge computing (MEC) system, where each device follows a binary offloading policy. Specifically, the task of each device has to be either executed locally or offloaded to MEC servers as a whole with the aid of given number of IRS beamforming vectors available. By flexibly controlling the number of times for IRS reconfiguring phase-shifts, the system can achieve a balance between the performance and associated signalling overhead. We aim to maximize the sum computation rate by jointly optimizing the computational mode selection for each device, offloading time allocation, and IRS beamforming vectors across time. Since the resulting optimization problem is non-convex and NP-hard, there are generally no standard methods to solve it optimally. To tackle this problem, we first propose a penalty-based successive convex approximation algorithm, where all the associated variables in the inner-layer iterations are optimized simultaneously and the obtained solution is guaranteed to be locally optimal. Then, we further derive the offloading activation condition for each device by deeply exploiting the intrinsic structure of the original optimization problem. According to the offloading activation condition, a low-complexity algorithm based on the successive refinement method is proposed to obtain high-quality suboptimal solutions, which are more appealing for practical systems with a large number of devices and IRS elements. Moreover, the optimal condition for the proposed low-complexity algorithm is revealed. The effectiveness of the proposed algorithms is demonstrated through numerical examples. In addition, the results illustrate the practical significance of the IRS in MEC systems for achieving coverage extension and supporting multiple energy-limited devices for task offloading, and also unveil the fundamental performance-cost tradeoff embedded in the proposed dynamic IRS beamforming framework.}
}


@article{DBLP:journals/jsac/ZhuJ23,
	author = {Xiangming Zhu and
                  Chunxiao Jiang},
	title = {Delay Optimization for Cooperative Multi-Tier Computing in Integrated
                  Satellite-Terrestrial Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {366--380},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227083},
	doi = {10.1109/JSAC.2022.3227083},
	timestamp = {Thu, 20 Apr 2023 16:33:36 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhuJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The integrated satellite-terrestrial network is promising to provide global broadband communication service. However, the long propagation delay of satellite-terrestrial links will lead to high communication delay when users access the Internet via satellites. In this paper, we investigate the cooperative multi-tier computing in the integrated satellite-terrestrial network, in which the computation tasks of users are processed by leveraging the cooperation of devices, edge nodes, and cloud servers. Based on the proposed three-tier computing framework, we formulate the cooperative edge-cloud offloading problem to minimize the total delay of the network. Considering the computation task is dividable, we propose the optimal task splitting strategy based on the partial offloading model, in which the closed-form solution is derived for each computation task. With the optimal task splitting strategy obtained, the original optimization problem is reformulated as the problem of the time slot allocation strategy and the computation capacity allocation strategy. Then, we further propose the cooperative edge-cloud computing strategy to optimize the delay performance of the network. Finally, numerical results are presented to demonstrate the performance of the proposed three-tier computing architecture and the offloading strategies.}
}


@article{DBLP:journals/jsac/CaoYSYZHPH23,
	author = {Xuelin Cao and
                  Bo Yang and
                  Yulong Shen and
                  Chau Yuen and
                  Yan Zhang and
                  Zhu Han and
                  H. Vincent Poor and
                  Lajos Hanzo},
	title = {Edge-Assisted Multi-Layer Offloading Optimization of {LEO} Satellite-Terrestrial
                  Integrated Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {381--398},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227032},
	doi = {10.1109/JSAC.2022.3227032},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CaoYSYZHPH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sixth-Generation (6G) technologies will revolutionize the wireless ecosystem by enabling the delivery of futuristic services through satellite-terrestrial integrated networks (STINs). As the number of subscribers connected to STINs increases, it becomes necessary to investigate whether the edge computing paradigm may be applied to low Earth orbit satellite (LEOS) networks for supporting computation-intensive and delay-sensitive services for anyone, anywhere, and at any time. Inspired by this research dilemma, we investigate a LEOS edge-assisted multi-layer multi-access edge computing (MEC) system. In this system, the MEC philosophy will be extended to LEOS, for defining the LEOS edge, in order to enhance the coverage of the multi-layer MEC system and address the users’ computing problems both in congested and isolated areas. We then design its operating offloading framework and explore its feasible implementation methodologies. In this context, we formulate a joint optimization problem for the associated communication and computation resource allocation for minimizing the overall energy dissipation of our LEOS edge-assisted multi-layer MEC system while maintaining a low computing latency. To solve the optimization problem effectively, we adopt the classic alternating optimization (AO) method for decomposing the original problem and then solve each sub-problem using low-complexity iterative algorithms. Finally, our numerical results show that the offloading scheme conceived achieves low computing latency and energy dissipation compared to the state-of-the-art solutions, a single layer MEC supported by LEOS or base stations (BS).}
}


@article{DBLP:journals/jsac/ChenMQC23,
	author = {Qian Chen and
                  Weixiao Meng and
                  Tony Q. S. Quek and
                  Shuyi Chen},
	title = {Multi-Tier Hybrid Offloading for Computation-Aware IoT Applications
                  in Civil Aircraft-Augmented {SAGIN}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {399--417},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227031},
	doi = {10.1109/JSAC.2022.3227031},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenMQC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Satellites and civil aircrafts (CAs) with computing ability are valuable access platforms, making it possible for Internet of Things (IoT) devices to offload their computation-intensive tasks in remote areas without network infrastructures. Unlike existing works mainly focused on the static scenarios or the interaction between any two types of local, edge and cloud nodes, we propose an innovative multi-tier hybrid parallel computation architecture in CA-augmented space-air-ground integrated networks (CAA-SAGIN). Specifically, devices perform local computing, CAs and satellites act as edge servers, and ground stations of satellite networks operate cloud computing. Aiming to minimize the weighted sum of end-to-end (E2E) delay and energy consumption, we formulate a partial computation offloading problem by jointly considering access strategy, transmit power, computing resource allocation, offloading ratio and delay tolerance. The platform selection exists both within and between layers, and there are inner- and inter-coupling relationships between communication and computing resources. The issue is solved by the proposed multi-tier partial task offloading (MPTO) algorithm. The original problem is firstly decomposed into primal and master subproblems by generalized benders decomposition (GBD) method, and parallel successive convex approximation (SCA) theory is utilized to transform the multi-variable NP-hard master problem into a convex one. Simulation results demonstrate the convergence and optimality of the MPTO algorithm and the advantages of this multi-tier hybrid computation offloading system. Also, the optimal tradeoff between E2E delay and energy consumption can be achieved by the MPTO algorithm.}
}


@article{DBLP:journals/jsac/LiuYL23,
	author = {Wei Liu and
                  Huiting Yang and
                  Jiandong Li},
	title = {Multi-Functional Time Expanded Graph: {A} Unified Graph Model for
                  Communication, Storage, and Computation for Dynamic Networks Over
                  Time},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {418--431},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3233533},
	doi = {10.1109/JSAC.2022.3233533},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuYL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space-air-ground integrated network (SAGIN) aided multi-tier computing network can be modelled as a dynamic and predictable network. For the SAGIN aided multi-tier computing network, the traditional time expanded graph (TEG) can only jointly model communication and storage capability, as well as one computing function for one mission flow within one same node. However, for multiple computing functions for one mission flow in one same node, TEG is not applicable. In this paper, for SAGIN aided multi-tier computing networks, we propose an multi-functional time expanded graph (MF-TEG) to jointly model the communication, storage, and computation capability of nodes where multiple computing functions for one mission flow in one same node can be characterized. Specifically, based on TEG, for each node having computation functions, we adopt the virtual network graph (VNG) to virtually decompose it into three virtual components: sub-virtual node, virtual computing nodes, and virtual transmission links, where the virtual computing node provides the computing function. We characterize the amount of data flow on each link and also present four kinds of fundamental constraints for the data flow in the MF-TEG for joint communication, storage, and computing function: computation capacity constraints, communication capacity constraints, storage capacity constraints, and flow conservation constraints. We provide one example of using MF-TEG to model the SAGIN aided multi-tier computing network with a service function chain (SFC), where satellite nodes could provide communication, storage, and multiple computing functions for one mission flow in one same node, where TEG is not valid. Furthermore, simulation results show that for SAGIN aided multi-tier computing network, the proposed MF-TEG model significantly outperforms the snapshot graph-aided VNG (SSG-aided VNG) model. The reason for that is only communication and computation capability is considered by the SSG-aided VNG model, while storage capability is not exploited.}
}


@article{DBLP:journals/jsac/FengLZWAC23,
	author = {Weiyang Feng and
                  Siyu Lin and
                  Ning Zhang and
                  Gongpu Wang and
                  Bo Ai and
                  Lin Cai},
	title = {Joint {C-V2X} Based Offloading and Resource Allocation in Multi-Tier
                  Vehicular Edge Computing System},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {432--445},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227081},
	doi = {10.1109/JSAC.2022.3227081},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FengLZWAC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging intelligent transportation services are latency-sensitive with heavy demand for computing resources, which can be supported by a multi-tier computing system composed of vehicular edge computing (VEC) servers along the roads and micro servers on vehicles. In this work, we investigate the dual Uu/PC5 interface offloading and resource allocation strategy in Cellular Vehicle-to-Everything (C-V2X) enabled multi-tier VEC system. The successful transmission probability is characterized to obtain the normalized transmission rate of PC5 interface. We aim to minimize the system latency of task processing while satisfying the resource requirements of Uu and PC5 interfaces. Due to the non-convex and variables coupling, we decompose the original problem into two subproblems, i.e., resource allocation and offloading strategy subproblems. Specifically, we derive the closed-form expressions of packet transmit frequency of PC5 interface, transmission power of Uu interface, and CPU computation frequency in the resource allocation subproblem. Moreover, for the offloading strategy subproblem, the offloading ratio matrix is obtained by proposing the PC5 interface based greedy offloading (PC5-GO) algorithm, which concludes offloading decision and ratio. Simulation results are provided that the proposed PC5-GO algorithm can significantly improve the system performance compared with other baseline schemes by 13.7% at least.}
}


@article{DBLP:journals/jsac/ZhangFLLK23,
	author = {Haijun Zhang and
                  Lizhe Feng and
                  Xiangnan Liu and
                  Keping Long and
                  George K. Karagiannidis},
	title = {User Scheduling and Task Offloading in Multi-Tier Computing 6G Vehicular
                  Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {446--456},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227097},
	doi = {10.1109/JSAC.2022.3227097},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangFLLK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many real-time application scenarios are developed in 6G communications. Driven by the low-latency data processing requirements, multi-tier computing has become an important technology to improve user experience and reduce network overhead. In this paper, we consider a multi-tier computation offloading network structure for 6G applications, in which the cloud computing center and the nearby vehicle edge server (VES) are able to partially calculate the tasks offloaded from the user equipment (UE), and the remaining task is processed locally in the UE. By jointly optimizing user scheduling, cloud offloading ratio, VES offloading ratio, and VES mobility, the objective function is to minimize the delay of the system transmission and computation under the constraints of discrete variables and energy consumption. To solve the problem, a primal-dual deep deterministic policy gradient (PD-DDPG) algorithm based on multi-tier computation offloading is proposed. Simultaneously, compared with baseline algorithms, PD-DDPG algorithm has an obvious advantage in both the speed of convergence and the system delay.}
}


@article{DBLP:journals/jsac/XiaoSJMCH23,
	author = {Zhu Xiao and
                  Jinmei Shu and
                  Hongbo Jiang and
                  Geyong Min and
                  Hongyang Chen and
                  Zhu Han},
	title = {Perception Task Offloading With Collaborative Computation for Autonomous
                  Driving},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {457--473},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227027},
	doi = {10.1109/JSAC.2022.3227027},
	timestamp = {Sat, 30 Sep 2023 10:20:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XiaoSJMCH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Autonomous driving has so far received numerous attention from academia and industry. However, the inevitable occlusion is a great menace to safety and reliable driving. Existing works have primarily focused on improving the perception ability of a single autonomous vehicle (AV), but the safety problem brought by occlusions remains unanswered. In this paper, we propose a multi-tier perception task offloading framework with a collaborative computing approach, where an AV is able to achieve a comprehensive perception of the concerned region-of-interest (RoI) by leveraging collaborative computation with nearby AVs and road side units (RSUs). Besides, the collaborative computation provides offloading service for computationally intensive tasks so as to reduce processing delay. Specifically, we formulate a joint problem of perception task assignment, offloading and resource allocation, by fully considering the AV’s mobility, task dependency, and delay requirement. The collaborative offloading is modeled as a mixed-integer nonlinear programming (MINLP) problem. We design a two-layer binary intelligent firefly (TL-BIFA) algorithm to solve MINLP, with the goal of minimizing execution delay. The proposed TL-BIFA synthesizes the advantages of heuristic methods and deterministic methods. Through extensive simulations, the proposed collaborative offloading approach and the TL-BIFA show superiority in enhancing the autonomous driving system’s safety, efficiency and resource utilization.}
}


@article{DBLP:journals/jsac/HouWBDRH23,
	author = {Xiangwang Hou and
                  Jingjing Wang and
                  Tong Bai and
                  Yansha Deng and
                  Yong Ren and
                  Lajos Hanzo},
	title = {Environment-Aware {AUV} Trajectory Design and Resource Management
                  for Multi-Tier Underwater Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {474--490},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227103},
	doi = {10.1109/JSAC.2022.3227103},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HouWBDRH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of underwater things (IoUT) is envisioned to be an essential part of maritime activities. Given the IoUT devices’ wide-area distribution and constrained transmit power, autonomous underwater vehicles (AUVs) have been widely adopted for collecting and forwarding the data sensed by IoUT devices to the surface-stations. In order to accommodate the diverse requirements of IoUT applications, it is imperative to conceive a multi-tier underwater computing (MTUC) framework by carefully harnessing both the computing and the communications as well as the storage resources of both the surface-station and of the AUVs as well as of the IoUT devices. Furthermore, to meet the stringent energy constraints of the IoUT devices and to reduce the operating cost of the MTUC framework, a joint environment-aware AUV trajectory design and resource management problem is formulated, which is a high-dimensional NP-hard problem. To tackle this challenge, we first transform the problem into a Markov decision process (MDP) and solve it with the aid of the asynchronous advantage actor-critic (A3C) algorithm. Our simulation results demonstrate the superiority of our scheme.}
}


@article{DBLP:journals/jsac/LinHJLSL23,
	author = {Chuan Lin and
                  Guangjie Han and
                  Jinfang Jiang and
                  Chao Li and
                  Syed Bilal Hussain Shah and
                  Qian Liu},
	title = {Underwater Pollution Tracking Based on Software-Defined Multi-Tier
                  Edge Computing in 6G-Based Underwater Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {491--503},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3233625},
	doi = {10.1109/JSAC.2022.3233625},
	timestamp = {Sat, 17 Jun 2023 16:21:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LinHJLSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The forthcoming 6G networks are expected to provide a vision of overlapping aerial-ground-underwater wireless networks. Meanwhile, the rapid development of the Internet of Underwater Things (IoUTs) brings forth many categories of Autonomous Underwater Vehicle (AUV)-assisted Underwater Wireless Networks (UWNs). In this paper, we argue that the AUV-assisted UWNs can be intelligently utilized to track underwater pollution. To perform smart underwater pollution tracking, we propose the paradigm of AUV flock-based networking system and Software-Defined Networking (SDN)-enabled AUV flock Networking System (SDN-AUVNS). We introduce the concept of Mobile Edge Computing (MEC) into the control of SDN-AUVNS and propose the upgrade of the control plane of the SDN-AUVNS to with the multi-tier edge computing ability. By the proposed system architecture, we adopt the artificial potential field theory to construct the network controlling model. And we present the underwater tracking model for SDN-AUVNS, especially for the underwater pollution equipotential line of a particular concentration. Furthermore, to provide accurate path planning for the equipotential line tracking, we utilize the linearizability mechanism to optimize and revise the control input for the SDN-AUVNS. Lastly, we give a fast united control algorithm that can intelligently schedule the SDN-AUVNS to track underwater pollution equipotential lines. In particular, we propose a smart approach with the name of ’Inverse Distance Weighting’ to optimize the detection sample of the SDN-AUVNS. Evaluation results indicate that our proposal is able to track/survey the equipotential lines within a satisfactory error.}
}


@article{DBLP:journals/jsac/LuoLSF23,
	author = {Quyuan Luo and
                  Tom H. Luan and
                  Weisong Shi and
                  Pingzhi Fan},
	title = {Deep Reinforcement Learning Based Computation Offloading and Trajectory
                  Planning for Multi-UAV Cooperative Target Search},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {504--520},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3228558},
	doi = {10.1109/JSAC.2022.3228558},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LuoLSF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) are widely used for surveillance and monitoring to complete target search tasks. However, the short battery life and moderate computational capability hinder UAVs to process computation-intensive tasks. The emerging edge computing technologies can alleviate this problem by offloading tasks to the ground edge servers. How to evaluate the search process so as to make optimal offloading decisions and make optimal flying trajectories represent fundamental research challenges. In this paper, we propose to utilize the concept of uncertainty to evaluate the search process, which reflects the reliability of the target search results. Thereafter, we propose a deep reinforcement learning (DRL) technique to jointly make optimal computation offloading decisions and flying orientation choices for multi-UAV cooperative target search. Specifically, we first formulate an uncertainty minimization problem based on the established system model. By introducing a reward function, we prove that the uncertainty minimization problem is equivalent to a reward maximization problem, which is further analyzed by a Markov decision process (MDP). To obtain the optimal task offloading decisions and flying orientation choices, a deep Q-network (DQN) based DRL architecture with a separated Q-network is then proposed. Finally, extensive simulations validate the effectiveness of the proposed techniques, and comprehensive discussions on how different parameters affect the search performance are given.}
}


@article{DBLP:journals/jsac/Van-HuynhNCKPD23,
	author = {Dang Van{-}Huynh and
                  Van{-}Dinh Nguyen and
                  Symeon Chatzinotas and
                  Saeed R. Khosravirad and
                  H. Vincent Poor and
                  Trung Q. Duong},
	title = {Joint Communication and Computation Offloading for Ultra-Reliable
                  and Low-Latency With Multi-Tier Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {521--537},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227088},
	doi = {10.1109/JSAC.2022.3227088},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/Van-HuynhNCKPD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study joint communication and computation offloading (JCCO) for hierarchical edge-cloud systems with ultra-reliable and low latency communications (URLLC). We aim to minimize the end-to-end (e2e) latency of computational tasks among multiple industrial Internet of Things (IIoT) devices by jointly optimizing offloading probabilities, processing rates, user association policies and power control subject to their service delay and energy consumption requirements as well as queueing stability conditions. The formulated JCCO problem belongs to a difficult class of mixed-integer non-convex optimization problem, making it computationally intractable. In addition, a strong coupling between binary and continuous variables and the large size of hierarchical edge-cloud systems make the problem even more challenging to solve optimally. To address these challenges, we first decompose the original problem into two subproblems based on the unique structure of the underlying problem and leverage the alternating optimization (AO) approach to solve them in an iterative fashion by developing newly convex approximate functions. To speed up optimal user association searching, we incorporate a penalty function into the objective function to resolve uncertainties of a binary nature. Two sub-optimal designs for given user association policies based on channel conditions and random user associations are also investigated to serve as state-of-the-art benchmarks. Numerical results are provided to demonstrate the effectiveness of the proposed algorithms in terms of the e2e latency and convergence speed.}
}


@article{DBLP:journals/jsac/LiuYLFSYGVL23,
	author = {Shumei Liu and
                  Yao Yu and
                  Xiao Lian and
                  Yuze Feng and
                  Changyang She and
                  Phee Lep Yeoh and
                  Lei Guo and
                  Branka Vucetic and
                  Yonghui Li},
	title = {Dependent Task Scheduling and Offloading for Minimizing Deadline Violation
                  Ratio in Mobile Edge Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {538--554},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3233532},
	doi = {10.1109/JSAC.2022.3233532},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuYLFSYGVL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers computation offloading for mobile applications with task-dependency requirements in mobile edge computing (MEC) systems. Based on the online arrival patterns and various delay constraints of practical applications, we focus on minimizing the system deadline violation ratio (DVR) to improve the overall reliability performance. Specifically, we propose a DVR minimization computation offloading scheme with task migration and merging, in which the task migration and merging model is designed to construct an overall directed acyclic graph (DAG) for all currently dependent tasks. We consider a multi-slot MEC system where applications arrive slot-by-slot without prior knowledge of future arrivals. Then given the number of application arrivals at each time slot, we equivalently transform the DVR minimization problem into a problem that maximizes the number of completed applications in a finite time horizon. The above problem is challenging to determine the optimal task execution order for different applications with various task dependencies and delay constraints. To address this, we develop a migration-enabled multi-priority task sequencing algorithm, which creatively introduces several task priority metrics and determines the optimal task execution order. Then, a deep deterministic policy gradient (DDPG)-based learning algorithm is developed to find the optimal offloading policy. Experimental results demonstrate that the proposed scheme can reduce the system DVR by 60.34%~70.3% compared with existing benchmark schemes under various network scenarios.}
}


@article{DBLP:journals/jsac/ZhangCMZLGG23,
	author = {Rui Zhang and
                  Xuesen Chu and
                  Ruhui Ma and
                  Meng Zhang and
                  Liwei Lin and
                  Honghao Gao and
                  Haibing Guan},
	title = {{OSTTD:} Offloading of Splittable Tasks With Topological Dependence
                  in Multi-Tier Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {2},
	pages = {555--568},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3227023},
	doi = {10.1109/JSAC.2022.3227023},
	timestamp = {Tue, 31 Jan 2023 20:44:59 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangCMZLGG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the arrival of the Internet of Things (IoT) era, multi-tier computing has attracted significant attention. The multi-tier computing can organize many computing devices and provide sufficient computing resources to support various IoT applications. However, due to the complex architecture and the dynamic system status of the multi-tier computing network, task offloading for multi-tier computing is still challenging. This paper proposes a novel task offloading method named OSTTD, to deal with the Offloading of Splittable Tasks with Topological Dependence in multi-tier computing networks. OSTTD formulates the task offloading as a sequential decision-making problem and learns the task offloading policy by Deep Reinforcement Learning (DRL). Compared with existing task offloading approaches, OSTTD is the first method in which the topological dependence among sub-tasks of the splittable task is fully considered. OSTTD makes offloading decisions intelligently based on the dynamic system status and can be applied to various multi-tier network topology structures. To verify the effectiveness of OSTTD, we extend and build a latency-aware multi-tier computing simulation platform. Extensive simulations show that OSTTD can significantly reduce the task processing time, thus, improving the overall task processing efficiency in multi-tier computing networks.}
}


@article{DBLP:journals/jsac/WangYJZNTJ23a,
	author = {Kunlun Wang and
                  Yang Yang and
                  Jiong Jin and
                  Tao Zhang and
                  Arumugam Nallanathan and
                  Chintha Tellambura and
                  Bijan Jabbari},
	title = {Guest Editorial Multi-Tier Computing for Next Generation Wireless
                  Networks - Part {II}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {569--573},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3228094},
	doi = {10.1109/JSAC.2022.3228094},
	timestamp = {Sun, 12 Nov 2023 02:18:02 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangYJZNTJ23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-tier computing effectively enables flexible computation and communication resource sharing by offloading computation-intensive tasks to nearby servers along the cloud-to-thing continuum. In essence, multi-tier computing networks can distribute computing, storage, and communication functions anywhere between the cloud and the endpoint to take full advantage of the resources available along this continuum, thus extending the traditional cloud computing architecture to the edge of the network. With multi-tier computing, some application component processing, such as delay-sensitive components, can take place at the edge of the network, while other components, such as time-tolerant and computation-intensive components, can be performed in the cloud. To best meet user requirements, centralized cloud computing with extensive resources, secure environments, and powerful algorithms is still needed, but also must be complemented by distributed fog and edge computing with shared resources, accessible environments, and simple algorithms for real-time decision-making. Given heterogeneous computing resources and collaborative service architectures, future multi-tier computing networks will be capable of supporting a full range of computing and networking services for different environments and applications. Multi-tier computing enables low-latency processing by allowing data to be processed at the network edge close to end devices. It also facilitates the distribution of fog/edge nodes to collect data from end devices. Therefore, multi-tier computing effectively complements the cloud computing architecture.}
}


@article{DBLP:journals/jsac/WangMLXZ23,
	author = {Zhaolin Wang and
                  Xidong Mu and
                  Yuanwei Liu and
                  Xiaodong Xu and
                  Ping Zhang},
	title = {NOMA-Aided Joint Communication, Sensing, and Multi-Tier Computing
                  Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {574--588},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229447},
	doi = {10.1109/JSAC.2022.3229447},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangMLXZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A non-orthogonal multiple access (NOMA)-aided joint communication, sensing, and multi-tier computing (JCSMC) framework is proposed. In this framework, a multi-functional base station (BS) simultaneously carries out target sensing and provide edge computing services to the nearby users. To enhance the computation efficiency, the multi-tier computing structure is exploited, where the BS can further offload the computation tasks to a powerful Cloud server (CS). The potential benefits of employing NOMA in the proposed JCSMC framework are investigated, which can maximize the computation offloading capacity and suppress inter-functionality interference. Based on the proposed framework, the transmit beamformer of the BS and computing resource allocation among the BS and CS are jointly optimized to maximize the computation rate subject to the communication-computation causality and the sensing quality constraints. Both partial and binary computation offloading modes are considered: 1) For the partial offloading mode, a weighted minimum mean square error based alternating optimization algorithm is proposed to solve the corresponding non-convex optimization problem. It is proved that a Karush–Kuhn–Tucker optimal solution can be obtained; 2) For the binary offloading mode, the resultant highly-coupled mixed-integer optimization problem is first transformed to an equivalent but more tractable form. Then, the reformulated problem is solved by utilizing the alternating direction method of multipliers approach to obtain a nearly optimal solution. Finally, numerical results verify the effectiveness of the proposed algorithms and reveal that: i) the computation rate can be significantly enhanced by exploiting the multi-tier computing architecture when the BS is resource-limited, and ii) the proposed NOMA-aided JSCMC framework is superior in inter-functionality interference management and can achieve high-quality sensing and computing performance simultaneously compared with other benchmark schemes.}
}


@article{DBLP:journals/jsac/ShaoGL23,
	author = {Yulin Shao and
                  Deniz G{\"{u}}nd{\"{u}}z and
                  Soung Chang Liew},
	title = {Bayesian Over-the-Air Computation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {589--606},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229428},
	doi = {10.1109/JSAC.2022.3229428},
	timestamp = {Sat, 11 Mar 2023 00:13:48 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ShaoGL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an important piece of the multi-tier computing architecture for future wireless networks, over-the-air computation (OAC) enables efficient function computation in multiple-access edge computing, where a fusion center aims to compute a function of the data distributed at edge devices. Existing OAC relies exclusively on the maximum likelihood (ML) estimation at the fusion center to recover the arithmetic sum of the transmitted signals from different devices. ML estimation, however, is much susceptible to noise. In particular, in the misaligned OAC where there are channel misalignments among received signals, ML estimation suffers from severe error propagation and noise enhancement. To address these challenges, this paper puts forth a Bayesian approach by letting each edge device transmit two pieces of statistical information to the fusion center such that Bayesian estimators can be devised to tackle the misalignments. Numerical and simulation results verify that, 1) For the aligned and synchronous OAC, our linear minimum mean squared error (LMMSE) estimator significantly outperforms the ML estimator. In the low signal-to-noise ratio (SNR) regime, the LMMSE estimator reduces the mean squared error (MSE) by at least 6 dB; in the high SNR regime, the LMMSE estimator lowers the error floor of MSE by 86.4%; 2) For the asynchronous OAC, our LMMSE and sum-product maximum a posteriori (SP-MAP) estimators are on an equal footing in terms of the MSE performance, and are significantly better than the ML estimator. Moreover, the SP-MAP estimator is computationally efficient, the complexity of which grows linearly with the packet length.}
}


@article{DBLP:journals/jsac/ZhaoDK23,
	author = {Zixiao Zhao and
                  Qinghe Du and
                  George K. Karagiannidis},
	title = {Improved Grant-Free Access for {URLLC} via Multi-Tier-Driven Computing:
                  Network-Load Learning, Prediction, and Resource Allocation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {607--622},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3234696},
	doi = {10.1109/JSAC.2023.3234696},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoDK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Grant-Free (GF) access has been recognized as a promising candidate for Ultra-Reliable and Low-Latency Communications (URLLC). However, even with GF access, URLLC still may not effectively gain high reliability and millimeter-level latency, simultaneously. This is because the network load is typically time-varying and not known to the base station (BS), and thus, the resource allocated for GF access cannot well adapt to variations of the network load, resulting in low resource utilization efficiency under light network load and leading to severe collisions under heavy network load. To tackle this problem, we propose a multi-tier-driven computing framework and the associated algorithms for URLLC to support users with different QoS requirements. Especially, we concentrate on\nK\n- repetition GF access in light of its simplicity and well-balanced performance for practical systems. In particular, our framework consists of three tiers of computation, namely network-load learning, network-load prediction, and adaptive resource allocation. In the first tier, the BS can learn the network-load information from the states (success, collision, and idle) of random-access resources in terms of resource blocks (RB) and time slots. In the second tier, the network-load variation is effectively predicted based on estimation results from the first tier. Finally, in the third tier, by deriving and weighing the failure probabilities of different groups of users, their QoS requirements, and the predicted network loads, the BS is able to dynamically allocate sufficient resources accommodating the varying network loads. Simulation results show that our proposed approach can estimate the network load more accurately compared with the baseline schemes. Moreover, with the assistance of network-load prediction, our adaptive resource allocation offers an effective way to enhance the QoS for different URLLC services, simultaneously.}
}


@article{DBLP:journals/jsac/QinLSMHZ23,
	author = {Xiaoqi Qin and
                  Yanlin Li and
                  Xianxin Song and
                  Nan Ma and
                  Chuan Huang and
                  Ping Zhang},
	title = {Timeliness of Information for Computation-Intensive Status Updates
                  in Task-Oriented Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {623--638},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229431},
	doi = {10.1109/JSAC.2022.3229431},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QinLSMHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Moving beyond just interconnected devices, the increasing interplay between communication and computation has fed the vision of real-time networked control systems. To obtain timely situational awareness, IoT devices continuously sample computation-intensive status updates, generate perception tasks and offload them to edge servers for processing. In this sense, the timeliness of information is considered as one major contextual attribute of status updates. In this paper, we derive the closed-form expressions of timeliness of information for computation offloading at both edge tier and fog tier, where two-stage tandem queues are exploited to abstract the transmission and computation process. Moreover, we exploit the statistical structure of Gauss-Markov process, which is widely adopted to model temporal dynamics of system states, and derive the closed-form expression for process-related timeliness of information. The obtained analytical formulas explicitly characterize the dependency among task generation, transmission and execution, which can serve as objective functions for system optimization. Based on the theoretical results, we formulate a computation offloading optimization problem at edge tier, where the timeliness of status updates is minimized among multiple devices by joint optimization of task generation, bandwidth allocation, and computation resource allocation. An iterative solution procedure is proposed to solve the formulated problem. Numerical results reveal the intertwined relationship among transmission and computation stages, and verify the necessity of factoring in the task generation process for computation offloading strategy design.}
}


@article{DBLP:journals/jsac/XiaoSSN23,
	author = {Yong Xiao and
                  Zijian Sun and
                  Guangming Shi and
                  Dusit Niyato},
	title = {Imitation Learning-Based Implicit Semantic-Aware Communication Networks:
                  Multi-Layer Representation and Collaborative Reasoning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {639--658},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229419},
	doi = {10.1109/JSAC.2022.3229419},
	timestamp = {Mon, 28 Aug 2023 21:38:11 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XiaoSSN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communication has recently attracted significant interest from both industry and academia due to its potential to transform the existing data-focused communication architecture towards a more generally intelligent and goal-oriented semantic-aware networking system. Despite its promising potential, semantic communications and semantic-aware networking are still in their infancy. Most existing works focus on transporting and delivering the explicit semantic information, e.g., labels or features of objects, that can be directly identified from the source signal. The original definition of semantics as well as recent results in cognitive neuroscience suggest that it is the implicit semantic information, in particular the hidden relations connecting different concepts and feature items that play the fundamental role in recognizing, communicating, and delivering the real semantic meanings of messages. Motivated by this observation, we propose a novel reasoning-based implicit semantic-aware communication network architecture that allows destination users to directly learn a reasoning mechanism that can automatically generate complex implicit semantic information based on a limited clue information sent by the source users. Our proposed architecture can be implemented in a multi-tier cloud/edge computing networks in which multiple tiers of cloud data center (CDC) and edge servers can collaborate and support efficient semantic encoding, decoding, and implicit semantic interpretation for multiple end-users. We introduce a new multi-layer representation of semantic information taking into consideration both the hierarchical structure of implicit semantics as well as the personalized inference preference of individual users. We model the semantic reasoning process as a reinforcement learning process and then propose an imitation-based semantic reasoning mechanism learning (iRML) solution to learning a reasoning policy that imitates the inference behavior of the source user. A federated graph convolutional network (GCN)-based collaborative reasoning solution is proposed to allow multiple edge servers to jointly construct a shared semantic interpretation model based on decentralized semantic message samples. Extensive experiments have been conducted based on real-world datasets to evaluate the performance of our proposed architecture. Numerical results confirm that iRML offers up to 25.8 dB improvement on the semantic symbol error rate, compared to the semantic-irrelevant communication solutions.}
}


@article{DBLP:journals/jsac/LiLHJHL23,
	author = {Shaoran Li and
                  Chengzhang Li and
                  Yan Huang and
                  Brian A. Jalaian and
                  Y. Thomas Hou and
                  Wenjing Lou},
	title = {Enhancing Resilience in Mobile Edge Computing Under Processing Uncertainty},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {659--674},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3234706},
	doi = {10.1109/JSAC.2023.3234706},
	timestamp = {Thu, 11 May 2023 21:27:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiLHJHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task offloading is a powerful tool in Mobile Edge Computing (MEC). However, in many practical scenarios, the number of required processing cycles of a task is unknown beforehand and only known until its completion. This poses a serious challenge in making offloading decisions as the number of processing cycles is a key parameter to determine whether a task’s deadline can be met. To cope with such processing uncertainty, we formulate a Chance-Constrained Program (CCP) that offers probabilistic guarantees to task deadlines. The goal is to minimize energy consumption for the users while meeting the probabilistic task deadlines. We assume that only the means and variances of the random processing cycles are available, without any knowledge of distribution functions. We employ a powerful tool called Exact Conic Reformulation (ECR) that reformulates probabilistic deadline constraints into deterministic ones. Subsequently, we design an online solution called EPD (Energy-minimized solution with Probabilistic Deadline guarantee) for periodic scheduling and schedule updates during run-time. We show that EPD can address the processing uncertainty with probabilistic deadline guarantees while minimizing the users’ energy consumption.}
}


@article{DBLP:journals/jsac/KimSCIZHJ23,
	author = {Taejin Kim and
                  Sandesh Dhawaskar Sathyanarayana and
                  Siqi Chen and
                  Youngbin Im and
                  Xiaoxi Zhang and
                  Sangtae Ha and
                  Carlee Joe{-}Wong},
	title = {MoDEMS: Optimizing Edge Computing Migrations for User Mobility},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {675--689},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229425},
	doi = {10.1109/JSAC.2022.3229425},
	timestamp = {Tue, 07 May 2024 20:20:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KimSCIZHJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing capabilities in 5G wireless networks promise to benefit mobile users: computing tasks can be offloaded from user devices to nearby edge servers, reducing users’ experienced latencies. Few works have addressed how this offloading should handle long-term user mobility: as devices move, they will need to offload to different edge servers, which may require migrating data or state information from one edge server to another. In this paper, we introduce MoDEMS, a system model and architecture that provides a rigorous theoretical framework and studies the challenges of such migrations to minimize the service provider cost and user latency. We show that this cost minimization problem can be expressed as an integer linear programming problem, which is hard to solve due to resource constraints at the servers and unknown user mobility patterns. We show that finding the optimal migration plan is in general NP-hard, and we propose alternative heuristic solution algorithms that perform well in both theory and practice. We finally validate our results with real user mobility traces, ns-3 simulations, and an LTE testbed experiment. Migrations reduce the latency experienced by users of edge applications by 33% compared to previously proposed migration approaches.}
}


@article{DBLP:journals/jsac/SunLWWHL23,
	author = {Chuan Sun and
                  Xiuhua Li and
                  Junhao Wen and
                  Xiaofei Wang and
                  Zhu Han and
                  Victor C. M. Leung},
	title = {Federated Deep Reinforcement Learning for Recommendation-Enabled Edge
                  Caching in Mobile Edge-Cloud Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {690--705},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3235443},
	doi = {10.1109/JSAC.2023.3235443},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunLWWHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To support rapidly increasing services and applications from users, multi-tier computing is emerged as a promising system-level computing architecture by distributing computing/caching/communication/networking capabilities between cloud servers to users, especially deploying edge servers at network edges (e.g., base stations). However, due to heterogeneous content requests of users and a high-cost hit manner with direct hits, edge caching is still a most serious issue to be addressed. In this paper, we investigate the issue of recommendation-enabled edge caching in mobile two-tier (edge-cloud) computing networks. Particularly, we integrate recommender systems and edge caching to support both direct hits and soft hits and thus improve the resource utilization of edge servers. We model the factors affecting the user quality of experience as a comprehensive system cost and further formulate the problem as a multi-agent Markov decision process with the goal of minimizing the long-term average system cost. To address the formulated problem, we propose a decentralized recommendation-enabled edge caching framework that leverages a discrete multi-agent variant of soft actor-critic and federated learning. The proposed framework enables each edge server to learn its best policy locally and generate judicious decisions independently. Finally, trace-driven simulation results demonstrate that the proposed framework converges to a better caching policy and outperforms several existing algorithms on average system cost reduction.}
}


@article{DBLP:journals/jsac/TangHCFLH23,
	author = {Shunpu Tang and
                  Ke He and
                  Lunyuan Chen and
                  Lisheng Fan and
                  Xianfu Lei and
                  Rose Qingyang Hu},
	title = {Collaborative Cache-Aided Relaying Networks: Performance Evaluation
                  and System Optimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {706--719},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3234693},
	doi = {10.1109/JSAC.2023.3234693},
	timestamp = {Mon, 28 Aug 2023 21:38:10 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TangHCFLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies a multi-tier cache-aided relaying network, where the destination\nD\nis randomly located in the network and it requests files from the source\nS\nthrough the help of cache-aided base station (BS) and\nN\nrelays. In this system, the multi-tier architecture imposes a significant impact on the system collaborative caching and file delivery, which brings a big challenge to the system performance evaluation and optimization. To address this problem, we first evaluate the system performance by deriving analytical outage probability expression, through fully taking into account the random location of the destination and different file delivery modes related to the file caching status. We then perform the asymptotic analysis on the system outage probability when the signal-to-noise ratio (SNR) is high, to enclose some important and meaningful insights on the network. We further optimize the caching strategies among the relays and BS, to improve the network outage probability. Simulations are performed to show the effectiveness of the derived analytical and asymptotic outage probability for the proposed caching strategy. In particular, the proposed caching is superior to the conventional caching strategies such as the most popular content (MPC) and equal probability caching (EPC) strategies.}
}


@article{DBLP:journals/jsac/ZengYHZYC23,
	author = {Liekang Zeng and
                  Chongyu Yang and
                  Peng Huang and
                  Zhi Zhou and
                  Shuai Yu and
                  Xu Chen},
	title = {{GNN} at the Edge: Cost-Efficient Graph Neural Network Processing
                  Over Distributed Edge Servers},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {720--739},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229422},
	doi = {10.1109/JSAC.2022.3229422},
	timestamp = {Mon, 26 Feb 2024 10:36:56 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZengYHZYC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge intelligence has arisen as a promising computing paradigm for supporting miscellaneous smart applications that rely on machine learning techniques. While the community has extensively investigated multi-tier edge deployment for traditional deep learning models (e.g. CNNs, RNNs), the emerging Graph Neural Networks (GNNs) are still under exploration, presenting a stark disparity to its broad edge adoptions such as traffic flow forecasting and location-based social recommendation. To bridge this gap, this paper formally studies the cost optimization for distributed GNN processing over a multi-tier heterogeneous edge network. We build a comprehensive modeling framework that can capture a variety of different cost factors, based on which we formulate a cost-efficient graph layout optimization problem that is proved to be NP-hard. Instead of trivially applying traditional data placement wisdom, we theoretically reveal the structural property of quadratic submodularity implicated in GNN’s unique computing pattern, which motivates our design of an efficient iterative solution exploiting graph cuts. Rigorous analysis shows that it provides parameterized constant approximation ratio, guaranteed convergence, and exact feasibility. To tackle potential graph topological evolution in GNN processing, we further devise an incremental update strategy and an adaptive scheduling algorithm for lightweight dynamic layout optimization. Evaluations with real-world datasets and various GNN benchmarks demonstrate that our approach achieves superior performance over de facto baselines with more than 95.8% cost reduction in a fast convergence speed.}
}


@article{DBLP:journals/jsac/LiuDML23,
	author = {Xingchi Liu and
                  Mahsa Derakhshani and
                  Lyudmila Mihaylova and
                  Sangarapillai Lambotharan},
	title = {Risk-Aware Contextual Learning for Edge-Assisted Crowdsourced Live
                  Streaming},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {740--754},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229423},
	doi = {10.1109/JSAC.2022.3229423},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuDML23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes an edge-assisted crowdsourced live video transcoding approach where the transcoding capabilities of the edge transcoders are unknown and dynamic. The resilience and trustworthiness of highly unstable transcoders in decision making are characterized with mean-variance-based measures to avoid making highly risky decisions. The risk level of each device’s situation is assessed and two upper confidence bounds of the variance of transcoding performance are presented. Based on the derived bounds and by leveraging the contextual information of devices, two risk-aware contextual learning schemes are developed to efficiently estimate the transcoding capabilities of the edge devices. Combining context awareness and risk sensitivity, a novel transcoding task assignment and viewer association algorithm is proposed. Simulation results demonstrate that the proposed algorithm achieves robust task offloading with superior network utility performance as compared to the linear upper confidence bound and the risk-aware mean-variance upper confidence bound-based algorithms. In particular, an epoch-based task assignment strategy is designed to reduce the task switching costs incurred in assigning the same transcoding task to different transcoders over time. This strategy also reduces the computational time needed. Numerical results confirm that this strategy achieves up to 86.8% switching costs reduction and 92.3% computational time reduction.}
}


@article{DBLP:journals/jsac/DengLMWSDC23,
	author = {Xiumei Deng and
                  Jun Li and
                  Chuan Ma and
                  Kang Wei and
                  Long Shi and
                  Ming Ding and
                  Wen Chen},
	title = {Low-Latency Federated Learning With {DNN} Partition in Distributed
                  Industrial IoT Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {755--775},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229436},
	doi = {10.1109/JSAC.2022.3229436},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DengLMWSDC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) empowers Industrial Internet of Things (IIoT) with distributed intelligence of industrial automation thanks to its capability of distributed machine learning without any raw data exchange. However, it is rather challenging for lightweight IIoT devices to perform computation-intensive local model training over large-scale deep neural networks (DNNs). Driven by this issue, we develop a communication-computation efficient FL framework for resource-limited IIoT networks that integrates DNN partition technique into the standard FL mechanism, wherein IIoT devices perform local model training over the bottom layers of the objective DNN, and offload the top layers to the edge gateway side. Considering imbalanced data distribution, we derive the device-specific participation rate to involve the devices with better data distribution in more communication rounds. Upon deriving the device-specific participation rate, we propose to minimize the training delay under the constraints of device-specific participation rate, energy consumption and memory usage. To this end, we formulate a joint optimization problem of device scheduling and resource allocation (i.e. DNN partition point, channel assignment, transmit power, and computation frequency), and solve the long-term min-max mixed integer non-linear programming based on the Lyapunov technique. In particular, the proposed dynamic device scheduling and resource allocation (DDSRA) algorithm can achieve a trade-off to balance the training delay minimization and FL performance. We also provide the FL convergence bound for the DDSRA algorithm with both convex and non-convex settings. Experimental results demonstrate the derived device-specific participation rate in terms of feasibility, and show that the DDSRA algorithm outperforms baselines in terms of test accuracy and convergence time.}
}


@article{DBLP:journals/jsac/QinXSDZS23,
	author = {Fei Qin and
                  Yucong Xiao and
                  Xian Sun and
                  Xuewu Dai and
                  Wuxiong Zhang and
                  Fei Shen},
	title = {Inverse-GMM: {A} Latency Distribution Shaping Method for Industrial
                  Cooperative Deep Learning Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {776--788},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229448},
	doi = {10.1109/JSAC.2022.3229448},
	timestamp = {Sat, 11 Mar 2023 00:13:48 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QinXSDZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The front deployed deep learning is a promising technology of the next generation industrial applications, which can extract essential information from high dimension sensors. However, part of these heavy computation tasks at resource constrained front devices have to be offloaded to the edge or cloud devices, which forms the cooperative deep learning system through the exchange of intermediate data. The inference efficiency of cooperative deep learning system will then be highly correlated with the communication latency caused by the non-stationary industrial multipath-rich fading channel. This paper proposes a novel method to control the distribution of communications latency, which is able to support efficient cooperative deep learning architecture in the harsh industrial environment. The proposed method is essentially an inverse process of Gaussian Mixture Model (GMM), which adjusts latency samples to approach the given arbitrary shape function. To achieve this objective, a new variation of Expectation-Maximization (EM) algorithm in analytical domain is derived to decompose arbitrary distribution shape with multiple Gaussian kernels and an optimized stochastic resource allocation algorithm is proposed to approximate each Gaussian kernels. The performance of proposed method is verified by both classical Rician channel model and field measured industrial fading channel responses.}
}


@article{DBLP:journals/jsac/XuQYMZZ23,
	author = {Yunting Xu and
                  Bo Qian and
                  Kai Yu and
                  Ting Ma and
                  Lian Zhao and
                  Haibo Zhou},
	title = {Federated Learning Over Fully-Decoupled {RAN} Architecture for Two-Tier
                  Computing Acceleration},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {789--801},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3236003},
	doi = {10.1109/JSAC.2023.3236003},
	timestamp = {Wed, 22 Mar 2023 21:18:25 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XuQYMZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Two-tier computing paradigm that takes full advantage of both the end-user and the cloud computation capabilities has emerged as a promising way to deal with computationally-intensive tasks in the next generation wireless networks. For promoting the integration of the two-tier computing, federated learning (FL) provides an effective framework to enable the collaboration between the end-user and the cloud. However, the key performance metric, i.e., FL training latency, will be severely affected by the worst wireless link quality in both uplink and downlink. In this paper, aiming at accelerating the FL enabled end-cloud two-tier computing over the wireless networks, we introduce the uplink and downlink fully-decoupled radio access network (FD-RAN) architecture to enhance the minimum wireless link rate via multiple base stations (BSs) access collaboration and power management solution. First, the Lagrange dual decomposition and the binary variable relaxation methods are leveraged to obtain an optimal multiple BS access scheme for the enhancement of minimum uplink and downlink SINR. Subsequently, we exploit the successive convex approximation (SCA) algorithm to deal with the uplink power control and downlink power allocation with a proved data rate lower bound. Furthermore, considering the dynamic channel realizations, a stochastic optimization technique with a convex surrogate function is utilized to find the best end-cloud two-tier computing scheme for FL applications. Simulation results have demonstrated the effectiveness of our proposed joint multiple access collaboration and power management solution over FD-RAN for achieving a faster FL enabled two-tier computing task.}
}


@article{DBLP:journals/jsac/YangCS23,
	author = {Kun Yang and
                  Shengbo Chen and
                  Cong Shen},
	title = {On the Convergence of Hybrid Server-Clients Collaborative Training},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {802--819},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229443},
	doi = {10.1109/JSAC.2022.3229443},
	timestamp = {Tue, 23 Apr 2024 21:46:00 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangCS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern distributed machine learning (ML) paradigms, such as federated learning (FL), utilize data distributed at different clients to train a global model. In such paradigm, local datasets never leave the clients for better privacy protection, and the parameter server (PS) only performs simple aggregation. In practice, however, there is often some amount of data available at the PS, and its computation capability is strong enough to carry out more demanding tasks than simple model aggregation. The focus of this paper is to analyze the model convergence of a new hybrid learning architecture, which leverages the PS dataset and its computation power for collaborative model training with clients. Different from FL where stochastic gradient descent (SGD) is always computed in parallel across clients, the new architecture has both parallel SGD at clients and sequential SGD at PS. We analyze the convergence rate upper bounds of this aggregate-then-advance design for both strongly convex and non-convex loss functions. We show that when the local SGD has an \\mathcal {O}(1/t)\nstepsize, the server SGD needs to scale its stepsize to no slower than \\mathcal {O}(1/t^{2})\nin order to strictly outperform local SGD with strongly convex loss functions. The theoretical findings are corroborated by numerical experiments, where advantages in terms of both accuracy and convergence speed over clients-only (local SGD and FED AVG) and server-only training are demonstrated.}
}


@article{DBLP:journals/jsac/ShenRJWWL23,
	author = {Shihao Shen and
                  Yuanming Ren and
                  Yanli Ju and
                  Xiaofei Wang and
                  Wenyu Wang and
                  Victor C. M. Leung},
	title = {EdgeMatrix: {A} Resource-Redefined Scheduling Framework for SLA-Guaranteed
                  Multi-Tier Edge-Cloud Computing Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {820--834},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229444},
	doi = {10.1109/JSAC.2022.3229444},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ShenRJWWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of networking technology, the computing system has evolved towards the multi-tier paradigm gradually. However, challenges, such as multi-resource heterogeneity of devices, resource competition of services, and networked system dynamics, make it difficult to guarantee service-level agreement (SLA) for the applications. In this paper, we propose a multi-tier edge-cloud computing framework, EdgeMatrix, to maximize the throughput of the system while guaranteeing different SLA priorities. First, in order to reduce the impact of physical resource heterogeneity, EdgeMatrix introduces the Networked Multi-agent Actor-Critic (NMAC) algorithm to re-define physical resources with the same quality of service as logically isolated resource units and combinations, i.e., cells and channels. In addition, a multi-task mechanism is designed in EdgeMatrix to solve the problem of Joint Service Orchestration and Request Dispatch (JSORD) for matching the requests and services, which can significantly reduce the optimization runtime. For integrating above two algorithms, EdgeMatrix is designed with two time-scales, i.e., coordinating services and resources at the larger time-scale, and dispatching requests at the smaller time-scale. Realistic trace-based experiments proves that the overall throughput of EdgeMatrix is 36.7% better than that of the closest baseline, while the SLA priorities are guaranteed still.}
}


@article{DBLP:journals/jsac/WangGLZ23,
	author = {Shangguang Wang and
                  Yan Guo and
                  Xiao Liu and
                  Ao Zhou},
	title = {Service Routing in Multi-Tier Edge Computing: {A} Matching Game Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {3},
	pages = {835--844},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2022.3229378},
	doi = {10.1109/JSAC.2022.3229378},
	timestamp = {Sat, 25 Feb 2023 21:35:13 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangGLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Although microservice envisioned as a promising approach for edge applications which can improves the development efficiency and the deployment productivity, it also leads to the operational complexity during service runtime. This leads to the emergence of service mesh, a decided infrastructure layer over microservices for reliable service-to-service communication. However, when migrating the service mesh to the multi-tier edge computing, the corresponding extensions on the top of service mesh is needed to overcome the challenges brought by the multi-tier edge servers, shared microservices and diverse QoS requirements. Therefore, in this paper, we investigate the problem of service routing to achieve the efficient microservice-based service provision in multi-tier edge computing. The objective is to route user requests to the optimal microservice instances with low service delay and resource cost. To this end, we first formulate the service routing problem as an integer non-linear optimization problem, which is NP-hard. Then we map it to a many-to-one matching problem by employing the matching game theory and propose a dependency-aware deferred acceptance algorithm with dynamic quota. The experimental results based on a real-world dataset demonstrate that our proposed algorithm can significantly outperform existing representative algorithms in terms of service delay and resource cost.}
}


@article{DBLP:journals/jsac/CaoBDELPZ23,
	author = {Xuanyu Cao and
                  Tamer Basar and
                  Suhas N. Diggavi and
                  Yonina C. Eldar and
                  Khaled B. Letaief and
                  H. Vincent Poor and
                  Junshan Zhang},
	title = {Guest Editorial Communication-Efficient Distributed Learning Over
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {845--850},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3241848},
	doi = {10.1109/JSAC.2023.3241848},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaoBDELPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed machine learning is envisioned as the bedrock of future intelligent networks, where agents exchange information with each other to train models collaboratively without uploading data to a central processor. Despite its broad applicability, a downside of distributed learning is the need for iterative information exchange between agents, which may lead to high communication overhead unaffordable in many practical systems with limited communication resources. To resolve this communication bottleneck, we need to devise communication-efficient distributed learning algorithms and protocols that can reduce the communication cost and simultaneously achieve satisfactory learning/optimization performance. Accomplishing this goal necessitates synergistic techniques from a diverse set of fields, including optimization, machine learning, wireless communications, game theory, and network/graph theory. This Special Issue is dedicated to communication-efficient distributed learning from multiple perspectives, including fundamental theories, algorithm design and analysis, and practical considerations.}
}


@article{DBLP:journals/jsac/CaoBDELPZ23a,
	author = {Xuanyu Cao and
                  Tamer Basar and
                  Suhas N. Diggavi and
                  Yonina C. Eldar and
                  Khaled B. Letaief and
                  H. Vincent Poor and
                  Junshan Zhang},
	title = {Communication-Efficient Distributed Learning: An Overview},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {851--873},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242710},
	doi = {10.1109/JSAC.2023.3242710},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaoBDELPZ23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed learning is envisioned as the bedrock of next-generation intelligent networks, where intelligent agents, such as mobile devices, robots, and sensors, exchange information with each other or a parameter server to train machine learning models collaboratively without uploading raw data to a central entity for centralized processing. By utilizing the computation/communication capability of individual agents, the distributed learning paradigm can mitigate the burden at central processors and help preserve data privacy of users. Despite its promising applications, a downside of distributed learning is its need for iterative information exchange over wireless channels, which may lead to high communication overhead unaffordable in many practical systems with limited radio resources such as energy and bandwidth. To overcome this communication bottleneck, there is an urgent need for the development of communication-efficient distributed learning algorithms capable of reducing the communication cost and achieving satisfactory learning/optimization performance simultaneously. In this paper, we present a comprehensive survey of prevailing methodologies for communication-efficient distributed learning, including reduction of the number of communications, compression and quantization of the exchanged information, radio resource management for efficient learning, and game-theoretic mechanisms incentivizing user participation. We also point out potential directions for future research to further enhance the communication efficiency of distributed learning in various scenarios.}
}


@article{DBLP:journals/jsac/HuCL23,
	author = {Chung{-}Hsuan Hu and
                  Zheng Chen and
                  Erik G. Larsson},
	title = {Scheduling and Aggregation Design for Asynchronous Federated Learning
                  Over Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {874--886},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242719},
	doi = {10.1109/JSAC.2023.3242719},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) is a collaborative machine learning (ML) framework that combines on-device training and server-based aggregation to train a common ML model among distributed agents. In this work, we propose an asynchronous FL design with periodic aggregation to tackle the straggler issue in FL systems. Considering limited wireless communication resources, we investigate the effect of different scheduling policies and aggregation designs on the convergence performance. Driven by the importance of reducing the bias and variance of the aggregated model updates, we propose a scheduling policy that jointly considers the channel quality and training data representation of user devices. The effectiveness of our channel-aware data-importance-based scheduling policy, compared with state-of-the-art methods proposed for synchronous FL, is validated through simulations. Moreover, we show that an “age-aware” aggregation weighting design can significantly improve the learning performance in an asynchronous FL setting.}
}


@article{DBLP:journals/jsac/ChiuZHWS23,
	author = {Cho{-}Chun Chiu and
                  Xusheng Zhang and
                  Ting He and
                  Shiqiang Wang and
                  Ananthram Swami},
	title = {Laplacian Matrix Sampling for Communication- Efficient Decentralized
                  Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {887--901},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242735},
	doi = {10.1109/JSAC.2023.3242735},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChiuZHWS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of training a given machine learning model by decentralized parallel stochastic gradient descent over training data distributed across multiple nodes, which arises in many application scenarios. Although extensive studies have been conducted on improving the communication efficiency by optimizing what to communicate between nodes (e.g., model compression) and how often to communicate, recent studies have shown that it is also important to customize the communication patterns between each pair of nodes, which is the focus of this work. To this end, we propose a framework and efficient algorithms to design the communication patterns through Laplacian matrix sampling (LMS), which governs not only which nodes should communicate with each other but also what weights the communicated parameters should carry during parameter aggregation. Our framework is designed to minimize the total cost incurred until convergence based on any given cost model that is additive over iterations, with focus on minimizing the communication cost. Besides achieving a theoretically guaranteed performance in the special case of additive homogeneous communication costs, our solution also achieves superior performance under a variety of network settings and cost models in experiments based on real datasets and topologies, saving 24–50% of the cost compared to the state-of-the-art design without compromising the quality of the trained model.}
}


@article{DBLP:journals/jsac/ChenXWLLCZ23,
	author = {Chen Chen and
                  Hong Xu and
                  Wei Wang and
                  Baochun Li and
                  Bo Li and
                  Li Chen and
                  Gong Zhang},
	title = {{GIFT:} Toward Accurate and Efficient Federated Learning With Gradient-Instructed
                  Frequency Tuning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {902--914},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242721},
	doi = {10.1109/JSAC.2023.3242721},
	timestamp = {Fri, 24 Nov 2023 13:28:38 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenXWLLCZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables distributed clients to collectively train a global model without revealing their private data, and for efficiency clients synchronize their gradients periodically. However, this can lead to the inaccuracy in model convergence due to inconsistent data distributions among clients. In this work, we find that there is a strong correlation between FL accuracy loss and the synchronization frequency, and seek to fine tune the synchronization frequency at training runtime to make FL accurate and also efficient. Specifically, aware that under the FL privacy requirement only gradients can be utilized for making frequency tuning decisions, we propose a novel metric called gradient consistency, which can effectively reflect the training status despite the instability of realistic FL scenarios. We further devise a feedback-driven algorithm called Gradient-Instructed Frequency Tuning (GIFT), which adaptively increases or decreases the synchronization frequency based on the gradient consistency metric. We have implemented GIFT in PyTorch, and large-scale evaluations show that it can improve FL accuracy by up to 10.7% with a time reduction of 58.1%.}
}


@article{DBLP:journals/jsac/YangLK23,
	author = {Jiarong Yang and
                  Yuan Liu and
                  Rahif Kassab},
	title = {Client Selection for Federated Bayesian Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {915--928},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242720},
	doi = {10.1109/JSAC.2023.3242720},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangLK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric distributed learning framework for federated Bayesian learning, where multiple clients jointly train a machine learning model by communicating a number of non-random and interacting particles with the server. Since communication resources are limited, selecting the clients with most informative local learning updates can improve the model convergence and communication efficiency. In this paper, we propose two selection schemes for DSVGD based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive the upper bound on the decrease of the global free energy per iteration for both schemes, which is then minimized to speed up the model convergence. We evaluate and compare our schemes with conventional schemes in terms of model accuracy, convergence speed, and stability using various learning tasks and datasets.}
}


@article{DBLP:journals/jsac/AyacheDR23,
	author = {Ghadir Ayache and
                  Venkat R. Dasari and
                  Salim El Rouayheb},
	title = {Walk for Learning: {A} Random Walk Approach for Federated Learning
                  From Heterogeneous Data},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {929--940},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3244250},
	doi = {10.1109/JSAC.2023.3244250},
	timestamp = {Sat, 03 Jun 2023 13:54:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AyacheDR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of a Parameter Server (PS) that wishes to learn a model that fits data distributed on the nodes of a graph. We focus on Federated Learning (FL) as a canonical application. One of the main challenges of FL is the communication bottleneck between the nodes and the parameter server. A popular solution in the literature is to allow each node to do several local updates on the model in each iteration before sending it back to the PS. While this mitigates the communication bottleneck, the statistical heterogeneity of the data owned by the different nodes has proven to delay convergence and bias the model. In this work, we study random walk (RW) learning algorithms for tackling the communication and data heterogeneity problems. The main idea is to leverage available direct connections among the nodes themselves, which are typically “cheaper” than the communication to the PS. In a random walk, the model is thought of as a “baton” that is passed from a node to one of its neighbors after being updated in each iteration. The challenge in designing the RW is the data hetErogeneity and the uncertainty about the data distributions. Ideally, we would want to visit more often nodes that hold more informative data. We cast this problem as a sleeping multi-armed bandit (MAB) to design near-optimal node sampling strategy that achieves a variance reduced gradient estimates and approaches sub-linearly the optimal sampling strategy. Based on this framework, we present an adaptive random walk learning algorithm. We provide theoretical guarantees on its convergence. Our numerical results validate our theoretical findings and show that our algorithm outperforms existing random walk algorithms.}
}


@article{DBLP:journals/jsac/GeLFDLL23,
	author = {Keshi Ge and
                  Kai Lu and
                  Yongquan Fu and
                  Xiaoge Deng and
                  Zhiquan Lai and
                  Dongsheng Li},
	title = {Compressed Collective Sparse-Sketch for Distributed Data-Parallel
                  Training of Deep Learning Models},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {941--963},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242733},
	doi = {10.1109/JSAC.2023.3242733},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GeLFDLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed data-parallel training (DDP) is prevalent in large-scale deep learning. To increase the training throughput and scalability, high-performance collective communication methods such as AllReduce have recently proliferated for DDP use. However, these approaches require long communication periods with increasing model sizes. Collective communication transmits many sparse gradient values that can be efficiently compressed to reduce the required training time. State-of-the-art compression approaches do not provide mergeable compression for AllReduce and lack convergence bounds. We present a sparse sketch reducer (S2Reducer), a sparsity-preserving sketch-based collective communication method. S2Reducer preserves gradient sparsity and reduces communication costs via a bitmap informed count sketch structure and adapts to efficient AllReduce operators. We tune the count sketch organization to minimize the hash conflicts in a fixed-size budget. We prove that our method has the same convergence rate as vanilla data-parallel training and a much smaller communication overhead than those of state-of-the-art methods. We implement a GPU-accelerated S2Reducer for the Ring AllReduce-based DDP system. We perform extensive evaluations against four state-of-the-art methods across seven deep learning models. Our results show that S2Reducer converges to the same accuracy as that of state-of-the-art approaches while reducing the sparse communication overhead by up to 86% and achieving a speedup of up to 3.5\\times\nin distributed training.}
}


@article{DBLP:journals/jsac/ParkC23,
	author = {Sangjun Park and
                  Wan Choi},
	title = {Regulated Subspace Projection Based Local Model Update Compression
                  for Communication-Efficient Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {964--976},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242722},
	doi = {10.1109/JSAC.2023.3242722},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ParkC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite high utility in distributed networks, federated learning entails enormous communication overhead due to the requirement of trained model exchange at every global iteration. When the communication resources are limited, as in wireless environments, learning performance can be severely degraded by the communication overhead. On this account, communication efficiency is one of the primary concerns in federated learning. In this paper, we put forth a communication-efficient federated learning system based on the projection of local model updates. Leveraging the correlation of consecutive local model updates, we devise a novel local model update compression scheme based on the projection onto the selected subspace. Furthermore, to avoid error propagation over global iterations and thus improve learning performance, we also develop novel criteria for deciding whether to compress the local model updates or not. The convergence of the proposed algorithm is also mathematically proved by deriving an upper bound on the mean square error of the global parameter. The merits of the proposed algorithm over the state-of-the-art benchmark schemes are verified by various simulations.}
}


@article{DBLP:journals/jsac/JahaniNezhadMLC23,
	author = {Tayyebeh Jahani{-}Nezhad and
                  Mohammad Ali Maddah{-}Ali and
                  Songze Li and
                  Giuseppe Caire},
	title = {SwiftAgg+: Achieving Asymptotically Optimal Communication Loads in
                  Secure Aggregation for Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {977--989},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242702},
	doi = {10.1109/JSAC.2023.3242702},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JahaniNezhadMLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose SwiftAgg+ , a novel secure aggregation protocol for federated learning systems, where a central server aggregates local models of N \\in \\mathbb {N}\ndistributed users, each of size L \\in \\mathbb {N}\n, trained on their local data, in a privacy-preserving manner. SwiftAgg+ can significantly reduce the communication overheads without any compromise on security, and achieve optimal communication loads within diminishing gaps. Specifically, in presence of at most D=o(N)\ndropout users, SwiftAgg+ achieves a per-user communication load of \\left({1+\\mathcal {O}\\left({\\frac {1}{N}}\\right)}\\right)L\nsymbols and a server communication load of \\left({1+\\mathcal {O}\\left({\\frac {1}{N}}\\right)}\\right)L\nsymbols, with a worst-case information-theoretic security guarantee, against any subset of up to T=o(N)\nsemi-honest users who may also collude with the curious server. Moreover, the proposed SwiftAgg+ allows for a flexible trade-off between communication loads and the number of active communication links. In particular, for T< N-D\nand for any K\\in \\mathbb {N}\n, SwiftAgg+ can achieve the server communication load of \\left({1+\\frac {T}{K}}\\right)L\nsymbols, and per-user communication load of up to \\left({1+\\frac {T+D}{K}}\\right)L\nsymbols, where the number of pair-wise active connections in the network is \\frac {N}{2}(K+T+D+1)\n.}
}


@article{DBLP:journals/jsac/ChenSQLPC23,
	author = {Rui Chen and
                  Dian Shi and
                  Xiaoqi Qin and
                  Dongjie Liu and
                  Miao Pan and
                  Shuguang Cui},
	title = {Service Delay Minimization for Federated Learning Over Mobile Devices},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {990--1006},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242711},
	doi = {10.1109/JSAC.2023.3242711},
	timestamp = {Fri, 14 Jul 2023 09:13:58 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenSQLPC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) over mobile devices has fostered numerous intriguing applications/services, many of which are delay-sensitive. In this paper, we propose a service delay efficient FL (SDEFL) scheme over mobile devices. Unlike traditional communication efficient FL, which regards wireless communications as the bottleneck, we find that under many situations, the local computing delay is comparable to the communication delay during the FL training process, given the development of high-speed wireless transmission techniques. Thus, the service delay in FL should be computing delay + communication delay over training rounds. To minimize the service delay of FL, simply reducing local computing/communication delay independently is not enough. The delay trade-off between local computing and wireless communications must be considered. Besides, we empirically study the impacts of local computing control and compression strategies (i.e., the number of local updates, weight quantization, and gradient quantization) on computing, communication and service delays. Based on those trade-off observation and empirical studies, we develop an optimization scheme to minimize the service delay of FL over heterogeneous devices. We establish testbeds and conduct extensive emulations/experiments to verify our theoretical analysis. The results show that SDEFL reduces notable service delay with a small accuracy drop compared to peer designs.}
}


@article{DBLP:journals/jsac/YangZTYHZW23,
	author = {Yuzhi Yang and
                  Zhaoyang Zhang and
                  Yuqing Tian and
                  Zhaohui Yang and
                  Chongwen Huang and
                  Caijun Zhong and
                  Kai{-}Kit Wong},
	title = {Over-the-Air Split Machine Learning in Wireless {MIMO} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1007--1022},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242701},
	doi = {10.1109/JSAC.2023.3242701},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangZTYHZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In split machine learning (ML), different partitions of a neural network (NN) are executed by different computing nodes, requiring a large amount of communication cost. As over-the-air computation (OAC) can efficiently implement all or part of the computation at the same time of communication, thus by substituting the wireless transmission in the traditional split ML framework with OAC, the communication load can be eased. In this paper, we propose to deploy split ML in a wireless multiple-input multiple-output (MIMO) communication network utilizing the intricate interplay between MIMO-based OAC and NN. The basic procedure of the OAC split ML system is first provided, and we show that the inter-layer connection in a NN of any size can be mathematically decomposed into a set of linear precoding and combining transformations over a MIMO channel carrying out multi-stream analog communication. The precoding and combining matrices which are regarded as trainable parameters, and the MIMO channel matrix, which are regarded as unknown (implicit) parameters, jointly serve as a fully connected layer of the NN. Most interestingly, the channel estimation procedure can be eliminated by exploiting the MIMO channel reciprocity of the forward and backward propagation, thus greatly saving the system costs and/or further improving its overall efficiency. The generalization of the proposed scheme to the conventional NNs is also introduced, i.e., the widely used convolutional NNs. We demonstrate its effectiveness under both the static and quasi-static memory channel conditions with comprehensive simulations.}
}


@article{DBLP:journals/jsac/GuoWCZL23,
	author = {Chi Guo and
                  Cong Wang and
                  Lin Cui and
                  Qiuzhan Zhou and
                  Juan Li},
	title = {Radio Resource Management for {C-V2X:} From a Hybrid Centralized-Distributed
                  Scheme to a Distributed Scheme},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1023--1034},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242723},
	doi = {10.1109/JSAC.2023.3242723},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuoWCZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spectrum sharing in cellular vehicle-to-everything (C-V2X) has been conceived as a promising solution to improve spectrum efficiency. However, the co-channel interference incurred with it may cause severe performance degradation to vehicular links. Thereby, radio resource management (RRM) is motivated and designed to ensure communication reliability and increase system capacity. One challenge is that RRM involves channel allocation and power control, which are tightly coupled and hard to optimize simultaneously. Another challenge for this is the difficulty adapting centralized RRM schemes, requiring global channel state information (CSI) and causing high signaling overhead. To tackle these challenges, we propose the hybrid centralized-distributed RRM scheme and the distributed RRM scheme. Specifically, we prove a decoupling method that provides a theoretical lower bound so that channel allocation and power control can be optimized independently. Given the decoupling method, the hybrid centralized-distributed RRM scheme is based on graph matching and reinforcement learning (GMRL) to maximize system capacity and guarantee reliability requirements. Further, to decrease computation complexity and signaling overhead, the distributed RRM scheme that only requires local CSI with hybrid-framework reinforcement learning (HFRL) is exploited. Finally, both schemes are numerically evaluated through experiments and outperform other deep Q-network (DQN)-based schemes.}
}


@article{DBLP:journals/jsac/DuJJSH23,
	author = {Jun Du and
                  Bingqing Jiang and
                  Chunxiao Jiang and
                  Yuanming Shi and
                  Zhu Han},
	title = {Gradient and Channel Aware Dynamic Scheduling for Over-the-Air Computation
                  in Federated Edge Learning Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1035--1050},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242727},
	doi = {10.1109/JSAC.2023.3242727},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DuJJSH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To satisfy the expected plethora of computation-heavy applications, federated edge learning (FEEL) is a new paradigm featuring distributed learning to carry the capacities of low-latency and privacy-preserving. To further improve the efficiency of wireless data aggregation and model learning, over-the-air computation (AirComp) is emerging as a promising solution by using the superposition characteristics of wireless channels. However, the fading and noise of wireless channels can cause aggregate distortions in AirComp enabled federated learning. In addition, the quality of collected data and energy consumption of edge devices may also impact the accuracy and efficiency of model aggregation as well as convergence. To solve these problems, this work proposes a dynamic device scheduling mechanism, which can select qualified edge devices to transmit their local models with a proper power control policy so as to participate the model training at the server in federated learning via AirComp. In this mechanism, the data importance is measured by the gradient of local model parameter, channel condition and energy consumption of the device jointly. In particular, to fully use distributed datasets and accelerate the convergence rate of federated learning, the local updates of unselected devices are also retained and accumulated for future potential transmission, instead of being discarded directly. Furthermore, the Lyapunov drift-plus-penalty optimization problem is formulated for searching the optimal device selection strategy. Simulation results validate that the proposed scheduling mechanism can achieve higher test accuracy and faster convergence rate, and is robust against different channel conditions.}
}


@article{DBLP:journals/jsac/WuLQZSZLS23,
	author = {Wen Wu and
                  Mushu Li and
                  Kaige Qu and
                  Conghao Zhou and
                  Xuemin Shen and
                  Weihua Zhuang and
                  Xu Li and
                  Weisen Shi},
	title = {Split Learning Over Wireless Networks: Parallel Design and Resource
                  Management},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1051--1066},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242704},
	doi = {10.1109/JSAC.2023.3242704},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuLQZSZLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Split learning (SL) is a collaborative learning framework, which can train an artificial intelligence (AI) model between a device and an edge server by splitting the AI model into a device-side model and a server-side model at a cut layer. The existing SL approach conducts the training process sequentially across devices, which incurs significant training latency especially when the number of devices is large. In this paper, we design a novel SL scheme to reduce the training latency, named C luster-based P arallel SL (CPSL) which conducts model training in a “first-parallel-then-sequential” manner. Specifically, the CPSL is to partition devices into several clusters, parallelly train device-side models in each cluster and aggregate them, and then sequentially train the whole AI model across clusters, thereby parallelizing the training process and reducing training latency. Furthermore, we propose a resource management algorithm to minimize the training latency of CPSL considering device heterogeneity and network dynamics in wireless networks. This is achieved by stochastically optimizing the cut layer selection, device clustering, and radio spectrum allocation. The proposed two-timescale algorithm can jointly make the cut layer selection decision in a large timescale and device clustering and radio spectrum allocation decisions in a small timescale. Extensive simulation results on non-independent and identically distributed data demonstrate that the proposed solution can greatly reduce the training latency as compared with the existing SL benchmarks, while adapting to network dynamics.}
}


@article{DBLP:journals/jsac/BenHurGKKC23,
	author = {Yuval Ben{-}Hur and
                  Asaf Goren and
                  Da El Klang and
                  Yongjune Kim and
                  Yuval Cassuto},
	title = {Ensemble Classification With Noisy Real-Valued Base Functions},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1067--1080},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242713},
	doi = {10.1109/JSAC.2023.3242713},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BenHurGKKC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In data-intensive applications, it is advantageous to perform partial processing close to the data, and communicate intermediate results to a central processor, instead of the data itself. When the communication or computation medium is noisy, the resulting degradation in computation quality at the central processor must be mitigated. We study this problem for the setup of binary classification performed by an ensemble of base functions communicating real-valued confidence levels. We propose a noise-mitigation solution that optimizes the transmission gains and aggregation coefficients of the base functions. Toward that, we formulate a post-training gradient-based optimization algorithm that minimizes the error probability given the training dataset and the noise parameters. We further derive lower and upper bounds on the optimized error probability, and show empirical results that demonstrate the enhanced performance achieved by our approach on real data.}
}


@article{DBLP:journals/jsac/LiuCMW23,
	author = {Zhenyu Liu and
                  Andrea Conti and
                  Sanjoy K. Mitter and
                  Moe Z. Win},
	title = {Communication-Efficient Distributed Learning Over Networks - Part
                  {I:} Sufficient Conditions for Accuracy},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1081--1101},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242731},
	doi = {10.1109/JSAC.2023.3242731},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuCMW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed learning is an important task in emerging applications such as localization and navigation, Internet-of-Things, and autonomous vehicles. This paper establishes a theoretical framework for learning states that evolve in real time over networks. Specifically, each agent node in the network aims to infer a time-varying state in a decentralized manner by using the node’s local observations and the messages received from other nodes within its communication range. As a result, the inference accuracy of a node is significantly affected by the quality of its received messages. This calls for carefully designed strategies for generating messages that are able to provide sufficient information for the receiver and are robust to channel impairments. This paper presents communication-efficient encoding strategies for generating transmitted messages and derives a sufficient condition for the boundedness of the distributed inference error of all the agent nodes over time. The findings of this paper provide guidelines for the design of communication-efficient distributed learning in complex networked systems.}
}


@article{DBLP:journals/jsac/LiuCMW23a,
	author = {Zhenyu Liu and
                  Andrea Conti and
                  Sanjoy K. Mitter and
                  Moe Z. Win},
	title = {Communication-Efficient Distributed Learning Over Networks - Part
                  {II:} Necessary Conditions for Accuracy},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1102--1119},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242738},
	doi = {10.1109/JSAC.2023.3242738},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuCMW23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed learning is crucial for many applications such as localization and tracking, autonomy, and crowd sensing. This paper investigates communication-efficient distributed learning of time-varying states over networks. Specifically, the paper considers a network of nodes that infer their current states in a decentralized manner using observations obtained via local sensing and messages obtained via noisy inter-node communications. The paper derives a necessary condition in terms of the sensing and communication capabilities of the network for the boundedness of the learning error over time. The necessary condition is compared with the sufficient condition established in a companion paper and the gap between the two conditions is discussed. The paper provides guidelines for efficient management of the sensing and communication resources for distributed learning in complex networked systems.}
}


@article{DBLP:journals/jsac/SchynolP23,
	author = {Lukas Schynol and
                  Marius Pesavento},
	title = {Coordinated Sum-Rate Maximization in Multicell {MU-MIMO} With Deep
                  Unrolling},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1120--1134},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242716},
	doi = {10.1109/JSAC.2023.3242716},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SchynolP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coordinated weighted sum-rate maximization in multicell MIMO networks with intra- and intercell interference and local channel state at the base stations is recognized as an important yet difficult problem. A classical, locally optimal solution is obtained by the weighted minimum mean squared error (WMMSE) algorithm which facilitates a distributed implementation in multicell networks. However, it often suffers from slow convergence and therefore large communication overhead. To obtain more practical solutions, the unrolling/unfolding of traditional iterative algorithms gained significant attention. In this work, we demonstrate a complete unfolding of the WMMSE algorithm for transceiver design in multicell MU-MIMO interference channels with local channel state information. The resulting architecture termed GCN-WMMSE applies ideas from graph signal processing and is agnostic to different wireless network topologies, while exhibiting a low number of trainable parameters and high efficiency w.r.t. training data. It significantly reduces the number of required iterations while achieving performance similar to the WMMSE algorithm, alleviating the overhead in a distributed deployment. Additionally, we review previous architectures based on unrolling the WMMSE algorithm and compare them to GCN-WMMSE in their specific applicable domains.}
}


@article{DBLP:journals/jsac/ZhouZLL23,
	author = {Ruiting Zhou and
                  Xueying Zhang and
                  John C. S. Lui and
                  Zongpeng Li},
	title = {Dynamic Pricing and Placing for Distributed Machine Learning Jobs:
                  An Online Learning Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1135--1150},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242707},
	doi = {10.1109/JSAC.2023.3242707},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouZLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays distributed machine learning (ML) jobs usually adopt a parameter server (PS) framework to train models over large-scale datasets. Such ML job deploys hundreds of concurrent workers, and model parameter updates are exchanged frequently between workers and PSs. Current practice is that workers and PSs may be placed on different physical servers, bringing uncertainty in jobs’ runtime. Existing cloud pricing policy often charges a fixed price according to the job’s runtime. Although this pricing strategy is simple to implement, such pricing mechanism is not suitable for distributed ML jobs whose runtime is stochastic and can only be estimated according to its placement after job admission. To supplement existing cloud pricing schemes, we design a dynamic pricing and placement algorithm, DPS, for distributed ML jobs. DPS aims to maximize the cloud service provider’s profit, which dynamically calculates unit resource price upon a job’s arrival, and determines job’s placement to minimize its runtime if offered price is accepted to users. Our design exploits the multi-armed bandit (MAB) technique to learn unknown information based on past sales. DPS balances the exploration and exploitation stage, and selects the best price based on the reward which is related to job runtime. Our learning-based algorithm can increase the provider’s profit by 200%, and achieves a sub-linear regret with both the time horizon and the total job number, compared to benchmark pricing schemes. Extensive evaluations using real-world data also validates the efficacy of DPS.}
}


@article{DBLP:journals/jsac/WangWBL23,
	author = {Shuo Wang and
                  Yongcai Wang and
                  Xuewei Bai and
                  Deying Li},
	title = {Communication Efficient, Distributed Relative State Estimation in
                  {UAV} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1151--1166},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242708},
	doi = {10.1109/JSAC.2023.3242708},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangWBL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed estimation of 6-DOF relative states, including three-dimensional relative poses and three-dimensional relative positions, is a key problem in UAV (Unmanned Aerial Vehicle) networks, which generally requires vision-involved iterative state estimation. How to achieve communication efficiency is a crucial challenge considering the large volume of vision data. This paper jointly considers the communication efficiency, latency, and accuracy for distributed relative state estimation involving vision data in UAV networks. The key is to solve a distributed graph optimization problem, which includes two key steps: (1) local graph construction and node state initialization in an initialization phase, and (2) iterative state update and communication with neighbors until convergence in online iteration phase. A communication efficient, Locating Then Informing (LTI) initialization scheme is proposed, which is run only once by each node to initialize each node’s local graph and initial states. For online iteration, a RIPPLE-like distributed state iteration scheme is proposed. It inherits the advantages of traditional sequential and parallel methods while avoiding their drawbacks. It enables nodes’ states to converge quickly using fewer rounds of communications. The communication costs for the initialization and online iteration processes are analyzed theoretically. Extensive evaluations use synthetic data generated by AirSim (a widely used UAV network simulation platform) and real-world data are presented. The results show that the proposed method provides accuracy comparable to the centralized graph optimization method and significantly outperforms the other distributed methods in terms of accuracy, communication cost, and latency.}
}


@article{DBLP:journals/jsac/HuangYHB23,
	author = {Chao Huang and
                  Haoran Yu and
                  Jianwei Huang and
                  Randall Berry},
	title = {An Online Inference-Aided Incentive Framework for Information Elicitation
                  Without Verification},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1167--1185},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242706},
	doi = {10.1109/JSAC.2023.3242706},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuangYHB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the design of incentive mechanisms for the problem of information elicitation without verification (IEWV). In IEWV, a data requester seeks to design proper incentives to optimize the tradeoff between the quality of information (collected from distributed crowd workers) and the total cost of incentives (provided to crowd workers) without verifiable ground truth. While prior work often relies on sufficient knowledge of worker information, we study a scenario where the data requester cannot access workers’ heterogeneous information quality and costs ex-ante. We propose a continuum-armed bandit-based incentive mechanism that dynamically learns the optimal reward level from workers’ reported information. A key challenge is that the data requester cannot evaluate the workers’ information quality without verification, which motivates the design of an inference algorithm. The inference problem is non-convex, yet we reformulate it as a bi-convex problem and derive an approximate solution with a performance guarantee, which ensures the effectiveness of our online reward design. We further enhance the inference algorithm using part of the workers’ historical reports. We also propose a novel rule for the data requester to aggregate workers’ solutions more effectively. We show that our mechanism achieves a sub-linear regret\nO\n~\n(\nT\n1/2\n)\nand outperforms several celebrated benchmarks.}
}


@article{DBLP:journals/jsac/LiuLH23,
	author = {Zhiyan Liu and
                  Qiao Lan and
                  Kaibin Huang},
	title = {Resource Allocation for Multiuser Edge Inference With Batching and
                  Early Exiting},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1186--1200},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242724},
	doi = {10.1109/JSAC.2023.3242724},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The deployment of inference services at the network edge, called edge inference, offloads computation-intensive inference tasks from mobile devices to edge servers, thereby enhancing the former’s capabilities and battery lives. In a multiuser system, the joint allocation of communication-and-computation (C2) resources (i.e., scheduling and bandwidth allocation) is made challenging by adopting efficient inference techniques, batching and early exiting, and further complicated by the heterogeneity in users’ requirements on accuracy and latency. Batching groups multiple tasks into a single batch for parallel processing to reduce time-consuming memory access and thereby boosts the throughput (i.e., completed task per second). On the other hand, early exiting allows a task to exit from a deep-neural network without traversing the whole network, thereby supporting a tradeoff between accuracy and latency. In this work, we study optimal C2 resource allocation with batching and early exiting, which is an NP-complete integer programming problem. A set of efficient algorithms are designed under the criterion of maximum throughput by tackling the challenge. First, consider the case with batching but without early exiting. The target problem is solved optimally using a proposed best-shelf-packing algorithm that nests a threshold-based scheme, which selects users with the best channels and meeting the computation-time constraints, in a sequential search for the maximum batch size. Next, consider the general case with batching and early exiting. A low-complexity sub-optimal algorithm for C2 resource allocation is developed by modifying the preceding algorithm to exploit early exiting for latency reduction. On the other hand, the optimal approach is developed based on nesting a depth-first tree-search with intelligent online pruning into a sequential search for the maximum batch size. The key idea is to derive pruning criteria based on the simple greedy solution for the target problem without a bandwidth constraint and apply the result to designing an intelligent online pruning scheme. Experimental results demonstrate that both optimal and sub-optimal C2 resource allocation algorithms can leverage integrated batching and early exiting to double the inference throughput compared with conventional schemes.}
}


@article{DBLP:journals/jsac/RodriguesK23,
	author = {Tiago Koketsu Rodrigues and
                  Nei Kato},
	title = {Hybrid Centralized and Distributed Learning for MEC-Equipped Satellite
                  6G Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1201--1211},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242700},
	doi = {10.1109/JSAC.2023.3242700},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RodriguesK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For future networks in the 6G, it will be important to maintain a ubiquitous connection, bring processing heavy applications to remote areas, and analyze big amounts of data to efficiently provide services. To achieve such goals, the literature has utilized satellite networks to reach areas far away from the network core, and there has even been research into equipping such satellites with edge cloud servers to provide computation offloading to remote devices. However, analyzing the big data created by these devices is still a problem. One could transfer the data to a central server, but this has a high transmission cost. One could process the data through distributed machine learning, but such a technique is not as efficient as centralized learning. Thus, in this paper, we analyze the learning costs behind centralized and distributed learning and propose a hybrid solution that adaptively uses the advantages of both in a cloud server-equipped satellite network. Our proposal can identify the best learning strategy for each device based on the current scenario. Results show that the proposal is not only efficient in solving machine learning tasks, but it is also dynamic to react to different configurations while maintaining top performance.}
}


@article{DBLP:journals/jsac/ZhangYZRYHG23,
	author = {Zhanhua Zhang and
                  Shusen Yang and
                  Cong Zhao and
                  Xuebin Ren and
                  Hanqiao Yu and
                  Qing Han and
                  Siyan Guo},
	title = {RTCoInfer: Real-Time Collaborative {CNN} Inference for Stream Analytics
                  on Ubiquitous Images},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1212--1226},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242730},
	doi = {10.1109/JSAC.2023.3242730},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangYZRYHG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging intelligent applications based on accurate and timely stream analytics require real-time CNN inference of massive data continuously generated at the pervasive end devices. Due to the resource constraints, neither computing locally at end devices nor transmitting to remote servers is competent for computation-intensive CNN inference on large-volume images in real-time. Therefore, Collaborative Inference (CI), which conducts inference sequentially from the local device to the remote server with compressed intermediate inference data, is rapidly promoted. Due to the essential communication in collaboration, the CI efficiency is sensitive to network conditions, and will degrade under the unpredictable network fluctuations in practice, which may cause a severe delay in CI and degrade the responsiveness of stream analytics. For accurate and timely stream analytics in practical fluctuating networks, we present RTCoInfer, the real-time CI framework with run-time transmission adaption considering the network conditions. Specifically, we propose a novel Switchable CNN integrating CNNs with different compression rates on the partition layer for the run-time transmission adjustment, and construct a real-time controller determining the compression rate to maintain the real-time CI for stream analytics. Extensive experiments show that, compared with state-of-the-art methods, RTCoInfer achieves better efficiency and unprecedented resilience in real-time stream analytics.}
}


@article{DBLP:journals/jsac/XieS23,
	author = {Zhijie Xie and
                  Shenghui Song},
	title = {FedKL: Tackling Data Heterogeneity in Federated Reinforcement Learning
                  by Penalizing {KL} Divergence},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1227--1242},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242734},
	doi = {10.1109/JSAC.2023.3242734},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XieS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {One of the fundamental issues for Federated Learning (FL) is data heterogeneity, which causes accuracy degradation, slow convergence, and the communication bottleneck issue. Although the impact of data heterogeneity on supervised FL has been widely studied, the related investigation for Federated Reinforcement Learning (FRL) is still in its infancy. In this paper, we first define the type and level of data heterogeneity for FRL systems. By inspecting the connection between the global and local objective functions, we prove that local training can benefit the global objective, if the local update is properly penalized by the total variation (TV) distance between the local and global policies. A necessary condition for the global policy to be learn-able from the local environments is also derived, which is directly related to the heterogeneity level. Based on the theoretical result, a Kullback-Leibler (KL) divergence based penalty is proposed to directly constrain the model outputs in the distribution space and the convergence proof of the proposed algorithm is also provided. By jointly penalizing the divergence of the local policy from the global policy with a global penalty and penalizing each iteration of the local training with a local penalty, the proposed method achieves a better trade-off between training speed (step size) and convergence. Experiment results on two popular Reinforcement Learning (RL) experiment platforms demonstrate the advantage of the proposed algorithm over existing methods in accelerating and stabilizing the training process with heterogeneous data.}
}


@article{DBLP:journals/jsac/XuLCMD23,
	author = {Xiaoxia Xu and
                  Yuanwei Liu and
                  Qimei Chen and
                  Xidong Mu and
                  Zhiguo Ding},
	title = {Distributed Auto-Learning {GNN} for Multi-Cell Cluster-Free {NOMA}
                  Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {4},
	pages = {1243--1258},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242703},
	doi = {10.1109/JSAC.2023.3242703},
	timestamp = {Tue, 28 Mar 2023 19:50:24 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XuLCMD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A multi-cell cluster-free NOMA framework is proposed, where both intra-cell and inter-cell interference are jointly mitigated via flexible cluster-free successive interference cancellation (SIC) and coordinated beamforming design. The joint design problem is formulated to maximize the system sum rate while satisfying the SIC decoding requirements and users’ minimum data rate requirements. To address this highly complex and coupling non-convex mixed integer nonlinear programming (MINLP), a novel distributed auto-learning graph neural network (AutoGNN) architecture is proposed to alleviate the overwhelming information exchange burdens among base stations (BSs). The proposed AutoGNN can train the GNN model weights whilst automatically optimizing the GNN architecture, namely the GNN network depth and message embedding sizes, to achieve communication-efficient distributed scheduling. Based on the proposed architecture, a bi-level AutoGNN learning algorithm is further developed to efficiently approximate the hypergradient in model training. It is theoretically proved that the proposed bi-level AutoGNN learning algorithm can converge to a stationary point. Numerical results reveal that: 1) the proposed cluster-free NOMA framework outperforms the conventional cluster-based NOMA framework in the multi-cell scenario; and 2) the proposed AutoGNN architecture significantly reduces the computation and communication overheads compared to the conventional convex optimization-based methods and the conventional GNNs with fixed architectures.}
}


@article{DBLP:journals/jsac/ClerckxMJYLEN23,
	author = {Bruno Clerckx and
                  Yijie Mao and
                  Eduard A. Jorswieck and
                  Jinhong Yuan and
                  David J. Love and
                  Elza Erkip and
                  Dusit Niyato},
	title = {Guest Editorial Rate Splitting for Future Wireless Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1259--1264},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240013},
	doi = {10.1109/JSAC.2023.3240013},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ClerckxMJYLEN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate splitting (RS) and rate splitting multiple access (RSMA) have emerged as a promising and powerful multiple access, interference management, and multi-user strategy for next-generation wireless systems and networks. This Special Issue is entirely dedicated to the theory, design, optimization, and applications of RS and RSMA in various network configurations. It starts with a guest editor-authored tutorial paper [A1] that delineates the basic principles and applications of RS and RSMA. The tutorial paper is then followed by 17 technical papers.}
}


@article{DBLP:journals/jsac/ClerckxMJYLEN23a,
	author = {Bruno Clerckx and
                  Yijie Mao and
                  Eduard A. Jorswieck and
                  Jinhong Yuan and
                  David J. Love and
                  Elza Erkip and
                  Dusit Niyato},
	title = {A Primer on Rate-Splitting Multiple Access: Tutorial, Myths, and Frequently
                  Asked Questions},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1265--1308},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3242718},
	doi = {10.1109/JSAC.2023.3242718},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ClerckxMJYLEN23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate-Splitting Multiple Access (RSMA) has emerged as a powerful multiple access, interference management, and multi-user strategy for next generation communication systems. In this tutorial, we depart from the orthogonal multiple access (OMA) versus non-orthogonal multiple access (NOMA) discussion held in 5G, and the conventional multi-user linear precoding approach used in space-division multiple access (SDMA), multi-user and massive MIMO in 4G and 5G, and show how multi-user communications and multiple access design for 6G and beyond should be intimately related to the fundamental problem of interference management. We start from foundational principles of interference management and rate-splitting, and progressively delineate RSMA frameworks for downlink, uplink, and multi-cell networks. We show that, in contrast to past generations of multiple access techniques (OMA, NOMA, SDMA), RSMA offers numerous benefits: 1) enhanced spectral, energy and computation efficiency; 2) universality by unifying and generalizing OMA, SDMA, NOMA, physical-layer multicasting, multi-user MIMO under a single framework that holds for any number of antennas at each node (SISO, SIMO, MISO, and MIMO settings); 3) flexibility by coping with any interference levels (from very weak to very strong), network loads (underloaded, overloaded), services (unicast, multicast), traffic, user deployments (channel directions and strengths); 4) robustness to inaccurate channel state information (CSI) and resilience to mixed-critical quality of service; 5) reliability under short channel codes and low latency. We then discuss how those benefits translate into numerous opportunities for RSMA in over forty different applications and scenarios of 6G, e.g., multi-user MIMO with statistical/quantized CSI, FDD/TDD/cell-free massive MIMO, millimeter wave and terahertz, cooperative relaying, physical layer security, reconfigurable intelligent surfaces, cloud-radio access network, internet-of-things, massive access, joint communication and jamming, non-orthogonal unicast and multicast, multigroup multicast, multibeam satellite, space-air-ground integrated networks, unmanned aerial vehicles, integrated sensing and communications, grant-free access, network slicing, cognitive radio, optical/visible light communications, mobile edge computing, machine/federated learning, etc. We finally address common myths and answer frequently asked questions, opening the discussions to interesting future research avenues. Supported by the numerous benefits and applications, the tutorial concludes on the underpinning role played by RSMA in next generation networks, which should inspire future research, development, and standardization of RSMA-aided communication for 6G.}
}


@article{DBLP:journals/jsac/WangJ23,
	author = {Junge Wang and
                  Syed Ali Jafar},
	title = {Robust Sum-GDoF of Symmetric 2 {\texttimes} 2 {\texttimes} 2 Weak
                  Interference Channel With Heterogeneous Hops},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1309--1319},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240780},
	doi = {10.1109/JSAC.2023.3240780},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The symmetric\n2×2×2\nweak interference channel setting with heterogeneous hops is explored from a Generalized Degrees of Freedom (GDoF) perspective, especially under the robust assumption that limits the channel state information at the transmitters (CSIT) to finite precision. Specifically, in the\nℓ\nth\nhop,\nℓ∈{1,2}\n, both direct channels have strength\nα\n[ℓ]\n, both cross channels have strength\nβ\n[ℓ]\n(in logarithmic scale), and\nβ\n[ℓ]\n≤0.5\nα\n[ℓ]\n. Thus, while assuming symmetry within each hop, the model allows heterogeneity across hops\n(\nα\n[1]\n,\nβ\n[1]\n)≠(\nα\n[2]\n,\nβ\n[2]\n)\n. Because\nβ\n[ℓ]\n≤0.5\nα\n[ℓ]\n, each hop corresponds to an interference channel in the weak interference regime where power control and treating interference as noise are known to be sum-GDoF optimal in a 1-hop setting. The main result of this work is the exact sum-GDoF of the symmetric\n2×2×2\nweak interference channel for heterogeneous hops under finite precision CSIT. Compared to prior work that assumes homogeneous hops, heterogeneous hops require not only more sophisticated optimal rate-splitting arguments, but also quantize-and-forward ideas which were not needed for homogeneous hops. The converse proof similarly involves generalizations to accommodate hop heterogeneity, as well as new bounds beyond the homogeneous case, based on sum-set inequalities and aligned images arguments. Additional results include sum-GDoF for perfect CSIT, and for a natural dual strong interference setting where\nβ\n[ℓ]\n≥2\nα\n[ℓ]\n.}
}


@article{DBLP:journals/jsac/VanLFNK23,
	author = {Nguyen Thi Thanh Van and
                  Nguyen Cong Luong and
                  Shaohan Feng and
                  Van{-}Dinh Nguyen and
                  Dong In Kim},
	title = {Evolutionary Games for Dynamic Network Resource Selection in RSMA-Enabled
                  6G Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1320--1335},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240779},
	doi = {10.1109/JSAC.2023.3240779},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/VanLFNK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we address a dynamic network resource selection problem for mobile users in a rate-splitting multiple access (RSMA)-enabled network by leveraging evolutionary games. Particularly, mobile users are able to locally and dynamically make their selection on orthogonal resource blocks (RBs), which are also considered as network resources (NRs), over time to achieve their desired utilities. Then, RSMA is used for each group of users selecting the same NR. With the use of RSMA, the main goal is to optimize the beamformers of the common and private messages for users in the same group to maximize their sum rate. The resulting problem is generally non-convex, and thus we develop a successive convex approximation (SCA)-based algorithm to efficiently solve it in an iterative fashion. To model the NR adaptation of users, we propose to use two evolutionary games, i.e. a traditional evolutionary game (TEG) and fractional evolutionary game (FEG). The FEG approach enables users to incorporate memory effects (i.e. their past experiences) for their decision-making, which is more realistic than the TEG approach. We then theoretically verify the existence of the equilibrium of the proposed game approaches. Simulation results are provided to validate their consistency with the theoretical analysis and merits of the proposed approaches. They also reveal that, compared with TEG, FEG enables users to leverage past information for their decision-making, resulting in less communication overhead, while still guaranteeing convergence.}
}


@article{DBLP:journals/jsac/MosqueraG23,
	author = {Carlos Mosquera and
                  Felipe G{\'{o}}mez{-}Cuba},
	title = {Link Adaptation for Rate Splitting Systems With Partial {CSIT}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1336--1350},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240787},
	doi = {10.1109/JSAC.2023.3240787},
	timestamp = {Wed, 17 May 2023 21:56:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MosqueraG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate Splitting (RS) has emerged as a flexible multiple-access scheme for Multi-User Multiple-Input Multiple-Output (MU-MIMO) systems, generalizing more conventional approaches such as linear precoding or Non-Orthogonal Multiple Access (NOMA), and providing additional robustness against imperfect channel state information at the transmitter. Actual achievable rates cannot be pre-calculated when facing imperfect and/or outdated channel state information at the transmitter (CSIT), so practical real-time mechanisms in the form of link adaptation (LA) are required. The available CSIT is used to design the RS precoders, which can accommodate some degree of robustness, whereas the physical layer feedback reports the outcome of the decoding of the codewords, in the form of acknowledgement (ACK) or negative ACK (NACK), providing the additional meta-robustness needed when the statistical model of CSIT errors is not accurate, and/or the precoders cannot fully track the channel dynamics. We characterize the outage rate region of RS and analyze the convergence of the complete LA-RS system under partial CSIT (non-accurate and non-permanent), providing relevant insights for addressing the adaptation of private and common rates, designing robust precoders with throughput as relevant metric, and handling the time-variant CSIT quality.}
}


@article{DBLP:journals/jsac/AmorJU23,
	author = {Donia Ben Amor and
                  Michael Joham and
                  Wolfgang Utschick},
	title = {Rate Splitting in {FDD} Massive {MIMO} Systems Based on the Second
                  Order Statistics of Transmission Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1351--1365},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240703},
	doi = {10.1109/JSAC.2023.3240703},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AmorJU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we present new results for the application of rate splitting multiple access (RSMA) to the downlink (DL) of a massive multiple-input-multiple-output system (MIMO) operating in frequency-division-duplex (FDD) mode. We propose a statistical precoding design relying on the channels’ second-order information when one-layer RS is implemented. The advantage of the statistical precoding lies in simplifying the precoder design by basing the optimization objective on the slowly-varying channel covariance matrices instead of the fast-changing instantaneous channel estimates. To this end, we use the so- called bilinear precoder, which was shown in Neumann et al. (2018) and Ben Amor et al. (2020) to have limited performance in the high SNR regime due to the imperfect channel state information (CSI) available at the base station (BS). We formulate the DL throughput maximization problem based on a widely used lower bound on the achievable sum rate and propose an iterative approach to solve the underlying optimization problem. Numerical results demonstrate the benefit of implementing one-layer RS. Furthermore, the proposed iterative approach achieves excellent results in terms of spectral efficiency compared to the state-of-the-art techniques.}
}


@article{DBLP:journals/jsac/ZhengZCLNA23,
	author = {Jiakang Zheng and
                  Jiayi Zhang and
                  Julian Cheng and
                  Victor C. M. Leung and
                  Derrick Wing Kwan Ng and
                  Bo Ai},
	title = {Asynchronous Cell-Free Massive {MIMO} With Rate-Splitting},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1366--1382},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240709},
	doi = {10.1109/JSAC.2023.3240709},
	timestamp = {Mon, 21 Aug 2023 15:51:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengZCLNA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In practical cell-free (CF) massive multiple-input multiple-output (MIMO) networks with distributed and low-cost access points, the asynchronous arrival of signals at the user equipments increases multi-user interference that degrades the system performance. Meanwhile, rate-splitting (RS), exploiting the transmission of both common and private messages, has demonstrated to offer considerable spectral efficiency (SE) improvements and its robustness against channel state information (CSI) imperfection. The signal performance of a CF massive MIMO system is first analyzed for asynchronous reception capturing the joint effects of propagation delays and oscillator phases of transceivers. Taking into account the imperfect CSI caused by asynchronous phases and pilot contamination, we derive novel and closed-form downlink SE expressions for characterizing the performance of both the RS-assisted and conventional non-RS-based systems adopting coherent and non-coherent data transmission schemes, respectively. Moreover, we formulate the design of robust precoding for the common messages as an optimization problem that maximizes the minimum individual SE of the common message. To address the non-convexity of the design problem, a bisection method is proposed to solve the problem optimally. Simulation results show that asynchronous reception indeed destroys both the orthogonality of the pilots and the coherent data transmission resulting in poor system performance. Besides, thanks to the uniform coverage properties of CF massive MIMO systems, RS with a simple low-complexity precoding for the common message obtained by the equal ratio sum of the private precoding is able to achieve substantial downlink sum SE gains, while the application of robust precoding to the common message is shown to be useful in some extreme cases, e.g., serious oscillator mismatch and unknown delay phase.}
}


@article{DBLP:journals/jsac/JiangYEWWGA23,
	author = {Hanyu Jiang and
                  Li You and
                  Ahmed Elzanaty and
                  Jue Wang and
                  Wenjin Wang and
                  Xiqi Gao and
                  Mohamed{-}Slim Alouini},
	title = {Rate-Splitting Multiple Access for Uplink Massive {MIMO} With Electromagnetic
                  Exposure Constraints},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1383--1397},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240788},
	doi = {10.1109/JSAC.2023.3240788},
	timestamp = {Mon, 31 Jul 2023 09:43:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JiangYEWWGA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past few years, the prevalence of wireless devices has become one of the essential sources of electromagnetic (EM) radiation to the public. Facing with the swift development of wireless communications, people are skeptical about the risks of long-term exposure to EM radiation. As EM exposure is required to be restricted at user terminals, it is inefficient to blindly decrease the transmit power, which leads to limited spectral efficiency and energy efficiency (EE). Recently, rate-splitting multiple access (RSMA) has been proposed as an effective way to provide higher wireless transmission performance, which is a promising technology for future wireless communications. To this end, we propose using RSMA to increase the EE of massive MIMO uplink while limiting the EM exposure of users. In particularly, we investigate the optimization of the transmit covariance matrices and decoding order using statistical channel state information (CSI). The problem is formulated as non-convex mixed integer program, which is in general difficult to handle. We first propose a modified water-filling scheme to obtain the transmit covariance matrices with fixed decoding order. Then, a greedy approach is proposed to obtain the decoding permutation. Numerical results verify the effectiveness of the proposed EM exposure-aware EE maximization scheme for uplink RSMA.}
}


@article{DBLP:journals/jsac/WangWW23,
	author = {Yuan Wang and
                  Vincent W. S. Wong and
                  Jiaheng Wang},
	title = {Flexible Rate-Splitting Multiple Access With Finite Blocklength},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1398--1412},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240783},
	doi = {10.1109/JSAC.2023.3240783},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate-splitting multiple access (RSMA) is a promising multiple access (MA) technique. It employs rate-splitting (RS) at the transmitter and successive interference cancellation (SIC) at the receiver. Most of the existing works on RSMA assume that all users use SIC to decode the common stream and the blocklength is infinite. The first assumption causes the data rate of the common stream to be limited by the user with the worst channel quality. The second assumption may lead to suboptimal performance in practical systems with finite blocklength. In this paper, we propose a flexible RSMA scheme, which allows the system to decide whether a user should use SIC to decode the common stream or not. We consider the effective throughput as the performance metric, which incorporates the data rate as well as the error performance of RSMA with finite blocklength. We first derive the effective throughput expression and then formulate an effective throughput maximization problem by jointly optimizing the beamforming vectors, transmission data rates, and RS-user selection. We develop an optimal algorithm as well as a low-complexity algorithm for beamforming design. We derive a semi-closed-form solution of the optimal data rates and propose an efficient algorithm for the RS-user selection. Numerical results demonstrate that the proposed algorithm obtains a higher effective throughput than space division multiple access (SDMA), non-orthogonal multiple access (NOMA), and two other RSMA schemes.}
}


@article{DBLP:journals/jsac/ZhangXLFNL23,
	author = {Ruichen Zhang and
                  Ke Xiong and
                  Yang Lu and
                  Pingyi Fan and
                  Derrick Wing Kwan Ng and
                  Khaled B. Letaief},
	title = {Energy Efficiency Maximization in RIS-Assisted {SWIPT} Networks With
                  {RSMA:} {A} PPO-Based Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1413--1430},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240707},
	doi = {10.1109/JSAC.2023.3240707},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangXLFNL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates reconfigurable intelligent surface (RIS)-assisted simultaneous wireless information and power transfer (SWIPT) networks with rate splitting multiple access (RSMA). An energy efficiency (EE) maximization problem is formulated subject to the power budget at the transmitter and the quality of service (QoS) requirements of both information communication and energy harvesting, where the beamforming vectors, the power splitting (PS) ratios, the common message rates, and the discrete phase shifts are jointly optimized. To tackle the non-convex problem with both discrete and continuous variables, a deep reinforcement learning-based approach is proposed with the proximal policy optimization (PPO) framework. Different from traditional optimization approaches which optimizes the beamforming vectors and phase shifts separately and alternatively, our proposed PPO-based approach optimizes all the variables in unison. Besides, to perform beamforming design in action space, the beamforming vectors for the common stream and the private stream are respectively designed based on the maximum-ratio transmission and the zero forcing to enhance both energy and information transmission. To evaluate the performance of the PPO-based approach, a successive convex approximation (SCA) and Dinkelbach’s method based solution scheme (named SCA-D scheme) is also presented. Simulation results show that the system EE obtained by the proposed PPO-based approach is close to that obtained by the SCA-D scheme while outperforming various benchmarks. The RSMA contributes to the EE of the system greatly compared with traditional scheme. As for the case of time-varying channels, the proposed PPO-based approach is with much smaller running time by only sacrificing a slight EE performance compared with the SCA-D scheme.}
}


@article{DBLP:journals/jsac/WuGHXNZ23,
	author = {Minghui Wu and
                  Zhen Gao and
                  Yang Huang and
                  Zhenyu Xiao and
                  Derrick Wing Kwan Ng and
                  Zhaoyang Zhang},
	title = {Deep Learning-Based Rate-Splitting Multiple Access for Reconfigurable
                  Intelligent Surface-Aided Tera-Hertz Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1431--1451},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240781},
	doi = {10.1109/JSAC.2023.3240781},
	timestamp = {Mon, 09 Oct 2023 17:01:04 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuGHXNZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) can significantly enhance the service coverage of Tera-Hertz massive multiple-input multiple-output (MIMO) communication systems. However, obtaining accurate high-dimensional channel state information (CSI) with limited pilot and feedback signaling overhead is challenging, severely degrading the performance of conventional spatial division multiple access. To improve the robustness against CSI imperfection, this paper proposes a deep learning (DL)-based rate-splitting multiple access (RSMA) scheme for RIS-aided Tera-Hertz multi-user MIMO systems. Specifically, we first propose a hybrid data-model driven DL-based RSMA precoding scheme, including the passive precoding at the RIS as well as the analog active precoding and the RSMA digital active precoding at the base station (BS). To realize the passive precoding at the RIS, we propose a Transformer-based data-driven RIS reflecting network (RRN). As for the analog active precoding at the BS, we propose a match-filter based analog precoding scheme considering that the BS and RIS adopt the LoS-MIMO antenna array architecture. As for the RSMA digital active precoding at the BS, we propose a low-complexity approximate weighted minimum mean square error (AWMMSE) digital precoding scheme, and further design a model-driven deep unfolding active precoding network (DFAPN) by combining the proposed AWMMSE scheme with DL. Then, to acquire accurate CSI at the BS for the investigated RSMA precoding scheme to achieve higher spectral efficiency, we propose a CSI acquisition network (CAN) with low pilot and feedback signaling overhead. The proposed DL-based RSMA scheme for RIS-aided Tera-Hertz multi-user MIMO systems can exploit the advantages of RSMA and DL to improve the robustness against CSI imperfection, thus achieving higher spectral efficiency with lower signaling overhead.}
}


@article{DBLP:journals/jsac/NiuLAWZAW23,
	author = {Hehao Niu and
                  Zhi Lin and
                  Kang An and
                  Jiangzhou Wang and
                  Gan Zheng and
                  Naofal Al{-}Dhahir and
                  Kai{-}Kit Wong},
	title = {Active {RIS} Assisted Rate-Splitting Multiple Access Network: Spectral
                  and Energy Efficiency Tradeoff},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1452--1467},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240718},
	doi = {10.1109/JSAC.2023.3240718},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/NiuLAWZAW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increasing demand of high data rate and massive access in both ultra-dense and industrial Internet-of-things networks, spectral efficiency (SE) and energy efficiency (EE) are regarded as two important and inter-related performance metrics for future networks. In this paper, we investigate a novel integration of rate-splitting multiple access (RSMA) and reconfigurable intelligent surface (RIS) into cellular systems to achieve a desirable tradeoff between SE and EE. Different from the commonly used passive RIS, we adopt reflection elements with active load to improve a newly defined metric, called resource efficiency (RE), which is capable of striking a balance between SE and EE. This paper focuses on the RE optimization by jointly designing the base station (BS) transmit precoding and RIS beamforming (BF) while guaranteeing the transmit and forward power budgets of the BS and RIS, respectively. To efficiently tackle the challenges for solving the RE maximization problem due to its fractional objective function, coupled optimization variables, and discrete coefficient constraint, the formulated nonconvex problem is solved by proposing a two-stage optimization framework. For the outer stage problem, a quadratic transformation is used to recast the fractional objective into a linear form, and a closed-form solution is obtained by using auxiliary variables. For the inner stage problem, the system sum rate is approximated into a linear function. Then, an alternating optimization (AO) algorithm is proposed to optimize the BS precoding and RIS BF iteratively, by utilizing the penalty dual decomposition (PDD) method. Simulation results demonstrate the superiority of the proposed design compared to other benchmarks.}
}


@article{DBLP:journals/jsac/ZhangZYLH23,
	author = {Shengyu Zhang and
                  Shiyao Zhang and
                  Weijie Yuan and
                  Yonghui Li and
                  Lajos Hanzo},
	title = {Efficient Rate-Splitting Multiple Access for the Internet of Vehicles:
                  Federated Edge Learning and Latency Minimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1468--1483},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240716},
	doi = {10.1109/JSAC.2023.3240716},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangZYLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate-Splitting Multiple Access (RSMA) has recently found favour in the multi-antenna-aided wireless downlink, as a benefit of relaxing the accuracy of Channel State Information at the Transmitter (CSIT), while in achieving high spectral efficiency and providing security guarantees. These benefits are particularly important in high-velocity vehicular platoons since their high Doppler affects the estimation accuracy of the CSIT. To tackle this challenge, we propose an RSMA-based Internet of Vehicles (IoV) solution that jointly considers platoon control and FEderated Edge Learning (FEEL) in the downlink. Specifically, the proposed framework is designed for transmitting the unicast control messages within the IoV platoon, as well as for privacy-preserving FEEL-aided downlink Non-Orthogonal Unicasting and Multicasting (NOUM). Given this sophisticated framework, a multi-objective optimization problem is formulated to minimize both the latency of the FEEL downlink and the deviation of the vehicles within the platoon. To efficiently solve this problem, a Block Coordinate Descent (BCD) framework is developed for decoupling the main multi-objective problem into two sub-problems. Then, for solving these non-convex sub-problems, a Successive Convex Approximation (SCA) and Model Predictive Control (MPC) method is developed for solving the FEEL-based downlink problem and platoon control problem, respectively. Our simulation results show that the proposed RSMA-based IoV system outperforms both the popular Multi-User Linear Precoding (MU–LP) and the conventional Non-Orthogonal Multiple Access (NOMA) system. Finally, the BCD framework is shown to generate near-optimal solutions at reduced complexity.}
}


@article{DBLP:journals/jsac/YangCZH23,
	author = {Zhaohui Yang and
                  Mingzhe Chen and
                  Zhaoyang Zhang and
                  Chongwen Huang},
	title = {Energy Efficient Semantic Communication Over Wireless Networks With
                  Rate Splitting},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1484--1495},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240713},
	doi = {10.1109/JSAC.2023.3240713},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangCZH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the problem of wireless resource allocation and semantic information extraction for energy efficient semantic communications over wireless networks with rate splitting is investigated. In the considered model, a base station (BS) first extracts semantic information from its large-scale data, and then transmits the small-sized semantic information to each user which recovers the original data based on its local common knowledge. At the BS side, the probability graph is used to extract multi-level semantic information. In the downlink transmission, a rate splitting scheme is adopted, while the private small-sized semantic information is transmitted through private message and the common knowledge is transmitted through common message. Due to limited wireless resource, both computation energy and transmission energy are considered. This joint computation and communication problem is formulated as an optimization problem aiming to minimize the total communication and computation energy consumption of the network under computation, latency, and transmit power constraints. To solve this problem, an alternating algorithm is proposed where the closed-form solutions for semantic information extraction ratio and computation frequency are obtained at each step. Numerical results verify the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/jsac/GaoLY23,
	author = {Peng Gao and
                  Lixiang Lian and
                  Jinpei Yu},
	title = {Cooperative {ISAC} With Direct Localization and Rate-Splitting Multiple
                  Access Communication: {A} Pareto Optimization Framework},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1496--1515},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240714},
	doi = {10.1109/JSAC.2023.3240714},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GaoLY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated sensing and communication (ISAC) has been a promising technology in beyond 5G and 6G network to simultaneously support high-speed information transfer and high-quality environmental perception. Cloud radio access networks (C-RAN) enables the cooperation among multiple base stations (BSs) to provide additional cooperation gains for both communication and sensing functionalities. In this paper, we investigate the cooperative ISAC (CoISAC) system with rate-splitting multiple access (RSMA) transmission scheme for advanced interference management and direct localization sensing scheme to provide high-accuracy positioning services. Specifically, we adopt Pareto optimization framework to characterize the achievable performance region of the CoISAC system built on the sum rate of multiple communication users and the positioning error bound of radar target. Then, we formulate communication-centric and radar-centric optimization problems in the CoISAC system to systematically search for the optimal Pareto boundary of the achievable performance region. Two iterative algorithms based on successive convex approximation (SCA) are proposed to effectively solve these two optimization problems, respectively, therefore near-optimal Pareto boundary can be found. Numerical results show that the RSMA-assisted CoISAC system achieves the best trade-off performance among various baselines. The cooperative scheme can fully unveil the potential of RSMA in the ISAC system by providing more freedom for rate-splitting policy design under limited resources. Therefore, incorporating RSMA in the CoISAC system can significantly boost the overall performance.}
}


@article{DBLP:journals/jsac/HuangWS23,
	author = {Rui Huang and
                  Vincent W. S. Wong and
                  Robert Schober},
	title = {Rate-Splitting for Intelligent Reflecting Surface-Aided Multiuser
                  {VR} Streaming},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1516--1535},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240710},
	doi = {10.1109/JSAC.2023.3240710},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HuangWS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growing demand for virtual reality (VR) applications requires wireless systems to provide a high transmission rate to support 360-degree video streaming to multiple users simultaneously. In this paper, we propose an intelligent reflecting surface (IRS)-aided rate-splitting (RS) VR streaming system. In the proposed system, RS facilitates the exploitation of the shared interests of the users in VR streaming, and IRS creates additional propagation channels to support the transmission of high-resolution 360-degree videos. IRS also enhances the capability to mitigate the performance bottleneck caused by the requirement that all RS users have to be able to decode the common message. We formulate an optimization problem for maximization of the achievable bitrate of the 360-degree video subject to the quality-of-service (QoS) constraints of the users. We propose a deep deterministic policy gradient with imitation learning (Deep-GRAIL) algorithm, in which we leverage deep reinforcement learning (DRL) and the hidden convexity of the formulated problem to optimize the IRS phase shifts, RS parameters, beamforming vectors, and bitrate selection of the 360-degree video tiles. We also propose RavNet, which is a deep neural network customized for the policy learning in our Deep-GRAIL algorithm. Performance evaluation based on a real-world VR streaming dataset shows that the proposed IRS-aided RS VR streaming system outperforms several baseline schemes in terms of system sum-rate, achievable bitrate of the 360-degree videos, and online execution runtime. Our results also reveal the respective performance gains obtained from RS and IRS for improving the QoS in multiuser VR streaming systems.}
}


@article{DBLP:journals/jsac/HieuNHD23,
	author = {Nguyen Quang Hieu and
                  Diep N. Nguyen and
                  Dinh Thai Hoang and
                  Eryk Dutkiewicz},
	title = {When Virtual Reality Meets Rate Splitting Multiple Access: {A} Joint
                  Communication and Computation Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1536--1548},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240704},
	doi = {10.1109/JSAC.2023.3240704},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HieuNHD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Rate Splitting Multiple Access (RSMA) has emerged as an effective interference management scheme for applications that require high data rates. Although RSMA has shown advantages in rate enhancement and spectral efficiency, it has yet not to be ready for latency-sensitive applications such as virtual reality streaming, which is an essential building block of future 6G networks. Unlike conventional High-Definition streaming applications, streaming virtual reality applications requires not only stringent latency requirements but also the computation capability of the transmitter to quickly respond to dynamic users’ demands. Thus, conventional RSMA approaches usually fail to address the challenges caused by computational demands at the transmitter, let alone the dynamic nature of the virtual reality streaming applications. To overcome the aforementioned challenges, we first formulate the virtual reality streaming problem assisted by RSMA as a joint communication and computation optimization problem. A novel multicast approach is then proposed to cluster users into different groups based on a Field-of-View metric and transmit multicast streams in a hierarchical manner. After that, we propose a deep reinforcement learning approach to obtain the solution for the optimization problem. Extensive simulations show that our framework can achieve the millisecond-latency requirement, which is much lower than other baseline schemes.}
}


@article{DBLP:journals/jsac/ChenLYYKT23,
	author = {Pengxu Chen and
                  Hongwu Liu and
                  Yinghui Ye and
                  Liang Yang and
                  Kyeong Jin Kim and
                  Theodoros A. Tsiftsis},
	title = {Rate-Splitting Multiple Access Aided Mobile Edge Computing With Randomly
                  Deployed Users},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1549--1565},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240786},
	doi = {10.1109/JSAC.2023.3240786},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenLYYKT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a rate-splitting multiple access (RSMA) scheme is proposed to aid a mobile edge computing (MEC) system where multiple randomly deployed users offload their computation tasks to a MEC server. Considering that users are divided into the center and edge groups, a cognitive radio (CR)-inspired rate-splitting is designed to enable the paired users to simultaneously offload to MEC server. Under the CR principles, the rate-splitting parameters are jointly designed to attain the maximum achievable rate for the secondary user, meanwhile maintaining the primary user’s offloading performance same as in orthogonal multiple access. For the case of the paired users with fixed locations, we derive a closed-form expression for the successful computation probability (SCP) achieved by the RSMA-aided MEC (RSMA-MEC) scheme and formulate a SCP maximization problem to obtain the optimal offloading parameters. To reveal the impact of the user locations on the offloading performance of the RSMA-MEC system, various distance-based user pairing schemes are investigated by invoking stochastic geometry techniques. We provide closed-form expressions for the SCPs achieved by the user pairing schemes to characterize the offloading performance. Pros and cons of the user locations to maximize the SCP are highlighted. Simulation results verify the accuracy of the analytical results and clarify the superior offloading performance achieved by the RSMA-MEC scheme, which attains a higher SCP than the existing schemes.}
}


@article{DBLP:journals/jsac/ReifertDASASA23,
	author = {Robert{-}Jeron Reifert and
                  Hayssam Dahrouj and
                  Alaa Alameer Ahmad and
                  Aydin Sezgin and
                  Tareq Y. Al{-}Naffouri and
                  Basem Shihada and
                  Mohamed{-}Slim Alouini},
	title = {Rate-Splitting and Common Message Decoding in Hybrid Cloud/Mobile
                  Edge Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {5},
	pages = {1566--1583},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3240711},
	doi = {10.1109/JSAC.2023.3240711},
	timestamp = {Sat, 29 Apr 2023 19:26:27 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ReifertDASASA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes, and evaluates the benefits of, a hybrid central cloud (CC) and mobile edge computing (MEC) platform, especially introduced to balance the network resources for joint communication and computation. The transmission is further empowered by splitting the users’ messages into private and common parts, to mitigate the interference within the CC and MEC platforms. While several power-hungry, computationally-limited unmanned aerial vehicles (UAVs) are deployed at the cell-edge to boost the CC connectivity and relieve part of its computation burden, the CC connects to the base-stations via capacity-limited fronthauls. The paper then considers the problem of maximizing the weighted sum-rate subject to fronthaul and computation capacity, achievable rates, power, delay, and data-split constraints. Thereby determining the beamforming vectors associated with the private and common messages, the computation allocations, and the data-split factors. Such intricate non-convex optimization problem is tackled using an iterative algorithm that relies on well-chosen discrete relaxation, successive convex approximation, and fractional programming, and can be compellingly implemented in a distributed fashion. The simulations illustrate the proposed algorithm’s capabilities for empowering joint communication and computation, and highlight the pronounced role of rate-splitting and common message decoding in alleviating large-scale interference in hybrid CC/MEC networks.}
}


@article{DBLP:journals/jsac/ChenLLTSCL23,
	author = {Wanshi Chen and
                  Xingqin Lin and
                  Juho Lee and
                  Antti Toskala and
                  Shu Sun and
                  Carla{-}Fabiana Chiasserini and
                  Lingjia Liu},
	title = {Guest Editorial Special Issue on 3GPP Technologies: 5G-Advanced and
                  Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1587--1591},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3274048},
	doi = {10.1109/JSAC.2023.3274048},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenLLTSCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the start of 5G New Radio (NR) work in the 3rd Generation Partnership Project (3GPP) in early 2016, tremendous progress has been made in both standardization and commercial deployments. The first 5G NR release (Release 15) laid out a solid foundation in accommodating a diverse set of services, a wide range of spectra, and a variety of deployment scenarios, while being forward compatible. Expansion to vertical domain services [e.g., vehicle to everything (V2X), non-terrestrial networks (NTN)] was introduced in Release 16. Such an expansion was further accelerated in Release 17, with the standardization work being completed despite the extreme challenges due to COVID-19.}
}


@article{DBLP:journals/jsac/ChenLLTSCL23a,
	author = {Wanshi Chen and
                  Xingqin Lin and
                  Juho Lee and
                  Antti Toskala and
                  Shu Sun and
                  Carla{-}Fabiana Chiasserini and
                  Lingjia Liu},
	title = {5G-Advanced Toward 6G: Past, Present, and Future},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1592--1619},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3274037},
	doi = {10.1109/JSAC.2023.3274037},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenLLTSCL23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the start of 5G work in 3GPP in early 2016, tremendous progress has been made in both standardization and commercial deployments. 3GPP is now entering the second phase of 5G standardization, known as 5G-Advanced , built on the 5G baseline in 3GPP Releases 15, 16, and 17. 3GPP Release 18, the start of 5G-Advanced, includes a diverse set of features that cover both device and network evolutions, providing balanced mobile broadband evolution and further vertical domain expansion and accommodating both immediate and long-term commercial needs. 5G-Advanced will significantly expand 5G capabilities, address many new use cases, transform connectivity experiences, and serve as an essential step in developing mobile communications towards 6G. This paper provides a comprehensive overview of the 3GPP 5G-Advanced development, introducing the prominent state-of-the-art technologies investigated in 3GPP and identifying key evolution directions for future research and standardization.}
}


@article{DBLP:journals/jsac/CaoWZLLLJWWHYW23,
	author = {Yang Cao and
                  Pan Wang and
                  Kang Zheng and
                  Xianghu Liang and
                  Dongjie Liu and
                  Mengting Lou and
                  Jing Jin and
                  Qixing Wang and
                  Dongming Wang and
                  Yongming Huang and
                  Xiaohu You and
                  Jiangzhou Wang},
	title = {Experimental Performance Evaluation of Cell-Free Massive {MIMO} Systems
                  Using {COTS} {RRU} With {OTA} Reciprocity Calibration and Phase Synchronization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1620--1634},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3276057},
	doi = {10.1109/JSAC.2023.3276057},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaoWZLLLJWWHYW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Downlink coherent multiuser transmission is an essential technique for cell-free massive multiple-input multiple-output (MIMO) systems, and the availability of channel state information (CSI) at the transmitter is a basic requirement. To avoid CSI feedback in a time-division duplex system, the uplink channel parameters should be calibrated to obtain the downlink CSI due to the radio frequency circuit mismatch of the transceiver. In this paper, a design of a reference signal for over-the-air reciprocity calibration is proposed. The frequency domain generated reference signals can make full use of the flexible frame structure of the fifth-generation (5G) new radio, which can be completely transparent to commercial off-the-shelf (COTS) remote radio units (RRU) and commercial user equipments. To further obtain the calibration of multiple RRUs, an interleaved RRU grouping with a genetic algorithm is proposed, and an averaged Argos calibration algorithm is also presented. We develop a cell-free massive MIMO prototype system with COTS RRUs, demonstrate the statistical characteristics of the calibration error and the effectiveness of the calibration algorithm, and evaluate the impact of the calibration delay on the different cooperative transmission schemes.}
}


@article{DBLP:journals/jsac/JinLZZLFZOST23,
	author = {Huangping Jin and
                  Kunpeng Liu and
                  Min Zhang and
                  Leiming Zhang and
                  Gilwon Lee and
                  Emad N. Farag and
                  Dalin Zhu and
                  Eko N. Onggosanusi and
                  Mansoor Shafi and
                  Harsh Tataria},
	title = {Massive {MIMO} Evolution Toward 3GPP Release 18},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1635--1654},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273768},
	doi = {10.1109/JSAC.2023.3273768},
	timestamp = {Tue, 02 Apr 2024 16:26:26 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/JinLZZLFZOST23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the introduction of fifth-generation new radio (5G-NR) in Third Generation Partnership Project (3GPP) Release 15, swift progress has been made to evolve 5G with 3GPP Release 18 emerging. A critical aspect is the design of massive multiple-input multiple-output (MIMO) technology. In this line, this paper makes several important contributions: We provide a comprehensive overview of the evolution of standardized massive MIMO features from 3GPP Release 15 to 17 for both time/frequency-division duplex operation across frequency range in (FR)-1 and FR 2-1. We analyze the progress on channel state information (CSI) frameworks, beam management frameworks and present enhancements for uplink CSI. We shed light on emerging 3GPP Release 18 problems requiring imminent attention. These include advanced codebook design and sounding reference signal design for coherent joint transmission (CJT) with multiple transmission/reception points (multi TRPs). We discuss advancements in uplink demodulation reference signal design, enhancements for mobility to provide accurate CSI estimates, and unified transmission configuration indicator framework tailored for FR 2–1 bands. For each concept, we provide system level simulation results to highlight their performance benefits. Via field trials in an outdoor environment at Shanghai Jiaotong University, we demonstrate the gains of multi-TRP CJT relative to single TRP at 3.7 GHz.}
}


@article{DBLP:journals/jsac/TedeschiniNW23,
	author = {Bernardo Camajori Tedeschini and
                  Monica Nicoli and
                  Moe Z. Win},
	title = {On the Latent Space of mmWave {MIMO} Channels for {NLOS} Identification
                  in 5G-Advanced Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1655--1669},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273769},
	doi = {10.1109/JSAC.2023.3273769},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TedeschiniNW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In mission-critical verticals such as automated driving, 5G-advanced networks must provide centimeter-level dynamic positioning along with ultra-reliable low-latency communication services. Massive Multiple-Input Multiple-Output (mMIMO) and millimeter waves (mmWave) are the key enablers, allowing high accuracy angle and delay estimation. Still, extracting such information from highly-dimensional Channel Impulse Responses (CIRs) results in a complex task, due to channel sparsity and intermittent blockage. In this paper we focus on non-line-of-sight (NLOS) identification from CIR data, proposing a Deep Autoencoding Kernel Density Model (DAKDM) to characterize the statistics of the channel latent features. We formulate the problem as a semi-supervised anomaly detection task in which only LOS samples, i.e., normal data, are adopted for training. DAKDM is a single-stage training model that takes as input the full CIR thanks to an AutoEncoder (AE) structure. The proposed method is able to learn the latent distribution by means of a Kernel Density Estimator (KDE) in combination with a deep learning likelihood network. We validate the proposed solution in a 5G Urban micro (UMi) vehicular scenario. Results show that the proposed model can significantly outperform conventional algorithms and obtain similar performances to variational Bayes algorithms at one tenth of the inference time.}
}


@article{DBLP:journals/jsac/GaoWLG23,
	author = {Kaixuan Gao and
                  Huiqiang Wang and
                  Hongwu Lv and
                  Pengfei Gao},
	title = {A DL-Based High-Precision Positioning Method in Challenging Urban
                  Scenarios for {B5G} CCUAVs},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1670--1687},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3275621},
	doi = {10.1109/JSAC.2023.3275621},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GaoWLG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) facilitate services in civilian and industrial fields but suffer from a limited direct link operating range and unreliable satellite positioning in urban canyons. Fortunately, cellular-connected UAVs (CCUAVs) overcome these shortcomings, benefitting from the beyond 5th generation (B5G) network’s city-level coverage and high-precision positioning capabilities , and are considered a paradigm of 5G-advanced and beyond. However, in a challenging airspace (e.g., urban canyon), the CCUAV localization accuracy deteriorates due to low signal-to-interference-plus-noise (SINR) air-ground channels and strong multipath effects . To solve these problems, we first construct channel amplitude-phase response (CAPR) images to characterize the cellular channel in a challenging airspace for CCUAV positioning. In particular, the effect of down-tilted antennas and high-dimensional channel features are embedded into CAPR images, to meet the relevant cellular communication criteria. Subsequently, a deep learning (DL) model, the scale-shared quarter network (SSQ-Net), is devised for CAPR image-based positioning, along with a robustness enhancement method. With this method, the multipath effects and interference in challenging environments are exploited to improve positioning accuracy and robustness, instead of being treated as detriments. Finally, the experimental results in a typical urban canyon show that our method outperforms state-of-the-art methods in terms of accuracy and robustness.}
}


@article{DBLP:journals/jsac/TorsoliWC23,
	author = {Gianluca Torsoli and
                  Moe Z. Win and
                  Andrea Conti},
	title = {Blockage Intelligence in Complex Environments for Beyond 5G Localization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1688--1701},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3275612},
	doi = {10.1109/JSAC.2023.3275612},
	timestamp = {Thu, 27 Jul 2023 08:18:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TorsoliWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Location awareness is vital for several applications in wireless ecosystems, including fifth generation (5G) and beyond networks defined by the 3rd Generation Partnership Project (3GPP). However, complex wireless environments such as indoor factories are characterized by harsh multipath propagation and non-line-of-sight (NLOS) conditions, which are detrimental to localization accuracy. This paper introduces the concept of blockage intelligence (BI) to provide a probabilistic description of wireless propagation conditions. Then, it discusses its integration in both conventional and soft information (SI)-based localization algorithms. Case studies are presented in the 3GPP indoor factory scenario with various gNodeBs (gNBs) deployments. Results show that BI together with SI-based localization significantly outperforms existing localization techniques. The rich information provided by BI is vital to perform accurate localization in 5G and beyond networks operating in complex wireless environments.}
}


@article{DBLP:journals/jsac/YingGCZZCOP23,
	author = {Keke Ying and
                  Zhen Gao and
                  Sheng Chen and
                  Mingyu Zhou and
                  Dezhi Zheng and
                  Symeon Chatzinotas and
                  Bj{\"{o}}rn E. Ottersten and
                  H. Vincent Poor},
	title = {Quasi-Synchronous Random Access for Massive MIMO-Based {LEO} Satellite
                  Constellations},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1702--1722},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273699},
	doi = {10.1109/JSAC.2023.3273699},
	timestamp = {Thu, 13 Jul 2023 08:26:09 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YingGCZZCOP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low earth orbit (LEO) satellite constellation-enabled communication networks are expected to be an important part of many Internet of Things (IoT) deployments due to their unique advantage of providing seamless global coverage. In this paper, we investigate the random access problem in massive multiple-input multiple-output-based LEO satellite systems, where the multi-satellite cooperative processing mechanism is considered. Specifically, at edge satellite nodes, we conceive a training sequence padded multi-carrier system to overcome the issue of imperfect synchronization, where the training sequence is utilized to detect the devices’ activity and estimate their channels. Considering the inherent sparsity of terrestrial-satellite links and the sporadic traffic feature of IoT terminals, we utilize the orthogonal approximate message passing-multiple measurement vector algorithm to estimate the delay coefficients and user terminal activity. To further utilize the structure of the receive array, a two-dimensional estimation of signal parameters via rotational invariance technique is performed for enhancing channel estimation. Finally, at the central server node, we propose a majority voting scheme to enhance activity detection by aggregating backhaul information from multiple satellites. Moreover, multi-satellite cooperative linear data detection and multi-satellite cooperative Bayesian dequantization data detection are proposed to cope with perfect and quantized backhaul, respectively. Simulation results verify the effectiveness of our proposed schemes in terms of channel estimation, activity detection, and data detection for quasi-synchronous random access in satellite systems.}
}


@article{DBLP:journals/jsac/KongLHZDA23,
	author = {Huaicong Kong and
                  Min Lin and
                  Lve Han and
                  Wei{-}Ping Zhu and
                  Zhiguo Ding and
                  Mohamed{-}Slim Alouini},
	title = {Uplink Multiple Access With Semi-Grant-Free Transmission in Integrated
                  Satellite-Aerial-Terrestrial Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1723--1736},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273707},
	doi = {10.1109/JSAC.2023.3273707},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KongLHZDA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates a semi-grant-free (SGF) based transmission strategy to provide a flexible connectivity for various kinds of users in an integrated satellite-aerial-terrestrial network (ISATN). Herein, a high-altitude platform (HAP) termed as a grant-based user (GBU), which serves multiple mobile terminals (MTs) through space division multiple access (SDMA), wants to access a satellite network with multiple earth stations (ESs) termed as grant-free users (GFUs) simultaneously via non-orthogonal multiple access (NOMA) assisted SGF. To this end, we first propose two SGF-based uplink transmission schemes for both perfect channel state information (CSI) and imperfect CSI cases. When perfect CSI is available, a zero-forcing based beamforming (BF) scheme is used in HAP network while an adaptive transmit power allocation (ATPA) approach is adopted for SGF transmission. When only imperfect CSI is available, BF scheme employing the derived channel correlation matrix of HAP-MT link is proposed to achieve SDMA, and a novel ATPA strategy with rate probability constraint is proposed to guarantee quality-of-service of the GBU. Next, we derive the closed-form throughput expressions to evaluate the performance of the considered ISATN with the proposed two SGF-based schemes. Finally, computer simulations are conducted to validate the theoretical performance analysis and show the superiority of the proposed schemes over the related works. Moreover, our numerical results not only demonstrate a satisfactory performance of the proposed SGF-based scheme using imperfect CSI, but also reveal the impact of CSI errors on the system performance.}
}


@article{DBLP:journals/jsac/YangLLQ23,
	author = {Huiting Yang and
                  Wei Liu and
                  Jiandong Li and
                  Tony Q. S. Quek},
	title = {Space Information Network With Joint Virtual Network Function Deployment
                  and Flow Routing Strategy With QoS Constraints},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1737--1756},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273704},
	doi = {10.1109/JSAC.2023.3273704},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangLLQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space information network (SIN) can provide global coverage in 6G network. Furthermore, SIN with network function virtualization (NFV) can achieve flexible deployment of network functions and improve the utilization of resources. In SIN with NFV, network functions can be virtualized into virtual network functions (VNFs). However, in SIN with NFV, the mission flow must satisfy the service function chain (SFC) constraint, i.e., the mission flow must be processed by all VNFs in the predefined order. Furthermore, each VNF can be deployed on multiple physical nodes. Moreover, different kinds of services may have the diverse quality of service (QoS) requirements. In this paper, we investigate the joint VNFs deployment and flow routing strategy (VNF-R) to maximize the number of completed missions with the guaranteed end-to-end latency under SFC constraints in time-varying SINs. Specifically, the problem can be formulated as a mixed integer linear programming (MILP) problem, which is proved to be NP-hard. In order to effectively solve the problem, we propose a novel low-complexity near-optimal penalty successive upper bound minimization rounding LP relaxation iterative rounding (PSUM-R-LRIR) algorithm. The simulation results show that the PSUM-R-LRIR algorithm can achieve near-optimal performance, and our proposed VNF-R scheme significantly outperforms the fixed VNF deployment scheme.}
}


@article{DBLP:journals/jsac/HandeTDXMLS23,
	author = {Prashanth Hande and
                  Peerapol Tinnakornsrisuphap and
                  Jelena Damnjanovic and
                  Huilin Xu and
                  Mickael Mondet and
                  Hyun Yong Lee and
                  Iyab I. Sakhnini},
	title = {Extended Reality Over 5G - Standards Evolution},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1757--1771},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3276056},
	doi = {10.1109/JSAC.2023.3276056},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HandeTDXMLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High performance XR systems can offload intensive computation tasks such as graphic rendering to the edge server - enabling a richer experience while reducing the power consumption on the XR end-user device. This paper provides an overview of the requirements for such split-compute XR systems over 5G and the resulting traffic characteristics. The design of 5G system, from the very first release, provides support for such emerging use cases with low power requirement and traffic requirements of low latency, high bandwidth, and high reliability. As split XR systems scale in terms of the number of users, evolve to support more immersive experience with consumer-friendly device form-factors, and become a key enabler for metaverse experiences, 5G system will continue to evolve to meet the more demanding requirements as part of “5G-Advanced” phase of 5G standards development, starting with 3GPP Release 18. This paper provides an overview of the advances in 5G with 3GPP Release 18 across RAN and SA2 working groups to address the emerging XR requirements.}
}


@article{DBLP:journals/jsac/BojovicLKZWY23,
	author = {Biljana Bojovic and
                  Sandra Lag{\'{e}}n and
                  Katerina Koutlia and
                  Xiaodi Zhang and
                  Ping Wang and
                  Liwen Yu},
	title = {Enhancing 5G QoS Management for {XR} Traffic Through {XR} Loopback
                  Mechanism},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1772--1786},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273701},
	doi = {10.1109/JSAC.2023.3273701},
	timestamp = {Fri, 01 Sep 2023 14:19:52 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BojovicLKZWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G networks are designed to support a variety of services with highly demanding Quality-of-Service (QoS) requirements. This opened the door for novel extended reality (XR) media applications to emerge with 5G. However, recent 5G field tests and system-level simulation studies show that further XR enhancements are required to support a massive adoption of XR services in 5G networks. Such enhancements are expected to come into play with 5G-Advanced. In this line, we propose and study an XR loopback mechanism that adapts the XR traffic to the instantaneous 5G network conditions by exploiting an XR application feedback. We propose various XR loopback algorithms, strategies, and parameters’ configurations and study their impact on the 5G end-to-end performance. We conduct extensive simulation campaigns by building realistic end-to-end 5G network scenarios with 3GPP mixed XR traffic setups. Results show that the proposed XR loopback mechanism can boost XR performance in 5G networks by adapting to 5G network conditions, while keeping the XR QoS requirements under control. We provide various insights and practical directions on XR loopback design that allow us to take full advantage of the 5G network capabilities and progress toward 5G-Advanced network design.}
}


@article{DBLP:journals/jsac/LiSKYLSMSBPJ23,
	author = {Qiaoyu Li and
                  Philip Sisk and
                  Arumugam Kannan and
                  Taesang Yoo and
                  Tao Luo and
                  Gaurav Shah and
                  Badri Manjunath and
                  Chanaka Samarathungage and
                  Mahmoud Taherzadeh Boroujeni and
                  Hamed Pezeshki and
                  Himanshu Joshi},
	title = {Machine Learning Based Time Domain Millimeter-Wave Beam Prediction
                  for 5G-Advanced and Beyond: Design, Analysis, and Over-The-Air Experiments},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1787--1809},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3275613},
	doi = {10.1109/JSAC.2023.3275613},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiSKYLSMSBPJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Artificial intelligence (AI) or machine learning (ML) based beam prediction is currently studied in the 3rd Generation Partnership Project (3GPP) fifth generation (5G)-Advanced new ratio (NR) standardization for future commercialization and standard evolution towards sixth generation (6G) communications, wherein time domain (TD) beam prediction is an important use case. The targets for such 3GPP studies and standardization are to lower power consumed at user equipment (UE) and reference signal (RS) overhead that are currently needed by frequent beam measurements due to UE rotation and mobility. To meet such targets, in this paper, we investigate AI/ML based algorithms facilitating TD beam prediction suitable for 5G-Advanced beam management (BM), including RS receive power (RSRP) prediction and beam change prediction. The proposed AI/ML algorithms are first evaluated through computer simulations with new UE mobility models based on recent standard evolutions in 3GPP. Then we further present over-the-air test results achieved by such AI/ML algorithms, using based station (BS) and UE compliant with 3GPP standards. Evaluation results show that the proposed schemes can accurately predict future beams and reduce large amount of the power consumed at the UE for BM, which also demonstrate feasibility of AI/ML based BM for 5G-Advanced and future 6G communications.}
}


@article{DBLP:journals/jsac/MinKL23,
	author = {Kyungsik Min and
                  Yunjoo Kim and
                  Hyun{-}Suk Lee},
	title = {Meta-Scheduling Framework With Cooperative Learning Toward Beyond
                  5G},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1810--1824},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273698},
	doi = {10.1109/JSAC.2023.3273698},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MinKL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a novel meta-scheduling framework with cooperative learning that fully exploits a functional split structure of the base station (BS) consisting of a central unit (CU) and digital units (DUs). To this end, we first design a meta-scheduling policy structure to find a scheduling strategy that can be applied to any BS regardless of BS-specific characteristics such as the number of users, various quality-of-service requirements, and the number of resource blocks. In the proposed framework, the CU only needs to manage objective-specific meta-scheduling policies in a centralized manner while each DU performs scheduling in a decentralized manner by simply using the policy at the CU without any responsibility to manage or train policies. Besides, the policies at the CU can be cooperatively trained using the experiences obtained from all DUs. Therefore, the proposed framework not only provides computational efficiency but also supports network scalability. We show via experiments that the proposed meta-scheduling framework achieves competitive performances compared with the near-optimal conventional schedulers tailored to each BS even though it manages and exploits only one meta-scheduling policy for each objective. Furthermore, our meta-scheduling framework effectively adapts to the change of BS-specific characteristics.}
}


@article{DBLP:journals/jsac/PervejJD23,
	author = {Md. Ferdous Pervej and
                  Richeng Jin and
                  Huaiyu Dai},
	title = {Resource Constrained Vehicular Edge Federated Learning With Highly
                  Mobile Connected Vehicles},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1825--1844},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273700},
	doi = {10.1109/JSAC.2023.3273700},
	timestamp = {Thu, 13 Jul 2023 08:26:09 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PervejJD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a vehicular edge federated learning (VEFL) solution, where an edge server leverages highly mobile connected vehicles’ (CVs’) onboard central processing units (CPUs) and local datasets to train a global model. Convergence analysis reveals that the VEFL training loss depends on the successful receptions of the CVs’ trained models over the intermittent vehicle-to-infrastructure (V2I) wireless links. Owing to high mobility, in the full device participation case (FDPC), the edge server aggregates client model parameters based on a weighted combination according to the CVs’ dataset sizes and sojourn periods, while it selects a subset of CVs in the partial device participation case (PDPC). We then devise joint VEFL and radio access technology (RAT) parameters optimization problems under delay, energy and cost constraints to maximize the probability of successful reception of the locally trained models. Considering that the optimization problem is NP-hard, we decompose it into a VEFL parameter optimization sub-problem, given the estimated worst-case sojourn period, delay and energy expense, and an online RAT parameter optimization sub-problem. Finally, extensive simulations are conducted to validate the effectiveness of the proposed solutions with a practical 5G new radio (5G-NR) RAT under a realistic microscopic mobility model.}
}


@article{DBLP:journals/jsac/PradoSMKK23,
	author = {Anna Prado and
                  Franziska St{\"{o}}ckeler and
                  Fidan Mehmeti and
                  Patrick Kr{\"{a}}mer and
                  Wolfgang Kellerer},
	title = {Enabling Proportionally-Fair Mobility Management With Reinforcement
                  Learning in 5G Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1845--1858},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273705},
	doi = {10.1109/JSAC.2023.3273705},
	timestamp = {Thu, 13 Jul 2023 08:26:09 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PradoSMKK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobility management in 5G is challenging, and at higher frequencies, a larger number of cells is needed to provide similar coverage to that in 4G. Consequently, Base Stations (BSs) are placed much more densely and users experience frequent handovers, reducing network capacity. Advanced handover techniques are needed in 5G to perform smooth network operation. In this paper, we formulate an optimization problem, whose goal is to strive for fairness in data rates among users and to reduce handovers. To accomplish that, we consider jointly the decisions when to handover and to which BS a user is to be assigned. This is an integer nonlinear program, and by relaxing it, we obtain an upper bound. Further, due to its NP-hardness, we propose a centralized and a multi-agent Deep Q Network (DQN)-based algorithm, which both find near-optimal user-to-BS assignments. We evaluate our Reinforcement Learning-based solutions for networks of different sizes and users with different velocities. We compare our approaches with baselines and show that they outperform them considerably in terms of fairness and radio link failures while being within 95% of the optimum. Our DQN algorithms also reduce the handover rate by up to 93% and avoid ping-pong handovers almost completely.}
}


@article{DBLP:journals/jsac/AmiriYYHZPZ23,
	author = {Roohollah Amiri and
                  Srinivas Yerramalli and
                  Taesang Yoo and
                  Mohammed Hirzallah and
                  Marwen Zorgui and
                  Rajat Prakash and
                  Xiaoxia Zhang},
	title = {Indoor Environment Learning via RF-Mapping},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1859--1872},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273702},
	doi = {10.1109/JSAC.2023.3273702},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AmiriYYHZPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent integrated sensing and communication is one of key aspects of future wireless networks in which sensing can be leveraged to enhance communications and vice-versa. In this paper, we propose a novel sensing solution that can be used to represent an RF-environment. The proposed solution accounts for practical challenges such as limited time resolution due to limited bandwidth with no angle measurements while providing robustness to wireless propagation phenomena such as diffraction. Our proposed method leverages offline data collection during RF-mapping, and finds the location of virtual anchors (VAs), i.e., mirror images of a physical anchor w.r.t reflectors, through an iterative process called successive tap removal (STR). Afterwards, machine learning (ML) models are trained to predict dominant multipath components of the received wireless channel at a given location. Found VAs and their associated ML models stand for intermediate entities that represent an RF-environment. As an application, we use the developed models in the context of multipath assisted positioning to improve positioning accuracy in challenging indoor environments with heavy non-line-of-sight (NLoS) conditions. Finally, we extend our ideas to systems with multi-antenna transmitters and show that VA detection accuracy can be improved, bringing higher accuracy to the downstream positioning applications.}
}


@article{DBLP:journals/jsac/GuoL23,
	author = {Huayan Guo and
                  Vincent K. N. Lau},
	title = {Robust Deep Learning for Uplink Channel Estimation in Cellular Network
                  Under Inter-Cell Interference},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1873--1887},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3276765},
	doi = {10.1109/JSAC.2023.3276765},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuoL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep learning (DL)-based channel estimation has achieved remarkable success. However, most existing works focus on the white Gaussian noise which are inapplicable for cell-edge users under inter-cell interference (ICI). In this paper, we address this issue by proposing a novel DL-based channel estimation solution with a cascaded model-based and model-free deep neural network (DNN) structure. Specifically, the model-based module is designed by the variational Bayesian inference (VBI) technique to suppress the time-varying ICI, and the model-free module is designed by the Denoising Sparse Autoencoder (DSAE) structure to further refine the channel estimation. The proposed DNN is firstly pre-trained by offline supervised training, and various channel statistics are encapsulated in the DNN weights with the assist of a hyper-prior net modelling different sparse priors for different training samples. Then, an online Bayesian learning algorithm is proposed to train the model-based VBI module based on real-time pilot samples to track the online channel statistics. Simulation results verify that the proposed solution outperforms various state-of-the-art baseline schemes in a large SINR range with comparable performance to the estimator with genie-aided channel statistics.}
}


@article{DBLP:journals/jsac/IslamLL23,
	author = {Toufiqul Islam and
                  Daewon Lee and
                  Seau Sian Lim},
	title = {Enabling Network Power Savings in 5G-Advanced and Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1888--1899},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273706},
	doi = {10.1109/JSAC.2023.3273706},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/IslamLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Energy consumption is a critical concern for the 5G mobile operators. 5G NR network may have increased energy consumption compared to its predecessors as it aims to support a plethora of services demanding diverse QoS constraints and requiring deployment of radios at different frequency layers with potentially wider bandwidth and more antennas. Moreover, it is important for the operators to keep the operational expenses and carbon emissions as low as possible as well. At 3GPP, UE power saving has mainly been the focus from Rel 15 till Rel 17 and in Rel 18, a new study item has been introduced to investigate different network power saving techniques. In this paper, we first discuss why 5G NR poses unique challenges for energy consumption with respect to its predecessors. We aim to explore different potential techniques for network power savings via resource adaptation in time, frequency, spatial, and power domain. We also investigate power consumption based on energy consumption model adopted by 3GPP and provide energy efficiency - throughput trade-off analysis to identify which domain has the highest potential for power savings.}
}


@article{DBLP:journals/jsac/IbrahimACE23,
	author = {Mostafa Ibrahim and
                  Huseyin Arslan and
                  Hakan Ali {\c{C}}irpan and
                  Sabit Ekin},
	title = {Time-Frequency Warped Waveforms for Well-Contained Massive Machine
                  Type Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1900--1917},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3275622},
	doi = {10.1109/JSAC.2023.3275622},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/IbrahimACE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a novel time-frequency warped waveform for short symbols, massive machine-type communication (mMTC), and internet of things (IoT) applications. The waveform is composed of asymmetric raised cosine (RC) pulses to increase the signal containment in time and frequency domains. The waveform has low power tails in the time domain, hence better performance in the presence of delay spread and time offsets. The time-axis warping unitary transform is applied to control the waveform occupancy in time-frequency space and to compensate for the usage of high roll-off factor pulses at the symbol edges. The paper explains a step-by-step analysis for determining the roll-off factors profile and the warping functions. Gains are presented over the conventional Zero-tail Discrete Fourier Transform-spread-Orthogonal Frequency Division Multiplexing (ZT-DFT-s-OFDM), and Cyclic prefix (CP) DFT-s-OFDM schemes in the simulations section.}
}


@article{DBLP:journals/jsac/WuD23,
	author = {Zidong Wu and
                  Linglong Dai},
	title = {Multiple Access for Near-Field Communications: {SDMA} or LDMA?},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1918--1935},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3275616},
	doi = {10.1109/JSAC.2023.3275616},
	timestamp = {Thu, 13 Jul 2023 08:26:09 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WuD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spatial division multiple access (SDMA) is essential to improve the spectrum efficiency for multi-user multiple-input multiple-output (MIMO) communications. The classical SDMA for massive MIMO with hybrid precoding heavily relies on the angular orthogonality in the far field to distinguish multiple users at different angles, which fails to fully exploit spatial resources in the distance domain. With the dramatically increasing number of antennas, the extremely large-scale antenna array (ELAA) introduces additional resolution in the distance domain in the near field. In this paper, we propose the concept of location division multiple access (LDMA) to provide a new possibility to enhance spectrum efficiency compared with classical SDMA. The key idea is to exploit extra spatial resources in the distance domain to serve different users at different locations (determined by angles and distances) in the near field. Specifically, the asymptotic orthogonality of near-field beam focusing vectors in the distance domain is proved, which reveals that near-field beam focusing is able to focus signals on specific locations with limited leakage energy at other locations. This special property could be leveraged in hybrid precoding to mitigate inter-user interferences for spectrum efficiency enhancement. Moreover, we provide the spherical-domain codebook design method for LDMA communications with the uniform planar array, which provides the sampling method in the distance domain. Additionally, performance analysis of LDMA is provided to reveal that the asymptotic optimal spectrum efficiency could be achieved with the increasing number of antennas. Finally, simulation results verify the superiority of the proposed LDMA over SDMA in different scenarios.}
}


@article{DBLP:journals/jsac/HooliKTHKKBH23,
	author = {Kari Hooli and
                  Pasi Kinnunen and
                  Esa Tiirola and
                  Sami Hakola and
                  Zsombor Kiss and
                  Jorma Kaikkonen and
                  Matthew Baker and
                  Ilkka Harjula},
	title = {Extending 5G to Narrow Spectrum Allocations},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1936--1944},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3273703},
	doi = {10.1109/JSAC.2023.3273703},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HooliKTHKKBH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G New Radio (NR) has been a great success story since its introduction. It offers high reliability, low communication latency, flexibility, and efficiency that further enable 5G networks to be used for mission critical applications. Currently, there are specialized networks serving industry verticals that would benefit from the NR properties. Key use cases include communications for critical infrastructure such as railways, public safety, and smart energy and infrastructure. These networks often use dedicated spectrum allocations that are narrower than the original minimum operating bandwidth of NR. In this paper, we describe how new flexibility is being introduced into the NR design to allow operation with bandwidths between about 3 and 5 MHz. As a use case study, we focus on railway communications, and study the co-existence of the Global System for Mobile Communications–Railway (GSM-R) and Future Railway Mobile Communication System (FRMCS) based on NR during the decade-long migration period. We discuss the changes required for the NR control channels and signaling and study the effect of the changes on the system performance.}
}


@article{DBLP:journals/jsac/MiaoZTTZGL23,
	author = {Haiyang Miao and
                  Jianhua Zhang and
                  Pan Tang and
                  Lei Tian and
                  Xinyu Zhao and
                  Bolun Guo and
                  Guangyi Liu},
	title = {Sub-6 GHz to mmWave for 5G-Advanced and Beyond: Channel Measurements,
                  Characteristics and Impact on System Performance},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {6},
	pages = {1945--1960},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3274175},
	doi = {10.1109/JSAC.2023.3274175},
	timestamp = {Fri, 07 Jul 2023 23:31:55 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MiaoZTTZGL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the 5G-Advanced and beyond systems, multi-frequency cooperative networking will become an inevitable development trend. However, the channels have not been fully investigated at multi-frequency bands and in multi-scenarios by using the same channel sounder, especially for the sub-6 GHz to millimeter-wave (mmWave) bands. In this paper, we carry out channel measurements at four frequency bands (3.3, 6.5, 15, and 28 GHz) in two scenarios including Urban Micro (UMi) and Outdoor-to-Indoor (O2I) with the same channel sounder. The channel characteristics are extracted and modeled, including path loss (PL), shadow fading, frequency dependence of cluster features, root mean square (RMS) delay spread (DS), Ricean K-factor, and the correlation properties. We mainly focus on the analysis of large-scale parameters and more consideration of the link budget coverage problem. We present the frequency dependence model of the channel characteristics. Among them, in the non-line-of-sight (NLoS) condition, it is found that except for theoretical value brought by higher frequency, additional path loss increment will be generated. Based on these channel characteristics, the impact on performance of the wireless system is analyzed including cell coverage radius, data rate and bit error rate (BER). The results can give an insight into the spectrum selection and optimization in 5G-Advanced and beyond multi-frequency communication systems.}
}


@article{DBLP:journals/jsac/ShePDQSSZ23,
	author = {Changyang She and
                  Cunhua Pan and
                  Trung Q. Duong and
                  Tony Q. S. Quek and
                  Robert Schober and
                  Meryem Simsek and
                  Peiying Zhu},
	title = {Guest Editorial xURLLC in 6G: Next Generation Ultra-Reliable and Low-Latency
                  Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {1963--1968},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3282543},
	doi = {10.1109/JSAC.2023.3282543},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShePDQSSZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {AS ONE of the new communication scenarios in 5th-generation (5G) mobile communication systems, ultra-reliable and low-latency communications (URLLC) have stringent requirements on latency (around 1 ms) and reliability (up to 99.99999%). Nevertheless, existing 5G URLLC alone cannot fulfill all the Key Performance Indicators (KPIs) in emerging mission-critical applications like industrial automation, intelligent transportation, telemedicine, Tactile Internet, and Virtual/Augmented Reality (VR/AR). The 6th generation (6G) communication systems need to meet additional requirements on some of the following KPIs in combination with URLLC: high spectrum efficiency (SE)/throughput/energy efficiency (EE)/network availability/security as well as low Age of Information (AoI)/jitter/round-trip delay. These new requirements pose unprecedented challenges in terms of design methodologies and enabling technologies in 6G. To fill the gap between 5G URLLC and the diverse KPI requirements of the neXt generation URLLC (xURLLC), novel methodologies and innovative technologies are much needed.}
}


@article{DBLP:journals/jsac/KislalLDS23,
	author = {A. Oguz Kislal and
                  Alejandro Lancho and
                  Giuseppe Durisi and
                  Erik G. Str{\"{o}}m},
	title = {Efficient Evaluation of the Error Probability for Pilot-Assisted {URLLC}
                  With Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {1969--1981},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280972},
	doi = {10.1109/JSAC.2023.3280972},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KislalLDS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a numerically efficient method for evaluating the random-coding union bound with parameter\ns\non the error probability achievable in the finite-blocklength regime by a pilot-assisted transmission scheme employing Gaussian codebooks and operating over a memoryless block-fading channel. Our method relies on the saddlepoint approximation, which, differently from previous results reported for similar scenarios, is performed with respect to the number of fading blocks (a.k.a. diversity branches) spanned by each codeword, instead of the number of channel uses per block. This different approach avoids a costly numerical averaging of the error probability over the realizations of the fading process and of its pilot-based estimate at the receiver and results in a significant reduction of the number of channel realizations required to estimate the error probability accurately. Our numerical experiments for both single-antenna communication links and massive multiple-input multiple-output (MIMO) networks show that, when two or more diversity branches are available, the error probability can be estimated accurately with the saddlepoint approximation with respect to the number of fading blocks using a numerical method that requires about two orders of magnitude fewer Monte-Carlo samples than with the saddlepoint approximation with respect to the number of channel uses per block.}
}


@article{DBLP:journals/jsac/TuninettiSSD23,
	author = {Daniela Tuninetti and
                  Paul Sheldon and
                  Besma Smida and
                  Natasha Devroye},
	title = {On Second Order Rate Regions for the Static Scalar Gaussian Broadcast
                  Channel},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {1982--1999},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280991},
	doi = {10.1109/JSAC.2023.3280991},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TuninettiSSD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers the single antenna, static, scalar Gaussian broadcast channel in the finite blocklength regime. Second order achievable and converse rate regions are presented. Both a global reliability and per-user reliability requirements are considered. The two-user case is analyzed in detail, and generalizations to the\nK\n-user case are also discussed. The largest second order achievable regions presented here require both superposition and rate splitting in the code construction, as opposed to the (infinite blocklength, first order) capacity region which does not require rate splitting. Indeed, the finite blocklength penalty causes superposition alone to under-perform other coding techniques in some parts of the region. In addition, the proposed scheme uses joint simultaneous decoding as opposed to successive interference cancellation. Interestingly, in the two-user case with per-user reliability requirements, the capacity achieving superposition encoding order (with the codeword intended for the user with the smallest received SNR as cloud center) does not necessarily give the largest second order region. Instead, the message of the user with the smallest point-to-point second order capacity should be encoded in the cloud center in order to obtain the largest second order region for the proposed scheme.}
}


@article{DBLP:journals/jsac/WangNCN23,
	author = {Kunlun Wang and
                  Dusit Niyato and
                  Wen Chen and
                  Arumugam Nallanathan},
	title = {Task-Oriented Delay-Aware Multi-Tier Computing in Cell-Free Massive
                  {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2000--2012},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280965},
	doi = {10.1109/JSAC.2023.3280965},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/WangNCN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-tier computing can enhance the task computation by multi-tier computing nodes. In this paper, we propose a cell-free massive multiple-input multiple-output (MIMO) aided computing system by deploying multi-tier computing nodes to improve the computation performance. At first, we investigate the computational latency and the total energy consumption for task computation, regarded as total cost. Then, we formulate a total cost minimization problem to design the bandwidth allocation and task allocation, while considering realistic heterogenous delay requirements of the computational tasks. Due to the binary task allocation variable, the formulated optimization problem is non-convex. Therefore, we solve the bandwidth allocation and task allocation problem by decoupling the original optimization problem into bandwidth allocation and task allocation subproblems. As the bandwidth allocation problem is a convex optimization problem, we first determine the bandwidth allocation for given task allocation strategy, followed by conceiving the traditional convex optimization strategy to obtain the bandwidth allocation solution. Based on the asymptotic property of received signal-to-interference-plus-noise ratio (SINR) under the cell-free massive MIMO setting and bandwidth allocation solution, we formulate a dual problem to solve the task allocation subproblem by relaxing the binary constraint with Lagrange partial relaxation for heterogenous task delay requirements. At last, simulation results are provided to demonstrate that our proposed task offloading scheme performs better than the benchmark schemes, where the minimum-cost optimal offloading strategy for heterogeneous delay requirements of the computational tasks may be controlled by the asymptotic property of the received SINR in our proposed cell-free massive MIMO-aided multi-tier computing systems.}
}


@article{DBLP:journals/jsac/HaoHC23,
	author = {Yixue Hao and
                  Long Hu and
                  Min Chen},
	title = {Joint Sensing Adaptation and Model Placement in 6G Fabric Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2013--2024},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280968},
	doi = {10.1109/JSAC.2023.3280968},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HaoHC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sensing and computing based on intelligent fabrics can meet the ultra-reliable and low-latency communication (URLLC) needs of sixth-generation wireless (6G) by integrating sensing units into fabric fibers to perceive user data. Although some researchers have designed sensing or computing solutions, such solutions have not been well explored. In this paper, we consider the joint sensing adaptation and model placement in a 6G fabric space. We first propose an intelligent-fiber-driven 6G fabric computing network to minimize acquisition latency while ensuring accuracy. Then, we formulate an optimization model that takes the fabric sampling rate, sampling density, and model placement as variables. To solve the model, we propose an effective learning algorithm based on deep reinforcement learning. That is, by transforming the optimization problem into a state space, action space, and reward function, we design an optimal sensing and placement scheme. The simulation results show that our proposed scheme can achieve optimal sensing and computing compared with several baseline algorithms.}
}


@article{DBLP:journals/jsac/YangGTLP23,
	author = {Yuwen Yang and
                  Feifei Gao and
                  Xiaoming Tao and
                  Guangyi Liu and
                  Chengkang Pan},
	title = {Environment Semantics Aided Wireless Communications: {A} Case Study
                  of mmWave Beam Prediction and Blockage Prediction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2025--2040},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280966},
	doi = {10.1109/JSAC.2023.3280966},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YangGTLP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose an environment semantics aided wireless communication framework to reduce the transmission latency and improve the transmission reliability, where semantic information is extracted from environment image data, selectively encoded based on its task-relevance, and then fused to make decisions for channel related tasks. As a case study, we develop an environment semantics aided network architecture for mmWave communication systems, which is composed of a semantic feature extraction network, a feature selection algorithm, a task-oriented encoder, and a decision network. With images taken from street cameras and user’s identification information as the inputs, the environment semantics aided network architecture is trained to predict the optimal beam index and the blockage state for the base station. It is seen that without pilot training or costly beam scans, the environment semantics aided network architecture can realize extremely efficient beam prediction and timely blockage prediction, thus meeting requirements for ultra-reliable and low-latency communications (URLLCs). Simulation results demonstrate that compared with existing works, the proposed environment semantics aided network architecture can reduce system overheads such as storage space and computational cost while achieving satisfactory prediction accuracy and protecting user privacy.}
}


@article{DBLP:journals/jsac/ShiZLMG23,
	author = {Hongjian Shi and
                  Weichu Zheng and
                  Zifei Liu and
                  Ruhui Ma and
                  Haibing Guan},
	title = {Automatic Pipeline Parallelism: {A} Parallel Inference Framework for
                  Deep Learning Applications in 6G Mobile Communication Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2041--2056},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280970},
	doi = {10.1109/JSAC.2023.3280970},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShiZLMG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of wireless communication, achieving the neXt generation Ultra-Reliable and Low-Latency Communications (xURLLC) in 6G mobile communication systems has become a critical problem. Among many applications in xURLLC, deep learning model inference requires improvement over its efficiency. Due to the heterogeneous hardware environment in 6G, parallel schedules from distributed machine learning and edge computing has been borrowed to tackle the efficiency problem. However, traditional parallel schedules suffer from high latency, low throughput, and low device utility. In this paper, we propose Automatic Pipeline Parallelism ( AP^{2}\n), a parallel inference framework for deep learning applications in 6G mobile communication systems, to improve the model inference efficiency while maintaining reliability. AP^{2}\ncontains three sub-modules. A task-device affinity predictor predicts a task’s expected execution time on a given device. The parallel inference arrangement optimizer finds the most suitable device for each task. The parallel inference scheduler converts the arrangement to a schedule that can be directly executed in the system. The experimental results show that AP^{2}\ncan achieve better latency, throughput, reliability, and device utility than other parallel schedules. Also, the priority of the sub-module designs has been approved through the experiments.}
}


@article{DBLP:journals/jsac/TangYSZ23,
	author = {Zhifeng Tang and
                  Nan Yang and
                  Parastoo Sadeghi and
                  Xiangyun Zhou},
	title = {Age of Information in Downlink Systems: Broadcast or Unicast Transmission?},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2057--2070},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280986},
	doi = {10.1109/JSAC.2023.3280986},
	timestamp = {Fri, 21 Jul 2023 22:26:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/TangYSZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We analytically decide whether the broadcast transmission scheme or the unicast transmission scheme achieves the optimal age of information (AoI) performance of a multiuser system where a base station (BS) generates and transmits status updates to multiple user equipments (UEs). In the broadcast transmission scheme, the status update for all UEs is jointly encoded into a packet for transmission, while in the unicast transmission scheme, the status update for each UE is encoded individually and transmitted by following the round robin policy. For both transmission schemes, we examine three packet management strategies, namely the non-preemption strategy, the preemption in buffer strategy, and the preemption in serving strategy. We first derive new closed-form expressions for the average AoI achieved by two transmission schemes with three packet management strategies. Based on them, we compare the AoI performance of two transmission schemes in two systems, namely, the remote control system and the dynamic system. Aided by simulation results, we verify our analysis and investigate the impact of system parameters on the average AoI. For example, the unicast transmission scheme is more appropriate for the system with a large number of UEs. Otherwise, the broadcast transmission scheme is more appropriate.}
}


@article{DBLP:journals/jsac/YueYZP23,
	author = {Zhiling Yue and
                  Howard H. Yang and
                  Meng Zhang and
                  Nikolaos Pappas},
	title = {Age of Information Under Frame Slotted ALOHA-Based Status Updating
                  Protocol},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2071--2089},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280990},
	doi = {10.1109/JSAC.2023.3280990},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YueYZP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a frame slotted ALOHA (FSA)-based protocol for a random access network where sources transmit status updates to their intended destinations. We evaluate the effect of such a protocol on the network’s timeliness performance using the Age of Information (AoI) metric. Specifically, we leverage tools from stochastic geometry to model the spatial positions of the source-destination pairs and capture the entanglement amongst the nodes’ spatial-temporal attributes through the interference they caused to each other. We derive analytical expressions for the average and variance of AoI over a typical transmission link in Poisson bipolar and cellular networks, respectively. Our analysis shows that in densely deployed networks, the FSA-based status updating protocol can significantly decrease the average AoI and in addition, stabilizes the age performance by substantially reducing the variance of AoI. Furthermore, under the same updating frequency, converting a slotted ALOHA protocol into an FSA-based one always leads to a reduction in the average AoI. Moreover, implementing FSA in conjunction with power control can further benefit the AoI performance, although the particular values of framesize and power control factor must be adequately tuned to achieve the optimal gain.}
}


@article{DBLP:journals/jsac/ZhangYWXZJLW23,
	author = {Ziyang Zhang and
                  Xiaohu You and
                  Dongming Wang and
                  Xinjiang Xia and
                  Pengcheng Zhu and
                  Yanxiang Jiang and
                  Chulong Liang and
                  Jiangzhou Wang},
	title = {Performance of Multidevice Downlink Cell-Free System Under Finite
                  Blocklength for uRLLC With Hard Deadlines},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2090--2106},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280962},
	doi = {10.1109/JSAC.2023.3280962},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangYWXZJLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an important part of beyond the fifth-generation (B5G) and the sixth-generation (6G) mobile communication systems, ultra-reliable and low latency communications (uRLLC) puts forward strict requirements for delay and reliability (e.g., 99.9999% reliability and\n500μs\nlatency). At present, the evaluation measures of delay and reliability are usually based on infinite block length and rely on long-term statistics, which cannot meet the requirement of low latency. The cell-free system, with a very large number of distributed antennas, has the characteristics of macro-diversity and spatial sparsity, which can further enhance the performance of uRLLC. In this paper, the downlink multidevice cell-free system with hard deadlines is considered and analyzed in the finite block length (FBL) regime. The communication’s delay and reliability are described based on two instantaneous evaluation measures: transmission error (TE) and time overflow (TO) probability. From the perspective of information theory, this paper analyzes the analytic expression of TE probability for a single device and the performance impact of FBL on the traditional channel capacity analysis. Considering the multidevice TO probability in a cell-free system, the closed-form expressions of upper and lower bounds are derived and compared with the gamma approximation results. This paper further provides three methods, namely, transmission rate selection, device grouping and space division multiplexing, to balance the delay and reliability of the system and analyzes the performance.}
}


@article{DBLP:journals/jsac/LiCP23,
	author = {Changkun Li and
                  Wei Chen and
                  H. Vincent Poor},
	title = {Diversity Enabled Low-Latency Wireless Communications With Hard Delay
                  Constraints},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2107--2122},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280969},
	doi = {10.1109/JSAC.2023.3280969},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiCP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging next generation Ultra-Reliable and Low-Latency Communications (xURLLC) is expected to play a central role in supporting mission-critical mobile applications because it holds the promise of improving the Quality-of-Service (QoS) substantially. However, it is quite challenging to satisfy the hard delay constraint in harsh wireless environments due to sporadic deep fades, especially when the average power is strictly limited. In this paper, we aim at assuring hard delay constraints with the aid of frequency or spatial diversity techniques. To this end, we focus on both parallel and multiple-input-multiple-output (MIMO) fading channels, in which time domain power adaptation is exploited to provide just-in-time services (JITS). It is shown that the hard delay constraint can be satisfied with a finite average power when the frequency or spatial diversity gains are no less than two. By adopting the implicit function theorem, we reveal the relationship between the required average power, the delay constrained throughput, and the outage probability without power adaptation. Furthermore, by adopting Ferrari’s solution to fourth order algebraic equations, we show that hard delay constrained transmission is feasible even when the sub-channels in the frequency and spatial domains are highly but not fully correlated.}
}


@article{DBLP:journals/jsac/ZhuYHSS23,
	author = {Yao Zhu and
                  Xiaopeng Yuan and
                  Yulin Hu and
                  Rafael F. Schaefer and
                  Anke Schmeink},
	title = {Trade Reliability for Security: Leakage-Failure Probability Minimization
                  for Machine-Type Communications in {URLLC}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2123--2137},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280960},
	doi = {10.1109/JSAC.2023.3280960},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhuYHSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to provide information security while fulfilling ultra reliability and low-latency requirements is one of the major concerns for enabling the next generation of ultra-reliable and low-latency communications service (xURLLC), specially in machine-type communications. In this work, we investigate the reliability-security tradeoff by defining the leakage-failure probability, a metric that jointly characterizes both reliability and security performances for short-packet transmissions. We discover that the system performance can be enhanced, counter-intuitively, by allocating fewer resources for the transmission with finite blocklength (FBL) codes. In order to solve the corresponding optimization problem for the joint resource allocation, we propose an optimization framework, that leverages lower-bounded approximations for the decoding error probability in the FBL regime. We characterize the convexity of the reformulated problem and establish an efficient iterative searching method, the convergence of which is guaranteed. To show the extendability of the framework, we further discuss the blocklength allocation schemes with practical requirements of reliable-secure performance, as well as the transmissions with the statistical channel state information (CSI). Numerical results verify the accuracy of the proposed approach and demonstrate the reliability-security tradeoff under various setups.}
}


@article{DBLP:journals/jsac/YuCZ23,
	author = {Wenhan Yu and
                  Terence Jie Chua and
                  Jun Zhao},
	title = {Asynchronous Hybrid Reinforcement Learning for Latency and Reliability
                  Optimization in the Metaverse Over Wireless Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2138--2157},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280988},
	doi = {10.1109/JSAC.2023.3280988},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YuCZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Technology advancements in wireless communications and high-performance Extended Reality (XR) have empowered the developments of the Metaverse. The demand for the Metaverse applications and hence, real-time digital twinning of real-world scenes is increasing. Nevertheless, the replication of 2D physical world images into 3D virtual objects is computationally intensive and requires computation offloading. The disparity in transmitted object dimension (2D as opposed to 3D) leads to asymmetric data sizes in uplink (UL) and downlink (DL). To ensure the reliability and low latency of the system, we consider an asynchronous joint UL-DL scenario where in the UL stage, the smaller data size of the physical world images captured by multiple extended reality users (XUs) will be uploaded to the Metaverse Console (MC) to be construed and rendered. In the DL stage, the larger-size 3D virtual objects need to be transmitted back to the XUs. We design a novel multi-agent reinforcement learning algorithm structure, namely Asynchronous Actors Hybrid Critic (AAHC), to optimize the decisions pertaining to computation offloading and channel assignment in the UL stage and optimize the DL transmission power in the DL stage. Extensive experiments demonstrate that compared to proposed baselines, AAHC obtains better solutions with satisfactory training time.}
}


@article{DBLP:journals/jsac/DuLNKXZK23,
	author = {Hongyang Du and
                  Jiazhen Liu and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Junshan Zhang and
                  Dong In Kim},
	title = {Attention-Aware Resource Allocation and QoE Analysis for Metaverse
                  xURLLC Services},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2158--2175},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280978},
	doi = {10.1109/JSAC.2023.3280978},
	timestamp = {Fri, 28 Jun 2024 14:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DuLNKXZK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Metaverse encapsulates our expectations of the next-generation Internet, while bringing new key performance indicators (KPIs). Although conventional ultra-reliable and low-latency communications (URLLC) can satisfy objective KPIs, it is difficult to provide a personalized immersive experience that is a distinctive feature of the Metaverse. Since the quality of experience (QoE) can be regarded as a comprehensive KPI, the URLLC is evolved towards the next generation URLLC (xURLLC) with a personalized resource allocation scheme to achieve higher QoE. To deploy Metaverse xURLLC services, we study the interaction between the Metaverse service provider (MSP) and the network infrastructure provider (InP), and provide an optimal contract design framework. Specifically, the utility of the MSP, defined as a function of Metaverse users’ QoE, is to be maximized, while ensuring the incentives of the InP. To model the QoE mathematically, we propose a novel metric named Meta-Immersion that incorporates both the objective KPIs and subjective feelings of Metaverse users. Furthermore, we develop an attention-aware rendering capacity allocation scheme to improve QoE in xURLLC. Using a user-object-attention level dataset, we validate that the xURLLC can achieve an average of 20.1% QoE improvement compared to the conventional URLLC with a uniform resource allocation scheme. The code for this paper is available at https://github.com/HongyangDu/AttentionQoE .}
}


@article{DBLP:journals/jsac/LiZWZY23,
	author = {Kang Li and
                  Pengcheng Zhu and
                  Yan Wang and
                  Fu{-}Chun Zheng and
                  Xiaohu You},
	title = {Joint Uplink and Downlink Resource Allocation Toward Energy-Efficient
                  Transmission for {URLLC}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2176--2192},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280967},
	doi = {10.1109/JSAC.2023.3280967},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiZWZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-reliable and low-latency communications (URLLC) is firstly proposed in 5G networks, and expected to support applications with the most stringent quality-of-service (QoS). However, since the wireless channels vary dynamically, the transmit power for ensuring the QoS requirements of URLLC may be very high, which conflicts with the power limitation of a real system. To fulfil the successful URLLC transmission with finite transmit power, we propose an energy-efficient packet delivery mechanism incorparated with frequency-hopping and proactive dropping in this paper. To reduce uplink outage probability, frequency-hopping provides more chances for transmission so that the failure hardly occurs. To avoid downlink outage from queue clearing, proactive dropping controls overall reliability by introducing an extra error component. With the proposed packet delivery mechanism, we jointly optimize bandwidth allocation and power control of uplink and downlink, antenna configuration, and subchannel assignment to minimize the average total power under the constraint of URLLC transmission requirements. Via theoretical analysis (e.g., the convexity with respect to bandwidth, the independence of bandwidth allocation, the convexity of antenna configuration with inactive constraints), the simplication of finding the global optimal solution for resource allocation is addressed. A three-step method is then proposed to find the optimal solution for resource allocation. Simulation results validate the analysis and show the performance gain by optimizing resource allocation with the proposed packet delivery mechanism.}
}


@article{DBLP:journals/jsac/PengRDEWH23,
	author = {Qihao Peng and
                  Hong Ren and
                  Mianxiong Dong and
                  Maged Elkashlan and
                  Kai{-}Kit Wong and
                  Lajos Hanzo},
	title = {Resource Allocation for Cell-Free Massive MIMO-Aided {URLLC} Systems
                  Relying on Pilot Sharing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2193--2207},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280976},
	doi = {10.1109/JSAC.2023.3280976},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/PengRDEWH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource allocation is conceived for cell-free (CF) massive multi-input multi-output (MIMO)-aided ultra-reliable and low latency communication (URLLC) systems. Specifically, to support multiple devices with limited pilot overhead, pilot reuse among the users is considered, where we formulate a joint pilot length and pilot allocation strategy for maximizing the number of devices admitted. Then, the pilot power and transmit power are jointly optimized while simultaneously satisfying the devices’ decoding error probability, latency, and data rate requirements. Firstly, we derive the lower bounds (LBs) of ergodic data rate under finite channel blocklength (FCBL). Then, we propose a novel pilot assignment algorithm for maximizing the number of devices admitted. Based on the pilot allocation pattern advocated, the weighted sum rate (WSR) is maximized by jointly optimizing the pilot power and payload power. To tackle the resultant NP-hard problem, the original optimization problem is first simplified by sophisticated mathematical transformations, and then approximations are found for transforming the original problems into a series of subproblems in geometric programming (GP) forms that can be readily solved. Simulation results demonstrate that the proposed pilot allocation strategy is capable of significantly increasing the number of admitted devices and the proposed power allocation achieves substantial WSR performance gain.}
}


@article{DBLP:journals/jsac/LiuZDN23,
	author = {Yan Liu and
                  Hui Zhou and
                  Yansha Deng and
                  Arumugam Nallanathan},
	title = {Channel Access Optimization in Unlicensed Spectrum for Downlink {URLLC:}
                  Centralized and Federated {DRL} Approaches},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2208--2222},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280982},
	doi = {10.1109/JSAC.2023.3280982},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuZDN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The sixth-generation (6G) communication research is currently in the early stage, where ultra-reliable low-latency communication (URLLC) is still an important service as in the fifth-generation (5G). Since 6G networks are expected to provide even higher levels of massive connectivity, high spectrum efficiency, high reliability, and low latency than 5G communication, it would confront much more severe spectrum scarcity problems, which make the new radio in unlicensed spectrum (NR-U) technology attractive. However, how to achieve URLLC requirements in NR-U networks is extremely challenging due to interference and collisions among multiple radio access technologies (e.g., WiFi). Therefore, it is urgent to design efficient spectrum-sharing algorithms to support URLLC in emerging 6G networks. In this paper, we develop novel centralized deep reinforcement learning (CDRL) and federated DRL (FDRL) frameworks, respectively, to optimize the downlink URLLC transmission in NR-U and WiFi coexistence systems through dynamically adjusting energy detection (ED) thresholds. Our results show that both CDRL and FDRL approaches have improved the reliability of the NR-U system significantly, but the CDRL framework has sacrificed the reliability of the WiFi system. To guarantee the reliability of the WiFi system while improving the NR-U system, we take fairness into account by redesigning the reward of CDRL.}
}


@article{DBLP:journals/jsac/KeGZZNP23,
	author = {Malong Ke and
                  Zhen Gao and
                  Mingyu Zhou and
                  Dezhi Zheng and
                  Derrick Wing Kwan Ng and
                  H. Vincent Poor},
	title = {Next-Generation {URLLC} With Massive Devices: {A} Unified Semi-Blind
                  Detection Framework for Sourced and Unsourced Random Access},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2223--2244},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280981},
	doi = {10.1109/JSAC.2023.3280981},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KeGZZNP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a unified semi-blind detection framework for sourced and unsourced random access (RA), which enables next-generation ultra-reliable low-latency communications (URLLC) with a massive number of devices. Specifically, the active devices transmit their uplink access signals in a grant-free manner to realize ultra-low access latency. Meanwhile, the base station aims to achieve ultra-reliable data detection under severe inter-device interference without exploiting explicit channel state information (CSI). We first propose an efficient transmitter design, where a small amount of reference information (RI) is embedded in the access signal to resolve the inherent ambiguities incurred by the unknown CSI. At the receiver, we further develop a successive interference cancellation-based semi-blind detection scheme, where a bilinear generalized approximate message passing algorithm is utilized for joint channel and signal estimation (JCSE), while the embedded RI is exploited for ambiguity elimination. Particularly, a rank selection approach and a RI-aided initialization strategy are incorporated to reduce the algorithmic computational complexity and to enhance the JCSE reliability, respectively. Besides, four enabling techniques are integrated to satisfy the stringent latency and reliability requirements of massive URLLC. Numerical results demonstrate that the proposed semi-blind detection framework offers a better scalability-latency-reliability tradeoff than the state-of-the-art detection schemes dedicated to sourced or unsourced RA.}
}


@article{DBLP:journals/jsac/LiuLYLN23,
	author = {Chang Liu and
                  Shuangyang Li and
                  Weijie Yuan and
                  Xuemeng Liu and
                  Derrick Wing Kwan Ng},
	title = {Predictive Precoder Design for OTFS-Enabled {URLLC:} {A} Deep Learning
                  Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2245--2260},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280984},
	doi = {10.1109/JSAC.2023.3280984},
	timestamp = {Tue, 07 May 2024 20:20:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuLYLN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the orthogonal time frequency space (OTFS) transmission for enabling ultra-reliable low-latency communications (URLLC). To guarantee excellent reliability performance, pragmatic precoder design is an effective and indispensable solution. However, the design requires accurate instantaneous channel state information at the transmitter (ICSIT) which is not always available in practice. Motivated by this, we adopt a deep learning (DL) approach to exploit implicit features from estimated historical delay-Doppler domain channels (DDCs) to directly predict the precoder to be adopted in the next time frame for minimizing the frame error rate (FER), that can further improve the system reliability without the acquisition of ICSIT. To this end, we first establish a predictive transmission protocol and formulate a general problem for the precoder design where a closed-form theoretical FER expression is derived serving as the objective function to characterize the system reliability. Then, we propose a DL-based predictive precoder design framework which exploits an unsupervised learning mechanism to improve the practicability of the proposed scheme. As a realization of the proposed framework, we design a DDCs-aware convolutional long short-term memory (CLSTM) network for the precoder design, where both the convolutional neural network and LSTM modules are adopted to facilitate the spatial-temporal feature extraction from the estimated historical DDCs to further enhance the precoder performance. Simulation results demonstrate that the proposed scheme facilitates a flexible reliability-latency tradeoff and achieves an excellent FER performance that approaches the lower bound obtained by a genie-aided benchmark requiring perfect ICSI at both the transmitter and receiver.}
}


@article{DBLP:journals/jsac/QiuHY23,
	author = {Min Qiu and
                  Yu{-}Chih Huang and
                  Jinhong Yuan},
	title = {Downlink Transmission With Heterogeneous {URLLC} Services: Discrete
                  Signaling With Single-User Decoding},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2261--2277},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280994},
	doi = {10.1109/JSAC.2023.3280994},
	timestamp = {Fri, 21 Jul 2023 22:26:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/QiuHY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The problem of designing downlink transmission schemes for supporting heterogeneous ultra-reliable low-latency communications (URLLC) and/or with other types of services is investigated. We consider the broadcast channel, where the base station sends superimposed signals to multiple users. Under heterogeneous blocklength constraints, strong users who are URLLC users cannot wait to receive the entire transmission frame and perform successive interference cancellation (SIC) due to stringent latency requirements, in contrast to the conventional infinite blocklength cases. Even if SIC is feasible, SIC may be imperfect under finite blocklength constraints. To cope with the heterogeneity in latency and reliability requirements, we propose a practical downlink transmission scheme with discrete signaling and single-user decoding (SUD), i.e., without SIC . We carefully design the discrete input distributions to enable efficient SUD by exploiting the structural interference. Furthermore, we derive the second-order achievable rate under heterogenous blocklength and error probability constraints and use it to guide the design of channel coding and modulations. It is shown that in terms of achievable rate under short blocklength, the proposed scheme with regular quadrature amplitude modulations and SUD can operate extremely close to the benchmark schemes that assume perfect SIC with Gaussian signaling.}
}


@article{DBLP:journals/jsac/ZamanPFSW23,
	author = {Fakhar Zaman and
                  Saw Nang Paing and
                  Ahmad Farooq and
                  Hyundong Shin and
                  Moe Z. Win},
	title = {Concealed Quantum Telecomputation for Anonymous 6G {URLLC} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {7},
	pages = {2278--2296},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3280989},
	doi = {10.1109/JSAC.2023.3280989},
	timestamp = {Tue, 07 May 2024 20:20:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZamanPFSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed learning and multi-tier computing are the key ingredients to ensure ultra-reliable and low-latency communication (URLLC) in 6G networks. The distinct transition from connected things in 5G URLLC networks to connected intelligence in 6G URLLC networks requires ultra-secure communication due to the massive amount of private data. However, it is a challenging task to ensure stringent 6G URLLC requirements along with user privacy and data security in distributed networks. In this paper, we devise a distributed quantum computation protocol to perform a nonlocal controlled unitary operation on a bipartite input state in concealed and counterfactual manner and integrate it with anonymous quantum communication networks. This distributed protocol allows Bob to apply an arbitrary singlequbit unitary operator on Alice’s qubit in a controlled and probabilistic fashion, without revealing the operator to her and without transmitting any physical particle over the quantum channel-called the counterfactual concealed telecomputation (CCT). It is shown that the CCT protocol neither requires the preshared entanglement nor depends on the bipartite input state and that the single-qubit unitary teleportation is a special case of CCT. The quantum circuit for CCT can be implemented using the (chained) quantum Zeno gates. The protocol becomes deterministic with simplified circuit implementation if the initial composite state of Alice and Bob is a Bell-type state. Furthermore, we provide numerical examples of quantum anonymous broadcast networks using the CCT protocol and show their degrees of anonymity in the presence of malicious users.}
}


@article{DBLP:journals/jsac/AlexiouDRSV23,
	author = {Angeliki Alexiou and
                  M{\'{e}}rouane Debbah and
                  Marco Di Renzo and
                  Emilio Calvanese Strinati and
                  Harish Viswanathan},
	title = {Guest Editorial Beyond Shannon Communications - {A} Paradigm Shift
                  to Catalyze 6G},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2299--2305},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288796},
	doi = {10.1109/JSAC.2023.3288796},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AlexiouDRSV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Targeting ultra-reliable and scalable connectivity of extremely high data rates, in the 100 Gbps to Tbps range, at almost “zero-latency” in 6G systems would require taking advantage of breakthrough novel technology concepts, including THz wireless links, broadband and spectrally efficient RF-frontends for a variety of different bands, the employment of intelligent materials (e.g., reconfigurable intelligent surfaces) and the design of machine learning-based models, protocols, and management techniques. To materialize the 6G vision, novel system techniques will need to be devised, including channel modeling and estimation, waveforms, beamforming, and multiple-access schemes, all tailored to the particularities of the adopted breakthrough technologies. As challenging Tbit/s usage scenarios are becoming ever more relevant for 6G systems, including non-line of sight connectivity based on intelligent surfaces and ad hoc connectivity in fast-moving network topologies, e.g., based on drones or V2X links, performance targets need to be reassessed. In such scenarios, apart from the high data rates in the order of Tbit/s other critical parameters may arise as more relevant: range, reliability, adaptability, reconfigurability, and agility, to name just a few.}
}


@article{DBLP:journals/jsac/YanHHSMGY23,
	author = {Wencai Yan and
                  Wanming Hao and
                  Chongwen Huang and
                  Gangcan Sun and
                  Osamu Muta and
                  Haris Gacanin and
                  Chau Yuen},
	title = {Beamforming Analysis and Design for Wideband THz Reconfigurable Intelligent
                  Surface Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2306--2320},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288235},
	doi = {10.1109/JSAC.2023.3288235},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YanHHSMGY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS)-aided terahertz (THz) communications have been regarded as a promising candidate for future 6G networks because of its ultra-wide bandwidth and ultra-low power consumption. However, there exists the beam split problem, especially when the base station (BS) or RIS owns the large-scale antennas, which may lead to serious array gain loss. Therefore, in this paper, we investigate the beam split and beamforming design problems in the THz RIS communications. Specifically, we first analyze the beam split effect caused by different RIS sizes, shapes and deployments. On this basis, we apply the fully connected time delayer phase shifter hybrid beamforming (FC-TD-PS-HB) architecture at the BS and deploy distributed RISs to cooperatively mitigate the beam split effect. We aim to maximize the achievable sum rate by jointly optimizing the hybrid analog/digital beamforming, time delays at the BS and reflection coefficients at the RISs. To solve the formulated problem, we first design the analog beamforming and time delays based on different RISs’ physical directions, and then it is transformed into an optimization problem by jointly optimizing the digital beamforming and reflection coefficients. Next, we propose an alternatively iterative optimization algorithm to deal with it. Specifically, for given the reflection coefficients, we propose an iterative algorithm based on the minimum mean square error technique to obtain the digital beamforming. After, we apply Lagrangian dual reformulation (LDR) and multidimensional complex quadratic transform (MCQT) methods to transform the original problem to a quadratically constrained quadratic program, which can be solved by alternating direction method of multipliers (ADMM) technique to obtain the reflection coefficients. Finally, the digital beamforming and reflection coefficients are obtained via repeating the above processes until convergence. Simulation results verify that the proposed scheme can effectively alleviate the beam split effect and improve the system capacity.}
}


@article{DBLP:journals/jsac/BereyhiAOMSP23,
	author = {Ali Bereyhi and
                  Saba Asaad and
                  Chongjun Ouyang and
                  Ralf R. M{\"{u}}ller and
                  Rafael F. Schaefer and
                  H. Vincent Poor},
	title = {Channel Hardening of IRS-Aided Multi-Antenna Systems: How Should IRSs
                  Scale?},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2321--2335},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288237},
	doi = {10.1109/JSAC.2023.3288237},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/BereyhiAOMSP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is widely believed that large IRS-aided MIMO settings maintain the fundamental features of massive MIMO systems. This work gives a rigorous proof that confirms this belief. We show that using a large passive IRS, the end-to-end MIMO channel between the transmitter and the receiver always hardens, even if the IRS elements are strongly correlated. For fading direct and reflection links between the transmitter and the receiver, our derivations demonstrate that for a large number of reflecting elements on the IRS, the capacity of the end-to-end channel is accurately approximated by a real-valued Gaussian random variable whose variance goes to zero as the number of IRS elements grows unboundedly large. The order of this drop depends on how the physical dimensions of the IRS grow. We derive this order explicitly. Numerical experiments show that the closed-form approximation very closely matches the histogram of the capacity term, even in practical scenarios. As a sample application of the results, we characterize the dimensional trade-off between the transmitter and the IRS. The result is intuitive: For a target performance, the larger the IRS is, the fewer transmit antennas are required.}
}


@article{DBLP:journals/jsac/NicolaidesPKYK23,
	author = {Andreas Nicolaides and
                  Constantinos Psomas and
                  Ghassan M. Kraidy and
                  Sheng Yang and
                  Ioannis Krikidis},
	title = {Outage and {DMT} Analysis of Partition-Based Schemes for RIS-Aided
                  {MIMO} Fading Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2336--2349},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288264},
	doi = {10.1109/JSAC.2023.3288264},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/NicolaidesPKYK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the performance of multiple-input multiple-output (MIMO) fading channels assisted by a reconfigurable intelligent surface (RIS), through the employment of partition-based RIS schemes. The proposed schemes are implemented without requiring any channel state information knowledge at the transmitter side; this characteristic makes them attractive for practical applications. In particular, the RIS elements are partitioned into sub-surfaces, which are periodically modified in an efficient way to assist the communication. Under this framework, we propose two low-complexity partition-based schemes, where each sub-surface is adjusted by following an amplitude-based or a phase-based approach. Specifically, the activate-reflect (AR) scheme activates each sub-surface consecutively, by changing the reflection amplitude of the corresponding elements. On the other hand, the flip-reflect (FR) scheme adjusts periodically the phase shift of the elements at each sub-surface. Through the sequential reconfiguration of each sub-surface, an equivalent parallel channel in the time domain is produced. We analyze the performance of each scheme in terms of outage probability and provide expressions for the achieved diversity-multiplexing tradeoff. Our results show that the asymptotic performance of the considered network under the partition-based schemes can be significantly enhanced in terms of diversity gain compared to the conventional case, where a single partition is considered. Moreover, the FR scheme always achieves the maximum multiplexing gain, while for the AR scheme this maximum gain can be achieved only under certain conditions with respect to the number of elements in each sub-surface.}
}


@article{DBLP:journals/jsac/ZhangD23,
	author = {Zijian Zhang and
                  Linglong Dai},
	title = {Pattern-Division Multiplexing for Multi-User Continuous-Aperture {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2350--2366},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288244},
	doi = {10.1109/JSAC.2023.3288244},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, thanks to the advances in meta-materials, the concept of continuous-aperture MIMO (CAP-MIMO) is reinvestigated to achieve improved communication performance with limited antenna apertures. Unlike the classical MIMO composed of discrete antennas, CAP-MIMO has a quasi-continuous antenna surface, which is expected to generate any current distribution (i.e., pattern) and induce controllable spatial electromagnetic (EM) waves. In this way, the information is directly modulated on the EM waves, which makes it promising to approach the ultimate capacity of finite apertures. The pattern design is the key factor to determine the communication performance of CAP-MIMO, but it has not been well studied in the literature. In this paper, we develop pattern-division multiplexing (PDM) to design the patterns for CAP-MIMO. Specifically, we first study and model a typical multi-user CAP-MIMO system, which allows us to formulate the sum-rate maximization problem. Then, we develop a general PDM technique to transform the design of the continuous pattern functions to the design of their projection lengths on finite orthogonal bases, which can overcome the challenge of functional programming. Utilizing PDM, we further propose a block coordinate descent (BCD) based pattern design scheme to solve the formulated sum-rate maximization problem. Simulation results show that, the sum-rate achieved by the proposed scheme is higher than that achieved by benchmark schemes, which demonstrates the effectiveness of the developed PDM for CAP-MIMO.}
}


@article{DBLP:journals/jsac/DengZZDZPS23,
	author = {Ruoqi Deng and
                  Yutong Zhang and
                  Haobo Zhang and
                  Boya Di and
                  Hongliang Zhang and
                  H. Vincent Poor and
                  Lingyang Song},
	title = {Reconfigurable Holographic Surfaces for Ultra-Massive {MIMO} in 6G:
                  Practical Design, Optimization and Implementation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2367--2379},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288248},
	doi = {10.1109/JSAC.2023.3288248},
	timestamp = {Mon, 12 Feb 2024 16:07:18 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DengZZDZPS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ultra-massive multiple-input multiple-output (MIMO) is expected to be one of the key enablers in the forthcoming 6G networks to handle various user demands by exploiting spatial diversity. In this paper, a new paradigm termed holographic radio is considered for ultra-massive MIMO via integrating numerous antenna elements into a compact space, thereby achieving a spatially quasi-continuous aperture and realizing high beampattern gain. We propose a practical path to implement holographic radio by a novel metasurface-based antenna called a reconfigurable holographic surface (RHS). Specifically, the RHS is capable of holographic beamforming over the spatially quasi-continuous apertures by incorporating densely packed tunable metamaterial elements with low power consumption. To enhance the performance of the RHS as an antenna array for achieving ultra-massive MIMO, a holographic beamforming optimization algorithm is developed for beampattern gain maximization based on the hardware design and full-wave analyses of RHSs. We then implement a prototype of an RHS and build an RHS-aided communication platform to further substantiate the feasibility of RHS-enabled holographic radio. Both simulation and experimental results verify the effectiveness of the proposed holographic beamforming optimization algorithm. It is also proved that the RHS-aided communication platform is capable of supporting real-time transmission of high-definition video.}
}


@article{DBLP:journals/jsac/AnXNAHYH23,
	author = {Jiancheng An and
                  Chao Xu and
                  Derrick Wing Kwan Ng and
                  George C. Alexandropoulos and
                  Chongwen Huang and
                  Chau Yuen and
                  Lajos Hanzo},
	title = {Stacked Intelligent Metasurfaces for Efficient Holographic {MIMO}
                  Communications in 6G},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2380--2396},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288261},
	doi = {10.1109/JSAC.2023.3288261},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AnXNAHYH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A revolutionary technology relying on Stacked Intelligent Metasurfaces (SIM) is capable of carrying out advanced signal processing directly in the native electromagnetic (EM) wave regime. An SIM is fabricated by a sophisticated amalgam of multiple stacked metasurface layers, which may outperform its single-layer metasurface counterparts, such as reconfigurable intelligent surfaces (RIS) and metasurface lenses. We harness this new SIM for implementing holographic multiple-input multiple-output (HMIMO) communications without requiring excessive radio-frequency (RF) chains, which is a substantial benefit compared to existing implementations. First of all, we propose an HMIMO communication system based on a pair of SIM at the transmitter (TX) and receiver (RX), respectively. In sharp contrast to the conventional MIMO designs, SIM is capable of automatically accomplishing transmit precoding and receiver combining, as the EM waves propagate through them. As such, each spatial stream can be directly radiated and recovered from the corresponding transmit and receive port. Secondly, we formulate the problem of minimizing the error between the actual end-to-end channel matrix and the target diagonal one, representing a flawless interference-free system of parallel subchannels. This is achieved by jointly optimizing the phase shifts associated with all the metasurface layers of both the TX-SIM and RX-SIM. We then design a gradient descent algorithm to solve the resultant non-convex problem. Furthermore, we theoretically analyze the HMIMO channel capacity bound and provide some fundamental insights. Finally, extensive simulation results are provided for characterizing our SIM-aided HMIMO system, which quantifies its substantial performance benefits, e.g., 150% capacity improvement over both conventional MIMO and its RIS-aided counterparts.}
}


@article{DBLP:journals/jsac/ChenWLJ23,
	author = {Weicong Chen and
                  Chao{-}Kai Wen and
                  Xiao Li and
                  Shi Jin},
	title = {Multi-Timescale Channel Customization for Transmission Design in RIS-Assisted
                  {MIMO} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2397--2413},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288267},
	doi = {10.1109/JSAC.2023.3288267},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenWLJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance of transmission schemes is heavily influenced by the wireless channel, which is typically considered an uncontrollable factor. However, the introduction of reconfigurable intelligent surfaces (RISs) to wireless communications enables the customization of a preferred channel for adopted transmissions by reshaping electromagnetic waves. In this study, we propose multi-timescale channel customization for RIS-assisted multiple-input multiple-output systems to facilitate transmission design. Specifically, we customize a high-rank channel for spatial multiplexing (SM) transmission and a highly correlated rank-1 channel for beamforming (BF) transmission by designing the phase shifters of the RIS with statistical channel state information in the angle-coherent time to improve spectral efficiency (SE). We derive closed-form expressions for the approximation and upper bound of the ergodic SE and compare them to investigate the relative SE performance of SM and BF transmissions. In terms of reliability enhancement, we customize a fast-changing channel in the symbol timescale to achieve more diversity gain for SM and BF transmissions. Extensive numerical results demonstrate that flexible customization of channel characteristics for a specific transmission scheme can achieve a tradeoff between SE and bit error ratio performance.}
}


@article{DBLP:journals/jsac/AkroutSBMH23,
	author = {Mohamed Akrout and
                  Volodymyr Shyianov and
                  Faouzi Bellili and
                  Amine Mezghani and
                  Robert W. Heath Jr.},
	title = {Super-Wideband Massive {MIMO}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2414--2430},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288269},
	doi = {10.1109/JSAC.2023.3288269},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AkroutSBMH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present a unified model for connected antenna arrays with a large number of tightly integrated (i.e., coupled) antennas in a compact space within the context of massive multiple-input multiple-output (MIMO) communication. We refer to this system as tightly-coupled massive MIMO. From an information-theoretic perspective, scaling the design of tightly-coupled massive MIMO systems in terms of the number of antennas, the operational bandwidth, and form factor was not addressed in prior art. We investigate this open research problem using a physically consistent modeling approach for far-field (FF) MIMO communication based on multi-port circuit theory. In doing so, we turn mutual coupling (MC) from a foe to a friend of MIMO systems design, thereby challenging a basic percept in antenna systems engineering that promotes MC mitigation/compensation. We show that tight MC widens the operational bandwidth of antenna arrays thereby unleashing a missing MIMO gain that we coin “bandwidth gain”. Furthermore, we derive analytically the asymptotically optimum spacing-to-antenna-size ratio by establishing a condition for tight coupling in the limit of large-size antenna arrays with quasi-continuous apertures. We also optimize the antenna array size while maximizing the achievable rate under fixed transmit power and inter-element spacing. Then, we study the impact of MC on the achievable rate of MIMO systems under line-of-sight (LoS) and Rayleigh fading channels. These results reveal new insights into the design of tightly-coupled massive antenna arrays as opposed to the widely-adopted “disconnected” designs that disregard MC by putting faith in the half-wavelength spacing rule.}
}


@article{DBLP:journals/jsac/ShiZNA23,
	author = {Enyu Shi and
                  Jiayi Zhang and
                  Derrick Wing Kwan Ng and
                  Bo Ai},
	title = {Uplink Performance of RIS-Aided Cell-Free Massive {MIMO} System With
                  Electromagnetic Interference},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2431--2445},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288265},
	doi = {10.1109/JSAC.2023.3288265},
	timestamp = {Mon, 21 Aug 2023 15:51:16 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ShiZNA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cell-free (CF) massive multiple-input multiple-output (MIMO) and reconfigurable intelligent surface (RIS) are two promising technologies for realizing future beyond-fifth generation (B5G) networks. In this paper, we consider a practical spatially correlated RIS-aided CF massive MIMO system with multi-antenna access points (APs) over spatially correlated fading channels. Different from previous work, the electromagnetic interference (EMI) at RIS is considered to further characterize the system performance of the actual environment. Then, we derive the closed-form expression for the system spectral efficiency (SE) with the maximum ratio (MR) combining at the APs and the large-scale fading decoding (LSFD) at the central processing unit (CPU). Moreover, to counteract the near-far effect and EMI, we propose practical fractional power control (FPC) and max-min power control algorithms to further improve the system performance. We unveil the impact of EMI, channel correlations, and different signal processing methods on the uplink SE of user equipments (UEs). The accuracy of our derived analytical results is verified by extensive Monte-Carlo simulations. Our results show that the EMI can substantially degrade the SE, especially for those UEs with unsatisfactory channel conditions. Besides, increasing the number of RIS elements is always beneficial in terms of the SE, but with diminishing returns when the number of RIS elements is sufficiently large. Furthermore, the existence of spatial correlations among RIS elements can deteriorate the system performance when RIS is impaired by EMI.}
}


@article{DBLP:journals/jsac/LiSC23,
	author = {Hongyu Li and
                  Shanpu Shen and
                  Bruno Clerckx},
	title = {Beyond Diagonal Reconfigurable Intelligent Surfaces: {A} Multi-Sector
                  Mode Enabling Highly Directional Full-Space Wireless Coverage},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2446--2460},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288251},
	doi = {10.1109/JSAC.2023.3288251},
	timestamp = {Mon, 11 Sep 2023 14:51:37 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiSC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) has gained much traction due to its potential to manipulate the propagation environment via nearly-passive reconfigurable elements. In our previous work, we have analyzed and proposed a beyond diagonal RIS (BD-RIS) model, which is not limited to traditional diagonal phase shift matrices, to unify different RIS modes/architectures. In this paper, we create a new branch of BD-RIS supporting a multi-sector mode. A multi-sector BD-RIS is modeled as multiple antennas connected to a multi-port group-connected reconfigurable impedance network. More specifically, antennas are divided into\nL\n(\nL≥2\n) sectors and arranged as a polygon prism with each sector covering\n1/L\nspace. Different from the recently introduced concept of intelligent omni-surface (or simultaneously transmitting and reflecting RIS), the multi-sector BD-RIS not only achieves a full-space coverage, but also has significant performance gains thanks to the highly directional beam of each sector. We derive the constraint of the multi-sector BD-RIS and the corresponding channel model taking into account the relationship between antenna beamwidth and gain. With the proposed model, we first derive the scaling law of the received signal power for a multi-sector BD-RIS -assisted single-user system. We then propose efficient beamforming design algorithms to maximize the sum-rate of the multi-sector BD-RIS -assisted multiuser system. Simulation results verify the effectiveness of the proposed design and demonstrate the performance enhancement of the proposed multi-sector BD-RIS.}
}


@article{DBLP:journals/jsac/SubhashKEKAA23,
	author = {Athira Subhash and
                  Abla Kammoun and
                  Ahmed Elzanaty and
                  Sheetal Kalyani and
                  Yazan H. Al{-}Badarneh and
                  Mohamed{-}Slim Alouini},
	title = {Optimal Phase Shift Design for Fair Allocation in RIS-Aided Uplink
                  Network Using Statistical {CSI}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2461--2475},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288266},
	doi = {10.1109/JSAC.2023.3288266},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SubhashKEKAA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surfaces (RIS) can be crucial in next-generation communication systems. However, designing the RIS phases according to the instantaneous channel state information (CSI) can be challenging in practice due to the short coherent time of the channel. In this regard, we propose a novel algorithm based on the channel statistics of massive multiple input multiple output systems rather than the instantaneous CSI. The beamforming at the base station (BS), power allocation of the users, and phase shifts at the RIS elements are optimized to maximize the minimum signal-to-interference and noise ratio (SINR), guaranteeing fair operation among various users. In particular, we design the RIS phases by leveraging the asymptotic deterministic equivalent of the minimum SINR that depends only on the channel statistics. This significantly reduces the computational complexity and the amount of controlling data between the BS and RIS for updating the phases. This setup is also useful for electromagnetic fields (EMF)-aware systems with constraints on the maximum user’s exposure to EMF. The numerical results show that the proposed algorithms achieve more than 100 % gain in terms of minimum SINR, compared to a system with random RIS phase shifts, when 40 RIS elements, 20 antennas at the BS and 10 users, are considered.}
}


@article{DBLP:journals/jsac/ChenWGHZ23,
	author = {Yuanbin Chen and
                  Ying Wang and
                  Xufeng Guo and
                  Zhu Han and
                  Ping Zhang},
	title = {Location Tracking for Reconfigurable Intelligent Surfaces Aided Vehicle
                  Platoons: Diverse Sparsities Inspired Approaches},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2476--2496},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288262},
	doi = {10.1109/JSAC.2023.3288262},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenWGHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the employment of reconfigurable intelligent surfaces (RISs) into vehicle platoons, functioning in tandem with a base station (BS) in support of the high-precision location tracking. In particular, the use of a RIS imposes additional structured sparsity that, when paired with the initial sparse line-of-sight (LoS) channels of the BS, facilitates beneficial group sparsity. The resultant group sparsity significantly enriches the energies of the original direct-only channel, enabling a greater concentration of the LoS channel energies emanated from the same vehicle location index. Furthermore, the burst sparsity is exposed by representing the non-line-of-sight (NLoS) channels as their sparse copies. This thus constitutes the philosophy of the diverse sparsities of interest. Then, a diverse dynamic layered structured sparsity (DiLuS) framework is customized for capturing different priors for this pair of sparsities, based upon which the location tracking problem is formulated as a maximum a posterior (MAP) estimate of the location. Nevertheless, the tracking issue is highly intractable due to the ill-conditioned sensing matrix, intricately coupled latent variables associated with the BS and RIS, and the spatial-temporal correlations among the vehicle platoon. To circumvent these hurdles, we propose an efficient algorithm, namely DiLuS enabled spatial-temporal platoon localization (DiLuS-STPL), which incorporates both variational Bayesian inference (VBI) and message passing techniques for recursively achieving parameter updates in a turbo-like way. Finally, we demonstrate through extensive simulation results that the localization relying exclusively upon a BS and a RIS may achieve the comparable precision performance obtained by the two individual BSs, along with the robustness and superiority of our proposed algorithm as compared to various benchmark schemes.}
}


@article{DBLP:journals/jsac/ZhangYZ23,
	author = {Yunpu Zhang and
                  Changsheng You and
                  Beixiong Zheng},
	title = {Multi-Active Multi-Passive {(MAMP)-IRS} Aided Wireless Communication:
                  {A} Multi-Hop Beam Routing Design},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2497--2513},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288233},
	doi = {10.1109/JSAC.2023.3288233},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Prior studies on intelligent reflecting surface (IRS) have mostly considered wireless communication systems aided by a single passive IRS, which, however, has limited control over wireless propagation environment and suffers severe product-distance path-loss. To address these issues, we propose in this paper a new multi-active multi-passive (MAMP) -IRS aided wireless communication system, where a number of active and passive IRSs are deployed to assist the communication between a base station (BS) and a remote user in complex environment, by establishing a multi-hop reflection path across active and passive IRSs. In particular, the active IRSs enable to opportunistically amplify the reflected signal along the multi-reflection link, thus effectively compensating for the severe product-distance path-loss. For the new MAMP-IRS aided system, an optimization problem is formulated to maximize the achievable rate of a typical user by designing the active-and-passive IRS routing path as well as the joint beamforming of the BS and selected active/passive IRSs. To draw useful insights into the optimal design, we first consider a special case of the single-active multi-passive (SAMP) -IRS aided system. For this case, we propose an efficient algorithm to obtain its optimal solution by first optimizing the joint beamforming given any SAMP-IRS routing path, and then optimizing the routing path by using a new path decomposition method and graph theory. Moreover, we show that the active IRS should be selected to establish the beam routing path when its amplification power and/or number of active reflecting elements are sufficiently large. Next, for the general MAMP-IRS aided system, we show that its challenging beam routing optimization problem can be efficiently solved by a new two-phase approach. Its key idea is to first optimize the inner passive-IRS beam routing between each two active IRSs for effective channel power gain maximization, followed by an outer active-IRS beam routing optimization for rate maximization. Last, numerical results are provided to validate our analytical results and demonstrate the effectiveness of the proposed MAMP-IRS beam routing scheme as compared to various benchmark schemes.}
}


@article{DBLP:journals/jsac/ZhaoLCWCA23,
	author = {Bai Zhao and
                  Min Lin and
                  Ming Cheng and
                  Jun{-}Bo Wang and
                  Julian Cheng and
                  Mohamed{-}Slim Alouini},
	title = {Robust Downlink Transmission Design in IRS-Assisted Cognitive Satellite
                  and Terrestrial Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2514--2529},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288234},
	doi = {10.1109/JSAC.2023.3288234},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoLCWCA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cognitive satellite and terrestrial network (CSTN) is considered as a promising technology to provide ubiquitous connectivity for various users within wide-coverage. This paper proposes a robust downlink transmission scheme for multiple intelligent reflecting surfaces (IRSs) assisted CSTN. Here, the satellite network adopts multigroup multicast transmission scheme to serve many earth stations, while the terrestrial network exploits space division multiple access and multi-IRS-enhanced non-orthogonal multiple access technology to communicate with many terrestrial users. By assuming that these two networks share the same frequency band having only the angular information based imperfect channel state information of each user, we formulate an optimization problem to minimize the total transmit power subject to the constraints of quality-of-service requirement for each user, per-antenna transmit power budgets of satellite and BS, and unit-modulus requirement for each reflecting element. To tackle this mathematically intractable problem, we then employ angular discretization together with the successive convex approximation method to obtain the active beamforming (BF) vectors of satellite and BS, the passive BF vector of IRS, and the power allocation coefficients. Moreover, we propose a generalized zero forcing BF and alternative optimization to obtain the suboptimal solutions of the optimization problem with low computational complexity. Finally, simulation results are given to demonstrate the effectiveness and superiority of the proposed two schemes over the benchmarks.}
}


@article{DBLP:journals/jsac/RasilainenPBPS23,
	author = {Kimmo Rasilainen and
                  Duy Tung Phan and
                  Markus Berg and
                  Aarno P{\"{a}}rssinen and
                  Ping Jack Soh},
	title = {Hardware Aspects of Sub-THz Antennas and Reconfigurable Intelligent
                  Surfaces for 6G Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2530--2546},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288250},
	doi = {10.1109/JSAC.2023.3288250},
	timestamp = {Tue, 12 Sep 2023 07:57:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RasilainenPBPS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need for unrestricted, high-quality, and high-speed communications in planned sixth generation (6G) wireless systems drives the development and research towards the sub-terahertz (sub-THz) bands which so far have been relatively unused for wireless communications applications. Additionally, the sub-THz bands have gained an increasing interest as a potential spectral region at which to go even beyond the well-known Shannon limits. This review paper provides a technological overview on some of the key hardware aspects of sub-THz wireless communications (at 100–300GHz), namely antennas, reconfigurable intelligent surfaces (RISs), and reconfigurable antenna systems based on state-of-the-art technologies reported in recent literature. Different technologies of antennas and RISs are compared to understand their possibilities and limitations, and to identify the most promising technological approaches to transform 6G from a vision into a commercially viable solution. The paper also presents the authors’ interpretations of possible hardware and design trends that can shape the future research directions.}
}


@article{DBLP:journals/jsac/DuWNKXZS23,
	author = {Hongyang Du and
                  Jiacheng Wang and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Junshan Zhang and
                  Xuemin Shen},
	title = {Semantic Communications for Wireless Sensing: RIS-Aided Encoding and
                  Self-Supervised Decoding},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2547--2562},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288231},
	doi = {10.1109/JSAC.2023.3288231},
	timestamp = {Fri, 28 Jun 2024 14:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DuWNKXZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Semantic communications can reduce the resource consumption by transmitting task-related semantic information extracted from source messages. However, when the source messages are utilized for various tasks, e.g., wireless sensing data for localization and activities detection, semantic communication technique is difficult to be implemented because of the increased processing complexity. In this paper, we propose the inverse semantic communications as a new paradigm. Instead of extracting semantic information from messages, we aim to encode the task-related source messages into a hyper-source message for data transmission or storage. Following this paradigm, we design an inverse semantic-aware wireless sensing framework with three algorithms for data sampling, reconfigurable intelligent surface (RIS)-aided encoding, and self-supervised decoding, respectively. Specifically, on the one hand, we propose a novel RIS hardware design for encoding several signal spectrums into one MetaSpectrum. To select the task-related signal spectrums for achieving efficient encoding, a semantic hash sampling method is introduced. On the other hand, we propose a self-supervised learning method for decoding the MetaSpectrums to obtain the original signal spectrums. Using the sensing data collected from real-world, we show that our framework can reduce the data volume by 95% compared to that before encoding, without affecting the accomplishment of sensing tasks. Moreover, compared with the typically used uniform sampling scheme, the proposed semantic hash sampling scheme can achieve 67% lower mean squared error in recovering the sensing parameters. In addition, experiment results demonstrate that the amplitude response matrix of the RIS enables the encryption of the sensing data. The code for this paper is available at https://github.com/HongyangDu/SemSensing .}
}


@article{DBLP:journals/jsac/MuL23,
	author = {Xidong Mu and
                  Yuanwei Liu},
	title = {Exploiting Semantic Communication for Non-Orthogonal Multiple Access},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2563--2576},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288242},
	doi = {10.1109/JSAC.2023.3288242},
	timestamp = {Fri, 18 Aug 2023 08:46:14 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MuL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A novel semantics-empowered two-user uplink non-orthogonal multiple access (NOMA) framework is proposed for resource efficiency enhancement. More particularly, a secondary far user (F-user) employs the semantic communication (SemCom) while a primary near user (N-user) employs the conventional bit-based communication (BitCom). The fundamental performance limit, namely semantic-versus-bit (SvB) rate region, of the proposed semantics-empowered NOMA framework is characterized. The equivalent SvB rate region achieved by the conventional BitCom-based NOMA is provided as the baseline scheme. It unveils that, compared to BitCom, SemCom can significantly improve the F-user’s performance when its permitted transmit power is strictly capped, but may perform worse when its permitted transmit power is high. Guided by this result, the proposed semantics-empowered NOMA framework is investigated over fading channels. An opportunistic SemCom and BitCom scheme is proposed, which enables the secondary F-user to participate in NOMA via the most suitable communication method at each fading state, thus striking a good tradeoff between its own achieved performance and the interference imposed on the primary N-user. Two scenarios are considered for employing the opportunistic scheme, namely on-off resource management and continuous resource management. For each scenario, the optimal communication policy over fading channels is derived for maximizing the ergodic semantic rate achieved at the secondary F-user, subject to the minimum ergodic bit rate constraint of the primary N-user. Numerical results show that: 1) proposed opportunistic scheme in both scenarios can achieve higher communication performance for NOMA than the baseline schemes merely employing SemCom or BitCom; 2) SemCom can better guarantee the performance of the F-user admitted in NOMA than BitCom when the communication requirement of the primary N-user is high; and 3) continuous power control at the F-user is necessary for ensuring high performance over fading channels, while the on-off time scheduling is sufficient.}
}


@article{DBLP:journals/jsac/XieMDSTW23,
	author = {Songjie Xie and
                  Shuai Ma and
                  Ming Ding and
                  Yuanming Shi and
                  MingJian Tang and
                  Youlong Wu},
	title = {Robust Information Bottleneck for Task-Oriented Communication With
                  Digital Modulation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2577--2591},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288252},
	doi = {10.1109/JSAC.2023.3288252},
	timestamp = {Fri, 18 Aug 2023 08:46:15 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XieMDSTW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Task-oriented communications, mostly using learning-based joint source-channel coding (JSCC), aim to design a communication-efficient edge inference system by transmitting task-relevant information to the receiver. However, only transmitting task-relevant information without introducing any redundancy may cause robustness issues in learning due to the channel variations, and the JSCC which directly maps the source data into continuous channel input symbols poses compatibility issues on existing digital communication systems. In this paper, we address these two issues by first investigating the inherent tradeoff between the informativeness of the encoded representations and the robustness to information distortion in the received representations, and then propose a task-oriented communication scheme with digital modulation, named discrete task-oriented JSCC (DT-JSCC), where the transmitter encodes the features into a discrete representation and transmits it to the receiver with the digital modulation scheme. In the DT-JSCC scheme, we develop a robust encoding framework, named robust information bottleneck (RIB), to improve the communication robustness to the channel variations, and derive a tractable variational upper bound of the RIB objective function using the variational approximation to overcome the computational intractability of mutual information. The experimental results demonstrate that the proposed DT-JSCC achieves better inference performance than the baseline methods with low communication latency, and exhibits robustness to channel variations due to the applied RIB framework.}
}


@article{DBLP:journals/jsac/NanLZCCDZTHQ23,
	author = {Guoshun Nan and
                  Zhichun Li and
                  Jinli Zhai and
                  Qimei Cui and
                  Gong Chen and
                  Xin Du and
                  Xuefei Zhang and
                  Xiaofeng Tao and
                  Zhu Han and
                  Tony Q. S. Quek},
	title = {Physical-Layer Adversarial Robustness for Deep Learning-Based Semantic
                  Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2592--2608},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288249},
	doi = {10.1109/JSAC.2023.3288249},
	timestamp = {Fri, 18 Aug 2023 08:46:15 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/NanLZCCDZTHQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {End-to-end semantic communications (ESC) rely on deep neural networks (DNN) to boost communication efficiency by only transmitting the semantics of data, showing great potential for high-demand mobile applications. We argue that central to the success of ESC is the robust interpretation of conveyed semantics at the receiver side, especially for security-critical applications such as automatic driving and smart healthcare. However, robustifying semantic interpretation is challenging as ESC is extremely vulnerable to physical-layer adversarial attacks due to the openness of wireless channels and the fragileness of neural models. Toward ESC robustness in practice, we ask the following two questions: Q1: For attacks, is it possible to generate semantic-oriented physical-layer adversarial attacks that are imperceptible, input-agnostic and controllable? Q2: Can we develop a defense strategy against such semantic distortions and previously proposed adversaries? To this end, we first present MobileSC , a novel semantic communication framework that considers the computation and memory efficiency in wireless environments. Equipped with this framework, we propose SemAdv , a physical-layer adversarial perturbation generator that aims to craft semantic adversaries over the air with the abovementioned criteria, thus answering the Q1. To better characterize the real-world effects for robust training and evaluation, we further introduce a novel adversarial training method \\texttt {SemMixed}\nto harden the ESC against SemAdv attacks and existing strong threats, thus answering the Q2. Extensive experiments on three public benchmarks verify the effectiveness of our proposed methods against various physical adversarial attacks. We also show some interesting findings, e.g., our MobileSC can even be more robust than classical block-wise communication systems in the low SNR regime.}
}


@article{DBLP:journals/jsac/DaiWYTQSNZ23,
	author = {Jincheng Dai and
                  Sixian Wang and
                  Ke Yang and
                  Kailin Tan and
                  Xiaoqi Qin and
                  Zhongwei Si and
                  Kai Niu and
                  Ping Zhang},
	title = {Toward Adaptive Semantic Communications: Efficient Data Transmission
                  via Online Learned Nonlinear Transform Source-Channel Coding},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2609--2627},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288246},
	doi = {10.1109/JSAC.2023.3288246},
	timestamp = {Fri, 05 Jan 2024 14:17:39 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DaiWYTQSNZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging field semantic communication is driving the research of end-to-end data transmission. By utilizing the powerful representation ability of deep learning models, learned data transmission schemes have exhibited superior performance than the established source and channel coding methods. While, so far, research efforts mainly concentrated on architecture and model improvements toward a static target domain. Despite their successes, such learned models are still suboptimal due to the limitations in model capacity and imperfect optimization and generalization, particularly when the testing data distribution or channel response is different from that adopted for model training, as is likely to be the case in real-world. To tackle this, in this paper, we propose a novel online learned joint source and channel coding approach that leverages the deep learning model’s overfitting property. Specifically, we update the off-the-shelf pre-trained models after deployment in a lightweight online fashion to adapt to the distribution shifts in source data and environment domain. We take the overfitting concept to the extreme, proposing a series of implementation-friendly methods to adapt the codec model or representations to an individual data or channel state instance, which can further lead to substantial gains in terms of the end-to-end rate-distortion performance. Accordingly, the streaming ingredients include both the semantic representations of source data and the online updated decoder model parameters. The system design is formulated as a joint optimization problem whose goal is to minimize the loss function, a tripartite trade-off among the data stream bandwidth cost, model stream bandwidth cost, and end-to-end distortion. The proposed methods enable the communication-efficient adaptation for all parameters in the network without sacrificing decoding speed. Extensive experiments, including user study, on continually changing target source data and wireless channel environments, demonstrate the effectiveness and efficiency of our approach, on which we outperform existing state-of-the-art engineered transmission scheme (VVC combined with 5G LDPC coded transmission).}
}


@article{DBLP:journals/jsac/SunYCGSP23,
	author = {Lunan Sun and
                  Yang Yang and
                  Mingzhe Chen and
                  Caili Guo and
                  Walid Saad and
                  H. Vincent Poor},
	title = {Adaptive Information Bottleneck Guided Joint Source and Channel Coding
                  for Image Transmission},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2628--2644},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288238},
	doi = {10.1109/JSAC.2023.3288238},
	timestamp = {Fri, 18 Aug 2023 08:46:15 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SunYCGSP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Joint source and channel coding (JSCC) for image transmission has attracted increasing attention due to its robustness and high efficiency. However, the existing deep JSCC research mainly focuses on minimizing the distortion between the transmitted and received information under a fixed number of available channels. Therefore, the transmitted rate may be far more than its required minimum value. In this paper, an adaptive information bottleneck (IB) guided joint source and channel coding (AIB-JSCC) method is proposed for image transmission. The goal of AIB-JSCC is to reduce the transmission rate while improving the image reconstruction quality. In particular, a new IB objective for image transmission is proposed so as to minimize the distortion and the transmission rate. A mathematically tractable lower bound on the proposed objective is derived, and then, adopted as the loss function of AIB-JSCC. To trade off compression and reconstruction quality, an adaptive algorithm is proposed to adjust the hyperparameter of the proposed loss function dynamically according to the distortion during the training. Experimental results show that AIB-JSCC can significantly reduce the required amount of transmitted data and improve the reconstruction quality and downstream task accuracy.}
}


@article{DBLP:journals/jsac/ErdemirTDG23,
	author = {Ecenaz Erdemir and
                  Tze{-}Yang Tung and
                  Pier Luigi Dragotti and
                  Deniz G{\"{u}}nd{\"{u}}z},
	title = {Generative Joint Source-Channel Coding for Semantic Image Transmission},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2645--2657},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288243},
	doi = {10.1109/JSAC.2023.3288243},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ErdemirTDG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent works have shown that joint source-channel coding (JSCC) schemes using deep neural networks (DNNs), called DeepJSCC, provide promising results in wireless image transmission. However, these methods mostly focus on the distortion of the reconstructed signals with respect to the input image, rather than their perception by humans. However, focusing on traditional distortion metrics alone does not necessarily result in high perceptual quality, especially in extreme physical conditions, such as very low bandwidth compression ratio (BCR) and low signal-to-noise ratio (SNR) regimes. In this work, we propose two novel JSCC schemes that leverage the perceptual quality of deep generative models (DGMs) for wireless image transmission, namely InverseJSCC and GenerativeJSCC. While the former is an inverse problem approach to DeepJSCC, the latter is an end-to-end optimized JSCC scheme. In both, we optimize a weighted sum of mean squared error (MSE) and learned perceptual image patch similarity (LPIPS) losses, which capture more semantic similarities than other distortion metrics. InverseJSCC performs denoising on the distorted reconstructions of a DeepJSCC model by solving an inverse optimization problem using the pre-trained style-based generative adversarial network (StyleGAN). Our simulation results show that InverseJSCC significantly improves the state-of-the-art DeepJSCC in terms of perceptual quality in edge cases. In GenerativeJSCC, we carry out end-to-end training of an encoder and a StyleGAN-based decoder, and show that GenerativeJSCC significantly outperforms DeepJSCC both in terms of distortion and perceptual quality.}
}


@article{DBLP:journals/jsac/XieQL23,
	author = {Huiqiang Xie and
                  Zhijin Qin and
                  Geoffrey Ye Li},
	title = {Semantic Communication With Memory},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2658--2669},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288236},
	doi = {10.1109/JSAC.2023.3288236},
	timestamp = {Tue, 12 Sep 2023 07:57:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XieQL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While semantic communication succeeds in efficiently transmitting due to the strong capability to extract the essential semantic information, it is still far from the intelligent or human-like communications. In this paper, we introduce an essential component, memory, into semantic communications to mimic human communications. Particularly, we investigate a deep learning (DL) based semantic communication system with memory, named Mem-DeepSC, by considering the scenario question answer task. We exploit the universal Transformer based transceiver to extract the semantic information and introduce the memory module to process the context information. Moreover, we derive the relationship between the length of semantic signal and the channel noise to validate the possibility of dynamic transmission. Specially, we propose two dynamic transmission methods to enhance the transmission reliability as well as to reduce the communication overheads by masking some unessential elements, which are recognized through training the model with mutual information. Numerical results show that the proposed Mem-DeepSC is superior to benchmarks in terms of answer accuracy and transmission efficiency, i.e., number of transmitted symbols.}
}


@article{DBLP:journals/jsac/SeoPKCBK23,
	author = {Sejin Seo and
                  Jihong Park and
                  Seung{-}Woo Ko and
                  Jinho Choi and
                  Mehdi Bennis and
                  Seong{-}Lyun Kim},
	title = {Toward Semantic Communication Protocols: {A} Probabilistic Logic Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2670--2686},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288268},
	doi = {10.1109/JSAC.2023.3288268},
	timestamp = {Wed, 06 Mar 2024 14:13:34 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SeoPKCBK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Classical medium access control (MAC) protocols are interpretable, yet their task-agnostic control signaling messages (CMs) are ill-suited for emerging mission-critical applications. By contrast, neural network (NN) based protocol models (NPMs) learn to generate task-specific CMs, but their rationale and impact lack interpretability. To fill this void, in this article we propose, for the first time, a semantic protocol model (SPM) constructed by transforming an NPM into an interpretable symbolic graph written in the probabilistic logic programming language (ProbLog). This transformation is viable by extracting and merging common CMs and their connections, while treating the NPM as a CM generator. By extensive simulations, we corroborate that the SPM tightly approximates its original NPM while occupying only 0.02% memory. By leveraging its interpretability and memory-efficiency, we demonstrate several SPM-enabled applications such as SPM reconfiguration for collision-avoidance, as well as comparing different SPMs via semantic entropy calculation and storing multiple SPMs to cope with non-stationary environments.}
}


@article{DBLP:journals/jsac/LengerkeHCRF23,
	author = {Caspar von Lengerke and
                  Alexander Hefele and
                  Juan A. Cabrera and
                  Martin Reisslein and
                  Frank H. P. Fitzek},
	title = {Beyond the Bound: {A} New Performance Perspective for Identification
                  via Channels},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2687--2706},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288239},
	doi = {10.1109/JSAC.2023.3288239},
	timestamp = {Fri, 18 Aug 2023 08:46:15 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LengerkeHCRF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identification via channels (ID) is a goal-oriented (Post-Shannon) communications paradigm that verifies the matching of message (identity) pairs at source and sink. To date, ID research has focused on the upper bound\nλ\nfor the probability of a false-positive (FP) identity match, mainly through ID tagging codes that represent the identities through ID codeword sets consisting of position-tag tuples. We broaden the ID research scope by introducing novel ID performance metrics: the expected FP-error probability\np\nfp\n¯\n¯\n¯\n¯\n¯\n¯\nwhich considers distance properties of ID codeword sets in conjunction with the probability for selecting ID pairs, the threshold probabilities\np\nϵ\nthat characterize quantiles of FP-probabilities, and the distance tail uplift ratio DiTUR giving the fraction of ID pairs whose distance is increased above the minimum distance (which corresponds to\nλ\n). We define a No-Code (NC) approach that directly conducts the ID operations with the messages (identities) without any additional coding as a baseline for ID. We investigate a concatenated Reed-Solomon ID code and a Reed-Muller ID code, and find that they do not always yield advantages over using no ID code. We analytically characterize the reduction of error-prone ID pairs through sending multiple tags. Overall, our insights point to investigating the distance distribution of ID codes and to incorporating the ID pair distributions of real ID systems in future ID research.}
}


@article{DBLP:journals/jsac/CaleffiSC23,
	author = {Marcello Caleffi and
                  Kyrylo Simonov and
                  Angela Sara Cacciapuoti},
	title = {Beyond Shannon Limits: Quantum Communications Through Quantum Paths},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {8},
	pages = {2707--2724},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3288263},
	doi = {10.1109/JSAC.2023.3288263},
	timestamp = {Tue, 12 Sep 2023 07:57:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/CaleffiSC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A crucial step towards the 6th generation (6G) of networks would be a shift in communication paradigm beyond the limits of Shannon’s theory. In both classical and quantum Shannon’s information theory, communication channels are generally assumed to combine through classical trajectories, so that the associated network path traversed by the information carrier is well-defined. Counter-intuitively, quantum mechanics enables a quantum information carrier to propagate through a quantum path, i.e., through a path such that the causal order of the constituting communications channels becomes indefinite. Quantum paths exhibit astonishing features, such as providing non-null capacity even when no information can be sent through any classical path. In this paper, we study the quantum capacity achievable via a quantum path and establish upper and the lower bounds for it. Our findings reveal the substantial advantage achievable with a quantum path over any classical placements of communications channels in terms of ultimate achievable communication rates. Furthermore, we identify the region where a quantum path incontrovertibly outperforms the amount of transmissible information beyond the limits of conventional quantum Shannon’s theory, and we quantify this advantage over classical paths through a conservative estimate.}
}


@article{DBLP:journals/jsac/SmidaSFASC23,
	author = {Besma Smida and
                  Ashutosh Sabharwal and
                  G{\'{a}}bor Fodor and
                  George C. Alexandropoulos and
                  Himal A. Suraweera and
                  Chan{-}Byoung Chae},
	title = {Guest Editorial Full Duplex and its Applications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2725--2728},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3292659},
	doi = {10.1109/JSAC.2023.3292659},
	timestamp = {Mon, 11 Mar 2024 15:42:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SmidaSFASC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The capability of nodes to transmit and receive data simultaneously within the same frequency band, referred to as in-band FD, disrupts the conventional assumptions underlying wireless network design. This new feature enhances spectral efficiency and reduces latency, which are essential drivers in advancing next-generation networks. In the past few years, full-duplex (FD) has evolved from being a laboratory idea to being incorporated into telecommunications standards and proof of concepts. From 2010 to 2020, considerable research and development efforts were devoted to advancing FD wireless communications. By 2015, the cable modem industry had already implemented in-band FD technology to establish the DOCSIS 4.0 standard, enabling next-generation cable modems to operate in FD mode. By 2020, FD wireless products started to emerge in the market.}
}


@article{DBLP:journals/jsac/SmidaSFASC23a,
	author = {Besma Smida and
                  Ashutosh Sabharwal and
                  G{\'{a}}bor Fodor and
                  George C. Alexandropoulos and
                  Himal A. Suraweera and
                  Chan{-}Byoung Chae},
	title = {Full-Duplex Wireless for 6G: Progress Brings New Opportunities and
                  Challenges},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2729--2750},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287612},
	doi = {10.1109/JSAC.2023.3287612},
	timestamp = {Mon, 11 Mar 2024 15:42:29 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SmidaSFASC23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The use of in-band full-duplex (FD) enables nodes to simultaneously transmit and receive on the same frequency band, which challenges the traditional assumption in wireless network design. The full-duplex capability enhances spectral efficiency and decreases latency, which are two key drivers pushing the performance expectations of next-generation mobile networks. In less than ten years, in-band FD has advanced from being demonstrated in research labs to being implemented in standards, presenting new opportunities to utilize its foundational concepts. Some of the most significant opportunities include using FD to enable wireless networks to sense the physical environment, integrate sensing and communication applications, develop integrated access and backhaul solutions, and work with smart signal propagation environments powered by reconfigurable intelligent surfaces. However, these new opportunities also come with new challenges for large-scale commercial deployment of FD technology, such as managing self-interference, combating cross-link interference in multi-cell networks, and coexistence of dynamic time division duplex, subband FD and FD networks.}
}


@article{DBLP:journals/jsac/GuoHWM23,
	author = {Zijun Guo and
                  Zhangcheng Hao and
                  Yi{-}Wen Wu and
                  Zhuo{-}Wei Miao},
	title = {A Planar Shared-Aperture Co-Polarized Array Antenna With a High Isolation
                  for Millimeter-Wave Full Duplex Communication},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2751--2764},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287545},
	doi = {10.1109/JSAC.2023.3287545},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/GuoHWM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We provide the design of a co-polarized simultaneous transmit and receive (STAR) array antenna, aiming to address the issue of undesired isolation in the antenna domain of the millimeter-wave (mm-Wave) full-duplex co-polarized communication system. As the cornerstone for the design and development of the antenna, the guided-wave structures of the virtual-electric-wall (VEW)-based ridge substrate-integrated waveguide (RSIW) and ridge parallel plate waveguide (RPPW) are proposed and applied to the full-duplex antenna design. Compared with conventional RSIW, VEW-based RSIW has the merits of lower ohmic loss and releasing the fabrication tolerance in the array antenna. Additionally, on the basis of parallel plate waveguide (PPW), the proposed RPPW exhibits a Chebyshev filtering response. The radiating part of the proposed array employs a slot-pair array excited along orthogonal directions by using the preceding guided-wave structures. Among them, the RSIW array serves as the Tx antenna, and the RPPW array is regarded as the Rx antenna. To achieve co-polarized radiation patterns in the boresight direction, the substrate-integrated waveguide (SIW) feed network is integrated into the reverse side of the antenna, providing differential and common-mode excitations for the RSIW-excited and RPPW-fed arrays. Furthermore, the inter-port isolation is effectively ameliorated by inserting the mushroom-type metamaterial between linear arrays. As a proof-of-concept, a shared-aperture full-duplex array operating in the ${Q}$ -band is developed and fabricated to demonstrate the above concept by using standard printed circuited board technology. The measured overlapped impedance bandwidth covers from 38.18 GHz to 39.13 GHz. The measured Tx-to-Rx isolation is greater than 25.5dB in the Q-band. The measured peak gains in Tx- and Rx-mode are 18.4 dBi and 17.8 dBi. The simulation and measurement results show good consistency, verifying the effectiveness of the proposed array in inter-port isolation, array size, planarization, and lightweight, making it suitable for mm-Wave shared-aperture full-duplex communication systems.}
}


@article{DBLP:journals/jsac/AskarK23,
	author = {Ramez Askar and
                  Wilhelm Keusgen},
	title = {Lossless Decoupling Networks for {RF} Self-Interference Cancellation
                  in {MIMO} Full-Duplex Transceivers},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2765--2779},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287544},
	doi = {10.1109/JSAC.2023.3287544},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AskarK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The article discusses a radio-frequency-based self-interference cancellation (SIC) technique to handle the self-interference signal segment associated with the self-interference caused by antennas’ mutual coupling, particularly for dedicated-transmit-and-receive antenna configurations. The technique is a novel SIC technique based on antenna decoupling utilizing a lossless network in the radio frequency domain. At first, the authors present a fundamental physical and mathematical description of the self-interference radio channel, employing antenna scattering matrix representation and spherical vector wave expansion. Then, the article presents the experimental results of two antenna mutual coupling measurements – including a single-input-single-output setup and a 2\\times 2\ndual-polarized multiple-input-multiple-output (MIMO) setup. Afterward, the authors discuss the lossless network decoupling technique for MIMO full-duplex wireless transceivers. Finally, the authors present a generalized- \\Pi\nsynthesizing topology of the lossless decoupling network and empirically evaluate its SIC performance by leveraging the conducted antenna mutual coupling measurement results. The empirical evaluation validates the effectiveness of the lossless decoupling network in canceling the self-interference entirely for a specific design frequency – narrowband systems. Moreover, the evaluation reveals the degradation of the network SIC performance with respect to system bandwidth and the deviation of antennas’ separation distance from its nominal design value.}
}


@article{DBLP:journals/jsac/RobertsCNVA23,
	author = {Ian P. Roberts and
                  Aditya Chopra and
                  Thomas David Novlan and
                  Sriram Vishwanath and
                  Jeffrey G. Andrews},
	title = {Spatial and Statistical Modeling of Multi-Panel Millimeter Wave Self-Interference},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2780--2795},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3292958},
	doi = {10.1109/JSAC.2023.3292958},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/RobertsCNVA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Characterizing self-interference is essential to the design and evaluation of in-band full-duplex communication systems. Until now, little has been understood about this coupling in full-duplex systems operating at millimeter wave (mmWave) frequencies, and it has been shown that the highly-idealized models proposed for such do not align with practice. This work presents the first spatial and statistical model of mmWave self-interference backed by measurements, enabling engineers to draw realizations that exhibit the large-scale and small-scale spatial characteristics observed in our nearly 6.5 million measurements taken at 28 GHz. Core to our model is its use of system and model parameters having real-world meaning, which facilitates its extension to systems beyond our own phased array platform through proper parameterization. We demonstrate this by collecting nearly 13 million additional measurements to show that our model can generalize to two other system configurations. We assess our model by comparing it against actual measurements to confirm its ability to align spatially and in distribution with real-world self-interference. In addition, using both measurements and our model of self-interference, we evaluate an existing beamforming-based full-duplex mmWave solution to illustrate that our model can be reliably used to design new solutions and validate the performance improvements they may offer.}
}


@article{DBLP:journals/jsac/MorgensternRHMALB23,
	author = {Carl W. Morgenstern and
                  Yu Rong and
                  Andrew Herschfelt and
                  Alyosha C. Molnar and
                  Alyssa B. Apsel and
                  David G. Landon and
                  Daniel W. Bliss},
	title = {Analog-Domain Self-Interference Cancellation for Practical Multi-Tap
                  Full-Duplex System: Theory, Modeling, and Algorithm},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2796--2807},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287608},
	doi = {10.1109/JSAC.2023.3287608},
	timestamp = {Tue, 12 Sep 2023 07:57:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MorgensternRHMALB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Practical, in-band, full-duplex (IBFD) systems typically require more than 100 dB of self-interference cancellation (SIC). Digital processing alone is insufficient for achieving this target, which drives us towards supplementary analog mitigation techniques. We propose an analog-domain, self-interference cancellation circuit to enable pass-band, analog SIC in an IBFD system. Analog SIC is limited by several hardware constraints and design choices, including finite tap-delay resolution, non-negative tap constraints, and bit precision quantization. We characterize the performance impact of each of these limitations as a function of signal bandwidth, carrier frequency, bit precision, and other system design parameters. We further characterize the achievable system performance under all of these limitations combined. We simulate several realistic examples to illustrate the relationship between the achievable self-mitigation performance and various system design choices. We implement a simple constrained optimization algorithm informed by these results to optimize the tap-delay weights of the analog circuit under these system constraints. We simulate the achievable mitigation performance and demonstrate as much as 45 dB of analog-domain, self-interference mitigation of a wide-band signal with realistic system configurations.}
}


@article{DBLP:journals/jsac/KolodziejJSP23,
	author = {Kenneth E. Kolodziej and
                  Brian A. Janice and
                  Adrienne I. Sands and
                  Bradley T. Perry},
	title = {Scalable In-Band Full-Duplex Phased Arrays: Complexity Reduction and
                  Distributed Processing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2808--2820},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287543},
	doi = {10.1109/JSAC.2023.3287543},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/KolodziejJSP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Compared to traditional omnidirectional systems, in-band full-duplex (IBFD) phased arrays provide higher antenna gains and offer the ability to electronically steer beams spatially to improve communications/radar links and enable multifunction operation. While a variety of approaches have successfully incorporated self-interference cancellation (SIC) into arrays, their techniques do not scale beyond the limited sizes demonstrated. In addition to providing an overview on state-of-the-art IBFD arrays, this paper presents a novel complexity reduction method that uniquely applies infinite impulse response (IIR) filter modeling for the purpose of compensating for the dispersion effects of the array hardware. Using measured statistical performance, this process is demonstrated for a scalable array design covering 2700 to 3500 MHz, and exhibits a 10x reduction in computations for an array with 1000 elements. Additionally, the distribution of on-array SIC adaptive processing is discussed for aperture-level IBFD designs, including the use of cache-oblivious algorithms that can help facilitate real-time processing and lead to the realization of large-scale IBFD arrays.}
}


@article{DBLP:journals/jsac/ChengCSFE23,
	author = {Jen{-}Hao Cheng and
                  Tien{-}Min Chang and
                  Chung{-}An Shen and
                  Mohammed E. Fouda and
                  Ahmed M. Eltawil},
	title = {High-Throughput Independent Component Analysis Processor for Full
                  Duplex Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2821--2832},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287614},
	doi = {10.1109/JSAC.2023.3287614},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChengCSFE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the algorithm and very-large-scale integration (VLSI) architecture of a high-throughput and highly efficient independent component analysis (ICA) processor for self-interference cancellation (SIC) in in-band full-duplex (IBFD) systems. This is the first VLSI architecture reported in the literature based on the state-of-the-art entropy bound minimization (EBM) approach. A novel ICA algorithm is presented in this paper with momentum gradient descent optimization. Simulation results show that the number of iterations for the proposed algorithm is significantly reduced compared to the conventional ICA algorithms. Furthermore, a novel early-distribution estimation scheme is proposed in the designed ICA processor to compute multiple distribution functions with low latency and low complexity. The processing flow and the efficiency for the hardware utilization are specifically designed so that the processing speed is maximized with minimum employment of hardware components. The proposed ICA processor is designed and implemented based on the application-specific-integrated circuit (ASIC) flow. The post-layout estimations show that compared with the conventional EBM-based scheme, the proposed design improves the throughput and efficiency by 30x. In addition, compared to prior designs shown in the literature, the proposed ICA processor also demonstrates a significant enhancement in terms of throughput and efficiency.}
}


@article{DBLP:journals/jsac/MohammadiVNM23,
	author = {Mohammadali Mohammadi and
                  Tung Thanh Vu and
                  Hien Quoc Ngo and
                  Michail Matthaiou},
	title = {Network-Assisted Full-Duplex Cell-Free Massive {MIMO:} Spectral and
                  Energy Efficiencies},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2833--2851},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287613},
	doi = {10.1109/JSAC.2023.3287613},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MohammadiVNM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider network-assisted full-duplex (NAFD) cell-free massive multiple-input multiple-output (CF-mMIMO) systems, where full-duplex (FD) transmission is virtually realized via half-duplex (HD) hardware devices. The HD access points (APs) operating in uplink (UL) mode and those operating in downlink (DL) mode simultaneously serve DL and UL user equipments (UEs) in the same frequency bands. We comprehensively analyze the performance of NAFD CF-mMIMO from both a spectral efficiency (SE) and energy efficiency (EE) perspectives. Specifically, we propose a joint optimization approach that designs the AP mode assignment, power control, and large-scale fading (LSFD) weights to improve the sum SE and EE of NAFD CF-mMIMO systems. We formulate two mixed-integer nonconvex optimization problems of maximizing the sum SE and EE, under realistic power consumption models, and the constraints on minimum individual SE requirements, maximum transmit power at each DL AP and UL UE. The challenging formulated problems are transformed into tractable forms and two novel algorithms are proposed to solve them using successive convex approximation techniques. More importantly, our approach can be applied to jointly optimize power control and LSFD weights for maximizing the sum SE and EE of HD and FD CF-mMIMO systems, which, to date, has not been studied. Numerical results show that: (a) our joint optimization approach significantly outperforms the heuristic approaches in terms of both sum SE and EE; (b) in CF-mMIMO systems, the NAFD scheme can provide approximately 30% SE gains, while achieving a remarkable EE gain of up to 200% compared with the HD and FD schemes.}
}


@article{DBLP:journals/jsac/FawazLHK23,
	author = {Hassan Fawaz and
                  Samer Lahoud and
                  Melhem El Helou and
                  Kinda Khawam},
	title = {Queue-Aware Resource Allocation in Full-Duplex Multi-Cellular Wireless
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2852--2863},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287541},
	doi = {10.1109/JSAC.2023.3287541},
	timestamp = {Sat, 30 Sep 2023 10:20:13 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/FawazLHK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we aim to tackle the challenges of resource block scheduling and power allocation in the context of multi-cell full-duplex wireless networks. This is a more realistic setting than the single cell scenario, and it better envisions how full-duplex wireless communications could eventually be implemented. We propose an optimal queue-aware joint scheduling and power allocation algorithm for full-duplex wireless networks in a multi-cell scenario. Because of its mathematical intractability, we decouple the problem and solve it for scheduling first, and for power allocation second. We consider both indoor and outdoor scenarios and show that the gains of multi-cell full-duplex wireless networks, with respect to their half-duplex counterparts, are not always prevalent. Furthermore, we highlight the importance of inter-cell cooperation when it comes to scheduling resources and show that depending on the scenario at hand, interference mitigation from inter-cell cooperation can improve the performance of user equipment in terms of throughput and waiting delay. Finally, we show that power allocation can improve user equipment throughput with its efficiency being tied to the deployment scenario at hand.}
}


@article{DBLP:journals/jsac/AlkhrijahCR23,
	author = {Yazeed Alkhrijah and
                  Joseph Camp and
                  Dinesh Rajan},
	title = {Multi-Band Full Duplex {MAC} Protocol {(MB-FDMAC)}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2864--2878},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287546},
	doi = {10.1109/JSAC.2023.3287546},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/AlkhrijahCR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we propose a multi-band medium access control (MAC) protocol for an infrastructure-based network with an access point (AP) that supports In-Band full-duplex (IBFD) and multiuser transmission to multi-band-enabled stations. The Multi-Band Full Duplex MAC (MB-FDMAC) protocol mainly uses the sub-6 GHz band for control-frame exchange, transmitted at the lowest rate per IEEE 802.11 standards, and uses the 60 GHz band, which has significantly higher instantaneous bandwidth, exclusively for data-frame exchange. We also propose a selection method that ensures fairness among uplink and downlink stations. Our result shows that MB-FDMAC effectively improves the spectral efficiency in the mmWave band by 324%, 234%, and 189% compared with state-of-the-art MAC protocols. In addition, MB-FDMAC significantly outperforms the combined throughput of sub-6 GHz and 60 GHz IBFD multiuser MIMO networks that operate independently by more than 85%. In addition, we study multiple network variables such as the number of stations in the network, the percentage of mmWave band stations, the size of the contention stage, and the selection method on MB-FDMAC by evaluating the change in the throughput, packet delay, and fairness among stations. Finally, we propose a method to improve the utilization of the high bandwidth of the mmWave band by incorporating time duplexing into MB-FDMAC, which we show can enhance the fairness by 12.5 % and significantly reduces packet delay by 80%.}
}


@article{DBLP:journals/jsac/SkouroumounisK23,
	author = {Christodoulos Skouroumounis and
                  Ioannis Krikidis},
	title = {Fluid Antenna-Aided Full Duplex Communications: {A} Macroscopic Point-of-View},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2879--2892},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3292959},
	doi = {10.1109/JSAC.2023.3292959},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/SkouroumounisK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The synergy of fluid-based reconfigurable antenna (FA) technology and full-duplex (FD) communications can be jointly beneficial, as FD can enhance the spectral efficiency of a point-to-point link, while the new degree of freedom offered by the FA technology can be exploited to handle the overall interference. Hence, in this paper, an analytical framework based on stochastic geometry is developed, aiming to assess both the outage and average sum-rate performance of large-scale FA-aided FD cellular networks. In contrast to existing studies, where perfect channel state information is assumed, the developed framework accurately captures the impact of channel estimation (CE) on the performance of the considered network deployments, as well as the existence of residual loop-interference (LI) at the FD transceivers. Particularly, we focus on a limited coherence interval scenario, where a novel sequential linear minimum-mean-squared-error-based CE method is performed for all FA ports and LI links, followed by data reception from the port with the strongest estimated channel. By using stochastic geometry tools, analytical expressions for the outage and the average sum-rate performance are derived. Our results reveal that FA-aided FD communications experience an improved average sum-rate performance of around 45% compared to conventional FD communications.}
}


@article{DBLP:journals/jsac/YuQLSSPLZKHLJCS23,
	author = {Bin Yu and
                  Chen Qian and
                  Juho Lee and
                  Shihai Shao and
                  Ying Shen and
                  Wensheng Pan and
                  Peng Lin and
                  Zhiya Zhang and
                  Sundo Kim and
                  Su Hu and
                  Kwonjong Lee and
                  Jungsoo Jung and
                  Sunghyun Choi and
                  Chengjun Sun},
	title = {Realizing High Power Full Duplex in Millimeter Wave System: Design,
                  Prototype and Results},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2893--2906},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287609},
	doi = {10.1109/JSAC.2023.3287609},
	timestamp = {Wed, 10 Apr 2024 16:20:18 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/YuQLSSPLZKHLJCS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Full duplex (FD) communication is considered as a promising technology in the development of 5G-advanced and 6G systems as it theoretically doubles the capability of channel. However, this improvement relying on a simultaneously bidirectional manner of communication induces intrinsic self-interference (SI) demanding to be fully cancelled, which is generally intractable. This challenge is minimized in the scenario of integrated access and backhaul (IAB) networks operated in millimeter wave (mmW) band, as its transceivers are stationary and the complexity of SI is greatly reduced by beamforming technique. As a matter of fact, FD can be a pioneering technique to unlock the full potential mmW-based IAB network by releasing its suffering of the half-duplex inefficiency. This article presents the design principle and validation of a practical SI cancellation (SIC) technique in the case of high transmission power class in mmW-based IAB networks. The proposed technique sequentially eliminates SI from the spatial, RF, and digital domains that reduces the SI down to the noise floor. To validate the technique, a prototype system is developed in accordance with 5G IAB specifications and field tests are conducted. Results suggest that the designed mmW SIC transceiver significantly reduces residual-interference to noise ratio (R-INR) to 2.1 dB or less. Additionally, a system-level simulation is conducted in line with 5G IAB evaluation methodology, which explores the potential performance gain of the proposed technique in presence of cross-link interference (CLI). Results indicate that the method could yield a cell throughput gain of about 84%, compared to the current time division duplex deployment.}
}


@article{DBLP:journals/jsac/LiuALC23,
	author = {Ziang Liu and
                  Sundar Aditya and
                  Hongyu Li and
                  Bruno Clerckx},
	title = {Joint Transmit and Receive Beamforming Design in Full-Duplex Integrated
                  Sensing and Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2907--2919},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287542},
	doi = {10.1109/JSAC.2023.3287542},
	timestamp = {Mon, 11 Sep 2023 14:51:37 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiuALC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated sensing and communication (ISAC) has been envisioned as a solution to realize the sensing capability required for emerging applications in wireless networks. For a mono-static ISAC transceiver, as signal transmission durations are typically much longer than the radar echo round-trip times, the radar returns are drowned by the strong residual self interference (SI) from the transmitter, despite adopting sufficient SI cancellation techniques before digital domain - a phenomenon termed the echo-miss problem. A promising approach to tackle this problem involves the ISAC transceiver to be full-duplex (FD), and in this paper we jointly design the transmit and receive beamformers at the transceiver, transmit precoder at the uplink user, and receive combiner at the downlink user to simultaneously 1) maximize the uplink and downlink communication rate; 2) maximize the transmit and receive radar beampattern power at the target; and 3) suppress the residual SI. To solve this optimization problem, we proposed a penalty-based iterative algorithm. Numerical results illustrate that the proposed design can effectively achieve up to 60 dB digital-domain SI cancellation, a higher average sum-rate, and more accurate radar parameter estimation compared with previous ISAC FD studies.}
}


@article{DBLP:journals/jsac/HeXSNEY23,
	author = {Zhenyao He and
                  Wei Xu and
                  Hong Shen and
                  Derrick Wing Kwan Ng and
                  Yonina C. Eldar and
                  Xiaohu You},
	title = {Full-Duplex Communication for {ISAC:} Joint Beamforming and Power
                  Optimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2920--2936},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287540},
	doi = {10.1109/JSAC.2023.3287540},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/HeXSNEY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Beamforming design has been widely investigated for integrated sensing and communication (ISAC) systems with full-duplex (FD) sensing and half-duplex (HD) communication, where the base station (BS) transmits and receives radar sensing signals simultaneously while the integrated communication operates in either downlink or uplink. To achieve higher spectral efficiency, in this paper, we extend existing ISAC beamforming design to a general case by considering the FD capability for both radar and communication. Specifically, we consider an FD ISAC system, where the BS performs target detection and communicates with multiple downlink users and uplink users reusing the same time and frequency resources. We jointly optimize the downlink dual-functional transmit signal and the uplink receive beamformers at the BS and the transmit power at the uplink users. The problems are formulated under two criteria: power consumption minimization and sum rate maximization. The downlink and uplink transmissions are tightly coupled due to both the desired target echo and the undesired interference received at the BS, making the problems challenging. To handle these issues in both cases, we first determine the optimal receive beamformers in closed forms with respect to the BS transmit beamforming and the user transmit power. Subsequently, we invoke these results to obtain equivalent optimization problems and propose iterative algorithms to solve them. In addition, we consider a special case under the power minimization criterion and propose an alternative low complexity design. Numerical results demonstrate that the optimized FD communication-based ISAC brings tremendous improvements in terms of both power efficiency and spectral efficiency compared to the conventional ISAC with HD communication.}
}


@article{DBLP:journals/jsac/MarinBR23,
	author = {Jaakko Marin and
                  Micael Bernhardt and
                  Taneli Riihonen},
	title = {Full-Duplex Constant-Envelope Jamceiver and Self-Interference Suppression
                  by Highpass Filter: Experimental Validation for Wi-Fi Security},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2937--2950},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287615},
	doi = {10.1109/JSAC.2023.3287615},
	timestamp = {Tue, 12 Sep 2023 07:57:28 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/MarinBR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unauthorized access to data has been a recognized risk of wireless systems for many decades. While security solutions in communications engineering have typically revolved around cryptography in the higher layers, a semi-recent development is the elevating interest into security in the physical layer, namely by utilizing jamming for protection. In this paper, we present an experimental study into a full-duplex jammer–receiver (i.e., “jamceiver”) that is able to simultaneously interfere with the same radio resources it is actively receiving from. The radio architecture is loosely based on frequency-modulated continuous-wave radars that are constant-envelope radio transceivers, which benefit from simple-but-efficient self-interference suppression in the analog baseband domain by using a passive highpass filter. Its limitation to constant-envelope transmission is not an issue for efficient jamming waveforms unlike it would be with conventional direct-conversion transceivers in full-duplex communications. To show the performance limits of a practical jamceiver, we present comprehensive measurement results from a laboratory environment as well as a jamming case study from an open park area with actual Wi-Fi signals. Especially, the experiments validate the feasibility of preventing eavesdropping with continuous low-power jamming in a large area around a full-duplex jamceiver that acts as an access point for simultaneously offering decent Wi-Fi service to an off-the-shelf laptop.}
}


@article{DBLP:journals/jsac/LuoGR23,
	author = {Haifeng Luo and
                  Navneet Garg and
                  Tharmalingam Ratnarajah},
	title = {A Channel Frequency Response-Based Secret Key Generation Scheme in
                  In-Band Full-Duplex {MIMO-OFDM} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2951--2965},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287610},
	doi = {10.1109/JSAC.2023.3287610},
	timestamp = {Thu, 31 Aug 2023 19:51:25 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LuoGR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Physical layer-based secret key generation (PHY-SKG) schemes have attracted significant attention in recent years due to their lightweight implementation and ability to achieve information-theoretical security. In this paper, we study a channel frequency response (CFR)-based SKG scheme for in-band full-duplex (IBFD)-multi-input and multi-output (MIMO) systems. We formulate the intrinsic practical imperfections and derive their effects on the probing errors. Then we derive closed-form expressions for the secret key capacity (SKC) in the presence of a passive eavesdropper accordingly. We analyze the asymptotic behavior of the SKC in the high-SNR regime and reveal the fundamental limits for IBFD and HD probing. Based on the asymptotic SKC, we investigate the conditions under which IBFD can outperform HD. Numerical results illustrate that effective analog self-interference cancellation (ASIC) depth is the basis for IBFD probing to gain benefits over HD. Finally, we analyze the properties of the collected samples of the CFR-based SKG scheme and propose an averaging pre-processing and a segmental quantization, which reduce the key disagreement rate and remove the effects of large-scale fading to guarantee randomness. 3GPP specification-based simulations and the National Institute of Standards and Technology (NIST) test suite verify the theoretical analysis and the effectiveness of the proposed SKG scheme.}
}


@article{DBLP:journals/jsac/ZamanKDSW23,
	author = {Fakhar Zaman and
                  Uman Khalid and
                  Trung Q. Duong and
                  Hyundong Shin and
                  Moe Z. Win},
	title = {Quantum Full-Duplex Communication},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2966--2980},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287611},
	doi = {10.1109/JSAC.2023.3287611},
	timestamp = {Tue, 07 May 2024 20:20:17 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZamanKDSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating the full-duplex capability with quantum communication potentially equips emerging wireless networks with a quantum layer of security for the stringent communication efficiency and security requirements. This paper proposes two new full-duplex quantum communication protocols to exchange classical or quantum information between two remote parties simultaneously without transferring a physical particle over the quantum channel. The first protocol, called quantum duplex coding, enables the exchange of a classical bit using a preshared maximally entangled pair of qubits by means of counterfactual disentanglement. The second protocol, called quantum telexchanging, enables the exchange of an arbitrary unknown qubit without using preshared entanglement by means of counterfactual entanglement and disentanglement. We demonstrate that quantum duplex coding and quantum telexchanging can be achieved by exploiting counterfactual electron-photon interaction gates. It is shown that these tasks can be viewed as full-duplex transmission of bits and qubits via binary erasure channels and quantum erasure channels, respectively.}
}


@article{DBLP:journals/jsac/DuWNKXK23,
	author = {Hongyang Du and
                  Jiacheng Wang and
                  Dusit Niyato and
                  Jiawen Kang and
                  Zehui Xiong and
                  Dong In Kim},
	title = {AI-Generated Incentive Mechanism and Full-Duplex Semantic Communications
                  for Information Sharing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {9},
	pages = {2981--2997},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3287547},
	doi = {10.1109/JSAC.2023.3287547},
	timestamp = {Fri, 28 Jun 2024 14:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/DuWNKXK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The next generation of Internet services, such as Metaverse, rely on mixed reality (MR) technology to provide immersive user experiences. However, limited computation power of MR headset-mounted devices (HMDs) hinders the deployment of such services. Therefore, we propose an efficient information-sharing scheme based on full-duplex device-to-device (D2D) semantic communications to address this issue. Our approach enables users to avoid heavy and repetitive computational tasks, such as artificial intelligence-generated content (AIGC) in the view images of all MR users. Specifically, a user can transmit the generated content and semantic information extracted from their view image to nearby users, who can then use this information to obtain the spatial matching of computation results under their view images. We analyze the performance of full-duplex D2D communications, including the achievable rate and bit error probability, by using generalized small-scale fading models. To facilitate semantic information sharing among users, we design a contract theoretic AI-generated incentive mechanism. The proposed diffusion model generates the optimal contract design, outperforming two deep reinforcement learning algorithms, i.e., proximal policy optimization and soft actor-critic algorithms. Our numerical analysis experiment proves the effectiveness of our proposed methods. The code for this paper is available at https://github.com/HongyangDu/SemSharing .}
}


@article{DBLP:journals/jsac/MumtazCGREMXA23,
	author = {Shahid Mumtaz and
                  Soumaya Cherkaoui and
                  Mohsen Guizani and
                  Joel J. P. C. Rodrigues and
                  Abdulmotaleb El{-}Saddik and
                  Sabita Maharjan and
                  Yang Xiao and
                  Ikram Ashraf},
	title = {Guest Editorial Digital Twins for Mobile Networks - Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3001--3007},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313016},
	doi = {10.1109/JSAC.2023.3313016},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MumtazCGREMXA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twins (DTs), defined as the virtual representation of a real-world entity or system, act as a mirror to provide a way to simulate, predict physical behaviors, and possibly control the real-world entity where applicable. Originating in the industry, advances in computing capacity and recent progress in artificial intelligence (AI)-based analytics make DTs attractive to a broader set of use cases including mobile networks.}
}


@article{DBLP:journals/jsac/VanHuynhNKKD23,
	author = {Dang Van{-}Huynh and
                  Van{-}Dinh Nguyen and
                  Saeed R. Khosravirad and
                  George K. Karagiannidis and
                  Trung Q. Duong},
	title = {Distributed Communication and Computation Resource Management for
                  Digital Twin-Aided Edge Computing With Short-Packet Communications},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3008--3021},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310087},
	doi = {10.1109/JSAC.2023.3310087},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/VanHuynhNKKD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For future networks, it is highly demanding to satisfy a wide range of time-sensitive and computation-intensive services. This is a very challenging task, since it requires a combination of aspects from information, communication and computation in order to establish a digital representation of the real network environment. This paper introduces a fairness-aware latency minimisation (FALM) framework in the digital twin (DT) aided edge computing with ultra-reliable and low latency communications (URLLC), which jointly optimises various communication and computation parameters, namely, bandwidth allocation, transmission power, task offloading portions, and processing rate of user equipments (UEs) and edge servers (ESs). The formulated problem is highly complicated, due to non-convex constraints and strong coupling among optimisation variables. To deal with this problem, we develop both centralised and distributed optimisation approaches. In particular, we first resort to successive convex approximation (SCA) method to develop a low-complexity iterative algorithm and solve the problem in a centralised manner. Combining tools from SCA and alternating direction method of multipliers (ADMM), we develop an efficient distributed solution with parallel computation processing at ESs under global consensus in each iteration and strong theoretical performance guaranteed. Numerical results are provided to validate the proposed solutions in terms of convergence speed and overall latency as well as improving fairness among all UEs.}
}


@article{DBLP:journals/jsac/HaoWHGHC23,
	author = {Yixue Hao and
                  Jiaxi Wang and
                  Dongkun Huo and
                  Nadra Guizani and
                  Long Hu and
                  Min Chen},
	title = {Digital Twin-Assisted URLLC-Enabled Task Offloading in Mobile Edge
                  Network via Robust Combinatorial Optimization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3022--3033},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310051},
	doi = {10.1109/JSAC.2023.3310051},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HaoWHGHC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin (DT)-assisted mobile edge network can achieve energy-efficient task offloading by optimizing the decision-making in real time. Although many DT-assisted task offloading solutions in mobile edge networks have been designed, stochastic asynchronizations between the DTs and physical entities are still ignored. In this paper, we investigate a task offloading problem in a DT-assisted URLLC-enabled mobile edge network which considered the uncertain deviation between DT estimated values and physical actual values. Specifically, we formulate a latency and energy consumption minimization problem by optimizing task offloading, resource allocation, and power management. To solve this problem, we propose a DT-assisted robust task offloading scheme (DTRTO) based on learning composed of decision and deviation networks. The deviation network predicts the worst-case deviations based on the pre-decision, and the decision network optimize the decision considered the worst-case deviation. The simulation results show that, compared to the baseline algorithms, the DTRTO scheme can realize low latency and energy consumption in task offloading while maintaining high robustness.}
}


@article{DBLP:journals/jsac/ZhangHM23,
	author = {Yongchao Zhang and
                  Jia Hu and
                  Geyong Min},
	title = {Digital Twin-Driven Intelligent Task Offloading for Collaborative
                  Mobile Edge Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3034--3045},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310058},
	doi = {10.1109/JSAC.2023.3310058},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangHM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative mobile edge computing (MEC) is a new paradigm that allows cooperative peer offloading among distributed MEC servers to balance their computing workloads. However, the highly dynamic workloads and wireless network conditions pose great challenges to achieving efficient task offloading in collaborative MEC. To address this challenge, digital twin (DT) has emerged as one promising solution by building a high-fidelity virtual mirror of the physical MEC to simulate its behaviors and help make optimal operational decisions. In this paper, we propose a DT-driven intelligent task offloading framework for collaborative MEC, where DT is employed to map the collaborative MEC system into a virtual space and optimize the task offloading decisions. We model the task offloading process as a Markov decision process (MDP) with the objective of maximizing the MEC system’s total income from providing computing services, and then develop a deep reinforcement learning (DRL)-based intelligent task offloading scheme (INTO) to jointly optimize the peer offloading and resource allocation decisions. An efficient action refinement method is proposed to ensure that the action selected by the DRL agent is feasible. Experimental results show that our proposed approach can effectively adapt the task offloading decisions according to the dynamic environment, and significantly improve the MEC system’s income through extensive comparison with three state-of-the-art algorithms.}
}


@article{DBLP:journals/jsac/CaoLLLH23,
	author = {Bin Cao and
                  Ziming Li and
                  Xin Liu and
                  Zhihan Lv and
                  Hua He},
	title = {Mobility-Aware Multiobjective Task Offloading for Vehicular Edge Computing
                  in Digital Twin Environment},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3046--3055},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310100},
	doi = {10.1109/JSAC.2023.3310100},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CaoLLLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In vehicular edge computing (VEC), vehicle users (VUs) can offload their computation-intensive tasks to edge server (ES) that provides additional computation resources. Due to the edge server being closer to VUs, the propagation delay between the ESs and the VUs is lower compared to cloud computing. Applying digital twin to VEC allows for low-cost trial in task offloading. In real-word, the mobility of VUs cannot be ignored and the downlink delay in receiving process results from ES is related to the mobility of VUs. Therefore, a five-objective optimization model including downlink delay, computation delay, energy consumption, load balancing, and user satisfaction of the VUs is constructed. To solve the above model, an improved CMA-ES algorithm based on the guiding point (GP-CMA-ES) is proposed. When the number of VUs increases, the dimension of variables also increases. Therefore, a convergence-related variable grouping strategy based on the relationship detection between variables and objectives is proposed. The performance of algorithm GP-CMA-ES is compared with five algorithms in the digital twin environment.}
}


@article{DBLP:journals/jsac/XuTYZK23,
	author = {Chi Xu and
                  Zixuan Tang and
                  Haibin Yu and
                  Peng Zeng and
                  Linghe Kong},
	title = {Digital Twin-Driven Collaborative Scheduling for Heterogeneous Task
                  and Edge-End Resource via Multi-Agent Deep Reinforcement Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3056--3069},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310066},
	doi = {10.1109/JSAC.2023.3310066},
	timestamp = {Fri, 08 Mar 2024 13:21:35 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XuTYZK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the interdisciplinary advances of mobile communication and edge computing, massive heterogeneous tasks are accessing wireless networks and competing for the edge-end computing and communication resources. Digital twin (DT), which establishes the digital models of physical objects for simulation, analysis and optimization, provides a promising method for network scheduling and management. This paper proposes a DT-driven edge-end collaborative scheduling algorithm for heterogeneous tasks and heterogeneous computing/communication resources. Specifically, multiple end devices (EDs) cooperate with each other to accomplish a complex job, where each ED can offload individual task to multiple edge servers (ESs) for parallel computing. By fully considering deadline requirements of heterogeneous tasks, maximum computing capabilities of ESs and EDs, computing resource estimation deviations of DT, maximum transmit powers of EDs and tolerable peak interference powers to coexisting EDs, we formulate a job completion time minimization problem to jointly optimize the edge-end task division, transmit power control, computing resource type matching and allocation. To solve this non-convex problem, we first reformulate it by multi-agent Markov decision process, where a compound reward leveraging latency reward and deadline reward according to the task criticality is designed. Then, we propose a multi-agent deep reinforcement learning-based scheduling algorithm, where Actor-Critic framework with estimation and target networks is designed for policy and value iterations. Meanwhile, a step-by-step $\\epsilon $ -greedy algorithm is proposed to balance exploration and exploitation, avoiding local optimal trap. Through offline centralized training by DT and online distributed execution by EDs, we realize edge-end collaborative computing for heterogeneous tasks. Experimental results demonstrate that, comparing with typical benchmark algorithms, the proposed algorithm converges with the highest reward and achieves the smallest job completion time, where the deadlines of heterogeneous tasks can be well satisfied respectively.}
}


@article{DBLP:journals/jsac/GuoTK23,
	author = {Qi Guo and
                  Fengxiao Tang and
                  Nei Kato},
	title = {Resource Allocation for Aerial Assisted Digital Twin Edge Mobile Network},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3070--3079},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310065},
	doi = {10.1109/JSAC.2023.3310065},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GuoTK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of the 5G/6G mobile network, high levels of requirements such as ultra-high data transmission rate, support for the high mobility node and seamless connection need to be handled. Additionally, ensuring user quality of service (QoS) in high-density and high-traffic mobile networks presents a significant challenge. Unmanned aerial vehicles (UAVs) have emerged as key components in providing flexible assistance in aerial spaces. To further enhance the network performance in dynamic and heterogeneous environments, an intelligent resource allocation strategy with low communication overhead is essential. In this paper, we construct a UAV-assisted mobile network to provide efficient communication for all mobile users in high-density and high-traffic environments, at the same time, a digital twin-empowered dynamic resource allocation strategy based on online training with low communication overhead is proposed. Our proposal employs digital twin-empowered multi-task learning to meet various resource allocation requirements for different node types. Moreover, we propose a deep-Q network-based reinforcement learning mechanism with experience replay memory to execute resource allocation decisions based on evaluated rewards. The simulation results show that the proposal achieves significant network performance compared with baseline algorithms.}
}


@article{DBLP:journals/jsac/JiaW23,
	author = {Pengyi Jia and
                  Xianbin Wang},
	title = {A New Virtual Network Topology-Based Digital Twin for Spatial-Temporal
                  Load-Balanced User Association in 6G HetNets},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3080--3094},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310104},
	doi = {10.1109/JSAC.2023.3310104},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JiaW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dynamically associating distributed mobile users with proper base stations in 6G heterogeneous networks (HetNets) becomes critical to achieve both diverse quality of service (QoS) requirements of all users and entire network performance. However, the significantly increased complexity of matching the irregularly distributed users and base stations as well as highly dynamic network traffic often cause unbalanced spatial-temporal loads for multi-tier base stations during user association. To overcome this challenge, we propose a new virtual network topology-based digital twin to reduce the complexity of load-balanced user association in 6G HetNets. During the digital twin construction stage, instead of using highly dynamic low-level physical layer attributes (e.g., channel conditions and SINR), we intentionally consider more stable and relevant communication performance indicators and physical statistics to effectively reflect both real-time link quality and overall network dynamics. To assist overall network operation, fast update of the digital twin for HetNets is achieved by adopting principal component analysis to discover specific network areas with changes. To improve the overall QoS provisioning and network performance, the proposed virtual topology-based digital twin is further utilized to predict the spatial-temporal dynamics of HetNets for more balanced user association by bipartite graph matching. Simulation results show that the proposed method can construct effective digital twins and support load-balanced user association with maximized network-wide QoS satisfaction.}
}


@article{DBLP:journals/jsac/GuoZWLB23,
	author = {Hongzhi Guo and
                  Xiaoyi Zhou and
                  Jiadai Wang and
                  Jiajia Liu and
                  Abderrahim Benslimane},
	title = {Intelligent Task Offloading and Resource Allocation in Digital Twin
                  Based Aerial Computing Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3095--3110},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310067},
	doi = {10.1109/JSAC.2023.3310067},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GuoZWLB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To meet the future demands for ubiquitous communication coverage and temporary / unexpected computing resources, aerial computing networks have been envisioned as a new paradigm. Nevertheless, dynamic changes on the network make it particularly challenging to achieve global optimal resource allocation. As an emerging technology, digital twin (DT) can represent real objects in physical network by creating virtual models. With the help of DT, we can easily obtain comprehensive real-world high-fidelity state information for model training, so as to achieve intelligent efficient decision-making. Accordingly, DT-based aerial computing networks have emerged as a potential solution. Note that available researches mostly assumed simple ground user distribution like uniform distribution, and adopted binary / partial offloading in task processing, neglecting the task separability and data inter-dependency among subtasks. Toward this end, we introduce DT into aerial computing networks, and study the problem of intelligent UAV deployment and resource allocation. Specifically, we firstly propose a DT-assisted UAV deployment strategy and model the data inter-dependency among subtasks. After that, two DT-assisted hybrid (binary and partial) task offloading schemes are presented, i.e., heuristic greedy and DQN-based schemes. Extensive analysis and numerical results confirm the effectiveness of our proposed DT-assisted UAV deployment and hybrid task offloading strategies.}
}


@article{DBLP:journals/jsac/ZhangMLLS23,
	author = {Haijun Zhang and
                  Xu Ma and
                  Xiangnan Liu and
                  Linpei Li and
                  Kai Sun},
	title = {GNN-Based Power Allocation and User Association in Digital Twin Network
                  for the Terahertz Band},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3111--3121},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313192},
	doi = {10.1109/JSAC.2023.3313192},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangMLLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The digital twin (DT) and terahertz (THz) wireless communication technologies have promoted the innovative development and application of 6G networks. Combining DT can obtain efficient, collaborative, and intelligent management for THz wireless networks. However, the conflicts between large amounts of twin data and limited network resources make it difficult to improve the performance of DT networks. In this paper, a DT architecture for THz wireless networks is proposed, which maps a physical network in the THz band into a virtual DT network and represents the DT network as a graph structure. Furthermore, the THz channel model is provided, and the resource management problem with weighted mean rate as the optimization objective is proposed, which is transformed into a graph optimization problem. Based on this, a distributed message propagation algorithm is proposed, which uses the graph neural network to provide a solution. Simulation results show that the proposed scheme improves the weighted mean rate of the DT network for the THz band and outperforms the benchmark methods. It is also proved that the proposed distributed message propagation algorithm is scalable and can maintain good performance under different conditions.}
}


@article{DBLP:journals/jsac/LiaoZJSTRF23,
	author = {Haijun Liao and
                  Zhenyu Zhou and
                  Zehan Jia and
                  Yiling Shu and
                  Muhammad Tariq and
                  Jonathan Rodriguez and
                  Valerio Frascolla},
	title = {Ultra-Low AoI Digital Twin-Assisted Resource Allocation for Multi-Mode
                  Power IoT in Distribution Grid Energy Management},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3122--3132},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310101},
	doi = {10.1109/JSAC.2023.3310101},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiaoZJSTRF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of information (AoI) is an important metric of information timeliness, which determines digital twin (DT) consistency and energy management precision. However, AoI guarantee in the time-averaged sense is unreliable to avoid the occurrence of extreme event. In this paper, we propose a novel information timeliness metric named ultra-low AoI (ULAoI). Compared with AoI, ULAoI further considers the occurrence of extreme event and higher-order statistical characteristics of excess AoI value. Multi-dimensional resources of power internet of things (PIoT) are jointly allocated to achieve ULAoI guarantee from the perspective of sensing-communication-control integration. ULAoI-DT-Prioritized deep Q network (DQN) is proposed to achieve coordinated resource allocation by approximating unobservable information with the assistance of ULAoI-DT, and preventing DQN training from using samples with large AoI based on ULAoI-induced priority. Simulation results demonstrate the superior performance of the proposed algorithm in global loss function, ULAoI guarantee, and energy management optimality.}
}


@article{DBLP:journals/jsac/ZhengDZX23,
	author = {Zhigao Zheng and
                  Bo Du and
                  Chen Zhao and
                  Peichen Xie},
	title = {Path Merging Based Betweenness Centrality Algorithm in Delay Tolerant
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3133--3145},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310071},
	doi = {10.1109/JSAC.2023.3310071},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhengDZX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Delay Tolerant Network (DTN) is a widely used network in computer network and wireless network, there are no permanent end-to-end connections between source and destination nodes (vertices). Betweenness centrality (BC) is used to find the key nodes (vertices) of DTNs, and there are kinds of implementations of the BC algorithm for DTNs. However, most recent algorithms in BC computation suffer from the problem of high auxiliary memory consumption. To reduce BC computing’s memory consumption, we propose a path-merging-based algorithm called Galliot to calculate the BC values using GPU, which aims to minimize the on-board memory consumption and enable the BC computation of large-scale graphs on GPU. The proposed algorithm requires \\mathcal {O}(n)\nspace and runs in \\mathcal {O}(mn)\ntime on unweighted graphs. We present the theoretical principle for the proposed path merging method. Moreover, we propose a locality-oriented policy to maintain and update the worklist to improve GPU data locality. In addition, we conducted extensive experiments on NVIDIA GPUs to show the performance of Galliot. The results show that Galliot can process the larger graphs, which have 11.32\\times\nmore vertices and 5.67\\times\nmore edges than the graphs that recent works can process. Moreover, Galliot can achieve up to 38.77\\times\nspeedup over the existing methods.}
}


@article{DBLP:journals/jsac/RuahSA23,
	author = {Clement Ruah and
                  Osvaldo Simeone and
                  Bashir M. Al{-}Hashimi},
	title = {A Bayesian Framework for Digital Twin-Based Control, Monitoring, and
                  Data Collection in Wireless Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3146--3160},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310093},
	doi = {10.1109/JSAC.2023.3310093},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/RuahSA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Commonly adopted in the manufacturing and aerospace sectors, digital twin (DT) platforms are increasingly seen as a promising paradigm to control, monitor, and analyze software-based, “open”, communication systems that are expected to dominate 6G deployments. Notably, DT platforms provide a sandbox in which to test artificial intelligence (AI) solutions for communication systems, potentially reducing the need to collect data and test algorithms in the field, i.e., on the physical twin (PT). A key challenge in the deployment of DT systems is to ensure that virtual control optimization, monitoring, and analysis at the DT are safe and reliable, avoiding incorrect decisions caused by “model exploitation”. To address this challenge, this paper presents a general Bayesian framework with the aim of quantifying and accounting for model uncertainty at the DT that is caused by limitations in the amount and quality of data available at the DT from the PT. In the proposed framework, the DT builds a Bayesian model of the communication system, which is leveraged to enable core DT functionalities such as control via multi-agent reinforcement learning (MARL), monitoring of the PT for anomaly detection, prediction, data-collection optimization, and counterfactual analysis. To exemplify the application of the proposed framework, we specifically investigate a case-study system encompassing multiple sensing devices that report to a common receiver. Experimental results validate the effectiveness of the proposed Bayesian framework as compared to standard frequentist model-based solutions.}
}


@article{DBLP:journals/jsac/QiXWNYZ23,
	author = {Lianyong Qi and
                  Xiaolong Xu and
                  Xiaotong Wu and
                  Qiang Ni and
                  Yuan Yuan and
                  Xuyun Zhang},
	title = {Digital-Twin-Enabled 6G Mobile Network Video Streaming Using Mobile
                  Crowdsourcing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3161--3174},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310077},
	doi = {10.1109/JSAC.2023.3310077},
	timestamp = {Wed, 08 Nov 2023 07:52:21 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QiXWNYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital-twin-enabled cloud-centric architecture is a promising evolution trend of sixth generation (6G) network, which brings new opportunities and challenges for mobile video streaming-related services requiring the exponentially increasing traffic demands. Device-to-Device (D2D) communication paradigm is an attractive technique to alleviate the problem. However, the previous research work on D2D built on individuals’ random mobility or position snapshot and cannot guarantee the stable communication flow. In this paper, we leverage the cybertwin as a centric controller and take advantages of crowdsourcing technology to attract mobile users to follow the specified path and share their network resources with other users. The design of the specified path is formulated as a problem of user recruitment optimization with cost constraint, which is a NP-Hard problem. Firstly, we investigate a special case of only one mobile user to offer the network resource and present a pseudo-polynomial time algorithm. Secondly, we present a graph-partition-based approach to solve the more complex case of multiple mobile users. Thirdly, we discuss the least expected budget to achieve the maximum utility in an ideal model. Fourthly, we perform extensive experiments to evaluate and compare the performance with the typical ones in simulated digital-twin-enabled 6G networks.}
}


@article{DBLP:journals/jsac/CuiLNJ23,
	author = {Yingping Cui and
                  Tiejun Lv and
                  Wei Ni and
                  Abbas Jamalipour},
	title = {Digital Twin-Aided Learning for Managing Reconfigurable Intelligent
                  Surface-Assisted, Uplink, User-Centric Cell-Free Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3175--3190},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310050},
	doi = {10.1109/JSAC.2023.3310050},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CuiLNJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper puts forth a new, reconfigurable intelligent surface (RIS)-assisted, uplink, user-centric cell-free (UCCF) system managed with the assistance of a digital twin (DT). Specifically, we propose a novel learning framework that maximizes the sum-rate by jointly optimizing the access point and user association (AUA), power control, and RIS beamforming. This problem is challenging and has never been addressed due to its prohibitively large and complex solution space. Our framework decouples the AUA from the power control and RIS beamforming (PCRB) based on the different natures of their variables, hence reducing the solution space. A new position-adaptive binary particle swarm optimization (PABPSO) method is designed for the AUA. Two twin-delayed deep deterministic policy gradient (TD3) models with new and refined state pre-processing layers are developed for the PCRB. Another important aspect is that a DT is leveraged to train the learning framework with its replay of channel estimates stored. The AUA, power control, and RIS beamforming are only tested in the physical environment at the end of selected epochs. Simulations show that using RISs contributes to considerable increases in the sum-rate of UCCF systems, and the DT dramatically reduces overhead with marginal performance loss. The proposed framework is superior to its alternatives in terms of sum-rate and convergence stability.}
}


@article{DBLP:journals/jsac/ZhouZCSLYYSW23,
	author = {Xiaokang Zhou and
                  Xuzhe Zheng and
                  Xuesong Cui and
                  Jiashuai Shi and
                  Wei Liang and
                  Zheng Yan and
                  Laurence T. Yang and
                  Shohei Shimizu and
                  Kevin I{-}Kai Wang},
	title = {Digital Twin Enhanced Federated Reinforcement Learning With Lightweight
                  Knowledge Distillation in Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3191--3211},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310046},
	doi = {10.1109/JSAC.2023.3310046},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhouZCSLYYSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The high-speed mobile networks offer great potentials to many future intelligent applications, such as autonomous vehicles in smart transportation systems. Such networks provide the possibility to interconnect mobile devices to achieve fast knowledge sharing for efficient collaborative learning and operations, especially with the help of distributed machine learning, e.g., Federated Learning (FL), and modern digital technologies, e.g., Digital Twin (DT) systems. Typically, FL requires a fixed group of participants that have Independent and Identically Distributed (IID) data for accurate and stable model training, which is highly unlikely in real-world mobile network scenarios. In this paper, in order to facilitate the lightweight model training and real-time processing in high-speed mobile networks, we design and introduce an end-edge-cloud structured three-layer Federated Reinforcement Learning (FRL) framework, incorporated with an edge-cloud structured DT system. A dual-Reinforcement Learning (dual-RL) scheme is devised to support optimizations of client node selection and global aggregation frequency during FL via a cooperative decision-making strategy, which is assisted by a two-layer DT system deployed in the edge-cloud for real-time monitoring of mobile devices and environment changes. A model pruning and federated bidirectional distillation (Bi-distillation) mechanism is then developed locally for the lightweight model training, while a model splitting scheme with a lightweight data augmentation mechanism is developed globally to separately optimize the aggregation weights based on a splitted neural network structure (i.e., the encoder and classifier) in a more targeted manner, which can work together to effectively reduce the overall communication cost and improve the non-IID problem. Experiment and evaluation results compared with three baseline methods using two different real-world datasets demonstrate the usefulness and outstanding performance of our proposed FRL model in communication-efficient model training and non-IID issue alleviation for high-speed mobile network scenarios.}
}


@article{DBLP:journals/jsac/ChenHBYMLZ23,
	author = {Xiangyi Chen and
                  Guangjie Han and
                  Yuanguo Bi and
                  Zimeng Yuan and
                  Mahesh K. Marina and
                  Yufei Liu and
                  Hai Zhao},
	title = {Traffic Prediction-Assisted Federated Deep Reinforcement Learning
                  for Service Migration in Digital Twins-Enabled {MEC} Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3212--3229},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310047},
	doi = {10.1109/JSAC.2023.3310047},
	timestamp = {Thu, 21 Mar 2024 16:25:33 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenHBYMLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Mobile Edge Computing (MEC) networks, dynamic service migration can support service continuity and reduce user-perceived delay. However, service migration in MEC networks faces significant challenges due to the uncertainty in future traffic demands, the distributed architecture of MEC networks, high operating costs and the dynamism of network resources. Digital Twins (DT), which achieve the mapping of physical entities to virtual digital models in cyberspace, provide new perspectives for intelligent and efficient service provisioning in MEC networks. In this paper, we propose a traffic prediction-assisted federated deep reinforcement learning scheme to efficiently migrate services and improve the cost efficiency of DT-enabled MEC networks. Specifically, to address the coupled spatio-temporal dependencies of mobile traffic and the imbalance in traffic data, a Multi-order Spatio-temporal information integration-based distributed Traffic Prediction (MSTP) scheme is proposed, which achieves high-accuracy mobile traffic prediction at a low cost. Then, we propose a Federated Cooperative cost-efficient Service Migration (FCSM) algorithm that adaptively adjusts service migration strategies in a distributed manner to respond to future traffic demands. Moreover, a theoretical model is developed to analyze the convergence of FCSM and derive the upper bound of the time-average squared gradient norm. Finally, extensive simulations demonstrate that the proposed schemes achieve excellent traffic prediction performance, enhance users’ Quality of Service (QoS), and significantly reduce the system cost of MEC networks.}
}


@article{DBLP:journals/jsac/MuOHYCJ23,
	author = {Junsheng Mu and
                  Wenjiang Ouyang and
                  Tao Hong and
                  Weijie Yuan and
                  Yuanhao Cui and
                  Zexuan Jing},
	title = {Digital Twins-Enabled Federated Learning in Mobile Networks: From
                  the Perspective of Communication-Assisted Sensing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3230--3241},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310082},
	doi = {10.1109/JSAC.2023.3310082},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MuOHYCJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the continuous evolution of emerging technologies such as mobile network, machine learning (ML), 5G, etc., digital twins (DT) bursts out great potential by its capacity of data analysis, data tracking, data prediction, etc, building a bridge between the physical and information world. Meanwhile, mobile network is moving towards data-driven paradigm, the issue of data privacy and data security seem to be a bottleneck. As a result, federated learning (FL) and mobile network are deeply converging. However, the mobile network is time-varying and the parameters of FL-empowered mobile network is huge and continue to increase with exponential growth of wireless terminals, result in the failure of traditional modeling. In the mobile networks, DT is conducive to prototyping, testing, and optimization, enabling mobile networks to be modelled more efficiently in a virtual environment and thus providing guidance for practical application. To this end, a communication-assisted sensing scenario is considered in this paper with FL in DT-empowered mobile networks. More specifically, two communication-assisted sensing architectures are proposed to improve communication efficiency of mobile network, namely, centralized architecture of federated transfer learning (FTL) and decentralized architecture of FTL. For centralized architecture of FTL, feature extraction of sensing information is conducted by FL between partial nodes and central server while the remaining nodes are used to train the fully connected layers at the central server. Considering data safety during the communication between sensing nodes, a decentralized architecture is designed based on FTL and Blockchain, where the feature extraction module is obtained by the fusion of sharing model (by Blockchain) and local model. The performance of proposed schemes is evaluated and demonstrated by the simulations.}
}


@article{DBLP:journals/jsac/HuFHZ23,
	author = {He{-}Xuan Hu and
                  Yi Feng and
                  Qiang Hu and
                  Ye Zhang},
	title = {A Masked One-Dimensional Convolutional Autoencoder for Bearing Fault
                  Diagnosis Based on Digital Twin Enabled Industrial Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3242--3253},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310098},
	doi = {10.1109/JSAC.2023.3310098},
	timestamp = {Fri, 16 Feb 2024 14:06:11 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuFHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bearings are the core component of mechanical equipment. The health status of bearings is the key to the stable operation of the system. Bearing fault diagnosis model can discover damaged bearings in time, which has a large economic value for enterprises. The previous bearings fault diagnosis model suffers from problems such as small fault data and unrepresentative features, which leads to poor model generalization performance. Therefore, in this work, we propose a masked one-dimensional convolutional autoencoder (MOCAE) for bearing fault diagnosis based on digital twin enabled industrial internet of things (IIoT). The model monitors the bearing data using a set of IIoT platforms. The digital twin technology is used to build a digital twin model of the bearing device, and the parameters of the digital twin model are trained by the fault data obtained from the IIoT platform. The trained digital twin model can then simulate whether the bearing is faulty. In this digital twin model, MOCAE model is proposed for diagnosing faulty bearing signals. The MOCAE model first extracts the features from the time series signal of the bearing using a one-dimensional convolutional autoencoder, which can enhance the reconstruction ability of hidden features to make them more representative. Next, the MOCAE model automatically extracts the feature information contained in the time series signal data by self-training in order to reduce the dependence on the labeled data. The comprehensive experimental results on real bearing datasets show the superiority of the MOCAE model.}
}


@article{DBLP:journals/jsac/XuWPLV23,
	author = {Hansong Xu and
                  Jun Wu and
                  Qianqian Pan and
                  Xing Liu and
                  Christos V. Verikoukis},
	title = {Digital Twin and Meta {RL} Empowered Fast-Adaptation of Joint User
                  Scheduling and Task Offloading for Mobile Industrial IoT},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3254--3266},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310081},
	doi = {10.1109/JSAC.2023.3310081},
	timestamp = {Wed, 17 Jul 2024 16:21:23 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XuWPLV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The industrial Internet of Things (IoT) system is integrated with the emerging artificial intelligence (AI) paradigms to empower industrial automation and self-evolving capabilities. AI-driven resource allocation across cyber-physical domains for mobile industrial IoT must consider its fundamental requirements and key characteristics such as high reliability, low latency, and environmental dynamics. The challenge is twofold. Industrial systems are fault-sensitive, which makes them intolerable of trial-and-error-based learning and optimization approaches. In addition, learning models cannot adapt to changing industrial IoT environment with dynamic communication noise and machinery disturbances. In this paper, we propose joint optimization for the nonorthogonal multiple access (NOMA) and multi-tier hybrid cloud-edge computing empowered industrial IoT that results in improved utilization of communication and computing resources. Second, we establish the fine-grained digital twin for industrial IoT (DT-IIoT) to simulate the changing industrial environment to support trial-and-error-based safe learning. Third, we leverage meta reinforcement learning (meta RL) to improve the generalization and fast adaptation of the learning models for DT-IIoT. Finally, the feasibility and efficiency of these schemes are evaluated through extensive experiments.}
}


@article{DBLP:journals/jsac/SunLLZ23,
	author = {Le Sun and
                  Chenyang Li and
                  Bo Liu and
                  Yanchun Zhang},
	title = {Class-Driven Graph Attention Network for Multi-Label Time Series Classification
                  in Mobile Health Digital Twins},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3267--3278},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310064},
	doi = {10.1109/JSAC.2023.3310064},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SunLLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital Twins for Mobile Networks (DTMN) can enhance mobile health (mHealth) by increasing diagnostic and monitoring capabilities. Classifying multi-label time series mHealth data in DTMN is challenging due to complex class relevance and feature extraction difficulties. This paper proposes a Class-Driven Graph Attention network learning framework (C-DGAM) for Multi-label classification of mHealth data in DTMN. C-DGAM captures the complex class relationships by constructing a unique class relevance graph for each time series. It uses a temporal context attention module to generates class representation vectors by fusing multi-dimensional features of time and class. Then, it dynamically models different relevance among the class representation vectors through a dynamic graph attention module which improves the performance of multi-label time series classification while maintaining a smaller parameter size and lower computational complexity. The mean Average Precision achieved by C-DGAM on two different multi-label time series datasets are 0.955 and 0.776, respectively, with corresponding F1 scores of 0.867 and 0.80. It demonstrates leading performance compared to existing state-of-art works. It provides more accurate and generalized algorithmic support for DMTN systems.}
}


@article{DBLP:journals/jsac/LiSDWZZWLCC23,
	author = {Bohan Li and
                  Xinyang Song and
                  Tianlun Dai and
                  Wenlong Wu and
                  Di Zhu and
                  Xiangping Bryce Zhai and
                  Hao Wen and
                  Qinyong Lin and
                  Huazhou Chen and
                  Ken Cai},
	title = {Trust Management Strategy for Digital Twins in Vehicular Ad Hoc Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3279--3292},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310070},
	doi = {10.1109/JSAC.2023.3310070},
	timestamp = {Mon, 08 Apr 2024 20:42:04 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/LiSDWZZWLCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an essential part of mobile networks, vehicular ad hoc networks (VANETs) are beneficial to the improvement of traffic efficiency and safety through real-time information sharing between vehicles. Digital Twins (DT) have been utilized to facilitate the design, testing, and deployment of VANETs. However, constructing Digital Twins still faces interference from malicious vehicles. Despite most vehicles following communication rules honestly, the reliability and authenticity of traffic messages cannot be guaranteed due to the network’s openness and vulnerability. Meanwhile, vehicles may suffer tracking attacks during the interaction without an effective privacy-preserving method, leading to the leakage of sensitive data. To address these issues, a decentralized trust management scheme embedded with blockchain that considers identity authentication is proposed to detect malicious DT-vehicles. In our method, each vehicle in the Digital Twin of VANETs (DT-VANETs) is equipped with a certificate recorded on the blockchain as a legal identity, which is also served as a pseudonym for security during message transmission. The trustworthiness of the vehicle is evaluated based on direct trust and recommendation trust. Direct interaction between vehicles consists of message authenticity verification and active detection, which are the basis of direct trust calculation. For other vehicles, these direct trust opinions are treated as second-hand information to obtain recommendation trust. Unreliable recommendations are filtered by our proposed RTF algorithm, further resisting cooperation attacks. Vehicles judged to be malicious will have their certificates revoked and removed from DT-VANETs, providing a guarantee for the establishment of trust in DT-VANETs. Experimental results show that the proposed scheme can effectively resist malicious attacks in DT-VANETs.}
}


@article{DBLP:journals/jsac/ZhangXLTKKN23,
	author = {Tao Zhang and
                  Changqiao Xu and
                  Yibo Lian and
                  Haijiang Tian and
                  Jiawen Kang and
                  Xiaohui Kuang and
                  Dusit Niyato},
	title = {When Moving Target Defense Meets Attack Prediction in Digital Twins:
                  {A} Convolutional and Hierarchical Reinforcement Learning Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3293--3305},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310072},
	doi = {10.1109/JSAC.2023.3310072},
	timestamp = {Fri, 28 Jun 2024 14:57:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ZhangXLTKKN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With rapid development of emerging technologies for Internet of Things (IoT), digital twins (DT) have been proposed to support a wide variety of applications. A mobile network is expected to be integrated with DT to form a DT mobile network (DTMN). Unfortunately, DTMN still faces security threats, which have attracted great research attention. Current defense mechanisms are mostly static, i.e., responding after attacks happening. To solve the aforementioned problem, moving target defense (MTD) has been proposed as an innovative solution. However, there exist three major challenges when applying MTD into DTMN. Firstly, less emphasis was paid to collaborative scheduling between multiple MTD schemes, which can improve the security of DTMN. Secondly, MTD schemes require lots of network resources, but few works focus on the time allocation of multiple MTD schemes to reduce network resource consumption. Thirdly, existing defense strategies only rely on current information, but do not consider future information. In this paper, we propose a collaborative mutation-based MTD (CM-MTD) in DTMN. We mainly consider two MTD schemes called host address mutation (HAM) and route mutation (RM), respectively, which adjust network properties and invalidate different stages of cyber kill chain. We firstly formulate a semi-Markov decision process (SMDP) to model time-varying security events and dynamic deployment of multiple MTD schemes. Then, security events are predicted by long short-term memory (LSTM), which are regarded as network states in SMDP. Next, infeasible actions that do not satisfy network constraints will be removed from the action space of the SMDP. Lastly, we design a hierarchical deep reinforcement learning algorithm for collaborative scheduling. Simulation results highlight the effectiveness of CM-MTD compared with baseline solutions.}
}


@article{DBLP:journals/jsac/XiongQHY23,
	author = {Hu Xiong and
                  Zheng Qu and
                  Xin Huang and
                  Kuo{-}Hui Yeh},
	title = {Revocable and Unbounded Attribute-Based Encryption Scheme With Adaptive
                  Security for Integrating Digital Twins in Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3306--3317},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310076},
	doi = {10.1109/JSAC.2023.3310076},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XiongQHY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoTs) has been a burgeoning field that transforms the ubiquitous objects to interconnected devices and intelligent system. Today, with the emerging of innovative technologies such as cloud computing, the IoT sector is in a race to leverage these novel technologies to achieve optimal performance. Naturally the Digital Twins (DTs) architecture acts as an indispensable intermediary bridge to couple the IoT domain with these lastest technologies together. However, a tremendous obstacle is that the current Revocable Attribute-Based Encryption (RABE) schemes applied in the DTs paradigm fail to balance the efficiency, security and scalability simultaneously. In this paper, we tackle this challenge by presenting an unbounded and efficient direct RABE scheme with adaptive security. Compared with the previous schemes in this domain, our approach achieves revocable and fine-grained access control efficiently by employing the arithmetic span program (ASP) as the access structure. In this way, the expensive bilinear pairing and exponentiation operations are reduced significantly. Moreover, the unbounded property is satisfied in our scheme since the parameters are not required to be predefined in the setup phase. At last, with the support of the Matrix Decisional Diffie-Hellman (MDDH) assumption, the proposed scheme is proved to achieve adaptive security by adopting dual system encryption methodology. Theoretical comparison and implementation results demonstrate our proposed scheme possesses prominent practicability, scalability and efficiency.}
}


@article{DBLP:journals/jsac/WeiAKS23,
	author = {Wei Wei and
                  Bochao An and
                  Qiao Ke and
                  Jun Shen},
	title = {A Blockchain-Based Multi-Users Oblivious Data Sharing Scheme for Digital
                  Twin System in Industrial Internet of Things},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3318--3332},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310105},
	doi = {10.1109/JSAC.2023.3310105},
	timestamp = {Wed, 22 Nov 2023 13:37:55 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WeiAKS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin (DT) constructs virtual counterparts of physical devices to monitor and optimize their life cycle processes. With the emergence of industry 4.0, Industrial Internet of Things (IIoT) has became the backbone of the DT by providing a fundamental way to transform physical devices to their virtual counterparts. With the deployment of IIoT, built-in sensors enable real-time collection of critical DT data involving various physical parameters associated with devices during their life cycle. However, traditional data sharing services rely on a centralized infrastructure, which inevitably brings severe security threats to share large volume of sensitive DT data derived from numerous sensors. To address the above issue, this paper presents a blockchain based Multi-users Oblivious Data Sharing scheme (MODS) for the digital twin system in the context of IIoT. MODS supports a broad range of security properties including confidentiality, obliviousness, and access control for the DT data stored on the blockchain. MODS adopts a hybrid design approach by combing trusted hardware and cryptography to achieve well balances between security and efficiency. To demonstrate the design advantages of MODS, we explore the design space of a multi-users oblivious data sharing scheme by using pure cryptographic approach, which incurs several design tradeoffs that must be addressed. We show that MODS performs well in these tradeoffs. A comprehensive evaluation has been conducted to demonstrate that MODS is practical to support secure data sharing via blockchain for IIoT.}
}


@article{DBLP:journals/jsac/FangQWZXMC23,
	author = {Kai Fang and
                  Jiefan Qiu and
                  Tingting Wang and
                  Kailu Zheng and
                  Liyao Xing and
                  Keji Mao and
                  Kaikai Chi},
	title = {IDRes: Identity-Based Respiration Monitoring System for Digital Twins
                  Enabled Healthcare},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {10},
	pages = {3333--3348},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310095},
	doi = {10.1109/JSAC.2023.3310095},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FangQWZXMC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Currently, powerful and ubiquitous mobile devices provide an opportunity to map physical conditions to cyberspace and realize Digital Twins enabled Healthcare (DTeH). Especially, the impact of the COVID-19 epidemic renders it necessary to keep an eye on the changing trend of respiration. Long-term respiration monitoring helps to assess personal health status and thus becomes an important issue in DTeH. However, previous mobile device-assistant methods mostly implement the monitoring via short-time detection in a best-effort way and with less consideration of identity recognition, the only mean to bind physical vital signs into personal profiles in digital twins space. Thus, it is necessary to introduce the identification to complete string multiple short-time detections and form long-term personal monitoring. To this end, we propose IDRes, an identity-based respiration monitoring system for DTeH. This system employs mobile devices to generate a high-frequency sonar signal to complete respiration detection and identity recognition. As well as it also estimates the respiration rate by tracking the phase change of the sonar signal and recognizes identity via the Doppler frequency shift of the signal to capture characteristics of chest movement. Moreover, via band-pass filtering to remove the low-frequency voice component of the received signals, the usage of the high-frequency sonar signal also enhances security at the physical level. At last, we conduct a series of experiments under different conditions. Experimental results illustrate that IDRes achieves the mean detection error of 0.49bpm with over 93.3% recognition accuracy, and manifest that IDRes can satisfy the requirements of mapping the accurate vital sign data to the personal profile of DTeH.}
}


@article{DBLP:journals/jsac/MumtazCGREMXA23a,
	author = {Shahid Mumtaz and
                  Soumaya Cherkaoui and
                  Mohsen Guizani and
                  Joel J. P. C. Rodrigues and
                  Abdulmotaleb El{-}Saddik and
                  Sabita Maharjan and
                  Yang Xiao and
                  Ikram Ashraf},
	title = {Guest Editorial Digital Twins for Mobile Networks - Part {II}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3349--3354},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313017},
	doi = {10.1109/JSAC.2023.3313017},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/MumtazCGREMXA23a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {6G communication networks are expected to become an integral part of the infrastructure needed for developing a smart society in the future. Addressing the challenges on the road towards realizing 6G network requirements in terms of quality of service, user experience, and security, is therefore of utmost importance. The digital twin (DT) technology can potentially improve the efficiency, reliability, and security of 6G networks. Digital twins for mobile networks (DTMNs) are seen as a key factor in harnessing the full benefits of 6G. Using digital twins can help address several problems, including network optimization, fault diagnosis, and fault management. Furthermore, DTMNs can characterize the physical entities in a 6G network and their relationships to each other, build their virtual models, and use simulation, learning, and reasoning capabilities to make predictions and support informed decision-making,}
}


@article{DBLP:journals/jsac/FengLHPW23,
	author = {Jie Feng and
                  Lei Liu and
                  Xiangwang Hou and
                  Qingqi Pei and
                  Celimuge Wu},
	title = {QoE Fairness Resource Allocation in Digital Twin-Enabled Wireless
                  Virtual Reality Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3355--3368},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313195},
	doi = {10.1109/JSAC.2023.3313195},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/FengLHPW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless virtual reality (VR) is expected to be a technology that revolutionizes human interaction and perceived media, where the quality of experience (QoE) is an important indicator to measure user service perception. However, existing schemes only consider general and time-invariant QoE optimization, which may suffer performance degradation. Moreover, it is also necessary to ensure the fairness of the individual user’s performance in wireless VR. To address these challenges, we employ digital twin technology to investigate a max-min QoE-optimal problem for wireless VR systems in this paper. Specifically, we maximize the QoE of the worst-case head-mounted displays (HDMs) client, where the QoE model is the linear weighting combination of video quality, service delay, and energy efficiency. The formulated optimization problem is characterized by multidimensional control, which jointly optimizes model selection, transmit power, computation time, and GPU-cycle frequency. Due to the mixed combinatorial features of the optimization problem, we give a low-complexity algorithm design by decoupling the optimization variables. Notably, we first obtain the allocation of the transmit power by employing the generalized fractional programming theory and the Lagrangian dual decomposition, followed by attaining the optimal allocation of GPU-cycle frequency in VR mode is derived by the proposed adaptive modified harmony search algorithm, and finally achieve the computation time by the barrier method. Meanwhile, we devise a greedy-style heuristic algorithm for mode selection. In the simulation, three baseline schemes are established as comparisons to assess the effectiveness of the proposed scheme. Meanwhile, the simulation results manifest that the proposed algorithms have good convergence performance and better increase the QoE of the DT-enabled wireless VR system compared to benchmark solutions.}
}


@article{DBLP:journals/jsac/HazarikaSLST23,
	author = {Bishmita Hazarika and
                  Keshav Singh and
                  Chih{-}Peng Li and
                  Anke Schmeink and
                  Kim Fung Tsang},
	title = {RADiT: Resource Allocation in Digital Twin-Driven UAV-Aided Internet
                  of Vehicle Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3369--3385},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310048},
	doi = {10.1109/JSAC.2023.3310048},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HazarikaSLST23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin (DT) has emerged as a promising technology for improving resource allocation decisions in Internet of Vehicles (IoV) networks. In this paper, we consider an IoV network where mobile edge computing (MEC) servers are deployed at the roadside units (RSUs). The IoV network provides ubiquitous connections even in areas uncovered by RSUs with the assistance of unmanned aerial vehicles (UAVs) which can act as a relay between RSUs and task vehicles. A virtual representation of the IoV network is established in the aerial network as DT which captures the dynamics of the entities of the physical network in real-time in order to perform efficient resource allocation for delay-intolerant tasks. We investigate an intelligent delay-sensitive task offloading scheme for the dynamic vehicular environment which provides computation resources via local execution, vehicle-to-vehicle (V2V), and vehicle-to-roadside-unit (V2I) offloading modes based on the energy consumption of the system. Moreover, we also propose a multi-network deep reinforcement learning (DRL)-based resource allocation algorithm (RADiT) in the DT-assisted network for maximizing the utility of the IoV network while optimizing the task offloading strategy. Further, we compare the performance of the proposed algorithm with and without the presence of V2V computation mode. RADiT is further evaluated by comparing it with another benchmark DRL algorithm called soft actor-critic (SAC) and a non-DRL approach called greedy. Finally, simulations are performed to demonstrate that the utility of the proposed RADiT algorithm is higher under every condition compared to its respective conditions in SAC and greedy approach. Consequently, the proposed framework jointly improves energy efficiency and reduces the overall delay of the network. The proposed algorithm with UAV relay further increases the efficiency of the network by increasing the task completion rate.}
}


@article{DBLP:journals/jsac/ZhaoZZHATH23,
	author = {Liang Zhao and
                  Zijia Zhao and
                  Enchao Zhang and
                  Ammar Hawbani and
                  Ahmed Yassin Al{-}Dubai and
                  Zhiyuan Tan and
                  Amir Hussain},
	title = {A Digital Twin-Assisted Intelligent Partial Offloading Approach for
                  Vehicular Edge Computing},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3386--3400},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310062},
	doi = {10.1109/JSAC.2023.3310062},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoZZHATH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle Edge Computing (VEC) is a promising paradigm that exposes Mobile Edge Computing (MEC) to road scenarios. In VEC, task offloading can enable vehicles to offload the computing tasks to nearby Roadside Units (RSUs) that deploy computing capabilities. However, the highly dynamic network topology, strict low-delay constraints, and massive data of tasks of VEC pose significant challenges for implementing efficient offloading. Digital Twin-based VEC is emerging as a promising solution that enables real-time monitoring of the state of the VEC network through mapping and interaction between the physical and virtual worlds, thus assisting in making sound offload decisions in the physical world. Thus, this paper proposes an intelligent partial offloading scheme, namely, Digital Twin-Assisted Intelligent Partial Offloading (IGNITE). First, to find the optimal offloading space in advance, we combine the improved clustering algorithm with the Digital Twin (DT) technique, in which unreasonable decisions can be avoided by reducing the size of the decision space. Second, to reduce the overall cost of the system, Deep Reinforcement Learning (DRL) algorithm is employed to train the offloading strategy, allowing for automatic optimization of computational delay and vehicle service price. To improve the efficiency of cooperation between digital and physical spaces, a feedback mechanism is established. It can adjust the parameters of the clustering algorithm based on the final offloading results in this clustering. To the best of our knowledge, this is the first study on DT-assisted vehicle offloading that proposes a feedback mechanism, forming a complete closed loop as prediction-offloading-feedback. Extensive experiments demonstrate that IGNITE has significant advantages in terms of total system computational cost, total computational delay, and offloading success rate compared with its counterparts.}
}


@article{DBLP:journals/jsac/YaoXLW23,
	author = {Zhixiu Yao and
                  Shichao Xia and
                  Yun Li and
                  Guangfu Wu},
	title = {Cooperative Task Offloading and Service Caching for Digital Twin Edge
                  Networks: {A} Graph Attention Multi-Agent Reinforcement Learning Approach},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3401--3413},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310080},
	doi = {10.1109/JSAC.2023.3310080},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YaoXLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) enables various services to be cached in close proximity to the user equipments (UEs), thereby reducing the service delay of many emerging applications. However, the limitation of storage, computation, and radio resources, the dynamics of the decentralized MEC environment, and the complex spatial relationships of service request types and wireless network states between edge nodes make it difficult to realize efficient edge computing services. To address these challenges, this paper integrates the digital twin (DT) technology with a multi-cell MEC network to study an intelligent cooperative task offloading and service caching scheme, aiming at maximizing a quality of services (QoE)-based system utility. Specifically, we first construct a digital twin edge network (DITEN) to reflect the physical MEC system in real-time and provide data for training. With the help of DT technology, it is easy to access data resources in the DITEN to improve the simulation ability and reduce the communication cost. Then, we propose a graph attention-based multi-agent reinforcement learning (GatMARL) algorithm to learn the optimal task offloading and service caching strategies in the DITEN. The GatMARL employs a graph attention-based value decomposition network to capture the potential spatial relationships between edge nodes to learn better attentive cooperation policy. Simulation results demonstrate that the proposed GatMARL algorithm exhibits an effective performance improvement compared with state-of-the-art benchmarks.}
}


@article{DBLP:journals/jsac/JiWJ23,
	author = {Zhe Ji and
                  Sheng Wu and
                  Chunxiao Jiang},
	title = {Cooperative Multi-Agent Deep Reinforcement Learning for Computation
                  Offloading in Digital Twin Satellite Edge Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3414--3429},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313595},
	doi = {10.1109/JSAC.2023.3313595},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JiWJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of commercial off-the-shelf hardware, low Earth orbit (LEO) satellites are promising to provide flexible edge computing services. In this paper, we investigate a digital twin (DT)-empowered satellite-terrestrial cooperative edge computing network, where computation tasks from terrestrial users can be partially offloaded to the associated base station (BS) edge server, the associated LEO satellite edge server, and an adjacent LEO satellite edge server. We formulate a multi-tier computation offloading optimization problem to minimize the weighted sum of total system delay and satellite energy consumption, where a LEO-layer problem and a DT-layer problem are involved. The LEO-layer problem optimizes the three-tier computation resource allocation and task splitting ratio. From the multi-satellite network perspective, the DT-layer problem optimizes how many resources will be shared between adjacent satellites. We then propose a multi-agent double actors twin delayed deterministic policy gradient (MA-DATD3) algorithm to optimize the LEO-layer problem, and adopt a centralized training and decentralized execution (CTDE) paradigm. The proposed MA-DATD3 algorithm is extended to solve the DT-layer problem in a centralized way, and the resource sharing between adjacent satellites is optimized to maximize the time-averaged reward. Simulation results show that our algorithm achieves a better performance than the MADDPG algorithm, and effectively improves the computation offloading performance while balancing the energy consumption and the total delay.}
}


@article{DBLP:journals/jsac/YiLCWL23,
	author = {Bo Yi and
                  Jianhui Lv and
                  Jiahao Chen and
                  Xingwei Wang and
                  Keqin Li},
	title = {Digital Twin Constructed Spatial Structure for Flexible and Efficient
                  Task Allocation of Drones in Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3430--3443},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3313193},
	doi = {10.1109/JSAC.2023.3313193},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YiLCWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applying the Multiple Drones System (MDS) to perform the repetitive and dangerous tasks for human in many complex environments has become a trend all around the world, due to the increasing capacity of mobile communication and the increasing intelligence of drone robots. However, to fulfill the target with less cost as much as possible, drones need to collaborate deeply with each other to make the optimal decision, which is now important and challenging. In this work, we focus on addressing the efficient task allocation among large-scale drones with the object of minimizing the resource waste and cost, which is proved to be NP-hard. Specifically, we first introduce the Digital Twin (DT) technology to dynamically construct the spatial structure for drones, in which a density clustering based algorithm is proposed to decompose the large-scale task allocation problem among all drones into smaller sub-problems among partial drones. Then, for each sub-problem, we propose an improved auction algorithm to allocate the sub-tasks to local drones according to the task difficulty and drone ability. The experimental results indicate that the proposed method outperforms the state-of-the-art methods in terms of the moving distance, resource utilization and task completion time, etc.}
}


@article{DBLP:journals/jsac/DengYLWNZ23,
	author = {Min Deng and
                  Zhiqiang Yao and
                  Xingwang Li and
                  Han Wang and
                  Arumugam Nallanathan and
                  Zeyang Zhang},
	title = {Dynamic Multi-Objective {AWPSO} in DT-Assisted {UAV} Cooperative Task
                  Assignment},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3444--3460},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310056},
	doi = {10.1109/JSAC.2023.3310056},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/DengYLWNZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, more and more attention has been paid to the unmanned aerial vehicle (UAV) cooperative task assignment. In order to complete the task with the lowest cost, some researchers use multi-objective optimization to solve the assignment problem. But few of them consider the complex dynamic scenarios. In this article, the time-varying resource supply and demands are provided by established digital twins (DTs) of UAVs and targets, thereby enabling accurate decision guidance for dynamic task assignment. It takes the scheduling cost, path cost, risk cost and total task time cost as the optimization objectives. To solve this model, an improved dynamic multi-objective adaptive weighted particle swarm Optimization algorithm (DMOAWPSO) is proposed. In the initialization stage, a heuristic method is used to increase the effectiveness of the solution. Besides, the adaptive mutation and subgroup methods are adopted to improve the diversity of the solution. Then, effective environment change detection and response strategies are designed to adapt to dynamic scenarios. Finally, the evaluation metrics are calculated in different instances. Compared with the popular and classic dynamic multi-objective algorithms, the simulation results verify that the proposed algorithm is effective and can cope with the environment changes better in solving the task assignment problem.}
}


@article{DBLP:journals/jsac/HeGYYZWZZ23,
	author = {Danping He and
                  Ke Guan and
                  Dong Yan and
                  Haofan Yi and
                  Zhao Zhang and
                  Xiping Wang and
                  Zhangdui Zhong and
                  Nizar Zorba},
	title = {Physics and AI-Based Digital Twin of Multi-Spectrum Propagation Characteristics
                  for Communication and Sensing in 6G and Beyond},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3461--3473},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310108},
	doi = {10.1109/JSAC.2023.3310108},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HeGYYZWZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To realize intelligent connection of everything and the digital twin (DT) of the physical world in 6G and beyond, new communication and sensing solutions are demanded. The potential of multiple spectrums is maximized for various applications and scenarios. In such a context, an accurate, efficient, and pervasive multi-spectrum propagation model is needed as a critical and unified baseline for testing the performance of the solutions in various scenarios. This work presents ray-tracing (RT) oriented methods for the DT presentation of radio propagation at multiple frequency bands from microwave to visible light. The material- and field-measurement-based approaches are proposed to characterize the electromagnetic properties of materials. On that basis, the propagation mechanisms are developed and validated, and the corresponding parameters are inverted. For the real-time simulation demand, RT and artificial intelligence (AI) algorithms are fused to develop a super-resolution modeling method. The experimental results indicate that the proposed method outperforms the baseline model regarding stability and accuracy. It can significantly reduce the computation time with comparable accuracy to the RT-only approach. The proposed methodologies and the in-depth discussions in this work are expected to pave the way to realize the DT of multi-spectrum propagation for evaluating 6G and beyond technologies.}
}


@article{DBLP:journals/jsac/CuiYZML23,
	author = {Yuanhao Cui and
                  Weijie Yuan and
                  Zhiyue Zhang and
                  Junsheng Mu and
                  Xinyu Li},
	title = {On the Physical Layer of Digital Twin: An Integrated Sensing and Communications
                  Perspective},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3474--3490},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3314826},
	doi = {10.1109/JSAC.2023.3314826},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/CuiYZML23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The digital twin (DT), which effectively represents the actual real-world physical system or process, has reshaped the classic manufacturing, construction, as well as healthcare industry. As for realizing DT, both sensing and communication functionalities are demanded, which fully builds the connectivity between the physical world and the digital world. We first conducted a survey on the current situation of DT combined with communication and sensing. Inspired from this survey and the current development of communication and sensing, in this paper, we attempt to study the communication annd sensing technologies of physical layer in DT, to reduce the hardware and spectrum overhead. First, we studied the degree of freedom (DoF) problem in general communication and sensing system, and contribute to the DoF definition in the sensing system. Then, in order to improve the spectrum efficiency in DT system, we proposed an iterative optimization framework to address the coexistence of communication and sensing, and some examples are provided. Finally, in order to pursue a better integration gain, we proposed a new waveform design method based on DoF completion. The proposed optimization method can achieve the mean square error (MSE) lower bound. Simulation results demonstrate the effectiveness of various problems in the above scenarios.}
}


@article{DBLP:journals/jsac/ZhaoNWZ23,
	author = {Lindong Zhao and
                  Shouxiang Ni and
                  Dan Wu and
                  Liang Zhou},
	title = {Cloud-Edge-Client Collaborative Learning in Digital Twin Empowered
                  Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3491--3503},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310060},
	doi = {10.1109/JSAC.2023.3310060},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ZhaoNWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin (DT) has emerged as a key enabler for the intelligent-oriented evolution of mobile networks. With the rise of privacy concerns for enabling intelligent applications in DT-empowered mobile networks (DTMNs), federated learning has garnered wide attention due to its potential on breaking down data silos. However, the data privacy of federated learning is greatly threatened by emerging gradient leakage attacks, and the need for frequent knowledge exchange limits its training efficiency over resource-constrained DTMNs. To circumvent such dilemmas, this work first proposes a privacy-enhanced federated learning framework based on cloud-edge-client collaborations. Particularly, model splitting between clients and edge servers makes gradient leakage attacks computationally prohibitive, and cloud-side partial model aggregation provides hierarchical data utility. To improve the training efficiency of the proposed learning framework, we further establish its communication and computation cost models, and develop a DT-assisted multi-agent deep reinforcement learning-based resource scheduler for joint client association and channel assignment. Finally, as a case study of intelligent applications in DTMNs, a human-robot collaborative nursing task is designed to evaluate the practical performance of our proposed scheduler. Experimental results show its superiority in saving training costs and preserving learning accuracy.}
}


@article{DBLP:journals/jsac/AloqailyRK23,
	author = {Moayad Aloqaily and
                  Ismaeel Al Ridhawi and
                  Salil Subhash Kanhere},
	title = {Reinforcing Industry 4.0 With Digital Twins and Blockchain-Assisted
                  Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3504--3516},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310068},
	doi = {10.1109/JSAC.2023.3310068},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AloqailyRK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Internet of Things (IoT) has revolutionized the manufacturing process in the industry. It has created a new ecosystem allowing a diversified set of devices to be controlled remotely with minimal human intervention. Today, with the advances in intelligence, processing, storage, communication, and networking capabilities of IoT devices, we are one step closer to realizing the vision of Industry 4.0. Cyber-physical systems (CPS) are now significantly more intelligent and automated with the aid of advances in Machine Learning (ML). Intelligent IoT (IIoT), Digital Twins (DT) and the advances in mobile networks are now paving the path towards decentralized self-managed CPS in the industry. DT permits mobile networks to provide adaptive and dynamic configurations for cooperative CPS. Moreover, trustworthy cooperation may be realized with blockchain. In this article, we present a blockchain-assisted hierarchical federated learning (FL)-enabled platform (HFL) for Industry 4.0. The solution integrates DT into CPS to accurately capture the characteristics of industrial IoT devices and assist in the HFL process. A two-stage FL algorithm is used that groups Internet-enabled factory machinery and their DTs into groups in accordance with their organizational structure. A global model is created for the groups from the averaged local models and the DT model in the first stage. During the second stage, federated aggregation is used to create a global model from the first-stage models. Blockchain is used to cross-verify and validate newly added blocks with the support of validator nodes. Numerical analysis is performed to compare between the presented DT-enabled and blockchain-assisted HFL solution and benchmark solutions in terms of network overhead, block optimization, and accuracy.}
}


@article{DBLP:journals/jsac/QiYYQ23,
	author = {Saiyu Qi and
                  Xu Yang and
                  Jiahe Yu and
                  Yong Qi},
	title = {Blockchain-Aware Rollbackable Data Access Control for IoT-Enabled
                  Digital Twin},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3517--3532},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310061},
	doi = {10.1109/JSAC.2023.3310061},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/QiYYQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Internet of Things (IoT) enables digital twin (DT) technology to precisely represent a real product in a virtual space by generating a multitude of IoT data items to record many aspects of the product. To support various DT-based applications, the generated IoT data items need to be shared among multiple parties involving the lifecycle of the product, which raises increasing demand for data access control. The decentralization and tamper-proofing properties of blockchain enable it a promising technology to support immutability protection of shared IoT data items. Meanwhile, to protect the confidentiality of the shared IoT data items, attribute-based encryption (ABE) can be used as a common tool to construct a cryptographic enforced data access control scheme. However, its adoption has been severely hindered by the incompatibility between the immutability of blockchain and secure authority update of cryptographic enforced data access control. In this paper, a blockchain-aware rollbackable data access control scheme (Bdacs) is proposed to reconcile the above tension. Bdacs uses two novel encryption schemes named hierarchical encryption scheme and privacy-preserving rollback re-encryption scheme to realize secure dynamic access control while preserving the immutability of blockchain. We prove the security of Bdacs and evaluate it through theoretical comparison and experimental analysis to confirm its efficiency. This work can serve as a basis of development of future DT-based applications to enable privacy-preserving IoT data-sharing systems deployed on blockchain.}
}


@article{DBLP:journals/jsac/OkegbileCZCY23,
	author = {Samuel Dayo Okegbile and
                  Jun Cai and
                  Hao Zheng and
                  Jiayuan Chen and
                  Changyan Yi},
	title = {Differentially Private Federated Multi-Task Learning Framework for
                  Enhancing Human-to-Virtual Connectivity in Human Digital Twin},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3533--3547},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310106},
	doi = {10.1109/JSAC.2023.3310106},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/OkegbileCZCY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ensuring reliable update and evolution of a virtual twin in human digital twin (HDT) systems depends on any connectivity scheme implemented between such a virtual twin and its physical counterpart. The adopted connectivity scheme must consider HDT-specific requirements including privacy, security, accuracy and the overall connectivity cost. This paper presents a new, secure, privacy-preserving and efficient human-to-virtual twin connectivity scheme for HDT by integrating three key techniques: differential privacy, federated multi-task learning and blockchain. Specifically, we adopt federated multi-task learning, a personalized learning method capable of providing higher accuracy, to capture the impact of heterogeneous environments. Next, we propose a new validation process based on the quality of trained models during the federated multi-task learning process to guarantee accurate and authorized model evolution in the virtual environment. The proposed framework accelerates the learning process without sacrificing accuracy, privacy and communication costs which, we believe, are non-negotiable requirements of HDT networks. Finally, we compare the proposed connectivity scheme with related solutions and show that the proposed scheme can enhance security, privacy and accuracy while reducing the overall connectivity cost.}
}


@article{DBLP:journals/jsac/GuoLTHLLIM23,
	author = {Jingjing Guo and
                  Zhiquan Liu and
                  Siyi Tian and
                  Feiran Huang and
                  Jiaxing Li and
                  Xinghua Li and
                  Kostromitin Konstantin Igorevich and
                  Jianfeng Ma},
	title = {{TFL-DT:} {A} Trust Evaluation Scheme for Federated Learning in Digital
                  Twin for Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3548--3560},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310094},
	doi = {10.1109/JSAC.2023.3310094},
	timestamp = {Fri, 08 Mar 2024 13:21:35 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GuoLTHLLIM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the distributed collaboration and privacy protection features, federated learning is a promising technology to perform the model training in virtual twins of Digital Twin for Mobile Networks (DTMN). In order to enhance the reliability of the model, it is always expected that the users involved in federated learning have trustworthy behaviors. Yet, available trust evaluation schemes for federated learning have the problems of considering simplex evaluation factor and using coarse-grained trust calculation method. In this paper, we propose a trust evaluation scheme for federated learning in DTMN, which takes direct trust evidence and recommended trust information into account. A user behavior model is designed based on multiple attributes to depict users’ behavior in a fine-grained manner. Furthermore, the trust calculation methods for local trust value and recommended trust value of a user are proposed using the data of user behavior model as trust evidence. Several experiments were conducted to verify the effectiveness of the proposed scheme. The results show that the proposed method is able to evaluate the trust levels of users with different behavior patterns accurately. Moreover, it performs better in resisting attacks from users that alternately execute good and bad behaviors compared with state-of-the-art scheme. The code for the method proposed in this paper is available at: https://web.xidian.edu.cn/jjguo/en/code.html .}
}


@article{DBLP:journals/jsac/WangDWRGD23,
	author = {Huan Wang and
                  Xiaoqiang Di and
                  Yan Wang and
                  Bin Ren and
                  Ge Gao and
                  Junyi Deng},
	title = {An Intelligent Digital Twin Method Based on Spatio-Temporal Feature
                  Fusion for IoT Attack Behavior Identification},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3561--3572},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310091},
	doi = {10.1109/JSAC.2023.3310091},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangDWRGD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network attack identification effectively secures Internet of Things (IoT) application scenarios. However, dynamic scene changes, attack feature reliance, high data dimensions, and challenges with spatio-temporal feature fusion frequently pose limitations to attack traffic identification in IoT contexts. Definitive intelligent IoT attack identification enables intelligent algorithms to extract attack features for application scenarios with fixed topological environments but cannot construct the intricate changes of IoT application scenarios. Through the dynamic acquisition, feature awareness, and deep learning, intelligent digital twin-based attack detection can address these issues and enhance attack identification for IoT threats. Thus, this paper proposed an intelligent digital twin method based on spatio-temporal feature fusion for IoT attack behavior identification. Firstly, feature subsets are selected based on information gain to reduce the dimensionality of IoT data with high traffic; Secondly, a parallel spatio-temporal feature extraction model is designed unlike the existing tandem model, which uses a simplified Convolutional Neural Networks (CNN) model to learn the spatial features of the attack, a Bi-directional Long Short-Term Memory (BiLSTM) model to learn the temporal features of the attack, an attention mechanism to fuse the temporal and spatial features, and the (Deep Neural Networks) DNN to learn the combined features; Finally, the virtual instance space and topology of the attack scenario are simulated using digital twin (DT) to build a digital version of the complex system for IoT applications and tested in a simulation environment. Based on experimental results using the UNSW-NB15 and CICIDS2017 datasets, this paper shows that the proposed method can extract spatio-temporal features from network attack traffic and has a 5% improvement in test accuracy.}
}


@article{DBLP:journals/jsac/XiaXPZYZWL23,
	author = {Hui Xia and
                  Shuo Xu and
                  Jiaming Pei and
                  Rui Zhang and
                  Zhi Yu and
                  Weitao Zou and
                  Lukun Wang and
                  Chao Liu},
	title = {FedME\({}^{\mbox{2}}\): Memory Evaluation {\&} Erase Promoting
                  Federated Unlearning in {DTMN}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3573--3588},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310049},
	doi = {10.1109/JSAC.2023.3310049},
	timestamp = {Mon, 04 Dec 2023 21:30:04 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XiaXPZYZWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital Twins (DTs) can generate digital replicas for mobile networks (MNs) that accurately reflect the state of MN. Machine learning (ML) models trained in DT for MN (DTMN) virtual environments can be more robustly implemented in MN. This can avoid the training difficulties and runtime errors caused by MN instability and multiple failures. However, when using data from various devices in the MN system, DTs must prioritize data privacy. Federated learning (FL) enables the construction of models without data leaving devices to protect DTMN data privacy. Nevertheless, FL’s privacy protection needs further improvement for it only guarantees device-level data ownership but ignores that models may retain private information from data. Therefore, this paper focuses on data forgetting in privacy protection, and proposes a novel FL-based unlearning framework (FedME2), which contains MEval and MErase modules. Guided by memory evaluation information from MEval and employing MErase’s multi-loss training approach, FedME2 gets accurate data forgetting in DTMN. In four DTMN virtual environments, FedME2 achieves an average data forgetting rate of approximately 75% for global models under FL and kept the influence on global models’ accuracy below 4%. FedME2 has better data forgetting and improves DTMN data privacy protection while guaranteeing model accuracy.}
}


@article{DBLP:journals/jsac/LiuYWLAAQ23,
	author = {Jingxian Liu and
                  Junjie Yan and
                  Dehuan Wan and
                  Xuran Li and
                  Saba Al{-}Rubaye and
                  Anwer Al{-}Dulaimi and
                  Zhi Quan},
	title = {Digital Twins Based Intelligent State Prediction Method for Maneuvering-Target
                  Tracking},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3589--3606},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310109},
	doi = {10.1109/JSAC.2023.3310109},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiuYWLAAQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Maneuvering-target tracking has always been an important and challenge work because the unknown and changeable motion-models can easily lead to the failure of model-driven target tracking. Recently, many neural network methods are proposed to improve the tracking accuracy by constructing direct mapping relationships from noisy observations to target states. However, limited by the coverage of training data, those data-driven methods suffer other problems, such as weak generalization abilities and unstable tracking effects. In this paper, a digital twin system for maneuvering-target tracking is built, and all kinds of simulated data are created with different motion-models. Based on those data, the features of noisy observations and their relationship to target states are found by two specially designed neural networks: one eliminates the observation noises and the other one predicts the target states according to the noise-limited observations. Combining the above two networks, the state prediction method is proposed to intelligently predict targets by understanding the information of motion-model hidden in noisy observations. Simulation results show that, in comparison with the state-of-the-art model-driven and data-driven methods, the proposed method can correctly and timely predict the motion-models, increase the tracking generalization ability and reduce the tracking root-mean-squared-error by over 50% in most of maneuvering-target tracking scenes.}
}


@article{DBLP:journals/jsac/YuZFLFZXG23,
	author = {Peng Yu and
                  Junye Zhang and
                  Honglin Fang and
                  Wenjing Li and
                  Lei Feng and
                  Fanqin Zhou and
                  Pei Xiao and
                  Song Guo},
	title = {Digital Twin Driven Service Self-Healing With Graph Neural Networks
                  in 6G Edge Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3607--3623},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310063},
	doi = {10.1109/JSAC.2023.3310063},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/YuZFLFZXG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {6G edge networks strive to offer ubiquitous intelligent services, requiring a greater emphasis on network stability and reliability. However, current networks present a low automation degree of the operation, administration and maintenance process. Consequently, active service migration away from abnormal network nodes and links, as well as automatic and transparent service recovery from sudden anomalies, become challenging tasks. These conditions underscore the urgency for an innovative service self-healing mechanism for 6G edge networks. Digital twin (DT) technology uses modeling to represent physical entities, thereby facilitating lifecycle management. However, the application of DT technology in networks is still a burgeoning field of study. In this paper, we explore the DT-driven service self-healing mechanism in 6G edge networks. Initially, we design a DT-based architecture for service self-healing. Subsequently, we construct a performance prediction mechanism leveraging graph neural networks (GNNs) to devise an efficient prediction model, which aims to accurately infer network performance and promptly detect abnormal network conditions. To maintain fine-grained service stability amidst potential network anomalies, we propose a DT-driven service redeployment mechanism enhanced by GNNs. Comprehensive experimental results reveal that our proposed mechanism can accurately predict flow-level delays and identify abnormal links and nodes. Furthermore, the DT-driven service redeployment mechanism effectively reduces service delay and enhances network load balance.}
}


@article{DBLP:journals/jsac/LiCYZZL23,
	author = {Mi Li and
                  Cen Chen and
                  Xulei Yang and
                  Joey Tianyi Zhou and
                  Tao Zhang and
                  Yangfan Li},
	title = {Toward Communication-Efficient Digital Twin via AI-Powered Transmission
                  and Reconstruction},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3624--3635},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310089},
	doi = {10.1109/JSAC.2023.3310089},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiCYZZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Digital twin technology has recently gathered pace in engineering communities as it allows for the convergence of the real structure and its digital counterpart. 3D point cloud data is a more effective way to describe the real world and to reconstruct the digital counterpart than the conventional 2D images or 360-degree images. Large-scale, e.g., city-scale digital twins, typically collect point cloud data via internet-of-things (IoT) devices and transmit it over wireless networks. However, the existing wireless transmission technology can not carry real-time point cloud transmission for digital twin reconstruction due to mass data volume, high processing overheads, and low delay-tolerance. We propose a novel artificial intelligence (AI) powered end-to-end framework, termed AIRec, for efficient digital twin communication from point cloud compression, wireless channel coding, and digital twin reconstruction. AIRec adopts the encoder-decoder architecture. In the encoder, a novel importance-aware pooling scheme is designed to adaptively select important points with learnable thresholds to reduce the transmission volume. We also design a novel noise-aware joint source and channel coding is proposed to adaptively adjust the transmission strategy based on SNR and map the features to error-resilient channel symbols for wireless transmission to achieve a good tradeoff between the transmission rate and reconstruction quality. The decoder can accurately reconstruct the digital twins from the received symbols. Extensive experiments of typical datasets and comparison with baselines show that we achieve a good reconstruction quality under\n24×\ncompression ratio.}
}


@article{DBLP:journals/jsac/AbdulRahmanOBM23,
	author = {Sawsan AbdulRahman and
                  Safa Otoum and
                  Ouns Bouachir and
                  Azzam Mourad},
	title = {Management of Digital Twin-Driven IoT Using Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3636--3649},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310102},
	doi = {10.1109/JSAC.2023.3310102},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AbdulRahmanOBM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT), Digital Twin (DT), and Federated Learning (FL) are redefining the future vision of globalization. While IoT is about sensing data from physical devices, DTs reflect their digital representation and enable optimized decision-making by tightly integrating Artificial Intelligence (AI). Although swiftly growing, DTs are raising new challenges in privacy concerns, which are nowadays addressed by FL. However, the limited IoT resources, the communication overhead, and the lack of trust among clients are major obstacles that hinder the effectiveness of learning systems. In this paper, we design a new IoT-based architecture empowered by DT to improve the efficiencies of limited-resources devices. On top of this architecture, we leverage FL to construct the DT models. We further propose CISCO-FL, a Clustered FL with Intelligent Selection and Computation Offloading. Particularly, we study the computing resources of the clients and the quality of their models, and we embed in the proposed approach an intelligent offloading model, where the clients with high computational resources can assist and optimize the model of those struggling with limited resources. As such, both communication cost and computation resources are reduced and optimized. Finally, thorough experimental results are presented to support our findings and validate our model.}
}


@article{DBLP:journals/jsac/StephanieKA23,
	author = {Veronika Stephanie and
                  Ibrahim Khalil and
                  Mohammed Atiquzzaman},
	title = {Digital Twin Enabled Asynchronous SplitFed Learning in E-Healthcare
                  Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3650--3661},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310103},
	doi = {10.1109/JSAC.2023.3310103},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/StephanieKA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancement of Industrial Internet of Things (IIoT) technology has resulted in the fourth industrial revolution, or Industry 4.0, enabling industries to enhance productivity. However, despite the benefits, there remain significant challenges, such as resource heterogeneity, communication efficiency, and data privacy, that limit the applications of IIoT in privacy-sensitive domains like healthcare. In order to protect data privacy, Federated Learning (FL) has been suggested as a solution, involving the sharing of model parameters rather than data itself. Current FL applications, however, still struggle with cost efficiency, especially when IIoT devices with heterogenous resources are involved. To address this, this paper proposes Digital Twin (DT) enabled Asynchronous SplitFed Learning (DT-ASFL) for classification tasks in the e-healthcare system over mobile networks. We first develop SplitFed Learning to introduce communication efficiency in the e-healthcare system, sending only extracted features during the learning process instead of the entire learning model. This enables resource-constrained devices to participate in the learning process by allowing the participants to train a partial learning model. DT is then employed to provide real-time statuses of IIoT devices deployed in the system, enabling asynchronous model updates in SplitFed Learning. The experimental results demonstrate the efficacy of DT-ASFL compared to the existing methods.}
}


@article{DBLP:journals/jsac/ChenWFLYLH23,
	author = {Junxin Chen and
                  Wei Wang and
                  Bo Fang and
                  Yu Liu and
                  Keping Yu and
                  Victor C. M. Leung and
                  Xiping Hu},
	title = {Digital Twin Empowered Wireless Healthcare Monitoring for Smart Home},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3662--3676},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310097},
	doi = {10.1109/JSAC.2023.3310097},
	timestamp = {Tue, 16 Jul 2024 10:23:07 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/ChenWFLYLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dramatic progresses of wireless technologies and wearable devices have significantly promoted the development and popularity of smart home, while digital twin (DT) emerges as a game changer benefiting from its enhanced capabilities of visualization and interaction. The DT is able to build a realtime and continuous visual replica of a physical object or process, and to provide realtime monitoring, anomaly prediction, smart interaction, and lifecycle management. This paper presents a DT model to empower healthcare monitoring in the smart home with the goals of graphical monitoring, healthcare prediction, and intelligent control. High fidelity DT of the house and its equipments is created for visualized monitoring, and two suites of devices are deployed for continuously acquiring the users’ electrocardiograph (ECG) waves and the WiFi signals in the house. Two intelligent algorithms are then developed to perform fall detection from WiFi signals and to screen atrial fibrillation from ECG waves collected by wearable devices. Experimental results well validate the proposed model’s effectiveness for smart home monitoring, and the advantages of the developed smart algorithms for healthcare prediction over counterparts.}
}


@article{DBLP:journals/jsac/XingLLL23,
	author = {Lumin Xing and
                  Wenjian Liu and
                  Xiaoliang Liu and
                  Xin Li},
	title = {An Enhanced Vision Transformer Model in Digital Twins Powered Internet
                  of Medical Things for Pneumonia Diagnosis},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3677--3689},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310096},
	doi = {10.1109/JSAC.2023.3310096},
	timestamp = {Thu, 09 Nov 2023 21:13:23 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XingLLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The computer-aided system and chest X-ray images play an important role in the diagnosis of pneumonia, which are the main way of pneumonia diagnosis. The traditional deep learning models have achieved some success in medical images, which captures the potential features of the image by continuously sliding the fixed convolution kernel. The disadvantage of this method is that it cannot effectively capture the long-distance dependencies in the image, and it does not have the ability of dynamic adaptive modeling. Next, the high-quality labeled data of chest X-ray images are very scarce. In order to achieve high-quality artificial intelligence diagnosis, a large number of high-quality annotated chest X-ray images are required. In this work, based on technologies such as Internet of Medical Things (IoMT) and Digital Twins, we built an intelligent IoMT platform for automatic diagnosis of pneumonia. For the digital twin of the lung, we propose an enhanced vision transformer model (EVTM) for analyzing chest X-ray images to determine whether the patient is infected with pneumonia. The EVTM model utilizes the vision transformer for training and inference on chest X-ray images. Then the EVTM model uses the variational autoencoder model for data augmentation, so that the amount of chest X-ray images meets the training requirements of the model. Finally, we conducted extensive experiments on the standard chest X-ray image dataset to verify the effectiveness of the EVTM model.}
}


@article{DBLP:journals/jsac/SuMLSHZ23,
	author = {Jian Su and
                  Qiankun Mao and
                  Zhenlong Liao and
                  Zhengguo Sheng and
                  Chenxi Huang and
                  Xuedong Zhang},
	title = {A Real-Time Cross-Domain Wi-Fi-Based Gesture Recognition System for
                  Digital Twins},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3690--3701},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310073},
	doi = {10.1109/JSAC.2023.3310073},
	timestamp = {Fri, 02 Feb 2024 16:58:36 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/SuMLSHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The rapid development of Internet of Things has led more realization of digital twins (DT), such as healthcare, smart homes, virtual reality, etc., gesture recognition is a fundamental component of DT. Its implementation can provide users with personalized services or improved human-computer interaction, such as smart home control, in-car interaction, etc., most of existing gesture recognition methods are based on vision or wearable device. However, the vision-based methods face the problem of privacy breach, whereas the wearable-based methods may bring inconvenience to users. With the wide deployment of Wi-Fi networks, lots of consumer devices are widely accessible in people’s homes. Motivated by the fact that Wi-Fi signal propagation can be affected by human motion, the opportunity to use Wi-Fi signals for gesture recognition can be further explored. However, the challenge is that the received Wi-Fi signal shows great differences when the same person performs the same gesture in different environments or different person performs the same gesture in the same environment. Therefore, the signal alignment across different domain needs to be solved. In this paper, we propose a gesture recognition system named Phase-Attention-based-Conv-CSI (PAC-CSI), which consists of two modules: data processing and gesture recognition. In the data processing module, we eliminate random phase noise in channel state information (CSI) and perform phase calibration. In the gesture recognition module, we feed the processed phase sequence into a lightweight deep neural network for gesture recognition. PAC-CSI can obtain the gesture category in about 200ms, which can meets the real-time requirements of DT. The gesture recognition accuracy of our proposed system in a single domain is 99.46%, and its performance across new locations, orientations, users, and environments is 98.77%, 98.90%, 97.54%, and 96.47%, respectively.}
}


@article{DBLP:journals/jsac/XieQLL23,
	author = {Weiliang Xie and
                  Fei Qi and
                  Lei Liu and
                  Qiang Liu},
	title = {Radar Imaging Based {UAV} Digital Twin for Wireless Channel Modeling
                  in Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {11},
	pages = {3702--3710},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3310085},
	doi = {10.1109/JSAC.2023.3310085},
	timestamp = {Tue, 18 Jun 2024 17:00:48 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/XieQLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper looks into the realization of digital twin (DT) technology in unmanned aerial vehicle (UAV) networks. We propose, in particular, a framework for DT-based UAV applications in which distinct jobs in the digital twin interact with UAVs in the physical world via task manager scheduling. Furthermore, we investigate the use of 3D mmWave Radar imaging on UAVs and apply it to the process of radio frequency (RF) characterizing. After that, the method of 3D ray-tracing is employed to accomplish channel modelling of UAVs, which reflects RF domain digital twin match. Finally, we present numerical results to demonstrate that our developed digital twin platform can provide accurate RF presentation of UAV and therefore accomplish smart operating and administration of the actual UAV network.}
}


@article{DBLP:journals/jsac/AmendolaCBJYZ23,
	author = {Danilo Amendola and
                  Nicola Cordeschi and
                  Fan Bai and
                  Yusheng Ji and
                  Shen Yan and
                  Weihua Zhuang},
	title = {Guest Editorial Special Issue on 5G/6G Precise Positioning on Cooperative
                  Intelligent Transportation Systems {(C-ITS)} and Connected Automated
                  Vehicles (CAV)-Part {I}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3713--3718},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322051},
	doi = {10.1109/JSAC.2023.3322051},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/AmendolaCBJYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advancement of connected intelligent transportation systems (C-ITS) and connected automated vehicles (CAV) has brought about a growing need for precise positioning solutions. Positioning technologies play a crucial role in many use cases such as emergency call systems, disaster rescue operations, automated robotics, and more. To ensure the availability, reliability, and quality of location systems both indoors and outdoors, the evolution of cellular technology, particularly in the form of 5G/6G networks, promises to provide a new pathway towards achieving high precision positioning.}
}


@article{DBLP:journals/jsac/JoSKCK23,
	author = {Joohyun Jo and
                  Jae{-}Nam Shim and
                  Byoungnam Kim and
                  Chan{-}Byoung Chae and
                  Dong Ku Kim},
	title = {AoA-Based Position and Orientation Estimation Using Lens {MIMO} in
                  Cooperative Vehicle-to-Vehicle Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3719--3735},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322809},
	doi = {10.1109/JSAC.2023.3322809},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/JoSKCK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Positioning accuracy is a critical requirement for vehicle-to-everything (V2X) use cases. Therefore, this paper derives the theoretical limits of estimation for the position and orientation of vehicles in a cooperative vehicle-to-vehicle (V2V) scenario, using a lens-based multiple-input multiple-output (lens-MIMO) system. Following this, we analyze the Cramér-Rao lower bounds (CRLBs) of the position and orientation estimation and explore a received signal model of a lens-MIMO for the particular angle of arrival (AoA) estimation with a V2V geometric model. Further, we propose a lower complexity AoA estimation technique exploiting the unique characteristics of the lens-MIMO for a single target vehicle; as a result, its estimation scheme is effectively extended by the successive interference cancellation (SIC) method for multiple target vehicles. Given these AoAs, we investigate the lens-MIMO estimation capability for the positions and orientations of vehicles. Subsequently, we prove that the lens-MIMO outperforms a conventional uniform linear array (ULA) in a certain configuration of a lens’s structure. Finally, we confirm that the proposed localization algorithm is superior to ULA’s CRLB as the resolution of the lens increases in spite of the lower complexity.}
}


@article{DBLP:journals/jsac/HuCZSL23,
	author = {Jingzhi Hu and
                  Zhe Chen and
                  Tianyue Zheng and
                  Robert Schober and
                  Jun Luo},
	title = {HoloFed: Environment-Adaptive Positioning via Multi-Band Reconfigurable
                  Holographic Surfaces and Federated Learning},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3736--3751},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322788},
	doi = {10.1109/JSAC.2023.3322788},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/HuCZSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Positioning is an essential service for various applications and is expected to be integrated with existing communication infrastructures in 5G and 6G. Though current Wi-Fi and cellular base stations (BSs) can be used to support this integration, the resulting precision is unsatisfactory due to the lack of precise control of the wireless signals. Recently, BSs adopting reconfigurable holographic surfaces (RHSs) have been advocated for positioning as RHSs’ large number of antenna elements enable generation of arbitrary and highly-focused signal beam patterns. However, existing designs face two major challenges: i) RHSs only have limited operating bandwidth, and ii) the positioning methods cannot adapt to the diverse environments encountered in practice. To overcome these challenges, we present HoloFed, a system providing high-precision environment-adaptive user positioning services by exploiting multi-band (MB)-RHS and federated learning (FL). For improving the positioning performance, a lower bound on the error variance is obtained and utilized for guiding MB-RHS’s digital and analog beamforming design. For better adaptability while preserving privacy, an FL framework is proposed for users to collaboratively train a position estimator, where we exploit the transfer learning technique to handle the lack of position labels of the users. Moreover, a scheduling algorithm for the BS to select which users train the position estimator is designed, jointly considering the convergence and efficiency of FL. Our performance evaluation based on simulations confirms that HoloFed achieves a 57% lower positioning error variance compared to a beam-scanning baseline and can effectively adapt to diverse environments.}
}


@article{DBLP:journals/jsac/TengYW23,
	author = {Boyu Teng and
                  Xiaojun Yuan and
                  Rui Wang},
	title = {Variational Bayesian Multiuser Tracking for Reconfigurable Intelligent
                  Surface-Aided {MIMO-OFDM} Systems},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3752--3767},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322792},
	doi = {10.1109/JSAC.2023.3322792},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TengYW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reconfigurable intelligent surface (RIS) has attracted enormous interest for its potential advantages in assisting both wireless communication and environmental sensing. In this paper, we study a challenging multiuser tracking problem in the multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system aided by multiple RISs. In particular, we assume that a multi-antenna base station (BS) receives the OFDM symbols from single-antenna users reflected by multiple RISs and tracks the positions of these users. Considering the users’ mobility and the blockage of light-of-sight (LoS) paths, we establish a probability transition model to characterize the tracking process, where the geometric constraints between channel parameters and multiuser positions are utilized. We further develop an online message passing algorithm, termed the Bayesian multiuser tracking (BMT) algorithm, to estimate the multiuser positions, the angles-of-arrivals (AoAs) at multiple RISs, and the time delay and the blockage of the LoS path. The Bayesian Cramér Rao bound (BCRB) is derived as the fundamental performance limit of the considered tracking problem. Based on the BCRB, we optimize the passive beamforming (PBF) of the multiple RISs to improve the tracking performance. Simulation results show that the proposed PBF design significantly outperforms the counterpart schemes, and our BMT algorithm can achieve up to centimeter-level tracking accuracy.}
}


@article{DBLP:journals/jsac/WangMFZ23,
	author = {Peilan Wang and
                  Weidong Mei and
                  Jun Fang and
                  Rui Zhang},
	title = {Target-Mounted Intelligent Reflecting Surface for Joint Location and
                  Orientation Estimation},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3768--3782},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322839},
	doi = {10.1109/JSAC.2023.3322839},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/WangMFZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflecting surface (IRS) has been widely recognized as an efficient technique to reconfigure the electromagnetic environment in favor of wireless communication performance. In this paper, we propose a new application of IRS for device-free target sensing via joint location and orientation estimation. In particular, different from the existing works that use IRS as an additional anchor node for localization/sensing, we consider mounting IRS on the sensing target, whereby estimating the IRS’s location and orientation as that of the target by leveraging IRS’s controllable signal reflection. To this end, we first propose a tensor-based method to acquire essential angle information between the IRS and the sensing transmitter as well as a set of distributed sensing receivers. Next, based on the estimated angle information, we formulate two optimization problems to estimate the location and orientation of the IRS/target, respectively, and obtain the locally optimal solutions to them by invoking two iterative algorithms, namely, gradient descent method and manifold optimization. In particular, we show that the orientation estimation problem admits a closed-form solution in a special case that usually holds in practice. Furthermore, theoretical analysis is conducted to draw essential insights into the proposed sensing system design and performance. Simulation results verify our theoretical analysis and demonstrate that the proposed methods can achieve high estimation accuracy which is close to the theoretical bound.}
}


@article{DBLP:journals/jsac/FamiliASWP23,
	author = {Alireza Famili and
                  Tolga O. Atalay and
                  Angelos Stavrou and
                  Haining Wang and
                  Jung{-}Min Park},
	title = {{OFDRA:} Optimal Femtocell Deployment for Accurate Indoor Positioning
                  of RIS-Mounted AVs},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3783--3798},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322821},
	doi = {10.1109/JSAC.2023.3322821},
	timestamp = {Tue, 18 Jun 2024 20:16:46 +0200},
	biburl = {https://dblp.org/rec/journals/jsac/FamiliASWP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The pursuit of high-accuracy localization without relying on the global positioning system (GPS) has gained significant interest in recent years. The deployment of autonomous vehicles (AVs) in diverse indoor applications exemplifies a prominent domain where the demand for a robust positioning system is evident. With the advancements in 5G and beyond radio access networks (RAN), the availability of new positioning signals presents an opportunity to deliver accurate location estimates for these applications. Nevertheless, these signals encounter substantial path losses in indoor environments. Additionally, the precise localization within existing frameworks requires stringent synchronization, which is challenging to meet. In this paper, we propose OFDRA: Optimal Femtocell Deployment for Accurate Indoor Positioning of RIS-Mounted AVs, a novel positioning framework that is robust against multipath and does not require strict synchronization between anchor-anchor or anchor-target entities. Specifically, OFDRA is designed to operate in scenarios where the line of sight (LOS) exists. The first design objective of OFDRA is the mitigation of ranging errors by leveraging a compact reconfigurable intelligent surface (RIS) mounted on top of AVs acting as a programmable mirror in a 5G network. The second design objective is to achieve optimal anchor placement in three-dimensional indoor spaces, thereby reducing the geometric dilution of precision (GDOP) and mitigating geometric-induced errors in the final position estimation. Our experimental verification reveals that the localization error is influenced by GDOP, encompassing both the\nX−Y\nplane and\nZ\n-axis estimations. Through optimized anchor placement, OFDRA demonstrates a seven-fold enhancement in\nZ\n-axis accuracy compared to the state-of-the-art, achieving a sub-1 m three-dimensional accuracy for more than 95% of cases.}
}


@article{DBLP:journals/jsac/TedeschiniN23,
	author = {Bernardo Camajori Tedeschini and
                  Monica Nicoli},
	title = {Cooperative Deep-Learning Positioning in mmWave 5G-Advanced Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3799--3815},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322795},
	doi = {10.1109/JSAC.2023.3322795},
	timestamp = {Fri, 08 Mar 2024 13:21:35 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/TedeschiniN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In application verticals that rely on mission-critical control, such as cooperative intelligent transport systems (C-ITS), 5G-Advanced networks must be able to provide dynamic positioning with accuracy down to the centimeter level. To achieve this level of precision, technology enablers, such as massive multiple-input multiple-output (mMIMO), millimeter waves (mmWave), machine learning and cooperation are of paramount importance. In this paper, we propose a cooperative deep learning (DL)-based positioning methodology that combines these key technologies into a new promising solution for precise 5G positioning. Sparse channel impulse response (CIR) data are used by the positioning infrastructure to extract position-dependent features. We model the problem as a joint task composed of non-line-of-sight (NLOS) identification and position estimation which permits to suitably handle geometrical location measurements and channel fingerprints. The network of base stations (BSs) automatically steers between egocentric (in case of NLOS) and cooperative (for LOS) positioning mode. We perform extensive standard-compliant simulations in a 5G urban micro (UMi) vehicular scenario obtained by ray-tracing and simulation of urban mobility (SUMO) software. Results show that the proposed cooperative DL architecture is able to outperform conventional geometrical positioning algorithms operating in LOS by 47%, achieving a median error of 71 cm on unseen trajectories.}
}


@article{DBLP:journals/jsac/KirmazSMG23,
	author = {Anil Kirmaz and
                  Taylan Sahin and
                  Diomidis S. Michalopoulos and
                  Wolfgang H. Gerstacker},
	title = {ToA and TDoA Estimation Using Artificial Neural Networks for High-Accuracy
                  Ranging},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3816--3830},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322803},
	doi = {10.1109/JSAC.2023.3322803},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/KirmazSMG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High-accuracy positioning enables various applications such as industrial asset tracking, autonomous driving and process automation. Accurate location information relies on accurate time-of-arrival (ToA) or time-difference-of-arrival (TDoA) estimation in widely utilized time-based ranging. In this paper, we propose artificial neural network (ANN) based methods either to estimate T(D)oA directly or to mitigate the error of the conventional estimators. Based on real-world channel measurements, we show that the proposed direct ANN estimator outperforms the conventional estimators at least by approximately 37% and 24% in the 90th percentile ranging error, derived from ToA and TDoA estimations, respectively. Additionally, the proposed T(D)oA error-mitigating ANNs outperform the benchmark error mitigation methods with a gain varying between 17–43% in the 90th percentile ranging error, depending on the underlying conventional estimator. The ranging accuracy delivered by the direct estimation and error mitigation methods using ANNs are similar. Furthermore, the ANN estimators yield a more robust performance than the conventional estimators when the carrier frequency of the positioning signal is varied. ANN-based ToA estimation yields a marginally better ranging accuracy than ANN-based TDoA estimation. This advantage comes at the expense of a larger communication latency, while avoiding the need for synchronization among the positioning anchors.}
}


@article{DBLP:journals/jsac/GaoWYKXC23,
	author = {Jun Gao and
                  Dongze Wu and
                  Feng Yin and
                  Qinglei Kong and
                  Lexi Xu and
                  Shuguang Cui},
	title = {MetaLoc: Learning to Learn Wireless Localization},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3831--3847},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322766},
	doi = {10.1109/JSAC.2023.3322766},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GaoWYKXC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing localization methods that intensively leverage the environment-specific received signal strength (RSS) or channel state information (CSI) of wireless signals are rather accurate in certain environments. However, these methods, whether based on pure statistical signal processing or data-driven approaches, often struggle to generalize to new environments, which results in considerable time and effort being wasted. To address this challenge, we propose MetaLoc, which is the first fingerprinting-based localization framework that leverages the Model-Agnostic Meta-Learning (MAML). Specifically, built on a deep neural network with strong representation capabilities, MetaLoc is trained on historical data sourced from well-calibrated environments, employing a two-loop optimization mechanism to obtain the meta-parameters. These meta-parameters act as the initialization for quick adaptation in new environments, reducing the need for much human effort. The framework introduces two paradigms for the optimization of meta-parameters: a centralized paradigm that simplifies the process by sharing data from all historical environments, and a distributed paradigm that maintains data privacy by training meta-parameters for each specific environment separately. Furthermore, the advanced distributed paradigm modifies the vanilla MAML loss function to ensure that the reduction of loss occurs in a consistent direction across various training domains, thus facilitating faster convergence during training. Our experiments on both synthetic and real datasets demonstrate that MetaLoc outperforms baseline methods in terms of localization accuracy, robustness, and cost-effectiveness. The code and datasets used in this study are publicly available at: https://github.com/WU-Dongze/MetaLoc .}
}


@article{DBLP:journals/jsac/ChenZXYJ23,
	author = {Zirui Chen and
                  Zhaoyang Zhang and
                  Zhuoran Xiao and
                  Zhaohui Yang and
                  Richeng Jin},
	title = {Deep Learning-Based Multi-User Positioning in Wireless {FDMA} Cellular
                  Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3848--3862},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322799},
	doi = {10.1109/JSAC.2023.3322799},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/ChenZXYJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Cooperative Intelligent Transportation Systems (C-ITS) and Connected Automated Vehicles (CAV), accessing multiple users and providing high-precision positioning are both vital. This paper aims to design an efficient deep learning approach to extend current Channel State Information (CSI)-based positioning to Frequency Division Multiple Access (FDMA) mode. In FDMA mode, different users are allocated with different subcarriers, making the user CSI have diverse frequency domain characteristics. The diverse frequency domain characteristics bring huge interference to the neural network for stable position inference, and efficient designs are required to handle this challenge. This paper proposes a novel approach named multi-frequency fusion learning for CSI-based positioning. By first using a shareable method to extract position-related features from CSI on each subcarrier independently and then fusing the obtained features, the designed neural network obtains excellent frequency domain flexibility to cope with the diverse frequency address challenge in FDMA mode. Meanwhile, we provide the feasibility analysis of this learning approach in massive Multiple-Input Multiple-Output (MIMO) systems to ensure its stable application. Based on the architecture of multi-frequency fusion learning, we propose two specific positioning schemes with differentiated designs. One is a Multi-Frequency Ensemble Network (MFENet), which extracts and fuses frequency-independent features to ensure the network is utterly unharmed by the complicated frequency domain characteristics. The other is a Multi-Frequency Cumulative Network (MFCNet), which uses sufficient feature accumulation to achieve high precision positioning. The key performance indices and applications on vehicles are comprehensively compared with popular deep-learning methods. Experiment results show the effectiveness and superiority of the proposed schemes.}
}


@article{DBLP:journals/jsac/XieSL23,
	author = {Lei Xie and
                  Shenghui Song and
                  Khaled B. Letaief},
	title = {Networked Sensing With AI-Empowered Interference Management: Exploiting
                  Macro-Diversity and Array Gain in Perceptive Mobile Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3863--3877},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322828},
	doi = {10.1109/JSAC.2023.3322828},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/XieSL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sensing will become an important service of future wireless networks to assist innovative applications such as autonomous driving and environment monitoring. Perceptive mobile networks (PMNs) were proposed to incorporate sensing capability into current cellular networks. However, the interference management between sensing and communication, as well as the collaborative sensing by multiple sensing nodes (SNs), faces significant challenges. In this paper, we first propose a two-stage protocol to tackle the interference between two sub-systems, where the echoes created by communication signals, i.e., interference for sensing, are estimated in the clutter estimation (CE) stage and then utilized for interference management in the target sensing (TS) stage. Then, a networked sensing detector is derived to exploit the perspectives provided by multiple SNs for sensing the same target. The macro-diversity from multiple SNs, the array gain, and the higher angular resolution from multiple receive antennas of each SN are then investigated to reveal the benefit of networked sensing. Furthermore, we derive the sufficient condition for one SN’s contribution to be positive, based on which a SN selection algorithm is proposed. To reduce the communication workload, we propose a distributed model-driven deep-learning algorithm that utilizes partially-sampled data for CE. Simulation results demonstrate the benefits of networked sensing and validate the higher efficiency of the proposed CE algorithm than existing methods.}
}


@article{DBLP:journals/jsac/LuLWZRZS23,
	author = {Huali Lu and
                  Feng Lyu and
                  Huaqing Wu and
                  Jie Zhang and
                  Ju Ren and
                  Yaoxue Zhang and
                  Xuemin Shen},
	title = {{FL-AMM:} Federated Learning Augmented Map Matching With Heterogeneous
                  Cellular Moving Trajectories},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3878--3892},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322841},
	doi = {10.1109/JSAC.2023.3322841},
	timestamp = {Mon, 25 Mar 2024 12:48:07 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LuLWZRZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Map matching is a fundamental component for location-based services (LBSs), such as vehicle mobility analysis, navigation services, traffic scheduling, etc. In this paper, we investigate federated learning augmented map matching based on heterogeneous cellular moving trajectories from different operator systems, the goal of which is to improve matching accuracy without violating the user privacy. First, we develop a data collection platform with one Android-based application, and conduct rigorous data collection campaigns. Second, we perform systematic data analytics to reveal the data-driven technical challenges, including the impact of sampling rate, high location error of cellular moving data, and poor heterogeneous matching performance. Third, we propose an augmented map matching model, named FL-AMM, i.e., F ederated L earning A ugmented M ap M atching, in which we i) adopt the vertical federated learning framework to achieve data collaboration and privacy protection for heterogeneous operators; ii) devise a data augmentation component to enhance the capability of representing the raw cellular data; and iii) design a map matching model to further learn the mapping function from cellular trajectory points to road segments. Finally, we conduct extensive data-driven experiments to corroborate the efficiency and robustness of the proposed FL-AMM.}
}


@article{DBLP:journals/jsac/LiCLZLM23,
	author = {Xuefei Li and
                  Mingzhe Chen and
                  Yuchen Liu and
                  Zhilong Zhang and
                  Danpu Liu and
                  Shiwen Mao},
	title = {Graph Neural Networks for Joint Communication and Sensing Optimization
                  in Vehicular Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3893--3907},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322761},
	doi = {10.1109/JSAC.2023.3322761},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/LiCLZLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, the problem of joint communication and sensing is studied in the context of terahertz (THz) vehicular networks. In the studied model, a set of service provider vehicles (SPVs) provide either communication service or sensing service to target vehicles, where it is essential to determine 1) the service mode (i.e., providing either communication or sensing service) for each SPV and 2) the subset of target vehicles that each SPV will serve. The problem is formulated as an optimization problem aiming to maximize the sum of the data rates of the communication target vehicles, while satisfying the sensing service requirements of the sensing target vehicles, by determining the service mode and the target vehicle association for each SPV. To solve this problem, a graph neural network (GNN) based algorithm with a heterogeneous graph representation is proposed. The proposed algorithm enables the central controller to extract each vehicle’s graph information related to its location, connection, and communication interference. Using this extracted graph information, a joint service mode selection and target vehicle association strategy is then determined to adapt to the dynamic vehicle topology with various vehicle types (e.g., target vehicles and service provider vehicles). Simulation results show that the proposed GNN-based scheme can achieve 93.66% of the sum rate achieved by the optimal solution, and yield up to 3.16% and 31.86% improvements in sum rate, respectively, over a homogeneous GNN-based algorithm and a conventional optimization algorithm without using GNNs.}
}


@article{DBLP:journals/jsac/GongJLS23,
	author = {Zijun Gong and
                  Fan Jiang and
                  Cheng Li and
                  Xuemin Shen},
	title = {Simultaneous Localization and Communications With Massive {MIMO-OTFS}},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3908--3924},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322818},
	doi = {10.1109/JSAC.2023.3322818},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/GongJLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Next generation cellular network is expected to provide the simultaneous high-accuracy localization and ultra-reliable communication services, even in high mobility scenarios. To that end, the novel orthogonal time frequency space (OTFS) modulation has been developed as a promising physical-layer transmission technique, evident by the outstanding performance in terms of robustness against time-frequency selective fading over the orthogonal frequency division multiplexing (OFDM) counterpart. However, when OTFS meets massive multiple-input multiple-output (MIMO), the specific conditions, under which the delay-Doppler (DD) domain channel model holds, are not identified. In addition, the channel estimation and localization performance in such system is rarely studied. In this work, we target at these new challenges, and conduct comprehensive modelling, performance analysis, and algorithm design for massive MIMO-OTFS based simultaneous localization and communications. Specifically, we derive new channel models for the massive MIMO-OTFS system, which captures both time-frequency dispersion and spatial wideband effects. The specific conditions, under which the new models hold has been unveiled as well. Based on the new models, we establish the theoretical foundations for channel estimation and localization, by deriving the Cramér-Rao lower bounds of channel parameter and location estimation errors. Such bounds have been achieved with the newly designed low-complexity channel estimation and localization algorithms. Numerical simulations of the proposed framework with prevailing pulse functions are also conducted and the results validate the proposed designs and analysis.}
}


@article{DBLP:journals/jsac/KwonLCPW23,
	author = {Girim Kwon and
                  Zhenyu Liu and
                  Andrea Conti and
                  Hyuncheol Park and
                  Moe Z. Win},
	title = {Integrated Localization and Communication for Efficient Millimeter
                  Wave Networks},
	journal = {{IEEE} J. Sel. Areas Commun.},
	volume = {41},
	number = {12},
	pages = {3925--3941},
	year = {2023},
	url = {https://doi.org/10.1109/JSAC.2023.3322760},
	doi = {10.1109/JSAC.2023.3322760},
	timestamp = {Sun, 10 Dec 2023 17:00:43 +0100},
	biburl = {https://dblp.org/rec/journals/jsac/KwonLCPW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrated localization and communication (ILC) at millimeter wave (mmWave MMWAVEinit) frequencies will be a key enabler for providing accurate location information and high data rate communication in beyond fifth generation (B5G) networks. This paper proposes a transmission frame structure and a soft information (SI)-based localization algorithm for position-assisted communications. In accordance with B5G specifications, we consider multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) networks. Theoretical limits are also derived to serve both as performance benchmark and as input for algorithm design. The proposed method enables cooperative ILC with improved localization accuracy and enhanced communication rate simultaneously. In particular, position-assisted communication at mmWave MMWAVEinit frequencies is explored accounting for the statistical characteristics of the wireless environment. Localization accuracy and communication rate are quantified in 3rd Generation Partnership Project (3GPP) network scenarios. Results show that the SI-based localization algorithm achieves decimeter-level accuracy, approaching the theoretical limit. Moreover, the position-assisted communication can provide higher communication rate with reduced overhead compared to existing techniques, especially in scenarios with high mobility.}
}
