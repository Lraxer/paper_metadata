@article{DBLP:journals/ton/ChenLXSZGC22,
	author = {Kuan{-}yin Chen and
                  Sen Liu and
                  Yang Xu and
                  Ishant Kumar Siddhrau and
                  Siyu Zhou and
                  Zehua Guo and
                  H. Jonathan Chao},
	title = {SDNShield: NFV-Based Defense Framework Against DDoS Attacks on {SDN}
                  Control Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {1--17},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105187},
	doi = {10.1109/TNET.2021.3105187},
	timestamp = {Thu, 17 Nov 2022 09:51:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLXSZGC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-defined networking (SDN) is increasingly popular in today’s information technology industry, but existing SDN control plane is insufficiently scalable to support on-demand, high-frequency flow requests. Weaknesses along SDN control paths can be exploited by malicious third parties to launch distributed denial-of-service (DDoS) attacks against the SDN control plane. Recently proposed solutions only partially solve the problem, by protecting either the SDN network edges or the centralized controller. We propose SDNShield, a solution based on emerging network function virtualization (NFV) technologies, which enforces more comprehensive defense against potential DDoS attacks on SDN control plane. SDNShield incorporates a three-stage overload control scheme. The first stage statistically identifies legitimate flows with low complexity and performance overhead. The second stage further performs in-depth TCP handshake verification to ensure good flows are eventually served. The third stage intellectually salvages the misclassified legitimate flows that are falsely dropped from the first two stages. Prototype tests and real data-driven simulation results show that SDNShield can achieve high resilience against brute-force attacks, and maintain good flow-level service quality at the same time.}
}


@article{DBLP:journals/ton/BuraRKSC22,
	author = {Archana Bura and
                  Desik Rengarajan and
                  Dileep Kalathil and
                  Srinivas Shakkottai and
                  Jean{-}Fran{\c{c}}ois Chamberland},
	title = {Learning to Cache and Caching to Learn: Regret Analysis of Caching
                  Algorithms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {18--31},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105880},
	doi = {10.1109/TNET.2021.3105880},
	timestamp = {Thu, 29 Sep 2022 17:06:27 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BuraRKSC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crucial performance metrics of a caching algorithm include its ability to quickly and accurately learn a popularity distribution of requests. However, a majority of work on analytical performance analysis focuses on hit probability after an asymptotically large time has elapsed. We consider an online learning viewpoint, and characterize the “regret” in terms of the finite time difference between the hits achieved by a candidate caching algorithm with respect to a genie-aided scheme that places the most popular items in the cache. We first consider the Full Observation regime wherein all requests are seen by the cache. We show that the Least Frequently Used (LFU) algorithm is able to achieve order optimal regret, which is matched by an efficient counting algorithm design that we call LFU-Lite. We then consider the Partial Observation regime wherein only requests for items currently cached are seen by the cache, making it similar to an online learning problem related to the multi-armed bandit problem. We show how approaching this “caching bandit” using traditional approaches yields either high complexity or regret, but a simple algorithm design that exploits the structure of the distribution can ensure order optimal regret. We conclude by illustrating our insights using numerical simulations.}
}


@article{DBLP:journals/ton/BhattacharyyaBR22,
	author = {Rajarshi Bhattacharyya and
                  Archana Bura and
                  Desik Rengarajan and
                  Mason Rumuly and
                  Bainan Xia and
                  Srinivas Shakkottai and
                  Dileep Kalathil and
                  Ricky K. P. Mok and
                  Amogh Dhamdhere},
	title = {QFlow: {A} Learning Approach to High QoE Video Streaming at the Wireless
                  Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {32--46},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3106675},
	doi = {10.1109/TNET.2021.3106675},
	timestamp = {Thu, 29 Sep 2022 17:06:27 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BhattacharyyaBR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The predominant use of wireless access networks is for media streaming applications. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to develop QFlow, a platform that instantiates this feedback loop, and instantiate a variety of control policies over it. We use the popular application of video streaming over YouTube as our use case. Our context is priority queueing, with the action space being that of determining which clients should be assigned to each queue at each decision period. We first develop policies based on model-based and model-free reinforcement learning. We then design an auction-based system under which clients place bids for priority service, as well as a more structured index-based policy. Through experiments, we show how these learning-based policies on QFlow are able to select the right clients for prioritization in a high-load scenario to outperform the best known solutions with over 25% improvement in QoE, and a perfect QoE score of 5 over 85% of the time.}
}


@article{DBLP:journals/ton/DingLYLZH22,
	author = {Yi Ding and
                  Ling Liu and
                  Yu Yang and
                  Yunhuai Liu and
                  Desheng Zhang and
                  Tian He},
	title = {From Conception to Retirement: {A} Lifetime Story of a 3-Year-Old
                  Wireless Beacon System in the Wild},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {47--61},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3107043},
	doi = {10.1109/TNET.2021.3107043},
	timestamp = {Thu, 19 Oct 2023 07:36:40 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DingLYLZH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We report a 3-year city-wide study of an operational indoor sensing system based on Bluetooth Low Energy (BLE) called aBeacon (short for a libaba Beacon ). aBeacon is pilot-studied, A/B tested, deployed, and operated in Shanghai, China to infer the indoor status of Alibaba couriers, e.g., arrival and departure at the merchants participating in the Alibaba Local Services platform. In its full operation stage (2018/01-2020/04), aBeacon consists of customized BLE devices at 12,109 merchants, interacting with 109,378 couriers to infer their status to assist the scheduling of 64 million delivery orders for 7.3 million customers with a total amount of \\$ 600 million order values. Although in an academic setting, using BLE devices to detect arrival and departure looks straightforward, it is non-trivial to design, build, deploy, and operate aBeacon from its conception to its retirement at city scale in a metric-based approach by considering the tradeoffs between various practical factors (e.g., cost and performance) during long-term system evolution. We report our study in two phases, i.e., an 8-month pilot study and a 28-month deployment and operation in the wild. We focus on an in-depth reporting on the five lessons learned and provide their implications in other systems with long-term operation and broad geospatial coverage, e.g., Edge Computing.}
}


@article{DBLP:journals/ton/AkramDT22,
	author = {Vahid Khalilpour Akram and
                  Orhan Dagdeviren and
                  B{\"{u}}lent Tavli},
	title = {A Coverage-Aware Distributed k-Connectivity Maintenance Algorithm
                  for Arbitrarily Large k in Mobile Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {62--75},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3104356},
	doi = {10.1109/TNET.2021.3104356},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AkramDT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile sensor networks (MSNs) have emerged from the interaction between mobile robotics and wireless sensor networks. MSNs can be deployed in harsh environments, where failures in some nodes can partition MSNs into disconnected network segments or reduce the coverage area. A\nk\n-connected network can tolerate at least\nk\n-1 arbitrary node failures without losing its connectivity. In this study, we present a coverage-aware distributed\nk\n-connectivity maintenance (restoration) algorithm that generates minimum-cost movements of active nodes after a node failure to preserve a persistent\nk\nvalue subject to a coverage conservation criterion. The algorithm accepts a coverage conservation ratio (as a trade-off parameter between coverage and movements) and facilitates coverage with the generated movements according to this value. Extensive simulations and testbed experiments reveal that the proposed algorithm restores\nk\n-connectivity more efficiently than the existing restoration algorithms. Furthermore, our algorithm can be utilized to maintain\nk\n-connectivity without sacrificing the coverage, significantly.}
}


@article{DBLP:journals/ton/ParkJMM22,
	author = {Jeman Park and
                  Rhongho Jang and
                  Manar Mohaisen and
                  David Mohaisen},
	title = {A Large-Scale Behavioral Analysis of the Open {DNS} Resolvers on the
                  Internet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {76--89},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105599},
	doi = {10.1109/TNET.2021.3105599},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ParkJMM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open DNS resolvers are resolvers that perform recursive resolution on behalf of any user. They can be exploited by adversaries because they are open to the public and require no authorization to use. Therefore, it is important to understand the state of open resolvers to gauge their potentially negative impact on the security and stability of the Internet. In this study, we conducted a comprehensive probing over the entire IPv4 address space and found that more than 3 million IP addresses of open resolvers still exist in the wild. Moreover, we found that many of them work in a way that deviates from the standard. More importantly, we found that many open resolvers answer queries with incorrect, even malicious, responses. Contrasting to results obtained in 2013, we found that while the number of open resolvers has decreased significantly, the number of resolvers providing incorrect responses is almost the same, while the number of open resolvers providing malicious responses has increased, highlighting the prevalence of their threat. Through an extended analysis, we also empirically show that the use of forwarders in the open resolver ecosystem and the possibility that incorrect or malicious responses can be manipulated by these forwarders.}
}


@article{DBLP:journals/ton/LiuXLYXWXLU22,
	author = {Xilai Liu and
                  Yan Xu and
                  Peng Liu and
                  Tong Yang and
                  Jiaqi Xu and
                  Lun Wang and
                  Gaogang Xie and
                  Xiaoming Li and
                  Steve Uhlig},
	title = {{SEAD} Counter: Self-Adaptive Counters With Different Counting Ranges},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {90--106},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3107418},
	doi = {10.1109/TNET.2021.3107418},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuXLYXWXLU22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Sketch is a compact data structure useful for network measurements. However, to cope with the high speeds of the current data plane, it needs to be held in the small on-chip memory (SRAM). Therefore, the product of the counter size and the number of counters must be below a certain limit. With small counters, some will overflow. With large counters, the total number of counters will be small, but each counter will be shared by more flows, leading to poor accuracy. To address this issue, we propose a generic technique: self-adaptive counters (SEAD Counter) . When the value of the counter is small, it works as a standard counter. When the value of the counter is large however, we increment it using a predefined probability, so as to represent this large value. Moreover, in the SEAD Counter, the probability decreases when the value increases. We show that this technique can significantly improve the accuracy of counters. This technique can be adapted to different circumstances. We theoretically analyze the improvements achieved by the SEAD Counter. We further show that our SEAD Counter can be extended to three typical sketches and Bloom filters. We conduct extensive experiments on three real datasets and one synthetic dataset. The experimental results show that, compared with the state-of-the-art, sketches using the SEAD Counter improve the accuracy by up to 13.6 times, while the Bloom filters using SEAD Counter can reduce the false positive rate by more than one order of magnitude.}
}


@article{DBLP:journals/ton/BhattacharyaZMG22,
	author = {Arani Bhattacharya and
                  Caitao Zhan and
                  Abhishek Maji and
                  Himanshu Gupta and
                  Samir R. Das and
                  Petar M. Djuric},
	title = {Selection of Sensors for Efficient Transmitter Localization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {107--119},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3104000},
	doi = {10.1109/TNET.2021.3104000},
	timestamp = {Sat, 30 Sep 2023 10:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BhattacharyaZMG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We address the problem of localizing an (unauthorized) transmitter using a distributed set of sensors. Our focus is on developing techniques that perform the transmitter localization in an efficient manner, wherein the efficiency is defined in terms of the number of sensors used to localize. Localization of unauthorized transmitters is an important problem which arises in many important applications, e.g., in patrolling of shared spectrum systems for any unauthorized users. Localization of transmitters is generally done based on observations from a deployed set of sensors with limited resources, thus it is imperative to design techniques that minimize the sensors’ energy resources. In this paper, we design greedy approximation algorithms for the optimization problem of selecting a given number of sensors in order to maximize an appropriately defined objective function of localization accuracy. The obvious greedy algorithm delivers a constant-factor approximation only for the special case of two hypotheses (potential locations). For the general case of multiple hypotheses, we design a greedy algorithm based on an appropriate auxiliary objective function—and show that it delivers a provably approximate solution for the general case. We develop techniques to significantly reduce the time complexity of the designed algorithms by incorporating certain observations and reasonable assumptions. We evaluate our techniques over multiple simulation platforms, including an indoor as well as an outdoor testbed, and demonstrate the effectiveness of our designed techniques—our techniques easily outperform prior and other approaches by up to 50-60% in large-scale simulations and up to 16% in small-scale testbeds.}
}


@article{DBLP:journals/ton/RenGYTWF22,
	author = {Bangbang Ren and
                  Deke Guo and
                  Yali Yuan and
                  Guoming Tang and
                  Weijun Wang and
                  Xiaoming Fu},
	title = {Optimal Deployment of SRv6 to Enable Network Interconnection Service},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {120--133},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105959},
	doi = {10.1109/TNET.2021.3105959},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RenGYTWF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many organizations nowadays have multiple sites at different geographic locations. Typically, transmitting massive data among these sites relies on the interconnection service offered by ISPs. Segment Routing over IPv6 (SRv6) is a new simple and flexible source routing solution which could be leveraged to enhance interconnection services. Compared to traditional technologies, e.g., physical leased lines and MPLS-VPN, SRv6 can easily enable quick-launched interconnection services and significantly benefit from traffic engineering with SRv6-TE. To parse the SRv6 packet headers, however, hardware support and upgrade are needed for the conventional routers of ISP. In this paper, we study the problem of SRv6 incremental deployment to provide a more balanced interconnection service from a traffic engineering view. We formally formulate the problem as an SRID problem with integer programming. After transforming the SRID problem into a graph model, we propose two greedy methods considering short-term and long-term impacts with reinforcement learning, namely GSI and GLI. The experiment results using a public dataset demonstrate that both GSI and GLI can significantly reduce the maximum link utilization, where GLI achieves a saving of 59.1% against the default method.}
}


@article{DBLP:journals/ton/ZhangGZ22,
	author = {Yu Zhang and
                  Tao Gu and
                  Xi Zhang},
	title = {MDLdroid: {A} ChainSGD-Reduce Approach to Mobile Deep Learning for
                  Personal Mobile Sensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {134--147},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3103846},
	doi = {10.1109/TNET.2021.3103846},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangGZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personal mobile sensing is fast permeating our daily lives to enable activity monitoring, healthcare and rehabilitation. Combined with deep learning, these applications have achieved significant success in recent years. Different from conventional cloud-based paradigms, running deep learning on devices offers several advantages including data privacy preservation and low-latency response for both model inference and update. Since data collection is costly in reality, Google’s Federated Learning offers not only complete data privacy but also better model robustness based on data from multiple users. However, personal mobile sensing applications are mostly user-specific and highly affected by environment. As a result, continuous local changes may seriously affect the performance of a global model generated by Federated Learning. In addition, deploying Federated Learning on a local server, e.g., edge server, may quickly reach the bottleneck due to resource limitation. Towards pushing deep learning on devices, we present MDLdroid, a novel decentralized mobile deep learning framework to enable resource-aware on-device collaborative learning for personal mobile sensing applications. To address resource limitation, we propose a ChainSGD-reduce approach which includes a novel chain-directed Synchronous Stochastic Gradient Descent algorithm to effectively reduce overhead among multiple devices. We also design an agent-based multi-goal reinforcement learning mechanism to balance resources in a fair and efficient manner. Our evaluations show that our model training on off-the-shelf mobile devices achieves 2x to 3.5x faster than single-device training, and 1.5x faster on average than the existing master-slave approach.}
}


@article{DBLP:journals/ton/ReisizadehPPA22,
	author = {Amirhossein Reisizadeh and
                  Saurav Prakash and
                  Ramtin Pedarsani and
                  Amir Salman Avestimehr},
	title = {CodedReduce: {A} Fast and Robust Framework for Gradient Aggregation
                  in Distributed Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {148--161},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3109097},
	doi = {10.1109/TNET.2021.3109097},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ReisizadehPPA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We focus on the commonly used synchronous Gradient Descent paradigm for large-scale distributed learning, for which there has been a growing interest to develop efficient and robust gradient aggregation strategies that overcome two key system bottlenecks: communication bandwidth and stragglers’ delays. In particular, Ring-AllReduce ( RAR ) design has been proposed to avoid bandwidth bottleneck at any particular node by allowing each worker to only communicate with its neighbors that are arranged in a logical ring. On the other hand, Gradient Coding ( GC ) has been recently proposed to mitigate stragglers in a master-worker topology by allowing carefully designed redundant allocation of the data set to the workers. We propose a joint communication topology design and data set allocation strategy, named CodedReduce ( CR ), that combines the best of both RAR and GC . That is, it parallelizes the communications over a tree topology leading to efficient bandwidth utilization, and carefully designs a redundant data set allocation and coding strategy at the nodes to make the proposed gradient aggregation scheme robust to stragglers. In particular, we quantify the communication parallelization gain and resiliency of the proposed CR scheme, and prove its optimality when the communication topology is a regular tree. Moreover, we characterize the expected run-time of CR and show order-wise speedups compared to the benchmark schemes. Finally, we empirically evaluate the performance of our proposed CR design over Amazon EC2 and demonstrate that it achieves speedups of up to\n27.2×\nand\n7.0×\n, respectively over the benchmarks GC and RAR .}
}


@article{DBLP:journals/ton/CabukTD22,
	author = {Umut Can {\c{C}}abuk and
                  Mustafa Tosun and
                  Orhan Dagdeviren},
	title = {MAX-Tree: {A} Novel Topology Formation for Maximal Area Coverage in
                  Wireless Ad-Hoc Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {162--175},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3110675},
	doi = {10.1109/TNET.2021.3110675},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CabukTD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For many wireless ad-hoc network (WANET) applications, including wireless sensor, robotic, and flying ad-hoc networks, area coverage is a major challenge. This challenge, which may include the number of required nodes, cumulative energy consumption, or total distance travelled, involves covering the largest possible area with the least cost. Whatever the scenario and cost functions, efficiently deploying the nodes throughout their entire missions is an important factor. For larger networks, this can best be performed using proper topology formations that are essentially designated geometric graph patterns. As a network topology formation and node deployment strategy, this study presents a unique tree-formed geometric graph pattern. With any given number of nodes, it guarantees the maximum possible combined area coverage. Considering its graph theory and Euclidean characteristics, the pattern is then described as both algebraic and algorithmic relations. The formation is remarkably scalable because it may include an unlimited number of nodes while ensuring connectivity in a tree topology. In addition to presenting a breakdown of multiple features and attributes, including symmetry and fractality, we will make a comparative analysis with six known regular lattices and a line formation. Our analyses then show that MAX-Tree covers up to twice as much area compared to all other regular lattices of the same node cardinalities while being more robust and reliable than a line formation. MAX-Tree can be extremely beneficial for drone networks, smart industry and agriculture applications, ocean-bed monitoring systems, and many other WANET scenarios.}
}


@article{DBLP:journals/ton/ShiYGHC22,
	author = {Zhiguo Shi and
                  Guang Yang and
                  Xiaowen Gong and
                  Shibo He and
                  Jiming Chen},
	title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data
                  Crowdsourcing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {176--189},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105427},
	doi = {10.1109/TNET.2021.3105427},
	timestamp = {Fri, 10 Jun 2022 13:58:40 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiYGHC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.}
}


@article{DBLP:journals/ton/LiuY22,
	author = {Xin Liu and
                  Lei Ying},
	title = {Universal Scaling of Distributed Queues Under Load Balancing in the
                  Super-Halfin-Whitt Regime},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {190--201},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105480},
	doi = {10.1109/TNET.2021.3105480},
	timestamp = {Mon, 02 Oct 2023 15:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers the steady-state performance of load balancing algorithms in a many-server system with distributed queues. The system has\nN\nservers, and each server maintains a local queue with buffer size\nb−1\n, i.e. a server can hold at most one job in service and\nb−1\njobs in the queue. Jobs in the same queue are served according to the first-in-first-out (FIFO) order. The system is operated in a heavy-traffic regime such that the workload per server is\nλ=1−\nN\n−α\nfor\n0.5≤α<1\n. We identify a set of algorithms such that the steady-state queues have the following universal scaling, where universal means that it holds for any\nα∈[0.5,1\n): (i) the number of busy servers is\nλN−o(1)\n; and (ii) the number of servers with two jobs (one in service and one in queue) is\nO(\nN\nα\nlogN)\n; and (iii) the number of servers with more than two jobs is\nO(1/\nN\nr(1−α)−1\n)\n, where\nr\ncan be any positive integer independent of\nN\n. The set of load balancing algorithms that satisfy the sufficient condition includes join-the-shortest-queue (JSQ), idle-one-first (I1F), and power-of-\nd\n-choices (Po\nd\n) with\nd≥2\nN\nα\nlogN\n. We further argue that the waiting time of such an algorithm is near optimal order-wise.}
}


@article{DBLP:journals/ton/SaadCNTM22,
	author = {Muhammad Saad and
                  Victor Cook and
                  Lan N. Nguyen and
                  My T. Thai and
                  David Mohaisen},
	title = {Exploring Partitioning Attacks on the Bitcoin Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {202--214},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3105604},
	doi = {10.1109/TNET.2021.3105604},
	timestamp = {Tue, 20 Dec 2022 21:20:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SaadCNTM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bitcoin is the leading example of a blockchain application that facilitates peer-to-peer transactions without the need for a trusted third party. This paper considers possible attacks related to the decentralized network architecture of Bitcoin. We perform a data driven study of Bitcoin and present possible attacks based on spatial and temporal characteristics of its network. Towards that, we revisit the prior work, dedicated to the study of centralization of Bitcoin nodes over the Internet, through a fine-grained analysis of network distribution, and highlight the increasing centralization of the Bitcoin network over time. As a result, we show that Bitcoin is vulnerable to spatial, temporal, spatio-temporal, and logical partitioning attacks with an increased attack feasibility due to the network dynamics. We verify our observations through data-driven analyses and simulations, and discuss the implications of each attack on the Bitcoin network. We conclude with suggested countermeasures.}
}


@article{DBLP:journals/ton/ZhangWLYKJJ22,
	author = {Xiaoxi Zhang and
                  Jianyu Wang and
                  Li{-}Feng Lee and
                  Tom Yang and
                  Akansha Kalra and
                  Gauri Joshi and
                  Carlee Joe{-}Wong},
	title = {Machine Learning on Volatile Instances: Convergence, Runtime, and
                  Cost Tradeoffs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {215--228},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3112082},
	doi = {10.1109/TNET.2021.3112082},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWLYKJJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the massive size of the neural network models and training datasets used in machine learning today, it is imperative to distribute stochastic gradient descent (SGD) by splitting up tasks such as gradient evaluation across multiple worker nodes. However, running distributed SGD can be prohibitively expensive because it may require specialized computing resources such as GPUs for extended periods of time. We propose cost-effective strategies to exploit volatile cloud instances that are cheaper than standard instances, but may be interrupted by higher priority workloads. To the best of our knowledge, this work is the first to quantify how variations in the number of active worker nodes (as a result of preemption) affect SGD convergence and the time to train the model. By understanding these trade-offs between preemption probability of the instances, accuracy, and training time, we are able to derive practical strategies for configuring distributed SGD jobs on volatile instances such as Amazon EC2 spot instances and other preemptible cloud instances. Experimental results show that our strategies achieve good training performance at substantially lower cost.}
}


@article{DBLP:journals/ton/TsaiW22,
	author = {Cho{-}Hsin Tsai and
                  Chih{-}Chun Wang},
	title = {Unifying AoI Minimization and Remote Estimation - Optimal Sensor/Controller
                  Coordination With Random Two-Way Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {229--242},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3111495},
	doi = {10.1109/TNET.2021.3111495},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TsaiW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ubiquitous usage of communication networks in modern sensing and control applications has kindled new interests on the timing coordination between sensors and controllers, i.e., how to use the “waiting time” to improve the system performance. Contrary to the common belief that a zero-wait policy is optimal, Sun et al. showed that a controller can strictly improve the data freshness, the so-called Age-of-Information (AoI), by postponing transmission in order to lengthen the duration of staying in a good state. The optimal waiting policy for the sensor side was later characterized in the context of remote estimation. Instead of focusing on the sensor and controller sides separately, this work develops the jointly optimal sensor/controller waiting policy in a Wiener-process system. This work generalizes the above two important results in the sense that not only do we consider joint sensor/controller designs (as opposed to sensor-only or controller-only schemes), but we also assume random delay in both the forward and feedback directions (as opposed to random delay in only one direction). In addition to provable optimality, extensive simulation is used to verify the performance of the proposed scheme.}
}


@article{DBLP:journals/ton/LiTZFYCL22,
	author = {Xu Li and
                  Feilong Tang and
                  Yanmin Zhu and
                  Luoyi Fu and
                  Jiadi Yu and
                  Long Chen and
                  Jiacheng Liu},
	title = {Processing-While-Transmitting: Cost-Minimized Transmission in SDN-Based
                  STINs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {243--256},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3107413},
	doi = {10.1109/TNET.2021.3107413},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiTZFYCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing Space-Terrestrial Integrated Network (STIN) applications collect all data from multiple satellites and terrestrial nodes to the specific analyze center on the earth for processing, which wastes lots of network resources. To save these resources, we propose a novel processing-while-transmitting pattern in the SDN-based STIN architecture. Through a logically centralized control plane, it cooperatively processes a complex task on appropriate nodes during data transmission. Here, the key point is to jointly determine the transmission path and place subtasks adaptive to data distributions, heterogeneous link costs, task characteristics, the dynamic topology, and network resources. In this paper, we firstly formulate the Transmission-cost-minimized joint Routing and Tasks placement Problem (TRTP) in time-varying STINs. We prove it is NP-hard and has no Polynomial-Time Approximation Scheme (PTAS). To solve the problem, we propose the Joint Routing and Task Placement (JRTP) algorithm. It first converts the time-varying STIN to a stable graph to cope with the network dynamics, according to the topology and resources during task processing. Then, it jointly decides the routing and task placement through a task-topology graph model , which converts the TRTP problem on the stable graph to the classic shortest path problem. We prove that the performance of JRTP is bounded in cases when transmission resources are sufficient and further improve it through the idea of reinforcement. The experimental results show that our processing pattern can significantly decrease the transmission cost and delay, and our algorithms outperform most related ones.}
}


@article{DBLP:journals/ton/LiXLGG22,
	author = {Chong Li and
                  Sisu Xi and
                  Chenyang Lu and
                  Roch Gu{\'{e}}rin and
                  Christopher D. Gill},
	title = {Virtualization-Aware Traffic Control for Soft Real-Time Network Traffic
                  on Xen},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {257--270},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3114055},
	doi = {10.1109/TNET.2021.3114055},
	timestamp = {Fri, 01 Apr 2022 11:22:48 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiXLGG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the role of virtualization technology becomes more prevalent, the range of applications deployed in virtualized systems is steadily growing. This increasingly includes applications with soft real-time requirements that benefit from low and predictable latency, even when co-located with other virtualized hosts with arbitrary traffic patterns. In this paper, we examine the policies and mechanisms affecting communication latency between virtual machines based on the Xen platform, and identify limitations that can result in long or unpredictable network stack latency for virtual machines deployed on this platform. To address these limitations, we propose and implement VATC , a Virtualization-Aware Traffic Control framework that supports differentiation (via rate-limited prioritization) of outbound and inbound network traffic from co-located virtualized hosts. Results of our experiments show how and why VATC can offer predictable (soft) latency guarantees to applications running on virtualized hosts with minimum overhead.}
}


@article{DBLP:journals/ton/HuZLZ22,
	author = {Yuchong Hu and
                  Xiaoyang Zhang and
                  Patrick P. C. Lee and
                  Pan Zhou},
	title = {NCScale: Toward Optimal Storage Scaling via Network Coding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {271--284},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3106394},
	doi = {10.1109/TNET.2021.3106394},
	timestamp = {Thu, 20 Jun 2024 15:06:43 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuZLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To adapt to the increasing storage demands and varying storage redundancy requirements, practical distributed storage systems need to support storage scaling by relocating currently stored data to different storage nodes. However, the scaling process inevitably transfers substantial data traffic over the network. Thus, minimizing the bandwidth cost of the scaling process is critical in distributed settings. In this paper, we show that optimal storage scaling is achievable in erasure-coded distributed storage based on network coding, by allowing storage nodes to send encoded data during scaling. We formally prove the information-theoretically minimum scaling bandwidth for both scale-out and scale-in cases. Based on our theoretical findings, we also build a distributed storage system prototype NCScale based on Hadoop Distributed File System, so as to realize network-coding-based scaling while preserving the necessary properties for practical deployment. Experiments on Amazon EC2 show that the scaling time can be reduced by up to 50% over the state-of-the-art.}
}


@article{DBLP:journals/ton/ZhangWJWQXL22,
	author = {Sheng Zhang and
                  Can Wang and
                  Yibo Jin and
                  Jie Wu and
                  Zhuzhong Qian and
                  Mingjun Xiao and
                  Sanglu Lu},
	title = {Adaptive Configuration Selection and Bandwidth Allocation for Edge-Based
                  Video Analytics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {285--298},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3106937},
	doi = {10.1109/TNET.2021.3106937},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWJWQXL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Major cities worldwide have millions of cameras deployed for surveillance, business intelligence, traffic control, crime prevention, etc. Real-time analytics on video data demands intensive computation resources and high energy consumption. Traditional cloud-based video analytics relies on large centralized clusters to ingest video streams. With edge computing, we can offload compute-intensive analysis tasks to nearby servers, thus mitigating long latency incurred by data transmission via wide area networks. When offloading video frames from the front-end device to an edge server, the application configuration (i.e., frame sampling rate and frame resolution) will impact several metrics, such as energy consumption, analytics accuracy and user-perceived latency. In this paper, we study the configuration selection and bandwidth allocation for multiple video streams, which are connected to the same edge node sharing an upload link. We propose an efficient online algorithm, called JCAB, which jointly optimizes configuration adaption and bandwidth allocation to address a number of key challenges in edge-based video analytics systems, including edge capacity limitation, unknown network variation, intrusive dynamics of video contents. Our algorithm is developed based on Lyapunov optimization and Markov approximation, works online without requiring future information, and achieves a provable performance bound. We also extend the proposed algorithms to the multi-edge scenario in which each user or video stream has an additional choice about which edge server to connect. Extensive evaluation results show that the proposed solutions can effectively balance the analytics accuracy and energy consumption while keeping low system latency in a variety of settings.}
}


@article{DBLP:journals/ton/MaCZXO22,
	author = {Chaoyi Ma and
                  Shigang Chen and
                  Youlin Zhang and
                  Qingjun Xiao and
                  Olufemi O. Odegbile},
	title = {Super Spreader Identification Using Geometric-Min Filter},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {299--312},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3108033},
	doi = {10.1109/TNET.2021.3108033},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaCZXO22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Super spreader identification has a lot of applications in network management and security monitoring. It is a more difficult problem than heavy hitter identification because flow spread is harder to measure than flow size due to the requirement of duplicate removal. The prior work either incurs heavy memory overhead or requires heavy computations. This paper designs a new super-spreader monitor capable of identifying all flows whose spreads are greater than a user-specified threshold with a probability that can be arbitrarily set. It introduces a generalized geometric hash function, a generalized geometric counter, and a novel geometric-min filter that blocks out the vast majority of small/medium flows from being tracked, allowing us to focus on a small number of flows in which super spreaders are identified. We provide an analytical way of properly setting the system threshold to meet probabilistically guaranteed identification of super spreaders, and implement it on both hardware (FPGA) and software platforms. We perform extensive experiments based on real Internet traffic traces from CAIDA. The results show that with proper parameter settings, the new monitor can identify more than 99% super spreaders with a low memory requirement, better than the prior art.}
}


@article{DBLP:journals/ton/XieQGWWC22,
	author = {Junjie Xie and
                  Chen Qian and
                  Deke Guo and
                  Minmei Wang and
                  Ge Wang and
                  Honghui Chen},
	title = {{COIN:} An Efficient Indexing Mechanism for Unstructured Data Sharing
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {313--326},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3110782},
	doi = {10.1109/TNET.2021.3110782},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieQGWWC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing promises a dramatic reduction in the network latency and the traffic volume, where many edge servers are placed at the edge of the Internet. Furthermore, those edge servers cache data to provide services for edge users. The data sharing among those edge servers can effectively shorten the latency to retrieve the data and further reduce the network bandwidth consumption. The key challenge is to construct an efficient data indexing mechanism no matter how the data is cached in the edge network. Although this is essential, it is still an open problem. Moreover, existing methods such as the centralized indexing and the DHT indexing in other fields fail to meet the performance demand of edge computing. This paper presents a COordinate-based INdexing (COIN) mechanism for the data sharing in edge computing. COIN maintains a virtual space where switches and data indexes are associated with their coordinates. Then, COIN distributes data indexes to indexing edge servers based on those coordinates. The COIN is effective because any query request from an edge server can be responded when the data has been stored in the edge network. More importantly, COIN is efficient in both routing path lengths and forwarding table sizes for publishing/querying data indexes. We implement COIN in a P4 prototype. Experimental results show that COIN uses 59% shorter path length and 30% less forwarding table entries to retrieve data indexes compared to using Chord, a well-known DHT solution.}
}


@article{DBLP:journals/ton/PangWLLWR22,
	author = {Xiaoyi Pang and
                  Zhibo Wang and
                  Defang Liu and
                  John C. S. Lui and
                  Qian Wang and
                  Ju Ren},
	title = {Towards Personalized Privacy-Preserving Truth Discovery Over Crowdsourced
                  Data Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {327--340},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3110052},
	doi = {10.1109/TNET.2021.3110052},
	timestamp = {Mon, 25 Mar 2024 12:48:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PangWLLWR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Truth discovery is an effective paradigm which could reveal the truth from crowdsouced data with conflicts, enabling data-driven decision-making systems to make quick and smart decisions. The increasing privacy concern promotes users to perturb or encrypt their private data before outsourcing, which poses significant challenges for truth discovery. Although several privacy-preserving truth discovery mechanisms have been proposed, none of them take personal privacy expectation into consideration. In this work, we propose a novel personalized privacy-preserving truth discovery (PPPTD) framework over crowdsourced data streams to achieve timely and accurate truth discovery while guaranteeing the protection of individual privacy. The key challenges of PPPTD lie in improving the accuracy of truth estimation from the perturbed streaming data with personalized protection level. To address these challenges, we first develop a personalized budget initialization mechanism to quantify each user’s privacy protection requirement, and allocate personalized privacy budgets to users according to their privacy requirements. Then we propose a deviation-aware weighted aggregation method to improve the accuracy of truth discovery from streaming data with varying degrees of perturbation. In order to achieve privacy-utility tradeoff, we further propose an influence-aware adaptive budget adjustment mechanism that adaptively re-allocates privacy budgets to users based on the evolution of their influence in the weighted aggregation. We prove that PPPTD can achieve\nϵ\n-differential privacy over the whole data generated by users and satisfy individual personalized privacy requirements. Extensive experiments on two real-world datasets demonstrate the effectiveness of PPPTD.}
}


@article{DBLP:journals/ton/HellemansH22,
	author = {Tim Hellemans and
                  Benny Van Houdt},
	title = {Improved Load Balancing in Large Scale Systems Using Attained Service
                  Time Reporting},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {341--353},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3110186},
	doi = {10.1109/TNET.2021.3110186},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HellemansH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Our interest lies in load balancing jobs in large scale systems consisting of multiple dispatchers and FCFS servers. In the absence of any information on job sizes, a popular load balancing method is the SQ(\nd\n) policy, which uses queue length information reported by the servers to assign incoming jobs. When job sizes are highly variable, using only queue length information is clearly suboptimal and performance can be improved if some indication can be provided to the dispatcher about the size of an ongoing job. In a FCFS server measuring the attained service time of the ongoing job is easy and servers can therefore report this attained service time together with the queue length when queried by a dispatcher. In this paper, we propose and analyse a variety of load balancing policies which improve the SQ(\nd\n) policy by exploiting both the queue length and attained service time to assign jobs, as well as policies for which only the attained service time of the job in service is used. We present a unified analysis for all these policies in a large scale system under the usual asymptotic independence assumptions. The accuracy of the proposed analysis is illustrated using simulation. We present extensive numerical experiments which clearly indicate that a significant improvement in waiting (and thus also in response) time may be achieved by using the attained service time information on top of the queue length of a server. Moreover, the policies which do not make use of the queue length still provide an improved waiting time for moderately loaded systems.}
}


@article{DBLP:journals/ton/BarbetteWKMPC22,
	author = {Tom Barbette and
                  Erfan Wu and
                  Dejan Kostic and
                  Gerald Q. Maguire Jr. and
                  Panagiotis Papadimitratos and
                  Marco Chiesa},
	title = {Cheetah: {A} High-Speed Programmable Load-Balancer Framework With
                  Guaranteed Per-Connection-Consistency},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {354--367},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3113370},
	doi = {10.1109/TNET.2021.3113370},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BarbetteWKMPC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large service providers use load balancers to dispatch millions of incoming connections per second towards thousands of servers. There are two basic yet critical requirements for a load balancer: uniform load distribution of the incoming connections across the servers, which requires to support advanced load balancing mechanisms, and per-connection-consistency (PCC), i.e, the ability to map packets belonging to the same connection to the same server even in the presence of changes in the number of active servers and load balancers. Yet, simultaneously meeting these requirements has been an elusive goal. Today’s load balancers minimize PCC violations at the price of non-uniform load distribution. This paper presents Cheetah, a load balancer that supports advanced load balancing mechanisms and PCC while being scalable, memory efficient, fast at processing packets, and offers comparable resilience to clogging attacks as with today’s load balancers. The Cheetah LB design guarantees PCC for any realizable server selection load balancing mechanism and can be deployed in both stateless and stateful manners, depending on operational needs. We implemented Cheetah on both a software and a Tofino-based hardware switch. Our evaluation shows that a stateless version of Cheetah guarantees PCC, has negligible packet processing overheads, and can support load balancing mechanisms that reduce the flow completion time by a factor of\n2−3×\n.}
}


@article{DBLP:journals/ton/ChenQLW22,
	author = {Ning Chen and
                  Tie Qiu and
                  Zilong Lu and
                  Dapeng Oliver Wu},
	title = {An Adaptive Robustness Evolution Algorithm With Self-Competition and
                  its 3D Deployment for Internet of Things},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {368--381},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3113916},
	doi = {10.1109/TNET.2021.3113916},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenQLW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) includes numerous sensing nodes that constitute a large scale-free network. Optimizing the network topology to increase resistance against malicious attacks is a complex problem, especially on 3-dimension (3D) topological deployment. Heuristic algorithms, particularly genetic algorithms, can effectively cope with such problems. However, conventional genetic algorithms are prone to falling into premature convergence owing to the lack of global search ability caused by the loss of population diversity during evolution. Although this can be alleviated by increasing population size, the additional computational overhead will be incurred. Moreover, after crossover and mutation operations, individual changes in the population are mixed, and loss of optimal individuals may occur, which will slow down the population’s evolution. Therefore, we combine the population state with the evolutionary process and propose an Adaptive Robustness Evolution Algorithm (AREA) with self-competition for scale-free IoT topologies. In AREA, the crossover and mutation operations are dynamically adjusted according to population diversity to ensure global search ability. A self-competitive mechanism is used to ensure convergence. We construct a 3D IoT topology that is optimized by AREA. The simulation results demonstrate that AREA is more effective in improving the robustness of scale-free IoT networks than several existing methods.}
}


@article{DBLP:journals/ton/CohenKLS22,
	author = {Reuven Cohen and
                  Matty Kadosh and
                  Alan Lo and
                  Qasem Sayah},
	title = {{LB} Scalability: Achieving the Right Balance Between Being Stateful
                  and Stateless},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {382--393},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3112517},
	doi = {10.1109/TNET.2021.3112517},
	timestamp = {Tue, 15 Mar 2022 10:19:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CohenKLS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A high performance Layer-4 load balancer (LB) is one of the most important components of a cloud service infrastructure. Such an LB uses network and transport layer information for deciding how to distribute client requests across a group of servers. A crucial requirement for a stateful LB is per connection consistency (PCC); namely, that all the packets of the same connection will be forwarded to the same server, as long as the server is alive, even if the pool of servers or the assignment function changes. The challenge is in designing a high throughput, low latency solution that is also scalable. This paper proposes a highly scalable LB, called Prism, implemented using a programmable switch ASIC. As far as we know, Prism is the first reported stateful LB that can process millions of connections per second and hundreds of millions connections in total, while ensuring PCC. This is due to the fact that Prism forwards all the packets in hardware, even during server pool changes, while avoiding the need to maintain a hardware state per every active connection . We implemented a prototype of the proposed architecture and showed that Prism can scale to 100 million simultaneous connections, and can accommodate more than one pool update per second.}
}


@article{DBLP:journals/ton/MaQLLLLG22,
	author = {Xiaobo Ma and
                  Jian Qu and
                  Jianfeng Li and
                  John C. S. Lui and
                  Zhenhua Li and
                  Wenmao Liu and
                  Xiaohong Guan},
	title = {Inferring Hidden IoT Devices and User Interactions via Spatial-Temporal
                  Traffic Fingerprinting},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {394--408},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3112480},
	doi = {10.1109/TNET.2021.3112480},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaQLLLLG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularization of Internet of Things (IoT) devices in smart home and industry fields, a huge number of IoT devices are connected to the Internet. However, what devices are connected to a network may not be known by the Internet Service Provider (ISP), since many IoT devices are placed within small networks (e.g., home networks) and are hidden behind network address translation (NAT). Without pinpointing IoT devices in a network, it is unlikely for the ISP to appropriately configure security policies and effectively manage the network. Additionally, inferring fine-grained user interactions of IoT devices is also an interesting yet unresolved problem. In this paper, we design an efficient and scalable system via spatial-temporal traffic fingerprinting from an ISP’s perspective in consideration of practical issues like learning-testing asymmetry. Our system can accurately identify typical IoT devices in a network, with the additional capability of identifying what devices are hidden behind NAT and the number of each type of device that share the same IP address. Our system can also detect user interactions and meanwhile identify their (concurrent) number through a multi-output regression model. Through extensive evaluation, we demonstrate that the system can generally identify IoT devices with an F1-Score above 0.999, and estimate the number of the same type of IoT device behind NAT with an average error below 5%. By studying 29 user interactions of 7 devices, we show that our system is promising in detecting user interactions.}
}


@article{DBLP:journals/ton/FengLSFX22,
	author = {Xuewei Feng and
                  Qi Li and
                  Kun Sun and
                  Chuanpu Fu and
                  Ke Xu},
	title = {Off-Path {TCP} Hijacking Attacks via the Side Channel of Downgraded
                  {IPID}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {409--422},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3115517},
	doi = {10.1109/TNET.2021.3115517},
	timestamp = {Tue, 21 Mar 2023 21:07:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FengLSFX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we uncover a new off-path TCP hijacking attack that can be used to terminate victim TCP connections or inject forged data into victim TCP connections by manipulating the new mixed IPID assignment method, which is widely used in Linux kernel version 4.18 and beyond. Our attack has three steps. First, an off-path attacker can downgrade the IPID assignment for TCP packets from the more secure per-socket-based policy to the less secure hash-based policy, thus building a shared IPID counter that forms a side channel in the victim. Second, the attacker detects the presence of TCP connections by observing the side channel of the shared IPID counter. Third, the attacker infers sequence and acknowledgment numbers of the detected connection by observing the side channel. Consequently, the attacker can completely hijack the connection, e.g., resetting the connection or poisoning the data stream. We evaluate the impacts of our attack in the real world, and we uncover that more than 20% of Alexa top 100k websites are vulnerable to our attack. Our case studies of SSH DoS, manipulating web traffic, and poisoning BGP routing tables show its threat on a wide range of applications. Moreover, we demonstrate that our attack can be further extended to exploit IPv4/IPv6 dual-stack networks on increasing the hash collisions and enlarging vulnerable populations. Finally, we analyze the root cause and develop a new IPID assignment method to defeat this attack. We prototype our defense in Linux 4.18 and confirm its effectiveness in the real world.}
}


@article{DBLP:journals/ton/YouZJTL22,
	author = {Lizhao You and
                  Jiahua Zhang and
                  Yili Jin and
                  Hao Tang and
                  Xiao Li},
	title = {Fast Configuration Change Impact Analysis for Network Overlay Data
                  Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {423--436},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3114448},
	doi = {10.1109/TNET.2021.3114448},
	timestamp = {Thu, 29 Dec 2022 17:39:15 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YouZJTL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents the first network configuration verifier that provides fast all-pair reachability analysis of incremental configuration changes for network overlay data center networks (DCNs). Network overlay DCNs leverage distributed routing protocol on edge leaf switches to disseminate overlay routes and establish overlay tunnels. In addition, network overlay DCNs use access control lists, microsegmentation policy, policy-based routing and firewall policy to control east-west and north-south traffic. Although some incremental verification approaches have been proposed, they either do not support certain forwarding features of the network, or are not efficient. Our configuration verifier addresses these issues through the following components: 1) a port predicate based forwarding model that is general to support all features; 2) fine-grained association technique to index possibly affected reachable pairs by changed interfaces in the original network; and 3) required waypoint path computation that finds all reachable pairs related to changed interfaces in the new network. Based on these components, our verifier presents two incremental verification algorithms that are specially designed for different service update cases. Experiment results show that our incremental verification algorithms are accurate and fast. For all-pair reachability, our verifier performs change-impact analysis within 15s for networks with 200 leafs (4000 subnets and 16 million pairs), outperforming existing approaches by up to 10x.}
}


@article{DBLP:journals/ton/ShapiraS22,
	author = {Tal Shapira and
                  Yuval Shavitt},
	title = {{SASA:} Source-Aware Self-Attention for {IP} Hijack Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {437--449},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3115935},
	doi = {10.1109/TNET.2021.3115935},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShapiraS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IP hijack attacks deflect traffic between endpoints through the attacker network, leading to man-in-the-middle attacks. Current detection solutions are only based on AS-level path analysis, while attacks that include data-plane manipulations may exhibit only geographic anomalies and preserve the AS-level route, or hide the problematic AS in the path. Thus, there is a need to develop data-plane analysis frameworks that examine the actual route packets traverse. We introduce here a deep learning system that examines the geography of traceroute measurements to detect malicious routes. We use multiple geolocation services, with various levels of confidence; each also suffers from location errors. Moreover, identifying a hijacked route is not sufficient since an operator presented with a hijack alert needs an indication of the cause for flagging out the problematic route. Thus, we introduce a novel deep learning layer, called Source-Aware Self-Attention ( SASA ), which is an extension of the attention mechanism. SASA learns each data source’s confidence and combines this score with the attention of each router in the route to point out the most problematic one. We validate our IP hijacking classification method using two router data types: coordinates and country location, and show that SASA outperforms the regular self-attention layer, using the same neural network architecture, and achieves extremely high accuracy.}
}


@article{DBLP:journals/ton/ShafieeG22,
	author = {Mehrnoosh Shafiee and
                  Javad Ghaderi},
	title = {Scheduling Coflows With Dependency Graph},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {450--463},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3116133},
	doi = {10.1109/TNET.2021.3116133},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShafieeG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Applications in data-parallel computing typically consist of multiple stages. In each stage, a set of intermediate parallel data flows ( Coflow ) is produced and transferred between servers to enable starting of next stage. While there has been much research on scheduling isolated coflows, the dependency between coflows in multi-stage jobs has been largely ignored. In this paper, we consider scheduling coflows of multi-stage jobs represented by general DAG s (Directed Acyclic Graphs) in a shared data center network, so as to minimize the total weighted completion time of jobs. This problem is significantly more challenging than the traditional coflow scheduling, as scheduling even a single multi-stage job to minimize its completion time is shown to be NP-hard. In this paper, we propose a polynomial-time algorithm with approximation ratio of\nO(μlog(m)/log(log(m)))\n, where\nμ\nis the maximum number of coflows in a job and\nm\nis the number of servers. For the special case that the jobs’ underlying dependency graphs are rooted trees , we modify the algorithm and improve its approximation ratio. To verify the performance of our algorithms, we present simulation results using real traffic traces that show up to 53% improvement over the prior approach. We conclude the paper by providing a result concerning an optimality gap for scheduling coflows with general DAGs.}
}


@article{DBLP:journals/ton/JhunjhunwalaM22,
	author = {Prakirt Raj Jhunjhunwala and
                  Siva Theja Maguluri},
	title = {Low-Complexity Switch Scheduling Algorithms: Delay Optimality in Heavy
                  Traffic},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {1},
	pages = {464--473},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3116606},
	doi = {10.1109/TNET.2021.3116606},
	timestamp = {Tue, 15 Mar 2022 10:19:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JhunjhunwalaM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by applications in data center networks, in this paper, we study the problem of scheduling in an input queued switch. While throughput maximizing algorithms in a switch are well-understood, delay analysis was developed only recently. It was recently shown that the well-known MaxWeight algorithm achieves optimal scaling of mean queue lengths in steady state in the heavy-traffic regime, and is within a factor less than 2 of a universal lower bound. However, MaxWeight is not used in practice because of its high time complexity. In this paper, we study several low complexity algorithms and show that their heavy-traffic performance is identical to that of MaxWeight. We first present a negative result that picking a random schedule does not have optimal heavy-traffic scaling of queue lengths even under uniform traffic. We then show that if one picks the best among two matchings or modifies a random matching even a little, using the so-called flip operation, it leads to MaxWeight like heavy-traffic performance under uniform traffic. We then focus on the case of non-uniform traffic and show that a large class of low time complexity algorithms have the same heavy-traffic performance as MaxWeight, as long as it is ensured that a MaxWeight matching is picked often enough. We also briefly discuss the performance of these algorithms in the large scale heavy-traffic regime when the size of the switch increases simultaneously with the load. Finally, we perform empirical study on a new algorithm to compare its performance with some existing algorithms.}
}


@article{DBLP:journals/ton/NegliaGL22,
	author = {Giovanni Neglia and
                  Michele Garetto and
                  Emilio Leonardi},
	title = {Similarity Caching: Theory and Algorithms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {475--486},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3126368},
	doi = {10.1109/TNET.2021.3126368},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NegliaGL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper focuses on similarity caching systems, in which a user request for an object\no\nthat is not in the cache can be (partially) satisfied by a similar stored object\no\n′\n, at the cost of a loss of user utility. Similarity caching systems can be effectively employed in several application areas, like multimedia retrieval, recommender systems, genome study, and machine learning training/serving. However, despite their relevance, the behavior of such systems is far from being well understood. In this paper, we provide a first comprehensive analysis of similarity caching in the offline, adversarial, and stochastic settings. We show that similarity caching raises significant new challenges, for which we propose the first dynamic policies with some optimality guarantees. We evaluate the performance of our schemes under both synthetic and real request traces.}
}


@article{DBLP:journals/ton/SongVS22,
	author = {Jianhan Song and
                  Gustavo de Veciana and
                  Sanjay Shakkottai},
	title = {Meta-Scheduling for the Wireless Downlink Through Learning With Bandit
                  Feedback},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {487--500},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3117783},
	doi = {10.1109/TNET.2021.3117783},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SongVS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study learning-assisted multi-user scheduling for the wireless downlink. There have been many scheduling algorithms developed that optimize for a plethora of performance metrics; however a systematic approach across diverse performance metrics and deployment scenarios is still lacking. We address this by developing a meta-scheduler – given a diverse collection of schedulers, we develop a learning-based overlay algorithm (meta-scheduler) that selects that “best” scheduler from amongst these for each deployment scenario. More formally, we develop a multi-armed bandit (MAB) framework for meta-scheduling that assigns and adapts a score for each scheduler to maximize reward (e.g., mean delay, timely throughput etc.). The meta-scheduler is based on a variant of the Upper Confidence Bound algorithm (UCB), but adapted to interrupt the queuing dynamics at the base-station so as to filter out schedulers that might render the system unstable. We show that the algorithm has a poly-logarithmic regret in the expected reward with respect to a genie that chooses the optimal scheduler for each scenario. Finally through simulation, we show that the meta-scheduler learns the choice of the scheduler to best adapt to the deployment scenario (e.g. load conditions, performance metrics).}
}


@article{DBLP:journals/ton/FengAWW22,
	author = {Cuiying Feng and
                  Jianwei An and
                  Kui Wu and
                  Jianping Wang},
	title = {Bound Inference and Reinforcement Learning-Based Path Construction
                  in Bandwidth Tomography},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {501--514},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3118006},
	doi = {10.1109/TNET.2021.3118006},
	timestamp = {Sun, 02 Oct 2022 15:51:56 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FengAWW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inferring the bandwidth of internal links from the bandwidth of end-to-end paths, so-termed bandwidth tomography, is a long-standing open problem in the network tomography literature. The difficulty is due to the fact that no existing mathematical tool is directly applicable to solve the inverse problem with a set of\nmin\n-equations. We systematically tackle this challenge by designing a polynomial-time algorithm that returns the exact bandwidth value for all identifiable links and the tightest error bound for unidentifiable links for a given set of measurement paths. When the measurement paths are not given in advance, we prove the hardness of building measurement paths that can be used for deriving the global tightest error bounds for unidentifiable links. Accordingly, we develop a reinforcement learning (RL) approach for measurement path construction, that utilizes the special knowledge in bandwidth tomography and integrates both offline training and online prediction. Evaluation results with real-world ISP topology as well as simulated networks demonstrate that compared to other path construction methods, Random and Diversity Preferred , our RL-based path construction method can build measurement paths that result in a much smaller average error bound of the link bandwidth.}
}


@article{DBLP:journals/ton/QuLQSYH22,
	author = {Dapeng Qu and
                  Guoxin Lv and
                  Shijun Qu and
                  Haiying Shen and
                  Yue Yang and
                  Zhaoyang Heng},
	title = {An Effective and Lightweight Countermeasure Scheme to Multiple Network
                  Attacks in {NDN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {515--528},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3121001},
	doi = {10.1109/TNET.2021.3121001},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QuLQSYH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Named Data Networking, cache pollution, cache poisoning and interest flooding are three popular types of attacks that can drastically degrade the network performance. However, previous methods for mitigating these attacks are not sufficiently effective or efficient. Also, they cannot simultaneously handle the three attacks, or the case that core routers or edge routers are compromised. To handle these problems, we propose an effective and lightweight countermeasure scheme. It consists of token-based router monitoring policy (TRM), hierarchical consensus-based trust management (HCT), and popularity-based probabilistic caching and caching replacement policy (PPC). In TRM, each edge router monitors and evaluates each data requester’s probability of launching the cache pollution attack and each data provider’s probability of launching the cache poisoning attack, and accordingly assigns, rewards and penalizes tokens to them to control their data request and data provision activities. Thus, the interest flooding attack can also be mitigated by limiting the consumption of tokens. In HCT, each core router manages its directly connected edge routers using TRM, and the core routers trust each other through adopting the concept of consensus in Blockchain. Thus, the edge and core routers executing monitoring and evaluation are trustable. PPC uses probabilistic caching and caching replacement based on the popularity of received content to further mitigate the attacks and reduce caching and data verification overhead. Results from simulation experiments demonstrate that our proposed scheme has better performance, in terms of interest satisfaction ratio and average end-to-end delay than current mechanisms.}
}


@article{DBLP:journals/ton/LiLYZLL22,
	author = {Zhuo Li and
                  Jindian Liu and
                  Liu Yan and
                  Beichuan Zhang and
                  Peng Luo and
                  Kaihua Liu},
	title = {Smart Name Lookup for {NDN} Forwarding Plane via Neural Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {529--541},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3119769},
	doi = {10.1109/TNET.2021.3119769},
	timestamp = {Sun, 29 Jan 2023 15:38:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLYZLL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Name lookup is a key technology for the forwarding plane of content router in Named Data Networking (NDN). To realize the efficient name lookup, what counts is deploying a high-performance index in content routers. So far, the proposed indexes have shown good performance, most of which are optimized for or evaluated with URLs collected from the current Internet, as the large-scale NDN names are not available yet. Unfortunately, the performance of these indexes is always impacted in terms of lookup speed, memory consumption and false positive probability, as the distributions of URLs retrieved in memory may differ from those of real NDN names independently generated by content-centric applications online. Focusing on this gap, a smart mapping model named Pyramid-NN via neural networks is proposed to build an index called LNI for NDN forwarding plane. Through learning the distributions of the names retrieved in the static memory, LNI that will be trained by real NDN names offline and preset in content routers in the future can not only reduce the memory consumption and the probability of false positive, but also ensure the performance of real NDN name lookup. Experimental results show that LNI-based FIB can reduce the memory consumption to 58.258 MB. Moreover, as it can be deployed on SRAMs, the throughput is about 177 MSPS, which well meets the current network requirement for fast packet processing.}
}


@article{DBLP:journals/ton/HuZBWQCTW22,
	author = {Shuihai Hu and
                  Gaoxiong Zeng and
                  Wei Bai and
                  Zilong Wang and
                  Baochen Qiao and
                  Kai Chen and
                  Kun Tan and
                  Yi Wang},
	title = {Aeolus: {A} Building Block for Proactive Transport in Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {542--556},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3119986},
	doi = {10.1109/TNET.2021.3119986},
	timestamp = {Tue, 09 Apr 2024 09:15:19 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuZBWQCTW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As datacenter network bandwidth keeps growing, proactive transport becomes attractive, where bandwidth is proactively allocated as “credits” to senders who then can send “scheduled packets” at a right rate to ensure high link utilization, low latency, and zero packet loss. Consequently, proactive solutions such as ExpressPass, NDP, Homa, etc., have been proposed recently. While promising, a fundamental challenge is that proactive transport requires at least one-RTT for credits to be computed and delivered. In this paper, we show such one-RTT “pre-credit” phase could carry a substantial amount of flows at high link-speeds, but none of existing proactive solutions treats it appropriately. We present Aeolus, a solution focusing on “pre-credit” packet transmission as a building block for proactive transports. Aeolus contains unconventional design principles such as scheduled-packet-first (SPF) that de-prioritizes the first-RTT packets, instead of prioritizing them as prior work. It further exploits the preserved, deterministic nature of proactive transport as a means to recover lost first-RTT packets efficiently. Aeolus is compatible with all existing proactive solutions and readily implementable with commodity switches. We have integrated Aeolus into ExpressPass, NDP and Homa, and shown, via both implementation and simulations, that the Aeolus-enhanced solutions deliver significant performance or deployability advantages. For example, it improves the average FCT of ExpressPass by 56%, cuts the tail FCT of Homa by\n20×\n, while achieving similar performance as NDP without switch modifications.}
}


@article{DBLP:journals/ton/SunLDWWWZ22,
	author = {Yu Sun and
                  Chi Lin and
                  Haipeng Dai and
                  Pengfei Wang and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Trading off Charging and Sensing for Stochastic Events Monitoring
                  in WRSNs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {557--571},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3122130},
	doi = {10.1109/TNET.2021.3122130},
	timestamp = {Fri, 24 Nov 2023 12:41:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunLDWWWZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an epoch-making technology, wireless power transfer incredibly achieves energy transmission wirelessly, enabling reliable energy supplement for Wireless Rechargeable Sensor Networks (WRSNs). Existing methods mainly concentrate on performance improvement theoretically, neglecting the fact that most Commercial Off-The-Shelf (COTS) rechargeable sensors ( e.g. , WISP and Powercast) are not allowed to conduct sensing and energy harvesting tasks simultaneously, termed charging exclusivity . Therefore, their schemes are not feasible for practical applications. In this paper, we focus on the charging exclusivity issue in stochastic events monitoring while improving network performance. In specific, we pay close attention to trading off charging and sensing tasks and formulate a combinatorial optimization problem with routing constraints. We introduce novel discretization techniques and investigate the routing problem to reformulate the original problem into maximization of a submodular function. With a slightly relaxed budget, the output of our proposed algorithm is better than\n(1−1/e)/2\nof the optimal solution to the original problem with a smaller charging radius\n(1−ξ)\nD\nc\n. Through extensive simulations, numerical results show that in terms of charging utility, our algorithm outperforms baseline algorithms by 21.3% on average. Moreover, we conduct test-bed experiments to demonstrate the feasibility of our scheme in real scenarios.}
}


@article{DBLP:journals/ton/WangGL22,
	author = {Shuai Wang and
                  Jinkun Geng and
                  Dan Li},
	title = {Impact of Synchronization Topology on {DML} Performance: Both Logical
                  Topology and Physical Topology},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {572--585},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3117042},
	doi = {10.1109/TNET.2021.3117042},
	timestamp = {Tue, 03 May 2022 16:07:27 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangGL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To tackle the increasingly larger training data and models, researchers and engineers resort to multiple servers in a data center for distributed machine learning (DML). On one hand, DML enables us to leverage the computation power of multiple servers, which can effectively accelerate those computation-intensive tasks. On the other hand, DML also incurs significant communication cost due to parameter synchronization among these servers. In this paper, we want to explore the impact of synchronization topology, including both logical topology and physical topology, on the DML performance. First, we revisit the existing logical topologies, e.g., parameter server and ring allreduce, for parameter synchronization, and we find that these flat synchronization topologies is inefficient when running a large-scale DML training. Therefore, we propose a hierarchical parameter synchronization topology, called HiPS, which can achieve efficient parameter synchronization even on a large scale. Then, we compare two representative physical network topologies, namely, Fat-Tree and BCube. Based on our analyses, BCube has many advantages over Fat-Tree, e.g., higher bandwidth, better load balance, and lower hardware cost. The simulation results also show that BCube is more friendly to RDMA. Relying on the advantages of HiPS and BCube, the GST of “HiPS+BCube” is 12% ~ 70% lower than other combinations. Moreover, when the cluster size increases from 16 to 1024, the performance of “HiPS+BCube” only drops by 6.5%, while the performance of “Ring+BCube” drops by 44.6%. Hence, we believe “HiPS+BCube” is the optimal solution to benefit DML in large scale.}
}


@article{DBLP:journals/ton/LiYYZYMC22,
	author = {Yuanpeng Li and
                  Xiang Yu and
                  Yilong Yang and
                  Yang Zhou and
                  Tong Yang and
                  Zhuo Ma and
                  Shigang Chen},
	title = {Pyramid Family: Generic Frameworks for Accurate and Fast Flow Size
                  Measurement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {586--600},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3120085},
	doi = {10.1109/TNET.2021.3120085},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiYYZYMC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketches, as a kind of probabilistic data structures, have been considered as the most promising solution for network measurement in recent years. Most sketches do not work well for skewed network traffic. To address this problem, we propose a family of sketch frameworks, namely the Pyramid family. The first member of our Pyramid family is the S-Pyramid framework, which includes two techniques: counter-pair sharing for high accuracy, and word acceleration for fast speed. The second member of our Pyramid family is the Mini-Pyramid framework, which projects the S-Pyramid framework into one counter, bringing more flexibility in application while keeping the accuracy. To demonstrate the generality of our Pyramid family, we apply both frameworks to sketches of CM, CU, Count, and Augmented. To demonstrate the flexibility of the Mini-Pyramid framework, we further apply Mini-Pyramid to SBF and the On-Off sketch. The experimental results show that, the S-Pyramid framework can reduce the ARE by up to 7.12 times compared with the original sketches, while improving the throughput by up to 2.37 times; the Mini-Pyramid framework can reduce the ARE by up to 29.2 times, at the cost of 21.3% lower throughput on average.}
}


@article{DBLP:journals/ton/HuangLHL22,
	author = {Yan Huang and
                  Shaoran Li and
                  Y. Thomas Hou and
                  Wenjing Lou},
	title = {{GPF+:} {A} Novel Ultrafast GPU-Based Proportional Fair Scheduler
                  for 5G {NR}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {601--615},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3118005},
	doi = {10.1109/TNET.2021.3118005},
	timestamp = {Thu, 11 May 2023 21:27:18 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangLHL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G NR is designed to operate over a broad range of frequency bands and support new applications with ultra-low latency requirements. To support its extremely diverse operating conditions, multiple OFDM numerologies have been defined in the 5G standards. Under these numerologies, it is necessary to perform scheduling with a time resolution of\n∼100μs\n. This requirement poses a new challenge beyond existing LTE and cannot be satisfied by any existing LTE schedulers. In this paper, we present the design of GPF+, which is a GPU-based proportional fair (PF) scheduler with timing performance under\n100μs\n. GPF+ is an improvement over our GPF in Huang et al. (2018). The key ideas include decomposing the original scheduling problem into a large number of small and independent sub-problems and selecting a subset of sub-problems from the most promising search space to fit into a GPU. By implementing GPF+ on an off-the-shelf NVIDIA Tesla V100 GPU, we show that GPF+ is able to achieve near-optimal PF performance with timing performance under\n100μs\n. GPF+ represents the fastest GPU-based PF scheduler that can meet the new real-time requirement in 5G NR.}
}


@article{DBLP:journals/ton/LananteR22,
	author = {Leonardo Lanante and
                  Sumit Roy},
	title = {Performance Analysis of the {IEEE} 802.11ax OBSS{\_}PD-Based Spatial
                  Reuse},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {616--628},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3117816},
	doi = {10.1109/TNET.2021.3117816},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LananteR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network densification has led to a renewed emphasis on means to improve aggregate network throughput for next-gen (High Efficiency) WLANs. The introduction of BSS color feature in support of enhanced spatial reuse sets IEEE 802.11ax apart from legacy WLAN. BSS color enables stations within a network to identify whether a transmission originates from an overlapped BSS and provides an option for mitigating the exposed node problem by allowing it to transmit (over the on-going OBSS transmission) albeit with reduced transmit power. As a result, distances between networks can be reduced, thereby improving the area spectral efficiency. To the best of our knowledge, this is the first work to develop an analytical model for IEEE 802.11ax spatial reuse that provides useful rules for optimizing network area throughput. We show that the spatial reuse gain is tightly linked to the interference range properties of each BSS in the network. Further, the analytical model predictions are validated and expanded via definitive ns-3 simulation results exploiting a recently concluded upgrade to 802.11 WLAN network stack (that implemented the necessary spatial reuse features).}
}


@article{DBLP:journals/ton/SunGLXLH22,
	author = {Penghao Sun and
                  Zehua Guo and
                  Junfei Li and
                  Yang Xu and
                  Julong Lan and
                  Yuxiang Hu},
	title = {Enabling Scalable Routing in Software-Defined Networks With Deep Reinforcement
                  Learning on Critical Nodes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {629--640},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3126933},
	doi = {10.1109/TNET.2021.3126933},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SunGLXLH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional routing schemes usually use fixed models for routing policies and thus are not good at handling complicated and dynamic traffic, leading to performance degradation (e.g., poor quality of service). Emerging Deep Reinforcement Learning (DRL) coupled with Software-Defined Networking (SDN) provides new opportunities to improve network performance with automatic traffic analysis and policy generation. However, existing DRL-based routing solutions usually rely on all node information to make routing decisions for the network and hence are both hard to converge in large networks and vulnerable to topology changes. In this paper, we propose ScaleDeep, a scalable DRL-based routing scheme for SDN, which improves the routing performance and is resilient to topology changes. Essentially, ScaleDeep takes advantage of partial control on network nodes and DRL. We select a set of critical nodes from a network as driver nodes, which can simulate the entire network operation, based on the control theory. By observing the traffic variation on the driver nodes, DRL dynamically adjusts some link weights for a weighted shortest path algorithm to change the routing paths and improve the routing performance. Limiting the control on driver nodes improves the convergence ability of DRL and reduces the dependency of the DRL agent on the fixed network topology. To validate the performance of ScaleDeep, we conduct packet-level simulations on different topologies. The results show that ScaleDeep outperforms existing DRL-based schemes by reducing the average flow completion time by up to 36% and exhibiting better robustness against minor topology changes.}
}


@article{DBLP:journals/ton/GuoSHZJSL22,
	author = {Xiuzhen Guo and
                  Longfei Shangguan and
                  Yuan He and
                  Jia Zhang and
                  Haotian Jiang and
                  Awais Ahmad Siddiqi and
                  Yunhao Liu},
	title = {Efficient Ambient LoRa Backscatter With On-Off Keying Modulation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {641--654},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3121787},
	doi = {10.1109/TNET.2021.3121787},
	timestamp = {Tue, 20 Dec 2022 21:20:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoSHZJSL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter communication holds potential for ubiquitous and low-cost connectivity among low-power IoT devices. To avoid interference between the carrier signal and the backscatter signal, recent works propose a frequency-shifting technique to separate these two signals in the frequency domain. Such proposals, however, have to occupy the precious wireless spectrum that is already overcrowded, and increase the power, cost, and complexity of the backscatter tag. In this paper, we revisit the classic ON-OFF Keying (OOK) modulation and propose Aloba, a backscatter system that takes the ambient LoRa transmissions as the excitation and piggybacks the in- band OOK modulated signals over the LoRa transmissions. Our design enables the backscatter signal to work in the same frequency band of the carrier signal, meanwhile achieving flexible data rate at different transmission range. The key contributions of Aloba include: i) the design of a low-power backscatter tag that can pick up the ambient LoRa signals from other signals; ii) a novel decoding algorithm to demodulate both the carrier signal and the backscatter signal from their superposition. We further adopt link coding mechanism and interleave operation to enhance the reliability of backscatter signal decoding. We implement Aloba and conduct head-to-head comparison with the state-of-the-art LoRa backscatter system PLoRa in various settings. The experiment results show Aloba can achieve 39.5–199.4 Kbps data rate at various distances, 10.4– 52.4\\times\nhigher than PLoRa.}
}


@article{DBLP:journals/ton/Martin-PerezMCG22,
	author = {Jorge Mart{\'{\i}}n{-}P{\'{e}}rez and
                  Francesco Malandrino and
                  Carla{-}Fabiana Chiasserini and
                  Milan Groshev and
                  Carlos J. Bernardos},
	title = {{KPI} Guarantees in Network Slicing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {655--668},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3120318},
	doi = {10.1109/TNET.2021.3120318},
	timestamp = {Wed, 18 May 2022 10:21:50 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Martin-PerezMCG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Thanks to network slicing, mobile networks can now support multiple and diverse services, each requiring different key performance indicators (KPIs). In this new scenario, it is critical to allocate network and computing resources efficiently and in such a way that all KPIs targeted by a service are met. Accounting for all sorts of KPIs (e.g., availability and reliability, besides the more traditional throughput and latency) is an aspect that has been scarcely addressed so far and that requires tailored models and solution strategies. We address this issue by proposing a novel methodology and resource orchestration scheme, named OKpi, which provides high-quality decisions on VNF (Virtual Network Function) placement and data routing, including the selection of radio points of attachment. Importantly, OKpi has polynomial computational complexity and accounts for all KPIs required by each service, and for any resource available from the fog to the cloud. We prove several properties of OKpi and demonstrate that it performs very closely to the optimum under real-world scenarios. We also implement OKpi in a testbed supporting a robot-based, smart factory service, and we present some field tests that further confirm the ability of OKpi to make high-quality decisions.}
}


@article{DBLP:journals/ton/AbolhassaniTE22,
	author = {Bahman Abolhassani and
                  John Tadrous and
                  Atilla Eryilmaz},
	title = {Single vs Distributed Edge Caching for Dynamic Content},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {669--682},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3121098},
	doi = {10.1109/TNET.2021.3121098},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AbolhassaniTE22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing content caching mechanisms are predominantly geared towards easy-access to content that is static once created. However, numerous applications, such as news and dynamic sources with time-varying states, generate ‘dynamic’ content where new updates replace previous versions. This motivates us in this work to study the freshness-driven caching algorithm for dynamic content, which accounts for the changing nature of data content. In particular, we provide new models and analyses of the average operational cost both for the single and distributed edge caching scenarios. In both scenarios, we characterize the performance of the optimal solution and develop algorithms to select the content and the update rate that the user(s) must employ to have low-cost access to fresh content. Moreover, our work reveals new and easy-to-calculate key metrics for quantifying the caching value of dynamic content in terms of their refresh rates, popularity, number of users in the distribute edge caching group, and the fetching and update costs associated with the optimal decisions. We compare the proposed freshness-driven caching strategies with benchmark caching strategies like cache the most popular content. Results demonstrate that freshness-driven caching strategies considerably enhance the utilization of the edge caches with possibly orders-of-magnitude cost reduction. Furthermore, our investigations reveal that the distributed edge caching scenario, benefiting from the multicasting property of wireless service to update the cached content, can be cost-effective compared to the single edge caching, as the number of edge caches increases.}
}


@article{DBLP:journals/ton/ReneAPP22,
	author = {Sergi Rene and
                  Onur Ascigil and
                  Ioannis Psaras and
                  George Pavlou},
	title = {A Congestion Control Framework Based on In-Network Resource Pooling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {683--697},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3128384},
	doi = {10.1109/TNET.2021.3128384},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ReneAPP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion control has traditionally relied on monitoring packet-level performance ( e.g. , latency, loss) through feedback signals propagating end-to-end together with various queue management practices ( e.g. , carefully setting various parameters, such as router buffer thresholds) in order to regulate traffic flow. Due to its end-to-end nature, this approach is known to transfer data according to the path’s slowest link, requiring several RTTs to transmit even a few tens of KB during slow start. In this paper, we take a radically different approach to control congestion, which obviates end-to-end performance monitoring and careful setting of network parameters. The resulting In-Network Resource Pooling Protocol (INRPP) extends the resource pooling principle to exploit in-network resources such as router storage and unused bandwidth along alternative sub-paths. In INRPP, content caches or large ( possibly bloated ) router buffers are used as a place of temporary custody for incoming data packets in a store and forward manner. Data senders push data in the network and when it hits the bottleneck link, in-network caches at every hop store data in excess of the link capacity; nodes progressively move/send data (from one cache to the next) towards the destination. At the same time alternative sub-paths are exploited to move data faster towards the destination. We demonstrate through extensive simulations that INRPP is TCP friendly, and improves flow completion time and fairness by as much as 50% compared to RCP, MPTCP and TCP, under realistic network conditions.}
}


@article{DBLP:journals/ton/LiuJSB22,
	author = {Yuchen Liu and
                  Yubing Jian and
                  Raghupathy Sivakumar and
                  Douglas M. Blough},
	title = {Maximizing Line-of-Sight Coverage for mmWave Wireless LANs With Multiple
                  Access Points},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {698--716},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3122378},
	doi = {10.1109/TNET.2021.3122378},
	timestamp = {Tue, 22 Nov 2022 07:48:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuJSB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we investigate the optimal line-of-sight (LoS) coverage problem for multiple access point (multi-AP) mmWave wireless LANs in indoor scenarios. Due to the weak diffraction ability of mmWave signals at 60 GHz, maintaining LoS communications between APs and client devices is critical to achieve ultra-high data rates with mmWave communications. We focus on the use of multiple APs deployed to maximize LoS coverage in a target area, and we develop multi-AP placements that maximize LoS coverage by means of both analytical and algorithmic methods. We consider two main scenarios, which differ in their assumptions about knowledge of obstacles and clients. In a random-obstacle, random-client scenario, we derive the LoS-optimal positions of APs by solving a thinnest covering problem. For a fixed-obstacle, random-client scenario, we propose an efficient algorithm that produces a multi-AP placement, which is shown through simulation to provide near-optimal LoS coverage. Finally, through extensive ns-3 simulations based on the IEEE 802.11ad protocol and mmWave-specific channel models, we show that our multi-AP placements are significantly better than existing placement approaches, both in terms of LoS coverage and aggregate throughput.}
}


@article{DBLP:journals/ton/CaoYTH22,
	author = {Hankun Cao and
                  Qifa Yan and
                  Xiaohu Tang and
                  Guojun Han},
	title = {Adaptive Gradient Coding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {717--734},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3122873},
	doi = {10.1109/TNET.2021.3122873},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaoYTH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper focuses on mitigating the impact of stragglers in distributed learning system. Unlike the existing results designated for a fixed number of stragglers, we develop a new scheme called Adaptive Gradient Coding (AGC) with flexible communication cost for varying number of stragglers. Our scheme gives an optimal tradeoff between computation load, straggler tolerance and communication cost by allowing workers to send multiple signals sequentially to the master. In particular, it can minimize the communication cost according to the unknown real-time number of stragglers in practical environments. In addition, we present a Group AGC (G-AGC) by combining the group idea with AGC to resist more stragglers in some situations. The numerical and simulation results demonstrate that our adaptive schemes can achieve the smallest average running time.}
}


@article{DBLP:journals/ton/LaiFZZLZ22,
	author = {Pan Lai and
                  Rui Fan and
                  Xiao Zhang and
                  Wei Zhang and
                  Fang Liu and
                  Joey Tianyi Zhou},
	title = {Utility Optimal Thread Assignment and Resource Allocation in Multi-Server
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {735--748},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3123817},
	doi = {10.1109/TNET.2021.3123817},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LaiFZZLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Achieving high performance in many multi-server systems (e.g., web hosting center, cloud) requires finding a good assignment of worker threads to servers and also effectively allocating each server’s resources to its assigned threads. The assignment and allocation components of this problem have been studied extensively but largely separately in the literature. In this paper, we introduce the assign and allocate (AA) problem, which seeks to simultaneously find an assignment and allocation that maximizes the total utility of the threads. Assigning and allocating the threads together can result in substantially better overall utility than performing the steps separately, as is traditionally done. We model each thread by a utility function giving its performance as a function of its assigned resources. We first prove that the AA problem is NP-hard. We then present a 2 (\\sqrt {2}-1) > 0.828\nfactor approximation algorithm for concave utility functions, which runs in O(mn^{2} + n (\\log mC)^{2})\ntime for n\nthreads and m\nservers with C\namount of resources each. We also give a faster algorithm with the same approximation ratio and O(n (\\log mC)^{2})\ntime complexity. We then extend the problem to two more general settings. First, we consider threads with nonconcave utility functions, and give a 1/2 factor approximation algorithm. Next, we give an algorithm for threads using multiple types of resources, and show the algorithm achieves good empirical performance. We conduct extensive experiments to test the performance of our algorithms on threads with both synthetic and realistic utility functions, and find that they achieve over 92% of the optimal utility on average. We also compare our algorithms with a number of practical heuristics, and find that our algorithms achieve up to 9 times higher total utility.}
}


@article{DBLP:journals/ton/LiLLWWLCJW22,
	author = {Bingyu Li and
                  Jingqiang Lin and
                  Fengjun Li and
                  Qiongxiao Wang and
                  Wei Wang and
                  Qi Li and
                  Guangshen Cheng and
                  Jiwu Jing and
                  Congli Wang},
	title = {The Invisible Side of Certificate Transparency: Exploring the Reliability
                  of Monitors in the Wild},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {749--765},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3123507},
	doi = {10.1109/TNET.2021.3123507},
	timestamp = {Tue, 24 May 2022 15:37:11 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLLWWLCJW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To detect fraudulent TLS server certificates and improve the accountability of certification authorities (CAs), certificate transparency (CT) is proposed to record certificates in publicly-visible logs, from which the monitors fetch all certificates and watch for suspicious ones. However, if the monitors, either domain owners themselves or third-party services, fail to return a complete set of certificates issued for a domain of interest, potentially fraudulent certificates may not be detected and then the CT framework becomes less reliable. This paper presents the first systematic study on CT monitors. We analyze the data in 88 public logs and the services of 5 active third-party monitors regarding 3,000,431 certificates of 6,000 selected Alexa Top-1M websites. We find that although CT allows ordinary domain owners to act as monitors, it is impractical for them to perform reliable processing by themselves, due to the rapidly increasing volume of certificates in public logs (e.g., on average about 5 million records or 28.29 GB daily for the minimal set of logs that need to be monitored in 2018, or more than 7 million records per day in 2020, according to the Chrome CT policy). Moreover, our study discloses that (\na\n) none of the third-party monitors guarantees to return the complete set of certificates for a domain, and (\nb\n) for some domains, even the union of the certificates returned by the five third-party monitors can probably be incomplete. As a result, the certificates accepted by CT-enabled browsers are not actually visible to the claimed domain owners, even when CT is adopted with well-functioning logs. The risk of invisible fraudulent certificates in public logs raises doubts on the reliability of CT in practice.}
}


@article{DBLP:journals/ton/DuijnJJKMSST22,
	author = {Ingo van Duijn and
                  Peter Gj{\o}l Jensen and
                  Jesper Stenbjerg Jensen and
                  Troels Beck Kr{\o}gh and
                  Jonas Sand Madsen and
                  Stefan Schmid and
                  Jir{\'{\i}} Srba and
                  Marc Tom Thorgersen},
	title = {Automata-Theoretic Approach to Verification of {MPLS} Networks Under
                  Link Failures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {766--781},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3126572},
	doi = {10.1109/TNET.2021.3126572},
	timestamp = {Thu, 23 Jun 2022 20:02:43 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DuijnJJKMSST22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future communication networks are expected to be highly automated, disburdening human operators of their most complex tasks. While the first powerful and automated network analysis tools are emerging, existing tools provide only limited and inefficient support of reasoning about failure scenarios . We present P-REX, a fast what-if analysis tool, that allows us to test important reachability and policy-compliance properties even under an arbitrary number of failures and in polynomial-time , i.e., without enumerating all failure scenarios (the usual approach today, if supported at all). P-REX targets networks based on Multiprotocol Label Switching (MPLS) and its Segment Routing (SR) extension which feature fast rerouting mechanisms with label stacks. In particular, P-REX allows to reason about recursive backup tunnels, by supporting potentially infinite state spaces. As P-REX directly operates on the actual dataplane configuration, i.e., forwarding tables, it is well-suited for debugging. Our tool comes with an expressive query language based on regular expressions. We also report on an industrial case study and demonstrate that our tool can perform what-if reachability analyses on average in about 5 seconds for a 24-router network with over 250,000 MPLS forwarding rules. This is a significant improvement to an earlier prototype of our tool presented in the conference version of our paper where the verification took on average about 1 hour.}
}


@article{DBLP:journals/ton/MoorthyMG22,
	author = {Sabarish Krishna Moorthy and
                  Maxwell McManus and
                  Zhangyu Guan},
	title = {{ESN} Reinforcement Learning for Spectrum and Flight Control in THz-Enabled
                  Drone Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {782--795},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3128836},
	doi = {10.1109/TNET.2021.3128836},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MoorthyMG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Terahertz (THz)-band communications have been envisioned as a key technology to support ultra-high-data-rate applications in 5G-beyond (or 6G) wireless networks. Compared to the microwave and mmWave bands, the main challenges with the THz band are in its i) large path loss hence limited network coverage and ii) visible-light-like propagation characteristics hence poor support of mobility in blockage-rich environments. This paper studies quantitatively the applicability of THz-band communications in blockage-rich mobile environments, focusing on a new network scenario called FlyTera . In FlyTera , a set of hotspots mounted on flying drones collaboratively provide data streaming services to ground users, in the microwave, mmWave and THz bands. We first provide a mathematical formulation of the FlyTera control problem, where the objective is to maximize the network spectral efficiency by jointly controlling the flight of the drone hotspots, their association to the ground users, and the spectrum bands used by the users. To solve the resulting problem, which is shown to be a mixed integer nonlinear nonconvex programming (MINLP) problem, we design distributed solution algorithms based on a combination of echo state learning and reinforcement learning. An extensive simulation campaign is then conducted with SimBAG, a newly developed Sim ulator of B roadband A erial- G round wireless networks. It is shown that no single spectrum band can meet the requirements of high data rate and wide coverage simultaneously. Moreover, from the network-level point of view, THz-band communications can significantly benefit from the mobility of the flying drones, and on average\n4−6\ntimes higher (rather than lower) throughput can be achieved in mobile than in static environments.}
}


@article{DBLP:journals/ton/PolachanPSTK22,
	author = {Kurian Polachan and
                  Joydeep Pal and
                  Chandramani Singh and
                  Prabhakar Venkata Tamma and
                  Fernando A. Kuipers},
	title = {TCPSbed: {A} Modular Testbed for Tactile Internet-Based Cyber-Physical
                  Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {796--811},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3124767},
	doi = {10.1109/TNET.2021.3124767},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PolachanPSTK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tactile Internet based Cyber-Physical Systems (TCPS) are highly sensitive to component and communication latencies and packet drops. Building a high performing TCPS, thus, necessitates experimenting with different hardware, algorithms, access technologies, and communication protocols. To facilitate such experiments, we have developed TCPSbed, a modular testbed for TCPS. TCPSbed facilitates the integration of different components, both real and simulated, to realize different TCPS applications and evaluate their latency and control performances. TCPSbed’s latency analyzer tool employs a novel method to isolate latencies of individual TCPS components such as the latencies contributed by actuation, sensing, algorithms, and by the network, all in an online fashion. TCPSbed’s method of analyzing stability is also novel. It involves the use of the step response analysis method, a classic control-theoretic method used for analyzing the stability of generic control systems. TCPSbed’s support for edge intelligence modules enables prediction of command and feedback signals at the network’s edge allowing TCPS applications to perform well in adverse network conditions. TCPSbed’s source-code, made available through our GitHub page TactileInternet , allows developers to extend its features and functionalities further. In this paper, we describe the architecture and implementation details of TCPSbed and demonstrate its features through several proof-of-concept experiments.}
}


@article{DBLP:journals/ton/XuXZLXLJD22,
	author = {Wenzheng Xu and
                  Tao Xiao and
                  Junqi Zhang and
                  Weifa Liang and
                  Zichuan Xu and
                  Xuxun Liu and
                  Xiaohua Jia and
                  Sajal K. Das},
	title = {Minimizing the Deployment Cost of UAVs for Delay-Sensitive Data Collection
                  in IoT Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {812--825},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3123606},
	doi = {10.1109/TNET.2021.3123606},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuXZLXLJD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the deployment of Unmanned Aerial Vehicles (UAVs) to collect data from IoT devices, by finding a data collection tour for each UAV. To ensure the ‘freshness’ of the collected data, the total time spent in the tour of each UAV that consists of the UAV flying time and data collection time must be no greater than a given delay B\n, e.g., 20 minutes. In this paper, we consider a problem of deploying the minimum number of UAVs and finding their data collection tours, subject to the constraint that the total time spent in each tour of any UAV is no greater than B\n. Specifically, we study two variants of the problem: one is that a UAV needs to fly to the location of each IoT device to collect its data; the other is that a UAV is able to collect the data of an IoT device if the Euclidean distance between them is no greater than the wireless transmission range of the IoT device. For the first variant of the problem, we propose a novel 4-approximation algorithm, which improves the best approximation ratio 4\\frac {4}{7}\nfor it so far. For the second variant, we devise the very first constant factor approximation algorithm. We also evaluate the performance of the proposed algorithms via extensive experiment simulations. Experimental results show that the numbers of UAVs deployed by the proposed algorithms are from 11% to 19% less than those by existing algorithms on average.}
}


@article{DBLP:journals/ton/ZhangAWMLT22,
	author = {Qiaolun Zhang and
                  Omran Ayoub and
                  Jun Wu and
                  Francesco Musumeci and
                  Gaolei Li and
                  Massimo Tornatore},
	title = {Progressive Slice Recovery With Guaranteed Slice Connectivity After
                  Massive Failures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {826--839},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3130576},
	doi = {10.1109/TNET.2021.3130576},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangAWMLT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In presence of multiple failures affecting their network infrastructure, operators are faced with the Progressive Network Recovery (PNR) problem, i.e., deciding the best sequence of repairs during recovery. With incoming deployments of 5G networks, PNR must evolve to incorporate new recovery opportunities offered by network slicing. In this study, we introduce the new problem of Progressive Slice Recovery (PSR), which is addressed with eight different strategies, i.e., allowing or not to change slice embedding during the recovery, and/or by enforcing different versions of slice connectivity (i.e., network vs. content connectivity). We propose a comprehensive PSR scheme, which can be applied to all recovery strategies and achieves fast recovery of slices. We first prove the PSR’s NP-hardness and design an integer linear programming (ILP) model, which can obtain the best recovery sequence and is extensible for all the recovery strategies. Then, to address scalability issues of the ILP model, we devise an efficient two-phases progressive slice recovery (2-phase PSR) meta-heuristic algorithm, small optimality gap, consisting of two main steps: i) determination of recovery sequence, achieved through a linear-programming relaxation that works in polynomial time; and ii) slice-embedding recovery, for which we design an auxiliary-graph-based column generation to re-embed failed slice nodes/links to working substrate elements within a given number of actions. Numerical results compare the different strategies and validate that amount of recovered slices can be improved up to 50% if operators decide to reconfigure only few slice nodes and guarantee content connectivity.}
}


@article{DBLP:journals/ton/LiuYLCCL22,
	author = {Tzu{-}Hsuan Liu and
                  Che{-}Hao Yu and
                  Yi{-}Jheng Lin and
                  Chia{-}Ming Chang and
                  Cheng{-}Shang Chang and
                  Duan{-}Shin Lee},
	title = {{ALOHA} Receivers: {A} Network Calculus Approach for Analyzing Coded
                  Multiple Access With {SIC}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {840--854},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3123685},
	doi = {10.1109/TNET.2021.3123685},
	timestamp = {Mon, 28 Aug 2023 21:30:47 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuYLCCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the need to hide the complexity of the physical layer from performance analysis in a layer 2 protocol, a class of abstract receivers, called Poisson receivers, was recently proposed by Yu et al. (2021) as a probabilistic framework for providing differentiated services in uplink transmissions in 5G networks. In this paper, we further propose a deterministic framework of ALOHA receivers that can be incorporated into the probabilistic framework of Poisson receivers for analyzing coded multiple access with successive interference cancellation. An ALOHA receiver is characterized by a success function of the number of packets that can be successfully received. Inspired by the theory of network calculus, we derive various algebraic properties for several operations on success functions and use them to prove various closure properties of ALOHA receivers, including (i) ALOHA receivers in tandem, (ii) cooperative ALOHA receivers, (iii) ALOHA receivers with traffic multiplexing, and (iv) ALOHA receivers with packet coding. By conducting extensive simulations, we show that our theoretical results match extremely well with the simulation results.}
}


@article{DBLP:journals/ton/WuMTLC22,
	author = {Renyong Wu and
                  Jieming Ma and
                  Zhixiang Tang and
                  Xiehua Li and
                  Kim{-}Kwang Raymond Choo},
	title = {A Generic Secure Transmission Scheme Based on Random Linear Network
                  Coding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {855--866},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3124890},
	doi = {10.1109/TNET.2021.3124890},
	timestamp = {Wed, 27 Apr 2022 20:10:30 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WuMTLC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unlike general routing strategies, network coding (NC) can combine encoding functions with multi-path propagation over a network. This allows network capacity to be achieved to support complex security solutions. Moreover, NC has intrinsic security advantages against passive attacks over traditional routing techniques. However, due to the transmission of the global encoding kernels, the system is fragile to eavesdropping attacks with multiple probes. This paper proposes a generic unicast secure transmission scheme based on random linear network coding (RLNC). Specifically, the intended receiver generates a random matrix upon receiving the request from the source node, and then transmits each row vector of this matrix over a link reversely to the source node. Each intermediate node rearranges all received vectors to form a matrix by row, and then post-multiplies its local encoding kernel by this matrix to obtain a new matrix. Similarly, each row vector of the new matrix is reversely transmitted over a link to the source node. This procedure is performed until we have the source node, where the generalized inverse of the received matrix (or part of it) can be used as its local encoding kernel. Hence, the intended receiver can use the generated matrix (or the corresponding part) to decode the received data packets directly. We also analyze the security to demonstrate that the proposed scheme is at least as secure as other methods against wiretapping attacks. We also evaluate the performance of the proposed scheme to demonstrate its utility.}
}


@article{DBLP:journals/ton/RongWLWLH22,
	author = {Chenghao Rong and
                  Jessie Hui Wang and
                  Juncai Liu and
                  Jilong Wang and
                  Fenghua Li and
                  Xiaolei Huang},
	title = {Scheduling Massive Camera Streams to Optimize Large-Scale Live Video
                  Analytics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {867--880},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3125359},
	doi = {10.1109/TNET.2021.3125359},
	timestamp = {Thu, 01 Dec 2022 09:58:01 +0100},
	biburl = {https://dblp.org/rec/journals/ton/RongWLWLH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In smart cities, more and more government departments will make use of live analytics of videos from surveillance cameras in their tasks, such as vehicle traffic monitoring and criminal detection. Obviously, it is costly for each individual department to deploy its own infrastructure, i.e. , cameras and analytics system. In this paper, we consider a scenario in which a city deploys an infrastructure and departments submit requests to access and analyze videos for their own purposes. The live analytics of massive streams is computation-intensive and the tasks might be latency-critical, which makes scheduling massive streams to optimize all tasks an essential and challenging work. We exploit an end-edge-cloud architecture and propose an adaptive system to schedule the massive camera streams and tasks, which considers all factors affecting the computation and networking resource consumption, e.g. , sharing of model computation, video quality, model partition, and task placement. Particularly, the resource consumption of Faster R-CNN + ResNet101 under each partition scheme is profiled for the first time and we notice the partition must be used together with lossless compression techniques to be beneficial. Furthermore, sometimes tasks might be required to migrate because the scheduling decision made by the system changes to adapt to the changing resource supply and demand. In order to avoid the performance degradation during migration, we propose a non-destructive migration scheme and implement it in the system. Simulations demonstrate our system achieves a total utility close to the maximum and our analytics system performs better than state-of-the-art solutions.}
}


@article{DBLP:journals/ton/XuSZLXSWJL22,
	author = {Wenzheng Xu and
                  Yueying Sun and
                  Rui Zou and
                  Weifa Liang and
                  Qiufen Xia and
                  Feng Shan and
                  Tian Wang and
                  Xiaohua Jia and
                  Zheng Li},
	title = {Throughput Maximization of {UAV} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {881--895},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3125982},
	doi = {10.1109/TNET.2021.3125982},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuSZLXSWJL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we study the deployment of multiple unmanned aerial vehicles (UAVs) to form a temporal UAV network for the provisioning of emergent communications to affected people in a disaster zone, where each UAV is equipped with a lightweight base station device and thus can act as an aerial base station for users. Unlike most existing studies that assumed that a UAV can serve all users in its communication range, we observe that both computation and communication capabilities of a single lightweight UAV are very limited, due to various constraints on its size, weight, and power supply. Thus, a single UAV can only provide communication services to a limited number of users. We study a novel problem of deploying K\nUAVs in the top of a disaster area such that the sum of the data rates of users served by the UAVs is maximized, subject to that (i) the number of users served by each UAV is no greater than its service capacity; and (ii) the communication network induced by the K\nUAVs is connected. We then propose a \\frac {1-1/e}{\\lfloor \\sqrt {K} \\rfloor }\n-approximation algorithm for the problem, improving the current best result of the problem by five times (the best approximation ratio so far is \\frac {1-1/e}{5(\\sqrt {K} +1)}\n), where e\nis the base of the natural logarithm. We finally evaluate the algorithm performance via simulation experiments. Experimental results show that the proposed algorithm is very promising. Especially, the solution delivered by the proposed algorithm is up to 12% better than those by existing algorithms.}
}


@article{DBLP:journals/ton/ChenXZOZK22,
	author = {Yanjiao Chen and
                  Meng Xue and
                  Jian Zhang and
                  Runmin Ou and
                  Qian Zhang and
                  Peng Kuang},
	title = {DetectDUI: An In-Car Detection System for Drink Driving and BACs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {896--910},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3125950},
	doi = {10.1109/TNET.2021.3125950},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenXZOZK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the biggest contributors to road accidents and fatalities, drink driving is worthy of significant research attention. However, most existing systems on detecting or preventing drink driving either require special hardware or require much effort from the user, making these systems inapplicable to continuous drink driving monitoring in a real driving environment. In this paper, we present DetectDUI , a contactless, non-invasive, real-time system that yields a relatively highly accurate drink driving monitoring by combining vital signs (heart rate and respiration rate) extracted from in-car WiFi system and driver’s psychomotor coordination through steering wheel operations. The framework consists of a series of signal processing algorithms for extracting clean and informative vital signs and psychomotor coordination, and integrate the two data streams using a self-attention convolutional neural network (i.e., C-Attention). In safe laboratory experiments with 15 participants, DetectDUI achieves drink driving detection accuracy of 96.6% and BAC predictions with an average mean error of\n2∼5mg/dl\n. These promising results provide a highly encouraging case for continued development.}
}


@article{DBLP:journals/ton/AnselmiW22,
	author = {Jonatha Anselmi and
                  Neil Walton},
	title = {Stability and Optimization of Speculative Queueing Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {911--922},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3128778},
	doi = {10.1109/TNET.2021.3128778},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AnselmiW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We provide a queueing-theoretic framework for job replication schemes based on the principle “ replicate a job as soon as the system detects it as a straggler ”. This is called job speculation . Recent works have analyzed replication on arrival, which we refer to as replication . Replication is motivated by its implementation in Google’s BigTable. However, systems such as Apache Spark and Hadoop MapReduce implement speculative job execution. The performance and optimization of speculative job execution is not well understood. To this end, we propose a queueing network model for load balancing where each server can speculate on the execution time of a job. Specifically, each job is initially assigned to a single server by a frontend dispatcher. Then, when its execution begins, the server sets a timeout. If the job completes before the timeout, it leaves the network, otherwise the job is terminated and relaunched or resumed at another server where it will complete. We provide a necessary and sufficient condition for the stability of speculative queueing networks with heterogeneous servers, general job sizes and scheduling disciplines. We find that speculation can increase the stability region of the network when compared with standard load balancing models and replication schemes. We provide general conditions under which timeouts increase the size of the stability region and derive a formula for the optimal speculation time, i.e., the timeout that minimizes the load induced through speculation. We compare speculation with redundant-\nd\nand redundant-to-idle-queue-\nd\nrules under an\nS&X\nmodel. For light loaded systems, redundancy schemes provide better response times. However, for moderate to heavy loadings, redundancy schemes can lose capacity and have markedly worse response times when compared with the proposed speculative scheme.}
}


@article{DBLP:journals/ton/WangCHWOCL22,
	author = {Ziyi Wang and
                  Yong Cui and
                  Xiaoyu Hu and
                  Xin Wang and
                  Wei Tsang Ooi and
                  Zhen Cao and
                  Yi Li},
	title = {MultiLive: Adaptive Bitrate Control for Low-Delay Multi-Party Interactive
                  Live Streaming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {923--938},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3129481},
	doi = {10.1109/TNET.2021.3129481},
	timestamp = {Mon, 28 Aug 2023 21:30:48 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangCHWOCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-party interactive live streaming, each user can act as both the sender and the receiver of a live video stream. Designing adaptive bitrate (ABR) algorithm for such applications poses three challenges: (i) due to the interaction requirement among the users, the playback buffer has to be kept small to reduce the end-to-end delay; (ii) the algorithm needs to decide what is the bitrate to receive and what is the set of bitrates to send ; (iii) the delay and quality requirements between each pair of users may differ, for instance, depending on whether the pair is interacting directly with each other. To address these challenges, we first develop a quality of experience (QoE) model for multi-party live streaming applications. Based on this model, we design MultiLive , an adaptive bitrate control algorithm for the multi-party scenario. MultiLive models the many-to-many ABR selection problem as a non-linear programming problem. Solving the non-linear programming equation yields the target bitrate for each pair of sender-receiver. To alleviate system errors during the modeling and measurement process, we update the target bitrate through the buffer feedback adjustment. To address the throughput limitation of the uplink, we cluster the ideal streams into a few groups, and aggregate these streams through scalable video coding for transmissions. We also deploy the algorithm on a commercial live streaming platform that provides such services for more than 2300 users. The experimental results show that MultiLive outperforms the fixed bitrate algorithm, with 2-\n5×\nimprovement in average QoE. Furthermore, the end-to-end delay is reduced to around 100 ms, much lower than the 400 ms threshold recommended for video conferencing.}
}


@article{DBLP:journals/ton/XiZZGSCWXW22,
	author = {Zhaowei Xi and
                  Yu Zhou and
                  Dai Zhang and
                  Kai Gao and
                  Chen Sun and
                  Jiamin Cao and
                  Yangyang Wang and
                  Mingwei Xu and
                  Jianping Wu},
	title = {Newton: Intent-Driven Network Traffic Monitoring},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {2},
	pages = {939--952},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3128557},
	doi = {10.1109/TNET.2021.3128557},
	timestamp = {Wed, 27 Apr 2022 20:10:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiZZGSCWXW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network monitoring systems are designed to fulfill operators’ intents and serve as essential tools to modern networks. As a result of rapidly increasing network bandwidth and scale nowadays, network monitors should satisfy on-demand network monitoring for continuously growing traffic volumes. However, existing monitoring systems either cannot satisfy flexible intents on demand or produce significant overheads. In this paper, we present Newton , an intent-driven traffic monitor that is able to specify operators’ intents with traffic monitoring queries and conduct dynamic and scalable network-wide queries deployment. Newton enables operators to customize and modify queries dynamically without interrupting the network workflow. Besides, Newton proposes systematic optimizations at device level and network-wide level to reduce resource consumption while deploying queries. Newton can combine the resources across switches to deploy complex queries with high resilience to dynamic network status. Evaluations prove that Newton is of high flexibility, scalability, and resource efficiency, which demonstrates Newton is promising to be deployed in large-scale programmable networks.}
}


@article{DBLP:journals/ton/KettanehATMA22,
	author = {Ibrahim Kettaneh and
                  Ahmed Alquraan and
                  Hatem Takruri and
                  Ali Jos{\'{e}} Mashtizadeh and
                  Samer Al{-}Kiswany},
	title = {Accelerating Reads With In-Network Consistency-Aware Load Balancing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {954--968},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3126203},
	doi = {10.1109/TNET.2021.3126203},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KettanehATMA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present FLAIR, a novel approach for accelerating read operations in leader-based consensus protocols. FLAIR leverages the capabilities of the new generation of programmable switches to serve reads from follower replicas without compromising consistency. The core of the new approach is a packet-processing pipeline that can track client requests and system replies, identify consistent replicas, and at line speed, forward read requests to replicas that can serve the read without sacrificing linearizability. An additional benefit of FLAIR is that it facilitates devising novel consistency-aware load balancing techniques. Following the new approach, we designed FlairKV, a key-value store atop Raft. FlairKV implements the processing pipeline using the P4 programming language. We evaluate the benefits of the proposed approach and compare it to previous approaches using a cluster with a Barefoot Tofino switch. Our evaluation indicates that, compared to state-of-the-art alternatives, the proposed approach can bring significant performance gains: up to 42% higher throughput and 35–97% lower latency for most workloads. Furthermore, our evaluation shows that our novel load balancing techniques can cope with heterogeneous load and hardware to achieve higher performance, and that FLAIR can scale to support large data sets and clusters.}
}


@article{DBLP:journals/ton/GuoDLFJXZ22,
	author = {Zehua Guo and
                  Songshi Dou and
                  Sen Liu and
                  Wendi Feng and
                  Wenchao Jiang and
                  Yang Xu and
                  Zhi{-}Li Zhang},
	title = {Maintaining Control Resiliency and Flow Programmability in Software-Defined
                  WANs During Controller Failures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {969--984},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3128771},
	doi = {10.1109/TNET.2021.3128771},
	timestamp = {Thu, 17 Nov 2022 09:51:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoDLFJXZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing resilient network control is a critical concern for deploying Software-Defined Networking (SDN) into Wide-Area Networks (WANs). For performance reasons, a Software-Defined WAN is divided into multiple domains controlled by multiple controllers with a logically centralized view. Under controller failures, we need to remap the control of offline switches from failed controllers to other active controllers. Existing solutions have three limitations: (1) the least flow programmability (e.g., the ability to change paths of flows) cannot be maintained; (2) active controllers could be overloaded, interrupting their normal operations; (3) network performance could be degraded because of the increasing controller-switch communication overhead. In this paper, we propose RetroFlow+ to recover the flow programmability and achieve low communication overhead during controller failures. By intelligently configuring a set of selected offline switches working under the legacy routing mode and several active controllers releasing a few control resources, RetroFlow+ enables active controllers to use the minimum control resource to sustain the flow programmability. RetroFlow+ also smartly transfers the control of offline switches with the SDN routing mode to active controllers to minimize the communication overhead from these offline switches to the active controllers. Simulation results show that RetroFlow+ realizes low communication overhead, recovers all offline flows under one and two controller failures, and improves the flow recovery percentage up to 70% under three controller failures, compared with the state-of-the-art solution.}
}


@article{DBLP:journals/ton/SadehRK22,
	author = {Yaniv Sadeh and
                  Ori Rottenstreich and
                  Haim Kaplan},
	title = {Optimal Weighted Load Balancing in TCAMs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {985--998},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3140124},
	doi = {10.1109/TNET.2021.3140124},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SadehRK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic splitting is a required functionality in networks, for example for load balancing over multiple paths or among different servers. The capacities of the servers determine the partition by which traffic should be split. A recent approach implements traffic splitting within the ternary content addressable memory (TCAM), which is often available in switches. It is important to reduce the amount of memory allocated for this task since TCAMs are power consuming and are often also required for other tasks such as classification and routing. Previous work showed how to compute the smallest prefix-matching TCAM necessary to implement a given partition exactly. In this paper we solve the more practical case, where at most\nn\nprefix-matching TCAM rules are available, restricting the ability to implement exactly the desired partition. We give simple and efficient algorithms to find\nn\nrules that generate a partition closest in\nL\n∞\nto the desired one. We do the same for a one-sided version of\nL\n∞\nwhich equals to the maximum overload on a server and for a relative version of it. We use our algorithms to evaluate how the expected error changes as a function of the number of rules, the number of servers, and the width of the TCAM.}
}


@article{DBLP:journals/ton/NiuSCXHWXX22,
	author = {Zhixiong Niu and
                  Qiang Su and
                  Peng Cheng and
                  Yongqiang Xiong and
                  Dongsu Han and
                  Keith Winstein and
                  Chun Jason Xue and
                  Hong Xu},
	title = {NetKernel: Making Network Stack Part of the Virtualized Infrastructure},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {999--1013},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3129806},
	doi = {10.1109/TNET.2021.3129806},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NiuSCXHWXX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a system called NetKernel that decouples the network stack from the guest virtual machine and offers it as an independent module. NetKernel represents a new paradigm where network stack can be managed as part of the virtualized infrastructure. It provides important efficiency benefits: By gaining control and visibility of the network stack, operators can perform network management more directly and flexibly, such as multiplexing VMs running different applications to the same network stack module to save CPU cores, and enforcing fair bandwidth sharing. Users also benefit from the simplified stack deployment and better performance: For example mTCP can be deployed without API change to support nginx natively, and shared memory networking can be readily enabled to improve performance of colocated VMs. Testbed evaluation using 100G NICs shows that NetKernel preserves the performance and scalability of both kernel and userspace network stacks, and provides the same isolation as the current architecture.}
}


@article{DBLP:journals/ton/WoldeyohannesTJ22,
	author = {Yordanos Tibebu Woldeyohannes and
                  Besmir Tola and
                  Yuming Jiang and
                  K. K. Ramakrishnan},
	title = {CoShare: An Efficient Approach for Redundancy Allocation in {NFV}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1014--1028},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3132279},
	doi = {10.1109/TNET.2021.3132279},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WoldeyohannesTJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An appealing feature of Network Function Virtualization (NFV) is that in an NFV-based network, a network function (NF) instance may be placed at any node. On the one hand this offers great flexibility in allocation of redundant instances, but on the other hand it makes the allocation a unique and difficult challenge. One particular concern is that there is inherent correlation among nodes due to the structure of the network, thus requiring special care in this allocation. To this aim, our novel approach, called CoShare , is proposed. Firstly, its design takes into consideration the effect of network structural dependency, which might result in the unavailability of nodes of a network after failure of a node. Secondly, to efficiently make use of resources, CoShare proposes the idea of shared reservation , where multiple flows may be allowed to share the same reserved backup capacity at an NF instance. Furthermore, CoShare factors in the heterogeneity in nodes, NF instances and availability requirements of flows in the design. The results from a number of experiments conducted using realistic network topologies show that the integration of structural dependency allows meeting availability requirements for more flows compared to a baseline approach. Specifically, CoShare is able to meet diverse availability requirements in a resource-efficient manner, requiring, e.g., up to 85% in some studied cases, less resource overbuild than the baseline approach that uses the idea of dedicated reservation commonly adopted for redundancy allocation in NFV.}
}


@article{DBLP:journals/ton/ShiZWW22,
	author = {Zhengkai Shi and
                  Yipeng Zhou and
                  Di Wu and
                  Chen Wang},
	title = {{PPVC:} Online Learning Toward Optimized Video Content Caching},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1029--1044},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3132038},
	doi = {10.1109/TNET.2021.3132038},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiZWW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s Internet traffic has been dominated by video contents. To efficiently serve online videos for millions of users, it is essential to cache frequently requested videos on various devices such as edge servers, personal computers, etc. Existing caching algorithms are mainly designed by leveraging the information of past request records. However, it is insufficient to only use simple statistics of past request records, e.g. , video popularity which ignores the occurrence time of past video request events and the discrepancies among individual devices. In this paper, we propose a radically different learning-based video caching algorithm for Internet devices, called PPVC (Point Process based Video Cache) which can take exact video request patterns into account. By utilizing Hawkes Process (HP), we are able to link the future video request rates of a device with three kinds of historical records, namely, historical requests for the same video launched by the device itself, historical requests for the same video from other devices with a similar request pattern, and historical requests for other similar videos from the same device. The video request patterns can be efficiently computed via Singular Value Decomposition (SVD). Parameters linking these historical events can be determined by maximizing the likelihood of historical events. To further reduce the computation load, we also propose an online version of PPVC, which can timely update cached videos with the incremental arrival of events. One nice property of PPVC is that it can adapt to the change of video popularity very fast and is also applicable to Internet devices at different levels, ranging from the high-level servers that serve a large population of users to the low-level user devices ( e.g. , personal computers). Finally, we conduct extensive simulations with real traces and the results show that PPVC can always achieve the best video caching performance in terms of the hit rate on all levels of Internet devices, and quickly adapt to the dynamics of video requests.}
}


@article{DBLP:journals/ton/MohammadpourB22,
	author = {Ehsan Mohammadpour and
                  Jean{-}Yves Le Boudec},
	title = {On Packet Reordering in Time-Sensitive Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1045--1057},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3129590},
	doi = {10.1109/TNET.2021.3129590},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MohammadpourB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-sensitive networks (IEEE TSN or IETF DetNet) may tolerate some packet reordering. Re-sequencing buffers are then used to provide in-order delivery, the parameters of which (timeout, buffer size) may affect worst-case delay and delay jitter. There is so far no precise understanding of per-flow reordering metrics nor of the dimensioning of re-sequencing buffers in order to provide worst-case guarantees, as required in such networks. First, we show that a previously proposed per-flow metric, reordering late time offset (RTO), determines the timeout value. If the network is lossless, another previously defined metric, the reordering byte offset (RBO), determines the required buffer. If packet losses cannot be ignored, the required buffer may be larger than RBO, and depends on jitter, an arrival curve of the flow at its source, and the timeout. Then we develop a calculus to compute the RTO for a flow path; the method uses a novel relation with jitter and arrival curve, together with a decomposition of the path into non order-preserving and order-preserving elements. We also analyse the effect of re-sequencing buffers on worst-case delay, jitter and propagation of arrival curves. We show in particular that, in a lossless (but non order-preserving) network, re-sequencing is “for free”, namely, it does not increase worst-case delay nor jitter, whereas in a lossy network, re-sequencing increases the worst-case delay and jitter. We apply the analysis to evaluate the performance impact of placing re-sequencing buffers at intermediate points and illustrate the results on two industrial test cases.}
}


@article{DBLP:journals/ton/KamranYM22,
	author = {Khashayar Kamran and
                  Edmund Yeh and
                  Qian Ma},
	title = {{DECO:} Joint Computation Scheduling, Caching, and Communication in
                  Data-Intensive Computing Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1058--1072},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3136157},
	doi = {10.1109/TNET.2021.3136157},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KamranYM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Driven by technologies such as IoT-enabled health care, machine learning applications at the edge, and industrial automation, mobile edge and fog computing paradigms have reinforced a general trend toward decentralized computing, where any network node can route traffic, compute tasks, and store data, possibly at the same time. In many such computing environments, there is a need to cache significant amounts of data, which may include large data sets, machine learning models, or executable code. In this work, we propose a framework for joint computation scheduling, caching, and request forwarding within such decentralized computing environments. We first characterize the stability region of a “genie-aided” computing network where data required by computation are instantly accessible, and develop a throughput optimal control policy for this model. Based on this, we develop a practically implementable distributed and adaptive algorithm, and show that it exhibits superior performance in terms of average task completion time, when compared to several baseline policies.}
}


@article{DBLP:journals/ton/RashelbachRS22,
	author = {Alon Rashelbach and
                  Ori Rottenstreich and
                  Mark Silberstein},
	title = {A Computational Approach to Packet Classification},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1073--1087},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3131879},
	doi = {10.1109/TNET.2021.3131879},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RashelbachRS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-field packet classification is a crucial component in modern software-defined data center networks. To achieve high throughput and low latency, state-of-the-art algorithms strive to fit the rule lookup data structures into on-die caches; however, they do not scale well with the number of rules. We present a novel approach, NuevoMatch , which improves the memory scaling of existing methods. A new data structure, Range Query Recursive Model Index (RQ-RMI), is the key component that enables NuevoMatch to replace most of the accesses to main memory with model inference computations. We describe an efficient training algorithm that guarantees the correctness of the RQ-RMI-based classification. The use of RQ-RMI allows the rules to be compressed into neural networks that fit into the hardware cache. Further, it takes advantage of the growing support for fast neural network processing in modern CPUs, such as wide vector instructions, achieving a latency of tens of nanoseconds per lookup. Our evaluation using 500K multi-field rules from the standard ClassBench benchmark shows a geometric mean compression factor of\n4.9×\n,\n8×\n, and\n82×\n, and average performance improvement of\n2.4×\n,\n2.6×\n, and\n1.6×\nin throughput compared to CutSplit, NeuroCuts, and TupleMerge, all state-of-the-art algorithms.}
}


@article{DBLP:journals/ton/LiZSCHSPF22,
	author = {Hao Li and
                  Peng Zhang and
                  Guangda Sun and
                  Wanyue Cao and
                  Chengchen Hu and
                  Danfeng Shan and
                  Tian Pan and
                  Qiang Fu},
	title = {Compiling Cross-Language Network Programs Into Hybrid Data Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1088--1103},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3132303},
	doi = {10.1109/TNET.2021.3132303},
	timestamp = {Sat, 30 Sep 2023 10:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiZSCHSPF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network programming languages (NPLs) empower operators to program network data planes (NDPs) with unprecedented efficiency. Currently, various NPLs and NDPs coexist and no one can prevail over others in the short future. Such diversity is raising many problems including: (1) programs written with different NPLs can hardly interoperate in the same network, (2) most NPLs are bound to specific NDPs, hindering their independent evolution, and (3) compilation techniques cannot be readily reused, resulting in much wasteful work. These problems are mostly owing to the lack of modularity in the compilers, where the missing part is an intermediate representation (IR) for NPLs. To this end, we propose Network Transaction Automaton (NTA) , a highly-expressive and language-independent IR, and show it can express semantics of 7 mainstream NPLs. Then, we design CODER , a modular compiler based on NTA, which currently supports 2 NPLs and 3 NDPs. Experiments with real and synthetic programs show CODER can correctly compile those programs for real networks within moderate time.}
}


@article{DBLP:journals/ton/TongWL22,
	author = {Shuai Tong and
                  Jiliang Wang and
                  Yunhao Liu},
	title = {Combating Packet Collisions Using Non-Stationary Signal Scaling in
                  LPWANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1104--1117},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3131704},
	doi = {10.1109/TNET.2021.3131704},
	timestamp = {Tue, 20 Dec 2022 21:20:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TongWL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa, a representative Low-Power Wide Area Network (LPWAN) technology, has been shown as a promising platform to connect Internet of Things. Practical LoRa deployments, however, suffer from collisions, especially in dense networks and wide coverage areas expected by LoRa applications. Existing collision resolving approaches do not exploit the modulation properties of LoRa and thus cannot work well for low-SNR LoRa signals. We propose NScale to decompose concurrent transmissions by leveraging subtle inter-packet time offsets for low SNR LoRa collisions. NScale (1) translates subtle time offsets, which are vulnerable to noise, to robust frequency features, and (2) further amplifies the time offsets by non-stationary signal scaling, i.e., scaling the amplitude of a symbol differently at different positions. In practical implementation, we propose a noise resistant iterative symbol recovery method to combat symbol distortion in low SNR, and address frequency shifts incurred by CFO and packet time offsets in decoding. We propose optimized designs for diminishing the time costs of computation-intensive tasks and meeting the real-time requirements of LoRa collision resolving. We theoretically show that NScale introduces < 1.7 dB SNR loss compared with the original LoRa. We implement NScale on USRP N210 and evaluate its performance in both indoor and outdoor networks. NScale is implemented in software at the gateway and can work for COTS LoRa nodes without any modification. The evaluation results show that NScale improves the network throughput by\n3.3×\nfor low SNR collided signals compared with other state-of-the-art methods.}
}


@article{DBLP:journals/ton/LiuYLCLWC22,
	author = {Jia Liu and
                  Xi Yu and
                  Xuan Liu and
                  Xingyu Chen and
                  Haisong Liu and
                  Yanyan Wang and
                  Lijun Chen},
	title = {Time-Efficient Range Detection in Commodity {RFID} Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1118--1131},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3138083},
	doi = {10.1109/TNET.2021.3138083},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuYLCLWC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RFID is becoming ubiquitously available in our daily life. After RFID tags are deployed to make attached objects identifiable, a natural next step is to communicate with the tags and collect their information for the purpose of tracking tagged objects or monitoring their surroundings in real-time. In this paper, we study an under-investigated problem range detection in a commodity RFID system, which aims to check if there are any tags with the data between an upper and lower boundary in a time-efficient way. This is important especially in a large RFID system, which can help users quickly pinpoint the target tags (if any) and give an early warning to users for taking urgent actions and reducing the potential risk in the nascent stage. We propose two tailored protocols, selective query and range query, to achieve range detection within the scope of the C1G2 standard. The novelty is that, instead of querying each tag, we exploit the capability of C1G2-compatible selection and quickly separate target tags from others by silencing most of tags. The final result is that range query is able to achieve a range detection with only one query command. We implement the proposed protocols in commodity RFID systems, with no need for any hardware modifications. Extensive experiments show that range query is able to improve the time efficiency by an order of magnitude, compared with the baseline.}
}


@article{DBLP:journals/ton/LiuPWP22,
	author = {Kaiyang Liu and
                  Jun Peng and
                  Jingrong Wang and
                  Jianping Pan},
	title = {Optimal Caching for Low Latency in Distributed Coded Storage Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1132--1145},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3133215},
	doi = {10.1109/TNET.2021.3133215},
	timestamp = {Thu, 23 Jun 2022 20:02:43 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuPWP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Erasure codes have been widely considered as a promising solution to enhance data reliability at low storage costs. However, in modern geo-distributed storage systems, erasure codes may incur high data access latency as they require data retrieval from multiple remote storage nodes. This hinders the extensive application of erasure codes to data-intensive applications. This paper proposes novel caching schemes to achieve low latency in distributed coded storage systems. Assuming that future data popularity and network latency information are available, an offline caching scheme is proposed to explore the optimal caching solution for low latency. The proposed scheme categorizes all feasible caching decisions into a set of cache partitions, and then obtains the optimal caching decision through market clearing price for each cache partition. Furthermore, guided by the optimal scheme, an online caching scheme is proposed according to the measured data popularity and network latency information in real time, without the need to completely override the existing caching decisions. Both theoretical analysis and experiment results demonstrate that the online scheme can approximate the offline optimal scheme well with dramatically reduced computation complexity.}
}


@article{DBLP:journals/ton/WangZXZZHY22,
	author = {Jingzhou Wang and
                  Gongming Zhao and
                  Hongli Xu and
                  Yutong Zhai and
                  Qianyu Zhang and
                  He Huang and
                  Yongqiang Yang},
	title = {A Robust Service Mapping Scheme for Multi-Tenant Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1146--1161},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3133293},
	doi = {10.1109/TNET.2021.3133293},
	timestamp = {Mon, 05 Feb 2024 20:24:14 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangZXZZHY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a multi-tenant cloud, cloud vendors provide services ( e.g. , elastic load-balancing, virtual private networks) on service nodes for tenants. Thus, the mapping of tenants’ traffic and service nodes is an important issue in multi-tenant clouds. In practice, unreliability of service nodes and uncertainty/dynamics of tenants’ traffic are two critical challenges that affect the tenants’ QoS. However, previous works often ignore the impact of these two challenges, leading to poor system robustness when encountering system accidents. To bridge the gap, this paper studies the problem of robust service mapping in multi-tenant clouds (RSMP). Due to traffic dynamics, we take a two-step approach: service node assignment and tenant traffic scheduling . For service node assignment, we prove its NP-Hardness and analyze its problem difficulty. Then, we propose an efficient algorithm with bounded approximation factors based on randomized rounding and knapsack. For tenant traffic scheduling, we design an approximation algorithm based on fully polynomial time approximation scheme (FPTAS). The proposed algorithm achieves the approximation factor of 2+ \\epsilon\n, where \\epsilon\nis an arbitrarily small value. Both small-scale experimental results and large-scale simulation results show the superior performance of our proposed algorithms compared with other alternatives.}
}


@article{DBLP:journals/ton/XiongSM22,
	author = {Sijie Xiong and
                  Anand D. Sarwate and
                  Narayan B. Mandayam},
	title = {Network Traffic Shaping for Enhancing Privacy in IoT Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1162--1177},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3140174},
	doi = {10.1109/TNET.2021.3140174},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiongSM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by traffic analysis attacks based on the packet sizes and timing information in the Internet of Things (IoT) networks, we establish a rigorous event-level differential privacy (DP) model on infinite packet streams. We propose a traffic shaper satisfying a first-come-first-served queuing discipline that outputs traffic dependent on the input using a DP mechanism. We show that in special cases the proposed mechanism recovers existing shapers which standardize the output independently from the input. To find the optimal shapers for given levels of privacy and transmission efficiency, we formulate the constrained problem of minimizing the expected delay per packet and propose using the expected queue size across time as a proxy. We further show that the constrained minimization is a convex program. We demonstrate the effect of shapers on both synthetic data and packet traces from actual IoT devices. The experimental results reveal inherent privacy-overhead tradeoffs: more shaping overhead provides better privacy protection. Under the same privacy level, there is a tradeoff between dummy traffic and delay. When shaping heavier or less bursty traffic, all shapers become more overhead-efficient. We also show that increased traffic from more IoT devices makes guaranteeing event-level privacy easier. The DP shaper offers tunable privacy that is invariant with the change in the input traffic distribution and has an advantage in handling burstiness over traffic-independent shapers. This approach accommodates heterogeneous network conditions and user demands in privacy and overhead.}
}


@article{DBLP:journals/ton/FangLMYWZKLZCC22,
	author = {Chongrong Fang and
                  Haoyu Liu and
                  Mao Miao and
                  Jie Ye and
                  Lei Wang and
                  Wansheng Zhang and
                  Daxiang Kang and
                  Biao Lyu and
                  Shunmin Zhu and
                  Peng Cheng and
                  Jiming Chen},
	title = {Towards Automatic Root Cause Diagnosis of Persistent Packet Loss in
                  Cloud Overlay Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1178--1192},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3137557},
	doi = {10.1109/TNET.2021.3137557},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FangLMYWZKLZCC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Persistent packet loss in the cloud-scale overlay network severely compromises tenant experiences. Cloud providers are keen to diagnose such problems efficiently. However, existing work is either designed for the physical network or insufficient to present the concrete reason of packet loss. We propose to record and analyze the on-site forwarding condition of packets during packet-level tracing. The cloud-scale overlay network presents great challenges to achieve this goal with its high network complexity, multi-tenant nature, and diversity of root causes. To address these challenges, we present VTrace, an automatic diagnostic system for persistent packet loss over the cloud-scale overlay network. Utilizing the “fast path-slow path” structure of virtual forwarding devices (VFDs), e.g., vSwitches, VTrace installs several “coloring-matching-logging” rules in VFDs to selectively track the target packets and inspect them in depth. The detailed forwarding situation at each hop is logged and then assembled to perform analysis with an efficient path reconstruction scheme. Experiments are conducted to demonstrate VTrace’s low overhead and quick response. Besides, based on the idea “coloring-matching-counting”, VTrace can be easily extended to VTrace-stats to identify the culprit device for transient packet loss. We share experiences of how VTrace and VTrace-stats efficiently work after deploying them in Alibaba Cloud for years.}
}


@article{DBLP:journals/ton/ShiC22,
	author = {Hehuan Shi and
                  Lin Chen},
	title = {Downlink Transmission Scheduling With Data Sharing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1193--1202},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3138940},
	doi = {10.1109/TNET.2021.3138940},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We formulate and analyze a fundamental downlink transmission scheduling problem in a wireless communication system, composed of a base station and a set of users, each requesting a packet to be served within a time window. Some packets are requested by several users and can be served simultaneously due to the broadcast nature of the wireless medium. The base station can choose from a set of transmission strategies, e.g., in terms of combination of data rate and coding scheme, to serve the users. Each request can be served by a subset of transmission strategies. We seek a downlink transmission scheduling algorithm generating maximum aggregated system-wide utility. In this paper, we develop approximation algorithms for the formulated downlink transmission scheduling problem in both offline and online settings. We first establish the NP-hardness of the offline scheduling problem and the approximation hardness and bound of the online scheduling problem in non-preemptive and preemptive-restart models, respectively. We then design approximation algorithms and derive the approximation and competitive ratios of our online and offline scheduling algorithms. Numerical simulations are performed to further evaluate our algorithms in a wide range of network settings.}
}


@article{DBLP:journals/ton/LiuZC22,
	author = {Qingyu Liu and
                  Haibo Zeng and
                  Minghua Chen},
	title = {Minimizing AoI With Throughput Requirements in Multi-Path Network
                  Communication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1203--1216},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3135494},
	doi = {10.1109/TNET.2021.3135494},
	timestamp = {Thu, 13 Apr 2023 16:08:57 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuZC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a single-unicast networking scenario where a sender periodically sends a batch of data to a receiver over a multi-hop network, possibly using multiple paths. We study problems of minimizing peak/average Age-of-Information ( AoI ) subject to throughput requirements based on a stylized deterministic model in this scenario. The consideration of batch generation and multi-path communication differentiates our AoI study from existing ones. We first show that our AoI minimization problems are NP-hard, but only in the weak sense, as we develop an optimal algorithm with a pseudo-polynomial time complexity. We then prove that minimizing AoI and minimizing maximum delay are “roughly” equivalent, in the sense that any optimal solution of the latter is an approximate solution of the former with bounded optimality loss. We leverage this understanding to design a general approximation framework for our problems. It can build upon any \\alpha\n-approximation algorithm of the maximum delay minimization problem to construct an (\\alpha +\\mathsf {c})\n-approximate solution for minimizing AoI . Here \\mathsf {c}\nis a constant depending on the throughput requirements. Furthermore, we show that our results can be extended to the multiple-unicast setting. Simulations over various network topologies validate the effectiveness of our approach. Our results make a major advance to optimizing AoI in multi-path communication, and hence can be of broad interest to the networking research community.}
}


@article{DBLP:journals/ton/Perez-BuenoGMM22,
	author = {Fernando P{\'{e}}rez{-}Bueno and
                  Luz Garc{\'{\i}}a and
                  Gabriel Maci{\'{a}}{-}Fern{\'{a}}ndez and
                  Rafael Molina},
	title = {Leveraging a Probabilistic {PCA} Model to Understand the Multivariate
                  Statistical Network Monitoring Framework for Network Security Anomaly
                  Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1217--1229},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3138536},
	doi = {10.1109/TNET.2021.3138536},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Perez-BuenoGMM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network anomaly detection is a very relevant research area nowadays, especially due to its multiple applications in the field of network security. The boost of new models based on variational autoencoders and generative adversarial networks has motivated a reevaluation of traditional techniques for anomaly detection. It is, however, essential to be able to understand these new models from the perspective of the experience attained from years of evaluating network security data for anomaly detection. In this paper, we revisit anomaly detection techniques based on PCA from a probabilistic generative model point of view, and contribute a mathematical model that relates them. Specifically, we start with the probabilistic PCA model and explain its connection to the Multivariate Statistical Network Monitoring (MSNM) framework. MSNM was recently successfully proposed as a means of incorporating industrial process anomaly detection experience into the field of networking. We have evaluated the mathematical model using two different datasets. The first, a synthetic dataset created to better understand the analysis proposed, and the second, UGR’16, is a specifically designed real-traffic dataset for network security anomaly detection. We have drawn conclusions that we consider to be useful when applying generative models to network security detection.}
}


@article{DBLP:journals/ton/TuZXZZ22,
	author = {Huaqing Tu and
                  Gongming Zhao and
                  Hongli Xu and
                  Yangming Zhao and
                  Yutong Zhai},
	title = {A Robustness-Aware Real-Time {SFC} Routing Update Scheme in Multi-Tenant
                  Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1230--1244},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3137418},
	doi = {10.1109/TNET.2021.3137418},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TuZXZZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In multi-tenant clouds, requests need to traverse a set of network functions (NFs) in a specific order, referred to as a service function chain (SFC), for security and business logic issues. Due to workload dynamics, the central controller of a multi-tenant cloud needs to frequently update the SFC routing, so as to optimize various network performance, such as load balancing. To achieve effective SFC routing update, we should consider two critical requirements: system robustness and real-time update . Without considering these two requirements, prior works either result in fragile clouds or suffer from large update delay. In this paper, we propose a robustness-aware real-time SFC routing update (R 3 -UA) scheme which takes both requirements into consideration. R 3 -UA pursues robustness-aware real-time routing update through two phases: robust NF instance assignment update and real-time SFC routing update. Two algorithms with bounded approximation ratios are proposed for these two phases, respectively. We implement R 3 -UA on a real testbed. Both small-scale experimental results and large-scale simulation results show the superior performance of R 3 -UA compared with other alternatives.}
}


@article{DBLP:journals/ton/ShiC22a,
	author = {Hehuan Shi and
                  Lin Chen},
	title = {From Spectrum Bonding to Contiguous-Resource Batching Task Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1245--1254},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3138991},
	doi = {10.1109/TNET.2021.3138991},
	timestamp = {Mon, 01 May 2023 21:14:28 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiC22a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We formulate and analyze a generic task scheduling problem: a set of tasks need to be executed on a pool of continuous resource such as spectrum and memory, each requiring a certain amount of time and contiguous resource; some tasks can be executed simultaneously in batch by sharing the resource, while others requiring exclusive use of the resource. We seek an optimal resource allocation and the related scheduling policy maximizing the overall system utility. This problem, termed as the contiguous-resource batching task scheduling problem, arises in a variety of engineering fields, where communication and storage resources are potential bottlenecks and thus need to be carefully scheduled. Two motivating examples are the spectrum bonding problem in dynamic spectrum access systems and the dynamic storage allocation problem in computer systems. In this paper, we investigate both offline and online scheduling settings. We first establish the NP-hardness of the offline setting and the inapproximability of the online setting in its generic form. Given the theoretical performance limit, we then develop approximation algorithms with mathematically proven performance guarantee in terms of approximation and competitive ratios for the offline and online settings.}
}


@article{DBLP:journals/ton/WuHCYZ22,
	author = {Qiong Wu and
                  Kaiwen He and
                  Xu Chen and
                  Shuai Yu and
                  Junshan Zhang},
	title = {Deep Transfer Learning Across Cities for Mobile Traffic Prediction},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1255--1267},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3136707},
	doi = {10.1109/TNET.2021.3136707},
	timestamp = {Mon, 26 Feb 2024 10:36:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WuHCYZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Precise citywide mobile traffic prediction is of great significance for intelligent network planning and proactive service provisioning. Current traffic prediction approaches mainly focus on training a well-performed model for the cities with a large amount of mobile traffic data. However, for the cities with scarce data, the prediction performance will be greatly limited. To tackle this problem, in this paper we propose a novel cross-city deep transfer learning framework named CCTP for citywide mobile traffic prediction in cities with data scarcity. Specifically, we first present a novel spatial-temporal learning model and pre-train the model by abundant data of a source city to obtain prior knowledge of mobile traffic dynamics. We then devise an efficient generative adversarial network (GAN) based cross-domain adapter for distribution alignment between target data and source data. To deal with data scarcity issue in some clusters of target city, we further design an inter-cluster transfer learning strategy for performance enhancement. Extensive experiments conducted on real-world mobile traffic datasets demonstrate that our proposed CCTP framework can achieve superior performance in citywide mobile traffic prediction with data scarcity.}
}


@article{DBLP:journals/ton/HusseinRM22,
	author = {Abdalla Hussein and
                  Catherine Rosenberg and
                  Patrick Mitran},
	title = {Hybrid {NOMA} in Multi-Cell Networks: From a Centralized Analysis
                  to Practical Schemes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1268--1282},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3135599},
	doi = {10.1109/TNET.2021.3135599},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HusseinRM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the performance of a hybrid non-orthogonal multiple access (NOMA) multi-cell downlink system (called hybrid as different users can have different successive interference cancellation (SIC) capabilities) by first formulating and solving a centralized proportional fair scheduling genie-assisted problem that jointly performs user selection, power allocation, power distribution, and modulation and coding scheme (MCS) selection. While such a genie is practically infeasible, it upper bounds the achievable performance. The results indicate that hybrid NOMA with a maximum of 2 multiplexed users can bring significant gains over a traditional OMA system (as long as enough users have the maximum SIC capability). Additionally, results show that the simple equal power allocation scheme (often used in the literature) yields performance lower than half the upper bound. Thus, we propose a simple static coordinated power allocation scheme across all cells for NOMA using a simple power map that is easily calibrated offline and show that with the calibrated power map, performance improves by 80%. Finally, we focus on the online scenario and propose a family of practical scheduling algorithms, each of them exhibiting a different trade-off between complexity (i.e., run-time) and performance.}
}


@article{DBLP:journals/ton/QiYZX22,
	author = {Yiwen Qi and
                  Wenke Yu and
                  Xudong Zhao and
                  Xindi Xu},
	title = {Event-Triggered Control for Network-Based Switched Systems With Switching
                  Signals Subject to Dual-Terminal DoS Attacks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1283--1293},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3135963},
	doi = {10.1109/TNET.2021.3135963},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QiYZX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates event-triggered control for saturated switched systems with switching signals subject to dual-terminal denial-of-service (DoS) attacks. The original switching signals are sampled and transmitted to sub-system and sub-controller through networks, respectively. However, the openness of network may cause switching signal transmission to be affected by possible DoS attacks. Moreover, the existing dual-terminal mismatched network delays and attacks could lead to asynchronous phenomenon. Considering the impacts of the attacked switching signals and delays, a time-delay closed-loop switched system is developed. Subsequently, by utilizing piecewise Lyapunov functional technique, various cases caused by DoS attacks and different delays are analyzed. Then, novel conditions are derived to ensure the stability of closed-loop switched system, and sufficient conditions for the desired controller gains and event-triggering parameters are presented. Finally, simulation examples are given to verify the effectiveness of the proposed method. The simulation results show that in the case of single-terminal and dual-terminal attacks, switched systems both have good anti-attack characteristics.}
}


@article{DBLP:journals/ton/ZhangCP22,
	author = {Chaoyun Zhang and
                  Xavier Costa{-}P{\'{e}}rez and
                  Paul Patras},
	title = {Adversarial Attacks Against Deep Learning-Based Network Intrusion
                  Detection Systems and Defense Mechanisms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1294--1311},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3137084},
	doi = {10.1109/TNET.2021.3137084},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangCP22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neural networks (NNs) are increasingly popular in developing NIDS, yet can prove vulnerable to adversarial examples. Through these, attackers that may be oblivious to the precise mechanics of the targeted NIDS add subtle perturbations to malicious traffic features, with the aim of evading detection and disrupting critical systems. Defending against such adversarial attacks is of high importance, but requires to address daunting challenges. Here, we introduce TIKI- TAKA, a general framework for (i) assessing the robustness of state-of-the-art deep learning-based NIDS against adversarial manipulations, and which (ii) incorporates defense mechanisms that we propose to increase resistance to attacks employing such evasion techniques. Specifically, we select five cutting-edge adversarial attack types to subvert three popular malicious traffic detectors that employ NNs. We experiment with publicly available datasets and consider both one-to-all and one-to-one classification scenarios, i.e., discriminating illicit vs benign traffic and respectively identifying specific types of anomalous traffic among many observed. The results obtained reveal that attackers can evade NIDS with up to 35.7% success rates, by only altering time-based features of the traffic generated. To counteract these weaknesses, we propose three defense mechanisms: model voting ensembling, ensembling adversarial training, and query detection. We demonstrate that these methods can restore intrusion detection rates to nearly 100% against most types of malicious traffic, and attacks with potentially catastrophic consequences (e.g., botnet) can be thwarted. This confirms the effectiveness of our solutions and makes the case for their adoption when designing robust and reliable deep anomaly detectors.}
}


@article{DBLP:journals/ton/LiaoCH22,
	author = {Guocheng Liao and
                  Xu Chen and
                  Jianwei Huang},
	title = {Privacy-Aware Online Social Networking With Targeted Advertisement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1312--1327},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3137513},
	doi = {10.1109/TNET.2021.3137513},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiaoCH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In an online social network, users exhibit personal information to enjoy social interaction. The social network provider (SNP) exploits users’ information for revenue generation through targeted advertisement, in which the SNP presents advertisements to proper users effectively. Therefore, an advertiser is more willing to pay for targeted advertisement to promote his product. However, the over-exploitation of users’ information would invade users’ privacy, which would negatively impact users’ social activeness. Motivated by this, we study the privacy policy (policies) of the SNP(s) with targeted advertisement, in both monopoly and duopoly markets. We characterize the privacy policy in terms of the fraction of users’ information that the provider should exploit, and formulate the interactions among users, advertiser, and SNP(s) as a three-stage Stackelberg game. By leveraging the model’s supermodularity property, we prove the threshold structure of users’ equilibrium information levels. We discover the overall information that can be exploited by an SNP is non-monotonic in the exploitation fraction. Monopoly (one SNP) study shows our proposed optimal privacy policy helps the SNP earn even more advertisement revenue than full exploitation policy does. The situation of the duopoly market is much more complicated. In that case, if the service quality gap between the two SNPs is large, the stronger SNP will choose a conservative privacy protection policy that drives the other SNP out of the market. However, if the service quality gap is small and the advertisement revenue is promising, the stronger SNP would choose an aggressive policy to exploit the advertisement revenue and both SNPs will have positive market shares.}
}


@article{DBLP:journals/ton/ZhouLHM22,
	author = {Hongyi Zhou and
                  Kefan Lv and
                  Longbo Huang and
                  Xiongfeng Ma},
	title = {Quantum Network: Security Assessment and Key Management},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1328--1339},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3136943},
	doi = {10.1109/TNET.2021.3136943},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhouLHM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an extension of quantum key distribution, secure communication among multiple users is a basic task in a quantum network. When the quantum network structure becomes complicated with a large number of users, it is important to investigate network issues, including security, key management, latency, reliability, scalability, and cost. In this work, we utilize the classical network theory and graph theory to address two critical issues in a quantum network, security and key management. First, we design a communication scheme with the highest security level that trusts a minimum number of intermediate nodes. Second, when the quantum key is a limited resource, we design key management and data scheduling schemes to optimize the utility of data transmission. Our results can be directly applied to the current metropolitan and free-space quantum network implementations and can potentially be a standard approach for future quantum network designs.}
}


@article{DBLP:journals/ton/HanW22,
	author = {Yanyan Han and
                  Hongyi Wu},
	title = {Privacy-Aware Participant Recruitment in Opportunistic Device to Device
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1340--1351},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3141069},
	doi = {10.1109/TNET.2022.3141069},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HanW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In most of the existing mobile applications for data collection and data analytics, either the privacy issue is frequently neglected or the privacy options are not configurable by the participants. This paper proposes configurable privacy level by potential crowdsourcing participants who are able to choose their desirable privacy level and get paid based on the quality of data they provide to the originator in Device to Device network (D2D). Combining with the encryption technique, not only the user’s privacy is protected, but also the encryption complexity is reduced. We first formulate the participant recruitment process into an optimization problem from the participants’ perspective by considering the competition and collaboration among existing and candidate participants in order to achieve the best utility. Then we design a distributed approximate scheme that relies on participants’ local knowledge to complete the overall recruitment task. We implement the approximate approach in Dell Streak tablets and carry out a campus-scale experiment for 21 days, plus run simulations for more extensive and detailed evaluation under various task settings. The results demonstrate the efficiency of the proposed approaches and disclose valuable insights for practical considerations in D2D based crowdsourcing.}
}


@article{DBLP:journals/ton/AnWHGCG22,
	author = {Jian An and
                  Zhenxing Wang and
                  Xin He and
                  Xiaolin Gui and
                  Jindong Cheng and
                  Ruowei Gui},
	title = {{PPQC:} {A} Blockchain-Based Privacy-Preserving Quality Control Mechanism
                  in Crowdsensing Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1352--1367},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3141582},
	doi = {10.1109/TNET.2022.3141582},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AnWHGCG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of embedded smart devices, a new data collection paradigm, mobile crowd-sensing (MCS), has been proposed. MCS allows individuals from the crowd to act as sensors and contribute their observation data. However, existing MCS systems are mostly based on third-party platforms, and there is no guarantee that a center is completely credible. In addition, security and privacy issues should not be ignored. During MCS’ execution, the participants’ various information and truth value are usually exposed, and the computation related to data privacy cannot be verified. In this paper, we integrate the blockchain into the MCS scenario to design a blockchain based privacy-preserving quality control mechanism, which prevents data from being tampered with, and denied, ensuring that the reward is distributed fairly. In the new system, we propose a privacy preserving participant selection scheme and the result can be verified (i.e., security against malicious node) without any third-party arbiter. Finally, considering the issues with sensing data privacy and efficiency in the truth discovery process, we propose a new privacy-aware crowdsensing design with iterative truth discovery based on rational secure multi-party computation. The experimental results show that compared to the prior result, the proposed solutions are highly practical and facilitate quality control without violating the participant’s privacy.}
}


@article{DBLP:journals/ton/WangYLCL22,
	author = {Zhiyuan Wang and
                  Jiancheng Ye and
                  Dong Lin and
                  Yipei Chen and
                  John C. S. Lui},
	title = {Approximate and Deployable Shortest Remaining Processing Time Scheduler},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1368--1381},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3142148},
	doi = {10.1109/TNET.2022.3142148},
	timestamp = {Tue, 20 Jun 2023 14:04:13 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangYLCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The scheduling policy installed on switches of datacenters plays a significant role on congestion control. Shortest-Remaining-Processing-Time (SRPT) achieves the near-optimal average message completion time (MCT) in various scenarios, but is difficult to deploy as viewed by the industry. The reasons are two-fold: 1) many commodity switches only provide FIFO queues, and 2) the information of remaining message size is not available. Recently, the idea of emulating SRPT using only a few FIFO queues and the original message size has been coined as the approximate and deployable SRPT (ADS) design. In this paper, we provide the first theoretical study on the optimal ADS design. Specifically, we first characterize a wide range of feasible ADS scheduling policies via a unified framework, and then derive the steady-state MCT, slowdown, and impoliteness in the M/G/1 setting. Hence we formulate the optimal ADS design as a non-linear combinatorial optimization problem, which aims to minimize the average MCT given the available FIFO queues. We also take into account the proportional fairness and temporal fairness constraints based on the maximal slowdown and impoliteness, respectively. The optimal ADS design problem is NP-hard in general, and does not exhibit monotonicity or sub-modularity. We leverage its decomposable structure and devise an efficient algorithm to solve the optimal ADS policy. We carry out extensive flow-level simulations and packet-level experiments to evaluate the proposed optimal ADS design. Results show that the optimal ADS policy installed on eight FIFO queues is capable of emulating the true SRPT.}
}


@article{DBLP:journals/ton/JangSY22,
	author = {Hyeryung Jang and
                  HyungSeok Song and
                  Yung Yi},
	title = {On Cost-Efficient Learning of Data Dependency},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1382--1394},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3141128},
	doi = {10.1109/TNET.2022.3141128},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JangSY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider the problem of learning a tree graph structure that represents the statistical data dependency among nodes for a set of data samples generated by nodes, which provides the basic structure to perform a probabilistic inference task. Inference in the data graph includes marginal inference and maximum a posteriori (MAP) estimation, and belief propagation (BP) is a commonly used algorithm to compute the marginal distribution of nodes via message-passing, incurring non-negligible amount of communication cost. We inevitably have the trade-off between the inference accuracy and the message-passing cost because the learned structure of data dependency and physical connectivity graph are often highly different. In this paper, we formalize this trade-off in an optimization problem which outputs the data dependency graph that jointly considers learning accuracy and message-passing costs. We focus on two popular implementations of BP, ASYNC-BP and SYNC-BP , which have different message-passing mechanisms and cost structures. In ASYNC-BP , we propose a polynomial-time learning algorithm that is optimal, motivated by finding a maximum weight spanning tree of a complete graph. In SYNC-BP , we prove the NP-hardness of the problem and propose a greedy heuristic. For both BP implementations, we quantify how the error probability that the learned cost-efficient data graph differs from the ideal one decays as the number of data samples grows, using the large deviation principle, which provides a guideline on how many samples are necessary to obtain a certain trade-off. We validate our theoretical findings through extensive simulations, which confirms that it has a good match.}
}


@article{DBLP:journals/ton/CaoLZHX22,
	author = {Jiamin Cao and
                  Ying Liu and
                  Yu Zhou and
                  Lin He and
                  Mingwei Xu},
	title = {TurboNet: Faithfully Emulating Networks With Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1395--1409},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3142126},
	doi = {10.1109/TNET.2022.3142126},
	timestamp = {Tue, 28 Jun 2022 21:08:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaoLZHX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Faithfully emulating networks is critical for verifying the correctness and effectiveness of new networking-related designs. Existing network experiment platforms either cannot faithfully emulate the functionality and performance of production networks or cannot scale well due to cost constraints. In this paper, we propose TurboNet , a new network emulator that utilizes one or more programmable switches to achieve faithful emulation of the network data plane and control plane. For data plane emulation, we propose a series of key designs, such as port mapper, queue mapper, and delayed queue, to emulate network topologies and performance metrics with high flexibility and accuracy. For control plane emulation, we support static routing configurations, distributed routing agents, and the centralized routing controllers. Meanwhile, we provide APIs for operators to simplify network emulation tasks. We implement TurboNet on Tofino switches. Evaluation results show that: (1) On the data plane, TurboNet can flexibly emulate various topologies, such as an 8-ary fat-tree with only one programmable switch and a 10-ary fat-tree with four programmable switches; (2) On the control plane, TurboNet supports about 200 BGP agents on a single programmable switch with a CPU usage of 25%; (3) TurboNet can accurately emulate different network performance metrics such as 10 −8 link loss, and microsecond to millisecond link delay.}
}


@article{DBLP:journals/ton/LeeWKNYPS22,
	author = {Seungsoo Lee and
                  Seungwon Woo and
                  Jinwoo Kim and
                  Jaehyun Nam and
                  Vinod Yegneswaran and
                  Phillip A. Porras and
                  Seungwon Shin},
	title = {A Framework for Policy Inconsistency Detection in Software-Defined
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1410--1423},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3140824},
	doi = {10.1109/TNET.2022.3140824},
	timestamp = {Tue, 16 Aug 2022 23:09:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LeeWKNYPS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN) has aggressively grown in data center networks, telecommunication providers, and enterprises by virtue of its programmable and extensible control plane. Also, there have been many kinds of research on the security of SDN components along with the growth of SDN. Some of them have inspected network policy inconsistency problems that can severely cause network reliability and security issues in SDN. However, they do not consider whether a single network policy itself is corrupted during processing inside and between SDN components. In this paper, we thus focus on the question of how to automatically identify cases in which the SDN stack fails to prevent policy inconsistencies from arising among those components. We then present AudiSDN, an automated fuzz-testing framework designed to formulate test cases in which policy inconsistencies can arise in OpenFlow networks, the most prevalent SDN protocol. To prove its feasibility, we applied AudiSDN to two widely used SDN controllers, Floodlight and ONOS, and uncovered three separate CVEs (Common Vulnerabilities and Exposures) that cause the network policy inconsistencies among SDN components. Furthermore, we investigate the design flaws that cause the inconsistencies in modern SDN components, suggesting specific validations to address such a serious but understudied pragmatic concern.}
}


@article{DBLP:journals/ton/HeZDZLDRNC22,
	author = {Xin He and
                  Jiaqi Zheng and
                  Haipeng Dai and
                  Chong Zhang and
                  Geng Li and
                  Wanchun Dou and
                  Wajid Rafique and
                  Qiang Ni and
                  Guihai Chen},
	title = {Continuous Network Update With Consistency Guaranteed in Software-Defined
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {3},
	pages = {1424--1438},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3143700},
	doi = {10.1109/TNET.2022.3143700},
	timestamp = {Wed, 22 Nov 2023 12:10:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeZDZLDRNC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network update enables Software-Defined Networks (SDNs) to optimize the data plane performance. The single update focuses on processing one update event at a time, i.e. , updating a set of flows from their initial routes to target routes, but it fails to handle continuously arriving update events in time incurred by high-frequency network changes. On the contrary, the continuous update proposed in “Update Algebra” can handle multiple update events concurrently and respond to the network condition changes at all times. However, “Update Algebra” only guarantees the blackhole-free and loop-free update. The congestion-free property cannot be respected. In this paper, we propose Coeus to achieve the continuous update while maintaining consistency, i.e. , ensuring the blackhole-free, loop-free, and congestion-free properties simultaneously. Firstly, we establish the continuous update model based on the update operations in update events. With the update model, we dynamically reconstruct the operation dependency graph (ODG) to capture the relationship between update operations and link utilization variations. Then, we develop a composition algorithm to eliminate redundant operations in update events. To further speed up the update procedure, we present a partition algorithm to split the operation nodes of the ODG into a series of suboperation nodes that can be executed independently. The partition algorithm is proven to be optimal. Finally, extensive evaluations show that Coeus can improve the update speed by at least 179% and reduce redundant operations by at least 52% compared with state-of-the-art approaches when the arrival rate of update events equals three times per second.}
}


@article{DBLP:journals/ton/BasatEKOVW22,
	author = {Ran Ben Basat and
                  Gil Einziger and
                  Isaac Keslassy and
                  Ariel Orda and
                  Shay Vargaftik and
                  Erez Waisbard},
	title = {Memento: Making Sliding Windows Efficient for Heavy Hitters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1440--1453},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3132385},
	doi = {10.1109/TNET.2021.3132385},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BasatEKOVW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud operators require timely identification of Heavy Hitters (HH) and Hierarchical Heavy Hitters (HHH) for applications such as load balancing, traffic engineering, and attack mitigation. However, existing techniques are slow in detecting new heavy hitters. In this paper, we present the case for identifying heavy hitters through sliding windows . Sliding windows are quicker and more accurate to detect new heavy hitters than current interval-based methods, but to date had no practical algorithms. Accordingly, we introduce, design, and analyze the Memento family of sliding window algorithms for the HH and HHH problems in the single-device and network-wide settings. We use extensive evaluations to show that our single-device solutions are orders of magnitude faster than existing sliding window techniques and comparable in speed to state-of-the-art non-windowed sampling based technique. Furthermore, we exemplify our network-wide HHH detection capabilities on a realistic testbed. To that end, we implemented Memento as an open-source extension to the popular HAProxy cloud load-balancer. In our evaluations, using an HTTP flood by 50 subnets, our network-wide approach detected the new subnets faster and reduced the number of undetected flood requests by up to\n37×\ncompared to the alternatives.}
}


@article{DBLP:journals/ton/KimKHLB22,
	author = {Junghoon Kim and
                  Taejoon Kim and
                  Morteza Hashemi and
                  David J. Love and
                  Christopher G. Brinton},
	title = {Minimum Overhead Beamforming and Resource Allocation in {D2D} Edge
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1454--1468},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3133022},
	doi = {10.1109/TNET.2021.3133022},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KimKHLB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Device-to-device (D2D) communications is expected to be a critical enabler of distributed computing in edge networks at scale. A key challenge in providing this capability is the requirement for judicious management of the heterogeneous communication and computation resources that exist at the edge to meet processing needs. In this paper, we develop an optimization methodology that considers the network topology jointly with device and network resource allocation to minimize total D2D overhead, which we quantify in terms of time and energy required for task processing. Variables in our model include task assignment, CPU allocation, subchannel selection, and beamforming design for multiple-input multiple-output (MIMO) wireless devices. We propose two methods to solve the resulting non-convex mixed integer program: semi-exhaustive search optimization, which represents a “best-effort” at obtaining the optimal solution, and efficient alternate optimization, which is more computationally efficient. As a component of these two methods, we develop a novel coordinated beamforming algorithm which we show obtains the optimal beamformer for a common receiver characteristic. Through numerical experiments, we find that our methodology yields substantial improvements in network overhead compared with local computation and partially optimized methods, which validates our joint optimization approach. Further, we find that the efficient alternate optimization scales well with the number of nodes, and thus can be a practical solution for D2D computing in large networks.}
}


@article{DBLP:journals/ton/ZhaoW22,
	author = {Jie Zhao and
                  Xin Wang},
	title = {On the Efficiency of Multi-Beam Medium Access for Millimeter-Wave
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1469--1480},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3137562},
	doi = {10.1109/TNET.2021.3137562},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The need of highly directional communications at mmWave band introduces high overhead for beam training and alignment, which also makes the medium access control (MAC) a grand challenge. However, the need of supporting highly directional multiple beams between transmitters and receivers makes the MAC design even harder. To harvest the gain of multi-beam mmWave communications, which benefits not only from the large bandwidth of mmWave spectrum but also the diversity of concurrent multi-user multi-beam transmissions, this paper studies the medium access control (MAC) layer related issues in multi-beam mmWave networks, including (1) efficient multi-beam training schemes to enable lower overhead thus faster AP association and beam alignment, (2) block-sparse mmWave channel estimation in different beam resolutions, and (3) effective concurrent radio resource allocation to facilitate better multi-user multi-beam transmissions. Simulation results demonstrate that the proposed schemes outperform existing techniques in improving the efficiency of mmWave communications thus achieving significantly higher network performances. To our best knowledge, we are the first to comprehensively consider both efficient training for beam alignment and resource scheduling in the MAC design to enable highly directional multi-user multi-beam concurrent transmissions in a mmWave network.}
}


@article{DBLP:journals/ton/XiaoWJ22,
	author = {Xuedou Xiao and
                  Wei Wang and
                  Tao Jiang},
	title = {Sensor-Assisted Rate Adaptation for {UAV} {MU-MIMO} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1481--1493},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3136911},
	doi = {10.1109/TNET.2021.3136911},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaoWJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Propelled by multi-user MIMO (MU-MIMO) technology, unmanned aerial vehicles (UAVs) as mobile hotspots have recently emerged as an attractive wireless communication paradigm. Rate adaptation (RA) becomes indispensable to enhance UAV communication robustness against UAV mobility-induced channel variances. However, existing MU-MIMO RA algorithms are mainly designed for ground communications with relatively stable channel coherence time, which incurs channel measurement staleness and sub-optimal rate selections when coping with highly dynamic air-to-ground links. In this paper, we propose SensRate, a new uplink MU-MIMO RA algorithm dedicated for low-altitude UAVs, which exploits inherent on-board sensors used for flight control with no extra cost. We propose a novel channel prediction algorithm that utilizes sensor-estimated flight states to assist channel direction prediction for each client and estimate inter-user interference for optimal rates. We provide an implementation of our design using a commercial UAV and show that it achieves an average throughput gain of\n1.24×\nand\n1.28×\ncompared with the bestknown RA algorithm for 2- and 3-antenna APs, respectively.}
}


@article{DBLP:journals/ton/JajooHL22,
	author = {Akshay Jajoo and
                  Y. Charlie Hu and
                  Xiaojun Lin},
	title = {A Case for Sampling-Based Learning Techniques in Coflow Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1494--1508},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2021.3138923},
	doi = {10.1109/TNET.2021.3138923},
	timestamp = {Wed, 10 May 2023 14:38:12 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JajooHL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Coflow scheduling improves data-intensive application performance by improving their networking performance. State-of-the-art online coflow schedulers in essence approximate the classic Shortest-Job-First (SJF) scheduling by learning the coflow size online. In particular, they use multiple priority queues to simultaneously accomplish two goals: to sieve long coflows from short coflows, and to schedule short coflows with high priorities. Such a mechanism pays high overhead in learning the coflow size: moving a large coflow across the queues delays small and other large coflows, and moving similar-sized coflows across the queues results in inadvertent round-robin scheduling. We propose Philae, a new online coflow scheduler that exploits the spatial dimension of coflows, i.e., a coflow has many flows, to drastically reduce the overhead of coflow size learning . Philae pre-schedules sampled flows of each coflow and uses their sizes to estimate the average flow size of the coflow. It then resorts to Shortest Coflow First, where the notion of shortest is determined using the learned coflow sizes and coflow contention. We show that the sampling-based learning is robust to flow size skew and has the added benefit of much improved scalability from reduced coordinator-local agent interactions. Our evaluation using an Azure testbed, a publicly available production cluster trace from Facebook shows that compared to the prior art Aalo, Philae reduces the coflow completion time (CCT) in average (P90) cases by\n1.50×\n(\n8.00×\n) on a 150-node testbed and\n2.72×\n(\n9.78×\n) on a 900-node testbed. Evaluation using additional traces further demonstrates Philae’s robustness to flow size skew.}
}


@article{DBLP:journals/ton/VazeN22,
	author = {Rahul Vaze and
                  Jayakrishnan Nair},
	title = {Speed Scaling on Parallel Servers With MapReduce Type Precedence Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1509--1524},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3142091},
	doi = {10.1109/TNET.2022.3142091},
	timestamp = {Wed, 01 Mar 2023 21:16:32 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VazeN22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A multiple server setting is considered, where each server has tunable speed, and increasing the speed incurs an energy cost. Jobs arrive to a single queue, and each job has two types of sub-tasks, map and reduce, and a precedence constraint among them: any reduce task of a job can only be processed once all the map tasks of the job have been completed. In addition to the scheduling problem, i.e., which task to execute on which server, with tunable speed, an additional decision variable is the choice of speed for each server, so as to minimize a linear combination of the sum of the flow times of jobs/tasks and the total energy cost. The precedence constraints present new challenges for the speed scaling problem with multiple servers, namely that the number of tasks that can be executed at any time may be small but the total number of outstanding tasks might be quite large. We present simple speed scaling algorithms that are shown to have competitive ratios, that depend on the power cost function, and/or the ratio of the size of the largest task and the shortest reduce task, but not on the number of jobs, or the number of servers.}
}


@article{DBLP:journals/ton/WangCLSXLC22,
	author = {Yuntao Wang and
                  Weiwei Chen and
                  Tom H. Luan and
                  Zhou Su and
                  Qichao Xu and
                  Ruidong Li and
                  Nan Chen},
	title = {Task Offloading for Post-Disaster Rescue in Unmanned Aerial Vehicles
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1525--1539},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3140796},
	doi = {10.1109/TNET.2022.3140796},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangCLSXLC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Natural disasters often cause huge and unpredictable losses to human lives and properties. In such an emergency post-disaster rescue situation, unmanned aerial vehicles (UAVs) are effective tools to enter the damaged areas to perform immediate disaster recovery missions, owing to their flexible mobilities and fast deployment. However, UAVs typically have very limited battery and computational capacities, which makes them harder to perform heavy computation tasks during the complicated disaster recovery process. This paper addresses the issue of the battery and computation resource limitation with a fog computing based UAV system. Specifically, we first introduce the vehicular fog computing (VFC) system in which the unmanned ground vehicles (UGVs) perform the computation tasks offloaded from UAVs. To avoid the transmission competitions yet enable cooperations among UAVs and UGVs, a stable matching algorithm is developed to transform the computation task offloading problem into a two-sided matching problem. An iterative algorithm is then developed which matches each UAV with the most suitable UGV for offloading. Finally, extensive simulations are carried out to demonstrate that the proposed scheme can effectively improve utilities of UAVs and reduce average delay through comparison with conventional schemes.}
}


@article{DBLP:journals/ton/Neely22,
	author = {Michael J. Neely},
	title = {A Converse Result on Convergence Time for Opportunistic Wireless Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1540--1553},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3146126},
	doi = {10.1109/TNET.2022.3146126},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Neely22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proves an impossibility result for stochastic network utility maximization for multi-user wireless systems, including multiple access and broadcast systems. Every time slot an access point observes the current channel states for each user and opportunistically selects a vector of transmission rates. Channel state vectors are assumed to be independent and identically distributed with an unknown probability distribution. The goal is to learn to make decisions over time that maximize a concave utility function of the running time average transmission rate of each user. Recently it was shown that a stochastic Frank-Wolfe algorithm converges to utility-optimality with an error of\nO(log(T)/T)\n, where\nT\nis the time the algorithm has been running. An existing\nΩ(1/T)\nconverse is known. The current paper improves the converse to\nΩ(log(T)/T)\n, which matches the known achievability result. It does this by constructing a particular (simple) system for which no algorithm can achieve a better performance. The proof uses a novel reduction of the opportunistic scheduling problem to a problem of estimating a Bernoulli probability\np\nfrom independent and identically distributed samples. Along the way we refine a regret bound for Bernoulli estimation to show that, for any sequence of estimators, the set of values\np∈[0,1]\nunder which the estimators perform poorly has measure at least 1/6.}
}


@article{DBLP:journals/ton/IqbalSS22,
	author = {Hassan Iqbal and
                  Anand Singh and
                  Muhammad Shahzad},
	title = {Characterizing the Availability and Latency in {AWS} Network From
                  the Perspective of Tenants},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1554--1568},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3148701},
	doi = {10.1109/TNET.2022.3148701},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/IqbalSS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scalability and performance requirements are driving tenants to increasingly move their applications to public clouds. Unfortunately, cloud providers do not provide a view of their networking infrastructure to the tenants, rather only provide some generic service level agreements (SLAs). Tenants are, therefore, forced to plan the deployments of their applications based on these SLAs. This limits the performance that the tenants can achieve. Keeping this in view, we present a detailed network measurement study of the largest public cloud, Amazon Web Services (AWS). We collected network data to characterize the availability and latency of AWS over a period of 100 days and studied various temporal trends across several geographical locations of AWS throughout the world. We performed our study at all three levels of cloud hierarchy: inside availability zones (AZs), across AZs, and across regions. Our results show that network behavior varies significantly over time at different geographical locations, levels of hierarchy, and temporal granularities. For example, while we observed high availability at monthly granularity, it deteriorates at daily and hourly granularities. This and many other such observations that we present have significant implications for cloud tenants. We further implemented our measurement approach on Google Cloud Platform (GCP) to demonstrate that it can be deployed on any cloud platform and present some preliminary comparative observations from this implementation. Based on our observations, we present several recommendations that tenants can use to better deploy their applications.}
}


@article{DBLP:journals/ton/HosseinalipourA22,
	author = {Seyyedali Hosseinalipour and
                  Sheikh Shams Azam and
                  Christopher G. Brinton and
                  Nicol{\`{o}} Michelusi and
                  Vaneet Aggarwal and
                  David J. Love and
                  Huaiyu Dai},
	title = {Multi-Stage Hybrid Federated Learning Over Large-Scale D2D-Enabled
                  Fog Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1569--1584},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3143495},
	doi = {10.1109/TNET.2022.3143495},
	timestamp = {Sat, 10 Sep 2022 21:00:27 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HosseinalipourA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning has generated significant interest, with nearly all works focused on a “star” topology where nodes/devices are each connected to a central server. We migrate away from this architecture and extend it through the network dimension to the case where there are multiple layers of nodes between the end devices and the server. Specifically, we develop multi-stage hybrid federated learning ( MH-FL ), a hybrid of intra-and inter-layer model learning that considers the network as a multi-layer cluster-based structure. MH-FL considers the topology structures among the nodes in the clusters, including local networks formed via device-to-device (D2D) communications, and presumes a semi-decentralized architecture for federated learning. It orchestrates the devices at different network layers in a collaborative/cooperative manner (i.e., using D2D interactions) to form local consensus on the model parameters and combines it with multi-stage parameter relaying between layers of the tree-shaped hierarchy. We derive the upper bound of convergence for MH-FL with respect to parameters of the network topology (e.g., the spectral radius) and the learning algorithm (e.g., the number of D2D rounds in different clusters). We obtain a set of policies for the D2D rounds at different clusters to guarantee either a finite optimality gap or convergence to the global optimum. We then develop a distributed control algorithm for MH-FL to tune the D2D rounds in each cluster over time to meet specific convergence criteria. Our experiments on real-world datasets verify our analytical results and demonstrate the advantages of MH-FL in terms of resource utilization metrics.}
}


@article{DBLP:journals/ton/NadigRB22,
	author = {Deepak Nadig and
                  Byrav Ramamurthy and
                  Brian Bockelman},
	title = {{SNAG:} SDN-Managed Network Architecture for GridFTP Transfers Using
                  Application-Awareness},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1585--1598},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3150000},
	doi = {10.1109/TNET.2022.3150000},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NadigRB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increasingly, academic campus networks support large-scale data transfer workflows for data-intensive science. These data transfers rely on high-performance, scalable, and reliable protocols for moving large amounts of data over a high-bandwidth, high-latency network. GridFTP is a widely used protocol for wide area network (WAN) data movement. However, as the GridFTP protocol does not share connection information with the network-layer, network operators have reduced flexibility, particularly in identifying/managing flows across the network. We address this problem by deploying a production “ application-aware ” software defined network (SDN) for managing GridFTP transfers for data-intensive science workflows. We first propose a novel application-aware architecture called SNAG (SDN-managed Network Architecture for GridFTP transfers). SNAG combines application-layer and network-layer collaboration (termed “application-awareness”) with SDN-enabled network management to classify, monitor and to manage network resources actively. Until now, our SNAG deployment has successfully classified over 1.5 Billion GridFTP connections at the Holland Computing Center (HCC), University of Nebraska-Lincoln (UNL). Next, we develop an application-aware SDN system to provide differentiated network services for distributed computing workflows. At HCC, we also demonstrate how our system ensures the quality of service (QoS) for high-throughput workflows such as Compact Muon Solenoid (CMS) and Laser Interferometer Gravitational-Wave Observatory (LIGO). Further, we also demonstrate how application-aware SDN can be exploited to create policy-driven approaches to achieve accurate resource accounting for each workflow. We present strategies for implementing differentiated network services and discuss their capacity improvement benefits. Lastly, we provide some guidelines and recommendations for developing application-aware SDN architectures for general-purpose applications.}
}


@article{DBLP:journals/ton/LiS22,
	author = {Zhuozhao Li and
                  Haiying Shen},
	title = {Co-Scheduler: {A} Coflow-Aware Data-Parallel Job Scheduler in Hybrid
                  Electrical/Optical Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1599--1612},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3143232},
	doi = {10.1109/TNET.2022.3143232},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To support higher demand for datacenter networks, networking researchers have proposed hybrid electrical/optical datacenter networks (Hybrid-DCN) that leverages optical circuit switching (OCS) along with traditional electrical packet switching (EPS). However, due to the high reconfiguration delay of OCS, OCS is used only for bulk data transfers between racks to amortize the reconfiguration delay. Existing job schedulers for data-parallel frameworks are not designed for Hybrid-DCN, since they neither place tasks to aggregate data traffic to take advantage of OCS, nor schedule tasks to minimize the Coflow completion time (CCT). In this paper, we describe the mismatch between existing job schedulers and the advanced Hybrid-DCN, introduce the requirements for the new scheduler, and present the implementation of Co-scheduler , a job scheduler for data-parallel frameworks that aims to improve job performance by placing the tasks of jobs to aggregate enough data traffic to better leverage OCS to minimize the CCT in Hybrid-DCN. Specifically, for every job, Co-scheduler computes guidelines on how many racks to place the job’s input data and the job’s tasks. The guidelines are dynamically generated based on the real-time job characteristics or predictable job characteristics from prior runs, with the aim of leveraging OCS whenever possible and efficient and minimizing CCT of jobs. Co-scheduler then schedules the tasks of jobs based on the guidelines. We evaluate the effectiveness of Co-scheduler using trace-driven simulation. The evaluation demonstrates that Co-scheduler can improve makespan, average job completion time, and average CCT of a workload by up to 56%, 61%, and 79%, respectively, compared to the state-of-the-art schedulers.}
}


@article{DBLP:journals/ton/DuJBGR22,
	author = {Jun Du and
                  Chunxiao Jiang and
                  Abderrahim Benslimane and
                  Song Guo and
                  Yong Ren},
	title = {SDN-Based Resource Allocation in Edge and Cloud Computing Systems:
                  An Evolutionary Stackelberg Differential Game Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1613--1628},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3152150},
	doi = {10.1109/TNET.2022.3152150},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DuJBGR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the boosting growth of computation-heavy applications raises great challenges for the Fifth Generation (5G) and future wireless networks. As responding, the hybrid edge and cloud computing (ECC) system has been expected as a promising solution to handle the increasing computational applications with low-latency and on-demand services of computation offloading, which requires new computing resource sharing and access control technology paradigms. This work establishes a software-defined networking (SDN) based architecture for edge/cloud computing services in 5G heterogeneous networks (HetNets), which can support efficient and on-demand computing resource management to optimize resource utilization and satisfy the time-varying computational tasks uploaded by user devices. In addition, resulting from the information incompleteness, we design an evolutionary game based service selection for users, which can model the replicator dynamics of service subscription. Based on this dynamic access model, a Stackelberg differential game based cloud computing resource sharing mechanism is proposed to facilitate the resource trading between the cloud computing service provider (CCP) and different edge computing service providers (ECPs). Then we derive the optimal pricing and allocation strategies of cloud computing resource based on the replicator dynamics of users’ service selection. These strategies can promise the maximum integral utilities to all computing service providers (CPs), meanwhile the user distribution can reach the evolutionary stable state at this Stackelberg equilibrium. Furthermore, simulation results validate the performance of the designed resource sharing mechanism, and reveal the convergence and equilibrium states of user selection, and computing resource pricing and allocation.}
}


@article{DBLP:journals/ton/SongYWHLPDQ22,
	author = {Guanglei Song and
                  Jiahai Yang and
                  Zhiliang Wang and
                  Lin He and
                  Jinlei Lin and
                  Long Pan and
                  Chenxin Duan and
                  Xiaowen Quan},
	title = {{DET:} Enabling Efficient Probing of IPv6 Active Addresses},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1629--1643},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3145040},
	doi = {10.1109/TNET.2022.3145040},
	timestamp = {Tue, 18 Oct 2022 08:35:31 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SongYWHLPDQ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fast IPv4 scanning significantly improves network measurement and security research. Nevertheless, it is infeasible to perform brute-force scanning of the IPv6 address space. Alternatively, one can find active IPv6 addresses through scanning the candidate addresses generated by state-of-the-art algorithms. However, the probing efficiency of such algorithms is often very low. In this paper, our objective is to improve the probing efficiency of IPv6 addresses. We first perform a longitudinal active measurement study and build a high-quality dataset, hitlist, including more than 1.95B IPv6 addresses distributed in 58.2K BGP prefixes and collected over 17 months period. Different from the previous works, we probe the announced BGP prefixes using a pattern-based algorithm. This results in a dataset without uneven address distribution and low active rates. Further, we propose an efficient address generation algorithm, DET, which builds a density space tree to learn high-density address regions of the seed addresses with linear time complexity and improves the active addresses’ probing efficiency. We then compare our algorithm DET against state-of-the-art algorithms on the public hitlist and our hitlist by scanning 50M addresses. Our analysis shows that DET increases the de-aliased active address ratio and active address (including aliased addresses) ratio by 10%, and 14%, respectively. Furthermore, we develop a fingerprint-based method to detect aliased prefixes. The proposed method for the first time directly verifies whether the prefix is aliased or not. Our method finds that 10.64% of the public aliased prefixes are false positive.}
}


@article{DBLP:journals/ton/LiSSC22,
	author = {Zhuozhao Li and
                  Tanmoy Sen and
                  Haiying Shen and
                  Mooi Choo Chuah},
	title = {A Study on the Impact of Memory DoS Attacks on Cloud Applications
                  and Exploring Real-Time Detection Schemes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1644--1658},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3144895},
	doi = {10.1109/TNET.2022.3144895},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiSSC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Even though memory denial-of-service attacks can cause severe performance degradations on co-located virtual machines, a previous detection scheme against such attacks cannot accurately detect the attacks and also generates high detection delay and high performance overhead since it assumes that cache-related statistics of an application follow the same probability distribution at all times, which may not be true for all types of applications. In this paper, we present the experimental results showing the impacts of memory DoS attacks on different types of cloud-based applications. Based on these results, we propose two lightweight and responsive Statistical based Detection Schemes (SDS/B and SDS/P) that can detect such attacks accurately. SDS/B constructs a profile of normal range of cache-related statistics for all applications and use statistical methods to infer an attack when the real-time collected statistics exceed this normal range, while SDS/P exploits the increased periods of access patterns for periodic applications to infer an attack. Upon SDS, we further leverage deep neural network (DNN) techniques to design a DNN-based detection scheme that is general to various types of applications and more robust to adaptive attack scenarios. Our evaluation results show that SDS/B, SDS/P and DNN outperform the state-of-the-art detection scheme, e.g., with 65% higher specificity, 40% shorter detection delay, and 7% less performance overhead. We also discuss how to use SDS and DNN-based detection schemes under different situations.}
}


@article{DBLP:journals/ton/HiltonHD22,
	author = {Alden Hilton and
                  Joel Hirschmann and
                  Casey T. Deccio},
	title = {Beware of IPs in Sheep's Clothing: Measurement and Disclosure of {IP}
                  Spoofing Vulnerabilities},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1659--1673},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3149011},
	doi = {10.1109/TNET.2022.3149011},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HiltonHD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Networks not employing destination-side source address validation (DSAV) expose themselves to a class of pernicious attacks which could be prevented by filtering inbound traffic purporting to originate from within the network. In this work, we survey the pervasiveness of networks vulnerable to infiltration using spoofed addresses internal to the network. We issue recursive Domain Name System (DNS) queries to a large set of known DNS servers world-wide using various spoofed-source addresses. In late 2019, we found that 49% of the autonomous systems we tested lacked DSAV. After a large-scale notification campaign run in late 2020, we repeated our measurements in early 2021 and found that 44% of ASes lacked DSAV—though importantly, as this is an observational study, we cannot conclude causality. As case studies illustrating the dangers of a lack of DSAV, we measure susceptibility of DNS resolvers to cache poisoning attacks and the NXNS attack, two attacks whose attack surface is significantly reduced when DSAV in place. We discover 309K resolvers vulnerable to the NXNS attack and 4K resolvers vulnerable to cache poisoning attacks, 70% and 59% of which would have been protected had DSAV been in place.}
}


@article{DBLP:journals/ton/WangMCW22,
	author = {Haibo Wang and
                  Chaoyi Ma and
                  Shigang Chen and
                  Yuanda Wang},
	title = {Fast and Accurate Cardinality Estimation by Self-Morphing Bitmaps},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1674--1688},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3147204},
	doi = {10.1109/TNET.2022.3147204},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangMCW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Estimating the cardinality of a data stream is a fundamental problem underlying numerous applications such as traffic monitoring in a network or a datacenter and query optimization of Internet-scale P2P data networks. Existing solutions suffer from high processing/query overhead or memory in-efficiency, which prevents them from operating online for data streams with very high arrival rates. This paper takes a new solution path different from the prior art and proposes a self-morphing bitmap, which combines operational simplicity with structural dynamics, allowing the bitmap to be morphed in a series of steps with an evolving sampling probability that automatically adapts to different stream sizes. We further generalize the design of self-morphing bitmap. We evaluate the self-morphing bitmap theoretically and experimentally. The results demonstrate that it significantly outperforms the prior art.}
}


@article{DBLP:journals/ton/LiLCJ22,
	author = {Xiao{-}Yan Li and
                  Wanling Lin and
                  Jou{-}Ming Chang and
                  Xiaohua Jia},
	title = {Transmission Failure Analysis of Multi-Protection Routing in Data
                  Center Networks With Heterogeneous Edge-Core Servers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1689--1702},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3147320},
	doi = {10.1109/TNET.2022.3147320},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLCJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recently proposed RCube network is a cube-based server-centric data center network (DCN), including two types of heterogeneous servers, called core servers and edge servers. Remarkably, it takes the latter as backup servers to deal with server failures and thus achieve high availability. This paper first points out that RCube is suitable as a candidate topology of DCNs for edge computing. Three transmission types are among core and edge servers based on the demand for applications’ computation and instant response. We then employ protection routing to analyze the transmission failure of RCube DCNs. Unlike traditional protection routing, which only tolerates a single link or node failure, we use the multi-protection routing scheme to improve fault-tolerance capability. To configure a protection routing in a network, according to Tapolcai’s suggestion, we need to construct two completely independent spanning trees (CISTs), which are edge-disjoint and inner-vertex-disjoint spanning trees. It is well-known that the problem of determining whether there exists a dual-CIST (i.e., two CISTs) in a network is NP-complete. A logic graph of RCube, denoted by $L$ - $RCube(n,m,k)$ , is a network with a recursive structure. Each basic building element consists of $n$ core servers and $m$ edge servers, where the order $k$ is the number of recursions applied in the structure. In this paper, we provide algorithms to construct $\\min \\{n,\\lfloor (n+m)/2\\rfloor \\}$ CISTs in $L$ - $RCube(n,m,k)$ for $n+m\\geqslant 4$ and $n>1$ . From a combination of the multiple CISTs, we can configure the desired multi-protection routing. In our simulation, we configure up to 10 protection routings for RCube DCNs. As far as we know, in past research, there were at most three protection routings developed in other network structures. Finally, we summarize some crucial analysis viewpoints about the transmission efficiency of DCNs with heterogeneous edge-core servers from the simulation results.}
}


@article{DBLP:journals/ton/CaiZCWSG22,
	author = {Yunxiang Cai and
                  Hongzi Zhu and
                  Shan Chang and
                  Xiao Wang and
                  Jiangang Shen and
                  Minyi Guo},
	title = {PeerProbe: Estimating Vehicular Neighbor Distribution With Adaptive
                  Compressive Sensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1703--1716},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3149008},
	doi = {10.1109/TNET.2022.3149008},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaiZCWSG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acquiring the geographical distribution of neighbors can support more adaptive media access control (MAC) protocols and other safety applications in Vehicular ad hoc network (VANETs). However, it is very challenging for each vehicle to estimate its own neighbor distribution in a fully distributed setting. In this paper, we propose an online distributed neighbor distribution estimation scheme, called PeerProbe, in which vehicles collaborate with each other to probe their own neighborhood via simultaneous symbol-level wireless communication. An adaptive compressive sensing algorithm is developed to recover a neighbor distribution based on a small number of random probes with non-negligible noise. Moreover, the needed number of probes adapts to the sparseness of the distribution. We implement a prototype system to verify the feasibility of PeerProbe in various typical vehicular channel conditions. We further conduct extensive simulations and the results demonstrate that PeerProbe is lightweight and can accurately recover highly dynamic neighbor distributions in critical channel conditions.}
}


@article{DBLP:journals/ton/LiuTCS22,
	author = {Chang Liu and
                  Jean Tourrilhes and
                  Chen{-}Nee Chuah and
                  Puneet Sharma},
	title = {Voyager: Revisiting Available Bandwidth Estimation With a New Class
                  of Methods - Decreasing- Chirp-Train Methods},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1717--1732},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3152175},
	doi = {10.1109/TNET.2022.3152175},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuTCS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The available bandwidth (ABW) of a network path is a crucial metric for various applications, such as traffic engineering, congestion control, multimedia streaming, and path selection in software-defined wide-area networks (SDWAN). In recent years, a new class of measurement methods have been proposed to estimate the available bandwidth, decreasing-chirp-train methods. However, the performance and limitations of this new class of methods are neither well studied nor fairly compared beyond simulation studies. In this work, we implement Voyager, a modular framework that allows us to conduct a fair and thorough comparison of how a variety of modern bandwidth estimation methods perform under different network paths and traffic conditions. We shed light on the characteristics and limitations of the internal algorithms of various methods, and propose two new methods. We investigate the impact of various bottleneck types and traffic types, and we explore the performance of these methods on high speed links where interrupt coalescence can cause measurement noise. We finally test Voyager on long-distance Internet links and with live traffic and report our findings.}
}


@article{DBLP:journals/ton/YangFMLZLZ22,
	author = {Edwin Yang and
                  Song Fang and
                  Ian D. Markwood and
                  Yao Liu and
                  Shangqing Zhao and
                  Zhuo Lu and
                  Haojin Zhu},
	title = {Wireless Training-Free Keystroke Inference Attack and Defense},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1733--1748},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3147721},
	doi = {10.1109/TNET.2022.3147721},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangFMLZLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing research work has identified a new class of attacks that can eavesdrop on the keystrokes in a non-invasive way without infecting the target computer to install malware. The common idea is that pressing a key of a keyboard can cause a unique and subtle environmental change, which can be captured and analyzed by the eavesdropper to learn the keystrokes. For these attacks, however, a training phase must be accomplished to establish the relationship between an observed environmental change and the action of pressing a specific key. This significantly limits the impact and practicality of these attacks. In this paper, we discover that it is possible to design keystroke eavesdropping attacks without requiring the training phase. We create this attack based on the channel state information extracted from the wireless signal. To eavesdrop on keystrokes, we establish a mapping between typing each letter and its respective environmental change by exploiting the correlation among observed changes and known structures of dictionary words. To defend against this attack, we propose a reactive jamming mechanism that launches the jamming only during the typing period. Experimental results on software-defined radio platforms validate the impact of the attack and the performance of the defense.}
}


@article{DBLP:journals/ton/GopalamHW22,
	author = {Swaroop Gopalam and
                  Stephen V. Hanly and
                  Philip Whiting},
	title = {Distributed and Local Scheduling Algorithms for mmWave Integrated
                  Access and Backhaul},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1749--1764},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3154367},
	doi = {10.1109/TNET.2022.3154367},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GopalamHW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the stability region of a mmWave integrated access and backhaul (IAB) network with stochastic arrivals and time-varying link rates. In the scheduling of links, we consider a limit on the number of RF chains, and the half-duplex constraint which occurs due to the wireless backhaul links. We characterize the stability region, and propose a back-pressure policy for the IAB network under the RF chains and half-duplex constraints. To implement the back-pressure policy, it is required to compute the maximum weighted schedule, which is a complex problem in general. For the IAB network, we present a distributed message passing scheme to compute the maximum weighted schedule, with almost linear complexity. We also investigate a class of local scheduling policies for the IAB network, which have a smaller stability region in general, but require no message passing. We characterize the stability region for the local class, and show that it is same as the global stability region, if the link rates are un-varying. We provide a bound on the gap between local and global regions when the links are time varying. We propose a local max-weight algorithm which achieves the stability region for the local class, and we present numerical results.}
}


@article{DBLP:journals/ton/FuWNCB22,
	author = {Junsong Fu and
                  Na Wang and
                  Leyao Nie and
                  Baojiang Cui and
                  Bharat K. Bhargava},
	title = {Defending Trace-Back Attack in 3D Wireless Internet of Things},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1765--1779},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3149293},
	doi = {10.1109/TNET.2022.3149293},
	timestamp = {Thu, 16 May 2024 21:47:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuWNCB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of 5G, it is unsurprising that most of the smart devices in the Internet of Things (IoT) will be wirelessly connected with each other in the near future. This kind of lightweight, scalable and green network architecture will be well-received. In a wide variety of IoT application scenarios, sensor nodes deployed in a local space, such as a multistory building, automatically form a distributed 3D wireless IoT and it can be employed to collect and analyze environmental information. Source-location privacy protection is of great importance in these networks and however, most existing schemes focus on only planar distributed networks which are not suitable for the 3D networks. In this paper, we consider a novel trace-back attack for 3D wireless IoT and then design a source-location privacy protection scheme, named DMR-3D, to defend this kind of novel attacks. In DMR-3D, the source node first selects a set of virtual locations to indirectly choose a set of agent nodes based on the cold start sphere structure and the ellipsoid communication pipeline. Then, a sophisticated mechanism is designed based on both the connected graph and Multiple Delaunay Triangulation (MDT) structure of the network to deliver packets from the source node to the destination node via these agent nodes in a relay manner. Analysis and simulation results illustrate that the proposed scheme can effectively protect source-location privacy with a moderate increment of path stretch, time delay and data transmission amount.}
}


@article{DBLP:journals/ton/MohammadpourB22a,
	author = {Ehsan Mohammadpour and
                  Jean{-}Yves Le Boudec},
	title = {Analysis of Dampers in Time-Sensitive Networks With Non-Ideal Clocks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1780--1794},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3152178},
	doi = {10.1109/TNET.2022.3152178},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MohammadpourB22a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Dampers are devices that reduce delay jitter in the context of time-sensitive networks, by delaying packets for the amount written in packet headers. Jitter reduction is required by some real-time applications; beyond this, dampers have the potential to solve the burstiness cascade problem of deterministic networks in a scalable way, as they can be stateless. Dampers exist in several variants: some apply only to earliest-deadline-first schedulers, whereas others can be associated with any packet schedulers; some enforce FIFO ordering whereas some others do not. Existing analyses of dampers are specific to some implementations and some network configurations; also, they assume ideal, non-realistic clocks. In this paper, we provide a taxonomy of all existing dampers in general network settings and analyze their timing properties in presence of non-ideal clocks. In particular, we give formulas for computing residual jitter bounds of networks with dampers of any kind. We show that non-FIFO dampers may cause reordering due to clock non-idealities and that the combination of FIFO dampers with non-FIFO network elements may very negatively affect the performance bounds. Our results can be used to analyze timing properties and burstiness increase in any time-sensitive network, as we illustrate on an industrial case-study.}
}


@article{DBLP:journals/ton/NasrallaEHME22,
	author = {Zaid H. Nasralla and
                  Taisir E. H. El{-}Gorashi and
                  Ali Hammadi and
                  Mohamed O. I. Musa and
                  Jaafar M. H. Elmirghani},
	title = {Blackout Resilient Optical Core Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1795--1806},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3156529},
	doi = {10.1109/TNET.2022.3156529},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NasrallaEHME22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A disaster may not necessarily demolish the telecommunications infrastructure, but instead it might affect the national grid and cause blackouts, consequently disrupting the network operation unless there is an alternative power source(s). In disaster-resilient networks, fiber cut, datacenter destruction, and node isolation have been studied before with different scenarios, but the power outage impact has not been investigated before. In this paper, power outages are considered, and the telecommunication network performance is evaluated during a blackout. A mixed Integer Linear Programming (MILP) model is developed to evaluate the network performance for a single node blackout under two scenarios: minimization of blocking and minimization of renewable and battery energy consumption. Insights analyzed from the MILP model results have demonstrated the trade-off between the two evaluated optimization cost functions and shown that the proposed scheme can extend the network lifetime while minimizing the required amount of backup energy.}
}


@article{DBLP:journals/ton/WangZLM22,
	author = {Yuting Wang and
                  Xiaolong Zheng and
                  Liang Liu and
                  Huadong Ma},
	title = {PolarTracker: Attitude-Aware Channel Access for Floating Low Power
                  Wide Area Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1807--1821},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3154937},
	doi = {10.1109/TNET.2022.3154937},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangZLM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Power Wide Area Networks (LPWAN) such as Long Range (LoRa) show great potential in emerging aquatic IoT applications. However, our deployment experience shows that the floating LPWAN suffers significant performance degradation, compared to the static terrestrial deployments. Our measurement results reveal the reason behind this is the polarization and directivity of the antenna. The dynamic attitude of a floating node incurs varying signal strength losses, which is ignored by the attitude-oblivious link model adopted in most of the existing methods. When accessing the channel at a misaligned attitude, packet errors can happen. In this paper, we propose an attitude-aware link model that explicitly quantifies the impact of node attitude on link quality. Based on the new model, we propose PolarTracker , a novel channel access method for floating LPWAN. PolarTracker tracks the node attitude alignment state and schedules the transmissions into the aligned periods with better link quality. To support concurrent access of multiple LoRa nodes, an attitude-based slotted-ALOHA protocol is proposed to reduce collision. We implement a prototype of PolarTracker on commercial LoRa platforms and extensively evaluate its performance in various real-world environments. The experimental results show that PolarTracker can efficiently improve the packet reception ratio by 50.6%, compared with ALOHA in LoRaWAN.}
}


@article{DBLP:journals/ton/SterzFSKKHF22,
	author = {Artur Sterz and
                  Patrick Felka and
                  Bernd Simon and
                  Sabrina Klos and
                  Anja Klein and
                  Oliver Hinz and
                  Bernd Freisleben},
	title = {Multi-Stakeholder Service Placement via Iterative Bargaining With
                  Incomplete Information},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1822--1837},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3157040},
	doi = {10.1109/TNET.2022.3157040},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SterzFSKKHF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing based on cloudlets is an emerging paradigm to improve service quality by bringing computation and storage facilities closer to end users and reducing operating cost for infrastructure providers (IPs) and service providers (SPs). To maximize their individual benefits, IP and SP have to reach an agreement about placing and executing services on particular cloudlets. We show that a Nash Bargaining Solution (NBS) yields the optimal solution with respect to social cost and fairness if IP and SP have complete information about the parameters of their mutual cost functions. However, IP and SP might not be willing or able to share all information due to business secrets or technical limitations. Therefore, we present a novel iterative bargaining approach without complete mutual information to achieve substantial cost reductions for both IP and SP. Furthermore, we investigate how different degrees of information sharing impact social cost and fairness of the different approaches. Our evaluation based on the mobile augmented reality game Ingress shows that our approach achieves up to about 82% of the cost reduction that the NBS achieves and a cost reduction of up to 147% compared to traditional Take-it-or-Leave-it approaches, despite incomplete information.}
}


@article{DBLP:journals/ton/AvinMS22,
	author = {Chen Avin and
                  Kaushik Mondal and
                  Stefan Schmid},
	title = {Demand-Aware Network Design With Minimal Congestion and Route Lengths},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1838--1848},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3153586},
	doi = {10.1109/TNET.2022.3153586},
	timestamp = {Thu, 25 Aug 2022 08:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AvinMS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging communication technologies allow to reconfigure the physical network topology at runtime, enabling demand-aware networks (DANs) : networks whose topology is optimized toward the workload they serve. However, today, only little is known about the fundamental algorithmic problems underlying the design of such demand-aware networks. This paper presents the first bounded-degree, demand-aware network, \\textit {cl-DAN}\n, which minimizes both congestion and route lengths. The degree bound \\Delta\nis given as part of the input. The designed network is provably (asymptotically) optimal in each dimension individually: we show that there do not exist any bounded-degree networks providing shorter routes (independently of the load), nor do there exist networks providing lower loads (independently of the route lengths). The main building block of the designed \\textit {cl-DAN}\nnetworks are \\textit {ego-trees}\n: communication sources arrange their communication partners in an optimal tree, individually . While the union of these ego-trees forms the basic structure of \\textit {cl-DANs}\n, further techniques are presented to ensure bounded degrees (for scalability).}
}


@article{DBLP:journals/ton/XueHYXW22,
	author = {Kaiping Xue and
                  Peixuan He and
                  Jiayu Yang and
                  Qiudong Xia and
                  David S. L. Wei},
	title = {{SCD2:} Secure Content Delivery and Deduplication With Multiple Content
                  Providers in Information Centric Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1849--1864},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3155110},
	doi = {10.1109/TNET.2022.3155110},
	timestamp = {Thu, 25 Aug 2022 08:36:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XueHYXW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the promising next generation network architectures, information centric networking (ICN) is highly anticipated to improve the bandwidth usage of the Internet and reduce duplicate traffic. Since contents in ICN are disseminated in the whole network, ICN is much more vulnerable and the issue of how to deliver contents securely has been intensively discussed. However, the scalability of the existing schemes is limited. A scalable scheme is expected to be able to achieve fine-grained access control and at the same time also support multiple content providers scenario with efficient key management at user side. Besides, different content providers may publish some identical contents and these contents may be cached in the same intermediate routers, which causes high data redundancy and in turn exerts an adverse impact on the performance of ICN. In this paper, we propose a Secure Content Delivery and Deduplication scheme, called SCD2, to achieve secure and efficient fine-grained access control in ICN with multiple content providers. We first propose a scalable key-policy attribute-based encryption (SKP-ABE) to provide fine-grained access control and allow different attribute authorities to share some public attributes to simplify the key management. Furthermore, based on SKP-ABE, we design a simple but effective mechanism to conduct content deduplication. Finally, we implement a prototype of SCD2 to test its performance and compare it with some existing schemes. The results show that SCD2 has lower storage overhead, a higher degree of deduplication, and better retrieval efficiency.}
}


@article{DBLP:journals/ton/JosiloD22,
	author = {Sladana Josilo and
                  Gy{\"{o}}rgy D{\'{a}}n},
	title = {Joint Wireless and Edge Computing Resource Management With Dynamic
                  Network Slice Selection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1865--1878},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3156178},
	doi = {10.1109/TNET.2022.3156178},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JosiloD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a promising approach for enabling low latency computation offloading in edge computing systems. In this paper, we consider an edge computing system under network slicing in which the wireless devices generate latency sensitive computational tasks. We address the problem of joint dynamic assignment of computational tasks to slices, management of radio resources across slices and management of radio and computing resources within slices. We formulate the Joint Slice Selection and Edge Resource Management (JSS-ERM) problem as a mixed-integer problem with the objective to minimize the completion time of computational tasks. We show that the JSS-ERM problem is NP-hard and develop an approximation algorithm with bounded approximation ratio based on a game theoretic treatment of the problem. We use extensive simulations to provide insight into the performance of the proposed solution from the perspective of the whole system and from the perspective of individual slices. Our results show that the proposed slicing policy can achieve significant gains compared to the equal slicing policy, and that the computational complexity of the proposed task placement algorithm is approximately linear in the number of devices.}
}


@article{DBLP:journals/ton/AggarwalSKPKW22,
	author = {Shivang Aggarwal and
                  Swetank Kumar Saha and
                  Imran Khan and
                  Rohan Pathak and
                  Dimitrios Koutsonikolas and
                  Joerg Widmer},
	title = {MuSher: An Agile Multipath-TCP Scheduler for Dual-Band 802.11ad/ac
                  Wireless LANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1879--1894},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3158678},
	doi = {10.1109/TNET.2022.3158678},
	timestamp = {Thu, 25 Aug 2022 08:36:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AggarwalSKPKW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future WLAN devices will combine both IEEE 802.11ad and 802.11ac interfaces. The former provides multi-Gbps rates but is susceptible to blockage, whereas the latter is slower but offers reliable connectivity. A fundamental challenge is thus how to combine those complementary technologies, to make the most of the advantages they offer. In this work, we leverage Multipath TCP (MPTCP) to use both interfaces simultaneously in order to achieve a higher overall throughput as well as seamlessly switch to a single interface when the other one fails. We find that standard MPTCP often performs sub-optimally and can yield a throughput much lower than that of single path TCP over the faster of the two interfaces. We analyze the cause of these performance issues in detail and then design MuSher , an agile MPTCP scheduler that allows MPTCP to fully utilize the channel resources available to both interfaces. Our evaluation in realistic scenarios shows that MuSher can provide a throughput improvement of 50%/130% under WLAN/Internet settings respectively, compared to the default MPTCP scheduler. It further speeds up the recovery of a traffic stream after disruption by a factor of 8x/75x.}
}


@article{DBLP:journals/ton/ShiSDZ22,
	author = {Bin Shi and
                  Haiying Shen and
                  Bo Dong and
                  Qinghua Zheng},
	title = {Memory/Disk Operation Aware Lightweight {VM} Live Migration},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {4},
	pages = {1895--1910},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3155935},
	doi = {10.1109/TNET.2022.3155935},
	timestamp = {Thu, 25 Aug 2022 08:36:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiSDZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Live virtual machine migration technique allows migrating an entire OS with running applications from one physical host to another, while keeping all services available without interruption. It provides a flexible and powerful way to balance system load, save power, and tolerate faults in data centers. Meanwhile, with the stringent requirements of latency, scalability, and availability, an increasing number of applications are deployed across distributed data-centers. However, existing live migration approaches still suffer from long downtime and serious performance degradation in cross data-center scenes due to the mass of dirty retransmission, which limits the ability of cross data-center scheduling. In this paper, we propose a system named Memory/disk operation aware Lightweight VM Live Migration across data-centers with low performance impact (MLLM). It significantly improves the cross data-center migration performance by reducing the amount of dirty data in the migration process. In MLLM, we predict disk read workingset (i.e., more frequently read contents) and memory write workingset (i.e., more frequently write contents) based on the access sequence traces. And then we adjust the migration models and data transfer sequence by the workingset information. We further proposed an improved algorithm for workingset estimation. Moreover, we discussed the potential use of machine learning (ML) to enhance the performance of the VM migration and also propose a two-level hierarchical network model to make the ML-based prediction more efficient. We implement MLLM and its improved versions on the QEMU/KVM platform and conduct several experiments. The experimental results show that 1) MLLM averagely reduces 62.9% of total migration time and 36.0% service downtime over existing methods; 2) The improved workingset estimation algorithm reduces 9.32% memory pre-copy time on average over the original algorithm.}
}


@article{DBLP:journals/ton/MatousekLJSKA22,
	author = {Jir{\'{\i}} Matousek and
                  Adam Lucansk{\'{y}} and
                  David Janecek and
                  Jozef Sabo and
                  Jan Korenek and
                  Gianni Antichi},
	title = {ClassBench-ng: Benchmarking Packet Classification Algorithms in the
                  OpenFlow Era},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1912--1925},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3155708},
	doi = {10.1109/TNET.2022.3155708},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MatousekLJSKA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet classification, i.e., the process of categorizing packets into flows, is a first-class citizen in any networking device. Every time a new packet has to be processed, one or more header fields need to be compared against a set of pre-installed rules. This is done for basic forwarding operations, to apply security policies, application-specific processing, or quality-of-service guarantees. A lot of research efforts have identified better lookup techniques, i.e., finding the best match between packet headers and rules, by capitalizing on the rule sets characteristics. Here, ClassBench has greatly served the community by enabling the generation of IPv4 rule sets. In this paper, we present a new tool, ClassBench-ng, that creates synthetic IPv4, IPv6, and OpenFlow rules. We start from an analysis of classification rules deployed in-the-wild and we use the findings to craft our solution. ClassBench-ng can generate a user-defined number of rules as well as an associated header trace matching them. Compared to state-of-the-art solutions, the rule set generation process is usually more accurate and it is able to produce rules matching a number of different use cases, i.e., from an IPv4 router to an OpenFlow switch, which is unique among current rule set generation tools.}
}


@article{DBLP:journals/ton/JahanianR22,
	author = {Mohammad Jahanian and
                  K. K. Ramakrishnan},
	title = {CoNICE: Consensus in Intermittently-Connected Environments by Exploiting
                  Naming With Application to Emergency Response},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1926--1939},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3156101},
	doi = {10.1109/TNET.2022.3156101},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JahanianR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many scenarios, information must be disseminated over intermittently-connected environments when the network infrastructure becomes unavailable, e.g., during disasters where first responders need to send updates about critical tasks. If such updates pertain to a shared data set, dissemination consistency is important. This can be achieved through causal ordering and consensus. Popular consensus algorithms, e.g., Paxos, are most suited for connected environments. While some work has been done on designing consensus algorithms for intermittently-connected environments, such as the One-Third Rule (OTR) algorithm, there is still need to improve their efficiency and timely completion. We propose CoNICE, a framework to ensure consistent dissemination of updates among users in intermittently-connected, infrastructure-less environments. It achieves efficiency by exploiting hierarchical namespaces for faster convergence, and lower communication overhead. CoNICE provides three levels of consistency to users, namely replication, causality and agreement. It uses epidemic propagation to provide adequate replication ratios, and optimizes and extends Vector Clocks to provide causality. To ensure agreement, CoNICE extends OTR to also support long-term network fragmentation and decision invalidation scenarios; we define local and global consensus pertaining to within and across fragments respectively. We integrate CoNICE’s consistency preservation with a naming schema that follows a topic hierarchy-based dissemination framework, to improve functionality and performance. Using the Heard-Of model formalism, we prove CoNICE’s consensus to be correct. Our technique extends previously established proof methods for consensus in asynchronous environments. Performing city-scale simulation, we demonstrate CoNICE’s scalability in achieving consistency in convergence time, utilization of network resources, and reduced energy consumption.}
}


@article{DBLP:journals/ton/BiswasDR22,
	author = {Nilanjan Biswas and
                  Goutam Das and
                  Priyadip Ray},
	title = {Buffer-Aware User Selection and Resource Allocation for an Opportunistic
                  Cognitive Radio Network: {A} Cross-Layer Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1940--1954},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3159819},
	doi = {10.1109/TNET.2022.3159819},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BiswasDR22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we focus on a cross-layer resource allocation problem for an opportunistic cognitive radio network, where secondary users (SUs) share a primary network’s licensed spectrum only when the primary network is sensed to be idle. We consider cooperative spectrum sensing, where SUs participate in sensing only when a benefit from resource allocation is guaranteed. Number of SUs for cooperative spectrum sensing is an important parameter as it defines both sensing and resource allocation performance. The fusion centre captures interaction between physical layer spectrum sensing, channel condition, and higher layer data buffer while selecting SUs for cooperative sensing. We consider heterogeneous data types at SUs’ data buffers during SU selection to make our system model more general. We form a mixed integer non-linear optimization problem, from which we select SUs’ set for sensing and resource allocation, appropriate sensing thresholds, and corresponding resource allocation parameters. Due to it’s combinatorial nature, the optimization problem is non-trivial. We devise an optimal algorithm to solve the optimization problem. Moreover, we also propose a computationally efficient and near-optimal greedy algorithm with suitable performance bounds. We also show advantage of our proposed optimal algorithms compared to other traditional methods through extensive simulations.}
}


@article{DBLP:journals/ton/XieTWXCJW22,
	author = {Kun Xie and
                  Jiazheng Tian and
                  Xin Wang and
                  Gaogang Xie and
                  Jiannong Cao and
                  Hongbo Jiang and
                  Jigang Wen},
	title = {Fast Retrieval of Large Entries With Incomplete Measurement Data},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1955--1969},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3160233},
	doi = {10.1109/TNET.2022.3160233},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XieTWXCJW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In network-wide monitoring, finding the large monitoring data entries is a fundamental network management function. However, the retrieval of large entries is extremely difficult and challenging as a result of incompleteness of network measurement data. Enlightened by tensor model’s strong capability of information representation and extraction, we model the network-wide monitoring data as a 3-way tensor. With tensor completion, the retrieval can be performed after recovering all missing entries. However, this not only incurs an extremely high cost when the tensor is large, but is also unnecessary. Instead, to quickly retrieve large entries at low cost, we transform the large entry retrieving problem to a cosine similarity searching problem, and propose two algorithms: 1) Quickly reordering the factor vectors based on Locality Sensitive Hashing (LSH) hash table so that vectors with small cosine distances are placed in the same hash bucket; 2) Quickly finding the similar vector of a queried one that the two together determine a large entry without incurring the high cost of recovering all entries through the dot products. In the process of LSH table building and similarity query, several novel techniques are proposed, including LSH table representation with the LSH forest, good hash table building to support the flexible search of cosine similarity, and bit-shifting-based quick similarity query. Our experimental studies on 4 real world datasets indicate that our technique is at least up to 60 times faster than the approach based on direct tensor completion.}
}


@article{DBLP:journals/ton/DasRWWWCN22,
	author = {Sushovan Das and
                  Afsaneh Rahbar and
                  Xinyu Crystal Wu and
                  Zhuang Wang and
                  Weitao Wang and
                  Ang Chen and
                  T. S. Eugene Ng},
	title = {Shufflecast: An Optical, Data-Rate Agnostic, and Low-Power Multicast
                  Architecture for Next-Generation Compute Clusters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1970--1985},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3158899},
	doi = {10.1109/TNET.2022.3158899},
	timestamp = {Tue, 07 May 2024 20:25:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DasRWWWCN22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An optical circuit-switched network core has the potential to overcome the inherent challenges of a conventional electrical packet-switched core of today’s compute clusters. As optical circuit switches (OCS) directly handle the photon beams without any optical-electrical-optical (O/E/O) conversion and packet processing, OCS-based network cores have the following desirable properties: a) agnostic to data-rate, b) negligible/zero power consumption, c) no need of transceivers, d) negligible forwarding latency, and e) no need for frequent upgrade. Unfortunately, OCS can only provide point-to-point (unicast) circuits. They do not have built-in support for one-to-many (multicast) communication, yet multicast is fundamental to a plethora of data-intensive applications running on compute clusters nowadays. In this paper, we propose Shufflecast, a novel optical network architecture for next-generation compute clusters that can support high-performance multicast satisfying all the properties of an OCS-based network core. Shufflecast leverages small fanout, inexpensive, passive optical splitters to connect the Top-of-rack (ToR) switch ports, ensuring data-rate agnostic, low-power, physical-layer multicast. We thoroughly analyze Shufflecast’s highly scalable data plane, light-weight control plane, and graceful failure handling. Further, we implement a complete prototype of Shufflecast in our testbed and extensively evaluate the network. Shufflecast is more power-efficient than the state-of-the-art multicast mechanisms. Also, Shufflecast is more cost-efficient than a conventional packet-switched network. By adding Shufflecast alongside an OCS-based unicast network, an all-optical network core with the aforementioned desirable properties supporting both unicast and multicast can be realized.}
}


@article{DBLP:journals/ton/TranTM22,
	author = {Hai Anh Tran and
                  Duc Tran and
                  Abdelhamid Mellouk},
	title = {State-Dependent Multi-Constraint Topology Configuration for Software-Defined
                  Service Overlay Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {1986--2001},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3155475},
	doi = {10.1109/TNET.2022.3155475},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TranTM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Service Overlay Network (SON) is an efficient solution for ensuring the end-to-end Quality of Service (QoS) in different real-world applications, including Video-on-Demand, Voice over IP, and other value-added Internet-based services. Although SON offers many advantages, such as ease of deployment and resilience to the node failures, it has to face the challenge of overlay network configuration that needs to dynamically adjust to the change in communication requirements. In this paper, we propose a novel method for adaptive overlay topology configuration, called AOTC based on Software-Defined Networks, deep learning, and reinforcement learning. The intuitive motivation is to address the above challenge, maximize the QoS from two aspects of customer preference and network cost. The obtained experimental results demonstrate the superiority of AOTC. Such a method can significantly reduce network cost while providing an improvement of 50% and 60% in terms of average delay and packet loss rate as compared to other traditional approaches.}
}


@article{DBLP:journals/ton/KarakocSRW22,
	author = {Nurullah Karako{\c{c}} and
                  Anna Scaglione and
                  Martin Reisslein and
                  Ruiyuan Wu},
	title = {Federated Edge Network Utility Maximization for a Multi-Server System:
                  Algorithm and Convergence},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2002--2017},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3156530},
	doi = {10.1109/TNET.2022.3156530},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KarakocSRW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose a novel Federated Edge Network Utility Maximization (FEdg-NUM) architecture for solving a large-scale distributed network utility maximization (NUM) problem. In FEdg-NUM, clients with private utilities communicate to a peer-to-peer network of edge servers. This represents a departure from the classical distributed NUM master-slave configuration and enables distributed computing harnessing local communications. Compared to a solution using cloud synchronization via Ring AllReduce, we prove that our federated edge computing model has shorter run-time in the presence of network congestion, thanks to its configuration and its ability to make progress in the presence of intermittent links. The paper studies its convergence and run-time performance both analytically and numerically, and illustrates several possible networking applications.}
}


@article{DBLP:journals/ton/VyavahareNM22,
	author = {Pooja Vyavahare and
                  Jayakrishnan Nair and
                  D. Manjunath},
	title = {Sponsored Data: On the Effect of {ISP} Competition on Pricing Dynamics
                  and Content Provider Market Structures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2018--2031},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3162856},
	doi = {10.1109/TNET.2022.3162856},
	timestamp = {Sat, 30 Sep 2023 10:29:32 +0200},
	biburl = {https://dblp.org/rec/journals/ton/VyavahareNM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We analyze the effect of sponsored data when Internet service providers (ISPs) compete for subscribers and content providers (CPs) compete for a share of the bandwidth usage by customers. Our model is of a full information, leader-follower game. ISPs lead and set sponsorship prices. CPs then make the binary decision of sponsoring or not sponsoring their content on the ISPs. Lastly, based on both of these, users make a two-part decision-choose the ISP to subscribe to, and amount of data to consume from each CPs through the chosen ISP. User consumption is determined by a utility maximization framework, sponsorship decision is determined by a non-cooperative game between CPs, and ISPs set their prices to maximize their profit in response to prices set by competing ISP. We analyze the dynamics of the prices set by ISPs, the sponsorship decisions of CPs, the market structure therein, and surpluses of the ISPs, CPs, users. This is the first analysis of the effect sponsored data in the presence of ISP competition. We show that inter-ISP competition does not inhibit ISPs from extracting a significant fraction of CP surplus, leaving CPs no better off (and sometimes worse off) as compared to the scenario where data sponsoring is disallowed. Moreover, ISPs often have an incentive to significantly skew the CP marketplace in favor of the most profitable CP.}
}


@article{DBLP:journals/ton/ZhangRPBZ22,
	author = {Zhenghao Zhang and
                  Raghav Rathi and
                  Steven Perez and
                  Jumana Bukhari and
                  Yaoguang Zhong},
	title = {{ZCNET:} Achieving High Capacity in Low Power Wide Area Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2032--2045},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3158482},
	doi = {10.1109/TNET.2022.3158482},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangRPBZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, a novel LPWAN technology, ZCNET, is proposed, which achieves significantly higher network capacity than existing solutions, such as LoRa, Sigfox, and RPMA. The capacity boost of ZCNET is mainly due to two reasons. First, a ZCNET node transmits signals that occupy a small fraction of the signal space, resulting in a low collision probability. Second, ZCNET supports 8 parallel root channels within a single frequency channel by using 8 Zadoff-Chu (ZC) root sequences. The root channels do not severely interfere with each other, mainly because the interference power is spread evenly over the entire signal space. A simple ALOHA-style protocol is used for medium access, with which a node randomly chooses the root channel and the range it occupies within the root channel. ZCNET has been extensively tested with both real-world experiments on the USRP and simulations, and the results confirm that ZCNET achieves significant gains over LoRa, Sigfox, and RPMA. ZCNET will likely better accommodate the explosive growth of IoT network sizes and meet the demand of IoT applications.}
}


@article{DBLP:journals/ton/LiLLCHLK22,
	author = {Chengzhang Li and
                  Qingyu Liu and
                  Shaoran Li and
                  Yongce Chen and
                  Y. Thomas Hou and
                  Wenjing Lou and
                  Sastry Kompella},
	title = {Scheduling With Age of Information Guarantee},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2046--2059},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3156866},
	doi = {10.1109/TNET.2022.3156866},
	timestamp = {Thu, 11 May 2023 21:27:18 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLLCHLK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information (AoI) is an application layer performance metric that quantifies the freshness of information. This paper investigates scheduling problems at network edge when there is an AoI requirement for each source node, which we call Maximum AoI Threshold (MAT). Specifically, we want to determine whether or not a vector of MATs corresponding to the source nodes is schedulable, and if so, find a feasible scheduler for it. For a small network, we present an optimal procedure called Cyclic Scheduler Detection (CSD) that can determine the schedulability with absolute certainty. For a large network where CSD is not applicable, we present a novel low-complexity procedure, called Fictitious Polynomial Mapping (FPM), and prove that FPM can find a feasible scheduler for any MAT vector when the load is under\nln2\n. We use extensive numerical results to validate our theoretical results and show that the performance of FPM is significantly better than a state-of-the-art scheduling algorithm.}
}


@article{DBLP:journals/ton/LiZZLCA22,
	author = {Meng Li and
                  Liehuang Zhu and
                  Zijian Zhang and
                  Chhagan Lal and
                  Mauro Conti and
                  Mamoun Alazab},
	title = {User-Defined Privacy-Preserving Traffic Monitoring Against n-by-1
                  Jamming Attack},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2060--2073},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3157654},
	doi = {10.1109/TNET.2022.3157654},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZZLCA22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic monitoring services collect traffic reports and respond to users’ traffic queries. However, the reports and queries may reveal the user’s identity and location. Although different anonymization techniques have been applied to protect user privacy, a new security threat arises, namely, n-by-1 jamming attack, in which an anonymous contributing driver impersonates\nn\ndrivers and uploads\nn\nnormal reports by using\nn\nreporting devices. Such an attack will mislead the traffic monitoring service provider and further degrade the service quality. Existing traffic monitoring services do not support customized queries, and private information retrieval techniques cannot be applied directly in traffic monitoring. We formally define the new attack and propose a traffic monitoring scheme TraJ to defend the attack and achieve user-defined location privacy. Specifically, we bridge anonymous contributing drivers without disclosing their speed set by using private set intersection. Each RSU collects time traffic reports and structures a weighted proximity graph to filter out malicious colluding drivers. We design a user-defined privacy-preserving query method by encoding complex road network. We leverage the uploading phase from private aggregation to collect traffic conditions and allow requesting drivers to dynamically and privately query traffic conditions. We provide a formal analysis of TraJ to prove its privacy and security properties. We also construct a prototype based on a real-world dataset and Android smartphones to demonstrate its feasibility and efficiency. A formal analysis demonstrates the privacy and security properties. Extensive experiments illustrate the performance and defense efficacy.}
}


@article{DBLP:journals/ton/ZengBCCHZC22,
	author = {Gaoxiong Zeng and
                  Wei Bai and
                  Ge Chen and
                  Kai Chen and
                  Dongsu Han and
                  Yibo Zhu and
                  Lei Cui},
	title = {Congestion Control for Cross-Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2074--2089},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3161580},
	doi = {10.1109/TNET.2022.3161580},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZengBCCHZC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Geographically distributed applications hosted on cloud are becoming prevalent. They run on cross-datacenter network that consists of multiple data center networks (DCNs) connected by a wide area network (WAN). Such a cross-DC network poses significant challenges in transport design because the DCN and WAN segments have vastly distinct characteristics ( e.g. , buffer depths, RTTs). In this paper, we find that existing DCN or WAN transport reacting to ECN or delay alone do not (and cannot be extended to) work well for such an environment. The key reason is that neither of the signals, by itself only, can simultaneously capture the location and degree of congestion, mainly due to the discrepancies between DCN and WAN. Motivated by this, we present the design and implementation of GEMINI that strategically integrates both ECN and delay signals for cross-DC congestion control. To achieve low latency, GEMINI bounds the inter-DC latency with delay signal and prevents the intra-DC packet loss with ECN. To maintain high throughput, GEMINI modulates the window dynamics and maintains low buffer occupancy utilizing both congestion signals. GEMINI is implemented in Linux kernel and evaluated by extensive testbed experiments. Results show that GEMINI achieves up to 53%, 31%, 76% and 2% reduction of small flow average completion times, and up to 34%, 39%, 9% and 58% reduction of large flow average completion times compared to TCP Cubic, DCTCP, BBR and TCP Vegas.}
}


@article{DBLP:journals/ton/LiYLW22,
	author = {Chun Li and
                  Yunyun Yang and
                  Hui Liang and
                  Boying Wu},
	title = {Learning Quantum Drift-Diffusion Phenomenon by Physics-Constraint
                  Machine Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2090--2101},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3158987},
	doi = {10.1109/TNET.2022.3158987},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYLW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, deep learning (DL) is widely used to detect physical phenomena and has obtained encouraging results. Several works have shown that it can learn quantum phenomenon. Subsequently, quantum machine learning (QML) has been paid more attention by academia and industry. Quantum drift-diffusion (QDD) is a commonplace physical phenomenon, which is a macroscopic description of electrons and holes in a semiconductor. They are commonly used to attain an understanding of the property of semiconductor devices in physics and engineering. We are motivated by the relaxation-time limit from the quantum-Navier-Stokes-Poisson system (QNSP) to the QDD equation and the existence of finite energy weak solutions to the QDD equation has been proved. Therefore, in this work, the quantum drift-diffusion learning neural network (QDDLNN) is proposed to investigate the quantum drift phenomena from limited observations. Furthermore, a piece of numerical evidence is found that the NNs can describe quantum transport phenomena by simulating the quantum confinement transport equation-quantum Navier-Stokes equation.}
}


@article{DBLP:journals/ton/SmithJSB22,
	author = {Kevin D. Smith and
                  Saber Jafarpour and
                  Ananthram Swami and
                  Francesco Bullo},
	title = {Topology Inference With Multivariate Cumulants: The M{\"{o}}bius
                  Inference Algorithm},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2102--2116},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3164336},
	doi = {10.1109/TNET.2022.3164336},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SmithJSB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many tasks regarding the monitoring, management, and design of communication networks rely on knowledge of the routing topology. However, the standard approach to topology mapping—namely, active probing with traceroutes—relies on cooperation from increasingly non-cooperative routers, leading to missing information. Network tomography, which uses end-to-end measurements of additive link metrics (like delays or log packet loss rates) across monitor paths, is a possible remedy. Network tomography does not require that routers cooperate with traceroute probes, and it has already been used to infer the structure of multicast trees. This paper goes a step further. We provide a tomographic method to infer the underlying routing topology of an arbitrary set of monitor paths using the joint distribution of end-to-end measurements, without making any assumptions on routing behavior. Our approach, called the Möbius Inference Algorithm (MIA), uses cumulants of this distribution to quantify high-order interactions among monitor paths, and it applies Möbius inversion to “disentangle” these interactions. In addition to MIA, we provide a more practical variant called Sparse Möbius Inference, which uses various sparsity heuristics to reduce the number and order of cumulants required to be estimated. We show the viability of our approach using synthetic case studies based on real-world ISP topologies.}
}


@article{DBLP:journals/ton/ZengMWWW22,
	author = {Guangyang Zeng and
                  Biqiang Mu and
                  Jieqiang Wei and
                  Wing Shing Wong and
                  Junfeng Wu},
	title = {Localizability With Range-Difference Measurements: Numerical Computation
                  and Error Bound Analysis},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2117--2130},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3162930},
	doi = {10.1109/TNET.2022.3162930},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZengMWWW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the localization problem using noisy range-difference measurements, or equivalently time difference of arrival (TDOA) measurements. There is a reference sensor, and for each other sensor, the TDOA measurement is obtained with respect to the reference one. By minimizing the sum of squared errors, a nonconvex constrained least squares (CLS) problem is formulated. In this work, we focus on devising an algorithm to seek the global minimizer of the CLS problem, hoping that the numerical solution meets some precision requirement in terms of relative error. Based on the Lagrange multiplier method, we first branch the feasible Lagrange multiplier set into several subsets and develop a workflow in terms of if-then-else control structure to seek the global minimizer by searching for the optimal Lagrange multiplier. The execution order is carefully organized so that it is in line with the general principle of putting the flow that one normally understands to be executed first. We then dive into detailed searching methods in different cases and conduct computational error analysis, giving the error bound on the Lagrange multiplier, when we search for it, to meet the precision requirement on an approximate solution. Based on the above achievements, a programmable global minimizer seeking algorithm is proposed for the CLS problem. Simulations and experimental tests on a public dataset demonstrate the effectiveness of the proposed algorithm.}
}


@article{DBLP:journals/ton/LiuFCCH22,
	author = {Yunshu Liu and
                  Zhixuan Fang and
                  Man Hon Cheung and
                  Wei Cai and
                  Jianwei Huang},
	title = {An Incentive Mechanism for Sustainable Blockchain Storage},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2131--2144},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3166459},
	doi = {10.1109/TNET.2022.3166459},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuFCCH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Miners in a blockchain system are suffering from ever-increasing storage costs, which in general have not been properly compensated by the users’ transaction fees. This reduces the incentives for the miners’ participation and may jeopardize the blockchain security. To mitigate this blockchain insufficient fee issue, we propose a Fee and Waiting Tax (FWT) mechanism, which explicitly considers the two types of negative externalities in the system. Specifically, we model the interactions between the protocol designer, users, and miners as a three-stage Stackelberg game. By characterizing the equilibrium of the game, we find that miners neglecting the negative externality in transaction selection cause they are willing to accept insufficient-fee transactions. This leads to the insufficient storage fee issue in the existing protocol (i.e., deployed in Bitcoin and Ethereum). Moreover, our proposed optimal FWT mechanism can motivate users to pay sufficient transaction fees to cover the storage costs and achieve the unconstrained social optimum. Numerical results show that the optimal FWT mechanism guarantees sufficient transaction fees and achieves an average social welfare improvement of 51.43% or more over the existing protocol. Furthermore, the optimal FWT mechanism reduces the average waiting time of low-fee transactions and all transactions by 68.49% and 61.56%, respectively.}
}


@article{DBLP:journals/ton/YangYBYZV22,
	author = {Hui Yang and
                  Qiuyan Yao and
                  Bowen Bao and
                  Ao Yu and
                  Jie Zhang and
                  Athanasios V. Vasilakos},
	title = {Multi-Associated Parameters Aggregation-Based Routing and Resources
                  Allocation in Multi-Core Elastic Optical Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2145--2157},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3164869},
	doi = {10.1109/TNET.2022.3164869},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangYBYZV22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Space division multiplexing (SDM), as a potential means of enhancing the capacity of optical transmission systems, has attracted widespread attention. However, the adoption of SDM technology has also additionally increased resource dimensions, introduced complex crosstalk, and made it difficult to integrate multi-dimensional fragments. These factors force the transmission constraints to be more complicated. Especially, some factors have a mutual restraint relationship, and excessive consideration of certain factors will cause the deterioration of other ones. Therefore, how to comprehensively consider the associated factors to achieve trade-offs and improve network performance is a problem worthy of study. This paper exploits the advantages of self-organizing feature mapping (SOFM) model to process multi-dimensional data with relevant features. Firstly, multiple constraints will be input into SOFM as mode vectors from the core level. Then, by judging the similarity between the competition layer neuron and the pattern vector, the position of the winning neuron is located, which determines the transmission level of each core. Finally, a routing, core, and spectrum allocation scheme is proposed by preferentially locating the core with higher transmission quality. Along the selected core, the available slots will be classified twice respectively by the number of adjacent cores and crosstalk direction to quickly find the spectrum blocks with relatively small crosstalk. Results indicate the scheme can reduce blocking probability and the resource fragmentation. Further, it can increase the resource utilization within tested network load.}
}


@article{DBLP:journals/ton/XieCLSGXY22,
	author = {Renjie Xie and
                  Jiahao Cao and
                  Qi Li and
                  Kun Sun and
                  Guofei Gu and
                  Mingwei Xu and
                  Yuan Yang},
	title = {Disrupting the {SDN} Control Channel via Shared Links: Attacks and
                  Countermeasures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2158--2172},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3169136},
	doi = {10.1109/TNET.2022.3169136},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XieCLSGXY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN). SDN enables network innovations with a centralized controller controlling the whole network through the control channel. Because the control channel delivers all network control traffic, its security and reliability are of great importance. For the first time in the literature, we propose the CrossPath attack that disrupts the SDN control channel by exploiting the shared links in paths of control traffic and data traffic. In this attack, crafted data traffic can implicitly disrupt the forwarding of control traffic in the shared links. As the data traffic does not enter the control channel, the attack is stealthy and cannot be easily perceived by the controller. In order to identify the target paths containing the shared links to attack, we develop a novel technique called adversarial path reconnaissance. Our experimental results show its feasibility and efficiency of identifying the target path. We systematically study the impacts of the attack on various network applications in a real SDN testbed. Experiments show the attack significantly degrades the performance of existing network applications and causes serious network anomalies, e.g., routing blackhole, flow table resetting, and even network-wide DoS. To defeat the CrossPath attack, we design a lightweight defense system named CrossGuard. Experiments demonstrate that it can effectively protect the control channel and quickly locate the attack flow with 98% accuracy while introducing a small overhead.}
}


@article{DBLP:journals/ton/Dai22,
	author = {Lin Dai},
	title = {A Theoretical Framework for Random Access: Stability Regions and Transmission
                  Control},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2173--2200},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3164458},
	doi = {10.1109/TNET.2022.3164458},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Dai22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As one of the two fundamental types of multiple access, random access has been widely adopted in various communication networks, and expected to play an increasingly central role owing to the rising popularity of Machine-to-Machine (M2M) communications. Despite decades of successful applications, the theory of random access has long been underdeveloped, with key fundamental issues unresolved. Among them, stability of random-access networks is the most long-standing one that has received continuous attention for almost half a century. The challenge lies in establishing an analytical framework where the coupled service processes of nodes’ queues can be characterized. In this paper, by extending our previously proposed analytical framework from the symmetric scenario to the general one, we tackle three open questions: 1) How to characterize the coupled service rates of nodes? 2) How to determine the stability region of input rates, only within which the network can be stabilized? 3) For given input rates within the stability region, how to tune the transmission probabilities of nodes to stabilize the network? We demonstrate that the key to characterizing the coupled service rates lies in properly establishing and solving the fixed-point equations of steady-state probabilities of successful transmission of Head-of-Line (HOL) packets of nodes. For the stability region of input rates, which closely depends on the definition of stability, two types of stability, i.e., queue-stability and throughput-stability, are considered, and both stability regions are shown to be determined by the sufficient and necessary condition of the existence of positive real roots of the fixed-point equations. To characterize the operating regions of transmission probabilities, constraints need to be further developed to ensure that the network operates at the specific steady-state point. The analysis shows that to stabilize the network, the transmission probabilities of nodes can be tuned only based on their long-term traffic input rates. Although the main results are illustrated based on Aloha with Constant Backoff, discussions on how to incorporate a general backoff function and other features of random access are also presented.}
}


@article{DBLP:journals/ton/LiuZZWCCS22,
	author = {Xuezheng Liu and
                  Zhicong Zhong and
                  Yipeng Zhou and
                  Di Wu and
                  Xu Chen and
                  Min Chen and
                  Quan Z. Sheng},
	title = {Accelerating Federated Learning via Parallel Servers: {A} Theoretically
                  Guaranteed Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2201--2215},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3168939},
	doi = {10.1109/TNET.2022.3168939},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuZZWCCS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the growth of participating clients, the centralized parameter server (PS) will seriously limit the scale and efficiency of Federated Learning (FL). A straightforward approach to scale up the FL system is to construct a Parallel FL (PFL) system with multiple parallel PSes. However, it is unclear whether PFL can really accelerate FL or reduce the training time of FL. Even if the answer is yes, it is non-trivial to design a highly efficient parameter average algorithm for a PFL system. In this paper, we propose a completely parallelizable FL algorithm called P-FedAvg under the PFL architecture. P-FedAvg extends the well-known FedAvg algorithm by allowing multiple PSes to cooperate and train a learning model together. In P-FedAvg, each PS is only responsible for a fraction of total clients, but PSes can mix model parameters in a dedicatedly designed way so that the FL model can well converge. Different from heuristic-based algorithms, P-FedAvg is with theoretical guarantees. To be rigorous, we theoretically analyze the convergence rate of P-FedAvg in terms of the number of conducted iterations, the communication cost of each global iteration and the optimal weights for each PS to mix parameters with its neighbors. Based on theoretical analysis, we conduct a case study on five typical overlay topolgoies formed by PSes to further examine the communication efficiency under different topologies, and investigate how the overlay topology affects the convergence rate, communication cost and robustness of a PFL system. Lastly, we perform extensive experiments with real datasets to verify our analysis and demonstrate that P-FedAvg can significantly speed up FL than traditional FedAvg and other competitive baselines. We believe that our work can help to lay a theoretical foundation for building more efficient PFL systems.}
}


@article{DBLP:journals/ton/TabatabaeeB22,
	author = {Seyed Mohammadhossein Tabatabaee and
                  Jean{-}Yves Le Boudec},
	title = {Deficit Round-Robin: {A} Second Network Calculus Analysis},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2216--2230},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3164772},
	doi = {10.1109/TNET.2022.3164772},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TabatabaeeB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deficit Round-Robin (DRR) is a widespread scheduling algorithm that provides fair queueing with variable-length packets. Bounds on worst-case delays for DRR were found by Boyer et al., who used a rigorous network calculus approach and characterized the service obtained by one flow of interest by means of a convex strict service curve. These bounds do not make any assumptions on the interfering traffic hence are pessimistic when the interfering traffic is constrained by some arrival curves. For such cases, two improvements were proposed. The former, by Soni et al., uses a correction term derived from a semi-rigorous heuristic; unfortunately, these bounds are incorrect, as we show by exhibiting a counter-example. The latter, by Bouillard, rigorously derive convex strict service curves for DRR that account for the arrival curve constraints of the interfering traffic. In this paper, we improve on these results in two ways. First, we derive a non-convex strict service curve for DRR that improves on Boyer et al. when there is no arrival constraint on the interfering traffic. Second, we provide an iterative method to improve any strict service curve (including Bouillard’s) when there are arrival constraints for the interfering traffic. As of today, our results provide the best-known worst-case delay bounds for DRR. They are obtained by using the method of the pseudo-inverse.}
}


@article{DBLP:journals/ton/LiZYCL22,
	author = {Feng Li and
                  Jichao Zhao and
                  Dongxiao Yu and
                  Xiuzhen Cheng and
                  Weifeng Lv},
	title = {Harnessing Context for Budget-Limited Crowdsensing With Massive Uncertain
                  Workers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2231--2245},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3169180},
	doi = {10.1109/TNET.2022.3169180},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZYCL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Crowdsensing is an emerging paradigm of ubiquitous sensing, through which a crowd of workers are recruited to perform sensing tasks collaboratively. Although it has stimulated many applications, an open fundamental problem is how to select among a massive number of workers to perform a given sensing task under a limited budget. Nevertheless, due to the proliferation of smart devices equipped with various sensors, it is very difficult to profile the workers in terms of sensing ability. Although the uncertainties of the workers can be addressed by conventional Combinatorial Multi-Armed Bandit (CMAB) framework through a trade-off between exploration and exploitation, we do not have sufficient allowance to directly explore and exploit the workers under the limited budget. Furthermore, since the sensor devices usually have quite limited resources, the workers may have bounded capabilities to perform the sensing task only few times, which further restricts our opportunities to learn the uncertainty. To address the above issues, we propose a Context-Aware Worker Selection (CAWS) algorithm in this paper. By leveraging the correlation between the context information of the workers and their sensing abilities, CAWS aims at maximizing the expected cumulative sensing revenue efficiently with both budget constraint and capacity constraints respected, even when the number of the uncertain workers is massive. The efficacy of CAWS can be verified by rigorous theoretical analysis and extensive experiments.}
}


@article{DBLP:journals/ton/XuXZLQH22,
	author = {Hongli Xu and
                  Peng Xi and
                  Gongming Zhao and
                  Jianchun Liu and
                  Chen Qian and
                  Liusheng Huang},
	title = {{SAFE-ME:} Scalable and Flexible Policy Enforcement in Middlebox Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2246--2261},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3167169},
	doi = {10.1109/TNET.2022.3167169},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuXZLQH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The past decades have seen a proliferation of middlebox deployment in various scenarios, including backbone networks and cloud networks. Since flows have to traverse specific service function chains (SFCs) for security and performance enhancement, it becomes much complex for SFC routing due to routing loops, traffic dynamics and scalability requirement. The existing SFC routing solutions may consume many resources (e.g., TCAM) on the data plane and lead to massive overhead on the control plane, which decrease the scalability of middlebox networks. Due to SFC requirement and potential routing loops, solutions like traditional default paths (e.g., using ECMP) that are widely used in non-middlebox networks will no longer be feasible. In this paper, we present and implement a scalable and flexible middlebox policy enforcement (SAFE-ME) system to minimize the TCAM usage and control overhead. To this end, we design the smart tag operations for construction of default SFC paths with less TCAM rules in the data plane, and present lightweight SFC routing update with less control overhead for dealing with traffic dynamics in the control plane. We implement our solution and evaluate its performance with experiments on both physical platform (Pica8) and Programming Protocol-independent Packet Processors (P4) based data plane, as well as large-scale simulations. Both experimental and simulation results show that SAFE-ME can greatly improve scalability (e.g., TCAM cost, update delay, and control overhead) in middlebox networks, especially for large-scale clouds. For example, our system can reduce the control traffic overhead by about 85% while achieving almost the similar middlebox load, compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/ChenLHY22,
	author = {Lin Chen and
                  Shan Lin and
                  Hua Huang and
                  Weihua Yang},
	title = {Charging Path Optimization in Mobile Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2262--2273},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3167781},
	doi = {10.1109/TNET.2022.3167781},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLHY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study a class of generic charging path optimization problems arising from emerging networking applications, where mobile chargers are dispatched to deliver energy to mobile agents (e.g., robots, drones, vehicles), which have specified tasks and mobility patterns. We instantiate our work by focusing on finding the charging path maximizing the number of nodes charged within a fixed time horizon. We show that this problem is APX-hard. By recursively decomposing the problem into sub-problems of searching sub-paths, we design quasi-polynomial-time algorithms achieving logarithmic approximation to the optimum charging path. Our approximation algorithms can be further adapted and extended to solve a variety of charging path optimization and scheduling problems with realistic constraints, such as limited time and energy budget.}
}


@article{DBLP:journals/ton/ZhangCWZZJ22,
	author = {Lei Zhang and
                  Yong Cui and
                  Mowei Wang and
                  Kewei Zhu and
                  Yibo Zhu and
                  Yong Jiang},
	title = {DeepCC: Bridging the Gap Between Congestion Control and Applications
                  via Multiobjective Optimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2274--2288},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3167713},
	doi = {10.1109/TNET.2022.3167713},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangCWZZJ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasingly complicated and diverse applications have distinct network performance demands, e.g., some desire high throughput while others require low latency. Traditional congestion controls (CC) have no perception of these demands. Consequently, literatures have explored the objective-specific algorithms, which are based on either offline training or online learning, to adapt to certain application demands. However, once generated, such algorithms are tailored to a specific performance objective function. Newly emerged performance demands in a changeable network environment require either expensive retraining (in the case of offline training), or manually redesigning a new objective function (in the case of online learning). To address this problem, we propose a novel architecture, DeepCC. It generates a CC agent that is generically applicable to a wide range of application requirements and network conditions. The key idea of DeepCC is to leverage both offline deep reinforcement learning and online fine-tuning. In the offline phase, instead of training towards a specific objective function, DeepCC trains its deep neural network model using multi-objective optimization. With the trained model, DeepCC offers near Pareto optimal policies w.r.t different user-specified trade-offs between throughput, delay, and loss rate without any redesigning or retraining. In addition, a quick online fine-tuning phase further helps DeepCC achieve the application-specific demands under dynamic network conditions. The simulation and real-world experiments show that DeepCC outperforms state-of-the-art schemes in a wide range of settings. DeepCC gains a higher target completion ratio of application requirements up to 67.4% than that of other schemes, even in an untrained environment.}
}


@article{DBLP:journals/ton/LiwangW22,
	author = {Minghui Liwang and
                  Xianbin Wang},
	title = {Overbooking-Empowered Computing Resource Provisioning in Cloud-Aided
                  Mobile Edge Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2289--2303},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3167396},
	doi = {10.1109/TNET.2022.3167396},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiwangW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conventional computing resource trading over mobile networks generally faces many challenges, e.g., excessive decision-making latency, undesired trading failures, and underutilization of dynamic resources, owing to the constraint of wireless networks. To improve resource utilization rate under dynamic network conditions, this paper introduces a novel computing resource provisioning mechanism empowered by overbooking, that allows the amount of booked resources to exceed the resource supply. Cloud-aided mobile edge networks are considered for the proposed framework, where an edge server can purchase more resources from a cloud server to offer computing services to multiple end-users with computation-intensive tasks. Specifically, the proposed mechanism relies on designing pre-signed forward trading contracts among edge and end-users, as well as between edge and cloud in advance to future practical trading; while encouraging an appropriate overbooking rate to improve resource utilization, via analyzing historical statistics associated with uncertainties such as dynamic resource supply/demand, and varying channel qualities. The contract design is formulated as a multi-objective optimization problem that aims to maximize the expected utilities of end-users, edge, and cloud, via evaluating potential risks; for which a two-phase multilateral negotiation scheme is proposed that facilitates the bargaining procedure among the three parties, to reach the final trading consensus (namely, contract terms). Experimental results demonstrate that the proposed mechanism achieves mutually beneficial utilities of three parties, while outperforming baseline methods on significant indicators such as task completion, trading failure, time efficiency, resource usage, etc., from various analytical angles.}
}


@article{DBLP:journals/ton/ShiCSLZ22,
	author = {Lei Shi and
                  Yuhua Cheng and
                  Jinliang Shao and
                  Qingchen Liu and
                  Wei Xing Zheng},
	title = {Locating Link Failures in WSNs via Cluster Consensus and Graph Decomposition},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2304--2314},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3171272},
	doi = {10.1109/TNET.2022.3171272},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiCSLZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularization of network equipment and the rapid development of information technology, the scale and complexity of wireless sensor networks (WSNs) continue to expand. How to effectively locate link failures has become a challenging problem in WSNs. In this paper, we propose a novel method of locating link failures based on distributed cluster consensus protocol and graph decomposition technique. In our method, the initial data is injected into sensor nodes for distributed interactions, and then link failures can be located by observing and comparing the output data of the nodes. The proposed method is suitable for the situations with both single-link failure and multi-link failures, and has no limitations on the number, distribution and correlation of link failures. Necessary and sufficient conditions are provided to guarantee the accuracy of the proposed method in locating link failures. At last, the effectiveness of the proposed method is verified by both real and simulation experiments.}
}


@article{DBLP:journals/ton/AbolhassaniTEY22,
	author = {Bahman Abolhassani and
                  John Tadrous and
                  Atilla Eryilmaz and
                  Edmund Yeh},
	title = {Fresh Caching of Dynamic Content Over the Wireless Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2315--2327},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3170245},
	doi = {10.1109/TNET.2022.3170245},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AbolhassaniTEY22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a framework and provably-efficient schemes for ‘fresh’ caching at the (front-end) local cache of content that is subject to ‘dynamic’ updates at the (back-end) database. We start by formulating the hard-cache-constrained problem for this setting, which quickly becomes intractable due to the limited cache. To bypass this challenge, we first propose a flexible time-based-eviction model to derive the average system cost function that measures the system’s cost due to the service of aging content in addition to the regular cache miss cost. Next, we solve the cache-unconstrained case, which reveals how the refresh dynamics and popularity of content affect optimal caching. Then, we extend our approach to a soft-cache-constrained version, where we can guarantee that the cache use is limited with arbitrarily high probability. The corresponding solution reveals the interesting insight that ‘whether to cache an item or not in the local cache?’ depends primarily on its popularity level and channel reliability, whereas ‘how long the cached item should be held in the cache before eviction?’ depends primarily on its refresh rate. Moreover, we investigate the cost-cache saving trade-offs and prove that substantial cache gains can be obtained while also asymptotically achieving the minimum cost as the database size grows.}
}


@article{DBLP:journals/ton/AnLYX22,
	author = {Zhenlin An and
                  Qiongzheng Lin and
                  Lei Yang and
                  Lei Xie},
	title = {Tagcaster: Activating Wireless Voice of Electronic Toll Collection
                  Systems With Zero Start-Up Cost},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2328--2342},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3169914},
	doi = {10.1109/TNET.2022.3169914},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AnLYX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work enhances the machine-to-human communication between electronic toll collection (ETC) systems and drivers by providing an AM broadcast service to deployed ETC systems. This study is the first to show that ultra-high radio frequency identification signals can be received by an AM radio receiver due to the presence of the nonlinearity effect in the AM receiver. Such a phenomenon allows the development of a previously infeasible cross-technology and cross-frequency communication, called Tagcaster, which converts an ETC reader to an AM station for broadcasting short messages (e.g., charged-fees and traffic forecast) to drivers at tollbooths. The key innovation in this work is the engineering of Tagcaster over off-the-shelf ETC systems using shadow carrier and baseband whitening without the need for hardware nor firmware changes. This feature allows zero-cost rapid deployment in the existing ETC infrastructure. Two prototypes of Tagcaster are designed, implemented, and evaluated over four general and five vehicle-mounted AM receivers (e.g., Toyota, Audi, and Jetta). Experiments reveal that Tagcaster can provide good-quality (PESQ>2) and stable AM broadcasting service with a 30 m coverage range. Tagcaster remarkably improves user experience at ETC stations, and two-thirds of volunteer drivers rate it with a score of 4+ out of 5.}
}


@article{DBLP:journals/ton/ChangVHM22,
	author = {Hyunseok Chang and
                  Matteo Varvello and
                  Fang Hao and
                  Sarit Mukherjee},
	title = {A Tale of Three Videoconferencing Applications: Zoom, Webex, and Meet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2343--2358},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3171467},
	doi = {10.1109/TNET.2022.3171467},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChangVHM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the outbreak of the COVID-19 pandemic, videoconferencing has become the default mode of communication in our daily lives at homes, workplaces and schools, and it is likely to remain an important part of our lives in the post-pandemic world. Despite its significance, there has not been any systematic study characterizing the user-perceived performance of existing videoconferencing systems other than anecdotal reports. In this paper, we present a detailed measurement study that compares three major videoconferencing systems: Zoom, Webex and Google Meet. Our study is based on 62 hours’ worth of more than 1.1K videoconferencing sessions, which were created with a mix of emulated videoconferencing clients deployed in the cloud, as well as real mobile devices running from a residential network over two separate periods with nine months apart. We find that the existing videoconferencing systems vary in terms of geographic scope and resource provisioning strategies, which in turns determine streaming lag experienced by users. We also observe that streaming rate can change under different conditions (e.g., available bandwidth, number of users in a session, mobile device status), which affects user-perceived streaming quality. Beyond these findings, our measurement methodology enables reproducible benchmark analysis for any types of comparative or longitudinal study on available videoconferencing systems.}
}


@article{DBLP:journals/ton/GraziaPHKC22,
	author = {Carlo Augusto Grazia and
                  Natale Patriciello and
                  Toke H{\o}iland{-}J{\o}rgensen and
                  Martin Klapez and
                  Maurizio Casoni},
	title = {Aggregating Without Bloating: Hard Times for {TCP} on Wi-Fi},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2359--2373},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3171594},
	doi = {10.1109/TNET.2022.3171594},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GraziaPHKC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Since the definition of the bufferbloat phenomenon, several Linux kernel modules have been introduced in its TCP/IP stack, and there is a lack of experimental studies on their effects when coupled with WLAN technologies, in particular, IEEE 802.11n and IEEE 802.11ac. One essential algorithm introduced is named TCP Small Queues (TSQ) and has the role of limiting the number of packets that a TCP socket can enqueue in the stack, waiting for the physical layer to send the packets before enqueueing extra data. A second significant TCP algorithm is named TCP Pacing (TP) and regulates the pace used by the socket to enqueue packets in the stack, regulating the formation of bursts of data. These mechanisms affect the frame aggregation logic on WLAN networks and compromise the throughput-latency tradeoff of all the TCP variants. This paper presents an experimental evaluation of these techniques investigating the wireless network performance of several TCP congestion control variants under the presence of different TSQ and TP policies, modeling also their interaction.}
}


@article{DBLP:journals/ton/LiuHLLWH22,
	author = {Jingling Liu and
                  Jiawei Huang and
                  Zhaoyi Li and
                  Yijun Li and
                  Jianxin Wang and
                  Tian He},
	title = {Achieving Per-Flow Fairness and High Utilization With Limited Priority
                  Queues in Data Center},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {5},
	pages = {2374--2387},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3172749},
	doi = {10.1109/TNET.2022.3172749},
	timestamp = {Sun, 13 Nov 2022 17:53:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuHLLWH22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern data centers often host multiple applications with diverse network demands. To provide fair bandwidth allocation to several thousand traversing flows, Approximate Fair Queueing (AFQ) utilizes multiple priority queues in switch to approximate ideal fair queueing. However, due to limited number of queues in programmable switches, AFQ easily experiences high packet loss and low link utilization. In this paper, we propose Elastic Fair Queueing (EFQ), which leverages limited priority queues to flexibly achieve both high network utilization and fair bandwidth allocation. EFQ dynamically assigns the free buffer space in priority queues for each packet to obtain high utilization without sacrificing flow-level fairness. The results of simulation experiments and real implementations show that EFQ reduces the average flow completion time by up to 82% over the state-of-the-art fair bandwidth allocation mechanisms.}
}


@article{DBLP:journals/ton/HanTWHL22,
	author = {Zhenhua Han and
                  Haisheng Tan and
                  Rui Wang and
                  Yuncong Hong and
                  Francis C. M. Lau},
	title = {Efficient Online Learning Based Cross-Tier Uplink Scheduling in HetNets},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2389--2402},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3173432},
	doi = {10.1109/TNET.2022.3173432},
	timestamp = {Sun, 12 Feb 2023 18:48:54 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HanTWHL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Heterogeneous cellular networks (HetNets), where low-power low-complexity base stations (Pico-BSs) are deployed inside the coverage of macro base stations (Macro-BSs), can significantly improve the spectrum efficiency by Pico- and Macro base station collaboration. Due to cross-tier interference, joint detection of uplink signals is widely adopted so that Pico-BS can either detect the uplink signals locally or forward them to Macro-BS for processing. The latter can achieve increased throughput at the cost of additional backhaul transmission. In this paper, we study the delay-optimal uplink scheduling problem in HetNets with limited backhaul capacity. Local signal detection or joint signal detection is scheduled in a unified delay-optimal framework. Specifically, we first prove that the problem is NP-hard and then formulate it as a Markov Decision Process. We propose an efficient algorithm, called OLIUS , that can deal with the exponentially growing state and action space. Furthermore, OLIUS is online learning-based which does not require any prior knowledge on user behavior or channel characteristics. We prove the convergence of OLIUS and derive an upper bound on its approximation error. Extensive experiments in various scenarios show our algorithm outperforms existing methods in reducing delay and power consumption.}
}


@article{DBLP:journals/ton/BankhamerES22,
	author = {Gregor Bankhamer and
                  Robert Els{\"{a}}sser and
                  Stefan Schmid},
	title = {Local Fast Rerouting With Low Congestion: {A} Randomized Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2403--2418},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174731},
	doi = {10.1109/TNET.2022.3174731},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BankhamerES22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most modern communication networks include fast rerouting mechanisms, implemented entirely in the data plane, to quickly recover connectivity after link failures. By relying on local failure information only, these data plane mechanisms provide very fast reaction times, but at the same time introduce an algorithmic challenge in case of multiple link failures: failover routes need to be robust to additional but locally unknown failures downstream. This paper presents local fast rerouting algorithms which not only provide a high degree of resilience against multiple link failures, but also ensure a low congestion on the resulting failover paths. We consider a randomized approach and focus on networks which are highly connected before the failures occur. Our main contributions are three simple algorithms which come with provable guarantees and provide interesting resilience-load tradeoffs, significantly outperforming any deterministic fast rerouting algorithm with high probability.}
}


@article{DBLP:journals/ton/AvinMS22a,
	author = {Chen Avin and
                  Kaushik Mondal and
                  Stefan Schmid},
	title = {Push-Down Trees: Optimal Self-Adjusting Complete Trees},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2419--2432},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174118},
	doi = {10.1109/TNET.2022.3174118},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AvinMS22a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies a fundamental algorithmic problem related to the design of demand-aware networks: networks whose topologies adjust toward the traffic patterns they serve, in an online manner. The goal is to strike a tradeoff between the benefits of such adjustments (shorter routes) and their costs (reconfigurations). In particular, we consider the problem of designing a self-adjusting tree network which serves single-source, multi-destination communication. The problem is a central building block for more general self-adjusting network designs and has interesting connections to self-adjusting datastructures. We present two constant-competitive online algorithms for this problem, one randomized and one deterministic. Our approach is based on a natural notion of Most Recently Used (MRU) tree, maintaining a working set. We prove that the working set is a cost lower bound for any online algorithm, and then present a randomized algorithm RANDOM- PUSH which approximates such an MRU tree at low cost, by pushing less recently used communication partners down the tree, along a random walk. Our deterministic algorithm Move-Half does not directly maintain an MRU tree, but its cost is still proportional to the cost of an MRU tree, and also matches the working set lower bound.}
}


@article{DBLP:journals/ton/XieNHM22,
	author = {Tian Xie and
                  Namitha Nambiar and
                  Ting He and
                  Patrick D. McDaniel},
	title = {Attack Resilience of Cache Replacement Policies: {A} Study Based on
                  {TTL} Approximation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2433--2447},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3171720},
	doi = {10.1109/TNET.2022.3171720},
	timestamp = {Mon, 28 Aug 2023 21:30:48 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieNHM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Caches are pervasively used in communication networks to speed up content access by reusing previous communications, where various replacement policies are used to manage the cached contents. The replacement policy of a cache plays a key role in its performance, and is thus extensively engineered to achieve a high hit ratio in benign environments. However, some studies showed that a policy with a higher hit ratio in benign environments may be more vulnerable to cache pollution attacks that intentionally send requests for unpopular contents. To understand the cache performance under such attacks, we analyze a suite of representative replacement policies under the framework of TTL approximation in how well they preserve the hit ratios for legitimate users, while incorporating the delay for the cache to obtain a missing content. We further develop a scheme to adapt the cache replacement policy based on the perceived level of attack. Our analysis and validation on real traces show that although no single policy is resilient to all the attack strategies, suitably adapting the replacement policy can notably improve the attack resilience of the cache. Motivated by these results, we implement selected policies as well as policy adaptation in an open-source SDN switch to manage flow rule replacement, which is shown to notably improve its resilience to pollution attacks.}
}


@article{DBLP:journals/ton/BlocherWES22,
	author = {Marcel Bl{\"{o}}cher and
                  Lin Wang and
                  Patrick Eugster and
                  Max Schmidt},
	title = {Holistic Resource Scheduling for Data Center In-Network Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2448--2463},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174783},
	doi = {10.1109/TNET.2022.3174783},
	timestamp = {Thu, 27 Jul 2023 08:18:52 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BlocherWES22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The recent trend towards more programmable switching hardware in data centers opens up new possibilities for distributed applications to leverage in-network computing (INC). Literature so far has largely focused on individual application scenarios of INC, leaving aside the problem of coordinating usage of potentially scarce and heterogeneous switch resources among multiple INC scenarios, applications, and users. Alas, the traditional model of resource pools of isolated compute containers does not fit an INC-enabled data center. This paper describes HIRE, a holistic INC-aware resource manager which allows for server-local and INC resources to be coordinated in unison. HIRE introduces a novel flexible resource (meta-)model to address heterogeneity and resource interchangeability, and includes two approaches for INC scheduling: (a) retrofitting existing schedulers; (b) designing a new one. For (a), HIRE presents a retrofitting API and demonstrates it with four state-of-the-art schedulers. For (b), HIRE proposes a flow-based scheduler, cast as a min-cost max-flow problem, where a unified cost model is used to integrate the different costs. Experiments with a workload trace of a 4000 machine cluster show that HIRE makes better use of INC resources by serving 8–30% more INC requests, while simultaneously reducing network detours by 20% and reducing tail placement latency by 50%.}
}


@article{DBLP:journals/ton/JepsenFMFCS22,
	author = {Theo Jepsen and
                  Ali Fattaholmanan and
                  Masoud Moshref and
                  Nate Foster and
                  Antonio Carzaniga and
                  Robert Soul{\'{e}}},
	title = {Forwarding and Routing With Packet Subscriptions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2464--2479},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3172066},
	doi = {10.1109/TNET.2022.3172066},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JepsenFMFCS22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we explore how programmable data planes can naturally provide a higher-level of service to user applications via a new abstraction called packet subscriptions. Packet subscriptions generalize forwarding rules, and can be used to express both traditional routing and more esoteric, content-based approaches. We present strategies for routing with packet subscriptions in which a centralized controller has a global view of the network, and the network topology has a hierarchical or general structure. We also describe a compiler for packet subscriptions that uses a novel BDD-based algorithm to efficiently translate predicates into P4 tables that can support O(100K) expressions. Using our system, we have built eight diverse applications. We show that these applications can be deployed in brownfield networks while performing line-rate message processing, using the full switch bandwidth of 6.5Tbps.}
}


@article{DBLP:journals/ton/WangWMZ22,
	author = {Tianjiao Wang and
                  Zengfu Wang and
                  William Moran and
                  Moshe Zukerman},
	title = {Submarine Cable Network Design for Regional Connectivity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2480--2492},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3171832},
	doi = {10.1109/TNET.2022.3171832},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangWMZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper optimizes path planning for a trunk-and-branch topology network in an irregular 2-dimensional manifold embedded in 3-dimensional Euclidean space with application to submarine cable network planning. We go beyond our earlier focus on the weighted costs of cables (cable laying cost, resilience, design level and repair rate) to include the cost of branching units (BUs), including material and labor, as well as submarine cable landing stations (CLSs). This optimization also includes choices of locations of BUs and CLSs. These are important issues for the economics of cable laying and significantly change the model and the optimization process. We pose the problem as a variant of the Steiner tree problem, but one in which the Steiner nodes can vary in number, while incurring a penalty. We refer to it as the weighted Steiner node problem. It differs from the Euclidean Steiner tree problem, where Steiner points are forced to have degree three; this is no longer the case, in general, when nodes incur a cost. We are able to prove that our algorithm is applicable to Steiner nodes with degree greater than three, enabling optimization of network costs in this context. The optimal solution is achieved in polynomial-time using dynamic programming.}
}


@article{DBLP:journals/ton/ShaoCHG22,
	author = {Xiaozhe Shao and
                  Zibin Chen and
                  Daniel E. Holcomb and
                  Lixin Gao},
	title = {Accelerating {BGP} Configuration Verification Through Reducing Cycles
                  in {SMT} Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2493--2504},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3176267},
	doi = {10.1109/TNET.2022.3176267},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShaoCHG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network verification has been proposed to help network operators eliminate the outage or security issues caused by misconfigurations. Recent studies have proposed SMT-based approaches to verify network properties with respect to network configurations. These approaches translate the verification problem into a Satisfiability Module Theories (SMT) problem. Although these approaches are attractive because of their broad coverage, they can scale to moderate-size networks only. In this paper, we propose BiNode to accelerate the network verification process. The key idea is to formulate the SMT constraints in a manner that reduces/eliminates the cyclic dependencies between its variables. By doing so, we expedite the solving of the SMT problem. We implement and evaluate the performance of BiNode through an off-the-shelf SMT solver. The experimental results show that BiNode can reduce verification time by an order of magnitude through reducing/eliminating the cyclic dependencies among SMT variables.}
}


@article{DBLP:journals/ton/CaiLTM22,
	author = {Yang Cai and
                  Jaime Llorca and
                  Antonia M. Tulino and
                  Andreas F. Molisch},
	title = {Ultra-Reliable Distributed Cloud Network Control With End-to-End Latency
                  Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2505--2520},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3179349},
	doi = {10.1109/TNET.2022.3179349},
	timestamp = {Fri, 19 Apr 2024 13:46:48 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaiLTM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We are entering a rapidly unfolding future driven by the delivery of real-time computation services, such as industrial automation and augmented reality, collectively referred to as augmented information (AgI) services, over highly distributed cloud/edge computing networks. The interaction intensive nature of AgI services is accelerating the need for networking solutions that provide strict latency guarantees. In contrast to most existing studies that can only characterize average delay performance, we focus on the critical goal of delivering AgI services ahead of corresponding deadlines on a per-packet basis, while minimizing overall cloud network operational cost. To this end, we design a novel queuing system able to track data packets’ lifetime and formalize the delay-constrained least-cost dynamic network control problem . To address this challenging problem, we first study the setting with average capacity (or resource budget) constraints, for which we characterize the delay-constrained stability region and design a throughput-optimal control policy leveraging Lyapunov optimization theory on an equivalent virtual network. Guided by the same principle, we tackle the peak capacity constrained scenario by developing the reliable cloud network control (RCNC) algorithm, which employs a two-way optimization method to make actual and virtual network flow solutions converge in an iterative manner. Extensive numerical results show the superior performance of the proposed control policy compared with the state-of-the-art cloud network control algorithm, and the value of guaranteeing strict end-to-end deadlines for the delivery of next-generation AgI services.}
}


@article{DBLP:journals/ton/ChackochanSD22,
	author = {Reena Chackochan and
                  Albert Sunny and
                  Senthilkumar Dhanasekaran},
	title = {Approximate Aggregate Utility Maximization Using Greedy Maximal Scheduling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2521--2530},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3179451},
	doi = {10.1109/TNET.2022.3179451},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChackochanSD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of aggregate utility maximization in multihop wireless networks. Following standard approaches, we consider the dual of a convex optimization problem that can be decomposed into two sub-problems. One of the sub-problem is a scheduling problem that requires enumeration of all maximal independent sets in the network—a well-known NP-hard problem. To overcome this computational complexity, we resort to polynomial-time sub-optimal Greedy Maximal Scheduling (GMS). The majority of this paper is dedicated towards quantifying the sub-optimality that arises in the aggregate utility maximization problem due to GMS. We also provide some insights into factors affecting aggregate utility maximization by providing bounds on the maximum achievable aggregate utility.}
}


@article{DBLP:journals/ton/BoroujenyM22,
	author = {Massieh Kordi Boroujeny and
                  Brian L. Mark},
	title = {Design of a Stochastic Traffic Regulator for End-to-End Network Delay
                  Guarantees},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2531--2543},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3181858},
	doi = {10.1109/TNET.2022.3181858},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BoroujenyM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Providing end-to-end network delay guarantees in packet-switched networks such as the Internet is highly desirable for mission-critical and delay-sensitive data transmission, yet it remains a challenging open problem. Since deterministic bounds are based on the worst-case traffic behavior, various frameworks for stochastic network calculus have been proposed to provide less conservative, probabilistic bounds on network delay, at least in theory. However, little attention has been devoted to the problem of regulating traffic according to stochastic burstiness bounds, which is necessary in order to guarantee the delay bounds in practice. We design and analyze a stochastic traffic regulator that can be used in conjunction with results from stochastic network calculus to provide probabilistic guarantees on end-to-end network delay. Two alternative implementations of the stochastic regulator are developed and compared. Numerical results are provided to demonstrate the performance of the proposed stochastic traffic regulator.}
}


@article{DBLP:journals/ton/VeitchMCB22,
	author = {Darryl Veitch and
                  Sathiya Kumaran Mani and
                  Yi Cao and
                  Paul Barford},
	title = {iHorology: Lowering the Barrier to Microsecond-Level Internet Time},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2544--2558},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174189},
	doi = {10.1109/TNET.2022.3174189},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VeitchMCB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {High accuracy, synchronized clocks are essential to a growing number of Internet applications. Standard protocols and their associated server infrastructure typically enable client clocks to synchronize to the order of tens of milliseconds. We address one of the key challenges to high precision Internet timekeeping – the intrinsic contribution to clock error of underlying path asymmetry between client and time server, a fundamental barrier to microsecond level accuracy. We first exploit results of a unique measurement study to reliably quantify asymmetry by taking routing changes into account for the first time, and then to infer the impacts on timing. We then describe three approaches to addressing the path asymmetry problem: LBBE, SBBE and K-SBBE, each based on timestamp exchange with multiple servers, with the goal of tightening bounds on asymmetry for each client. We explore their capabilities and limitations through simulation and model-based argument. We show that substantial improvements are possible, and discuss whether, and how, the goal of microsecond accuracy might be attained.}
}


@article{DBLP:journals/ton/HuangWC22,
	author = {Sijiang Huang and
                  Mowei Wang and
                  Yong Cui},
	title = {Traffic-Aware Buffer Management in Shared Memory Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2559--2573},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3173930},
	doi = {10.1109/TNET.2022.3173930},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangWC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Switch buffer serves an important role in the modern internet. To achieve efficiency, today’s switches often use on-chip shared memory. Shared memory switches rely on buffer management policies to allocate buffer among ports. To avoid waste of buffer resources or excessive buffer occupation by a few ports, existing policies tend to maximize overall buffer utilization and pursue queue length fairness. However, blind pursuit of utilization and misleading fairness definition based on queue length lead to buffer occupation with no benefit to throughput but extends queuing delay and undermines burst absorption of other ports. With analysis of current dynamic threshold policies, we demonstrate that meaningless buffer occupation can potentially impair the absorption capability of shared buffer, whereas none of the existing policies have addressed this problem. We contend that a buffer management policy should proactively detect port traffic and adjust buffer allocation accordingly. In this paper, we propose Traffic-aware Dynamic Threshold (TDT) policy. On the basis of the classic dynamic threshold policy, TDT proactively raises or lowers port threshold to absorb burst traffic or evacuate meaningless buffer occupation. We present detailed designs of port control state transition and state decision module that detect real-time traffic and change port thresholds accordingly. Simulation and DPDK-based real testbed demonstrate that TDT simultaneously optimizes for throughput, loss and delay, and reduces up to 50% flow completion time.}
}


@article{DBLP:journals/ton/TanWSTT22,
	author = {Qingfeng Tan and
                  Xuebin Wang and
                  Wei Shi and
                  Jian Tang and
                  Zhihong Tian},
	title = {An Anonymity Vulnerability in Tor},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2574--2587},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174003},
	doi = {10.1109/TNET.2022.3174003},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TanWSTT22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Privacy is currently one of the most concerned issues in Cyberspace. Tor is the most widely used system in the world for anonymously accessing Internet. However, Tor is known to be vulnerable to end-to-end traffic correlation attacks when an adversary is able to monitor traffic at both communication endpoints. In this paper, we present a set of novel Trapper Attacks that can be used to deanonymize user activities by both AS-level adversaries and Node-level adversaries in a Tor network. First, AS-level adversaries can exploit the occasional failures of censored network to selectively control entry guards of the Tor users. Second, the adversaries can exploit poor reliability of the Tor communication (e.g., natural churn) to compromise the exiting nodes and the anonymous path. Once the adversaries gain control of the routes, they can identify and inspect any traffic entering and leaving the Tor network, consequently, deanonymize a Tor user’s activity in the network. To demonstrate the effectiveness and feasibility of this attacks, we implemented a tool that can launch the proposed Trapper Attacks to automatic reveal communication relationships between a Tor user and its destinations running on a live Tor network. We also present a formal analysis framework to evaluate the integrity of the Tor network. With this framework, we successfully obtained quantitative estimates of Tor’s security vulnerability. The proposed Trapper Attacks are also designed to scale up in real-world Tor networks. Namely, it allows an adversary to perform deanonymization in honey relays effectively, and compromise the anonymity of Tor clients in real time. Our experimental results show that the proposed attacks succeed in less than 40 seconds achieving a 100% accuracy rate and a false positive rate close to 0.}
}


@article{DBLP:journals/ton/WangD22,
	author = {Xuehe Wang and
                  Lingjie Duan},
	title = {Dynamic Pricing and Mean Field Analysis for Controlling Age of Information},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2588--2600},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3174114},
	doi = {10.1109/TNET.2022.3174114},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangD22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today many mobile users in various zones are invited to sense and send back real-time useful information to keep the freshness of the content updates in such zones. However, due to the sampling cost in sensing and transmission, a user may not have the incentive to contribute real-time information to help reduce the age of information (AoI). We propose dynamic pricing for each zone to offer age-dependent monetary returns and encourage users to sample information at different rates over time. This dynamic pricing design problem needs to well balance the monetary payments as rewards to users and the AoI evolution, and is challenging to solve especially under the incomplete information about users’ arrivals and their private sampling costs. After formulating the problem as a nonlinear constrained dynamic program, to avoid the curse of dimensionality, we first propose to approximate the dynamic AoI reduction as a time-average term and successfully solve the approximate dynamic pricing in closed-form. Further, we extend the AoI control from a single zone to many zones with heterogeneous user arrival rates and initial ages, where each zone cares not only its own AoI dynamics but also the average AoI of all the zones in a mean field game system to provide a holistic service. Accordingly, we propose decentralized mean field pricing for each zone to self-operate by using a mean field term to estimate the average age dynamics of all the zones, which does not even require many zones to exchange their local data with each other.}
}


@article{DBLP:journals/ton/XuRLXZZXWL22,
	author = {Zichuan Xu and
                  Haozhe Ren and
                  Weifa Liang and
                  Qiufen Xia and
                  Wanlei Zhou and
                  Pan Zhou and
                  Wenzheng Xu and
                  Guowei Wu and
                  Mingchu Li},
	title = {Near Optimal Learning-Driven Mechanisms for Stable {NFV} Markets in
                  Multitier Cloud Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2601--2615},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3179295},
	doi = {10.1109/TNET.2022.3179295},
	timestamp = {Thu, 20 Jun 2024 15:06:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuRLXZZXWL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {More and more 5G and AI applications demand flexible and low-cost processing of their traffic through diverse virtualized network functions (VNFs) to meet their security and privacy requirements. As such, the Network Function Virtualization (NFV) market has been emerged as a major service market that allows network service providers to trade their network services among customers. Since each service market usually involves complex interplays among players with different roles, efficient mechanisms that guarantee stable and efficient operations of the NFV market are urgently needed. One fundamental problem in the NFV market is how to maximize the social welfare of all players so that all players have incentives to participate in the activities of the market. In this paper, we first formulate a novel social welfare maximization problem in an NFV market of a multi-tier edge cloud network, with the aim to maximize the total revenue collected from all players, and we implement VNF services on Virtual Machines (VMs) leased by service providers to fulfill customers with service requests, where the edge cloud network consists of both cloudlets in edge networks and remote data centers in the core network. We then design an efficient incentive-compatible mechanism for the problem, and analyze the existence of a Nash equilibrium of the mechanism. Also, we consider an online social welfare maximization problem with uncertain values of customers and without the knowledge of future request arrivals, for which we devise an online learning algorithm by adopting the Multi-Armed Bandits (MAB) method with a bounded regret. We finally evaluate the performance of the proposed mechanisms through simulations and a testbed. Results show that the proposed mechanisms deliver up to 27% higher social welfare than those of existing studies}
}


@article{DBLP:journals/ton/ChenZDZZF22,
	author = {Xianhao Chen and
                  Guangyu Zhu and
                  Haichuan Ding and
                  Lan Zhang and
                  Haixia Zhang and
                  Yuguang Fang},
	title = {End-to-End Service Auction: {A} General Double Auction Mechanism for
                  Edge Computing Services},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2616--2629},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3179239},
	doi = {10.1109/TNET.2022.3179239},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenZDZZF22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ubiquitous powerful personal computing facilities, such as desktop computers and parked autonomous cars, can function as micro edge computing servers by leveraging their spare resources. However, to harvest their resources for service provisioning, two significant challenges will arise: how to incentivize the server owners to contribute their computing resources, and how to guarantee the end-to-end (E2E) Quality-of-Service (QoS) for service buyers? In this paper, we address these two problems in a holistic way by advocating COMSA. Unlike the existing double auction schemes for edge computing which mostly focus on computing resource trading, COMSA addresses the joint problem of double auction mechanism design and network resource allocation by explicitly taking spectrum allocation and data routing into account, thereby providing E2E QoS guarantees for edge computing services. To handle the design complexity, COMSA employs a two-step procedure to decouple network optimization and mechanism design, which hence can be applied to general network optimization problems for edge computing. COMSA holds some critical economic properties, i.e., truthfulness, budget balance, and individual rationality. Our extensive simulation studies demonstrate the effectiveness of COMSA.}
}


@article{DBLP:journals/ton/DasalaJK22,
	author = {Keerthi Priya Dasala and
                  Josep Miquel Jornet and
                  Edward W. Knightly},
	title = {Scaling mmWave WLANs With Single {RF} Chain Multiuser Beamforming},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2630--2643},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3182976},
	doi = {10.1109/TNET.2022.3182976},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DasalaJK22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-user transmission in 60 GHz Wi-Fi can achieve data rates up to 100 Gbps by multiplexing multiple user data streams. However, a fundamental limit in the approach is that each RF chain is limited to supporting one stream or one user. In this paper, we scale multi-user 60 GHz WLAN data rate by overcoming this limit and propose SIngle RF chain Multi-user BeAmforming (SIMBA), a novel framework for multi-stream multi-user downlink transmission via a single RF chain. We build on single beamformed transmission via overlayed constellations to multiplex multiple users’ modulated symbols such that grouped users at different locations can share the same transmit beam from the AP. For this, we introduce user grouping and beam selection policies that span tradeoffs in data rate, training, and computation overhead. We implement a programmable WLAN testbed using software-defined radios and commercial 60 GHz transceivers and collect over-the-air measurements for different indoor WLAN deployments using a 12-element phased antenna array as well as horn antennas with varying beamwidth. We show that in comparison to single-user transmissions, SIMBA achieves\n2×\nimprovement in aggregate rate and two-fold delay reduction for simultaneous transmission to four users.}
}


@article{DBLP:journals/ton/QiuZCZLW22,
	author = {Tie Qiu and
                  Lidi Zhang and
                  Ning Chen and
                  Songwei Zhang and
                  Wenyuan Liu and
                  Dapeng Oliver Wu},
	title = {Born This Way: {A} Self-Organizing Evolution Scheme With Motif for
                  Internet of Things Robustness},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2644--2657},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3178408},
	doi = {10.1109/TNET.2022.3178408},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiuZCZLW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The span of Internet of Things (IoT) is expanding owing to numerous applications being linked to massive devices. Subsequently, node failures frequently occur because of malicious attacks, battery exhaustion, or other malfunctions. A reliable and robust network topology can alleviate the cascading collapse caused by local node failures. Existing optimization methods for fixed topologies enhance the robustness of the IoT topology by reconstructing the connections among the devices. However, the application of existing algorithms requires global topology optimizations or local adjustments when new nodes are added, which leads to high computational complexity. To address this problem, based on neuroevolution and network motifs, this study proposes an evolutionary algorithm to generate a robust IoT topology called “Born This Way: a self-organizing evolution scheme with Motif” (BTW-Motif). Using novel mutation and crossover operators, BTW-Motif generates an IoT topology with intrinsic robustness when new nodes are added. We design an adaptive edge density control mechanism to avoid an increase in energy consumption resulting from redundant connections. Specifically, BTW-Motif innovatively introduces network motif as a guide structure which has been proven to have a positive effect on the network robustness. Experiments indicate that BTW-Motif can effectively produce a robust topology. With different network sizes and edge densities, BTW-Motif can generate more robust topologies compared with the existing topology optimization algorithms. And the time consumption for the large-scale topology to achieve similar robustness is reduced by 50%.}
}


@article{DBLP:journals/ton/YaoDFTC22,
	author = {Zhiyuan Yao and
                  Yoann Desmouceaux and
                  Juan Antonio Cordero Fuertes and
                  Mark Townsley and
                  Thomas H. Clausen},
	title = {{HLB:} Toward Load-Aware Load Balancing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2658--2673},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3177163},
	doi = {10.1109/TNET.2022.3177163},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YaoDFTC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The purpose of network load balancers is to optimize quality of service to the users of a set of servers– basically, to improve response times and to reducing computing resources– by properly distributing workloads. This paper proposes a distributed, application-agnostic, Hybrid Load Balancer (HLB) that– without explicit monitoring or signaling– infers server occupancies and processing speeds, which allows making optimised workload placement decisions. This approach is evaluated both through simulations and extensive experiments, including synthetic workloads and Wikipedia replays on a real-world testbed. Results show significant performance gains, in terms of both response time and system utilisation, when compared to existing load-balancing algorithms.}
}


@article{DBLP:journals/ton/CohenES22,
	author = {Itamar Cohen and
                  Gil Einziger and
                  Gabriel Scalosub},
	title = {False Negative Awareness in Indicator-Based Caching Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2674--2687},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3177282},
	doi = {10.1109/TNET.2022.3177282},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CohenES22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed caching systems such as content distribution networks often advertise their content via lightweight approximate indicators (e.g., Bloom filters) to efficiently inform clients where each datum is likely cached. While false-positive indications are necessary and well understood, most existing works assume no false-negative indications. Our work illustrates practical scenarios where false-negatives are unavoidable and ignoring them significantly impacts system performance. Specifically, we focus on false-negatives induced by indicator staleness, which arises whenever the system advertises the indicator only periodically, rather than immediately reporting every change in the cache. Such scenarios naturally occur, e.g., in bandwidth-constraint environments or when latency impedes each client’s ability to obtain an updated indicator. Our work introduces novel false-negative aware access policies that continuously estimate the false-negative ratio and sometimes access caches despite negative indications. We present optimal policies for homogeneous settings and provide approximation guarantees for our algorithms in heterogeneous environments. We further perform an extensive simulation study with multiple real system traces. We show that our false-negative aware algorithms incur a significantly lower service cost than existing approaches or match the cost of these approaches while requiring an order of magnitude fewer resources (e.g., caching capacity or bandwidth).}
}


@article{DBLP:journals/ton/LiuDLLXX22,
	author = {Dianxiong Liu and
                  Zhiyong Du and
                  Xiaodu Liu and
                  Heyu Luan and
                  Yitao Xu and
                  Yifan Xu},
	title = {Task-Based Network Reconfiguration in Distributed {UAV} Swarms: {A}
                  Bilateral Matching Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2688--2700},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3181036},
	doi = {10.1109/TNET.2022.3181036},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuDLLXX22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study the problem of network reconfiguration when unmanned aerial vehicle (UAV) swarms suffer damage. Multiple UAVs are divided into several groups to perform various tasks. Each master UAV is connected to the ground control station and provides network services for small UAVs that perform various tasks, ensuring that the information of small UAVs can be transmitted back in a timely manner. When master UAVs are destroyed due to factors such as jamming or attacks, the associated small UAVs must select new master UAVs for network service and cooperate with other small UAVs to execute tasks. Based on the heterogeneity and relevance of tasks, we model and analyze the task relationship among different UAVs. Since both master UAVs and small UAVs have respective optimization objectives in the network reconfiguration process, we construct a many-to-one bilateral matching market to model the interaction between master UAVs and small UAVs. To realize an efficient solution for UAV network reconfiguration in complex environments, we propose a distributed matching algorithm and prove that the algorithm can converge to two-sided stable matching. Simulation results indicate that the proposed algorithm can significantly improve the task completion degree of the network compared with three other algorithms.}
}


@article{DBLP:journals/ton/ThomasMB22,
	author = {Ludovic Thomas and
                  Ahlem Mifdaoui and
                  Jean{-}Yves Le Boudec},
	title = {Worst-Case Delay Bounds in Time-Sensitive Networks With Packet Replication
                  and Elimination},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2701--2715},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3180763},
	doi = {10.1109/TNET.2022.3180763},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ThomasMB22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet replication and elimination functions are used by time-sensitive networks (as in the context of IEEE TSN and IETF DetNet) to increase the reliability of the network. Packets are replicated onto redundant paths by a replication function. Later the paths merge again and an elimination function removes the duplicates. This redundancy scheme has an effect on the timing behavior of time-sensitive networks and many challenges arise from conducting timing analyses. The replication can induce a burstiness increase along the paths of replicates, as well as packet mis-ordering that could increase the delays in the crossed bridges or routers. The induced packet mis-ordering could also negatively affect the interactions between the redundancy and scheduling mechanisms such as traffic regulators (as with per-flow regulators and interleaved regulators, implemented by TSN asynchronous traffic shaping). Using the network calculus framework, we provide a method of worst-case timing analysis for time-sensitive networks that implement redundancy mechanisms in the general use case, i.e., at end-devices and/or intermediate nodes. We first provide a network calculus toolbox for bounding the burstiness increase and the amount of reordering caused by the elimination function of duplicate packets. We then analyze the interactions with traffic regulators and show that their shaping-for-free property does not hold when placed after a packet elimination function. We provide a bound for the delay penalty when using per-flow regulators and prove that the penalty is not bounded with interleaved regulators. Finally, we use an industrial use-case to show the applicability and the benefits of our findings.}
}


@article{DBLP:journals/ton/ZhaoMMG22,
	author = {Zhiwei Zhao and
                  Wenliang Mao and
                  Geyong Min and
                  Weifeng Gao},
	title = {Joint Multichannel-Spatial Diversity for Efficient Opportunistic Routing
                  in Low-Power Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2716--2729},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3181581},
	doi = {10.1109/TNET.2022.3181581},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoMMG22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-power wireless networks (LPWNs) are of paramount importance for the pervasive deployment of Internet-of-Things (IoT). To deal with the lossy nature of LPWNs, opportunistic routing (OR) and multichannel communications (MC) have received significant research interests. In particular, coupling OR with MC has become an important way to further enhance the communication performance in LPWNs. However, as OR requires nodes in the same channel while MC separates nodes into different channels, the benefits of MC-OR combination are largely under-utilized. To address this problem, we investigate the challenging issue of establishing opportunistic routing in multichannel LPWNs. Different from the existing studies that separately assign channels and select forwarders, we propose a synergistic multichannel and opportunistic routing (SMOpp) approach, which jointly combines the benefits of both OR and MC by considering link correlation. SMOpp explicitly evaluates the routing opportunities of each channel/forwarder set and then employs a forwarder-initiated scheme to select the best combinations of senders, forwarders, and channels. The testbed evaluation shows that compared to the existing methods, SMOpp significantly improves the transmission efficiency for LPWNs.}
}


@article{DBLP:journals/ton/ZhangZLWW22,
	author = {Miao Zhang and
                  Yifei Zhu and
                  Jiangchuan Liu and
                  Feng Wang and
                  Fangxin Wang},
	title = {CharmSeeker: Automated Pipeline Configuration for Serverless Video
                  Processing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2730--2743},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3183231},
	doi = {10.1109/TNET.2022.3183231},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZLWW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Video processing plays an essential role in a wide range of cloud-based applications. It typically involves multiple pipelined stages, which well fits the latest fine-grained serverless computing paradigm if properly configured to match the cost and delay constraints of video. Existing configuration tools, however, are primarily developed for traditional virtual machine clusters with general workloads. This paper presents CharmSeeker, an automated configuration tuning tool for serverless video processing pipelines. We first carefully examine the key steps and the performance bottlenecks for video processing over modern serverless platforms. Then, we identify the configuration space for processing pipelines and leverage a carefully designed Sequential Bayesian Optimization search scheme to identify promising configurations. We further address the practical challenges toward integrating our solution into real-world systems and develop a prototype with AWS Lambda. Evaluation results show that CharmSeeker can find out the optimal or near-optimal configurations that improve the relative processing time up to 408.77%. It is also more robust and scalable to various video processing pipelines compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/FuZQKWC22,
	author = {Luoyi Fu and
                  Jiapeng Zhang and
                  Shan Qu and
                  Huquan Kang and
                  Xinbing Wang and
                  Guihai Chen},
	title = {Measuring Social Network De-Anonymizability by Means of Morphism Property},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2744--2759},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3180158},
	doi = {10.1109/TNET.2022.3180158},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FuZQKWC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anonymization techniques have tranquilized current social network users in terms of privacy leakage, however, it does not radically prevent adversaries from de-anonymizing users, as they may map the users to an un-anonymized network. Till now, researchers share a common thread in such de-anonymization attack: unveiling conditions leading to successful de-anonymization under the chosen network model. However, it has not yet been well understand how the structural property in different network models intrinsically determines de-anonymizability. We address the above issue in this paper by making the two contributions: (i) We discover that the automorphic degree and homomorphic degree of social networks determine their de-anonymizability universally. The automorphic degree characterizes the distinguishability of the users in a network, while the homomorphic degree models the similarities of users between two networks. We conclude that a smaller automorphic degree and a larger homomorphic degree conduce to a higher de-anonymizability. Such model-independent phenomenon refreshes us with a latitudinal study as it generalizes the essential commonness of de-anonymization in different network models. (ii) We derive explicit parametric bounds of the de-anonymizability for three classic network models, showing that such bounds correspond well to our conclusion about morphism property. We then algorithmically and experimentally show that such theoretical results literally make sense to adversaries. Such longitudinal study, including welding theory, algorithm and validation, promises applicability of our results on morphism property in real cases.}
}


@article{DBLP:journals/ton/XinLTYHW22,
	author = {Yao Xin and
                  Wenjun Li and
                  Guoming Tang and
                  Tong Yang and
                  Xiaohe Hu and
                  Yi Wang},
	title = {FPGA-Based Updatable Packet Classification Using TSS-Combined Bit-Selecting
                  Tree},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2760--2775},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3181295},
	doi = {10.1109/TNET.2022.3181295},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XinLTYHW22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {OpenFlow switches are being deployed in SDN to enable a wide spectrum of non-traditional applications. As a promising alternative to brutal force TCAMs, FPGA-based packet classification is being actively investigated. However, none of the existing FPGA designs can achieve high performance on both search and update for large-scale rule sets. To address this issue, we propose TcbTree, an FPGA-based algorithmic scheme for packet classification. Specifically, at the algorithmic side, i) a two-stage framework consisting of heterogeneous algorithms is proposed, where most rules can be mapped into several balanced trees without rule replications, ii) for the remaining few rules, a centralized TSS (Tuple Space Search) architecture together with a real-time feedback scheme is designed to enhance the efficiency of TSS search on FPGA, and iii) a tree dilution method is designed to equalize rule distribution in trees, so that the latency of tree search can be reduced. At the hardware side, i) an efficient data structure set is designed to convert tree traversal to addressing process, which breaks the constraints of limited tree depth and imbalanced node distribution, and ii) distinct from fully pipelined designs, multiple levels of parallelism are efficiently explored with multi-core, multi-search-engine and coarse-grained pipelines herein. Experimental results using ClassBench show that, with the implementation of TcbTree on FPGA, the average classification throughputs for 1k, 10k, 32k and 100k rule sets achieve 788.8 MPPS, 404.3 MPPS, 237 MPPS and 41.8 MPPS, respectively, and the update throughput for all benchmark rule sets is above 1 MUPS.}
}


@article{DBLP:journals/ton/ShakyaLC22,
	author = {Nishant Shakya and
                  Fan Li and
                  Jinyuan Chen},
	title = {On Distributed Computing With Heterogeneous Communication Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2776--2787},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3181234},
	doi = {10.1109/TNET.2022.3181234},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShakyaLC22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a distributed computing framework where the distributed nodes have different communication capabilities, motivated by the heterogeneous networks in data centers and mobile edge computing systems. Following the structure of MapReduce, this framework consists of Map computation phase, Shuffle phase, and Reduce computation phase. The Shuffle phase allows distributed nodes to exchange intermediate values, in the presence of heterogeneous communication bottlenecks for different nodes (heterogeneous communication load constraints). For this setting, we characterize the minimum total computation load and the minimum worst-case computation load in some cases, under the heterogeneous communication load constraints. While the total computation load depends on the sum of the computation loads of all the nodes, the worst-case computation load depends on the computation load of a node with the heaviest job. We show an interesting insight that, for some cases, there is a tradeoff between the minimum total computation load and the minimum worst-case computation load, in the sense that both cannot be achieved at the same time. The achievability schemes are proposed with careful design on the file assignment and the data shuffling. Beyond the cut-set bound, a novel converse is proposed using the proof by contradiction. For the general case, we identify two extreme regimes in which both the scheme with coding and the scheme without coding are optimal, respectively.}
}


@article{DBLP:journals/ton/FuM22,
	author = {Xinzhe Fu and
                  Eytan H. Modiano},
	title = {Learning-NUM: Network Utility Maximization With Unknown Utility Functions
                  and Queueing Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2788--2803},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3182890},
	doi = {10.1109/TNET.2022.3182890},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FuM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Utility Maximization (NUM) studies the problems of allocating traffic rates to network users in order to maximize the users’ total utility subject to network resource constraints. In this paper, we propose a new NUM framework, Learning-NUM, where the users’ utility functions are unknown apriori and the utility function values of the traffic rates can be observed only after the corresponding traffic is delivered to the destination, which means that the utility feedback experiences queueing delay. The goal is to design a policy that gradually learns the utility functions and makes rate allocation and network scheduling/routing decisions so as to maximize the total utility obtained over a finite time horizon\nT\n. In addition to unknown utility functions and stochastic constraints, a central challenge of our problem lies in the queueing delay of the observations, which may be unbounded and depends on the decisions of the policy. We first show that the expected total utility obtained by the best dynamic policy is upper bounded by the solution to a static optimization problem. Without the presence of feedback delay, we design an algorithm based on the ideas of gradient estimation and Max-Weight scheduling. To handle the feedback delay, we embed the algorithm in a parallel-instance paradigm to form a policy that achieves\nO\n~\n(\nT\n3/4\n)\n-regret, i.e., the difference between the expected utility obtained by the best dynamic policy and our policy is in\nO\n~\n(\nT\n3/4\n)\n. Furthermore, we extend our policy to deal with the case where the utility observations are noisy and show that it achieves\nO\n~\n(\nT\n7/8\n)\n-regret. Finally, to demonstrate the practical applicability of the Learning-NUM framework, we apply it to three application scenarios including database query, job scheduling and video streaming. We further conduct simulations on the job scheduling application to evaluate the empirical performance of our policy.}
}


@article{DBLP:journals/ton/ShenZ22,
	author = {Zhirong Shen and
                  Guanglin Zhang},
	title = {Competitive Online Stay-or-Switch Algorithms With Minimum Commitment
                  and Switching Cost},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2804--2817},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3183142},
	doi = {10.1109/TNET.2022.3183142},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShenZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we consider an online decision problem, where a decision maker has an option to buy a discount plan for his/her regular expenses. The discount plan costs an immediate upfront charge plus a commitment charge per time slot. Upon expiration, the discount period can be extended if the decision maker continues paying the commitment charge, or be canceled if he or she decides not to pay the commitment charge anymore. We investigate online algorithms for the decision maker to decide when to buy the discount plan and when to cancel it without the knowledge of his/her future expenses, aiming at minimizing the overall cost. The problem is an extension of the classic Bahncard Problem, which is applicable for a wide range of online decision scenarios. We propose a novel deterministic online algorithm which can achieve a closed-form competitive ratio upper bounded by 4. We further propose a randomized online algorithm with a smaller competitive ratio and two variants tailored for average-case inputs and time-varying parameters, respectively. Lastly, we evaluate our algorithms against state-of-the-art online benchmark algorithms in two real-world scenarios.}
}


@article{DBLP:journals/ton/MaWOCM22,
	author = {Chaoyi Ma and
                  Haibo Wang and
                  Olufemi O. Odegbile and
                  Shigang Chen and
                  Dimitrios Melissourgos},
	title = {Virtual Filter for Non-Duplicate Sampling With Network Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2818--2833},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3182694},
	doi = {10.1109/TNET.2022.3182694},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaWOCM22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sampling is key to handling mismatch between the line rate and the throughput of a network traffic measurement module. Flow-spread measurement requires non-duplicate sampling, which only samples the elements (carried in packet header or payload) in each flow when they appear for the first time and blocks them for subsequent appearances. The only prior work for non-duplicate sampling incurs considerable overhead, and has two practical limitations: It lacks a mechanism to set an appropriate sampling probability under dynamic traffic conditions, and it cannot efficiently handle multiple concurrent sampling tasks. This paper proposes a virtual filter design for non-duplicate sampling, which reduces the processing overhead by about half and reduces the memory overhead by an order of magnitude or more under some practical settings. It has a mechanism to automatically adapt its sampling probability to the traffic dynamics. It can be modified to handle sampling for multiple independent tasks with different probabilities. We also enhance the virtual filter for flow spread measurement and super spreader detection with a large measurement period.}
}


@article{DBLP:journals/ton/ShanHZLWL22,
	author = {Feng Shan and
                  Haodong Huo and
                  Jiaxin Zeng and
                  Zengbao Li and
                  Weiwei Wu and
                  Junzhou Luo},
	title = {Ultra-Wideband Swarm Ranging Protocol for Dynamic and Dense Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2834--2848},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3186071},
	doi = {10.1109/TNET.2022.3186071},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShanHZLWL22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, not only wearable and portable devices but also aerial and ground robots can be made smaller, lighter, cheaper, and thus as large as hundreds of them may form a swarm to participate in a complicated cooperative application, such as searching, rescuing, mapping, and war-battling. Devices and robots in such a swarm have three important features, namely, large number, high mobility and short distance, hence they form a dynamic and dense wireless network. Successful swarm cooperative applications require low latency communications and real-time localization. This paper proposes to use ultra-wideband (UWB) radio technology to implement both functionalities, because UWB is very time-sensitive that an accurate distance can be calculated using the transmission and reception timestamps of data messages. A UWB swarm ranging protocol is designed to achieve simultaneously wireless data communication and swarm ranging that allows a device/robot to compute the distances to all the peer neighbors at the same time. This protocol is designed for dynamic and dense networks, meanwhile it can also be used in various wireless networks and implemented on various types of devices/robots including low-end ones. In our experiment, this protocol is implemented on Crazyflies, STM32 microcontroller powered micro drones, with onboard UWB wireless transceiver chips DW1000. Extensive real-world experiments are conducted to verify the proposed protocol on various performance aspects, with a total of 9 Crazyflie drones in a compact area. The implemented swarm ranging protocol is open-sourced at https://github.com/SEU-NetSI/crazyflie-firmware}
}


@article{DBLP:journals/ton/ZhangZZZ22,
	author = {Liang Zhang and
                  Guangdeng Zong and
                  Xudong Zhao and
                  Ning Zhao},
	title = {Output Reachable Set Synthesis of Event-Triggered Control for Singular
                  Markov Jump Systems Under Multiple Cyber-Attacks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2849--2857},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3183862},
	doi = {10.1109/TNET.2022.3183862},
	timestamp = {Thu, 03 Aug 2023 08:59:45 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZZZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper investigates the asynchronous event-triggered state-feedback control for discrete-time nonlinear singular Markov jump systems subject to reachable set bounding and stochastic cyber-attacks. An event-triggered mechanism is introduced to regulate data transmission and save network resources. While this mechanism brings advantages, it also causes that the mode information of the system and the controller are not synchronized. On the other hand, network communication has brought convenience to signal transmission, and it has also resulted in the network channel being in an open and unreliable environment, which is vulnerable to cyber-attacks. In order to solve these problems, the secure asynchronous controller is designed to effectively resist the influence of attacks. In addition, by using the reachable set analysis approach, the constructed controller ensures that the estimation error is within the boundary of the ellipsoid, and the existence conditions of the required controller are given by solving the linear matrix inequalities. Finally, a simulation example is given to illustrate the effectiveness of the theoretical results.}
}


@article{DBLP:journals/ton/ChenTWPZ22,
	author = {Wei Chen and
                  Ye Tian and
                  Zhongxiang Wei and
                  Jiangyu Pan and
                  Xinming Zhang},
	title = {Task Scheduling for Probabilistic In -Band Network Telemetry},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {30},
	number = {6},
	pages = {2858--2869},
	year = {2022},
	url = {https://doi.org/10.1109/TNET.2022.3189370},
	doi = {10.1109/TNET.2022.3189370},
	timestamp = {Sun, 15 Jan 2023 18:31:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenTWPZ22.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-band Network Telemetry (INT) is a novel framework for monitoring network health in real-time, and its recent variant, Probabilistic INT (PINT), reduces its bandwidth consumption with a probabilistic approach. However, as we show in this paper, a PINT task can be successfully accomplished only when it is allocated a sufficient number of packets, and if there are many tasks executed in parallel, packets become a scarce resource. Meanwhile, today’s production network generally executes multiple measurement tasks for tracing different network states simultaneously. Therefore, in such a context, scheduling parallel PINT tasks on one single INT flow that has a limited number of packets becomes a critical problem. In this paper, we address this problem for the first time. We propose an algorithm that efficiently schedules multiple parallel PINT tasks on a flow by allocating the flow’s packets to the tasks and showing that the allocation is optimal. We realize the algorithm with a packet processing pipeline and implement it on software and hardware-programmable switches. Comprehensive evaluation on a FatTree testbed shows that at a low scheduling overhead, our algorithm can conduct parallel PINT tasks to detect various network faults in a timely and accurate manner. Additionally, the algorithm accomplishes more PINT tasks with higher quality than the alternative solutions.}
}
