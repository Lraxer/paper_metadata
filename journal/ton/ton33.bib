@article{DBLP:journals/ton/KindtC25,
	author = {Philipp H. Kindt and
                  Samarjit Chakraborty},
	title = {Performance Limits of Neighbor Discovery in Wireless Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {1--16},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3490542},
	doi = {10.1109/TNET.2024.3490542},
	timestamp = {Tue, 14 Oct 2025 19:49:17 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KindtC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Neighbor Discovery (ND) is the procedure employed by wireless devices to establish a first contact. All ND protocols involve devices sending beacons, and also listening for them. Protocols differ in terms of how the beacon transmissions and reception windows are scheduled, and the device sleeps in between consecutive transmissions and reception windows in order to save energy. A successful discovery constitutes a sending device’s beacon coinciding with a receiving device’s reception window. The goal of all ND protocols is to minimize the discovery latency. In spite of the ubiquity of ND protocols and active research on this topic for over two decades, the basic question “Given a power budget, what is the minimum guaranteed ND latency?”, however, has still remained unanswered. This paper is on the best-achievable ND latency for a given power budget between a pair of devices. In order to compute this lower bound, we introduce a concept called coverage maps, that allows us to analyze the ND procedure in a protocol-independent manner. Using it, we derive discovery latencies for different scenarios, e.g., when both devices have the same or different power budgets. We also show that some existing protocols can be parametrized such that they perform optimally. Our results are restricted to the case when a few devices discover each other at a time, as is the case in most real-life scenarios, while scenarios with large numbers of devices need further study.}
}


@article{DBLP:journals/ton/JavaniZ025,
	author = {Alireza Javani and
                  Marwen Zorgui and
                  Zhiying Wang},
	title = {Age of Information for Multiple-Source Multiple-Server Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {17--34},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3474052},
	doi = {10.1109/TNET.2024.3474052},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JavaniZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Having timely and fresh knowledge about the current status of information sources is critical in a variety of applications, where the status update arrives at the destination later than its generation time due to processing and communication delays. The freshness of the status update at the destination is captured by the notion of the age of information. In this study, we analyze a multiple sensing network with multiple sources, multiple servers, and a monitor (destination). Each source corresponds to an independent piece of information, and its age is individually measured. Given a particular source, the servers independently sense the source of information and send the status update to the monitor. We assume that updates arrive at the servers according to Poisson random processes. Each server sends its updates to the monitor through a direct link, which is modeled as a queue. The service time to transmit an update is considered to be an exponential random variable. We examine both homogeneous and heterogeneous service and arrival rates for the single-source case, and homogeneous arrival and service rates for the multiple-source case. We derive a closed-form expression for the average age of information under a last-come-first-serve (LCFS) queue for a single source and an arbitrary number of homogeneous servers. Using a recursive method, we derive the explicit average age of information for any number of sources and homogeneous servers. We also investigate heterogeneous servers and a single source, and present efficient algorithms for finding the average age of information. Optimal update scheduling strategies are also investigated in several scenarios, providing insights into enhancing the system performance in terms of update freshness.}
}


@article{DBLP:journals/ton/AmaroMS25,
	author = {Sebasti{\~{a}}o Amaro and
                  Miguel Matos and
                  Valerio Schiavoni},
	title = {Kollaps: Decentralized and Efficient Network Emulation for Large-Scale
                  Systems},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {35--50},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3478050},
	doi = {10.1109/TNET.2024.3478050},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AmaroMS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance and behavior of distributed systems is highly influenced by network properties, latency, bandwidth, packet loss, and jitter. When developing a distributed system, questions like, “how sensitive is the application’s performance to network latency and bandwidth?” commonly arise. Answering these questions systematically and in a reproducible manner is very hard due to the variability and lack of control over the network. Moreover, state-of-the-art approaches are focused exclusively on the control plane, lack support for network dynamics or do not scale beyond a single machine or small cluster, which further aggravates this problem. Kollaps is a distributed, scalable, and efficient network emulator addressing these limitations by hinging on two observations. First, from an application’s perspective, what matters are the emergent end-to-end properties (e. g., latency, bandwidth, jitter) rather than the internal state of the routers and switches leading to those properties. Second, this model is amenable to decentralized management, allowing the emulation to scale with the number of machines required by the application. This premise allows for building a simpler, dynamic emulation model that does not require maintaining the full network state. Kollaps is agnostic of the application language and transport protocol, scales to thousands of application nodes, and is accurate when compared against a bare-metal deployment or state-of-the-art approaches that emulate the full network state. We use Kollaps to accurately reproduce results from the literature and predict the behavior of complex unmodified distributed systems under different network dynamics.}
}


@article{DBLP:journals/ton/ZhouKE25,
	author = {Xujin Zhou and
                  Irem Koprulu and
                  Atilla Eryilmaz},
	title = {Age-Based Multi-Channel-Scheduling Under Constraints: Optimal and
                  Online Designs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {51--64},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3476144},
	doi = {10.1109/TNET.2024.3476144},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhouKE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the optimal scheduling problem where n source nodes attempt to transmit updates over L shared wireless on/off fading channels to optimize their age performance under energy and age-violation tolerance constraints. Specifically, we provide a generic formulation of age-optimization in the form of a constrained Markov Decision Process (CMDP), and obtain the optimal scheduler as the solution of an associated Linear Programming problem. We investigate the characteristics of the optimal single-user multi-channel scheduler under different age-related objectives where a usual threshold-based policy does not apply. We then investigate the stability region of the optimal scheduler for the multi-user case under age-violation tolerance constraints. Furthermore, we develop two online schedulers that do not require statistics and are amenable to scalable operation: Drift-plus-penalty-based design, and a novel variation of the well-known Q-learning-based reinforcement learning method that combines Q-learning with drift-minimization-methods successfully for the first time, to the best of our knowledge. Our numerical studies compare the performance of our online schedulers to the optimal scheduler to reveal that both algorithms capture the essential behavior of the optimal design under different scenarios with good scalability, with the Q-learning-based design providing even closer performance to the optimal one by utilizing the history of the drift in a novel way.}
}


@article{DBLP:journals/ton/NarasimhaKS25,
	author = {Dheeraj Narasimha and
                  Dileep Kalathil and
                  Srinivas Shakkottai},
	title = {Meta-Learning for Fast Adaption in Caching Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {65--77},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3478853},
	doi = {10.1109/TNET.2024.3478853},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NarasimhaKS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the proliferation of short form high quality video content, it has become increasing important to find light weight and efficient edge caching algorithms that can quickly adapt to changing trends. In this context we study an online caching problem where a set of users are connected to a set of caches. The users request files from these caches over a time horizon. These requests arrive sequentially, the sequence of requests are divided into tasks that have a certain degree of similarity. This similarity is leveraged so that we may learn the best policy for a new task using a very small number of sequential requests. We characterize the task averaged regret incurred in this setting, showing an improvement of   D / D ∗ D/D^{*}   where D is the diameter of the set of cache configurations and   D ∗ D^{*}   is a measure of task similarity. We provide the same theoretical guarantees under both a distributed and smoothed setting. Further, we validate our algorithm on trace based data as well as on synthetic data sets. In the trace based data sets we do not assume any inherent task structure or estimate of   D ∗ D^{*}  . These simulations show not only fast adaptation to new incoming tasks but also improved performance in highly non-stationary request settings.}
}


@article{DBLP:journals/ton/SunLG0R25,
	author = {Yuchen Sun and
                  Lailong Luo and
                  Deke Guo and
                  Li Liu and
                  Bangbang Ren},
	title = {Optimal Indexing: An Efficient Feature-Based Indexing Framework for
                  Similarity Data Sharing at the Network Edge},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {78--98},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3473853},
	doi = {10.1109/TNET.2024.3473853},
	timestamp = {Tue, 18 Nov 2025 13:13:04 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SunLG0R25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge storage systems have drawn many efforts to extend the storage and service capabilities of cloud data centers. A pivotal aspect lies in the data-sharing mechanism, which integrates geographically dispersed weak edge servers into an efficient storage system. It enables users to launch data operations at any server and retrieve the desired data across the distributed system. However, it remains open to meeting the increasing demand for similarity retrieval across edge servers. The intrinsic reason is that the existing solutions can only return an exact data match for a query while more general edge applications require the data similar to a query input from any server. To fill this gap, this paper pioneers the similarity edge data sharing mechanism, a new paradigm to support high-dimensional similarity search at network edges. First, through deeply thinking about the nature of similarity data sharing, we propose the problem of Optimal Indexing and formulate it as the optimal transport problem from the data space to the network space. On this basis, we propose Prophet, the first known architecture for similarity data indexing at the edge. We first divide the feature space of data into plenty of subareas, then project both subareas and edge servers into a virtual space where the distance between any two points can reflect not only data similarity but also network latency. When any edge server submits a request for data insert, delete, or query, it computes the data feature and the virtual coordinate; and then iteratively forwards the request via greedy routing based on the forwarding tables and the virtual coordinates. By Prophet, similar high-dimensional features would be stored by a common server or several nearby servers. Compared with distributed hash tables in P2P networks, Prophet requires to visit logarithmic servers for a data request and reduces the network latency from the logarithmic to the constant level of the server number. Evaluation results indicate that Prophet achieves the comparable retrieval accuracy and significantly shortens the query latency compared with centralized schemes, while the load balancing performance is nearly optimal.}
}


@article{DBLP:journals/ton/HuangXHHCWS025,
	author = {Xuanbo Huang and
                  Kaiping Xue and
                  Zixu Huang and
                  Jiangping Han and
                  Lutong Chen and
                  David S. L. Wei and
                  Qibin Sun and
                  Jun Lu},
	title = {SpiderNet: Enabling Bot Identification in Network Topology Obfuscation
                  Against Link Flooding Attacks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {99--113},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3473742},
	doi = {10.1109/TNET.2024.3473742},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangXHHCWS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Link-flooding attacks (LFAs) pose a significant challenge to Internet availability by attacking critical network links with high volumes of seemingly legitimate traffic. In response, researchers have developed network topology obfuscation (NTO) to safeguard critical links. However, state-of-the-art NTO defenses are coarse-grained, leading to less efficient security and usability. In addition, once under attack, NTO schemes cannot identify the attacker’s bot and launch counter-defensive measures. To address these issues, this paper introduces SpiderNet, which employs advanced obfuscation techniques to secure critical links while using strategically created honeypot links for effective bot identification. When adversaries probe the network, SpiderNet captures their probing behavior and deliberately feeds back misinformation about honeypot links. By analyzing the attack patterns directed at these decoy targets, SpiderNet correlates them with adversarial probing activities to effectively identify the bots. Our experiments demonstrate that SpiderNet is more robust than state-of-the-art NTO schemes in terms of security and usability, while also being capable of identifying LFA bots.}
}


@article{DBLP:journals/ton/JinYMX0Y25,
	author = {Chenlang Jin and
                  Haipeng Yao and
                  Tianle Mai and
                  Jiaqi Xu and
                  Qi Zhang and
                  F. Richard Yu},
	title = {A Resource-Efficient Content Sharing Mechanism in Large-Scale {UAV}
                  Named Data Networking},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {114--129},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3474888},
	doi = {10.1109/TNET.2024.3474888},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JinYMX0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, there has been significant attention in UAV Named Data Networking (UNDN) from both industry and academia. This network paradigm adopts a “request-reply” communication model that allows UAVs to access desired content without the need for specific information regarding the geographical location or IP address of the content producer. This IP-independent design is well-suited for dynamic UAV swarms, but it presents challenges in establishing matching policies between content consumers and producers. This is because that during the distributed decision-making process in content sharing, consumers cannot possess private information regarding producers, and producers may lack the motivation to distribute content. As a result, a revelation and incentive mechanism is needed to be formulated in the system. In this paper, a resource-efficient content-sharing mechanism is proposed to address the aforementioned challenges. First, we propose a contract-based mechanism to incentivize content producers to share content and reveal their private information at the same time. The problem of obtaining the optimal contract is discussed in both cases of information asymmetry and complete information. Then, the Gale-Shapley (GS) algorithm is adopted to make a stable many-to-one matching between content consumers and content producers. The simulation results verify the feasibility, effectiveness and energy efficiency of the proposed mechanism.}
}


@article{DBLP:journals/ton/ChandrasekaranV25,
	author = {Geetha Chandrasekaran and
                  Gustavo de Veciana and
                  Vishnu V. Ratnam and
                  Hao Chen and
                  Charlie Zhang},
	title = {Measurement Based Delay and Jitter Constrained Wireless Scheduling
                  With Near-Optimal Spectral Efficiency},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {130--145},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3481066},
	doi = {10.1109/TNET.2024.3481066},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChandrasekaranV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce two classes of measurement-based wireless schedulers. The Opportunistic Guaranteed Rate Scheduler (OGRS) meets a user’s delay constraints by opportunistically allocating the user the equivalent of a fixed service rate, which for a leaky-bucket constrained traffic ensures the delay requirements are met. By contrast, the Opportunistic Guaranteed Delay Schedulers (OGDS) schedules data transmissions when the current channel is better than what is expected in the time window before packet deadlines expire. Meeting such delay requirements requires a complementary admission control policy. We exhibit a simple measurement based policy, that indirectly accounts for heterogeneity in traffic, channel, and delay constraints by monitoring the statistics of user’s aggregate resource usage. We show that the spectral efficiency of our proposed approach is stochastically better than a wireless guaranteed rate scheduler. We bound spectral efficiency by considering an optimal offline policy with access to future channel rates and show via extensive simulations that OGRS can be within 10%-40% of the bound whereas OGDS is within 10% of the bound for a range of delay constraints. Additionally, we demonstrate that OGDS can exhibit better spectral efficiency at higher delay deadlines than schedulers leveraging neural network based predictions for future channel rates.}
}


@article{DBLP:journals/ton/CenZCSL25,
	author = {Wenkang Cen and
                  Jinbei Zhang and
                  Kechao Cai and
                  Shihai Sun and
                  John C. S. Lui},
	title = {A Fast Heuristic Entanglement Distribution Algorithm for Quantum Repeater
                  Chains},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {146--161},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3475378},
	doi = {10.1109/TNET.2024.3475378},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CenZCSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entanglement distribution via probabilistic entanglement swapping across a quantum repeater chain connecting two quantum nodes is a challenging problem. The difficulty lies in the exponential number of possible swapping structures within the repeater chain, necessitating efficient search algorithms, especially as the chain length increases. In this paper, we first explore the algorithmic design to facilitate the search for the optimal swapping structure along a repeater chain, aiming to maximize the entanglement distribution rate. Second, we examine the computational complexities of various algorithms and find that prior approaches exhibit excessively high complexities. Thus, we propose an efficient dynamic programming-based algorithm, FastHED, that leverages heuristics to expedite the search for the optimal swapping structure. Our theoretical analysis reveals that the upper bound of the proposed algorithm’s computational complexity is   O ( n ( log n ) 3 ) O(n(\\log n)^{3})   (more precisely,   O ( n ( log n ) 2 log log n ) O(n(\\log n)^{2} \\log \\log n)   when  n ≤ 2 29 n\\le 2^{29}  ), a significant improvement over the existing algorithm with a complexity of   O ( n 2 log n ) O(n^{2} \\log n)  , where n denotes the repeater chain’s length. Additionally, we design a best-first framework to evaluate the performance of different algorithms. Numerical results show that our algorithm achieves a higher average entanglement distribution rate than existing algorithms.}
}


@article{DBLP:journals/ton/Li0XCYS025,
	author = {Zhonghui Li and
                  Jian Li and
                  Kaiping Xue and
                  Lutong Chen and
                  Nenghai Yu and
                  Qibin Sun and
                  Jun Lu},
	title = {NarrowGap: Reducing Bottlenecks for End-to-End Entanglement Distribution
                  in Quantum Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {162--177},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3476342},
	doi = {10.1109/TNET.2024.3476342},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Li0XCYS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum networks, which work by establishing entanglement between distant quantum end nodes (known as end-to-end entanglement distribution), are the promising infrastructure for quantum applications. However, the inherent loss in quantum channels and quantum decoherence contribute to the scarcity of entanglement resources in quantum networks. Consequently, there is an inevitable gap between available entanglement resources and requests’ demands, significantly hindering concurrent end-to-end entanglement distributions. In this paper, we present NarrowGap, an end-to-end entanglement distribution design that can alleviate the negative impact of entanglement resource scarcity on the request service capability of quantum networks. At the heart of NarrowGap, the resource transfer scheme (RTS) is designed to transfer idle entanglement resources to boost the bottlenecks’ capacities based on the unique feature of entanglement swapping, thus narrowing the gap between available entanglement resources and requests’ demands for end-to-end entanglements. Besides, NarrowGap presents a resource allocation scheme (RAS) to guarantee fairness, considering both the success probability of end-to-end entanglement distribution and each request’s demand, to address resource competition in bottlenecks. Extensive simulations demonstrate that NarrowGap outperforms three representative schemes and can achieve more than twice the performance improvement in request service rate.}
}


@article{DBLP:journals/ton/ZhengC25,
	author = {Danyang Zheng and
                  Xiaojun Cao},
	title = {Provably Efficient Service Function Chain Embedding and Protection
                  in Edge Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {178--193},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3475248},
	doi = {10.1109/TNET.2024.3475248},
	timestamp = {Tue, 14 Oct 2025 19:49:18 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet-connected devices generate service function chain (SFC) requests for reliability-sensitive applications such as smart factories and intelligent healthcare. To facilitate reliable SFC provisioning, one can employ dedicated SFC protection approaches to protect the primary service function path (SFP) by constructing a fault-disjointed backup SFP. Notably, the construction processes of the fault-disjointed primary and backup SFPs are interdependent, and the direct application of existing SFC embedding approaches to construct these SFPs by separate processes may not effectively optimize overall resource consumption. In this work, for the first time, we comprehensively study how to embed and protect an SFC through collaborative processes that have provable bounds. We formally define the problem of SFC embedding and protection (SFCEP), for which we develop the novel techniques of a backup SFP identifier (BSI) and resource-aware Bellman-Ford loop (RBL) to address the challenges posed by collaborative embedding and protection. On the basis of these techniques, we propose an efficient algorithm called optimal SFC embedding and protection (Opt-SEP). When the network resources are sufficient to accommodate an incoming SFC request, we prove that Opt-SEP can minimize the overall resource cost of creating a pair of fault-disjointed primary and backup SFPs. Moreover, for cases in which the network resources are limited, our extensive simulation results show that Opt-SEP significantly outperforms the benchmarks.}
}


@article{DBLP:journals/ton/LiDZGS0LS25,
	author = {Bo Li and
                  Tiantian Duan and
                  Qinglin Zhao and
                  Yi Guo and
                  Zhaoxiong Song and
                  Hanwen Zhang and
                  Zhongcheng Li and
                  Yi Sun},
	title = {Performance Modeling of Relay Chain},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {194--209},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3487935},
	doi = {10.1109/TNET.2024.3487935},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiDZGS0LS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of blockchain applications, demand for cross-chain technology has been increasing. Relay chain mode is the state-of-the-art and mainstream solution nowadays. However, the relay chain mode suffers from poor performance, which stems from its core facility – the relay chain. Therefore, guiding its improvement and parameters configuration is vital. Currently, there is no specialized performance model of the relay chain. The cross-chain scenario involves receiving transactions from blockchains and uniformly verifying them, while general blockchain models are not applicable for it. Relay chains are characterized by the following features: transaction arrival in batches with uncertain sizes, updating block headers for simplified payment verification (SPV), Byzantine fault tolerance (BFT) type protocol, and different packaging rules. This work first proposes an analytical framework for relay chain performance. It captures the mentioned features by constructing a batch-arrival and bulk-service model. We give a concrete calculation of the relay chain with practical BFT (PBFT) consensus and develop a method to arrive at the computational forms of two essential performance descriptors: system throughput and cross-chain transaction confirmation delay. Through this model, we can judge accurately whether the relay chain is overloaded, and eliminate the overload state by tuning the parameters; and we can evaluate the system performance under different traffic and design parameters. Finally, we verify the model through experiments. With our study, operators can configure the system parameters effectively and improve the relay chain to meet the requirements of practical use.}
}


@article{DBLP:journals/ton/Huang00LDG25,
	author = {Hanlin Huang and
                  Ke Xu and
                  Tong Li and
                  Zhuotao Liu and
                  Xinle Du and
                  Xiangyu Gao},
	title = {DiffECN: Differential {ECN} Marking for Datacenter Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {210--225},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3477511},
	doi = {10.1109/TNET.2024.3477511},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Huang00LDG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {ECN marking has been integrated into datacenter switches to enable high-throughput and low-latency transport. We observe that current marking schemes are coarse-grained: they blindly mark all flows when congestion occurs, causing large flows to occupy undeserved bandwidth and preventing newly arriving small flows from finishing quickly. In this paper, we propose DiffECN, a differential marking strategy that marks only the flows that are the culprits of congestion and protects the remaining flows from being limited. We have implemented it in the Barefoot Tofino switch and performed extensive evaluations via both physical testbed and large-scale simulations. The results show that DiffECN can restrain flows responsible for congestion successfully while providing desirable network performance. For instance, compared to the legacy way of ECN marking, DiffECN achieves up to 32.5% (40.1%) lower average (99th percentile) flow completion time (FCT) for small flows while delivering similar FCT for large flows under production workloads.}
}


@article{DBLP:journals/ton/YuC0C25,
	author = {Minchen Yu and
                  Tingjia Cao and
                  Wei Wang and
                  Ruichuan Chen},
	title = {Pheromone: Restructuring Serverless Computing With Data-Centric Function
                  Orchestration},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {226--240},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3480031},
	doi = {10.1109/TNET.2024.3480031},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuC0C25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Serverless applications are typically composed of function workflows in which multiple short-lived functions are triggered to exchange data in response to events or state changes. Current serverless platforms coordinate and trigger functions by following high-level invocation dependencies but are oblivious to the underlying data exchanges between functions. This design is neither efficient nor easy to use in orchestrating complex workflows – developers often have to manage complex function interactions by themselves, with customized implementation and unsatisfactory performance. Therefore, we argue that function orchestration should follow a data-centric approach. In our design, the platform provides a data bucket abstraction to hold the intermediate data generated by functions. Developers can use a rich set of data trigger primitives to control when and how the output of each function should be passed to the next functions in a workflow. By making data consumption explicit and allowing it to trigger functions and drive the workflow, complex function interactions can be easily and efficiently supported. We present Pheromone – a scalable, low-latency serverless platform following this data-centric design. Compared to well-established commercial and open-source platforms, Pheromone cuts the latencies of function interactions and data exchanges by orders of magnitude, scales to large workflows, and enables easy implementation of complex applications.}
}


@article{DBLP:journals/ton/Wang00XLX00J025,
	author = {Ziming Wang and
                  Jing Li and
                  He Xue and
                  Wenzheng Xu and
                  Weifa Liang and
                  Zichuan Xu and
                  Jian Peng and
                  Pan Zhou and
                  Xiaohua Jia and
                  Sajal K. Das},
	title = {Approximation Algorithm and Applications for Connected Submodular
                  Function Maximization Problems},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {241--254},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3477532},
	doi = {10.1109/TNET.2024.3477532},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Wang00XLX00J025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study a connected submodular function maximization problem, which arises from many applications including deploying UAV networks to serve users and placing sensors to cover Points of Interest (PoIs). Specifically, given a budget K, the problem is to find a subset S with K nodes from a graph G, so that a given submodular function   f ( S ) f(S)   on S is maximized and the induced subgraph   G [ S ] G[S]   by the nodes in S is connected, where the submodular function f can be used to model many practical application problems, such as the number of users within different service areas of the deployed UAVs in S, the sum of data rates of users served by the UAVs, the number of covered PoIs by placed sensors, etc. We then propose a novel   1 − 1 / e 2 h + 2 \\frac {1-1/e}{2h+2}  -approximation algorithm for the problem, improving the best approximation ratio   1 − 1 / e 2 h + 3 \\frac {1-1/e}{2h+3}   for the problem so far, through estimating a novel upper bound on the problem and designing a smart graph decomposition technique, where e is the base of the natural logarithm, h is a parameter that depends on the problem and its typical value is 2. In addition, when   h = 2 h=2  , the algorithm approximation ratio is at least   1 − 1 / e 5 \\frac {1-1/e}{5}   and may be as large as 1 in some special cases when   K ≤ 23 K\\le 23  , and is no less than   1 − 1 / e 6 \\frac {1-1/e}{6}   when   K ≥ 24 K\\ge 24  , compared with the current best approximation ratio   1 − 1 / e 7 ( = 1 − 1 / e 2 h + 3 ) \\frac {1-1/e}{7}\\left ({{=\\frac {1-1/e}{2h+3}}}\\right)   for the problem. Finally, experimental results in the application of deploying a UAV network demonstrate that, the number of users within the service area of the deployed UAV network by the proposed algorithm is up to 7.5% larger than those by existing algorithms, and the throughput of the deployed UAV network by the proposed algorithm is up to 9.7% larger than those by the algorithms. Furthermore, the empirical approximation ratio of the proposed algorithm is between 0.7 and 0.99, which is close to the theoretical maximum value one.}
}


@article{DBLP:journals/ton/Tian00WX00025,
	author = {Jiazheng Tian and
                  Kun Xie and
                  Xin Wang and
                  Jigang Wen and
                  Gaogang Xie and
                  Jiannong Cao and
                  Wei Liang and
                  Kenli Li},
	title = {Reducing Network Distance Measurement Overhead: {A} Tensor Completion
                  Solution With a New Minimum Sampling Bound},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {255--270},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3480008},
	doi = {10.1109/TNET.2024.3480008},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Tian00WX00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network distance measurement is crucial for evaluating network performance, attracting significant research attention. However, conducting measurements for the entire network is exceedingly expensive and time-consuming, making the reduction of network distance measurement costs a top priority. The tensor completion method efficiently reduces measurement costs by utilizing a small amount of measured data to estimate the entire network’s distance data. Unfortunately, current tensor completion methods still suffer from issues such as complex sample selection, high measurement overhead, slow recovery, and low inference accuracy. To address the aforementioned challenges, we present an online network-wide distance measurement scheme. In this approach, continuous distance data are structured into sliding-window-based tensors. Our method incorporates a lightweight sample selection algorithm with a lowest sampling bound and a rapid, accurate unmeasured data inference algorithm. We have conducted extensive experiments using four real network distance datasets and two citywide crowd flow datasets. The empirical evaluations demonstrate the effectiveness of our approach, particularly in reducing measurement costs and enhancing data recovery accuracy.}
}


@article{DBLP:journals/ton/0041HYC25,
	author = {Tao Yang and
                  Bingnan Hou and
                  Yifan Yang and
                  Zhiping Cai},
	title = {Sweeping the IPv6 Internet: High-Efficiency Router Interface Discovery
                  With Weighted Sampling},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {271--285},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3479420},
	doi = {10.1109/TNET.2024.3479420},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0041HYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acquiring router interfaces is crucial for network measurement and security assessment. Established methods are readily available for IPv4 systems; however, efficiently pinpointing IPv6 router interfaces remains an unresolved issue, chiefly due to the vast IPv6 address space. Existing practices in this domain commonly suffer from inefficiencies. Thus, it is imperative to propose a methodology for fast enumeration of IPv6 router interfaces on a massive scale. In this study, we introduce Sweeper, a novel asynchronous IPv6 scanner that excels in discovering router interfaces from scratch, from few to many, on large-scale IPv6 networks. Unlike existing approaches, Sweeper requires merely the readily accessible IPv6 prefixes instead of seed addresses and can strategically optimize its probing direction based on a novel weighted sampling algorithm to increase the discovery rate. Real-world tests prove that Sweeper outperforms state-of-the-art works, discovering   33.2 % ∼ 48.8 % 33.2\\%\\sim 48.8\\%   more IPv6 router interface addresses than the baselines, with same computational resources. With Sweeper, we have collected approximately 6 million IPv6 router interface addresses from a single vantage point within less than one hour.}
}


@article{DBLP:journals/ton/Huang00LH25,
	author = {Guangjing Huang and
                  Xu Chen and
                  Qiong Wu and
                  Jingyi Li and
                  Qianyi Huang},
	title = {Joint Client and Cross-Client Edge Selection for Cost-Efficient Federated
                  Learning of Graph Convolutional Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {286--298},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3482356},
	doi = {10.1109/TNET.2024.3482356},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Huang00LH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Graph-structured data applications promote the development of Graph Neural Networks (GNN) in recent years. Due to privacy concerns, collecting graph data stored in massive client devices for centralized graph learning is prohibitive. It is natural to integrate federated learning (FL) in graph learning to address this issue, which enables clients to collaborate on training a shared model without uploading their data. This generates an emerging paradigm of federated graph learning (FGL). However, due to various costs incurred by FGL training, the collaboration between the server and clients is still a challenging issue in FGL, which remains largely unexplored in existing studies. To bridge this gap, we propose a cost-efficient collaboration framework for FGL of graph convolutional networks on semi-supervised node classification tasks, i.e., Joint Client and Cross-Client Edge Selection (JC3ES) for the server. Specifically, we first characterize how varies graph structure affect the final convergence performance of the FGL model. We then reveal the fundamental supermodular property in client selection. Based on this, we further devise an approximately optimal algorithm for the server and theoretically derive the performance gap between the proposed algorithm and the optimal solution. Extensive numerical evaluations show that our proposed algorithm achieves outstanding performance in cost-efficient collaboration for FGL on popular graph datasets.}
}


@article{DBLP:journals/ton/ElbediwyPDS25,
	author = {Mostafa Elbediwy and
                  Bill Pontikakis and
                  Jean{-}Pierre David and
                  Yvon Savaria},
	title = {Enabling Rank-Based {P4} Programmable Schedulers: Requirements, Implementation,
                  and Evaluation on BMv2 Switches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {299--310},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3481152},
	doi = {10.1109/TNET.2024.3481152},
	timestamp = {Tue, 14 Oct 2025 19:49:17 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ElbediwyPDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-defined networking (SDN) has revolutionized network infrastructure, offering programmability to meet evolving network demands. However, the fixed-function nature of the packet scheduler in current network equipment impedes the exploration of scheduling policies within a programmable network environment. This paper proposes a novel methodology to implement rank-based programmable schedulers in programmable BMv2 switches expressed with the network-specific programming language (P4). A proposed custom networking environment facilitates the study and evaluation of various scheduling policies. This environment is used to implement 20 different scheduling and shaping policies to identify the required language constructs and components needed to express these policies with the P4 language efficiently. Our experiments reveal that specific scheduling policies do not seamlessly align with a previously proposed architecture for rank-based scheduling policies. Thus, we propose rank-based versions for five previously reported scheduling policies, making them efficiently implementable in any rank-based schedulers and programmable network equipment. The reported results confirm that the rank-based versions of these scheduling policies accurately replicate the behavior and performance of the original policies, with a maximum error of 0.5% in the resulting flow completion times (FCTs).}
}


@article{DBLP:journals/ton/ObiriG0XC25,
	author = {Isaac Amankona Obiri and
                  Jianbin Gao and
                  Qi Xia and
                  Hu Xia and
                  Christian Nii Aflah Cobblah},
	title = {Hiba: Hierarchical High-Performance Blockchain Architecture},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {311--326},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3481488},
	doi = {10.1109/TNET.2024.3481488},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ObiriG0XC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sharding has the potential to overcome the scalability constraints of monolithic blockchains. However, some challenges are associated with sharding, such as optimizing the placement of transactions into shards to minimize cross-shard transactions, balancing workload as shard capacity increases, and identifying shards that process transactions maliciously. To address these challenges, we propose a hierarchical high-performance blockchain (Hiba) architecture. Hiba leverages inter-shard to facilitate cross-shard consensus, where a pre-selected subset of nodes from both transaction originating and receiving shards collaboratively participate in the validation process. This design ensures the validity of transactions and mitigates double-spending risks across various shards. Simultaneously, it reduces validation costs by eliminating the need for all nodes in both shards to actively participate in the consensus process. Additionally, Hiba implements a novel multi-tiered validation system. Following initial validation at the intra-shard and inter-shard levels, a subset of randomly chosen or suspicion-based transactions undergoes further validation through auxiliary consensus. This auxiliary consensus acts as a secondary validation layer, ensuring the integrity of the intra-shard/inter-shard consensus process. To improve transaction processing efficiency, we implement an optimized workload distribution scheme based on fitness functions to minimize the number of cross-shard transactions. The experimental results demonstrate that Hiba surpasses the existing works regarding throughput and latency.}
}


@article{DBLP:journals/ton/DuttaBB25,
	author = {Hrishikesh Dutta and
                  Amit Kumar Bhuyan and
                  Subir Biswas},
	title = {Using Multi-Armed Bandit Learning for Thwarting {MAC} Layer Attacks
                  in Wireless Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {327--339},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3481990},
	doi = {10.1109/TNET.2024.3481990},
	timestamp = {Mon, 15 Dec 2025 18:24:08 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DuttaBB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a learning-driven approach for medium access slot allocation in the presence of malicious nodes. Learning policies are developed with the goal of defending against several forms of quasi-random slot-scheduling attack models used by the malicious nodes. The primary learning objective for the non-malicious nodes is to minimize the degradation in network performance caused by the malicious nodes. This is accomplished while minimizing the bandwidth share of the malicious nodes. These objectives are achieved using a Multi-Armed Bandit (MAB) learning architecture that allows the nodes to learn transmission schedule on-the-fly, and without the need for any central arbitrator. Two different scheduling policies are introduced: robust and reactive policies. Following the design, a detailed characterization of these policies and their use in different application-specific scenarios are presented. An analytical model of the system is developed to find the benchmark throughput for different malicious attack models. It is demonstrated that the proposed framework allows network nodes to learn close-to -benchmark slot scheduling, while thwarting attacks from the malicious nodes. The proposed architecture is validated for various mesh networks and traffic conditions in the presence of different attack models enacted by the malicious nodes.}
}


@article{DBLP:journals/ton/Wei0QZS25,
	author = {Yannan Wei and
                  Qiang Ye and
                  Kaige Qu and
                  Weihua Zhuang and
                  Xuemin Shen},
	title = {Customized Transmission Protocol for Tile-Based 360{\textdegree} {VR}
                  Video Streaming Over Core Network Slices},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {340--354},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3485583},
	doi = {10.1109/TNET.2024.3485583},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Wei0QZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tile-based streaming has been proposed to address the challenge of high transmission rate demand in 360° virtual reality (VR) video streaming. However, it suffers from network and viewing behavior dynamics (i.e., head movements), while encoded video tiles have various properties in terms of transmission priority, deadline, and reliability requirement. Hence, a supporting transmission protocol is imperative. In this paper, we propose a customized transmission protocol based on Quick UDP Internet Connections (QUIC) which operates over a VR video network slice in the core network. The QUIC protocol is tailored to accommodate the characteristics of tile-based VR video streaming where explicit mapping relations between requested video tiles and QUIC streams are established. Two customized in-network protocol functionalities including packet filtering and caching-based packet retransmission are proposed, to filter out outdated video data due to field-of-view (FoV) prediction errors under viewing behavior dynamics and to achieve efficient packet retransmissions with disparate transmission reliability requirements. A slice-level packet header is designed to support enhanced slice-based VR video transmission with the proposed protocol functionalities. Key transport parameters are determined via theoretical analysis. Simulation results are presented to demonstrate the effectiveness of our proposed transmission protocol in achieving short average video segment downloading time and high average video segment quality.}
}


@article{DBLP:journals/ton/Xia0J0025,
	author = {Zhaoyue Xia and
                  Jun Du and
                  Chunxiao Jiang and
                  Zhu Han and
                  Yong Ren},
	title = {Latency Constrained Energy-Efficient Underwater Dynamic Federated
                  Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {355--368},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3481437},
	doi = {10.1109/TNET.2024.3481437},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Xia0J0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has emerged recently as an appealing and promising technique to deal with distributed learning issues in the sixth generation (6G) communication systems. Recent studies focus on developing FL schemes for terrestrial radio networks, where the variation in transmission data rates caused by transmission distance changes is negligible over one communication round. However, this variation has considerable influences for underwater acoustic channels. In this paper, we propose an underwater dynamic federated learning (UDFL) scheme by jointly considering characteristics of underwater acoustic channels and moving behavior of autonomous underwater vehicles. Moreover, an energy consumption minimization problem is formulated based on the scheme. To meet the challenges of transmission latency and FL performances, we consider them separately and provide closed-form solutions to the two individual problems. Specifically, we theoretically characterize the connections between transmission power and FL performances, and derive the optimal transmission policy given transmission latency constraints. Based on the two solutions, a dynamic programming based online power control algorithm is proposed to determine the transmission power across all time slots. Numerical simulations are conducted to demonstrate that the designed scheme is effective and the proposed online algorithm can achieve latency constrained energy-efficient UDFL.}
}


@article{DBLP:journals/ton/Yan0ZDWLS25,
	author = {Xiaoyong Yan and
                  Jiannong Cao and
                  Shigeng Zhang and
                  Chuntao Ding and
                  Chenhuang Wu and
                  Alex X. Liu and
                  Aiguo Song},
	title = {Cooperative Localization Using Expected Minimum Segment for Irregular
                  Multi-Hop Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {369--381},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3487904},
	doi = {10.1109/TNET.2024.3487904},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Yan0ZDWLS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For the creation of wireless network applications, node locations are frequently necessary. However, communication effectiveness, measurement accuracy, and localization stability will be low in irregular multi-hop networks when locating nodes using conventional algorithms. To this end, a novel cooperative localization algorithm using expected minimum segments (LEMS, for short) is proposed in this paper. LEMS begins by measuring the distance between paired nodes, which is completed along with network initialization. Then, each unlocated node constructs its own sub-network, including it, based on the error characteristics among anchor nodes. Finally, each unlocated node searches for its estimated location in its sub-region based on the objective function generated by the chaotic mapping. Simulation results demonstrate that the proposed algorithm significantly outperforms the state-of-the-art regarding efficiency, accuracy, and stability for various irregular networks. Specifically, our proposed algorithm achieves a median improvement in localization accuracy of 0.62 to 29.57 times and a reduction in the range of localization errors of 0.06 to 16.8 times.}
}


@article{DBLP:journals/ton/YangZXHQ25,
	author = {Lan Yang and
                  Yangming Zhao and
                  Hongli Xu and
                  Liusheng Huang and
                  Chunming Qiao},
	title = {Dynamic Entanglement Routing Based on Stream Processing for Quantum
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {382--397},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3485919},
	doi = {10.1109/TNET.2024.3485919},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangZXHQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum Networks (QNs) typically leverage teleportation to send quantum bits (called qubits) to their destinations. To teleport a data qubit from Alice to Bob, one Entanglement Connection (EC) between Alice and Bob needs to be established. Accordingly, we have to concurrently establish as many requested ECs as possible in order to maximize the network throughput. Conventional methods either assumed a known traffic matrix and calculated the Entanglement Paths (EPs) for all requests in one batch or maximized the number of ECs established between all Source-Destination (SD) pairs without considering the amount of data qubits to be teleported. These methods are not scalable in large scale QNs since it is time consuming to calculate the EPs for a batch of requests. In addition, there may be only very few data qubits to be teleported between some SD pairs. Accordingly, the latter method may establish many useless ECs. To address these issues, we propose a Dynamic Entanglement Routing (DER) scheme which determines the EPs based on stream processing. By introducing a method to derive an appropriate purification scheme along each EP, we further extend DER to Dynamic Entanglement Routing with Purification (DERP) that provides fidelity guarantee to the established ECs. Through extensive simulations, we demonstrate that DER outperforms two representative heuristics by up to 52.79% and 61.27%, respectively, in terms of average request completion time and when we have to ensure the fidelity of the established ECs, this performance improvement will become 21.05% and 48.67%, respectively, if DERP is adopted.}
}


@article{DBLP:journals/ton/Yue0ZT0025,
	author = {Xiaofei Yue and
                  Song Yang and
                  Liehuang Zhu and
                  Stojan Trajanovski and
                  Fan Li and
                  Xiaoming Fu},
	title = {Exploiting Wide-Area Resource Elasticity With Fine-Grained Orchestration
                  for Serverless Analytics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {398--413},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3486788},
	doi = {10.1109/TNET.2024.3486788},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Yue0ZT0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the flourishing of global services, low-latency analytics on large-volume geo-distributed data has been a regular requirement for application decision-making. Serverless computing, with its rapid function start-up and lightweight deployment, provides a compelling way for geo-distributed analytics. However, existing research focuses on elastic resource scaling at the stage granularity, struggling to heterogeneous resource demands across component functions in wide-area settings. The neglect potentially results in the cost inefficiency and Service Level Objective (SLO) violations. In this paper, we advocate for fine-grained function orchestration to exploit wide-area resource elasticity. We thereby present Demeter, a fine-grained function orchestrator that saves job execution costs for geo-distributed serverless analytics while ensuring SLO compliance. By learning from volatile and bursty environments, Demeter jointly makes per-function placement and resource allocation decisions using a well-optimized multi-agent reinforcement learning algorithm with a pruning mechanism. It prevent the irreparable performance loss by function congestion control. Ultimately, we implement Demeter and evaluate it with the realistic workloads. Experimental results reveal that Demeter outperforms the baselines by up to 46.6% on cost, while reducing SLO violation by over 23.7% and bringing it to below 15%.}
}


@article{DBLP:journals/ton/0020LX0WL025,
	author = {Yang Xu and
                  Yunming Liao and
                  Hongli Xu and
                  Zhiyuan Wang and
                  Lun Wang and
                  Jianchun Liu and
                  Chen Qian},
	title = {FedSNN: Training Slimmable Neural Network With Federated Learning
                  in Edge Computing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {414--429},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3487582},
	doi = {10.1109/TNET.2024.3487582},
	timestamp = {Wed, 29 Oct 2025 15:09:47 +0100},
	biburl = {https://dblp.org/rec/journals/ton/0020LX0WL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To provide a flexible tradeoff between inference accuracy and resource requirement at runtime, the slimmable neural network (SNN), a single network executable at different widths with the same deploying and management cost as that of a single model, has been proposed. However, how to effectively train SNN among massive devices in edge computing without revealing their local data remains an open problem. To this end, we leverage a novel distributed machine learning paradigm, i.e., federated learning, to realize effective on-device SNN training. As current FL schemes often train only one model with fixed architecture, and the existing SNN training algorithm is resource-intensive, integrating FL and SNN is non-trivial. Furthermore, two intrinsic features in edge computing, i.e., data and system heterogeneity, exacerbate the difficulty. Motivated by this, we redesign the model distribution, local training, and model aggregation phases in traditional FL, and propose FedSNN, a framework that ensures all widths in SNN can obtain high accuracy with less resource consumption. Specifically, for devices with heterogeneous training capacities and data distributions, the parameter server will distribute each of them with one proper width for adaptive local training guided by their uploaded model features, and their trained models will be weighted-averaged using the proposed multi-width SNN aggregation to improve their statistical utility. Extensive experiments on a distributed testbed show that FedSNN improves the model accuracy by about 2.18%-8.1%, and accelerates training by about   1.31 × 1.31\\times   -  6.84 × 6.84\\times   , compared with existing solutions.}
}


@article{DBLP:journals/ton/Xu00XYLN25,
	author = {Tingting Xu and
                  Xiaoliang Wang and
                  Chen Tian and
                  Yun Xiong and
                  Baoliu Ye and
                  Sanglu Lu and
                  Cam{-}Tu Nguyen},
	title = {Accelerating Network Features Deployment With Heterogeneous Platforms},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {430--445},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3491840},
	doi = {10.1109/TNET.2024.3491840},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Xu00XYLN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Enhancing the networking system with appropriate functions is a longstanding goal. Unfortunately, in today’s large-scale high-speed data centers, the feature velocity of network functions is slow because it is hard to verify the function in realistic scenarios. Recent advances in programmable switching ASICs have enabled the network data plane to move beyond its traditional role of packet forwarding. However, the current compromise between performance and flexibility results in limitations such as restricted memory/computation resources and programmable models. These limitations make it challenging for programmable switches to offer more features and to be deployed in large-scale production environments. In response, we present CLIP, a framework that works in collaboration with programmable devices and commodity servers to enhance the validation and deployment velocity of features. CLIP defines a cross-platform function definition framework and provides a set of tools to reduce the complexity of manually writing cross-platform programs. We propose an automatic traffic placement and scaling mechanism to coordinate packet processing performance across heterogeneous devices. Compared with software-based Network Functions (NFs), CLIP achieves a throughput ranging from   1.36 × 1.36\\times    to   16.06 × 16.06\\times    under different realistic traffic loads. Through the development and deployment of three self-defined functions within a realistic testbed, we demonstrate the feasibility and efficiency of CLIP.}
}


@article{DBLP:journals/ton/XiaoZWZ25,
	author = {Wenli Xiao and
                  Shizhen Zhao and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Segment {EDF:} {A} Scheduling Policy With Tight Deterministic Latency
                  Under Multi-Hop Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {1},
	pages = {446--461},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3489970},
	doi = {10.1109/TNET.2024.3489970},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaoZWZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate tight end-to-end delay guarantees for real-time flows with stochastic arrivals in multi-hop networks. The existing multi-hop scheduling policy either only serves for real-time flows with deterministic arrivals or cannot offer a tight end-to-end delay guarantee for multi-hop flows. In this paper, we prove a closed-form formula, which offers a sufficient and necessary condition for the EDF (Earliest Deadline First) scheduling policy to meet all the end-to-end deadlines for flows with stochastic arrivals in converge-cast tree networks. To the best of our knowledge, this is the first formula that characterizes the exact schedulability region for EDF in multi-hop networks. Moving beyond converge-cast tree networks to general multi-hop networks, we introduce the Segment EDF approach. This method partitions a general network into multiple converge-cast networks using a novel concept called the critical links. By determining segment deadlines within the schedulability region of each converge-cast tree, Segment EDF offers a tight end-to-end delay guarantee for each flow. We present a theoretical analysis showcasing the superior performance of Segment EDF over the existing scheduling policy of hop-by-hop EDF. Furthermore, we evaluate the performance of Segment EDF based on two key metrics: guaranteed flow completion time and admission ratio, in both real-world and synthetic networks. Our simulation results show that Segment EDF can provide 61.31%-96.4% tighter end-to-end delay guarantee and increase admission ratio by about 3.52%-107.59% for sequential arrival flow set and 1.04%-95.50% for batch arrival flow set than hop-by-hop EDF.}
}


@article{DBLP:journals/ton/0001LF25,
	author = {Qingsong Liu and
                  Zhuoran Li and
                  Zhixuan Fang},
	title = {Smoothed Online Decision Making in Communication: Algorithms and Applications},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {463--478},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3495522},
	doi = {10.1109/TNET.2024.3495522},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0001LF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Evolution of the 5G network introduces much higher QoS standards and energy saving objectives, which requires a more refined and smoothed online control method in many scenarios. To address this challenge, we study the online decision problem with switching costs where the agent incurs both a convex hitting cost and an additional switching cost of changing decisions, i.e., Smoothed Online Convex Optimization (SOCO). While there have been a wide variety of online algorithms designed, their theoretical performance relies on certain assumptions about loss functions, e.g., linearity and smoothness, predictability, or prior knowledge of regularity measures of environment. This paper addresses this limitation by developing a universal algorithm IOMD-SOCO that applies to general convex loss functions without predictions. We show that IOMD-SOCO achieves an order-optimal, universal dynamic regret bound. We also propose its parameter-free versions, i.e., without requiring the prior knowledge of path length of the comparator sequence, and achieve the same-order regret bound. We are the first to provide dynamic regret bounds for SOCO with general convex loss functions via parameter-free algorithms. Our numerical experiments show that IOMD-SOCO indeed achieves a substantial performance improvement. We also discuss potential applications of SOCO in communication networks.}
}


@article{DBLP:journals/ton/Liu0WL0000DC025,
	author = {Kexin Liu and
                  Chang Liu and
                  Qingyue Wang and
                  Zhiqiang Li and
                  Lu Lu and
                  Xiaoliang Wang and
                  Fu Xiao and
                  Ying Zhang and
                  Wanchun Dou and
                  Guihai Chen and
                  Chen Tian},
	title = {An Anatomy of Token-Based Congestion Control},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {479--493},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3491763},
	doi = {10.1109/TNET.2024.3491763},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Liu0WL0000DC025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion control protocols are crucial for optimizing the performance of datacenter network applications. Although reactive congestion control (RCC) protocols are commonly used in commercial datacenters, researchers have been exploring token-based proactive congestion control (TCC) protocols to further enhance network performance. Despite the development of numerous TCC variants, there has not been a thorough examination of the design space of TCC protocols until now. This paper aims to address this gap by introducing a framework for understanding the design choices within the TCC approach for TCC protocols. By analyzing various design aspects of TCC approaches, we create a novel TCC protocol called ToCC. At the central of ToCC design is that it leverages congestion control mechanisms over tokens. To implement ToCC, we tackle several challenges and integrate it into NP-based smart NICs. Comparing ToCC with state-of-the-art TCC and RCC protocols through extensive large-scale simulations and testbed evaluations, we find that ToCC consistently achieves low latency across different scenarios. Moreover, ToCC significantly reduces buffer occupancy by 4.8 times compared to existing methods, and during incast scenarios, it decreases flow completion time by up to 90%.}
}


@article{DBLP:journals/ton/DasalaK25,
	author = {Keerthi Priya Dasala and
                  Edward W. Knightly},
	title = {Scalable Multi-User Terahertz Wireless Networks With Angularly Dispersive
                  Links},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {494--509},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3492912},
	doi = {10.1109/TNET.2024.3492912},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DasalaK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {THz communication can realize the next order of magnitude in data rate and user densities due to the availability of wide THz-scale spectral bands. Wide bandwidth links can exhibit angular dispersion, i.e., frequency-dependent radiation direction. While angular dispersion has enabled path discovery and dynamic beam steering via frequency tuning, multi-user communication in THz links remains an unaddressed challenge. This paper presents the first study and performance evaluation of multi-user THz WLANs with angularly dispersive links. We employ a single parallel-plate Leaky-Wave Antenna (LWA) for THz directional transmission and present a multi-user communication strategy that exploits angular dispersion and angular separation of users and provides all-spectrum access to users located in different directions with the objective of aggregate rate maximization. With analytical model-driven evaluations and over-the-air experiments that inform our trace-driven emulation of multi-user conditions, we show how the multi-user performance of an angularly dispersive LWA link fundamentally depends on frequency, angle, and bandwidth utilized by users through non-linear mechanisms. As increasing bandwidth yields a larger signal footprint in LWA links, we demonstrate that as compared to the model prediction, not only is the aggregate data rate maximized with wider beams, but the experimental link is far better even for practical irregular beams with side lobes and asymmetry. Our experimental findings reveal the potential of leveraging angular dispersion and users’ angular separation to establish a scalable THz wireless link that offers contention-free or medium access control-free access. Our results demonstrate the feasibility of accommodating up to 11 simultaneous users, making it a promising candidate solution for densely populated user environments.}
}


@article{DBLP:journals/ton/Du0YZC25,
	author = {Zhuoxuan Du and
                  Jiaqi Zheng and
                  Hebin Yu and
                  Hongquan Zhang and
                  Guihai Chen},
	title = {Libra: {A} Congestion Control Framework for Diverse Application Preferences
                  and Network Conditions},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {510--525},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3492096},
	doi = {10.1109/TNET.2024.3492096},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Du0YZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the increase of diversity in application preferences and networks, existing congestion control algorithms (CCAs) do not accommodate this complicated reality. Previous classic CCAs are designed for a specific domain with fixed rules, failing to adapt to such diversities. Recently surged learning-based CCAs have great potential in adaptability and flexibility but are not practical due to unsatisfying performance on convergence, fairness, overhead, consistency and safety assurance. In this paper, we propose Libra, a unified congestion control framework, that can empower these properties by combining the wisdom of classic and reinforcement learning (RL)-based CCAs. Extensive evaluation of Libra’s Linux kernel implementations on both live Internet and emulated networks shows performance improvement under dynamic networks (e.g.,   1.2\\times  1.2\\times    throughput than Orca on average). At the same time, Libra can flexibly satisfy different application needs, reduce the running overhead by at most   0.88\\times  0.88\\times    and perform good fairness and convergence properties, well-fitting our theoretical analysis.}
}


@article{DBLP:journals/ton/HoganLSFR025,
	author = {Mary Hogan and
                  Devon Loehr and
                  John Sonchack and
                  Shir Landau Feibish and
                  Jennifer Rexford and
                  David Walker},
	title = {Automated Optimization of Parameterized Data-Plane Programs With Parasol},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {526--540},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3499833},
	doi = {10.1109/TNET.2024.3499833},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HoganLSFR025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable data planes allow for sophisticated applications that give operators the power to customize the functionality of their networks. Deploying these applications, however, often requires tedious and burdensome optimization of their layout and design, in which programmers must manually write, compile, and test an implementation, adjust the design, and repeat. In this paper we present Parasol, a framework that allows programmers to define general, parameterized network algorithms and automatically optimize their various parameters. The parameters of a Parasol program can represent a wide variety of implementation decisions, and may be optimized for arbitrary, high-level objectives defined by the programmer. Furthermore, optimization may be tailored to particular environments by providing a representative sample of traffic. We show how we implement the Parasol framework, which consists of a sketching language for writing parameterized programs, and a simulation-based optimizer for testing different parameter settings. We evaluate Parasol by implementing a suite of ten data-plane applications, and find that Parasol produces a solution with comparable performance to hand-optimized P4 code within a two-hour time budget.}
}


@article{DBLP:journals/ton/ZouNS025,
	author = {Yang Zou and
                  Xin Na and
                  Yimiao Sun and
                  Yuan He},
	title = {Trident: Interference Avoidance in Multi-Reader Backscatter Network
                  via Frequency-Space Division},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {541--553},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3495660},
	doi = {10.1109/TNET.2024.3495660},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZouNS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter is a key technology for battery-free sensing in industrial IoT applications. To fully cover numerous tags in the deployment area, one often needs to deploy multiple readers, each of which communicates with tags within its communication range. However, the actual backscattered signals from a tag are likely to reach a reader outside its communication range and cause interference. Conventional TDMA or CSMA based approaches for interference avoidance separate readers’ media access in time, leading to limited network throughput. In this paper, we propose Trident, a novel backscatter design that enables interference avoidance via frequency-space division. By incorporating a tunable bandpass filter and multiple terminal loads, a Trident tag can detect its channel condition and adaptively adjust the frequency and the power of its backscattered signals. We further propose a frequency assignment algorithm for the readers. With these designs, all the readers in the network can operate concurrently without being interfered. We implement Trident and evaluate its performance under various settings. The results demonstrate that Trident enhances the network throughput by   3.18\\times  3.18 × 3.18\\times   , compared to the TDMA-based scheme.}
}


@article{DBLP:journals/ton/JiangLCH0X25,
	author = {Donghong Jiang and
                  Yanbiao Li and
                  Yuxuan Chen and
                  Jing Hu and
                  Yi Huang and
                  Gaogang Xie},
	title = {Heuristic Binary Search: Adaptive and Fast IPv6 Route Lookup With
                  Incremental Prefix Updates},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {554--569},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3504244},
	doi = {10.1109/TNET.2024.3504244},
	timestamp = {Fri, 16 Jan 2026 20:27:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JiangLCH0X25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The advent of Software Defined Networking (SDN) and Network Function Virtualization (NFV) has revolutionized the deployment of software-based routing and forwarding devices in cloud and network systems. Yet, the performance of IPv6 route lookup in these devices remains a significant challenge due to two main factors: 1) the longer IPv6 addresses, which hinder high-speed lookup, and 2) the huge IPv6 IP address space, necessitates adaptability to varied length-based prefix distributions across various network scenarios. Existing IP lookup algorithms fall short in addressing these IPv6-specific challenges. To address these challenges, this paper proposes a novel Heuristic Binary Search (HBS) scheme. Building on the classical “Binary Search on Prefix Lengths” scheme, HBS employs three innovative techniques to achieve adaptive and fast IPv6 lookups with incremental updates: 1) a heuristic binary search method for fast lookup; 2) a tree rotation method for dynamic adjustment of binary search tree shapes in response to changes in prefix distribution; and 3) the introduction of the Associated Marker List (AML), a new data structure aimed at facilitating rapid incremental prefix updates. Our theoretical proofs and comprehensive evaluations demonstrate HBS’s superiority in lookup performance, dynamic adaptability, update speed, and memory efficiency.}
}


@article{DBLP:journals/ton/RenW0MLLHD25,
	author = {Zhe Ren and
                  Zihao Wang and
                  Xinghua Li and
                  Yinbin Miao and
                  Zhuowen Li and
                  Ximeng Liu and
                  Lei Han and
                  Robert H. Deng},
	title = {Deep Reinforcement Learning Based Scheduling Strategy in Blockchain
                  Payment Channel Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {570--582},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3492034},
	doi = {10.1109/TNET.2024.3492034},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RenW0MLLHD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the popularity of blockchains, low transaction throughput has become a significant bottleneck in applications such as cryptocurrencies. Payment channel networks (PCNs) have received attention as a way to improve throughput. However, due to the difficulty of predicting future transactions for nodes, the transactions are prone to failure when the channel balances do not meet required conditions. It has been shown that increasing buffers (queues) in PCNs can increase the success rate of transactions and throughput. Nevertheless, there is no effective transaction scheduling strategy in buffers when transaction values are flexible and variable. To solve this problem, we first formulate the Scheduling Problem in PCNs (named PSP), and then prove it is NP-hard. We design a neural network solver based on the Sequence to Sequence (Seq2Seq) architecture and train the solver using the reinforcement learning method. With the solver, we first give two scheduling strategies to maximize transaction throughput, and then design a PCN simulator for performance evaluation. Extensive experiments are conducted to show the superiority and various performances of our proposal and illustrate that our proposal can get a significant advantage in terms of the transaction throughput compared to the existing works.}
}


@article{DBLP:journals/ton/Huang0025,
	author = {Yin Huang and
                  Lei Wang and
                  Jie Xu},
	title = {Quantum Entanglement Path Selection and Qubit Allocation via Adversarial
                  Group Neural Bandits},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {583--594},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3510550},
	doi = {10.1109/TNET.2024.3510550},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Huang0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum Data Networks (QDNs) have emerged as a promising framework in the field of information processing and transmission, harnessing the principles of quantum mechanics. QDNs utilize a quantum teleportation technique through long-distance entanglement connections, encoding data information in quantum bits (qubits). Despite being a cornerstone in various quantum applications, quantum entanglement encounters challenges in establishing connections over extended distances due to probabilistic processes influenced by factors like optical fiber losses. The creation of long-distance entanglement connections between quantum computers involves multiple entanglement links and entanglement swapping techniques through successive quantum nodes, including quantum computers and quantum repeaters, necessitating optimal path selection and qubit allocation. Current research predominantly assumes known success rates of entanglement links between neighboring quantum nodes and overlooks potential network attackers. This paper addresses the online challenge of optimal path selection and qubit allocation, aiming to learn the best strategy for achieving the highest success rate of entanglement connections between two chosen quantum computers without prior knowledge of the success rate and in the presence of a QDN attacker. The proposed approach is based on multi-armed bandits, specifically adversarial group neural bandits, which treat each path as a group and view qubit allocation as arm selection. Our contributions encompass formulating an online adversarial optimization problem, introducing the EXPNeuralUCB bandits algorithm with theoretical performance guarantees, and conducting comprehensive simulations to showcase its superiority over established advanced algorithms.}
}


@article{DBLP:journals/ton/NezhadianBBNS25,
	author = {Fatemeh Nezhadian and
                  Enrico Branca and
                  Anna Barzolevskaia and
                  Andrei Natadze and
                  Natalia Stakhanova},
	title = {Measuring and Characterizing Propagation of Reuse {RSA} Certificates
                  and Keys Across {PKI} Ecosystem},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {595--611},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3495617},
	doi = {10.1109/TNET.2024.3495617},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NezhadianBBNS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The insecurities of public-key infrastructure on the Internet have been the focus of research for over a decade. The extensive presence of broken, weak, and vulnerable cryptographic keys has been repeatedly emphasized by many studies. Analyzing the security implications of cryptographic keys’ vulnerabilities, several studies noted the presence of public key reuse. While the phenomenon of private key sharing was extensively studied, the prevalence of public key sharing on the Internet remains largely unknown. In this work, we perform a large-scale analysis of public key reuse within the PKI ecosystem. We investigate the presence and distribution of duplicate X.509 certificates and reused RSA public keys across a large collection containing over 314 million certificates and over 13 million SSH keys collected by different sources at different times. We analyze the cryptographic weaknesses of duplicate certificates and reused keys and investigate the reasons and sources of reuse. Our results reveal that certificate and key sharing are common and persistent. Our findings show over 10 million certificates and 17 million public keys are reused across time and shared between our collections. We observe keys with non-compliant cryptographic elements stay available for an extended period of time.}
}


@article{DBLP:journals/ton/Ji00LW0W25,
	author = {Yixiong Ji and
                  Jiahao Cao and
                  Qi Li and
                  Yan Liu and
                  Tao Wei and
                  Ke Xu and
                  Jianping Wu},
	title = {Constructing {SDN} Covert Timing Channels Between Hosts With Unprivileged
                  Attackers},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {612--623},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3496997},
	doi = {10.1109/TNET.2024.3496997},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Ji00LW0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-defined networking (SDN) has been widely deployed due to its centralization and programmable features. However, these new features bring new threats at the same time. Previous studies have shown that SDN covert channels can be built with a privileged adversary that controls SDN key components, such as controller applications or SDN switches. In this paper, we propose new SDN covert timing channels between hosts without controlling applications, controllers, or having access to switches. Experiments in a real SDN testbed demonstrate the feasibility and effectiveness of our covert channels. To defend against the covert timing channels, we design a defense system named CovertGuard, which utilizes the timing characteristics of the covert channels’ delays to detect and eliminate covert channels effectively.}
}


@article{DBLP:journals/ton/0066YMG0SLXW25,
	author = {Bo Wang and
                  Xingxing Yang and
                  Zili Meng and
                  Yaning Guo and
                  Chen Sun and
                  Justine Sherry and
                  Hongqiang Harry Liu and
                  Mingwei Xu and
                  Jianping Wu},
	title = {Zhuge: Toward Consistent Low Latency With Minimal Control Loop Delay},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {624--639},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3502822},
	doi = {10.1109/TNET.2024.3502822},
	timestamp = {Tue, 04 Nov 2025 12:12:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/0066YMG0SLXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time communication (RTC) applications demand consistent low latency to ensure a smooth and interactive user experience. However, wireless networks, including WiFi and cellular, although they provide satisfactory median latency, often suffer from significant tail latency due to the highly variable network bandwidth. We observe that the control loop for managing the sending rate of RTC applications becomes inflated when congestion occurs at the wireless access point (AP), leading to untimely rate adaptation in response to wireless dynamics. Existing solutions fail to quickly adapt to bandwidth fluctuations due to the inflated control loop. In this paper, we propose Zhuge, a purely wireless AP-based solution that addresses these issues by separating congestion feedback from congested queues. Our approach involves the design of a Fortune Teller, which accurately estimates the wireless latency for each packet upon its arrival at the wireless AP. To ensure scalability, we also develop a Feedback Updater that translates the estimated latency into understandable feedback messages for various end-to-end protocols, delivering them back to the senders immediately for rate adaptation. Our evaluation, based on both trace-driven simulations and real-world scenarios, demonstrates that Zhuge significantly reduces the occurrence of large tail latency and alleviates RTC performance degradation by 22% to 95%.}
}


@article{DBLP:journals/ton/BountrogiannisE25,
	author = {Konstantinos Bountrogiannis and
                  Anthony Ephremides and
                  Panagiotis Tsakalides and
                  George Tzagkarakis},
	title = {Age of Incorrect Information With Hybrid {ARQ} Under a Resource Constraint
                  for N-Ary Symmetric Markov Sources},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {640--653},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3499372},
	doi = {10.1109/TNET.2024.3499372},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BountrogiannisE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Age of Incorrect Information (AoII) is a recently proposed metric for real-time remote monitoring systems. In particular, AoII measures the time the information at the monitor is incorrect, weighted by the magnitude of this incorrectness, thereby combining the notions of freshness and distortion. This paper addresses the definition of an AoII-optimal transmission policy in a discrete-time communication scheme with a resource constraint and a hybrid automatic repeat request (HARQ) protocol. Considering an N-ary symmetric Markov source, the problem is formulated as an infinite-horizon average-cost constrained Markov decision process (CMDP). Interestingly, it is proved that, under some conditions, the optimal transmission policy is to never transmit. This reveals a region of the source dynamics where communication is inadequate in reducing the AoII. Elsewhere, there exists an optimal transmission policy, which is a randomized mixture of two discrete threshold-based policies that randomize on at most one state. The optimal threshold and the randomization component are derived analytically. Numerical results illustrate the impact of the source dynamics, channel conditions, and resource constraints on the average AoII.}
}


@article{DBLP:journals/ton/ChenLYCM025,
	author = {Zheyi Chen and
                  Jie Liang and
                  Zhengxin Yu and
                  Hongju Cheng and
                  Geyong Min and
                  Jie Li},
	title = {Resilient Collaborative Caching for Multi-Edge Systems With Robust
                  Federated Deep Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {654--669},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3497958},
	doi = {10.1109/TNET.2024.3497958},
	timestamp = {Tue, 23 Sep 2025 09:24:28 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenLYCM025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a key technique for future networks, the performance of emerging multi-edge caching is often limited by inefficient collaboration among edge nodes and improper resource configuration. Meanwhile, achieving optimal cache hit rates poses substantive challenges without effectively capturing the potential relations between discrete user features and diverse content libraries. These challenges become further sophisticated when caching schemes are exposed to adversarial attacks that seriously impair cache performance. To address these challenges, we introduce RoCoCache, a resilient collaborative caching framework that uniquely integrates robust federated deep learning with proactive caching strategies, enhancing performance under adversarial conditions. First, we design a novel partitioning mechanism for multi-dimensional cache space, enabling precise content recommendations in user classification intervals. Next, we develop a new Discrete-Categorical Variational Auto-Encoder (DC-VAE) to accurately predict content popularity by overcoming posterior collapse. Finally, we create an original training mode and proactive cache replacement strategy based on robust federated deep learning. Notably, the residual-based detection for adversarial model updates and similarity-based federated aggregation are integrated to avoid the model destruction caused by adversarial updates, which enables the proactive cache replacement adapting to optimized cache resources and thus enhances cache performance. Using the real-world testbed and datasets, extensive experiments verify that the RoCoCache achieves higher cache hit rates and efficiency than state-of-the-art methods while ensuring better robustness. Moreover, we validate the effectiveness of the components designed in RoCoCache for improving cache performance via ablation studies.}
}


@article{DBLP:journals/ton/LiJL0L025,
	author = {Zhetao Li and
                  Kun Jiang and
                  Haolin Liu and
                  Tie Qiu and
                  Hongbin Luo and
                  Fu Xiao},
	title = {Alleviating Cold Start Problem by Improving User Retention in Mobile
                  Crowdsourcing Network},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {670--684},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3496750},
	doi = {10.1109/TNET.2024.3496750},
	timestamp = {Sun, 09 Nov 2025 17:05:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiJL0L025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile crowdsourcing (MCS) has attracted widespread attention by recruiting users with mobile devices to collect crowdsourcing data. Existing research on MCS assumes that the platform has sufficient users. However, platforms in their early stages of development face the cold start problem, which can lead to their inability to grow or even result in bankruptcy. While some studies try to solve it by recruiting users through social networks to participate in crowdsourcing tasks, they only focus on how to recruit more users without addressing the issue of user retention. This can lead to an increasing proportion of users losing interest in the platform and dropping out and thus it fails to solve the cold start problem truly. In light of this, we present a task recommendation-based method to recruit new users via the social network and keep registered users active on the platform. Specifically, we first use an extended independent cascade model to describe the recruitment of users through social networks. Secondly, we use a task acceptance model to describe user decisions. Finally, we utilize a fuzzy control system that incorporates spatiotemporal crowdsourcing information to predict user behaviour and recommend tasks to users most likely to complete them. Extensive experiments on large-scale real datasets were conducted to evaluate the proposed solution. The results indicate that compared to existing methods such as SocialRecruiter, our solution reduces the 30-day average user churn rate by 23.90% while significantly boosting user retention and task completion rates by up to 23.73% and 48.7%, respectively.}
}


@article{DBLP:journals/ton/Zhang0DP0QX0025,
	author = {Yongzhao Zhang and
                  Hao Pan and
                  Dian Ding and
                  Yue Pan and
                  Yi{-}Chao Chen and
                  Lili Qiu and
                  Guangtao Xue and
                  Ting Chen and
                  Xiaosong Zhang},
	title = {SwiftTrack+: Fine-Grained and Robust Fast Hand Motion Tracking Using
                  Acoustic Signal},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {685--700},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3504517},
	doi = {10.1109/TNET.2024.3504517},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Zhang0DP0QX0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Acoustic tracking technology, leveraging the ubiquitous presence of speakers and microphones in commercial off-the-shelf (COTS) mobile devices, has become a versatile tool across various applications. However, current phase-based acoustic tracking methods encounter significant limitations in tracking fast movements, thereby restricting their practical utility. This paper identifies three practical challenges to enable fast hand motion tracking using acoustic signals: 1) high mobility, 2) low signal-to-noise ratio (SNR), and 3) variations in hardware frequency response. The high mobility introduces Doppler shift and phase ambiguity which is the primary cause of failure in fast movement tracking, while the latter two factors can further impair the tracking performance in practical scenarios involving high mobility. To address the high mobility issue, we effectively compensate the Doppler shift in the Channel Impulse Response (CIR) for better selection of channel taps and then propose a novel phase derivative approach to mitigate the phase ambiguity. To enhance the real-world robustness, we integrate multiple algorithms including an SNR enhancement algorithm inspired by time-domain beamforming and a hardware frequency response compensation approach that addresses both amplitude and phase distortions. Additionally, an LSTM-based distance reconstruction algorithm is further implemented to correct residual phase noise. Implemented on Android platforms under the name SwiftTrack+, our system demonstrates superior performance in tracking fast movements. Through extensive evaluations, SwiftTrack+ proves its efficacy across diverse scenarios, significantly broadening the scope and reliability of acoustic tracking applications.}
}


@article{DBLP:journals/ton/LiD25,
	author = {Songhua Li and
                  Lingjie Duan},
	title = {Age of Information Diffusion on Social Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {701--712},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3500357},
	doi = {10.1109/TNET.2024.3500357},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To promote viral marketing, major social platforms (e.g., Facebook Marketplace and Pinduoduo) repeatedly select and invite different users (as seeds) in online social networks to share fresh information about a product or service with their friends. Thereby, we are motivated to optimize a multi-stage seeding process of viral marketing in social networks, and adopt the recent notions of the peak and the average age of information (AoI) to measure the timeliness of promotion information received by network users. Our problem is different from the literature on information diffusion in social networks, which limits to one-time seeding and overlooks AoI dynamics or information replacement over time. As a critical step, we manage to develop closed-form expressions that characterize and trace AoI dynamics over any social network. For the peak AoI problem, we first prove the NP-hardness of our multi-stage seeding problem by a highly non-straightforward reduction from the dominating set problem, and then present a new polynomial-time algorithm that achieves good approximation guarantees (e.g., less than 2 for linear network topology). To minimize the average AoI, we also prove that our problem is NP-hard by properly reducing it from the set cover problem. Benefiting from our two-sided bound analysis on the average AoI objective, we build up a new framework for approximation analysis and link our problem to a much simplified sum-distance minimization problem. This intriguing connection inspires us to develop another polynomial-time algorithm that achieves a good approximation guarantee. Additionally, our theoretical results are well corroborated by experiments on a real social network.}
}


@article{DBLP:journals/ton/ZhuZXHQ25,
	author = {Qiucheng Zhu and
                  Yangming Zhao and
                  Hongli Xu and
                  Liusheng Huang and
                  Chunming Qiao},
	title = {Beyond Entanglement Routing: Source Assignment and All-Optical Switching-Based
                  Distribution},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {713--728},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3499358},
	doi = {10.1109/TNET.2024.3499358},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhuZXHQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Entanglement routing plays a vital role in distributed quantum computing and quantum networks. Previous works on entanglement routing have addressed several design challenges due to limited quantum resources, failures to establish entanglement, and decoherence of established entanglement, but considered neither the limitations imposed by having a limited number of entangled photon sources (EPSes), nor the benefits of using all-optical (or quantum) switching to distribute entangled photons. In this paper, we first explore the problem of jointly optimizing entanglement routing and EPS assignment, assuming no all-optical switching capability. In other words, a pair of entangled photons generated by one EPS can be distributed to two neighboring quantum nodes. We then relax the above assumption so as to allow a pair of entangled photons generated by one EPS to be distributed to non-adjacent nodes using all-optical switching. We propose two corresponding solutions, namely, entanglement routing and EPS assignment (or ERSA), and ERSA with all-optical switching-based distribution (or ERSA+D) that aim to maximize the number of entanglement connections between the given set of source-destination (SD) pairs while avoiding starvation and achieving fairness among the SD pairs. In order to obtain efficient solutions in a large discrete solution space in a timely manner, we first formulate each optimization problem as an Integer Linear Programming (ILP), and then propose efficient algorithms to derive near-optimal solutions based on relaxation, Lagrangian decomposition, duality iteration, and rounding techniques. Extensive simulations show that ERSA+D can increase network throughput by up to 1652% and 113%, respectively, thanks to EPS assignment optimization, and all-optical switching based entanglement distribution.}
}


@article{DBLP:journals/ton/Li00SQHC25,
	author = {Xionglve Li and
                  Chengyu Wang and
                  Tao Yang and
                  Ao Shen and
                  Zhenyu Qiu and
                  Bingnan Hou and
                  Zhiping Cai},
	title = {Realizing Personalized and Adaptive Inference of {AS} Paths With a
                  Generative and Measurable Process},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {729--744},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3506156},
	doi = {10.1109/TNET.2024.3506156},
	timestamp = {Tue, 14 Oct 2025 19:49:17 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Li00SQHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the global Internet, understanding paths between autonomous systems (ASes) is valuable for improving the Internet routing system and optimizing various applications. However, due to the business and privacy concerns, only a small portion of paths are disclosed. Moreover, limited by the measurement resources, obtaining paths between any two ASes is impossible. Thus, path inference becomes necessary. Recent work proposes training individual model for each AS to infer paths, but it lacks personalization as it uses a shared approach and data for arbitrary ASes. Moreover, training models from scratch for all the ASes is time-consuming and resource-intensive. This paper introduces Personalized and Adaptive Generative Measurable Path Inference (PA-GMPI), a prefix-grained path inference process. PA-GMPI is capable of achieving superior performance and faster model training by fully leveraging the exclusive information of each AS. These improvements come from a personalized path generator, a 3-layer graph kernel based adaptive training warm-starter, and a real-world walks based AS representation learner. In evaluation, PA-GMPI significantly outperforms the state-of-the-art method, achieving a maximal accuracy improvement of 28.72% and ESR (exact same ratio) improvement of 49.95%. Furthermore, PA-GMPI achieves an average reduction of 20.21% in training resource consumption across over two thousand training sessions, using vantage ASes from five snapshots, which included 439 distinct ASes.}
}


@article{DBLP:journals/ton/DinhGHFKW25,
	author = {Phuc Dinh and
                  Moinak Ghoshal and
                  Yunmeng Han and
                  Yufei Feng and
                  Dimitrios Koutsonikolas and
                  Joerg Widmer},
	title = {Demystifying Resource Allocation Policies in Operational 5G mmWave
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {745--760},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3514787},
	doi = {10.1109/TON.2024.3514787},
	timestamp = {Fri, 17 Oct 2025 07:35:07 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DinhGHFKW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Five years after the initial 5G rollout, several research works have analyzed the performance of operational 5G mmWave networks. However, these measurement studies primarily focus on single-user performance, leaving the sharing and resource allocation policies largely unexplored. In this paper, we fill this gap by conducting the first systematic study, to our best knowledge, of resource allocation policies of current 5G mmWave mobile network deployments through an extensive measurement campaign across four major US cities and two major mobile operators. Our study reveals that resource allocation among multiple flows is strictly governed by the cellular operators and flows are not allowed to compete with each other in a shared queue. Operators employ simple threshold-based policies and often over-allocate resources to new flows with low traffic demands or reserve some capacity for future usage. Interestingly, these policies vary not only among operators but also for a single operator in different cities. We also discuss a number of anomalous behaviors we observe in our experiments across different cities and operators.}
}


@article{DBLP:journals/ton/Wang0ZFD25,
	author = {Jingrong Wang and
                  Ben Liang and
                  Zhongwen Zhu and
                  Emmanuel Thepie Fapi and
                  Hardik Dalal},
	title = {Communication-Efficient Network Topology in Decentralized Learning:
                  {A} Joint Design of Consensus Matrix and Resource Allocation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {761--776},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3511333},
	doi = {10.1109/TNET.2024.3511333},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Wang0ZFD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In decentralized machine learning over a network of workers, each worker updates its local model as a weighted average of its local model and all models received from its neighbors. Efficient consensus weight matrix design and communication resource allocation can increase the training convergence rate and reduce the wall-clock training time. In this paper, we jointly consider these two factors and propose a novel algorithm termed Communication-Efficient Network Topology (CENT), which reduces the latency in each training iteration by removing unnecessary communication links. CENT enforces communication graph sparsity by iteratively updating, with a fixed step size, a trade-off factor between the convergence factor and a weighted graph sparsity. We further extend CENT to one with an adaptive step size (CENT-A), which adjusts the trade-off factor based on the feedback of the objective function value, without introducing additional computation complexity. We show that both CENT and CENT-A preserve the training convergence rate while avoiding the selection of poor communication links. Numerical studies with real-world machine learning data in both homogeneous and heterogeneous scenarios demonstrate the efficacy of CENT and CENT-A and their performance advantage over state-of-the-art algorithms.}
}


@article{DBLP:journals/ton/000100K00025,
	author = {Hongyan Liu and
                  Xiang Chen and
                  Qun Huang and
                  Dezhang Kong and
                  Dong Zhang and
                  Chunming Wu and
                  Xuan Liu},
	title = {Elastically Scaling Control Channels in Network Measurement With Escala},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {777--792},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3504578},
	doi = {10.1109/TNET.2024.3504578},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/000100K00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In network measurement, data plane switches measure traffic and report events (e.g., heavy hitters) to the control plane via control channels. The control plane makes decisions to process events. However, current network measurement suffers from two problems. First, when traffic bursts occur, massive events are reported in a short time so that the control channels may be overloaded due to limited bandwidth capacity. Second, only a few events are reported in normal cases, making control channels underloaded and wasting network resources. In this paper, we propose   \\textsf {Escala} \\textsf {Escala}   to provide the elastic scaling of control channels at runtime. The key idea is to dynamically migrate event streams among control channels to regulate the loads of these channels.   \\textsf {Escala} \\textsf {Escala}   offers two components, including an   \\textsf {Escala} \\textsf {Escala}   monitor that detects scaling situations based on realtime network statistics, and an optimization framework that makes scaling decisions to eliminate overload and underload situations. We have implemented a prototype of   \\textsf {Escala} \\textsf {Escala}   on Tofino-based switches. Extensive experiments show that   \\textsf {Escala} \\textsf {Escala}   achieves timely elastic scaling while preserving high application-level accuracy.}
}


@article{DBLP:journals/ton/Zhang0Z0025,
	author = {Yining Zhang and
                  Lei Jiao and
                  Konglin Zhu and
                  Xiaojun Lin and
                  Lin Zhang},
	title = {Toward Market-Assisted {AI:} Cloud Inference for Streamed Data via
                  Model Ensembles From Auctions},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {793--806},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3505252},
	doi = {10.1109/TNET.2024.3505252},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Zhang0Z0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While ensemble methods can tackle concept drifts, obtaining pretrained models and conducting ensemble learning upon streamed data impose fundamental challenges, including the dynamic balance between system overhead and inference accuracy in uncertain system environments, and the interlacement between desired economic properties and long-term participation. In this paper, we propose the joint optimization which enables service providers to obtain models via repetitive auctions from the model providers and conduct ensemble methods online in a cost-efficient manner. We design polynomial-time online algorithms to solve the underlying non-linear mixed-integer social cost minimization problem, involving bid selection, payment allocation, model hosting, and ensemble model-weight adaption. We further rigorously prove the performance guarantees with our approach, such as the sub-linear dynamic regret for the bidding cost, the sub-linear dynamic fit for the long-term participation constraint, the truthfulness and the individual rationality for the auctions, the upper bound for ensemble inference loss, and the parameterized-constant competitive ratio for the long-term social cost. Through extensive trace-driven evaluations under real-world settings, we have validated the significant advantages of our approach over multiple baselines and state-of-the-art algorithms.}
}


@article{DBLP:journals/ton/SokolikNR25,
	author = {Yaakov Sokolik and
                  Mohammad Nassar and
                  Ori Rottenstreich},
	title = {Age-Aware Fairness in Blockchain Transaction Ordering for Reducing
                  Tail Latency},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {807--822},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3503758},
	doi = {10.1109/TNET.2024.3503758},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SokolikNR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In blockchain networks, transaction latency is crucial for determining the quality of service (QoS). The latency of a transaction is measured as the time between its issuance and its inclusion in a block in the chain. A block proposer often prioritizes transactions with higher fees or transactions from accounts it is associated with, to minimize their latencies. To maintain fairness among transactions, a block proposer is expected to select the included transactions randomly. The random selection might cause some transactions to experience high latency following the variance in the time a transaction waits until it is selected. We suggest an alternative, age-aware approach towards fairness so that transaction priority is increased upon observing a large waiting time. We explain that a challenge with this approach is that the age of a transaction is not absolute due to transaction propagation. Moreover, a node might present its transactions as older to obtain priority. We describe a new technique to enforce a fair block selection while prioritizing transactions that observed high latency. The technique is based on various declaration schemes in which a node declares its pending transactions, providing the ability to validate transaction age. By evaluating the solutions on Ethereum data and synthetic data of various scenarios, we demonstrate the advantages of the approach under realistic conditions and understand its potential impact to maintain fairness and reduce tail latency.}
}


@article{DBLP:journals/ton/0001S25,
	author = {Pei Peng and
                  Emina Soljanin},
	title = {Redundancy Management for Fast Service (Rates) in Edge Computing Systems},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {823--834},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3507372},
	doi = {10.1109/TNET.2024.3507372},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0001S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing operates between the cloud and end users and strives to provide low-latency computing services for simultaneous users. Redundant use of multiple edge nodes can reduce latency, as edge systems often operate in uncertain environments. However, since edge systems have limited computing and storage resources, directing more resources to some computing jobs will either block the execution of others or pass their execution to the cloud, thus increasing latency. This paper uses the average system computing time and blocking probability to evaluate edge system performance and analyzes the optimal resource allocation accordingly. We also propose blocking probability and average system time optimization algorithms. Simulation results show that both algorithms significantly outperform the benchmark for different service time distributions and show how the optimal replication factor changes with varying parameters of the system.}
}


@article{DBLP:journals/ton/0031WXL0YS025,
	author = {Jian Li and
                  Zhaoying Wang and
                  Kaiping Xue and
                  Zhonghui Li and
                  Ruidong Li and
                  Nenghai Yu and
                  Qibin Sun and
                  Jun Lu},
	title = {{DRM-ETP:} {A} Dynamic Rate Matching-Based Entanglement Transport
                  Protocol in Quantum Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {835--848},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3507785},
	doi = {10.1109/TNET.2024.3507785},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0031WXL0YS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The entanglement transport protocol with a connection-oriented mode ensures the reliable distribution of remote entanglement by reserving dedicated resources on the selected path for users in a quantum network. In most existing protocols, entanglement generation and resource allocation operate with the support of global network-synchronized time slot. However, such synchronization in a large-scale quantum network is challenging, and the idealized time slot model is not conducive to continuous and concurrent requests. Meanwhile, different link performance in memory capacity and entanglement generation rate brings out critical issues, such as long distribution delay and low resource utilization, which has not been adequately addressed by the existing protocols relying on a heuristic adoption of TCP-like transport modes. In light of these observations, we propose a dynamic rate matching-based entanglement transport protocol called DRM-ETP, which allocates different memory units on each link along an entanglement distribution path. Moreover, DRM-ETP incorporates periodic forward and backward interactions to implement fine-grained feedback and a dynamic memory allocation based on priority differentiation. These mechanisms mitigate congestion and unfairness arising from resource contention among burst requests on shared links. Extensive simulation results demonstrate that DRM-ETP significantly outperforms the existing protocols in terms of throughput and resource utilization, with less distribution delay and higher fidelity. Moreover, DRM-ETP exhibits rapid and fair convergence when handling burst requests. Our study opens up possibilities for deploying efficient entanglement transport in quantum networks, thereby holding the promise of enhanced compatibility and novel functionality.}
}


@article{DBLP:journals/ton/LinGHHC25,
	author = {Limei Lin and
                  Kaineng Guan and
                  Yanze Huang and
                  Sun{-}Yuan Hsieh and
                  Gaolin Chen},
	title = {Local Fault Diagnosis Analysis Based on Block Pattern of Regular Diagnosable
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {849--864},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3507152},
	doi = {10.1109/TNET.2024.3507152},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LinGHHC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fault diagnosability can reflect the actual self diagnosing capability of a multiprocessor system better. However, people usually focus on the overall information and neglect the important local information. In order to reflect the locality of a system at a node better, this paper proposes a novel fault diagnosis strategy, called x-block local fault diagnosability (x-BLFD), where the x-block condition requires more than x connected fault-free nodes. Then, we characterize some important properties about the x-BLFD of multiprocessors interconnected networks under the Preparata/Metze/Chien model (P/M/C), and further propose the x-BLFD in an   f(x) f(x)  -extended block network with the minimum   (x+1) (x+1)  -subnetwork degree at some node. We also establish an approximate algorithm to calculate the x-BLFD of a large-scale diagnosable network at some node, and analyze the experimental performance of large-scale networks. Furthermore, we apply our proposed conclusion to obtain the x-BLFD of 16 well-known networks at some node directly under P/M/C, including dual cubes, hierarchical cubic networks, DQcubes, twisted hypercubes, Bicube networks, crossed cubes, folded hypercubes, k-ary n-cubes, balanced hypercubes, BC graphs,   (n,k) (n,k)  -star graphs, Cayley graphs generated by transposition trees, bubble-sort star graphs, split-star networks, data center networks, and   (n,k) (n,k)  -arrangement graphs. Finally, we compare the x-BLFD with the diagnosability, conditional diagnosability, pessimistic diagnosability, and   t/k t/k  -diagnosability by a large number of detailed numerical analysis. It can be seen that the x-BLFD is greater than all the other types of fault diagnosabilities.}
}


@article{DBLP:journals/ton/YueHDC0Z25,
	author = {Sheng Yue and
                  Xingyuan Hua and
                  Yongheng Deng and
                  Lili Chen and
                  Ju Ren and
                  Yaoxue Zhang},
	title = {Momentum-Based Contextual Federated Reinforcement Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {865--880},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3510352},
	doi = {10.1109/TNET.2024.3510352},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YueHDC0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Reinforcement Learning (FRL) is an attractive edge learning paradigm for decision-making applications, which has garnered significant interest recently. However, owing to the inherent spatio-temporal non-stationarity of local state-action distributions, current FRL approaches typically suffer from high interaction and communication costs. In this paper, we introduce a new FRL method, which incorporates momentum, importance sampling, and server-side adjustments, capable of controlling the gradient shifts induced by the non-stationary data. We prove that by proper selection of momentum parameters and interaction frequency, it can achieve   \\tilde {\\mathcal {O}}(H N^{-1}\\epsilon ^{-3/2}) \\tilde {\\mathcal {O}}(H N^{-1}\\epsilon ^{-3/2})   and   \\tilde {\\mathcal {O}}(\\epsilon ^{-1}) \\tilde {\\mathcal {O}}(\\epsilon ^{-1})   interaction and communication complexities (N represents the agent number), where the interaction complexity achieves linear speedup with the number of agents, and the communication complexity aligns with the best achievable among existing first-order FL algorithms. Further, we leverage attention-based contextual representation extraction to enable the learning policy to adapt to heterogeneous tasks and environments. Extensive experiments demonstrate that our proposed method significantly outperforms existing baselines on a range of complex, high-dimensional single-task and multi-task benchmarks.}
}


@article{DBLP:journals/ton/VadnereHSLZ25,
	author = {Neha Vadnere and
                  Dijiang Huang and
                  Abdulhakim Sabur and
                  Jim Luo and
                  Ming Zhao},
	title = {Waterfall: Fast Network Flow Rules Checking and Conflict Resolution},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {881--896},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3508464},
	doi = {10.1109/TNET.2024.3508464},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/VadnereHSLZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Networking (SDN) enables a centralized manageable framework to control network devices and their policies using device-specific flow rules. When administrators deploy flow rules to support business policies, the network controller checks them against existing rules to detect conflicts and ensure consistency, security, and functionality in the data plane. Existing offline conflict detection methods are not scalable due to state explosion and often lead to networking chaos due to inefficiency. This paper presents Waterfall, designed to minimize the number of flow rule-checking operations. We propose a novel Equivalence Class (EC) creation and prioritization technique that simplifies conflict detection by organizing rules with similar patterns and processing them accordingly. Analogous to a multi-stage waterfall, our algorithm optimizes downstream stages by reducing unnecessary comparisons, ensuring efficient conflict detection. Our comprehensive evaluation demonstrates Waterfall’s effectiveness through significant reductions in computation time ( $O(mKH)$ , where m is the number of matched flow-rules which is far less than the total number of flow-rules, K is the number of attributes (headers) in flow rules, H is the number of hash functions in Bloom filter for attribute matching), making it ideal for real-time flow rule checking and conflict resolution in SDN environments. In our evaluation, Waterfall achieved a remarkable 1.3X improvement in conflict detection and 4.4X improvement for conflict resolution over the state-of-the-art solution for the Stanford topology which is a popular topology to represent real-world networking scenarios. We also evaluate the scalability of the solution using a synthetic dataset containing 15K flow rules that have three virtual network functions. Our solution achieved a  $90.53~\\mu $  s conflict detection and resolution time for the large synthetic dataset. This lightweight approach promises substantial benefits for real-time flow rule checking in SDN environments.}
}


@article{DBLP:journals/ton/FengZLC0025,
	author = {Yong Feng and
                  Hanyi Zhou and
                  Shuxin Liu and
                  Zhikang Chen and
                  Haoyu Song and
                  Bin Liu},
	title = {Enhancing Stateful Processing in Programmable Data Planes: Model and
                  Improved Architecture},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {897--912},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3510638},
	doi = {10.1109/TNET.2024.3510638},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FengZLC0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stateful data plane network applications are indispensable but their efficiency is hindered by prevalent network device architectures which utilize a Blocking Scheme to maintain state consistency. The Blocking Scheme results in poor throughput and latency performance due to its frequent halts during packet processing. In response to this issue, we propose an innovative Non-Blocking Scheme and construct a theoretical model based on the G/GI/m queueing model. The new scheme leverages the speculative execution method, avoiding unnecessary blocking by taking advantage of the fact that the state update ratio is usually much smaller than the incoming packet rate. In case of speculation failures, the affected packets are reprocessed to guarantee state consistency. We provide an approximate model for the Blocking Scheme, and show that, even with relaxed approximations, the Blocking Scheme still performs worse than the Non-Blocking Scheme. The superior performance of the Non-Blocking Scheme is further corroborated through rigorous simulations, conducted under both realistic and synthetic traces. Based on our model, we propose an enhanced architecture: SN_RAPID (Sequence Number and unidirectional Reverse path-Augmented PIpeline Dataplane), to support speculative execution. The architecture is simpler than the other architectures supporting stateful network functions. Serving as a design foundation for future iterations, we implement a prototype of SN_RAPID in FPGA which can run at line speed, and also develop a software ASIC emulator. The experiments show the superiority of the improved architecture.}
}


@article{DBLP:journals/ton/00790ZQ025,
	author = {Jie Chen and
                  Yong Ding and
                  Rongpei Zhou and
                  Zhifeng Qiu and
                  Weihua Gui},
	title = {A Distributed Symmetric Game Optimization to 3-Path Vertex Cover of
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {2},
	pages = {913--928},
	year = {2025},
	url = {https://doi.org/10.1109/TNET.2024.3511598},
	doi = {10.1109/TNET.2024.3511598},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/00790ZQ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a typical combinatorial optimization problem, the 3-path vertex cover problem has wide applications in practice. To solve the 3-path vertex cover problem from the perspective of distributed optimization, we treat each vertex as an agent (i.e., player) with computation, and decision-making capabilities. First, we establish a 3-player symmetric game model to describe the 3-path vertex cover problem, and design the corresponding cost function for each player. Then, we prove that under the established game model, strict Nash equilibriums (SNEs) act as the basis of the connection between 3-path vertex cover states and minimum 3-path vertex cover states. Next, we propose a novel memory-based synchronous learning (MSL) algorithm, where the initial profile strategy generation of players relies on the designed degree preference rule, and each player has a memory length for recording strategies and independently update their strategies concurrently based on the accessed local information. After that, we prove that our proposed MSL algorithm can guarantee that any strategy profile converges to an SNE, and provide a theoretical analysis of the algorithm’s complexity. Finally, we present numerous numerical simulations to demonstrate the performance of our proposed algorithm on various networks. Moreover, we find that increasing the memory length and adopting the degree preference initialization can yield a better SNE.}
}


@article{DBLP:journals/ton/RehmKR25,
	author = {Hunter Rehm and
                  Robert Kassouf{-}Short and
                  Puck Rombach},
	title = {Generating Dominating Sets Using Locally Defined Centrality Measures},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {930--937},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3524941},
	doi = {10.1109/TON.2024.3524941},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RehmKR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The dominating set problem has many practical applications but is well-known to be NP-hard. Therefore, there is a need for efficient heuristic algorithms, especially in applications such as ad hoc wireless networks. Most distributed algorithms proposed in the literature assume that each node has knowledge of the network structure. We propose a distributed heuristic algorithm that uses two rounds of communication, and where each node has only local information, both in terms of network structure and dominating set assignment. First, each node calculates a local centrality measure to determine whether it is part of the dominating set D. The second round guarantees D is a dominating set by adding any non-dominated nodes. We compare several centrality measures and show that the Shapley centrality, derived from the Shapley value in game theory, is theoretically motivated and performs well in practice on several synthetic and real-world networks.}
}


@article{DBLP:journals/ton/ZhuLL0W25,
	author = {Xueying Zhu and
                  Yingtao Li and
                  Xiang Li and
                  Jialin Li and
                  Zeke Wang},
	title = {Hare: {A} Systematic Framework for Efficient and Generally Automatic
                  Hotspot Offloading on Programmable Switches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {938--953},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3524580},
	doi = {10.1109/TON.2024.3524580},
	timestamp = {Fri, 17 Oct 2025 07:35:07 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhuLL0W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Switch-based hotspot offloading is a trendy solution for latency-sensitive applications to achieve high system throughput with an acceptable P99 query response latency. However, due to the varying object sizes, dynamic workloads, and complex query-processing functions of the latency-sensitive applications, existing switch-based dynamic hotspot offloading approaches struggle to handle these applications effectively. This is mainly because of their inefficient switch resource utilization and non-generalizable hotspot offloading designs. So we propose Hare, a systematic framework that consists of three techniques to address these issues. First, Hare uses a MAT-based cross-stage structure to store and perform hit-checks for large hotspots on the switch data plane. Second, Hare uses a switch-server co-offloading mechanism to support fast and precise offloading. Third, Hare is designed to enable generally automatic offloading by decoupling application-related query processing with hotspot offloading. Compared to the state-of-the-art approaches, Hare supports   8.86\\times \\sim 9.97\\times  8.86\\times \\sim 9.97\\times    larger hotspot size, achieves   1.27 \\times \\sim 6.61 \\times  1.27 \\times \\sim 6.61 \\times    higher system throughput, and can recover the system throughput and the P99 query response latency within 8s.}
}


@article{DBLP:journals/ton/Shi025,
	author = {Zai Shi and
                  Jian Tan},
	title = {Autoscaling via Online Optimization With Switching Cost Constraints},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {954--965},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3523506},
	doi = {10.1109/TON.2024.3523506},
	timestamp = {Tue, 14 Oct 2025 19:49:17 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Shi025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In cloud services, autoscaling is one of the most important features, which enables system intelligence to adaptively assign computing resources for users according to real-time workloads and quality of service requirements. In our paper, we treat the process of autoscaling as a sequential decision problem of online optimization with switching cost constraints and propose two algorithms to solve it using noisy predictions for future workloads. Particularly, these two algorithms, which are called C-AFHC and SC-AFHC respectively, are designed for different situations according to a parameter of the constraints. Both of them have theoretical guarantees in terms of our well-defined performance metrics. Using real workload data collected from an enterprise Cloud Service, we demonstrate the performance of our algorithms in different scenarios of autoscaling problems.}
}


@article{DBLP:journals/ton/Mei0Y25,
	author = {Hantao Mei and
                  Guang Cheng and
                  Yali Yuan},
	title = {High Precision and Efficient Anonymous Traffic Classification in the
                  Real-World},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {966--981},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3518976},
	doi = {10.1109/TON.2024.3518976},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Mei0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Various Traffic Classification (TC) technologies have been developed to de-anonymize anonymous tools, such as Tor, the most popular communication anonymous system. Although current TC methods boast high performance in closed-world scenarios, they frequently encounter challenges when dealing with the low base rate of anonymous traffic in the real open world, a phenomenon referred to as the base rate fallacy. In this paper, we introduce HPETC, an anonymous traffic classification system tailored for real-world scenarios, with a focus on achieving high precision, even in the presence of extremely low rates of anonymous traffic within expansive network environments. HPETC comprises an online classifier that efficiently filters anonymous traffic with minimal resource requirements, alongside an offline classifier responsible for extracting detailed information to support fine-grained classification. In response to the base rate fallacy, we introduce three Enhanced Techniques to enhance the performance of the classifiers within HPETC. Experimental findings illustrate that HPETC markedly diminishes resource consumption and greatly enhances the actual precision in comparison to state-of-the-art methods. Remarkably, in scenarios characterized by an extremely low rate of anonymous traffic (non-Tor/Tor=1000), our HPETC demonstrates an actual precision improvement that exceeds eightfold when benchmarked against commonly utilized models, specifically the Random Forest (RF) and Convolutional Neural Network (CNN) models.}
}


@article{DBLP:journals/ton/Dai0Z0JL25,
	author = {Xiangxiang Dai and
                  Xutong Liu and
                  Jinhang Zuo and
                  Hong Xie and
                  Carlee Joe{-}Wong and
                  John C. S. Lui},
	title = {Variance-Aware Bandit Framework for Dynamic Probabilistic Maximum
                  Coverage Problem With Triggered or Self-Reliant Arms},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {982--993},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3519568},
	doi = {10.1109/TON.2024.3519568},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Dai0Z0JL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Probabilistic Maximum Coverage (PMC) problem plays a pivotal role in modeling various network applications, such as mobile crowdsensing, which involves selecting nodes within a graph that probabilistically cover other nodes. Our study focuses on PMC within the framework of online learning, termed the PMC bandit, where the network parameters are initially unknown. In this scenario, the decision-maker is tasked with learning these parameters to maximize the cumulative rewards from covered nodes. Despite prior research on the PMC bandit, we propose a novel variant, dynamic PMC-G bandit, which extends the semi-bandit feedback model to represent applications more accurately. To tackle the complexities of the time-varying combinatorial arm set rather than traditional static, we enhance the Combinatorial Upper Confidence Bound (CUCB) algorithms by developing two innovative variance-aware strategies: the Variance-Adaptive Combinatorial Upper Confidence Bound (VACUCB) for probabilistically triggered arms, and the Action-Based Combinatorial Upper Confidence Bound (ABCUCB) for self-reliant arms, i.e., independent arms with probabilistically triggered outcomes. Based on variance-aware properties, our contributions notably reduce the dependence on the number of nodes K selected per round, demonstrating that: (i) VACUCB effectively minimizes the regret associated with the triggered arms, enhancing the CUCB by a factor of   \\tilde {O}(K) \\tilde {O}(K)  ; (ii) ABCUCB further diminishes the dependence on K in the leading term. Empirical results from synthetic and real-world datasets confirm that our proposed algorithms outperform current benchmarks in three network applications.}
}


@article{DBLP:journals/ton/0002QZ0025,
	author = {Qian Ma and
                  Yanling Qin and
                  Chaohui Zhu and
                  Lin Gao and
                  Xu Chen},
	title = {Joint Resource Trading and Task Scheduling in Edge-Cloud Computing
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {994--1008},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3516381},
	doi = {10.1109/TON.2024.3516381},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0002QZ0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge-cloud computing networks integrate dispersed computing resources of edges and clouds through networks, which improves resource utilization by flexibly scheduling tasks to suitable computing nodes. The performance of edge-cloud computing networks depends significantly on the amount of computing resources and the task scheduling scheme. In this work, we propose a novel computing resource trading and task scheduling framework for edge-cloud computing networks with arbitrary network topology. Specifically, we consider a third-party platform which incentivizes computing nodes to share computing resources by designing proper resource pricing mechanisms, and charges customers execution fees by scheduling tasks optimally in the edge-cloud computing network. The platform’s resource pricing and task scheduling optimization problem captures the unique features of edge-cloud computing networks including the heterogeneities of computing resources and tasks, as well as the multi-hop offloading in arbitrary topology, which is challenging to solve. We solve the problem for the homogeneous workload scenario and the heterogeneous workload scenario, respectively. For the homogeneous workload scenario, we propose a multi-round proposer-voter algorithm (MPV) that achieves the global optimum in polynomial time for the non-competitive case. For the heterogeneous workload scenario, we first propose a Gibbs sampling based iterative algorithm (GSI), which updates task scheduling strategies iteratively using Gibbs sampling and converges to the global optimum with high probability. We further propose a distributed alternating update algorithm (DAU), which converges to the local optimum in a distributed manner with linear complexity. Numerical results demonstrate the effectiveness of our proposed resource trading and task scheduling schemes.}
}


@article{DBLP:journals/ton/00020KWY0X00L25,
	author = {Shibo Wang and
                  Jianjun Xiao and
                  Xiao Kong and
                  Chenglei Wu and
                  Shusen Yang and
                  Cong Zhao and
                  Chenren Xu and
                  Hong Xu and
                  Jing Wang and
                  Honghao Liu},
	title = {A Practical Congestion Control Algorithm for Low-Latency Interactive
                  Video Streaming},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1009--1024},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3519902},
	doi = {10.1109/TON.2024.3519902},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/00020KWY0X00L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion control (CC) plays a pivotal role in low-latency interactive video streaming such as cloud gaming. However, existing end-to-end CC methods often cause self-induced network queuing. As a result, they may largely delay video frame transmission and undermine the user’s quality of experience. In this paper, we present a new, practical CC algorithm named Pudica that strives to achieve near-zero queuing delay and high link utilization while respecting cross-flow fairness. Pudica introduces several judicious approaches to utilize the paced frame to probe the bandwidth utilization ratio (BUR) instead of bandwidth itself. By leveraging BUR estimations, Pudica designs a holistic bitrate adjustment policy to balance low queuing, efficiency, and fairness. We conducted thorough and comprehensive evaluations in real production networks. In comparison to the state-of-the-art methods, Pudica reduces the average and tailed frame delay by   3.1\\times  3.1\\times    and   5.1\\times  5.1\\times   , respectively. Meanwhile, it increases the frame bitrate by 12.1%. Pudica has been deployed in a large-scale cloud gaming platform, currently serving millions of players.}
}


@article{DBLP:journals/ton/CuiSZLW25,
	author = {Laizhong Cui and
                  Xiaoxin Su and
                  Yipeng Zhou and
                  Jiangchuan Liu and
                  Shiting Wen},
	title = {Toward Optimized Federated Learning With Compressed Communications
                  by Rate Adaption},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1025--1040},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3520530},
	doi = {10.1109/TON.2024.3520530},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CuiSZLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is known that federated learning (FL) incurs heavy communication overhead for model training by exchanging model updates between clients and the parameter server (PS) over the Internet for multiple rounds. Compressing model updates is an effective approach to alleviating communication overhead in FL. Yet the tradeoff between compression and model accuracy in the networked environment remains unclear and, for simplicity, most implementations adopt a fixed compression rate only during the entire learning process. In this paper, we for the first time systematically examine this tradeoff, explicitly quantifying the relation between the compression error, the final model accuracy and the learning rate. Specifically, we factor the compression error of each global iteration into the convergence rate analysis under non-convex loss for both unbiased and biased compression algorithms. We then present an adaptation framework to maximize the final model accuracy by strategically adjusting the compression rate in each iteration. We further discuss key implementation issues of our framework in practical networks with classical compression algorithms. Experiments over the most representative MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate that our solutions effectively shrink network traffic volume while maintain high model accuracy in FL.}
}


@article{DBLP:journals/ton/NamLLCHL25,
	author = {Wooseung Nam and
                  Sungyong Lee and
                  Jinsung Lee and
                  Huijeong Choe and
                  Sangtae Ha and
                  Kyunghan Lee},
	title = {N-Epitomizer: {A} Semantic Offloading Framework Leveraging Essential
                  Information for Timely Neural Network Inferences},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1041--1055},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3523891},
	doi = {10.1109/TON.2024.3523891},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NamLLCHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offloading neural network inferences from resource-constrained mobile devices to an edge server over wireless networks is becoming more crucial as the neural networks get heavier. To this end, recent studies have tried to make this offloading process more efficient. However, the most fundamental question on extracting and offloading the minimal amount of necessary information that does not degrade the inference accuracy has remained unanswered. We call such an ideal offloading semantic offloading and propose N-epitomizer, a new offloading framework that enables semantic offloading, thus achieving more reliable and timely inferences in highly-fluctuated or even low-bandwidth wireless networks. To realize N-epitomizer, we design an autoencoder-based scalable encoder trained to extract the most informative data and scale its output size to meet the latency and accuracy requirements of inferences over a network. We also accelerate N-epitomizer by exploiting light-weight knowledge distillation for the encoder design and decoder slimming for the decoder design, reducing its overall computation time significantly. Moreover, we extend our N-epitomizer to support multiple DNNs by extracting and offloading the union of the essential information required for each DNN. Our evaluation shows that N-epitomizer achieves exceptionally high compression for images without compromising inference accuracy, which is   21\\times  21\\times   ,   77\\times  77\\times   , and   192\\times  192\\times    higher than JPEG compression, and   20\\times  20\\times   ,   55\\times  55\\times   , and   86\\times  86\\times    higher than the state-of-the-art DNN-aware image compression GRACE for semantic segmentation, depth estimation, and classification, respectively. Our results show N-epitomizer’s strong potential as the first semantic offloading system to guarantee end-to-end latency even under highly varying cellular networks.}
}


@article{DBLP:journals/ton/HuangHXHCS025,
	author = {Zixu Huang and
                  Xuanbo Huang and
                  Kaiping Xue and
                  Jiangping Han and
                  Lutong Chen and
                  Qibin Sun and
                  Jun Lu},
	title = {Defending Against Link-Flooding Attacks With Adversary Interest Prediction
                  and Grouped Online Load Balancing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1056--1069},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3520512},
	doi = {10.1109/TON.2024.3520512},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangHXHCS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A Link Flooding Attack (LFA) is a type of link-aimed Distributed Denial of Service (DDoS) attack that can overwhelm the Internet critical links to cut off connections with lots of low-rate, seemingly benign traffic. To defend against such threats, a promising solution involves mitigating the attack through load balancing. However, adaptive attacks employ two effective means to circumvent existing load balancing strategies. The first is the frequent changing of targets, known as rolling attacks. Rolling attacks exploit the delay between attack detection feedback and the mitigation of load balancing, depleting the defender’s resources. The second is the strategical selection of target links to create the worst-case scenario for load balancing algorithms. To address these challenges, we propose LinkDam. Specifically, LinkDam adopts a proactive approach by tracking and predicting potential victim links, providing defense against all targets of rolling attacks. Subsequently, we introduce a robust load balancing strategy to prevent the exploitation of selected link combinations. Additionally, LinkDam introduces a partial deployment approach, demanding a mere 40% of nodes be programmable (i.e., SDN nodes) while maintaining an acceptable 10% performance reduction from the maximum achievable. The experimental results indicate that LinkDam surpasses an 80% accuracy threshold, and exhibits a 57% higher tolerance to attack budgets compared to state-of-the-art solutions.}
}


@article{DBLP:journals/ton/SuN00XHX025,
	author = {Qiang Su and
                  Zhixiong Niu and
                  Ran Shu and
                  Peng Cheng and
                  Yongqiang Xiong and
                  Dongsu Han and
                  Chun Jason Xue and
                  Hong Xu},
	title = {Low-Overhead Intra-Host Container Communication With Hardware Offloading},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1070--1085},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3520210},
	doi = {10.1109/TON.2024.3520210},
	timestamp = {Sun, 02 Nov 2025 12:34:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SuN00XHX025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Containers are widely embraced for their deployment and performance benefits over virtual machines. Yet, for many data-intensive applications in containerized clouds, bulky data transfers may impose performance issues. In particular, communication across co-located containers on the same host incurs large overheads in memory copy and the kernel’s TCP stack. Existing solutions such as shared-memory networking and RDMA have their own limitations, including insufficient memory isolation and limited scalability. This paper presents PipeDevice, a new system for low overhead intra-host container communication. PipeDevice follows a hardware-software co-design approach — it offloads data forwarding entirely onto hardware, which accesses application data in hugepages on the host, thereby eliminating CPU overhead from memory copy and TCP processing. PipeDevice preserves memory isolation and scales well to connections, making it deployable in public clouds. Isolation is achieved by allocating dedicated memory to each connection from hugepages. To achieve high scalability, PipeDevice stores the connection states entirely in host DRAM and manages them in software. Evaluation with a prototype implementation on commodity FPGA shows that for delivering 80Gbps across containers PipeDevice saves 63.2% CPU compared to kernel TCP stack, and 40.5% over FreeFlow. PipeDevice provides salient benefits to applications. For example, we port baidu-allreduce to PipeDevice and obtain   \\sim 2.2\\times  ∼ 2.2 × \\sim 2.2\\times    gains in allreduce throughput.}
}


@article{DBLP:journals/ton/ZhangZH25,
	author = {Qifan Zhang and
                  Shuming Zhou and
                  Sun{-}Yuan Hsieh},
	title = {A Probabilistic Approach for Local Diagnosis in Large Multiprocessor
                  Systems},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1086--1096},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3520303},
	doi = {10.1109/TON.2024.3520303},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Fault diagnosis is constantly crucial to maintain a high level of multiprocessor systems’ reliability. In multiprocessor systems, global fault diagnosis has been extensively investigated under both deterministic and probabilistic models, while local fault diagnosis has only been committed to the deterministic models, such as PMC model and MM  ^{*} ^{*}   model. This work focuses on a probabilistic approach for local diagnosis at a node within the mixed structure under the PMC diagnostic model so that the state of this node can be identified correctly by utilizing maximum a posteriori probability. This work is devoted to the quantitative metric on global reliability of multiprocessor systems in terms of local fault probability of node under microscale. The proposed strategy effectively reduces diagnostic delays and enhances system response in practical applications and thus improves the efficiency and accuracy of fault detection. In addition, the probabilistic approach reduces the effect of uncertainty on the fault diagnosis, which in turn improves the reliability and safety of the system. In this work, we first perform a more precise syndrome analysis for this mixed structure under the PMC model by virtue of local testing results, and suggest a modified local diagnosis algorithm called MLDA. Subsequently, we implement the maximum a posteriori probabilistic local diagnosis algorithm called MAPPLDA for the mixed structure under the probabilistic PMC diagnostic model. Finally, numerical simulation results confirm the effectiveness of the syndrome analysis approach and the maximum a posteriori probability approach for the mixed structure when the node failure probability is very small.}
}


@article{DBLP:journals/ton/YueZCMMF25,
	author = {Zhenshuai Yue and
                  Haoran Zhu and
                  Xiaolin Chang and
                  Jelena V. Misic and
                  Vojislav B. Misic and
                  Junchao Fan},
	title = {{MBCT:} {A} Monero-Based Covert Transmission Approach With On-Chain
                  Dynamic Session Key Negotiation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1097--1111},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3519860},
	doi = {10.1109/TON.2024.3519860},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YueZCMMF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traditional covert transmission (CT) approaches have been hindering CT application while blockchain technology offers new avenue. Current blockchain-based CT approaches require off-chain negotiation of critical information and often overlook the dynamic updating of session keys, which increases the risk of message and key leakage. Additionally, in some approaches the covert transactions exhibit obvious characteristics that can be easily detected by third-parties. Moreover, most approaches do not address the issue of decreased reliability of message transmission in blockchain attack scenarios. Bitcoin- and Ethereum-based approaches also have the issue of transaction linkability, which can be tackled by Monero-based approaches because of the privacy protection mechanisms in Monero. However, Monero-based CT has the problem of sender repudiation. In this paper, we propose a novel Monero-Based CT approach (MBCT), which enables on-chain session key dynamically updating without off-chain negotiation. MBCT can assure confidentiality of on-chain session key, non-repudiation of transmission parties, reliability of message transmission under blockchain attack, unlinkability and obscurity of covert transactions. They are achieved by the three components in MBCT, namely, a sender authentication method, a dynamically on-chain session key updating method and a state feedback method. We implement MBCT in Monero-0.18.1.0 and the experiment results demonstrate its high embedding capacity of MBCT.}
}


@article{DBLP:journals/ton/00030025,
	author = {Ming Shi and
                  Xiaojun Lin and
                  Lei Jiao},
	title = {Power-of-2-Arms for Adversarial Bandit Learning With Switching Costs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1112--1127},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3522073},
	doi = {10.1109/TON.2024.3522073},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/00030025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by edge computing with artificial intelligence, in this paper we study an adversarial bandit-learning problem with switching costs. Existing results in the literature either incur   \\Theta \\left ({{T^{\\frac {2}{3}}}}\\right) \\Theta \\left ({{T^{\\frac {2}{3}}}}\\right)   regret with bandit feedback, or rely on free full-feedback in order to reduce the regret to   O(\\sqrt {T}) O(\\sqrt {T})  . In contrast, we expand our study to incorporate two new factors. First, full feedback could incur a cost. Second, the player may choose 2 (or more) arms at a time and observe their feedback, even though switching costs are still incurred when she changes the set of chosen arms. For the setting where the player pulls only one arm at a time, our new regret lower-bound shows that, even when costly full-feedback is added, the   \\Theta \\left ({{T^{\\frac {2}{3}}}}\\right) \\Theta \\left ({{T^{\\frac {2}{3}}}}\\right)   regret still cannot be improved. However, the dependence on the number of arms may be improved when the full-feedback cost is small. In contrast, for the setting where the player can choose 2 (or more) arms at a time, we provide a novel online learning algorithm that achieves a significantly lower regret equal to   O(\\sqrt {T}) O(\\sqrt {T})  . Further, our new algorithm does not need any full feedback at all. This sharp difference therefore reveals the surprising power of choosing 2 (or more) arms for this type of bandit learning problems with switching costs. Both our new algorithm and regret analysis involve several new ideas in choosing the primary and secondary arms, tuning the weight-decay parameters within and across episodes, and using the loss differences in the weight updates, which may be of independent interest.}
}


@article{DBLP:journals/ton/Fei0WXWL25,
	author = {Shufan Fei and
                  Zheng Yan and
                  Dongliang Wang and
                  Haomeng Xie and
                  Haiguang Wang and
                  Tieyan Li},
	title = {{B5G-NFM:} Blockchain-Based 5G Network Function Management With Enhanced
                  Security and Privacy},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1128--1143},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3522133},
	doi = {10.1109/TON.2024.3522133},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Fei0WXWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Within the 5G Service-Based Architecture (SBA), the prevalent centralized frameworks, particularly those managing public key certificates and Network Function (NF) services, face significant security and privacy vulnerabilities due to their reliance on centralized Certificate Authorities (CAs) and Network Repository Functions (NRFs). This centralization in current 5G standards poses two critical issues. One is the risk of single points of failure associated with CAs and NRFs. The other is a substantial risk of privacy leakage in cross-domain NF access authorization. These issues underscore the demand for decentralized NF management with enhanced security and privacy. However, existing related schemes still suffer from such drawbacks as poor security and privacy, low efficiency, and a lack of automation in the management of NF certificates. In this paper, we propose B5G-NFM, a pioneering Blockchain-based 5G Network Function Management scheme designed to bolster security and privacy. B5G-NFM integrates a blockchain middleware network function, a decentralized protocol for managing NF public key certificates, and a decentralized mechanism for NF service discovery coupled with a privacy-preserving NF access authorization protocol. Our thorough security and performance assessment verifies that B5G-NFM not only meets its security and privacy goals, but also aligns with efficiency demand, marking a substantial progress in strengthening 5G network infrastructure.}
}


@article{DBLP:journals/ton/0002ZZC0025,
	author = {Ziyi Wang and
                  Rui Zhang and
                  Songyu Zhang and
                  Wei Cheng and
                  Wendong Wang and
                  Yong Cui},
	title = {Edge-Assisted Adaptive Configuration for Serverless-Based Video Analytics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1144--1159},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3523956},
	doi = {10.1109/TON.2024.3523956},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0002ZZC0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The growth of video volumes and increased DNN capabilities have led to a growing desire for video analytics, which demands intensive computation resources. Traditional resource provisioning strategies, such as configuring a cluster per peak utilization, lead to low resource efficiency. Serverless computing is a promising way to avoid wasteful resource provisioning since video analytics regularly encounters bursty input workloads and fine-grained video content dynamics. For serverless-based video analytics, the application configuration (frame rate, detection model, and computation resources) will impact several metrics, such as computation cost and analytics accuracy. In this paper, we investigate the joint configuration adjustment problem for video knobs and computation resources provided by the serverless platform. We propose an algorithm that can efficiently adapt configurations for video streams to address two key challenges in serverless-based video analytics systems, including the complex relationships between the configurations and the key performance metrics, and the dynamically best configuration. Our adaptive configuration adjustment algorithm is developed based on Markov approximation to minimize the computation cost. To guarantee the accuracy, we then design the keyframe selection algorithm based on the secretary algorithm to identify significant changes in video content. We have developed a prototype over AWS Lambda and conducted extensive experiments with real-world video streams. The results show that our algorithm can greatly reduce the computation cost under the constraint of target accuracy.}
}


@article{DBLP:journals/ton/LiuZ000GS25,
	author = {Xuezheng Liu and
                  Yipeng Zhou and
                  Di Wu and
                  Miao Hu and
                  Min Chen and
                  Mohsen Guizani and
                  Quan Z. Sheng},
	title = {CPFedAvg: Enhancing Hierarchical Federated Learning via Optimized
                  Local Aggregation and Parameter Mixing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1160--1173},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3526866},
	doi = {10.1109/TON.2025.3526866},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuZ000GS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical federated learning (HFL) improves the scalability and efficiency of traditional federated learning (FL) by incorporating a hierarchical topology into the FL framework. In a typical HFL system, clients are divided into multiple tiers, and the training process involves both local and global model aggregation. However, existing HFL approaches have several significant drawbacks. Firstly, the root parameter server (PS) is vulnerable to single-point failure and also acts as a bottleneck for global aggregation. Additionally, frequent global aggregation over the wide area network (WAN) incurs substantial communication costs, which negatively affect training efficiency. In this paper, we propose a novel HFL algorithm called CPFedAvg to address the aforementioned challenges. CPFedAvg introduces a root-free hierarchical topology, where the top tier consists of multiple PSes, effectively resolving the issues associated with the root PS. Additionally, we substitute the expensive global aggregation with parameter mixing operations between the PSes in the top tier. We analyze the convergence rate of CPFedAvg under non-convex loss. Based on this analysis, we formulate a convex optimization problem to optimize the frequency of executing local aggregations between consecutive parameter mixing operations. To simulate real-world communication networks, we develop FedNetSimulator to simulate a diverse range of FL communication processes. Finally, we conduct extensive experiments using real datasets (i.e., CIFAR-10 and CIFAR-100). The experimental results demonstrate that CPFedAvg can improve model accuracy by up to 18% and the speedup can be as high as 6 compared with the state-of-the-art baselines.}
}


@article{DBLP:journals/ton/0004M0L25,
	author = {Zhiyuan Wang and
                  Qingkai Meng and
                  Shan Zhang and
                  Hongbin Luo},
	title = {Incentivizing Fresh Information Acquisition via Age-Based Reward},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1174--1188},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3524092},
	doi = {10.1109/TON.2024.3524092},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0004M0L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many Internet platforms are information-oriented and crowd-based. They collect fresh information of various points of interest (PoIs) relying on users who happen to be nearby the PoIs. The platform will offer rewards to incentivize users and compensate their costs incurred from information acquisition. In practice, a user’s cost is his/her private information, thus both the user cost and its distribution are hidden to the platform, making it challenging to determine the optimal rewarding decision. In this paper, we investigate how the platform dynamically rewards the users, aiming to jointly reduce the age of information (AoI) and the operational expenditure (OpEx). Due to the hidden cost distribution, this is an online non-convex learning problem with bandit feedback. To overcome the challenge, we first design an age-based reward scheme, which decouples the OpEx from the unknown cost distribution and enables the platform to accurately control its OpEx. We then take advantage of the age-based reward scheme and propose an exponentially discretizing and learning (EDAL) policy for platform operation. We prove that the EDAL policy performs asymptotically as well as the optimal decision (derived from the cost distribution). Simulation results show that the age-based reward scheme protects the platform’s OpEx from the influence of the user crowd characteristics, and also verify the asymptotic optimality of the EDAL policy.}
}


@article{DBLP:journals/ton/Wan0W0PP025,
	author = {Zirui Wan and
                  Jiao Zhang and
                  Yuxiang Wang and
                  Kefei Liu and
                  Haoyu Pan and
                  Yongchen Pan and
                  Tao Huang},
	title = {{RHCC:} Revisiting Intra-Host Congestion Control in {RDMA} Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1189--1202},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2024.3524247},
	doi = {10.1109/TON.2024.3524247},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Wan0W0PP025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {RDMA has been widely deployed in production datacenters. The conventional wisdom believes that the intra-host network delivers stable and high performance. However, intra-host resources witness a relative stagnation in technology trends compared to the evolving RDMA NIC (RNIC). Thus, the RNIC traffic may not get sufficient intra-host resources when it contends with CPU-to-memory traffic. A line of recent works from large-scale production datacenter operators demonstrates the emergence of intra-host congestion and associated performance collapse, which forces us to revisit the practice of intra-host congestion control. However, the ability to efficiently control RDMA intra-host networks is far less mature than inter-host networks, which brings challenges in congestion monitoring, intra-host resource allocation and RNIC traffic adjustment. In this paper, we propose RDMA intra-Host Congestion Control (RHCC), which combines CPU-to-memory traffic congestion avoidance with sub-RTT granularity and proactive RNIC traffic adjustment. RHCC ensures fast congestion avoidance and can work with different inter-host congestion control methods. We implement RHCC on commodity servers and RNICs and conduct experiments to evaluate the performance. The results show that RHCC can increase/decrease the network throughput/latency by up to   2\\times  2\\times    and   1.4\\times  1.4\\times   , respectively.}
}


@article{DBLP:journals/ton/FangH0DCF25,
	author = {Zhengru Fang and
                  Senkang Hu and
                  Jingjing Wang and
                  Yiqin Deng and
                  Xianhao Chen and
                  Yuguang Fang},
	title = {Prioritized Information Bottleneck Theoretic Framework With Distributed
                  Online Learning for Edge Video Analytics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1203--1219},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3526148},
	doi = {10.1109/TON.2025.3526148},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FangH0DCF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Collaborative perception systems leverage multiple edge devices, such as surveillance cameras or autonomous cars, to enhance sensing quality and eliminate blind spots. Despite their advantages, challenges such as limited channel capacity and data redundancy impede their effectiveness. To address these issues, we introduce the Prioritized Information Bottleneck (PIB) framework for edge video analytics. This framework prioritizes the shared data based on the signal-to-noise ratio (SNR) and camera coverage of the region of interest (RoI), reducing spatial-temporal data redundancy to transmit only essential information. This strategy avoids the need for video reconstruction at edge servers and maintains low latency. It leverages a deterministic information bottleneck method to extract compact, relevant features, balancing informativeness and communication costs. For high-dimensional data, we apply variational approximations for practical optimization. To reduce communication costs in fluctuating connections, we propose a gate mechanism based on distributed online learning (DOL) to filter out less informative messages and efficiently select edge servers. Moreover, we establish the asymptotic optimality of DOL by proving the sublinearity of its regrets. To validate the effectiveness of the PIB framework, we conduct real-world experiments on three types of edge devices with varied computing capabilities. Compared to five coding methods for image and video compression, PIB improves mean object detection accuracy (MODA) by 17.8% while reducing communication costs by 82.65% under poor channel conditions.}
}


@article{DBLP:journals/ton/LinLCJ25,
	author = {Wanling Lin and
                  Xiao{-}Yan Li and
                  Jou{-}Ming Chang and
                  Xiaohua Jia},
	title = {Link/Switch Failure Analysis of Data Center Networks on Matroidal
                  Connectivity},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1220--1235},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3525702},
	doi = {10.1109/TON.2025.3525702},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LinLCJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the surge of bandwidth demand for cloud applications and the exponential growth of data, data center networks (DCNs) are expanding rapidly, followed by the daily increasing likelihood of failures. Such failures, whether due to device or link issues, are inevitable and often lead to packet loss, transmission delays, and even system downtime. Thus, it is crucial to assess the fault-tolerant capabilities of data center networks using appropriate reliability metrics when failures occur. BCube is a well-known server-centric data center network with many advantages, such as rich low-diameter paths, high throughput, and excellent expandability. Not only do the recently proposed matroidal connectivity and conditional matroidal connectivity have reasonable fault assumptions that align well with the structural characteristics of data center networks, but they also significantly enhance the fault tolerance performance of DCNs. This paper determines the matroidal connectivity and conditional matroidal connectivity of BCube, which is the first study to apply the two reliability metrics in DCNs. Then, we extend the conclusions about (conditional) matroidal connectivity to analyze the fault tolerance of BCube in the occurrence of switch failures. In addition, we develop an efficient algorithm to identify the structural features of minimum faulty edge sets, where the cardinality of these edge sets corresponds to the conditional matroidal connectivity of BCube. Finally, we experimentally evaluate the effects of both link and switch failures on BCube’s performance under the matroidal restriction. The experimental analyses reveal that BCube DCNs exhibit high fault tolerance under matroidal constraints, with the ability to withstand both link and switch failures.}
}


@article{DBLP:journals/ton/XieTCZ25,
	author = {Xuexia Xie and
                  Binjun Tang and
                  Xiaoliang Chen and
                  Zuqing Zhu},
	title = {{P4INC-AOI:} All-Optical Interconnect Empowered by In-Network Computing
                  for {DML} Workloads},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1236--1251},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3526228},
	doi = {10.1109/TON.2025.3526228},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieTCZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increasing demands for distributed machine learning (DML) have posed significant pressure on data-center networks (DCNs). This promotes the adoption of reconfigurable all-optical interconnects (AOI) in DCNs leveraging optical circuit switching (OCS) for better performance on throughput, energy efficiency, and data transfer latency. Despite their benefits, these OCS-based DCNs (ODCNs) still bear limited flexibility due to the larger switching granularity and longer reconfiguration latency of OCS. To address this issue, this work introduces in-network computing (INC) in an ODCN to realize P4INC-AOI, which can orchestrate INC and AOI to explore their mutual benefits for accelerating the training of DML jobs with less AOI reconfigurations. In the control plane of P4INC-AOI, we address the scheduling of concurrent DML jobs by formulating a mixed integer linear programming (MILP) model and proposing a time-efficient heuristic, to allocate multi-dimensional resources and configure AOI for minimizing the longest job completion time (JCT) across workloads. For the data plane, we extend existing in-network gradient aggregation schemes to accelerate DML jobs more efficiently. We first implement P4INC-AOI and verify its performance in a small-scale ODCN testbed, and further justify its effectiveness with large-scale simulations. Our experimental results demonstrate that compared with an ODCN without INC, P4INC-AOI not only cuts down AOI reconfigurations effectively but also reduces the average JCT of DML jobs in ResNet50 and VGG16 by 46.66% and 56.34%, respectively.}
}


@article{DBLP:journals/ton/0010000L25,
	author = {Ning Chen and
                  Sheng Zhang and
                  Jie Wu and
                  He Huang and
                  Sanglu Lu},
	title = {Spliceosome: On-Camera Video Thinning and Tuning for Timely and Accurate
                  Analytics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1252--1264},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3526218},
	doi = {10.1109/TON.2025.3526218},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0010000L25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Running deep neural networks (DNNs) on large-scale videos from widely distributed cameras presents two significant challenges. Firstly, video quality for analytical purposes is severely impacted by the camera deployment environment, which is termed Pixel Recession in this paper. Secondly, low-latency video streaming from the source camera to edge servers is greatly hindered by the rapid expansion of video traffic. Despite numerous efforts such as enhancing the video structure, uneven encoding, and filtering frames captured on camera, these methods have proven insufficient to address the challenges at hand. We propose Spliceosome, a novel video analytics system that effectively overcomes the pixel recession and streaming bottlenecks. In brief, Spliceosome 1) recovers from pixel recession by adaptive video knobs (i.e., brightness and contrast) tuning in ARP (anchor region proposal) granularity, and 2) lowers the transmission volume by video thinning, which uses only single-channel information for video encoding. We implemented Spliceosome using only commercial off-the-shelf hardware. Our experimental results demonstrate that Spliceosome outperforms other alternative designs by 4.71-14.47%, 40.94-58.71%, and 14.28% in detection accuracy, end-to-end delay, and efficiency of DNNs inference, respectively.}
}


@article{DBLP:journals/ton/0008BL0D25,
	author = {Kun He and
                  Hao Bai and
                  Yuqing Li and
                  Jing Chen and
                  Ruiying Du},
	title = {ACE-pFL: Accurate, Efficient Personalized Federated Learning With
                  Knowledge Distillation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1265--1278},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3527467},
	doi = {10.1109/TON.2025.3527467},
	timestamp = {Fri, 30 Jan 2026 16:18:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/0008BL0D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Personalized Federated Learning (pFL) can collaboratively personalize models for multiple clients without sharing their private data. However, many pFL methods rely on server-side model parameters aggregation, which requires all models to have the same structure and size. One promising approach is leveraging knowledge distillation (KD) to transfer knowledge between models by exchanging soft predictions rather than model parameters, thus training heterogeneous models. Nevertheless, existing KD-based pFL solutions suffer from accuracy loss due to inadequate knowledge extraction as well as huge computing and communication overheads. In this paper, we present an accurate and efficient KD-based pFL framework, called ACE-pFL. Specifically, we first propose a privacy-preserving client clustering to reduce the impact of non-independent and identically distributed (non-IID) data on model accuracy and convergence, grouping clients with similar data distributions into the same cluster. Since the distillation temperature of traditional KD is fixed, which does not consider the dynamic model training process, we design a dynamic distillation temperature adjustment to accommodate this process, where clients incrementally increase the distillation temperature as training proceeds to facilitate model generalization to new data. Finally, we employ the triple distillation strategy to provide diverse and abundant knowledge, including explicit global knowledge, implicit local knowledge, and implicit global knowledge. Experiments on multiple datasets and tasks show that compared with existing schemes, ACE-pFL can significantly improve the test accuracy by 17.18%, reduce the training time by 57% and the communication overhead by   59.12\\times  59.12 × 59.12\\times    on average.}
}


@article{DBLP:journals/ton/00010Z0G0GL25,
	author = {Jingyu Wang and
                  Chenyang Zhao and
                  Zirui Zhuang and
                  Qi Qi and
                  Yuebin Guo and
                  Haifeng Sun and
                  Lingqi Guo and
                  Jianxin Liao},
	title = {Fast and Scalable Data Plane Verification for Burst Updates With Edge-Predicate},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1279--1294},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3528577},
	doi = {10.1109/TON.2025.3528577},
	timestamp = {Sun, 02 Nov 2025 12:34:24 +0100},
	biburl = {https://dblp.org/rec/journals/ton/00010Z0G0GL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There is an increasing interest in data plane verification, which is designed to automatically verify network correctness through directly analyzing the data plane. Recent data plane verifiers have been able to do real-time sub-millisecond per rule verification. However, we observe that in real-world networks, individual data plane updates rarely occur. On the contrary, there are always a certain number of updates generated in a short period of time, called as burst updates, due to high-level user intend or uncertain network events. When it comes to this real-life scenario, yet, the current equivalence class (EC) based methods are unable to solve the model-wide changes problem caused by the EC itself, which significantly slows down the verification speed of burst updates. To overcome this limitation, we present EPVerifier, a fast, scalable data plane verifier accelerating burst updates verification with edge-predicate (EP). Instead of classifying packets into ECs according to global forwarding behavior, the EPVerifier uses one EP per edge to represent all packets that can pass through. Furthermore, with EPs that clearly have localized properties, we introduce a rule type extension that does not require a change in the granularity of the network model to support ACLs and NATs that are prevalent in real devices, and obtain better-performing parallelism by dividing the verification task based on switches. Experiments on both dataset simulations and real-life deployments show that EPVerifier achieves 2-  10\\times  10\\times    faster data plane verification than the state-of-the-art and such advantage expand with the data plane’s complexity and update scale growth.}
}


@article{DBLP:journals/ton/MostafaeiP025,
	author = {Habib Mostafaei and
                  Maciej Pacut and
                  Stefan Schmid},
	title = {{RIFO:} Pushing the Efficiency of Programmable Packet Schedulers},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1295--1308},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3531129},
	doi = {10.1109/TON.2025.3531129},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MostafaeiP025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet scheduling is a fundamental networking task that recently received renewed attention in the context of programmable data planes. Programmable packet scheduling systems such as those based on Push-In First-Out (PIFO) abstraction enabled flexible scheduling policies, but are too resource-expensive for large-scale line rate operation. This prompted research into practical programmable schedulers (e.g., SP-PIFO, AIFO) approximating PIFO behavior on regular hardware. Yet, their scalability remains limited due to extensive number of memory operations. To address this, we design an effective yet resource-efficient packet scheduler, Range-In First-Out (RIFO), which uses only three mutable memory cells and one FIFO queue per PIFO queue. RIFO is based on multi-criteria decision-making principles and uses small guaranteed admission buffers. Our large-scale simulations in Netbench demonstrate that despite using fewer resources, RIFO generally achieves competitive flow completion times across all studied workloads, and is especially effective in workloads with a significant share of large flows, reducing flow completion time up to   4.91\\times  4.91\\times    in datamining workload compared to state-of-the-art solutions. Our prototype implementation using P4 on Tofino switches requires only 600 lines of code, is scalable, and runs at line rate.}
}


@article{DBLP:journals/ton/000100BA25,
	author = {Juncheng Wang and
                  Min Dong and
                  Ben Liang and
                  Gary Boudreau and
                  Ali Afana},
	title = {Exploring Temporal Similarity for Joint Computation and Communication
                  in Online Distributed Optimization},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1309--1325},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3530460},
	doi = {10.1109/TON.2025.3530460},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/000100BA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider online distributed optimization in a networked system, where multiple devices assisted by a server collaboratively minimize the accumulation of a sequence of global loss functions that can vary over time. To reduce the amount of communication, the devices send quantized and compressed local decisions to the server, resulting in noisy global decisions. Therefore, there exists a tradeoff between the optimization performance and the communication overhead. Existing works separately optimize computation and communication. In contrast, we jointly consider computation and communication over time, by proactively encouraging temporal similarity in the decision sequence to control the communication overhead. We propose an efficient algorithm, termed Online Distributed Optimization with Temporal Similarity (ODOTS), where the local decisions are both computation- and communication-aware. Furthermore, ODOTS uses a novel tunable virtual queue, which removes the commonly assumed Slater’s condition through a modified Lyapunov drift analysis. ODOTS delivers provable performance bounds on both the optimization objective and constraint violation. Furthermore, we consider a variant of ODOTS with multi-step local gradient descent updates, termed ODOTS-MLU, and show that it provides improved performance bounds. As an example application, we apply both ODOTS and ODOTS-MLU to enable communication-efficient federated learning. Our experimental results based on canonical image classification demonstrate that ODOTS and ODOTS-MLU obtain higher classification accuracy and lower communication overhead compared with the current best alternatives for both convex and non-convex loss functions.}
}


@article{DBLP:journals/ton/TaoZKGY25,
	author = {Lei Tao and
                  Shenglin Zhang and
                  Junhua Kuang and
                  Xiaowei Guo and
                  Canqun Yang},
	title = {Real-Time Anomaly Detection for Large-Scale Network Devices},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1326--1337},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3529861},
	doi = {10.1109/TON.2025.3529861},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TaoZKGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the booming of large-scale network devices, anomaly detection on multivariate time series (MTS), such as a combination of CPU utilization, average response time, and network packet loss, is important for system reliability. Although a collection of learning-based approaches have been designed for this purpose, our study shows that these approaches suffer from long initialization time for sufficient training data. Our previously proposed JumpStarter model stands as a MTS anomaly detection method characterized by its brief initialization time and commendable detection performance. However, it suffers from high computational cost and inappropriateness for periodic MTS. In this paper, we propose VersaGuardian, which introduces the Dynamic Mode Decomposition technique to MTS anomaly detection for diverse types of MTS in a rapidly initialized, computationally efficient manner. With real-world MTS datasets collected from three companies, our results show that VersaGuardian achieves an average F1 score of 94.42%, significantly outperforming the popular anomaly detection algorithms, with a much shorter initialization time of 20 minutes and detection time of 15.28 milliseconds.}
}


@article{DBLP:journals/ton/0008D25,
	author = {Hongbo Li and
                  Lingjie Duan},
	title = {To Analyze and Regulate Human-in-the-Loop Learning for Congestion
                  Games},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1338--1350},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3530450},
	doi = {10.1109/TON.2025.3530450},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0008D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In congestion games, selfish users behave myopically to crowd to the shortest paths, and the social planner designs mechanisms to regulate such selfish routing through information or payment incentives. However, such mechanism design requires the knowledge of time-varying traffic conditions and it is the users themselves to learn and report past road experiences to the social planner (e.g., Waze or Google Maps). When congestion games meet mobile crowdsourcing, it is critical to incentivize selfish users to explore non-shortest paths in the best exploitation-exploration trade-off. First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability   \\lambda  \\lambda   . We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum. Due to the myopic policy’s under-exploration, we prove that the caused price of anarchy (PoA) is larger than   \\frac {1}{1-\\rho ^{\\frac {1}{\\lambda }}} \\frac {1}{1-\\rho ^{\\frac {1}{\\lambda }}}  , which can be arbitrarily large as discount factor   \\rho \\rightarrow 1 \\rho \\rightarrow 1  . To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore. We prove that our mechanism successfully reduces PoA to be less than 2. Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes. In addition to the worst-case performance evaluation, we conduct extensive simulations with both synthetic and real transportation datasets to demonstrate the close-to-optimal average-case performance of our SID mechanism.}
}


@article{DBLP:journals/ton/0001LY25,
	author = {Wei Gong and
                  Lijie Liu and
                  Yifan Yang},
	title = {Universal Content-Agnostic Backscatter for {OFDM} WiFi},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1351--1360},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3532945},
	doi = {10.1109/TON.2025.3532945},
	timestamp = {Wed, 15 Oct 2025 19:23:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0001LY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ambient backscatter is one of the most promising solutions for the widespread deployment of low-power IoT. We present CAB, a content-agnostic backscatter system that can demodulate both tag and ambient data from ambient backscattered WiFi alone. In contrast to prior ambient backscatter systems that use ambient data (content) to demodulate tag data, we focus on zero-subcarriers, which are invariant and independent for any ambient OFDM WiFi. The idea of using zero-subcarriers to convey tag data is simple and elegant. Not only does it for the first time remove the dependency of tag-data demodulation on ambient data, but it also significantly improves the practicality of ambient backscatter. CAB is also universal for various OFDM-WiFi signals because the zero-subcarrier we use is virtual and is essentially the phase estimation of the pilot, which is not affected by different pilot patterns. In order to verify the universality, we prototype CAB using off-the-shelf FPGAs and SDRs. Extensive experiments show CAB is universal as it can work with multi-band, multi-stream, and multi-user ambient traffic, including WiFi 3/4/5/6. In addition, CAB can work with excitations at different data rates over various commercial NICs. As the first content-agnostic backscatter system with 340.9 Mbps aggregate throughput, we believe CAB takes a crucial step forward on ubiquitous battery-free IoTs.}
}


@article{DBLP:journals/ton/BramasLM25,
	author = {Quentin Bramas and
                  Jean{-}Romain Luttringer and
                  Pascal M{\'{e}}rindol},
	title = {A Simple and General Operational Framework to Deploy Optimal Routes
                  With Source Routing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1361--1372},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3534332},
	doi = {10.1109/TON.2025.3534332},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BramasLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Source Routing, currently facilitated by Segment Routing (SR), enables routers to add forwarding instructions to the packet to follow a particular path that deviates from the typical IGP path. These instructions form a list of detours (called segments). However, the number of segments that can be imposed at line-rate is tightly constrained by the hardware. Hence, the main challenge consists in incorporating this constraint into a path computation algorithm. The goal is to be able to solve many existing problems, including finding multi-constrained paths and re-routing packets after a failure, in a way that is deployeable in existing networks using Segment Routing. Existing solutions either lack generality, correctness, optimality, or practical computing efficiency – in particular for sparse realistic networks. In this paper, we address all such challenges with GOFOR-SR. Our framework extends usual path computation algorithms by integrating the SR constraint within the path computation itself and modifying the distance comparison method. GOFOR allows algorithm with various optimization objectives to efficiently compute optimal segment lists. Despite the loss of substructure optimality induced by SR, GOFOR proves particularly efficient, inducing only a linear overhead at worst. It also offers different strategies and path diversity options for intricate load-balancing. We formally prove the correctness and optimality of GOFOR, implement our framework for various practical use-cases, and demonstrate its performance and benefits on both real and challenging topologies.}
}


@article{DBLP:journals/ton/CiccoCM25,
	author = {Luca De Cicco and
                  Giuseppe Cilli and
                  Saverio Mascolo},
	title = {{ERUDITE:} {A} Deep Neural Network for Optimal Tuning of Adaptive
                  Video Streaming Controllers},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1373--1387},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3532041},
	doi = {10.1109/TON.2025.3532041},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CiccoCM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Adaptive video streaming systems are expected to provide the best user experience to improve service engagement. To the purpose, video players host a controller that dynamically chooses the most suitable video representation to be downloaded. It is well-known that finding one tuning of the controller’s parameters which performs satisfactorily in a wide range of scenarios is very challenging. This paper studies the problem of providing users with (near) optimal Quality of Experience (QoE) for Dynamic Adaptive Streaming over HTTP (DASH) systems. We present ERUDITE, a closed-loop system to optimally tune – at run-time – the adaptive streaming controller’s parameters to adapt to changing scenario’s parameters. ERUDITE employs a Deep Neural Network (DNN) which continuously provides the streaming controller with estimates of optimal parameters based on measured metrics such as bandwidth samples and overall obtained QoE. The DNN is trained using a dataset that we have built by finding, for thousands of realistic scenarios, robust optimal adaptive streaming controller’s parameters using a Bayesian optimization algorithm. Results, gathered considering a large number of diverse scenarios, show that ERUDITE is able to provide near optimal performances by reducing impairments due to rebuffering and video level switching.}
}


@article{DBLP:journals/ton/OuyangZH00025,
	author = {Tao Ouyang and
                  Kongyange Zhao and
                  Guihang Hong and
                  Xiaoxi Zhang and
                  Zhi Zhou and
                  Xu Chen},
	title = {Dynamic Edge-Centric Resource Provisioning for Online and Offline
                  Services Co-Location via Reactive and Predictive Approaches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {3},
	pages = {1388--1403},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3535499},
	doi = {10.1109/TON.2025.3535499},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/OuyangZH00025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the penetration of edge computing, a wide variety of workloads are sunk down to the network edge to alleviate huge pressure of the cloud. With the presence of high input workload dynamics and intensive edge resource contention, it is highly non-trivial for an edge proxy to optimize the scheduling of heterogeneous services with diverse QoS requirements. In general, online services should be quickly completed in a quite stable running environment to meet their tight latency constraint, while offline services can be processed loosely for their elastic soft deadlines. To well coordinate such services at the resource-limited edge cluster, in this paper, we study an edge-centric resource provisioning optimization for dynamic online and offline services co-location, where the proxy seeks to maximize timely online service performances while maintaining satisfactory long-term offline service performances. However, intricate hybrid couplings for provisioning decisions arise due to heterogeneous constraints of the co-located services and their different time-scale performances. We hence first propose a reactive provisioning approach without requiring a prior knowledge of future system dynamics, which leverages a Lagrange relaxation for devising constraint-aware stochastic subgradient algorithm to deal with the challenge of hybrid couplings. To further boost the performance by integrating powerful machine learning techniques, we then advocate a predictive provisioning approach, where future request arrivals can be estimated accurately. To align with practical deployments, we incorporate a tunable prediction window mechanism, which well balances the potential improvement and degradation of online performance in imperfect prediction scenarios. With rigorous theoretical analysis and extensive trace-driven evaluations, we show the superior performance of our proposed algorithms for online and offline services co-location at the edge.}
}


@article{DBLP:journals/ton/Perazzone0J025,
	author = {Jake B. Perazzone and
                  Shiqiang Wang and
                  Mingyue Ji and
                  Kevin Chan},
	title = {Communication-Efficient Device Scheduling for Federated Learning Using
                  Lyapunov Optimization},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1407--1421},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3539857},
	doi = {10.1109/TON.2025.3539857},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Perazzone0J025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a useful tool that enables the training of machine learning models over distributed data without having to collect data centrally. When deploying FL in constrained wireless environments, however, intermittent connectivity of devices, heterogeneous connection quality, and non-i.i.d. data can severely slow convergence. In this paper, we consider FL with arbitrary device participation probabilities for each round and show that by weighing each device’s update by the reciprocal of their per-round participation probability, we can guarantee convergence to a stationary point. Our bound applies to non-convex loss functions and non-i.i.d. datasets and recovers state-of-the-art convergence rates for both full and uniform partial participation, including linear speedup, with only a single-sided learning rate. Then, using the derived convergence bound, we develop a new online client selection and power allocation algorithm that utilizes the Lyapunov drift-plus-penalty framework to opportunistically minimize a function of the convergence bound and the average communication time under a transmit power constraint. We use optimization over manifold techniques to obtain a solution to the minimization problem. Thanks to the Lyapunov framework, one key feature of the algorithm is that knowledge of the channel distribution is not required and only the instantaneous channel state information needs to be known. Using the CIFAR-10 dataset with varying levels of data heterogeneity, we show through simulations that the communication time can be significantly decreased using our algorithm compared to uniformly random participation, especially for heterogeneous channel conditions.}
}


@article{DBLP:journals/ton/KanYZD0LX25,
	author = {Nuowen Kan and
                  Sa Yan and
                  Junni Zou and
                  Wenrui Dai and
                  Xing Gao and
                  Chenglin Li and
                  Hongkai Xiong},
	title = {GDPlan: Generative Network Planning via Graph Diffusion Model},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1422--1437},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3535518},
	doi = {10.1109/TON.2025.3535518},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KanYZD0LX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network planning is crucial to facilitate network service under limited network operation costs. However, adapting the network topology (i.e., connections and capacities for physical and IP links) to time-varying and stochastic network states is challenging due to high computational overhead of problem solving. Existing deep learning based methods suffer from prohibitive computational complexity due to simultaneously solving IP link capacities and allocated traffics or iteratively exploring search space with excessive samples via heuristic rules. In this paper, we propose GDPlan, the first generative framework that leverages conditional graph diffusion model to address this challenge. To achieve high solving efficiency, GDPlan decouples network planning into two separate stages, i.e., generation of diverse high-quality solutions to IP link capacity and refinement of the solutions under feasibility constraints. Specifically, we develop generation-oriented graph signal modeling that reformulates solving IP link capacities as generation of graph topology conditioned on the graph signals related to traffic demands, physical links and network costs. Consequently, energy-guided controllable graph generation is achieved to capture significant structural patterns of graph topology with score-based graph diffusion model and produce diverse solutions with a guarantee of feasibility under specific objectives or constraints. GDPlan successfully achieves graph generation based solution to network planning with an order of magnitude higher solving speed than the commercial solver Gurobi. Experimental results demonstrate that, compared with Gurobi, the proposed GDPlan obtains lowest 3.7% average gap with only about 7.5% average running time.}
}


@article{DBLP:journals/ton/LiuY0X0QH25,
	author = {Jianchun Liu and
                  Jiaming Yan and
                  Ji Qi and
                  Hongli Xu and
                  Shilong Wang and
                  Chunming Qiao and
                  Liusheng Huang},
	title = {Adaptive Local Update and Neural Composition for Accelerating Federated
                  Learning in Heterogeneous Edge Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1438--1453},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3539666},
	doi = {10.1109/TON.2025.3539666},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuY0X0QH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) enables distributed clients to collaboratively train models without exposing their private data. However, it is difficult to implement efficient FL due to limited resources. Most existing works compress the transmitted gradients or prune the global model to reduce the resource cost, but leave the compressed or pruned parameters under-optimized, which degrades the training performance. To address this issue, the neural composition technique constructs size-adjustable models by composing low-rank tensors, allowing every parameter in the global model to learn the knowledge from all clients. Nevertheless, some tensors can only be optimized by a small fraction of clients, thus the global model may get insufficient training, leading to a long completion time, especially in heterogeneous edge scenarios. To this end, we enhance the neural composition technique, enabling all parameters to be fully trained. Further, we propose a lightweight FL framework, called Heroes, with enhanced neural composition and adaptive local update. A greedy-based algorithm is designed to adaptively assign the proper tensors and local update frequencies for participating clients according to their heterogeneous capabilities and resource budgets. On this basis, we further propose an extension of Heroes, termed AdaHeroes, which further improves the training performance under the statistical heterogeneity scenario based on an adaptive client selection strategy. Extensive experiments demonstrate that Heroes can reduce traffic consumption by about 72.46% and provide up to   2.76\\times  2.76\\times    speedup compared to the baselines. Furthermore, with the setting of statistical heterogeneity, AdaHeroes can improve the test accuracy by about 4.77% compared with Heroes and the baselines.}
}


@article{DBLP:journals/ton/Zhu0Y25,
	author = {Haowen Zhu and
                  Zehua Guo and
                  Minghao Ye},
	title = {{DINA:} Toward Determined In-Network Aggregation for Distributed Machine
                  Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1454--1468},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3535707},
	doi = {10.1109/TON.2025.3535707},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Zhu0Y25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Machine Learning (DML) utilizes parallel computation on multiple training nodes to accelerate machine learning model training. Parameter Server (PS) is a typical DML enabler and is widely used in industry and academia. Existing works propose to apply the emerging In-Network Aggregation (INA) technique to improve model training efficiency by offloading the whole gradient aggregation process in PS from hosts to programmable switches. However, existing INA systems may suffer from undetermined model training efficiency and service quality, given that many gradient aggregation processes are still performed by the server under irrational gradient aggregation strategies. In this paper, we propose a Deterministic In-Network Aggregation (DINA) scheme to improve model training efficiency by enhancing the efficiency of INA utilization in DML. Our key observation is to further increase worker sending rates by reducing gradient packets’ RTT (i.e., realizing packet sub-RTT). Based on this observation, DINA rationally selects the optimal global gradient aggregation switch depending on the switches’ available memory, worker sending rate, and server processing capacity. As a result, DINA reduces the dependence of INA systems on the server, improves worker sending rates, and mitigates network traffic load. We formulate the sub-RTT-INA-based gradient aggregation problem as a mixed-integer nonlinear programming problem. To efficiently solve the problem, we simplify it by transforming the nonlinear constraints into linear constraints and propose a mixed solution that combines randomized rounding and heuristic mechanisms. Simulation results show that DINA can provide determined training by reducing communication time by 12%-17% and network load by 28%-50% compared with existing solutions, thus taking full advantage of INA and realizing a determined INA service.}
}


@article{DBLP:journals/ton/SongLHDXCK0LS25,
	author = {Haohao Song and
                  Yuling Lin and
                  Yangfan Huang and
                  Haizhou Du and
                  Qiao Xiang and
                  Yijian Chen and
                  Linghe Kong and
                  Qiang Li and
                  Franck Le and
                  Jiwu Shu},
	title = {Toward Verifying and Interpreting Learning-Based Networking Systems
                  With {SMT}},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1469--1483},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3540640},
	doi = {10.1109/TON.2025.3540640},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SongLHDXCK0LS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There has been a growing interest in applying machine learning to real-world tasks. However, due to the black-box nature of machine learning models, it is crucial to 1) verify important properties of a model and 2) understand the reasons behind a model’s prediction before deploying them in a production environment. Existing approaches typically handle them as two separate and sometimes orthogonal topics. In this paper, we show that the verification and interpretability of machine learning models are tightly related and can be unified by satisfiability modulo theories (SMT). Our key insight is: not only a wide range of properties of machine learning models can be formulated as SMT problems and verified accordingly, but many commonly studied interpretability questions can also be answered by iteratively checking the satisfiability and related properties of multiple SMT problems. Leveraging this insight, we design UINT, a general verification and interpretability framework for learning-based networking systems. UINT 1) allows operators to specify verification and interpretability problems as SMT formulas, 2) encodes the target machine learning models into SMT constraints, and 3) automatically simplifies and solves the corresponding verification and interpretability problems using commodity SMT solvers. We implement a prototype of UINT and evaluate it on real-world learning-based networking systems. Results demonstrate the efficiency and efficacy of UINT in verifying and interpreting key questions for these systems.}
}


@article{DBLP:journals/ton/FangHB25,
	author = {Wenzhi Fang and
                  Dong{-}Jun Han and
                  Christopher G. Brinton},
	title = {Federated Learning Over Hierarchical Wireless Networks: Training Latency
                  Minimization via Submodel Partitioning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1484--1499},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3538661},
	doi = {10.1109/TON.2025.3538661},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FangHB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional “star-topology” architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained wireless devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical cloud-edge-client networks. The key idea behind HIST is to divide the global model into disjoint partitions (or submodels) per round so that each group of clients (i.e., cells) is responsible for training only one partition of the model. We characterize the convergence behavior of HIST under mild assumptions, showing the impacts of several key attributes (e.g., submodel sizes, number of cells, edge and global aggregation frequencies) on the rate and stationarity gap. Building upon the theoretical results, we propose a submodel partitioning strategy to minimize the training latency depending on network resource availability and a target learning performance guarantee. We then demonstrate how HIST can be augmented with over-the-air computation (AirComp) to further enhance the efficiency of the model aggregation over the edge cells. Through numerical evaluations, we verify that HIST is able to save training time and communication costs by wide margins while achieving comparable accuracy as conventional HFL. Moreover, our experiments demonstrate that AirComp-assisted HIST provides further improvements in training latency.}
}


@article{DBLP:journals/ton/0057MXS0025,
	author = {Yu Liu and
                  Yingling Mao and
                  Xu Xu and
                  Xiaojun Shang and
                  Fan Ye and
                  Yuanyuan Yang},
	title = {A Nonblocking Multistage Switching Network for Distributed Quantum
                  Computing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1500--1513},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3544319},
	doi = {10.1109/TON.2025.3544319},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0057MXS0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum computing, utilizing the unique properties of quantum mechanics, has the potential to revolutionize various fields. However, current quantum processors face challenges in scaling the number of qubits, limiting their practical applications. In response, Distributed Quantum Computing (DQC) has emerged as a promising paradigm where multiple interconnected Quantum Processing Units (QPUs) collaborate to execute quantum circuits. In this paper, we focus on designing networks to interconnect QPUs for the implementation of DQC. We find that in real-world experiments and systems, the photon collection and coupling efficiency is low, leading to significant performance degradation in direct connection networks. To address this limitation, we propose a novel multistage switching network tailored for DQC, which has low system complexity and high entanglement generation rates. The proposed switching network comprises   \\log _{2}(N) \\log _{2}(N)   stages and   N/2 N/2   binary switches at each stage, where N represents the number of QPUs. We prove that the proposed network is nonblocking and develop an efficient routing algorithm with a time complexity of   \\mathcal {O}(N\\log (N)) \\mathcal {O}(N\\log (N))  . Additionally, we show the success probability of entanglement generation in the proposed switching network. Extensive simulations demonstrate that our network significantly outperforms the highly efficient circuit-switching Beneš network and three direct connection networks.}
}


@article{DBLP:journals/ton/ZhuHLQWD0T25,
	author = {Fengyuan Zhu and
                  Jiaquan He and
                  Jiajun Lin and
                  Yifan Qin and
                  Bingbing Wang and
                  Qilong Di and
                  Meng Jin and
                  Xiaohua Tian},
	title = {Inductor-Free LoRa Backscatter},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1514--1527},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3541422},
	doi = {10.1109/TON.2025.3541422},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhuHLQWD0T25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa backscatter achieves long-range communication at the cost of only tens of micro-watts of power when implemented in integrated circuits (ICs), which makes it a potential enabler for massive IoT. However, despite the above advantages, we find that the current tag design wastes approximately 72% of the chip functional area due to the use of large-size inductors in the impedance loads array. This inefficiency significantly increases the cost per chip during mass production. To address this die area issue, we propose OsTAG, a novel LoRa backscatter design that eliminates all inductors in the impedance loads array while maintaining the same quantization resolution. The key innovation lies in the creation of virtual impedance using oversampling. Realizing such design requires overcoming the technical challenges of mitigating approximation error and managing oversampling clock imperfections. To validate our design, we develop prototype and an IC, demonstrating promising results in terms of both performance and efficiency. OsTAG is expected to cost only 28% of the existing chip area while achieving comparable hundred-meter level communication range.}
}


@article{DBLP:journals/ton/AhmedOC25,
	author = {Imran Ahmed and
                  Eiji Oki and
                  Bijoy Chand Chatterjee},
	title = {AnDefrag: Analytical Model for Blocking Probabilities Considering
                  Defragmentation in Spectrally-Spatially Elastic Optical Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1528--1542},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3538715},
	doi = {10.1109/TON.2025.3538715},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AhmedOC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, space division multiplexing (SDM) technology, particularly multi-core and multi-mode fibers (MCMMFs), has been investigated to overcome physical limitations and enhance transport capacity. When SDM is integrated with elastic optical networks (EONs), it gives rise to an emerging technology known as spectrally-spatially elastic optical networks (SS-EONs). However, SS-EONs face significant challenges, such as fragmentation and crosstalk (XT), which increase blocking probability. Defragmentation is widely regarded as the most effective technique to mitigate fragmentation and reduce blocking probability. However, analytically assessing blocking probability with defragmentation is challenging due to the added constraints. Current studies on MCMMF-based SS-EONs typically rely on simulations or overlook defragmentation in their analytical models. This paper proposes AnDefrag, the first exact analytical continuous-time Markov chain model designed to calculate blocking probabilities in SS-EONs, taking into account defragmentation and the XT-avoided approach. AnDefrag generates all possible states and transitions, avoiding inter-core and inter-mode XTs for both single-class and multi-class requests. Single-class requests utilize an equal number of slots, while multi-class requests require different numbers of slots to meet clients’ needs. For cases where AnDefrag is not scalable, we introduce an iterative approximate model for single-hop links, which is extended to multi-hop networks. Numerical evaluations indicate that AnDefrag outperforms a non-defragmentation-aware benchmark model, as demonstrated through comparison with Monte Carlo simulations for a single-hop link.}
}


@article{DBLP:journals/ton/WangB0S25,
	author = {Shangshang Wang and
                  Simeng Bian and
                  Xin Liu and
                  Ziyu Shao},
	title = {Neural Constrained Combinatorial Bandits},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1543--1558},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3538709},
	doi = {10.1109/TON.2025.3538709},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangB0S25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Constrained combinatorial contextual bandits have emerged as trending tools in intelligent systems and networks to model reward and cost signals under combinatorial decision-making. On one hand, both signals are complex functions of the context, e.g., in federated learning, training loss (negative reward) and energy consumption (cost) are nonlinear functions of edge devices’ system conditions (context). On the other hand, there are cumulative constraints on costs, e.g., the accumulated energy consumption should be budgeted by energy resources. Besides, real-time systems often require such constraints to be guaranteed anytime or in each round, e.g., ensuring anytime fairness for task assignment to maintain the credibility of crowdsourcing platforms for workers. This bandit setting presents significant challenges, including modeling complex rewards/costs, satisfying anytime cumulative constraints, and balancing exploration and exploitation. Therefore, we propose a primal-dual algorithm (Neural-PD) with neural network-based estimations for rewards/costs and virtual queue-based optimization for constraints. Besides, we provide theoretical guarantees regarding the behavior of neural network training within the primal-dual framework and the dynamic neural tangent kernel (NTK) of the neural networks during online learning. By integrating NTK theory and Lyapunov-drift techniques, we prove Neural-PD achieves a sharp regret bound and a zero constraint violation. We also show Neural-PD outperforms existing algorithms with extensive experiments on both synthetic and real-world datasets.}
}


@article{DBLP:journals/ton/Zhu025,
	author = {Haowen Zhu and
                  Zehua Guo},
	title = {Revisiting the In-Network Aggregation in Distributed Machine Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1559--1573},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3539430},
	doi = {10.1109/TON.2025.3539430},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Zhu025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Distributed Machine Learning (DML) is proposed to accelerate machine learning model training by utilizing multiple training nodes to train models in parallel. Recent studies apply emerging In-Network Aggregation (INA) to further improve training efficiency by offloading the gradient aggregation process from hosts to programmable switches. However, existing INA solutions neither provide high training performance due to inefficient gradient aggregation with a single switch nor are easily deployed because of modifying the protocol stack in hosts. In this paper, we propose an easily deployable INA-based solution called Hierarchical In-Network Aggregation (HINA) to accelerate DML training process by hierarchically performing multiple aggregations in the Data Center Network (DCN). We formulate the gradient aggregation problem as the Joint Gradient Routing and Sending Rate (JGRSR) problem, which is a Mixed Integer Linear Programming (MILP) problem with high computation complexity. In addition, we propose HINA using progressive rounding and randomized rounding to determine the paths of gradient flows and the sending rates of training nodes to simplify and solve the JGRSR problem. Simulation results show that HINA reduces communication time by 61%-92% and decreases network load by 39%-66%, compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/AnselmiGR25,
	author = {Jonatha Anselmi and
                  Bruno Gaujal and
                  Louis{-}S{\'{e}}bastien Rebuffi},
	title = {Non-Stationary Gradient Descent for Optimal Auto-Scaling in Serverless
                  Platforms},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1574--1587},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3538982},
	doi = {10.1109/TON.2025.3538982},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AnselmiGR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To efficiently manage serverless computing platforms, a key aspect is the auto-scaling of services, i.e., the set of computational resources allocated to a service adapts over time as a function of the traffic demand. The objective is to find a compromise between user-perceived performance and energy consumption. In this paper, we consider the scale-per-request auto-scaling pattern and investigate how many function instances (or servers) should be spawned each time an unfortunate job arrives, i.e., a job that finds all servers busy upon its arrival. We address this problem by following a stochastic optimization approach: we develop a stochastic gradient descent scheme of the Kiefer-Wolfowitz type that applies over a single run of the state evolution. At each iteration, the proposed scheme computes an estimate of the number of servers to spawn each time an unfortunate job arrives to minimize some cost function. Under natural assumptions, we show that the sequence of estimates produced by our scheme is asymptotically optimal almost surely. In addition, we prove that its convergence rate is   O(n^{-2/3}) O ( n − 2 / 3 ) O(n^{-2/3})   where n is the number of iterations. From a mathematical point of view, the stochastic optimization framework induced by auto-scaling exhibits non-standard aspects that we approach from a general point of view. We consider the setting where a controller can only get samples of the transient – rather than stationary – behavior of the underlying stochastic system. To handle this difficulty, we develop arguments that exploit properties of the mixing time of the underlying Markov chain. By means of numerical simulations, we validate the proposed approach and quantify its gain with respect to common existing scale-up rules.}
}


@article{DBLP:journals/ton/0001LDS25,
	author = {Chao Luo and
                  Yinghua Li and
                  Fengqian Ding and
                  Rui Shao},
	title = {Spatio-Temporal Anomaly Detection for 5G-Clusters: {A} Multi-Scale
                  Fuzzy Contrastive Learning Approach},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1588--1602},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3540091},
	doi = {10.1109/TON.2025.3540091},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0001LDS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Anomaly detection is an important data-mining task closely related to specific applications. In recent years, spatio-temporal data deriving from various networks has been collected from many real-world scenarios, but how to detect anomalies in such data is still an open problem. In this article, we propose a novel anomaly detection framework, Multi-scale Fuzzy Contrastive Anomaly Detection (MFCAD), by capturing anomalous patterns of data from multiple spatio-temporal scales so as to learn distinguishable feature representations. Different from conventional reconstruction- or prediction-based anomaly detection methods, this approach is not concerned with the consistency of the encoded representation of the implicit layer and the discriminability of the implicit layer of the anomalies. MFCAD implements a more discriminative representation using fuzzy contrastive learning and explicitly performs anomaly detection in the potential space by measuring the distance between outliers and implicit values. This proposed method has been applied for the anomaly detection of 5G mobile network clusters (5G-MNCs) in China Mobile. Furthermore, in order to validate the generalizability of the proposed method, it is further tested on public datasets and experiment results show the promising performance.}
}


@article{DBLP:journals/ton/0002H25,
	author = {Nai{-}Wen Chang and
                  Sun{-}Yuan Hsieh},
	title = {Conditional Diagnosability of Enhanced Hypercubes Under the {PMC}
                  Model},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1603--1613},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3545178},
	doi = {10.1109/TON.2025.3545178},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0002H25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, due to the continuous expansion of the scale of multi-processor systems, processor fault diagnosis has become increasingly important in measuring system reliability. Diagnosability of many well-known multiprocessor systems has been extensively studied. Conditional diagnosability is a new system diagnostic measure that inserts an additional condition that all neighbors (adjacent nodes) of any node in the system cannot all fail at the same time. In this report, we evaluate the conditional diagnosability for enhanced hypercubes under the PMC model. We first give several properties about hypercubes and enhanced hypercubes, and then based on these properties, the conditional diagnosability of an   (n,k) (n,k)  -enhanced hypercube   Q_{n,k} Q_{n,k}   is proved to be   4n-7 4n-7   for   n\\ge 5 n\\ge 5   and   k=3 k=3  , and to be   4n-3 4n-3   for   n\\ge 6 n\\ge 6   and   5\\le k\\le n 5\\le k\\le n  .}
}


@article{DBLP:journals/ton/XuLZZM25,
	author = {Dongzhu Xu and
                  Rui Lin and
                  Huanhuan Zhang and
                  Anfu Zhou and
                  Huadong Ma},
	title = {Bridging Cross-Layer Interactions Between 5G {RAN} and {MEC} for Latency-Critical
                  Video Analytics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1614--1629},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3542456},
	doi = {10.1109/TON.2025.3542456},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuLZZM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) is a key component of 5G ecosystem, designed to support applications with stringent latency requirements. The fundamental idea is to deploy servers closer to end-users, such as on the network edge, rather than in remote clouds. While conceptually sound, operational 5G networks often lack coordination with MEC, leading to intolerably long response latency. In this work, we propose Sonata, which tightly integrates 5G RAN and MEC at the user space to ensure the performance of latency-critical video analytics. To achieve this, Sonata precisely customizes users’ service demands by fusing application-layer content changes from MEC servers with instantaneous physical-layer dynamics from the 5G Radio Access Network (RAN). It then enforces a deadline-strict resource provision to meet these service demands through real-time interactions between the 5G RAN and MEC servers, in a lightweight and standard-compatible manner. We prototype and evaluate Sonata on a software-defined 5G MEC platform. Our results demonstrate that Sonata achieves an average reduction in response latency of 67.82% compared to conventional 5G edge systems.}
}


@article{DBLP:journals/ton/XuF0025,
	author = {Songsong Xu and
                  Chuanpu Fu and
                  Qi Li and
                  Ke Xu},
	title = {"One Model Fits All Nodes": Neuron Activation Pattern Analysis-Based
                  Attack Traffic Detection Framework for {P2P} Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1630--1645},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3546735},
	doi = {10.1109/TON.2025.3546735},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuF0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) based network attack traffic detection is an emerging security paradigm, which is capable of capturing various advanced network attacks according to the features of traffic. When leveraging such promising security application to protect P2P services, particularly distributed cryptocurrency systems, one detection model should be deployed on many nodes to handle various unseen traffic patterns generated by nodes around the world. However, unseen yet benign traffic patterns are commonly classified as attack traffic, and thus trigger massive false-positive (FP) alarms. Unfortunately, the common practice of retraining models to reduce FPs is not salable for large-scale P2P networks, which incurs prohibitive labor efforts of collecting traffic on each node individually. To effectively deploy ML based attack traffic detection systems to protect distributed networks, we present tNeuron that automatically identifies FPs triggered by unseen traffic via neuron activation pattern analysis, such that it significantly improves the performance on various nodes. Specifically, we construct a shadow model with Transformer encoders to extract the knowledge of traffic patterns. Afterward, we train a model that learns how to classify FPs among alarms raised by ML models according to neuron activation patterns of the shadow model. Our experiments on real Ethereum nodes show that tNeuron can reduce 83.40% FP for seven state-of-the-art ML based attack detection systems, when detecting 15 kinds of P2P network attacks, thereby significantly improving detection accuracy in nine different metrics. In addition, tNeuron is robust against various adversarial examples constructed by existing evasion attacks. Besides, it achieves real-time detection and is capable of handling massive FPs generated by many nodes in large-scale distributed networks.}
}


@article{DBLP:journals/ton/HuangXZYZYAA00D25,
	author = {Chengyuan Huang and
                  Yibo Xiao and
                  Tianfan Zhang and
                  Chao Yang and
                  Dong Zhang and
                  Bingheng Yan and
                  Ahmed M. Abdelmoniem and
                  Gianni Antichi and
                  Xiaoliang Wang and
                  Fu Xiao and
                  Wanchun Dou and
                  Guihai Chen and
                  Hao Yin and
                  Chen Tian},
	title = {Troubleshooting Programmable Data Planes via Real-Time Table Information
                  Recording},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1646--1659},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3541884},
	doi = {10.1109/TON.2025.3541884},
	timestamp = {Tue, 14 Oct 2025 19:49:17 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangXZYZYAA00D25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the flexibility of programmable switches brings opportunities, it also introduces security risks. Hence, it is vital to conduct effective troubleshooting in the programmable switch to mitigate frequent network failures. However, troubleshooting programmable switch failures is challenging due to their enhanced flexibility and functionality compared to regular switches, posing increased difficulty in debugging, particularly with limited debugging tools and information. To address this problem, we propose an efficient troubleshooting method that records real-time information about packets in the data plane, including the tables involved in packet processing. Unfortunately, due to hardware limitations, it is infeasible to record all tables’ information in the data plane. Thus, the key is to find the table set reflecting the execution path a packet goes through while minimizing the resource overhead. We first represent P4 programs as a probabilistic transition directed acyclic graph (DAG) and employ information entropy to quantify the information within a set of tracked tables. Then, we adopt a two-step approach and design algorithms to find both optimal and approximately optimal table record plans. The evaluation results show the efficacy of the proposed method, including achieving the same path recovery rate as the related works with less than one-third of the resource consumption.}
}


@article{DBLP:journals/ton/ZhangMHZWR25,
	author = {Yiran Zhang and
                  Qingkai Meng and
                  Chaolei Hu and
                  Hu Zhang and
                  Shangguang Wang and
                  Fengyuan Ren},
	title = {ACK-Driven Congestion Control for Lossless Ethernet},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1660--1675},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3542770},
	doi = {10.1109/TON.2025.3542770},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangMHZWR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion control is a key enabler for lossless Ethernet at scale. In this paper, we revisit this classic topic from a new perspective, i.e., understanding and exploiting the intrinsic properties of the underlying lossless network. We experimentally and analytically find that the intrinsic properties of lossless networks, such as packet conservation, can indeed provide valuable implications in estimating pipe capacity and the precise number of excessive packets. Besides, we derive principles on how to treat congested flows and victim flows individually to handle HoL blocking efficiently. Then, we propose ACK-driven congestion control (ACC) for lossless Ethernet, which simply resorts to the knowledge of ACK time series (supports ACK coalescing) to exert a temporary halt to exactly drain out excessive packets of congested flows and then match its rate to pipe capacity. Testbed and large-scale simulations demonstrate that ACC ameliorates fundamental issues in lossless Ethernet (e.g., congestion spreading, HoL blocking, and deadlock) and achieves excellent low latency and high throughput performance. For instance, compared with existing schemes, ACC improves the average and 99th percentile FCT performance of small flows by   1.3\\sim 3.3\\times  1.3\\sim 3.3\\times    and   1.4\\sim 11.5\\times  1.4\\sim 11.5\\times   , respectively.}
}


@article{DBLP:journals/ton/Luo0YJP0025,
	author = {Huimin Luo and
                  Jiao Zhang and
                  Mingxuan Yu and
                  Jiafeng Jiang and
                  Yongchen Pan and
                  Tian Pan and
                  Tao Huang},
	title = {RoCELet: Host-Based Flowlet Load Balancing for RoCE},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1676--1688},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3542907},
	doi = {10.1109/TON.2025.3542907},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Luo0YJP0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote Direct Memory Access (RDMA) is becoming a popular high-speed networking technology. It uses kernel bypass and zero copy to achieve high throughput and low latency with little CPU overhead. However, standard RoCE transmission uses Equal Cost Multipath (ECMP) for load balancing, which can result in lower transmission performance due to hash conflicts. Meanwhile, it has been verified that, unlike TCP, the unique retransmission mode and flow characteristics of RoCE make previous load balancing algorithms not well applied to RoCE. In this paper, we introduce RoCELet, a load balancing algorithm for RoCE. It achieves fine-grained RoCE load balancing by actively generating flowlets, effectively utilizing the rich end-to-end paths in the data center. We implement a prototype based on DPDK and evaluate it through small-scale testbed experiments and large-scale simulations. Our results show that compared to state-of-the-art load balancing algorithms, RoCELet optimizes 48.2% and 16.4% in average FCT and   99^{th} 99 t h 99^{th}  -ile FCT, respectively.}
}


@article{DBLP:journals/ton/AslaniS25,
	author = {Rojin Aslani and
                  Ebrahim Saberinia},
	title = {Reinforcement Learning-Based Resource Allocation and Trajectory Design
                  for Full-Duplex {AAV} Communications in Highway Scenarios},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1689--1700},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3541993},
	doi = {10.1109/TON.2025.3541993},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AslaniS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper employs a full-duplex (FD) autonomous aerial vehicle (AAV) as a mobile base station (AAV-BS) to facilitate both uplink and downlink communication for a group of half-duplex (HD) vehicular users traversing a highway segment devoid of infrastructure. The system encounters self-interference (SI) from the FD AAV-BS and interference among HD vehicular users, thereby affecting the quality of service (QoS) provided to the users. To overcome these challenges, a reinforcement learning (RL)-based trajectory design and resource allocation scheme is proposed. This scheme optimizes the AAV-BS’s trajectory, transmission power, and frequency spectrum jointly for the AAV-BS and vehicular users, with the objective of maximizing the system data rate while ensuring QoS in both uplink and downlink transmissions. For this purpose, the trajectory design and resource allocation problem is modeled as an RL problem, and a Q-learning algorithm is proposed to address it. Simulation results demonstrate that the AAV-BS and vehicular users effectively learn to enhance the system data rate while meeting QoS requirements, adhering to spectrum allocation restrictions, managing limited power, and operating within trajectory constraints. The results also highlight the superior performance of our scheme compared to alternative approaches.}
}


@article{DBLP:journals/ton/Li0SFF025,
	author = {Haibin Li and
                  Qi Li and
                  Yingying Su and
                  Xuewei Feng and
                  Chuanpu Fu and
                  Ke Xu},
	title = {StateShield: Real-Time Defenses Against Information Leakage Over Connectionless
                  Protocols},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1701--1716},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3543734},
	doi = {10.1109/TON.2025.3543734},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Li0SFF025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Connectionless protocols such as ICMP and UDP are manipulated to construct novel information leakage channels by which attackers can disrupt TCP connections or leak secret information. Existing solutions have mainly focused on repairing vulnerable protocols through OS patches, which are OS-specific and slow to deploy. Other traditional defenses either cannot cover these attacks or are prone to incur unintended dropping of legitimate packets due to the heavily manipulated IP spoofing technique in these attacks. In this paper, we present StateShield, an in-network, real-time defense against state-of-the-art information leakage attacks over connectionless protocols. StateShield can detect and defend against various information leakage attacks without incurring unintended dropping of legitimate traffic, even when attackers heavily spoof the IP addresses of legitimate clients. To achieve that, we propose three indicators that can cover major attack vectors of connectionless information leakage channels and are effective for detecting more than ten attack variants. We design the architecture of StateShield based on programmable switches, with efficient data structures for monitoring and on-demand defense components in the data plane. We develop two novel defense components to mitigate UDP and ICMP-based information leakage channels automatically while achieving minimal unintended dropping of legitimate packets. Our extensive experiments show that StateShield can effectively mitigate more than ten attack variants in real time without hurting the services over legitimate connectionless packets, and the defense provided by StateShield is robust under high-intensive background traffic over connectionless protocols.}
}


@article{DBLP:journals/ton/0001WMLC25,
	author = {Chetna Singhal and
                  Yashuo Wu and
                  Francesco Malandrino and
                  Marco Levorato and
                  Carla{-}Fabiana Chiasserini},
	title = {Distributing Inference Tasks Over Interconnected Systems Through Dynamic
                  DNNs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1717--1730},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3543848},
	doi = {10.1109/TON.2025.3543848},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0001WMLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An increasing number of mobile applications leverage deep neural networks (DNN) as an essential component to adapt to the operational context at hand and provide users with an enhanced experience. It is thus of paramount importance that network systems support the execution of DNN inference tasks in an efficient and sustainable way. Matching the diverse resources available at the mobile-edge-cloud network tiers with the applications requirements and the complexity of their, while minimizing energy consumption, is however challenging. A possible approach to the problem consists in exploiting the emerging concept of dynamic DNNs, characterized by multi-branched architectures with early exits enabling sample-based adaptation of the model depth. We leverage this concept and address the problem of deploying portions of DNNs with early exits across the mobile-edge-cloud system and allocating therein the necessary network, computing, and memory resources. We do so by developing a 3-stage graph-modeling method that allows us to represent the characteristics of the system and the applications as well as the possible options for splitting the DNN over the multi-tier network nodes. Our solution, called Feasible Inference Graph (FIN), can determine the DNN split, deployment, and resource allocation that minimizes the inference energy consumption while satisfying the nodes’ constraints and the requirements of multiple, co-existing applications. FIN closely matches the optimum and leads to over 89% energy savings with respect to state-of-the-art alternatives.}
}


@article{DBLP:journals/ton/YangZZWYZD025,
	author = {Li Yang and
                  Yifei Zou and
                  Zuyuan Zhang and
                  Peng Wang and
                  Dongxiao Yu and
                  Anatolij Zubow and
                  Falko Dressler and
                  Xiuzhen Cheng},
	title = {Jamming-Resilient Physical-to-Virtual Communications in Digital Twin
                  Edge Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1731--1745},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3550426},
	doi = {10.1109/TON.2025.3550426},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangZZWYZD025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As an integration of digital twin and edge computing, the digital twin edge networks (DITENs) have been proposed in recent years to fill the gap between physical edge networks and digital systems. Meanwhile, the multi-access wireless environments in edge computing make it hard to provide ultra-reliable and low-latency communications for digital twin, especially when the jamming attacks can be launched by the adversaries. This paper studies the jamming-resilient physical-to-virtual communication (PTVC) problem in DITENs despite strong cooperative jamming. Note that the previous jamming models mainly focus on the jamming behaviors from individual adversaries and are restricted by the energy budget limitation and uniform jamming assumption. In this paper, we consider a more comprehensive jamming model, in which f adversaries can cooperatively launch their jamming attacks in totally k wireless channels with unlimited power budget and non-uniform jamming signals. Then, based on the new proposed   (k,f) (k,f)  -cooperative jamming model, we show that   k\\gt f k\\gt f   is the necessary and also sufficient condition to solve the PTVC problem. On one hand, we prove that the PTVC problem is insoluble when   f\\leq k f\\leq k  ; on the other hand, two distributed algorithms are given as the solutions of the PTVC problem among n physical objectives and one sink node when   k\\gt f k\\gt f  , the time complexity of which are   O\\left ({{\\frac {n \\log n}{k (\\log k-\\log f)}}}\\right) O\\left ({{\\frac {n \\log n}{k (\\log k-\\log f)}}}\\right)   and   O\\left ({{\\frac {n \\log n}{\\log k-\\log f}}}\\right) O\\left ({{\\frac {n \\log n}{\\log k-\\log f}}}\\right)   based on the communication modes with/without acknowledgement, respectively. Both of the theoretical results and empirical simulations are conducted to show the resilience of our algorithms despite such a strong cooperative jamming model.}
}


@article{DBLP:journals/ton/YuanW00MGZ025,
	author = {Xinjie Yuan and
                  Mingzhou Wu and
                  Zhi Wang and
                  Yifei Zhu and
                  Ming Ma and
                  Junjian Guo and
                  Zhi{-}Li Zhang and
                  Wenwu Zhu},
	title = {Understanding 5G Performance for Real-World Services: {A} Content
                  Provider's Perspective},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1746--1761},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3545790},
	doi = {10.1109/TON.2025.3545790},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuanW00MGZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent years have witnessed a rapid growth of both 5G coverage and 5G users, attracting several measurement studies on its coverage, reliability and quality of service. However, the capabilities and potential impacts of 5G, especially Standalone (SA) 5G, still remain to be fully understood from a content provider (CP)’s perspective. This paper fills this gap by studying 5G networks used by over 23 million users in one year in Kuaishou, a popular crowdsourced live streaming platform. With passive and active measurements, we have the following key findings: i) SA 5G generally provides end-to-end performance improvements compared to 4G or Non-Standalone (NSA) 5G, but its advantage depends on both the number of cellular users and CP-level configurations. ii) In the radio access networks, SA 5G is more sensitive to access density, but has better handover tolerance. iii) Controlled experiments with 29 mobile device models on energy consumption refute some “conventional wisdom,” including the notion that 5G always consumes more power. iv) Traceroute-based active experiments in over 300 cities show that although users are “closer” to the internet in SA 5G due to the control and user plane separation, whether end-to-end latency benefits from that partly depends on the routing policy at the gateways. Furthermore, we propose a 5G-aware rebuffer strategy tested by 9 million viewers in Kuaishou, showing a 7% reduction in rebuffer proportion. Finally, we also provide new design space for other 5G participants.}
}


@article{DBLP:journals/ton/RenZ00WZC25,
	author = {Jie Ren and
                  Weiting Zhang and
                  Hongchao Wang and
                  Dong Yang and
                  Shuang Wang and
                  Hongke Zhang and
                  Shuguang Cui},
	title = {Toward Deterministic Wide-Area Networks via Deadline-Aware Routing
                  and Scheduling},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1762--1778},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3546678},
	doi = {10.1109/TON.2025.3546678},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RenZ00WZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The widespread adoption of real-time services on the Internet has aroused interest in the study of low-latency and deterministic communications. Deterministic guarantee over wide-area networks (WANs), the primary infrastructure for communications, is essential to achieving end-to-end deterministic transmission. However, applying off-the-shelf deterministic schemes to WANs is challenging due to the statistical multiplexing nature of WANs and the non-periodic nature of WAN traffic. In this paper, we propose a novel deterministic framework for WANs, named DetWAN, which guarantees the timely delivery of WAN traffic via deadline-aware routing and scheduling. We design a coordinated earliest deadline first (CEDF) scheduling scheme in the data plane of the DetWAN, which provides determinism for non-periodic deadline-constrained traffic while following statistical multiplexing. To precisely estimate the capacity of deadline-constrained traffic that the DetWAN can satisfy, we derive an end-to-end deadline satisfiability criterion in the DetWAN by introducing the deadline curve into traffic modeling. Based on the criterion, we formulate the deadline-aware routing and scheduling problem as a stochastic optimization problem to maximize the timely delivery ratio. Furthermore, we propose a distributed admission control algorithm based on multi-agent deep reinforcement learning in the control plane to solve the problem in a highly autonomous manner. The algorithm can jointly determine optimal routes and per-hop deadline budgets for traffic flows in a decentralized mode. Extensive evaluation results validate the deterministic guarantee as well as the high throughput of the DetWAN and show that the proposed admission control algorithm can significantly improve the timely delivery ratio compared with benchmarks in WAN scenarios.}
}


@article{DBLP:journals/ton/ZhuHZC0Z25,
	author = {Tong Zhu and
                  Zhen Huang and
                  Lu Zhou and
                  Guoxing Chen and
                  Yan Meng and
                  Haojin Zhu},
	title = {Collaborative Ad Fraud Detection in Ad Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1779--1794},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3547977},
	doi = {10.1109/TON.2025.3547977},
	timestamp = {Sat, 15 Nov 2025 13:54:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuHZC0Z25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile advertising has been significantly propelled by the advent of in-app programmatic advertising and Real-Time Bidding (RTB) technologies. However, it suffers from ad fraud incidents in ad networks, including click injection, covert background ad activities, and etc. While previous research has predominantly focused on ad fraud localized within individual apps or specific devices, this paper delineates a newly identified form of collusion-based ad fraud, termed ad attribution laundering fraud (ALF). ALF involves multiple apps conspiring to obfuscate the true origins of where advertisements are displayed, thereby allowing lower-quality apps to leverage the reputations of ostensibly legitimate ones. To detect ALF, we developed the detection tool, AlfScan-X, which efficiently identifies potential collaborative apps among millions in the wild by heuristically prioritizing candidate apps likely to be involved in ALF for prompt analysis. AlfScan-X maintains an online APK crawler and an   \\textsf {AppID} \\textsf {AppID}   database to enhance AlfScan-X’s responsiveness, adaptability, and reduce false negatives. Overcoming challenges of identity extraction from diverse and obfuscated apps, AlfScan-X utilizes a combination of static and dynamic analysis techniques to cross-verify app identities, pinpointing instances of ALF. Our evaluation of AlfScan-X on a 200-app ground truth dataset yielded high effectiveness with 92% precision and 92% recall. AlfScan-X identified   4,515 4,515   unique fraudulent apps and   1,483 1,483   fraudulent clusters, revealing significant patterns and implications of fraudulent apps and highlighting reliability issues in both third-party app development frameworks and advertising networks.}
}


@article{DBLP:journals/ton/Zhang0SGWC25,
	author = {Hanwen Zhang and
                  He Huang and
                  Yu{-}E Sun and
                  Guoju Gao and
                  Zhaojie Wang and
                  Shigang Chen},
	title = {Multi-Information Sampling and Mixed Estimation for Multi-Task Spread
                  Measurement With Supercube},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1795--1810},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3546668},
	doi = {10.1109/TON.2025.3546668},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Zhang0SGWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Spread measurement is an essential problem in high-speed networks with broad applications, such as anomaly detection and network telemetry. Network administrators typically need to concurrently monitor the spreads of different types of flows to detect various abnormal behaviors. Although many studies have designed memory-efficient structures, such as sketches, for a specific spread measurement task, they have to deploy multiple sketches to support multiple spread measurement tasks, resulting in significant memory and computational overhead. This paper proposes an efficient multi-task information compression method to simultaneously estimate differently defined flow spreads. We introduce multi-information sampling to capture multi-task spread information from each arriving packet by one pass and store it in off-chip memory, thereby conserving on-chip memory and computational resources. Additionally, we carefully designed a one-access multi-dimensional structure called Supercube to preserve as much spread information as possible while catching up with the line rate, thereby enhancing estimation accuracy. We implement our estimator in hardware using NetFPGA. Experiments based on real Internet traces show that our method reduces the ARE by 83.36% for spread estimation compared to rSkt (SOTA) with 300KB of on-chip memory and increases update throughput by 251.252-fold compared to Supersketch. All source codes are available at https://github.com/Hanwen808/MIME.}
}


@article{DBLP:journals/ton/Yao00LXW25,
	author = {Zhiwei Yao and
                  Ji Qi and
                  Yang Xu and
                  Yunming Liao and
                  Hongli Xu and
                  Lun Wang},
	title = {PairingFL: Efficient Federated Learning With Model Splitting and Client
                  Pairing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1811--1825},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3547984},
	doi = {10.1109/TON.2025.3547984},
	timestamp = {Wed, 29 Oct 2025 15:09:47 +0100},
	biburl = {https://dblp.org/rec/journals/ton/Yao00LXW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has recently gained tremendous attention in edge computing and the Internet of Things, due to its capability of enabling clients to perform model training at the network edge or end devices (i.e., clients). However, these end devices are usually resource-constrained without the ability to train large-scale models. In order to accelerate the training of large-scale models on these devices, we incorporate Split Learning (SL) into Federated Learning (FL), and propose a novel FL framework, termed PairingFL. Specifically, we split a full model into a bottom model and a top model, and arrange participating clients into pairs, each of which collaboratively trains the two partial models as one client does in typical FL. Driven by the advantages of SL and FL, PairingFL is able to relax the computation burden on clients and protect model privacy. However, considering the features of system and statistical heterogeneity in edge networks, it is challenging to pair the clients by carefully developing the strategies of client partitioning and matching for efficient model training. To this end, we first theoretically analyze the convergence property of PairingFL, and obtain a convergence upper bound. Guided by this, we then design a greedy and efficient algorithm, which makes the joint decision of client partitioning and matching, so as to well balance the trade-off between convergence rate and model accuracy. The performance of PairingFL is evaluated through extensive simulation experiments. The experimental results demonstrate that PairingFL can speed up the training process by   4.6\\times  4.6\\times    compared to baselines when achieving the corresponding convergence accuracy.}
}


@article{DBLP:journals/ton/HanLXS025,
	author = {Jiangping Han and
                  Jinhao Liu and
                  Kaiping Xue and
                  Qibin Sun and
                  Jun Lu},
	title = {Toward High-Quality Real-Time Video Streaming: An Efficient Multi-Stream
                  and Multi-Path Scheduling Framework},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1826--1839},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3547278},
	doi = {10.1109/TON.2025.3547278},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HanLXS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Real-time video streaming requires high throughput and low delivery time for enhanced user’s Quality of Experience (QoE). This motivates the use of multi-path transmission to improve performance. However, ensuring target performance within specified deadlines and priorities for video frames is particularly crucial for real-time communication and video quality, especially in scenarios with limited resources. To address this challenge, we propose a novel framework, vStreamPth, to guarantee high-quality real-time video streaming through multi-path transmission. For essential quality assurance, vStreamPth incorporates key requirement indicators that guide the transmission decisions of video frames across predefined multiple paths. In this framework, lightweight and robust decision-making is achieved through the collaboration of application-oriented and network-oriented data scheduling. Specifically, it employs robustness estimation to maintain the non-blocking delivery of frames, and further applies online fine-tuning to correct variations caused by changes in end-to-end transmission and multi-path network conditions. We implement a prototype of vStreamPth in Linux user space and conduct a thorough evaluation. Experimental results demonstrate the absolute improvement of vStreamPth in achieving high QoE and deadline satisfaction ratio compared to existing multi-path solutions.}
}


@article{DBLP:journals/ton/GuoZ025,
	author = {Hengquan Guo and
                  Qi Zhu and
                  Xin Liu},
	title = {Safe Learning in Stochastic Continuum-Armed Bandit With Constraints
                  and Its Application to Network Resource Management},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1840--1853},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3550732},
	doi = {10.1109/TON.2025.3550732},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the problem of stochastic continuum-armed bandit with constraints (SCBwC), where we optimize an unknown reward function subject to an unknown constraint function over a continuous space. We model reward and constraint functions via Gaussian processes (GPs) and propose a Rectified Double-Optimistic Learning framework (RDOL), a penalty-based method incorporating double-optimistic GP bandit learning for reward and constraint functions, respectively. We consider the metric of cumulative constraint violation, which is strictly stronger than the traditional long-term constraint violation. The rectified design for the penalty update and the optimistic learning for the constraint function in RDOL guarantee the cumulative constraint violation is minimal. RDOL can achieve sublinear regret and cumulative constraint violation for SCBwC and its variants (e.g., under delayed feedback and non-stationary environment). These theoretical results match their unconstrained counterparts. We implement the framework into the problem of online resource allocation in data centers. The experimental results justify that RDOL outperforms several existing baseline algorithms.}
}


@article{DBLP:journals/ton/YangDXHWS025,
	author = {Jiayu Yang and
                  Kunpeng Ding and
                  Kaiping Xue and
                  Jiangping Han and
                  David S. L. Wei and
                  Qibin Sun and
                  Jun Lu},
	title = {{HPR-DS:} {A} Hybrid Proactive Reactive Defense Scheme Against Interest
                  Flooding Attack in Named Data Networking},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1854--1869},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3546547},
	doi = {10.1109/TON.2025.3546547},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangDXHWS025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named Data Networking (NDN) has emerged as a promising network paradigm for the future Internet. It revolutionizes content retrieval by decoupling it from specific locations, thereby overcoming the limitations of traditional IP addressing and significantly enhancing data delivery efficiency. Additionally, NDN’s stateful forwarding plane for routers enables robust aggregation of identical requests, bolstering resistance against Distributed Denial of Service (DDoS) attacks. Despite these advancements, NDN remains vulnerable to the Interest Flooding Attack (IFA), wherein excessive requests from attackers can compromise transmission quality by depleting router resources. In the current landscape, researchers have proposed various strategies aimed at improving the accuracy, timeliness, and cost-effectiveness of defenses against IFA attacks, presuming stable user behavior. However, several challenges persist in effectively countering IFA attacks, including the need to ensure transmission quality throughout users’ lifecycles, eliminate attacks at their origin, and adapt to dynamic user behaviors. In response to these challenges, this paper presents the Hybrid Proactive Reactive Defense Scheme (HPR-DS). HPR-DS employs distinct proactive and reactive modules for resource management and user behavior analysis, respectively, at intermediate and edge nodes. It employs time series analysis to gauge evolving resource requirements and maintains separate resource pools for each content. Additionally, HPR-DS utilizes multidimensional data clustering to accurately identify attackers. Simulation results demonstrate the superior performance of HPR-DS in safeguarding user transmission quality throughout the entirety of their lifecycle and in enhancing detection precision in dynamic network environments.}
}


@article{DBLP:journals/ton/YueQD0ZZ25,
	author = {Sheng Yue and
                  Zerui Qin and
                  Yongheng Deng and
                  Ju Ren and
                  Yaoxue Zhang and
                  Junshan Zhang},
	title = {AugFL: Augmenting Federated Learning With Pretrained Models},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1870--1885},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3548277},
	doi = {10.1109/TON.2025.3548277},
	timestamp = {Sun, 07 Dec 2025 22:17:56 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YueQD0ZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated Learning (FL) has garnered widespread interest in recent years. However, owing to strict privacy policies or limited storage capacities of training participants such as IoT devices, its effective deployment is often impeded by the scarcity of training data in practical decentralized learning environments. In this paper, we study enhancing FL with the aid of (large) pre-trained models (PMs), that encapsulate wealthy general/domain-agnostic knowledge, to alleviate the data requirement in conducting FL from scratch. Specifically, we consider a networked FL system formed by a central server and distributed clients. First, we formulate the PM-aided personalized FL as a regularization-based federated meta-learning problem, where clients join forces to learn a meta-model with knowledge transferred from a private PM stored at the server. Then, we develop an inexact-ADMM-based algorithm, AugFL, to optimize the problem with no need to expose the PM or incur additional computational costs to local clients. Further, we establish theoretical guarantees for AugFL in terms of communication complexity, adaptation performance, and the benefit of knowledge transfer in general non-convex cases. Extensive experiments corroborate the efficacy and superiority of AugFL over existing baselines.}
}


@article{DBLP:journals/ton/TanW00DHJ025,
	author = {Haisheng Tan and
                  Yi Wang and
                  Chi Zhang and
                  Guopeng Li and
                  Haohua Du and
                  Zhenhua Han and
                  Shaofeng H.{-}C. Jiang and
                  Xiang{-}Yang Li},
	title = {Asymptotically Tight Approximation for Online File Caching With Delayed
                  Hits and Bypassing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1886--1899},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3549289},
	doi = {10.1109/TON.2025.3549289},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TanW00DHJ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In latency-sensitive file caching systems such as Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC), the latency of fetching a missing file to the local cache can be significant. Recent studies have revealed that successive requests for the same missing file before the fetching process completes could still suffer latency (so-called delayed hits). Motivated by the practical scenarios, we study the online general file caching problem with delayed hits and bypassing, i.e., a request may be bypassed and processed directly at the remote data center. The objective is to minimize the total request latency. We present a general reduction that turns a traditional file caching algorithm into one that can handle delayed hits. Based on this reduction, we propose an efficient online file caching algorithm, called CaLa, with an asymptotically tight competitive ratio as   O(Z \\log K) O(Z \\log K)  , where Z is the maximum fetching latency of any file and K is the cache size. Extensive simulations on the production data trace from Google and the Yahoo benchmark illustrate that CaLa can reduce the latency by up to 8.48% compared with the state-of-the-art schemes dealing with delayed hits without bypassing, and this improvement increases to 26.00% if bypassing is allowed. Furthermore, by upgrading the method for estimating files’ weights in CaLa, we propose CaLa+, which further reduces the total latency by more than 5%.}
}


@article{DBLP:journals/ton/0024MZ0DXSF025,
	author = {Lei Zhang and
                  Yazhou Ma and
                  Mingzi Zuo and
                  Zhen Ling and
                  Changyu Dong and
                  Guangquan Xu and
                  Lin Shu and
                  Xiaochen Fan and
                  Qian Zhang},
	title = {Toward Cross-Environment Continuous Gesture User Authentication With
                  Commercial Wi-Fi},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1900--1915},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3548464},
	doi = {10.1109/TON.2025.3548464},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0024MZ0DXSF025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Behavior biometrics-based user authentication with Wi-Fi gains significant attention due to its ubiquitous and contact-free manners. An individual’s identity can be verified by analyzing activities induced signal variances, excellently balancing the security demands and user experience. However, the inherent complexity of Wi-Fi signals presents significant challenges for behavior biometrics-based user authentication. The susceptibility of Wi-Fi signals results in a poor cross-environment generalization capability, which is overlooked by the existing research. In addition, most existing works of behavior-based user authentication are based on one-off activity. This makes them vulnerable to zero-effort attacks and imitation attacks. To address these issues, we propose a cross-environment continuous gesture-based user authentication framework with Wi-Fi, dubbed Wi-CGAuth. Specifically, the cross-environment generalization capability is enhanced by the cross-layer joint optimization approach. At the lowest signal layer, the signals’ time, spatial, and frequency diversity are extended maximally, by a novel, subcarrier-level, cost-effective signal optimization strategy. At the middle layer, the multi-view fusion method, i.e., multi-transfer component analysis (TCA), is applied to refine the signals from transceiver pairs after signal preprocessing. The continuous gesture segmentation problem is modeled as the classification problem, which is solved by CNN. At the upper layer, a Convolutional Neural Network-Transformer (CNN-Transformer) model is employed to achieve the dual task of effective user authentication and accurate gesture recognition. After extensive experiments in three typical indoor scenarios, Wi-CGAuth can achieve an average authentication accuracy of 92.7%, demonstrating its robustness and effectiveness.}
}


@article{DBLP:journals/ton/FanSDHLYWY25,
	author = {Xiaoxuan Fan and
                  Jiaqi Sun and
                  Xianjun Deng and
                  Shibo He and
                  Shenghao Liu and
                  Lingzhi Yi and
                  Jing Wang and
                  Laurence Tianruo Yang},
	title = {Effective Multivariate Voice Liveness Detection System for Internet
                  of Things Security},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1916--1929},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3553313},
	doi = {10.1109/TON.2025.3553313},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FanSDHLYWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Voice assistants, as crucial components of the Internet of Things (IoT), are vulnerable to voice spoofing attacks and pose great threats to the security of IoT. Passive liveness detection distinguishes between genuine and spoofing voices by analyzing the collected voice, eliminating the need for deploying additional sensors. This method plays a crucial role in detecting spoofing speeches and ensuring the security of the IoT. However, current passive liveness detection methods typically require users to adopt specific gestures. Meanwhile, these methods are often designed for specific attacks and cannot accommodate multivariate attacks. To address these challenges, this paper proposes an efficient and robust liveness feature called VoiceID, which utilizes the inherent vocal cord vibrations and voiced language to authenticate the collected voice. The VoiceID is defined as the set of maximum magnitude-peak frequency bins in the magnitude spectrum of each frame for voice. VoiceID can be combined with existing acoustic features to compensate for the granularity gap in extracting fine-grained features and distinguishing between genuine and spoofing voices. Furthermore, to leverage VoiceID, this paper proposes a solid fake voice liveness detection system named SFSys and elaborates on a series of acoustic features that can work with VoiceID. Extensive experiments on authoritative ASVspoof 2019 and ASVspoof 2021 datasets reveal that VoiceID reduces the equal error rate and the minimum tandem decision cost function of the existing acoustic features by at most 6.19% and 0.2479. Moreover, SFSys outperforms existing voice liveness detection schemes and exhibits robustness in various advanced spoofing attack environments.}
}


@article{DBLP:journals/ton/HuangYCZLYPZ025,
	author = {Huawei Huang and
                  Zhaokang Yin and
                  Qinde Chen and
                  Jian Zheng and
                  Xiaofei Luo and
                  Guang Ye and
                  Xiaowen Peng and
                  Zibin Zheng and
                  Song Guo},
	title = {BrokerChain: {A} Blockchain Sharding Protocol by Exploiting Broker
                  Accounts},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1930--1945},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3550502},
	doi = {10.1109/TON.2025.3550502},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangYCZLYPZ025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {State-of-the-art blockchain sharding solutions, such as Monoxide, can cause severely imbalanced distribution of transaction (TX) workloads across all blockchain shards due to the deployment policy of their accounts. Imbalanced TX distributions then produce hot shards, in which the cross-shard TXs may experience an unlimited confirmation latency. Thus, how to address the hot-shard issue and how to reduce cross-shard TXs become significant challenges of blockchain sharding. Through reviewing the related studies, we find that a cross-shard TX protocol that can achieve workload balance among all shards and simultaneously reduce the quantity of cross-shard TXs is still absent from the literature. To this end, we propose BrokerChain, which is a cross-shard blockchain protocol dedicated to account-based state sharding. Essentially, BrokerChain exploits fine-grained state partition and account segmentation. We also elaborate on how BrokerChain handles cross-shard TXs through broker accounts. The security issues and other properties of BrokerChain are analyzed rigorously. Finally, we conduct comprehensive evaluations using an open-source blockchain sharding prototype named BlockEmulator. The evaluation results show that BrokerChain outperforms other baselines in terms of transaction throughput, transaction confirmation latency, the queue size of the transaction pool, and workload balance.}
}


@article{DBLP:journals/ton/Cobblah0KOXG25,
	author = {Christian Nii Aflah Cobblah and
                  Qi Xia and
                  Goodlet Akwasi Kusi and
                  Isaac Amankona Obiri and
                  Hu Xia and
                  Jianbin Gao},
	title = {A Blockchain-NDN Enabled Framework for Secure Vehicular Networking},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1946--1961},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3549042},
	doi = {10.1109/TON.2025.3549042},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Cobblah0KOXG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named Data Networking (NDN) has proven to be a suitable candidate for Vehicular Ad-hoc Networks (VANETs) because of its data-centric nature and as a worthy replacement for IP addressing, particularly for those with high mobility like VANETs. This has led to the emergence of Vehicular Named Data Networking (VNDN). With the blockchain’s ability to ensure immutability, transparency, accountability, and trust, the combination of blockchain and NDN in the VANETs environment has the propensity to alleviate many security issues in VANETs. This paper introduces a blockchain-enabled NDN framework that guarantees a trustworthy and secure data-sharing network in VNDN. Moreover, we utilized an effective reputation mechanism to facilitate a trustworthy and honest data provision in our mobility network. We also adopted a collaborative caching mechanism to improve our system performance, as caching is one of the pivotal reasons for VNDN. We simulated our work using SUMO and ndnSIM and tested our framework against other related systems. The findings show that our proposed approach enhances performance depending on the parameters used.}
}


@article{DBLP:journals/ton/0013LHY025,
	author = {Pengfei Wang and
                  Shiqi Li and
                  Yuqi Han and
                  Feiye Ye and
                  Qiang Zhang},
	title = {Fast-Response Edge Caching Scheme for Graph Data},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1962--1975},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3548904},
	doi = {10.1109/TON.2025.3548904},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0013LHY025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {By deploying distributed storage space on edge servers, mobile edge networks significantly enhance computation and transmission efficiency for wireless tasks. The selection of an appropriate caching policy not only optimizes bandwidth utilization but also alleviates network congestion. Given the intricate connectivity and vast data volume in edge computing, coupled with users’ demand for rapid response times, proposing a cache solution that closely matches data attributes becomes imperative to enhance overall efficiency. In this paper, we introduce RECG, a high-speed edge caching scheme designed specifically for graph data, leveraging the intricate data connectivity. RECG generates query graphs from edge servers to ensure swift and accurate identification of popular nodes. Additionally, we introduce a rapid hot-spot propagation partitioning technique to optimize the partitioning process, increasing the hit rate of partitioned subgraphs in the cache while reducing runtime. Our experimental evaluation, conducted on real-world datasets, compares RECG with specialized edge caching graph partitioning algorithms such as LGPE and other baseline algorithms. The thorough experimental results demonstrate the advantages of the proposed algorithm in terms of cache hit rate and processing delay.}
}


@article{DBLP:journals/ton/ZhengE25,
	author = {Yilin Zheng and
                  Atilla Eryilmaz},
	title = {State-Independent Control for Constrained Markov Decision Processes
                  With Birth-Death Dynamics},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1976--1988},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3553858},
	doi = {10.1109/TON.2025.3553858},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhengE25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In many applications, we regularly face the fundamental problem of allocating a common resource (funding, time, energy, etc.) among a network of processes that evolve in a continuous-space according to a birth-death dynamics. The state of each process tends to gradually improve with the resource and gradually degrade without it. Formulated as a Constrained Markov Decision Process (CMDP), the problem is typically attacked in the continuous space directly using the function approximation method or in discrete space with a fixed granularity. In this work, we investigated an alternative method based on the fact that the granularity of discretization has a crucial impact on the size and the evolution of the state-space. Increasing the granularity has the desirable effect of increasing the control of the processes (due to increased interaction regularity), but it also comes with the burden of an increasing state-space. Without function approximation, it is well-known that finding the optimal solution of CMDP is formidably difficult as the state-space grows. We have taken a fresh look at designing State-Independent policies whose complexity does not scale with the discretization granularity parameter n of the underlying continuous space processes. In particular, for a constrained-resource allocation problem over birth-death type processes, we developed state-independent policies that guarantee asymptotic-optimality as the discretization granularity n grows. We also show, through numerical comparisons, that our design has a lower running time compared to alternative designs including index-based methods and function approximation methods.}
}


@article{DBLP:journals/ton/HoangNL025,
	author = {Linh T. Hoang and
                  Chuyen T. Nguyen and
                  Hoang D. Le and
                  Anh T. Pham},
	title = {Adaptive 3D Placement of Multiple UAV-Mounted Base Stations in 6G
                  Airborne Small Cells With Deep Reinforcement Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {1989--2004},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3552097},
	doi = {10.1109/TON.2025.3552097},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HoangNL025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Uncrewed Aerial Vehicle-mounted Base Stations (UAV-BSs) have been envisioned as a promising solution to enable high-quality services in next-generation mobile networks. With inherent flexibility, one key challenge is placing the UAV-BSs adaptively to time-varying network conditions to maintain stable connections. Conventional methods mainly focus on optimizing UAV-BS deployment in static networks where users are static or with limited mobility. This study considers a dynamic network where multiple non-stationary UAV-BSs are deployed to serve mobile users with time-varying heterogeneous traffic. Incomplete downloads of users are backlogged in download queues at the UAV-BSs, and together with the newly requested traffic, the download queue size reflects the user’s instantaneous traffic demand. With constraints on the queue stability, we aim to maximize the user’s long-term mean opinion score (MOS), reflecting how the perceived data rate satisfies their traffic demand. Since the users’ location and traffic demand vary over time, we dynamically group users into clusters using a K-means-based algorithm and adjust the 3D location of UAV-BSs using an actor-critic deep reinforcement learning (DRL) framework. The actor module is encoded using a deep neural network (DNN) that obtains the UAV-BS’s current location and a traffic heatmap of users to predict the optimal movement for UAV-BSs. The critic module utilizes Lyapunov optimization to control the queue stability constraint and evaluate the actor’s decisions. Extensive simulations demonstrate the proposed method’s superior performance over conventional rate-maximization approaches.}
}


@article{DBLP:journals/ton/Chen0025,
	author = {Wei Chen and
                  Ye Tian and
                  Xinming Zhang},
	title = {AccelToR: Accelerating {TCP} for Circuit/Packet Hybrid Data Centers
                  With Packet Scheduling},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2005--2019},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3550902},
	doi = {10.1109/TON.2025.3550902},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/Chen0025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To overcome the inherent limitation of the optical circuit switch (OCS) while utilizing its high bandwidth, circuit/packet hybrid networks are widely proposed for modern data centers. However, as today’s OCS has reduced the reconfiguration delay to microseconds, the circuit from a source rack to a destination rack typically lasts fewer than 10 RTTs. Such a short circuit time brings a critical challenge to TCP, as it is difficult for a TCP sender to sufficiently grow its congestion window (CWND) and utilize the optical bandwidth. To address this problem, in this work, we present AccelToR, a top-of-rack switch for improving TCP performance in circuit/packet hybrid data center networks. AccelToR leverages end-host congestion control to “accelerate” a blocked TCP flow by temporarily scheduling its packets to be transferred through the packet network a few RTTs before its circuit is established, and after enlarging the flow’s CWND with the acceleration, the switch buffers the last window of packets. During the circuit time, the switch sends out the buffered packets, and the accelerated flows, which have their CWNDs already grown large, continue to send packets at high rates to achieve a high optical bandwidth utilization. Experiment results show that AccelToR achieves high throughputs for elephant flows and utilizes over   \\mathbf {90\\%} \\mathbf {90\\%}   of the optical bandwidth, and it preserves short flow completion times for mice flows at the same time. In addition, AccelToR is robust under unexpected packet losses, and can benefit a wide range of TCP congestion control algorithms.}
}


@article{DBLP:journals/ton/ChoCGWJ25,
	author = {Kun Woo Cho and
                  Marco Cominelli and
                  Francesco Gringoli and
                  Joerg Widmer and
                  Kyle Jamieson},
	title = {Scalable Multi-Modal Learning for Cross-Link Channel Prediction in
                  Massive IoT Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2020--2035},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3553023},
	doi = {10.1109/TON.2025.3553023},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChoCGWJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Tomorrow’s massive-scale Internet-of-Things (IoT) sensor networks are poised to drive uplink traffic demand, especially in areas of dense deployment. To meet this demand, however, network designers leverage tools that often require accurate estimates of Channel State Information (CSI), which incurs a high overhead and thus reduces network throughput. Furthermore, the overhead generally scales with the number of clients, and so is of special concern in such massive IoT sensor networks. While prior work has used transmissions over one frequency band to predict the channel of another frequency band on the same link, this paper takes the next step in the effort to reduce CSI overhead: predict the CSI of a nearby but distinct link. We propose Cross-Link Channel Prediction (CLCP), a technique that leverages multi-view representation learning to predict the channel response of a large number of users, thereby reducing channel estimation overhead further than previously possible. CLCP’s design is highly practical, exploiting existing transmissions rather than dedicated channel sounding or extra pilot signals. We have implemented CLCP for two different Wi-Fi versions, namely 802.11n and 802.11ax, the latter being the leading candidate for future IoT networks. We evaluate CLCP in two large-scale indoor scenarios involving both line-of-sight and non-line-of-sight transmissions with up to 144 different 802.11ax users. Moreover, we measure its performance with four different channel bandwidths, from 20 MHz up to 160 MHz. Our results show that CLCP provides a 2x throughput gain over baseline and a 30% throughput gain over existing prediction algorithms.}
}


@article{DBLP:journals/ton/GhiasvandRAP25,
	author = {Sajjad Ghiasvand and
                  Amirhossein Reisizadeh and
                  Mahnoosh Alizadeh and
                  Ramtin Pedarsani},
	title = {Robust Decentralized Learning With Local Updates and Gradient Tracking},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2036--2048},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3552423},
	doi = {10.1109/TON.2025.3552423},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GhiasvandRAP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As distributed learning applications such as Federated Learning, the Internet of Things (IoT), and Edge Computing grow, it is critical to address the shortcomings of such technologies from a theoretical perspective. As an abstraction, we consider decentralized learning over a network of communicating clients or nodes and tackle two major challenges: data heterogeneity and adversarial robustness. We propose a decentralized minimax optimization method that employs two important modules: local updates and gradient tracking. Minimax optimization is the key tool to enable adversarial training for ensuring robustness. Having local updates is essential in Federated Learning (FL) applications to mitigate the communication bottleneck, and utilizing gradient tracking is essential to proving convergence in the case of data heterogeneity. We analyze the performance of the proposed algorithm, Dec-FedTrack, in the case of nonconvex-strongly-concave minimax optimization, and prove that it converges a stationary point. We also conduct numerical experiments to support our theoretical findings.}
}


@article{DBLP:journals/ton/OuyangYA25,
	author = {Qiaolin Ouyang and
                  Neng Ye and
                  Jianping An},
	title = {On the Vulnerability of Mega-Constellation Networks Under Geographical
                  Failure},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2049--2062},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3551700},
	doi = {10.1109/TON.2025.3551700},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/OuyangYA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Assessing the vulnerability of mega-constellation networks (MCNs) to large-scale failure is challenging, due to the time-varying three-dimensional constellation. In this paper, we propose a geometrical method to assess the vulnerability of the MCNs under large-scale geographical failure based on the standard +Grid inter-satellite connectivity pattern, using hop count as the metric. We first equivalently map the original constellation to a two-dimensional flat torus, offering a simplified and time-invariant representation of the geographical failure. The failure’s properties are then revealed on the torus to identify the blockage on the inter-satellite paths that might increase the hop count between end users. Under the blockage, a closed-form hop count expression is then obtained by deriving the explicit expressions of the optimal detours in the scaling limit. Finally, we propose a low-complexity hop count estimation algorithm that achieves a maximum relative error of 1.5% compared with the network simulation while being   10^{5} 10^{5}   times faster. Evaluations using traffic sourced from the 100 most populous cities show that the geographical failure covering the latitude corresponding to the MCNs’ inclination has the greatest impact on the hop count, whereas the MCNs are generally robust even under extreme scenarios.}
}


@article{DBLP:journals/ton/JiaJ0L025,
	author = {Yunshan Jia and
                  Chao Jin and
                  Qing Li and
                  Xuanzhe Liu and
                  Xin Jin},
	title = {FaaSPR: Latency-Oriented Placement and Routing Optimization for Serverless
                  Workflow Processing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2063--2078},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3552407},
	doi = {10.1109/TON.2025.3552407},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JiaJ0L025.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Workflow processing enhances the applicability of serverless computing while retaining the characteristics of fine-grained resource management and elastic scalability. However, current serverless platforms lack targeted optimization of placement and routing strategies for workflow processing, leading to high overheads due to inter-server data transmission, instance cold starts, and function request queuing. We propose FaaSPR, a serverless scheduling system that exploits placement and routing optimizations to minimize workflow processing latency. FaaSPR groups instances with potential data transmission and proportionally distributes groups with heterogeneous instances across multiple servers, taking into account resource constraints and historical placement traces. This method addresses the issues of poor scalability and frequent instance migrations in existing solutions. Utilizing a routing algorithm based on multi-stage linear programming, FaaSPR minimizes cross-server data transmission within and between instance groups while ensuring load balancing among instances. Experiments show that, compared to the state-of-the-art solution FaaSFlow, FaaSPR decreases the average and 99th percentile tail latency by up to 68.03% and 93.46%, respectively. Additionally, reducing workflow processing latency leads to up to 46.18% decrease in resource consumption for FaaS users.}
}


@article{DBLP:journals/ton/BaekKPMK25,
	author = {Hankyul Baek and
                  Gyu Seon Kim and
                  Soohyun Park and
                  Andreas F. Molisch and
                  Joongheon Kim},
	title = {Slimmable Federated Reinforcement Learning for Energy-Efficient Proactive
                  Caching},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2079--2094},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3554608},
	doi = {10.1109/TON.2025.3554608},
	timestamp = {Wed, 15 Oct 2025 19:23:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BaekKPMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in deep learning have successfully replaced classical algorithms with machine learning models based on neural networks (NNs). This is particularly prevalent in proactive caching. As NNs grows more capable as their size in terms of storage and computation increases, NN-based proactive caching achieves performance improvement. Nonetheless, there remain challenges in implementing NN-based proactive caching in realistic environments with dynamic user movement. These are due to the fixed structure of NNs that should expand the input size to match the dimensions of the input with the dimensions of the NN’s input units. To address these challenges, this paper proposes a scalable proactive caching framework, named slimmable federated reinforcement learning (SlimFRL). By adopting slimmable neural networks (SNNs) in FRL, our SlimFRL easily adjusts the widths of the SNNs during training according to the number of users. Moreover, due to the scalability of SNNs, our SlimFRL can set the appropriate input dimension while not using imputation, leading to performance improvement. This paper also validates the performance and advantages of SlimFRL in terms of reward and additional cost functions. Additionally, this paper proposes several training algorithms for SlimFRL and corroborates their superiority with convergence analysis and various experiments.}
}


@article{DBLP:journals/ton/0005W25,
	author = {Wenhao Yuan and
                  Xuehe Wang},
	title = {A Game-Theoretic Framework for Privacy-Aware Client Sampling in Federated
                  Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {4},
	pages = {2095--2111},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3554494},
	doi = {10.1109/TON.2025.3554494},
	timestamp = {Tue, 14 Oct 2025 19:49:16 +0200},
	biburl = {https://dblp.org/rec/journals/ton/0005W25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In federated learning (FL) systems, the central server typically samples a subset of participating clients at each global iteration for model training. To mitigate privacy leakage, clients may insert noise into local parameters before uploading them for global aggregation, leading to FL model performance degradation. This paper aims to design a Privacy-aware Client Sampling framework in FEDerated learning, named FedPCS, to tackle the heterogeneous client sampling issues and improve model performance. First, we obtain a pioneering upper bound for the accuracy loss of the FL model with privacy-aware client sampling probabilities. Based on this, we model the interactions between the central server and participating clients as a two-stage Stackelberg game. In Stage I, the central server designs the optimal time-dependent reward for cost minimization by considering the trade-off between the accuracy loss of the FL model and the rewards allocated. In Stage II, each client determines the correction factor that dynamically adjusts its privacy budget based on the reward allocated to maximize its utility. To surmount the obstacle of approximating other clients’ private information, we introduce the mean-field estimator to estimate the average privacy budget. We analytically demonstrate the existence and convergence of the fixed point for the mean-field estimator and derive the Stackelberg Nash Equilibrium to obtain the optimal strategy profile. Through rigorously theoretical convergence analysis, we guarantee the robustness of our proposed FedPCS. Moreover, considering the conventional sampling strategy in privacy-preserving federated learning, we prove that the random sampling approach’s price of anarchy (PoA) can be arbitrarily large. To remedy such efficiency loss, we show that the proposed privacy-aware client sampling strategy successfully reduces PoA, which is upper bounded by a reachable constant. To address the challenge of varying privacy requirements throughout different training phases in FL, we extend our model and analysis and derive the adaptive optimal sampling ratio for the central server. Experimental results on different datasets demonstrate the superiority of FedPCS compared with the existing state-of-the-art FL strategies under IID and Non-IID datasets.}
}


@article{DBLP:journals/ton/WangLZLYZJT25,
	author = {Bingbing Wang and
                  Wenhui Li and
                  Fengyuan Zhu and
                  Jiazhen Lei and
                  Zeming Yang and
                  Linling Zhong and
                  Meng Jin and
                  Xiaohua Tian},
	title = {Constellation Mapping for Frequency-Agile {OFDM} Backscatter Network},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2115--2130},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3550934},
	doi = {10.1109/TON.2025.3550934},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLZLYZJT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents FaB, a frequency-agile backscatter system that can optionally leverage OFDM signals on different bands as carriers for backscatter communication. Compared with existing backscatter systems that are tailored to a specific frequency band, a frequency-agile backscatter yields two critical benefits: i) it can leverage the increased availability of “free rides” across a broad range of frequency band to improve its transmission efficiency; and ii) it becomes compatible with mainstream wireless communication standards, making it applicable to heterogeneous wireless networks. Based on these two features, FaB’s circuits can be migrated to various types of backscatter communication nodes without any modification, significantly reducing design and deployment costs. To show the efficacy of our design, we implement a PCB prototype of FaB and showcase its capability of leveraging OFDM Wi-Fi and LTE signals as carrier waves. Our extensive field studies show that FaB’s multi-band modulator can produce an error vector magnitude of under -15dB in any band below 6GHz with a precision of 10mV.}
}


@article{DBLP:journals/ton/NguyenM25,
	author = {Quang Minh Nguyen and
                  Eytan H. Modiano},
	title = {Optimal Control for Distributed Wireless {SDN:} Theory and Architecture},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2131--2147},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3560605},
	doi = {10.1109/TON.2025.3560605},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NguyenM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose Distributed Universal Max-Weight (DUMW) as a novel optimal control framework for distributed wireless SDN. DUMW is theoretically throughput-optimal and practically congruent with SDN system idiosyncrasies. Our algorithmic development non-trivially extends the throughput-optimal Universal Max-Weight (UMW) policy to permit distributed control and optimal inter-domain scheduling under the setting of heterogeneously delayed network state information. Furthermore, we design controller synchronization strategies that resolve the problem of multi-domain flow installation and are tailored to DUMW for maintaining throughput-optimality with negligible communication overhead. Extensive experiments validate our theoretical finding and demonstrate the favorable performance of DUMW. Under the setting of reliable links with wireless interference, DUMW achieves the same throughput as that of an optimal centralized controller and exhibits superior scalability.}
}


@article{DBLP:journals/ton/ZongZTZL25,
	author = {Ruixing Zong and
                  Jiapeng Zhang and
                  Zhuo Tang and
                  Wei Zhang and
                  Kenli Li},
	title = {Topology-Aware Interleaved All-Reduce Communication for Dragonfly
                  Network},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2148--2163},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3556013},
	doi = {10.1109/TON.2025.3556013},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZongZTZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the context of distributed deep learning, computational clusters place greater emphasis on static all-reduce communication latency while also needing to support large-scale networking. However, the communication efficacy of current all-reduce algorithms within specialized network topologies requires enhancement. Existing all-reduce communication algorithms inadequately exploit cluster bandwidth, leading to considerable bandwidth idleness. The optimization of communication algorithms becomes imperative to fully utilize the available bandwidth. Addressing this concern, we propose an innovative approach: a topology-aware interleaved all-reduce algorithm for Dragonfly networks (TIAD). Leveraging the inherent characteristics of the Dragonfly network, TIAD employs an interleaved communication mechanism for both intra- and inter-group data collection, significantly augmenting communication efficiency. Moreover, we refine the Dragonfly network with minimal adjustments, aligning it with the theoretical structure of interleaved communication. We also proposed an all-reduce communication method to complement the TIAD algorithm, specifically for scenarios where only a subset of nodes in the Dragonfly network participate in the communication task. Our experiments demonstrate that TIAD exhibits the shortest communication time across diverse node sizes and bandwidth conditions. Notably, our algorithm reduces communication time by up to 23.4% during the collection communication phase in comparison to the PAARD algorithm.}
}


@article{DBLP:journals/ton/ChenZZSH25,
	author = {Baojun Chen and
                  Shuai Zhang and
                  Jiawen Zhu and
                  Weiqiang Sun and
                  Weisheng Hu},
	title = {pVarband: Efficient and Performance Guaranteed Scheduling With Variable
                  Bandwidth in IRS-Assisted {FSO} Systems in DCNs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2164--2178},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3555775},
	doi = {10.1109/TON.2025.3555775},
	timestamp = {Mon, 26 Jan 2026 08:28:19 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenZZSH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Intelligent reflective surface (IRS)-assisted free-space optics (FSO) systems allow allocation of different number of IRS elements between communication end-points. It thus can provide connectivity with variable bandwidth in data centers, where the traffic between top-of-rack switches is often unevenly distributed in space and time. In this paper, we propose a novel traffic scheduling technique that allows variable bandwidth in the context of IRS-assisted FSO systems in data center networks (DCNs). We formalize the problem of scheduling with variable bandwidth, explore the performance space for designing scheduling algorithms, and provide insights on how to use heuristics in practice. Then, we propose a low-complexity and performance-guaranteed parallel scheduling algorithm, pVarband, that takes advantage of variable bandwidth to avoid the time-consuming maximum weight matching and can run at a complexity of   \\mathcal {O}(N\\log N) \\mathcal {O}(N\\log N)   in a DCN with N racks. We theoretically demonstrate that scheduling with variable bandwidth requires at most   \\mathcal {O}(\\log (1/\\epsilon)) \\mathcal {O}(\\log (1/\\epsilon))   reconfigurations to obtain an   \\epsilon  \\epsilon   -approximate decomposition for an arbitrary traffic matrix. Also, we provide insights for design and deployment of an IRS-assisted FSO system to achieve performance saturation. Our results show that scheduling allowing variable bandwidth can achieve almost optimal performance for unevenly distributed traffic, and also outperforms its fixed bandwidth counterpart in terms of running time and decomposition error. Our work also provides useful insights for scheduling with variable bandwidth in contexts other than IRS-assisted FSO systems.}
}


@article{DBLP:journals/ton/ShiCMHJD25,
	author = {Min Shi and
                  Jing Chen and
                  Zhuangzhuang Ma and
                  Kun He and
                  Meng Jia and
                  Ruiying Du},
	title = {A Formal Analysis of 5G {EAP-TLS} Protocol},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2179--2191},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3556374},
	doi = {10.1109/TON.2025.3556374},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiCMHJD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emergence of private 5G networks has garnered significant attention from enterprises. To ensure the security of communication devices within these networks, the 3GPP group proposed the 5G Extensible Authentication Protocol-Transport Layer Security (EAP-TLS). Despite its critical role, the security of 5G EAP-TLS has not been systematically studied. In this paper, we present the first comprehensive formal model of the 5G EAP-TLS protocol, detailing its flow and incorporating all parties and the certificate distribution mechanism as defined by the 5G specification. Additionally, we extract and interpret the security requirements outlined in the specification. Using the automated symbolic tool Tamarin, we analyze the protocol’s security goals and identify potential vulnerabilities. We propose and verify solutions to these issues, enhancing the protocol’s security. This work provides a foundational understanding and improvements for securing private 5G networks.}
}


@article{DBLP:journals/ton/ZouWSYGDC25,
	author = {Yifei Zou and
                  Peng Wang and
                  Shikun Shen and
                  Dongxiao Yu and
                  Jorge Torres G{\'{o}}mez and
                  Falko Dressler and
                  Xiuzhen Cheng},
	title = {Stable Age of Information Scheduling With {NOMA} in Edge Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2192--2207},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3559386},
	doi = {10.1109/TON.2025.3559386},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZouWSYGDC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we study a stable Age-of-Information (AoI) scheduling problem to handle the massive packet aggregation from arbitrary total of n end devices to an edge device via a single hop wireless channel when information constantly arrives at the end side. Specifically, we consider the arrivals of the information with an online injection mode, i.e., information is injected to end devices with an unknown injection rate in each time slot. After the information is injected, the AoI with respect to those end devices constantly increases until their fresh messages are received by the edge device, which characterizes the freshness of the information at destination. Based on the online injection mode, we propose the first distributed stable AoI scheduling algorithm combining NOMA (Non-Orthogonal Multiple-Access) technique in this paper, to minimize the expected average peak AoI (EAP-AoI) of our end-to-edge information system. Adopting NOMA technique enables k messages decoded from a mixed signal by the edge device in our algorithm, with parameter   k\\gt 1 k\\gt 1  . We prove that our algorithm is stable even under the asymptotically maximum injection rate of   O(k/n) O(k/n)   that any stable AoI scheduling algorithm may handle, and the EAP-AoI of our information system is bounded by   O(\\sqrt [{3}]{nk}) O(\\sqrt [{3}]{nk})   time slots under the injection rate of   O(k/n) O(k/n)  . Comparing with two existing results, the EAP-AoI in our algorithm is   O\\left ({{\\frac {n^{2/3}}{k^{4/3}}}}\\right) O\\left ({{\\frac {n^{2/3}}{k^{4/3}}}}\\right)   and   O\\left ({{\\frac {n^{2/3}}{k^{1/3}}}}\\right) O\\left ({{\\frac {n^{2/3}}{k^{1/3}}}}\\right)   times smaller. Numerical results also verify the stability and efficiency of our algorithm.}
}


@article{DBLP:journals/ton/ZhangZLLMK25,
	author = {Jiaqi Zhang and
                  Xiaolong Zheng and
                  Ruinan Li and
                  Liang Liu and
                  Huadong Ma and
                  Nei Kato},
	title = {Improving Data Collection Efficiency of UAV-Assisted LoRa Networks
                  via Directivity-Aware Link Model},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2208--2223},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3559889},
	doi = {10.1109/TON.2025.3559889},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZLLMK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned Aerial Vehicle (UAV) equipped with a gateway shows great potential for data collection in many scenarios, especially for the areas lacking of public network infrastructures. However, our in-field experiments on UAV-assisted LoRa networks show that a large throughput gap exists between the ground-to-air and ground-to-ground transmissions. We find that the misalignment of the radiation direction of transceiver antennas with height difference leads to additional signal strength loss, which is ignored by existing ground-to-ground transmissions. In this paper, we propose a directivity-aware ground-to-air link model called annulus model to quantify the impact of directivity on the ground-to-air link quality. Based on our model, a new ground-to-air channel access scheme for UAV-assisted LoRa networks, PreLoRa, is proposed. By predicting the link quality variations, PreLoRa schedules the transmission periods and adopts optimal transmission configurations for ground nodes to improve the link throughput. We implement PreLoRa on commercial LoRa platforms and extensively evaluate its performance in the wild. Experimental results show that PreLoRa can significantly improve data collection throughput by up to 65.5% compared to baseline methods.}
}


@article{DBLP:journals/ton/WangYZCNZ25,
	author = {Yazi Wang and
                  Xiaosong Yu and
                  Yongli Zhao and
                  Yuan Cao and
                  Avishek Nag and
                  Jie Zhang},
	title = {Time-Scheduled End-to-End Entanglement Establishment in Memory-Cell-Limited
                  Quantum Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2224--2240},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3557820},
	doi = {10.1109/TON.2025.3557820},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangYZCNZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum entanglement enables quantum networks to provide end-to-end sharing of entangled particles, establishing multi-hop path-to-path connections between remote parties. Implementing entanglement distribution plays a vital role in increasing the network scale, and practical entanglement algorithms are required to provide end-to-end multi-hop quantum entanglement. We consider the real-time entanglement distribution (R-TED) and pre-established entanglement distribution (P-EED) to meet this requirement. Based on these two types of entanglement distribution, we propose two algorithms, i.e., R-TED-based routing and entangled pairs allocation (REA) algorithm as well as P-EED-based REA algorithm for end-to-end entanglement establishment, where the practical physical factors (e.g., finite storage capacity and limited storage time) are considered. The R-TED-based REA algorithm can orchestrate the nodes in a route and perform entanglement swapping by adopting real-time entanglement. For the P-EED-based REA algorithm, remote entangled particle sharing can be achieved via pre-shared entanglement distribution and hop-by-hop entanglement swapping. This way, the entanglement routing selection satisfies the storage time constraint and allows two far-apart nodes to share long-distance entangled particles with limited memory cells. We evaluate the performance of the proposed algorithms under different network topologies and sizes, based on which we demonstrate that the network size can significantly affect the efficiency advantage achieved by the P-EED-based approach over the R-TED-based approach.}
}


@article{DBLP:journals/ton/HuangH25,
	author = {Yu{-}Di Huang and
                  Ting He},
	title = {Overlay Routing Over an Uncooperative Underlay},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2241--2255},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3561308},
	doi = {10.1109/TON.2025.3561308},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Overlay network is a non-intrusive mechanism to enhance the existing network infrastructure by building a logical distributed system on top of a physical underlay. A major difficulty in operating overlay networks is the lack of cooperation from the underlay, which is usually under a different network administration. In particular, the lack of knowledge about the underlay topology and link capacities makes the design of efficient overlay routing extremely difficult. In contrast to existing solutions for overlay routing based on simplistic assumptions such as known underlay topology or disjoint routing paths through the underlay, we aim at systematically optimizing overlay routing without causing congestion, by extracting information about the underlay from measurements taken at overlay nodes. To this end, we 1) identify the sufficient information for congestion-free overlay routing, and 2) develop polynomial-complexity algorithms to infer this information with guaranteed accuracy. Our evaluations in NS3 based on real network topologies demonstrate notable performance advantage of the proposed solution over existing solutions.}
}


@article{DBLP:journals/ton/ZhangXGPHHML25,
	author = {Yongchao Zhang and
                  Qingjun Xiao and
                  Chenyang Guo and
                  Guannan Pan and
                  Jun Huang and
                  Kun He and
                  Yu Miao and
                  Wenjin Li},
	title = {Bucket-Level Elastic Cuckoo Filter for Dynamic Set Membership Query
                  and Encoded Set Operations},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2256--2275},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3565545},
	doi = {10.1109/TON.2025.3565545},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangXGPHHML25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Approximate Membership Query (AMQ) filters such as Cuckoo filter are probabilistic data structures designed for set membership queries, with constant memory overhead and low time complexity. Previous works can enhance the cuckoo filter to support memory resizing and adapt to an increasing number of set elements. However, most of them can only support coarse-grained memory expansion at the filter or mini-filter level. In this paper, we propose a new Bucket-level Elastic Cuckoo Filter (BECF), to support the fine-grained bucket-level expansion. Our BECF combines cuckoo filter with consistent hashing technique: It divides a hash-ring into non-overlapping segments, and stores the elements mapped to a segment into the corresponding filter bucket. To allow a dynamic number of segments on a hash-ring, we support the splitting of a segment into two sub-segments. We design a segment re-assignment technique, that borrows extra bits from an element fingerprint to construct a longer segment prefix and re-assign it to another bucket. We minimize data movements between the corresponding buckets of the two sub-segments, and preserve the high time efficiency of a cuckoo filter after the expansion. Our experiments show that BECF attains 37% lower memory cost (while holding the same number of elements), 12% higher insertion speed and 20% faster query speed than other designs. We also describe how to apply set operations (intersection, union and subtraction) over two data sets encoded by BECF. Its advantages are the ability to merge two BECFs with different sizes and the low merging time cost.}
}


@article{DBLP:journals/ton/LiLZLYJDZLC25,
	author = {Qing Li and
                  Lie Lu and
                  Dan Zhao and
                  Zeyu Luan and
                  Yuan Yang and
                  Yong Jiang and
                  Jingpu Duan and
                  Ruobin Zheng and
                  Shaoteng Liu and
                  Dingding Chen},
	title = {Stateless and Proactive Routing for Dynamic Multicast With Deep Reinforcement
                  Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2276--2291},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3558784},
	doi = {10.1109/TON.2025.3558784},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLZLYJDZLC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Stateful multicast protocols manage multicast group memberships by maintaining state information about active groups and their members. They have seen limited adoption in the modern internet due to lack of scalability, simplicity, and flexibility. Although stateless multicast protocols, like BIER, eliminate extensive state management, they still face complex tree computation and limited scalability for concurrent requests. In this paper, we propose Hawkeye, a stateless multicast mechanism with deep reinforcement learning (DRL) for real-time responses to dynamic multicast requests with near-optimal multicast TE performance. This mechanism is suited for Software-Defined Networking (SDN) environment where the controller has a global view of the network and supports flexible configuration of network resources for traffic engineering. For real-time responses to multicast requests, we leverage DRL enhanced by a temporal convolutional network (TCN) to model the sequential feature of dynamic group membership, and thus are able to build multicast trees proactively for upcoming requests. We develop a novel source aggregation mechanism to facilitate the convergence of the DRL agent under high volume of multicast requests. Moreover, to improve the practicality and robustness of Hawkeye, we design incremental deployment and single failure handling mechanisms, which take advantages of source aggregation and fit well with multicast routing. Evaluation with real-world topologies and multicast requests demonstrates that Hawkeye responds effectively to dynamic multicast requests. It offers rapid routing decisions, e.g., making routing decisions in under 5ms on a tested topology, and reduces path latency variation by up to 89.5%, with less than a 10% increase in bandwidth consumption compared to the offline theoretical minimum.}
}


@article{DBLP:journals/ton/ZechariaS25,
	author = {Rami Zecharia and
                  Yuval Shavitt},
	title = {A Parallel Algorithm and Scalable Architecture for Routing in Bene{\v{s}}
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2292--2305},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3558552},
	doi = {10.1109/TON.2025.3558552},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZechariaS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Beneš/CLOS architectures are common scalable interconnection networks widely used in backbone routers, data centers, on-chip networks, multi-processor systems, and parallel computers. Recent advances in Silicon Photonic technology, especially in Mach-Zehnder interferometer technology, have made Beneš networks a very attractive scalable architecture for optical circuit switches. Numerous routing algorithms for Beneš networks were developed starting with linear algorithms having time complexity of   O(N\\log _{2}N) O(N\\log _{2}N)   steps. Parallel routing algorithms were developed to satisfy the stringent timing requirements of high-performance switching networks and have a time complexity of   O((\\log _{2}N)^{2}) O((\\log _{2}N)^{2})  . However, their implementation requires   O(N^{2}\\log _{2}N) O(N^{2}\\log _{2}N)   wires (termed connectivity complexity), and thus is difficult to scale. We present a new routing algorithm for Beneš networks combined with a scalable hardware architecture that supports full and partial input permutations. The processing time of the algorithm is limited to   O((\\log _{2}N)^{2}) O((\\log _{2}N)^{2})   steps (iterations) by potentially forfeiting routing of a few input demands; however, it achieves close to 100% utilization for both full and partial input permutations. The algorithm and architecture allow for a reduction in connectivity complexity to   O(N^{2}) O(N^{2})  , a   \\log N \\log N   improvement over previous solutions. We prove the algorithm correctness, and analyze its performance analytically and with large scale simulations.}
}


@article{DBLP:journals/ton/ZhangC25,
	author = {Yue Zhang and
                  Nan Cen},
	title = {{O-AAV:} Programmable Software-Defined Optical Wireless Communication
                  {AAV} Networking Testbed},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2306--2318},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3559767},
	doi = {10.1109/TON.2025.3559767},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optical Wireless Communication (OWC) Autonomous Aerial Vehicles (AAVs) networks have attracted ever-increasing attention of researchers in recent years. There have been significant efforts to advance the OWC AAV networking system to develop new solutions, including optimal resource allocation algorithms and networking control methods. However, current OWC-AAV research is either based on simulations or in-flexible hardware platforms that can support mostly point-to-point, low-data-rate applications. Therefore, in this paper, we are the first to propose and discuss the design of a new programmable software-defined optical wireless communication-based AAV networking platform, O-AAV, with characteristics in terms of data rate, transmission distance, and hardware/software flexibility in support of different light sources and distributed networking operations. The main architectural choices of the new platform are discussed in detail, as well as preliminary performance evaluation results. Data rates in the order of Mbps are achieved in a controlled lab environment, and 500 Kbps data rates achieved more than 10 m transmission distance in the outdoor environment, respectively. We also effectively address the PD saturation problem and interference from other ambient light sources, which most OWC testbeds face. Lastly, to the best of our knowledge, we are the first to demonstrate the networking capability of the proposed architecture and prototype by implementing ALOHA.}
}


@article{DBLP:journals/ton/LuoGLPTT25,
	author = {Chaochao Luo and
                  Jie Gan and
                  Xinli Li and
                  Zhuting Pan and
                  Jian Tang and
                  Zhihong Tian},
	title = {Your Botnet Is His Botnet? {A} Deep Dive Into the Supply Chain Attack
                  Against Cyber-Arm Industry},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2319--2335},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3566903},
	doi = {10.1109/TON.2025.3566903},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoGLPTT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In recent years, supply chain attacks have garnered significant attention from both enterprises and the security community due to their profound impact. Numerous studies have begun to focus on supply chain attacks targeting legitimate software. However, it is noteworthy that supply chain attacks also occur within cybercrime markets, which differ substantially from those observed in lawful markets. These attacks not only facilitate widespread infections but also enable the rapid formation of substantial botnets. Regrettably, supply chain attacks within cybercrime markets have largely been overlooked by the security community, resulting in a notable deficiency in systematic methodologies for comprehending such assaults. This work concentrates on a specific supply chain attack observed in cybercrime markets, denoted as the “Nigrita Attack.” To facilitate a systematic understanding of this attack, we introduce a model designed to delineate its propagation dynamics. Additionally, we propose an approach for forecasting its future threat potential. To assess the efficacy of the proposed propagation model and the accuracy of the method for predicting the scale of infections, we assembled a dataset comprising more than 40,342 distinct malware samples and conducted a comprehensive series of analyses. Empirical results substantiate both the effectiveness of the proposed propagation model and the precision of the approach in estimating the magnitude of potential infections.}
}


@article{DBLP:journals/ton/TanZLX25,
	author = {Qi Tan and
                  Yi Zhao and
                  Qi Li and
                  Ke Xu},
	title = {Expediting Federated Learning on Non-IID Data by Maximizing Communication
                  Channel Utilization},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2336--2351},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3565822},
	doi = {10.1109/TON.2025.3565822},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TanZLX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is at the core of intelligent Internet architecture. It allows clients to jointly train a model without direct data sharing. In such a process, clients and the central server share information through communication channels formed by parameters. However, the non-iid training data in clients significantly impacts global model convergence and brings difficulties for the evaluation of local contributions. Most of existing studies try to expand the communication channel by improving consistency with variance reduction or regularization, but such methods neglect an important factor, i.e., channel utilization, hence their capability for sharing information is under-utilized. Moreover, the issue of contribution evaluation is still unsolved. In this paper, we simultaneously solve the former two challenges (i.e., model convergence and contribution evaluation) by modeling the indirect data sharing of FL as a problem of information communication. We prove that FL with non-iid data forms noisy communication channels, which have limited capability for information transmission, i.e., limited channel capacity. The main factor in deciding the channel capacity is the Gradient Signal to Noise Ratio (GSNR). Through analyzing GSNR, we further prove that channel capacity can be reached by optimal local updates and propose a method FedGSNR to calculate it, which allows us to maximize channel utilization in FL, leading to faster model convergence. Moreover, as the contribution of the local dataset depends on the amount of provided information, the derived GSNR allows the server to accurately evaluate the contributions of different clients (i.e., the quality of local datasets).}
}


@article{DBLP:journals/ton/YuXHZG25,
	author = {Shiming Yu and
                  Xianjin Xia and
                  Ningning Hou and
                  Yuanqing Zheng and
                  Tao Gu},
	title = {XGate: Scaling LoRa Communications to Massive Logical Channels},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2352--2366},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3560408},
	doi = {10.1109/TON.2025.3560408},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuXHZG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa is a promising technology that provides widespread low-power IoT connectivity. With its capabilities for multi-channel communication, orthogonal transmission, and spectrum sharing, LoRaWAN is poised to connect millions of IoT devices across thousands of logical channels. However, current LoRa gateways rely on hardwired Rx chains that cover less than 1% of these channels, restricting the potential for large-scale LoRa communications. This paper introduces XGate, a groundbreaking gateway design that uses a single Rx chain to simultaneously receive packets from all logical channels, enabling scalable LoRa transmission and flexible network access. Unlike the hardwired Rx chains in existing gateway designs, XGate dynamically allocates resources, including software-controlled Rx chains and demodulators, based on the extracted meta-information of incoming packets. XGate overcomes several challenges to efficiently detect incoming packets without prior knowledge of their parameter configurations. Evaluations demonstrate that XGate enhances LoRa concurrent transmissions by   8.4\\times  8.4\\times    compared to state-of-the-art solutions.}
}


@article{DBLP:journals/ton/ShuDZGQLM25,
	author = {Zhenyang Shu and
                  Xiaoheng Deng and
                  Jingjing Zhang and
                  Jinsong Gui and
                  Huamei Qi and
                  Siyu Lin and
                  Geyong Min},
	title = {Decoupled Uplink-Downlink Multi-Connectivity Scheduling in Full-Duplex
                  Cell-Free Massive {MIMO} Networks: {A} {HGN-DRL} Approach},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2367--2382},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3561048},
	doi = {10.1109/TON.2025.3561048},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShuDZGQLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network-assisted full-duplex (NAFD) cell-free (CF) massive MIMO systems enable simultaneous uplink and downlink transmissions, where interference suppression and beamforming are critical for improving spectral efficiency and system performance. However, the asymmetric time-varying properties of current network traffic, coupled with the interference problems associated with complex network topologies, make existing resource allocation and interference management strategies difficult to handle, and unable to satisfy the low-latency, high-reliability Quality-of-Service (QoS) requirements of the growing number of terminal devices (TDs). To address these challenges, we propose a novel access method based on decoupled uplink-downlink multi-connectivity transmission to achieve flexible access selection and formulate an optimization problem that maximizes the cumulative fair spectral efficiency by simultaneously optimizing power allocation and link scheduling. To solve this mixed-integer nonlinear programming (MINLP) problem, we propose an optimized transfer scheme that reduces the dimensionality of the action and constraint spaces. Then, we characterize the network states as heterogeneous graph structures and employ node-level and metapath-level attention mechanisms for message passing and aggregation, and obtain graph-level scheduling policy via the heterogeneous graph neural network (HGNN). Finally, in light of the superior performance of Deep Reinforcement Learning (DRL) in exploration-based tasks, we design a holistic updating mechanism using environmental feedback and advantage state-action function, named as HGN-DRL for this end-to-end learning framework. Simulation results demonstrated the effectiveness and scalability of HGN-DRL in large-scale cell-free scenarios.}
}


@article{DBLP:journals/ton/HeMZH25,
	author = {Junyi He and
                  Qian Ma and
                  Meng Zhang and
                  Jianwei Huang},
	title = {Optimizing Fresh Data Sampling and Trading},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2383--2395},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3567065},
	doi = {10.1109/TON.2025.3567065},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeMZH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Existing works on data trading often overlook the impact of data freshness on its valuation. This paper explores a fresh data market, where a platform offers data with varying freshness levels, such as real-time traffic data, to users who arrive stochastically. We categorize data updates into two types: lightweight (e.g., noise level) and computation-intensive (e.g., traffic images). Initially focusing on lightweight updates, we introduce three pricing policies: uniform, dual, and dynamic. The challenge lies in jointly optimizing the platform’s data sampling and pricing, a complex non-smooth mixed integer programming problem. Nevertheless, we achieve closed-form optimal solutions for all three policies by analyzing a relaxed version of the problem. Our findings reveal the surprising insight that higher data acquisition costs lead the platform to lower uniform data prices due to staler, less valuable data. Our numerical analysis indicates that the optimal dual pricing policy closely matches the dynamic pricing policy in performance and substantially exceeds the uniform pricing, tripling profits in some cases. Extending our work to computation-intensive updates, which require preprocessing, adds extra complexity. We tackle this by applying fractional programming. Numerical results show that profits from optimal uniform and dual pricing closely approach those from dynamic pricing, as the platform can adjust processing time.}
}


@article{DBLP:journals/ton/TsanikidisG25,
	author = {Christos Tsanikidis and
                  Javad Ghaderi},
	title = {Online Scheduling and Routing With End-to-End Deadline Constraints
                  in Multihop Wireless Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2396--2409},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3565930},
	doi = {10.1109/TON.2025.3565930},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TsanikidisG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider scheduling deadline-constrained packets in multihop wireless networks. Packets with arbitrary deadlines and weights arrive at and are destined to different nodes. The goal is to design online admission, routing, and scheduling algorithms in order to maximize the cumulative weight of packets that reach their destinations within their deadlines. Under a general interference graph model of the wireless network, we provide online algorithms that are   (\\gamma ,\\mathrm {R}) (\\gamma ,\\mathrm {R})  -competitive, i.e., they achieve at least   1/\\gamma  1/\\gamma    fraction of the value of the optimal offline algorithm, and do not exceed the capacity by more than a factor   \\mathrm {R}\\geq 1 \\mathrm {R}\\geq 1  . In particular, our algorithm can achieve   \\gamma =O({\\psi }^{\\star } \\log (\\Delta \\rho L)/\\mathrm {R}) \\gamma =O({\\psi }^{\\star } \\log (\\Delta \\rho L)/\\mathrm {R})   when   \\mathrm {R}\\mathrm {C} = \\Omega ({\\psi }^{\\star } \\log (\\Delta \\rho L)) \\mathrm {R}\\mathrm {C} = \\Omega ({\\psi }^{\\star } \\log (\\Delta \\rho L))  , where   \\rho  \\rho    is the ratio of maximum weight to minimum weight of packets, L is the length of the longest route of packets, and C is the minimum link capacity or the number of channels. Here,   \\Delta  \\Delta    is the maximum degree and    {\\psi }^{\\star }   {\\psi }^{\\star }    is the local clique cover number of the interference graph. Our results translate directly to many networks of interest, for example, in one-hop interference networks,    {\\psi }^{\\star } =2  {\\psi }^{\\star } =2  , and in the case of wired networks (no interference),    {\\psi }^{\\star } =1  {\\psi }^{\\star } =1  . We further provide lower bounds that show that our results are asymptotically optimal in many settings. Finally, we present extensive simulations that show our algorithms provide significant improvement over the prior approaches.}
}


@article{DBLP:journals/ton/LiLYW25,
	author = {Youqi Li and
                  Fan Li and
                  Song Yang and
                  Yu Wang},
	title = {{BGEFL:} Enabling Communication-Efficient Federated Learning via Bandit
                  Gradient Estimation in Resource-Constrained Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2410--2425},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3562869},
	doi = {10.1109/TON.2025.3562869},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLYW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) has achieved state-of-the-art performance in distributed machine learning with privacy preservation, which promotes AIoT. However, FL is restricted by the expensive communication cost due to exchanging a large number of model parameters and model updates (e.g., gradients) between the aggregator and participants in multiple rounds. This could be challenging in resource-constrained networks where devices are often resource-constrained in terms of computation and communication. Existing works mainly focus on improving communication efficiency from local training and model/gradient compression; nevertheless, studying communication efficiency for FL from the perspective of gradient estimation remains unexplored. In this paper, we bridge this gap by conducting a systematic study on gradient estimation for the communication-efficient FL. We propose a bandit-based gradient estimation-aware FL (  \\mathtt{BGEFL } \\mathtt{BGEFL }  ) framework that can directly estimate participants’ gradients with limited bandit feedback (i.e., their local function values). We prove that   \\mathtt{BGEFL } \\mathtt{BGEFL }   enjoys an   \\mathcal {O}(1) \\mathcal {O}(1)   communication complexity, that is a constant-size uplink communication in which each client uploads only one point’s feedback in the uplink. Moreover, our bandit-based gradient estimator is communication-efficient, unbiased, and stable. We prove the convergence performance of   \\mathtt{BGEFL } \\mathtt{BGEFL }   for training strongly convex, general convex, and non-convex models. Finally, we evaluate our   \\mathtt{BGEFL } \\mathtt{BGEFL }   over several datasets and the experimental results demonstrate the effectiveness of   \\mathtt{BGEFL } \\mathtt{BGEFL }  .}
}


@article{DBLP:journals/ton/ChenLFHSZTZC25,
	author = {Xingyu Chen and
                  Jia Liu and
                  Chengxuan Fu and
                  He Huang and
                  Yu{-}E Sun and
                  Xu Zhang and
                  Ming Tao and
                  Zuojian Zhou and
                  Lijun Chen},
	title = {Advancing {RFID} Tag Counting With {COTS} Devices: The Average Time
                  Duration Method},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2426--2441},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3566269},
	doi = {10.1109/TON.2025.3566269},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLFHSZTZC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With 52.8 billion RFID tags used worldwide in 2024, a common basic functionality needed by RFID-enabled applications is cardinality estimation — to quickly estimate the number of distinct tags in an RFID system. Although many advanced solutions have been proposed over the past decade, they suffer from one major limitation in practical use: they need to either modify the existing RFID standard or obtain MAC-layer information, both of which however cannot be supported by commercial off-the-shelf (COTS) devices. In this paper, we revisit the counting problem and propose a novel counting scheme called average time duration based counter (ATD) that quickly estimates the number of distinct tags in a standards-compliant manner. Compared with existing work, the competitive advantage of ATD is that it can be directly deployed on a COTS RFID system, with no need for any hardware modifications. In ATD, we found a new and measurable indicator — the time duration between two adjacent singleton slots, which depends on the number of tags. Following this observation, we derive the theoretical relationship between the time indicator and the number of tags and then give the proof of the estimation as well as its parameter settings. Additionally, we propose a flag-flipping solution to address the overlapping problem in the multi-reader case. We implement ATD in a COTS RFID system with 1000 tags. Experimental results show that ATD is   4.2\\times  4.2 × 4.2\\times    faster than the baseline of tag inventory; the performance gain will be further increased in a larger RFID system.}
}


@article{DBLP:journals/ton/TianWYSYZ25,
	author = {Ying Tian and
                  Zhiliang Wang and
                  Xia Yin and
                  Xingang Shi and
                  Jiahai Yang and
                  Han Zhang},
	title = {Centralized Network Utility Maximization With Accelerated Gradient
                  Method},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2442--2457},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3562283},
	doi = {10.1109/TON.2025.3562283},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TianWYSYZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network utility maximization (NUM) is a fundamental problem for network traffic management and resource allocation. Due to the inherent decentralization and complexity of networks, much of the existing research has focused on developing decentralized algorithms for NUM. However, with the rise of Software-Defined Networking (SDN), especially in cloud networks and inter-datacenter networks managed by large enterprises, there has been growing interest in centralized NUM algorithms. To cope with the large and increasing number of flows in such SDN networks, existing studies on centralized NUM focus on the scalability of the algorithm with respect to the number of flows, but the efficiency is ignored. In this paper, we propose a centralized, efficient and scalable algorithm for the NUM problem. By designing smooth utility and penalty functions, we formulate the NUM problem with a smooth objective function, which enables the use of Nesterov’s accelerated gradient method (AGM). We prove that the proposed method achieves an   O(d/t^{2}) O(d/t^{2})   convergence rate, demonstrating superior convergence speed with respect to the number of iterations t, and our method is scalable with respect to the number of flows d in the network. Our smooth objective NUM formulation and AGM are effective not only in simple network scenarios with non-prioritized flows routed on one simple paths, but also in more complex and practical scenarios involving prioritized flows routed across multiple complex paths. Experiment results confirm that our method obtains accurate solutions with fewer iterations, and achieves close-to-optimal network utility.}
}


@article{DBLP:journals/ton/YuanWTZZK25,
	author = {Xun Yuan and
                  Xiaonan Wang and
                  Fengxiao Tang and
                  Qingping Zhou and
                  Ming Zhao and
                  Nei Kato},
	title = {{MPITE:} Multidimensional Performance Evaluator for Interpretable
                  and Traceable Network Performance Evaluation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2458--2473},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3562348},
	doi = {10.1109/TON.2025.3562348},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuanWTZZK25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the advancements in six-generation (6G) communication technology, there is a growing need for comprehensive and interpretable network performance evaluation for network optimization. Traditional evaluation methods often overlook uncertainties and are limited to a single time scale or performance dimension, while the recent machine learning-based method lacks interpretability. To address this issue, we propose a multidimensional performance evaluator for interpretable and traceable network performance evaluation (MPITE). MPITE, constructed with a three-layer evaluation model incorporating physical, logical, and causal topology structures, reflects the causal relationship of communication system configurations, the changing network states, and performance metrics. We introduce a multidimensional performance index that considers value, time, and certainty dimensions to evaluate network performance comprehensively. We propose interpretable Bayesian theory-based network inference algorithms to derive network certainty for interpretable network performance evaluation. Then, we intelligently derive optimal network configuration parameters through reverse inferencing for network tracing. Experimental results demonstrate the advantage, interpretability, and traceability of MPITE.}
}


@article{DBLP:journals/ton/LiHYF25,
	author = {Xiang Li and
                  Jianwei Huang and
                  Kai Yang and
                  Chenyou Fan},
	title = {Machine Learning Model Trading With Verification Under Information
                  Asymmetry},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2474--2488},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3567071},
	doi = {10.1109/TON.2025.3567071},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiHYF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) model trading, known for its role in protecting data privacy, faces a major challenge: information asymmetry. This issue can lead to model deception, a problem that current literature has not fully solved, where the seller misrepresents model performance to earn more. We propose a game-theoretic approach, adding a verification step in the ML model market that lets buyers check model quality before buying. However, this method can be expensive and offers imperfect information, making it harder for buyers to decide. Our analysis reveals that a seller might probabilistically conduct model deception considering the chance of model verification. This deception probability decreases with the verification accuracy and increases with the verification cost. To maximize seller payoff, we further design optimal pricing schemes accounting for heterogeneous buyers’ strategic behaviors. Interestingly, we find that reducing information asymmetry benefits both the seller and buyer. Meanwhile, protecting buyer order information doesn’t improve the payoff for the buyer or the seller. These findings highlight the importance of reducing information asymmetry in ML model trading and open new directions for future research.}
}


@article{DBLP:journals/ton/AkemBGF25,
	author = {Aristide Tanyi{-}Jong Akem and
                  Beyza B{\"{u}}t{\"{u}}n and
                  Michele Gucciardo and
                  Marco Fiore},
	title = {Practical and General-Purpose Flow-Level Inference With Random Forests
                  in Programmable Switches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2489--2506},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3564465},
	doi = {10.1109/TON.2025.3564465},
	timestamp = {Sat, 15 Nov 2025 13:54:57 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AkemBGF25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Integrating machine learning (ML) models directly in the network user plane enables inference on data traffic at line rate, and can dramatically reduce the latency and improve the scalability of key functionalities like traffic classification or intrusion detection. Yet, the hardware that can be used for this purpose, in particular programmable switches, present stringent constraints in terms of limited memory and little support for mathematical operations or data types that render ML model deployment a substantial technical challenge. In this paper, we make a step forward in user-plane ML by introducing Flowrest, a solution that redefines the state of the art in flow-level inference for programmable switches. Flowrest allows implementing general-purpose Random Forest (RF) models in industry-grade switches by (  \\boldsymbol {i} \\boldsymbol {i}  ) suitably handling stateful flow-level (FL) features in the switch ASIC, (  \\boldsymbol {ii} \\boldsymbol {ii}  ) achieving low-collision flow management, and (  \\boldsymbol {iii} \\boldsymbol {iii}  ) customizing RF models right from the design phase for in-switch operation. We develop Flowrest as an open-source software using the P4 language and evaluate its performance in an experimental testbed with Intel Tofino switches. Experiments with inference tasks of varying complexity prove that our solution improves accuracy by over 10 percent points on average with respect to the second-best competitor out of five recent approaches for RF-based in-switch inference, while maintaining sub-microsecond latency.}
}


@article{DBLP:journals/ton/LiDGZHLWTX25,
	author = {Tong Li and
                  Xinle Du and
                  Xiangyu Gao and
                  Guangmeng Zhou and
                  Hanlin Huang and
                  Zhuotao Liu and
                  Mowei Wang and
                  Kun Tan and
                  Ke Xu},
	title = {Revisiting Random Early Detection Tuning for High-Performance Datacenter
                  Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2507--2522},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3572145},
	doi = {10.1109/TON.2025.3572145},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiDGZHLWTX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Random Early Detection (RED) has been integrated into datacenter switches as a fundamental Active Queue Management (AQM) for decades. The accurate configuration of RED parameters is crucial to achieving high throughput and low latency. However, due to the highly dynamic nature of workloads in datacenter networks, maintaining consistently high performance with statically configured RED thresholds poses a challenge. Prior work applies reinforcement learning to predict proper thresholds, but their real-world deployment has been hindered by poor tail performance caused by instability. In this paper, we propose   \\textsf {PRED} \\textsf {PRED}  , a novel system that enables automatic and stable RED parameter adjustment in response to traffic dynamics. Specifically, the system employs a Multiplicative-Increase Multiplicative-Decrease (MIMD) strategy to dynamically adapt to flow concurrency while utilizing an Additive-Increase Additive-Decrease (AIAD) mechanism to adapt to flow distribution. We perform extensive evaluations on our physical testbed and large-scale simulations. The results demonstrate that   \\textsf {PRED} \\textsf {PRED}   can keep up with the real-time network dynamics generated by realistic workloads. For instance, compared with the static-threshold-based methods,   \\textsf {PRED} \\textsf {PRED}   keeps 66% shorter switch queue length and obtains up to 80% lower Flow Completion Time (FCT). Compared with the state-of-the-art learning-based method,   \\textsf {PRED} \\textsf {PRED}   reduces the tail FCT by 34%.}
}


@article{DBLP:journals/ton/MaoSY25,
	author = {Yingling Mao and
                  Xiaojun Shang and
                  Yuanyuan Yang},
	title = {Provable Approximation Algorithms for Online Traffic-Sensitive {SFC}
                  Deployment},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2523--2536},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3566728},
	doi = {10.1109/TON.2025.3566728},
	timestamp = {Sun, 09 Nov 2025 16:31:10 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaoSY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Function Virtualization (NFV) has the potential for cost-efficiency, manage-convenience, and flexibility services but meanwhile poses challenges for the service function chain (SFC) deployment problem, which is NP-hard. It is so complicated that existing work conspicuously neglects the flow changes along the chains and only gives heuristic algorithms without a performance guarantee. In this paper, we fill this gap by formulating a traffic-sensitive online joint SFC placement and flow routing (TO-JPR) model, with the objective of jointly optimize the resource cost and network latency, and proposing a novel two-stage scheme to solve it. We design a dynamic segmental packing (DSP) algorithm for the first stage, which not only maintains the minimal traffic burden for the network but also achieves an approximation ratio of a small constant on the resource cost. Besides, we propose the greedy mapping (GM) algorithm for the second stage, which can guarantee a global approximation ratio of O(d) on the network latency. Here d is the diameter of the network graph and is typically smaller than O(log(M)), where M is the number of servers in the network. Finally, we perform extensive simulations to demonstrate the outstanding performance of our algorithms compared with the optimal solutions and benchmarks.}
}


@article{DBLP:journals/ton/ZhuQZLCSQYL25,
	author = {Andong Zhu and
                  Ji Qi and
                  Sheng Zhang and
                  Gangyi Luo and
                  Ke Cheng and
                  Xiaohang Shi and
                  Zhuzhong Qian and
                  Baoliu Ye and
                  Sanglu Lu},
	title = {Machine-Centric High-Accuracy Multi-Video Analytics With Adaptive
                  Neural Codecs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2537--2552},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3565357},
	doi = {10.1109/TON.2025.3565357},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuQZLCSQYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Increased videos captured by widely deployed cameras are being analyzed by computer vision-based Deep Neural Networks (DNNs) on servers rather than being streamed for humans. Unfortunately, the conventional codecs (e.g., H.26x and MPEG-x) originally designed for video streaming lack content-aware feature extraction and hinder machine-centric video analytics, making it difficult to achieve the required high accuracy with tolerable delay. Neural codecs (e.g., autoencoder) now hold impressive compression performance and have been widely advocated in video streaming. While autoencoder shows transformative potential, the application in video analytics is hampered by low accuracy in detecting small objects of high-resolution videos and the serious challenges posed by multi-video streaming. To this end, we propose AdaStreamer with adaptive neural codecs to enable real machine-centric high-accuracy multi-video analytics. We also investigate how to achieve optimal accuracy under delay constraints via careful scheduling in Compression Ratios (CRs, the ratio of the compressed size to the original data size) and bandwidth allocation, and further propose a Markov-based Adaptive Compression and Bandwidth Allocation algorithm (MACBA). We have practically developed a prototype of AdaStreamer, based on which extensive experiments verify its accuracy improvement (up to 15%) compared to state-of-the-art coding and streaming solutions.}
}


@article{DBLP:journals/ton/FanHXLM25,
	author = {Yu Fan and
                  Bo Hou and
                  Pengjin Xie and
                  Liang Liu and
                  Huadong Ma},
	title = {EchoCC: Refining Learning-Based Congestion Control With WordBook},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2553--2568},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3566495},
	doi = {10.1109/TON.2025.3566495},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FanHXLM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over the past decade, learning-based congestion control algorithms (CCAs) have experienced a significant increase in popularity. Recently, some approaches aim to tackle the computational overhead challenge in previous CCAs caused by extensive neural network parameters and computation delays, which can hinder algorithm effectiveness. Though effective, these approaches have certain constraints in achieving an optimal balance between performance and efficiency. To guarantee decision performance with increased efficiency, we stand on the shoulders of existing learning-based CCAs, gather their wise decisions, and replay them in a suitable manner. In this paper, we propose EchoCC, a unified framework that refines any deterministic learning-based CCA to reduce computation overhead and keep the performance of the original model. EchoCC maintains a “WordBook” which offers effective mapping between inputs and decisions. To ensure the high-quality mapping, we propose action-supervised contrastive learning which aligns the distributions of inputs and decisions. To realize EchoCC in an actual network environment, we address two practical challenges: performance-oriented updating for adaptability and a redundancy-reduction method for limited storage. Extensive evaluations conducted on both live Internet networks and emulated environments, achieve up to 64% reductions in running time and 60% CPU utilization compared to the original model. Importantly, EchoCC maintains network throughput and latency.}
}


@article{DBLP:journals/ton/ShengCHTL25,
	author = {Siyuan Sheng and
                  Jiazhen Cai and
                  Qun Huang and
                  Lu Tang and
                  Patrick P. C. Lee},
	title = {Toward Distributed Write-Back Caching in Programmable Switches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2569--2584},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3569436},
	doi = {10.1109/TON.2025.3569436},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShengCHTL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Skewed write-intensive key-value storage workloads are increasingly observed in modern data centers, yet they also incur server overloads due to load imbalance. Programmable switches provide viable solutions for realizing load-balanced caching on the I/O path, and hence implementing write-back caching in programmable switches is a natural approach to absorb frequent writes and improve write performance. However, enabling in-switch write-back caching is challenged by not only the strict programming rules and limited stateful memory of programmable switches, but also the need for reliable protection against data loss due to switch failures. We first propose FarReach, a new caching framework that supports fast, available, and reliable in-switch write-back caching. FarReach carefully co-designs both the control and data planes for cache management in programmable switches, so as to achieve high data-plane performance with lightweight control-plane management. We further extend FarReach into DistReach, which reduces the reliability maintenance overhead via distributed switch deployment. Our experimental results on a Tofino-switch testbed show that FarReach achieves a throughput gain of up to   6.6 × 6.6\\times    over a state-of-the-art in-switch caching approach under skewed write-intensive workloads. Also, DistReach reduces the crash recovery time of FarReach by 77.4%.}
}


@article{DBLP:journals/ton/QiuRTLG25,
	author = {Changhao Qiu and
                  Bangbang Ren and
                  Guoming Tang and
                  Lailong Luo and
                  Deke Guo},
	title = {ChameleonNet: Topology Obfuscation Against Tomography With Critical
                  Information Hiding},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2585--2600},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3567771},
	doi = {10.1109/TON.2025.3567771},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/QiuRTLG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many network attacks, like link flooding attacks (LFAs), heavily rely on network topology information. Therefore, network topology obfuscation has been applied to counteract network topology inference and prevent topology information leakage. One effective way is to scheme a fake topology intentionally for attackers to map out. Focusing on reducing the similarity between the real and fake topologies, however, existing methods cannot promise that critical information of the network, such as critical nodes and links, is well hidden. To this end, we propose a new topology obfuscation mechanism, namely ChameleonNet, to protect the critical topology information of a given network. Specifically, ChameleonNet achieves topology obfuscation through a two-stage operation: 1) generating fake topology and 2) deploying fake topology. Our experiments on three real-world and two large-scale generated network topologies demonstrate that ChameleonNet can effectively reduce similarity between inferred and real topologies by 31%-37% and reliably hide critical topology information in terms of multiple statistical metrics.}
}


@article{DBLP:journals/ton/XunLZDWTZ25,
	author = {Qinglin Xun and
                  Weichao Li and
                  Jianer Zhou and
                  Jingpu Duan and
                  Yi Wang and
                  Xiaofeng Tao and
                  Jinbei Zhang},
	title = {Efficient Data Center Network Monitoring and Troubleshooting With
                  LMon: Leveraging {ECMP} Hashing Linearity and Lightweight Probing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2601--2616},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3570008},
	doi = {10.1109/TON.2025.3570008},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XunLZDWTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network performance monitoring and troubleshooting are crucial yet challenging tasks in datacenter management. Despite the numerous solutions that have been proposed in recent years, their efforts are often hindered by high costs and unreliable failure localization, making it difficult to deploy them in real-world environments. In this paper, we present LMon, a highly reliable and efficient system for monitoring and troubleshooting in datacenter networks. LMon utilizes the characteristic of ECMP hashing linearity to control probe packet routing, enabling the monitoring of targeted paths without any modification of underlying protocols and devices. Additionally, LMon leverages a lightweight probing technique to reduce monitoring overhead, as well as integrates the improved LASSO regression and hypothesis testing for higher accuracy and faster processing in link failure localization. We evaluate the performance of LMon in our testing environment. Compared to the monitoring system Pingmesh, LMon generates only one-third probes while maintaining 99% accuracy and 1% false negatives.}
}


@article{DBLP:journals/ton/YuanLPLGWC25,
	author = {Qianchen Yuan and
                  Fuliang Li and
                  Tian Pan and
                  Yuhua Lai and
                  Yetao Gu and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {INT-Partition: Hierarchical and Fault-Tolerant In-Band Network Telemetry},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2617--2631},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3569340},
	doi = {10.1109/TON.2025.3569340},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YuanLPLGWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the expansion of production networks, new challenges arise in scaling telemetry systems to accommodate the massive number of network devices. In-band Network Telemetry (INT) is widely adopted for its fine-grained and accurate measurements. However, system robustness and performance scalability remain key challenges for INT-based network-wide telemetry systems. Existing INT-based measurements manage the network as a whole and rely on a centralized controller for path planning and telemetry data collection. As networks scale and the probability of failures increases, frequent re-planning leads to prolonged telemetry interruptions. In this work, we propose INT-Partition, a hierarchical and fault-tolerant in-band network telemetry system with a divide-and-conquer paradigm. INT-Partition conducts telemetry in two stages: network partitioning and telemetry within each partition, enabling scalability for mega-scale networks. Our evaluations, including mega-scale simulations using BMv2 software switches and small-scale validations on Tofino hardware switches, demonstrate the effectiveness of our approach. With 1% of network equipment out of order, INT-Partition covers over 89.67% of the area and accurately locates faults. As the network scales, telemetry planning and deployment time is reduced by 75.62% to 96.02%, and hot reloading enables seamless switching of telemetry deployment.}
}


@article{DBLP:journals/ton/WanZSPYLH25,
	author = {Zirui Wan and
                  Jiao Zhang and
                  Yuzhen Su and
                  Haoyu Pan and
                  Mingxuan Yu and
                  Yuan Li and
                  Tao Huang},
	title = {Re-Architecting Traffic Control in Cross-Datacenter {RDMA} Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2632--2647},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3569630},
	doi = {10.1109/TON.2025.3569630},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WanZSPYLH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The network-intensive applications, like machine learning and cloud storage, are increasingly driving two critical trends: 1) RDMA has been widely deployed to provide high-speed networks; 2) applications are distributively deployed across multiple regional datacenters to satisfy demands for content providers and customers. To fully utilize the benefits of RDMA, we desire to extend it to support cross-datacenter networks. However, the long-haul transport suffers a considerably long control loop, and thus the hybrid of long-haul and intra-datacenter traffic can easily cause severe congestion. We revisit existing traffic control methods and find they are insufficient to resolve this hybrid traffic congestion. Generally, regional datacenters are connected using dedicated long-haul optical fiber and datacenter interconnection (DCI) switches. In this paper, we propose Approach Traffic Control (ATC), a novel solution focusing on two-side DCI-switches (i.e., the approach point for datacenters) to separately alleviate the hybrid traffic congestion in the local and distal datacenters, as a building block for host-driven control methods. This design principle helps ATC shorten the control loop to a single datacenter scale while aggregating congestion information of the whole datacenter range with minor deployment complexity. We implement ATC on P4-based switches and conduct evaluations using real-world testbeds and large-scale NS3 simulations. The results show that ATC ensures fast congestion avoidance and delivers significant performance. For example, ATC reduces the FCT of intra-datacenter and long-haul traffic by up to 88% and 52%, respectively.}
}


@article{DBLP:journals/ton/ShiJXHXLZWY25,
	author = {Xingang Shi and
                  Zitong Jin and
                  Bin Xiong and
                  Xinyao Huang and
                  Xiaotian Xi and
                  Dan Li and
                  Han Zhang and
                  Zhiliang Wang and
                  Xia Yin},
	title = {{HELA:} Inferring {AS} Relationships With a Hybrid of Empirical and
                  Learning Algorithms},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2648--2663},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3572148},
	doi = {10.1109/TON.2025.3572148},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShiJXHXLZWY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Knowledge of the business relationships between Autonomous Systems (ASes) is the basis for studying many aspects of the Internet. Despite the significant progress achieved by the latest inference algorithms, their inference results still suffer from errors on many special or critical links, thus hindering many relationship-related applications. We take an in-depth analysis on the challenges inherent in inferring AS relationships, including complex routing policies, limited and biased vantage point (VP) coverage, as well as a lack of accurate validation data. To address these challenges, we introduce HELA, a framework for inferring AS relationships with a hybrid of empirical and machine learning algorithms. HELA incorporates an array of grouping, voting, and machine learning algorithms and allows flexible substitution of each. We systematically evaluate various combinations of them to determine the most effective one for HELA. Furthermore, we describe the collection of varied validation datasets, including BGP community and RPSL records from Internet Routing Registries (IRRs), as well as OneStep community. Our up-to-date dataset corrects errors in previous published validation sets, contains 95% more labelled links, and exhibits a closer alignment to actual link distribution. Using routing data and validation datasets composed for each month during   2021\\sim 2023 2021\\sim 2023  , we access HELA’s superiority in both inference accuracy and stability compared to the state-of-the-art inference algorithms, i.e., AS-Rank, ProbLink, and HELA’s predecessor TopoScope. In particular, HELA achieves up to   2.9\\times  2.9\\times    reduction on error rates across overall datasets, up to   2.6\\times  2.6\\times    reduction with a   4.5\\times  4.5\\times    decrease on standard deviation on incomplete and biased datasets, and up to   1.7\\times  1.7\\times    reduction on various sources of validation datasets.}
}


@article{DBLP:journals/ton/ZhouGYZW25,
	author = {Zhaoxing Zhou and
                  Huaxi Gu and
                  Xiaoshan Yu and
                  Qian Zhang and
                  Yunhao Wang},
	title = {Halo: An Efficient and Scalable Distributed Control Strategy for AWGR-Based
                  Optical Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2664--2676},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3571639},
	doi = {10.1109/TON.2025.3571639},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouGYZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Optical networks based on arrayed waveguide grating routers (AWGRs) present a promising solution for rapidly evolving data centers (DCs). However, the contention issue remains a significant challenge to their widespread adoption. Existing control strategies typically rely on time-slots that are both synchronized across the network and of fixed duration. This results in the fixed switching granularity. The determination of time-slot duration often involves a trade-off between flexibility and control overhead. In this paper, we propose Halo, a distributed control strategy that avoids contention without relying on time-slots. Halo decentralizes the scheduling tasks across the network. Each node independently issues Grants, which are passed sequentially among source nodes with communication demands. The node holding the Grant is allowed to transmit a batch of data to the issuing node. Once the demand is no longer present, or the transmitted packet count exceeds a threshold, the Grant is forwarded to the next node. As a result, synchronization operations among nodes is unnecessary, allowing the switching granularity to become flexible. Experimental results demonstrate that Halo significantly outperforms existing control strategies. In trace-based simulations, it achieves up to a 75.13% reduction in flow completion time (FCT). Additionally, we developed a prototype system and executed several real applications to demonstrate the practical feasibility of the Halo.}
}


@article{DBLP:journals/ton/PhanekhamRN25,
	author = {Derek Phanekham and
                  Nageswara S. V. Rao and
                  Suku Nair},
	title = {Throughput Measurements and Profile Analysis of Cloud Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2677--2689},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3574550},
	doi = {10.1109/TON.2025.3574550},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PhanekhamRN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cloud networks utilize virtual connections to connect virtual machines distributed across cloud sites. They are increasingly deployed due to flexible provisioning using software and cost-effectiveness in not requiring to build physical network infrastructure. However, their extensive virtualization makes it unclear how well the established practices of conventional networks translate to them. We study throughput measurements over a Google Cloud network using a matching hardware-emulated conventional network, which provide production and exploratory conditions, respectively. The measurements span connections representing local, cross-continental and around the Earth distances. We study the effects of parallel flows, congestion control algorithms and retransmissions on the network throughput profile expressed as a function of RTT. We compare the throughput profile of Google Cloud network with those of emulated network under various loss conditions, including those too disruptive or expensive in the former. Our analysis based on the concave-convex shape and utilization-concavity coefficients of throughput profiles indicates an overall agreement of performance between the two networks, thereby justifying the use of conventional network emulations to analyze cloud networks. In terms of practical use, our study establishes that BBR and BBRv2 alpha TCP achieve higher throughput compared to loss-based congestion control algorithms under most network configurations, especially, under losses at large RTT.}
}


@article{DBLP:journals/ton/LiuWLLZY25,
	author = {Yunhao Liu and
                  Hongyi Wang and
                  Yang Li and
                  Zhenhua Li and
                  Guoquan Zhang and
                  Lei Yang},
	title = {A Five-Year Retrospective of Cellular Reliability Evolution: The Encouraging,
                  Disappointing, and Further Enhancements},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2690--2705},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3573839},
	doi = {10.1109/TON.2025.3573839},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuWLLZY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With recent advances on cellular technologies pushing the boundary of cellular performance, cellular reliability has become a key concern of their adoption and deployment. To fully understand cellular reliability, we work with a major Android phone vendor, Xiaomi, to conduct a long-term (2020-2024) and large-scale (involving 123M users) measurement study in China, with coarse-grained general statistics and fine-grained sampling diagnostics. Our measurement reveals contrasting evolution trends of cellular failures in different stages of the data connection: in the past five years, failures after connection establishment decrease remarkably (by 29%), while failures during connection setup exhibit a sharp increase (by 38%). Our analysis illustrates that the contrast stems from the joint impact of multiple stakeholders, including ISPs’ increasing deployment of 5G base stations, 5G infrastructure upgrade from NSA (Non-Standalone) to SA (Standalone) mode, software defects coming from Android’s adaptation to new cellular technologies, and so forth. Our work provides actionable insights for improving cellular reliability at scale. More importantly, we have built on our insights to develop enhancements that effectively address cellular reliability issues with remarkable real-world impact—our optimizations have reduced 38% cellular connection failures for 5G phones and 31% failure recovery time across all phones.}
}


@article{DBLP:journals/ton/DouGY25,
	author = {Songshi Dou and
                  Zehua Guo and
                  Kwan L. Yeung},
	title = {Unleashing the Potential of {LEO} Constellations in Building Resilient
                  and Low-Latency Control Plane for SD-WANs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2706--2719},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3573611},
	doi = {10.1109/TON.2025.3573611},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DouGY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Delivering seamless network services (e.g., video streaming, AR/VR, and cloud gaming) in Wide Area Networks (WANs) relies on flexible traffic management, which is facilitated by Software-Defined Wide Area Networks (SD-WANs), or SDN in WANs. In SD-WANs, control and data traffic usually share the same links for cost savings, a practice known as in-band control. The SDN controller can periodically send control messages to switches via control channels for routing policy updates to maintain satisfactory network performance. However, when a link failure occurs, control channels become disconnected. Since the controller can no longer communicate with the switches, the desired flexible traffic management cannot be promised. Although backup paths can be preconfigured to reconnect some control channels, high control latency may be introduced due to the meandering routes. Fortunately, commercial Low-Earth Orbit (LEO) mega-constellations, which provide pervasive and low-latency Internet services, present a promising solution to this control resiliency issue. Inspired by the rapid deployment of these LEO constellations, we propose a novel control plane design called SpaceHelper to leverage the LEO satellite network for improving SD-WANs’ control resiliency. SpaceHelper smartly integrates the LEO satellite network with terrestrial SD-WAN to reconnect control channels during link failure, which is formulated as an optimization problem to minimize overall control latency. A heuristic algorithm is proposed to solve the problem efficiently and guarantee prompt control channel reconnection. Performance evaluations are conducted using the Starlink constellation and real-world WAN topologies. Compared to the state-of-the-art in-band solution, we show that SpaceHelper can not only provide 100% resiliency, but also significantly reduce average control latency by up to 72.6% and 70.2% under GÉANT and Abilene topologies, respectively.}
}


@article{DBLP:journals/ton/GuoLXWC25,
	author = {Zhenbei Guo and
                  Fuliang Li and
                  Tangzheng Xie and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {NetGenius: Routing Configuration Recommendation Based on Graph Neural
                  Network},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2720--2733},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3573571},
	doi = {10.1109/TON.2025.3573571},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GuoLXWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Configurations deployed in routers govern the routing and addressing of IP-based networks. Automated tools for synthesizing and verifying configurations have been deployed to replace manual configurations, preventing network outages from misconfigurations. However, these tools offer limited assistance in real production networks as manual configuration remains the primary and preferred approach for daily operations. To address this issue, we present NetGenius, a configuration recommendation tool that assists operators in manually editing network configurations like a code editor. NetGenius employs a graph neural network to estimate the importance of neighboring commands to the center command, providing recommendations based on importance scores when the center command is used as input. First, NetGenius leverages a generic graph model named Knowledge Graph to model existing configurations and derives configuration features for each center command by composing it with its neighbors. Subsequently, NetGenius conducts a general approach based on the graph neural network to estimate and calculate corresponding feature importance scores. Finally, NetGenius uses a recommendation mechanism to recommend configurations based on importance scores while adhering to specific constraints. We extensively evaluate the performance of NetGenius using real-world configurations from different vendors. The experimental results demonstrate that NetGenius achieves the highest recommendation accuracy of 98.59% across various datasets. Moreover, when applied to a large-scale network comprising   152,475 152,475   configuration commands, NetGenius completes its training within 6 seconds and generates recommendations in 5 milliseconds.}
}


@article{DBLP:journals/ton/TianY25,
	author = {Yurun Tian and
                  Osman Yagan},
	title = {Correlated Social Contagions With Multiple Topics: {A} Generalized
                  Linear Threshold Model},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2734--2748},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3575086},
	doi = {10.1109/TON.2025.3575086},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TianY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The study of influence propagation over networks has received increasing attention in many scientific domains. In particular, the linear threshold model is widely studied due to its ability to capture the mechanism by which multiple sources of exposure are required for nodes in the network to take action. Most existing research on influence propagation concentrate on a single topic spreading in isolation over the network. However, real-life social contagions often involve multiple topics spreading simultaneously in a correlated manner; e.g., different conspiracy theories, political opinions on taxes, immigration, gun control, etc. In this work, we propose a multi-dimensional threshold model with correlated influences as an extension of the classical linear threshold model to incorporate multiple correlated topics spreading simultaneously over networks. We provide analytical results that accurately predict the threshold, probability, and expected size of global cascades, i.e., cases where a significant fraction of the population gets influenced. Through extensive simulations, we demonstrate that our analytical results match the numerical results near-perfectly in the finite node regime. These results reveal the interplay between the underlying network structure, the correlation among spreading topics, and the heterogeneous thresholds on the final results of the propagation.}
}


@article{DBLP:journals/ton/LouJ25,
	author = {Chiheng Lou and
                  Xin Jin},
	title = {Efficient Far Memory-Aware Scheduling With FaMAS},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2749--2763},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3574741},
	doi = {10.1109/TON.2025.3574741},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LouJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Benefiting from the low-latency RDMA network, far memory techniques that enable applications to leverage memory on remote machines have been proposed to improve job performance and resource utilization in datacenters. In a far memory-equipped datacenter, applications exhibit diverse characteristics when using far memory. However, existing datacenter schedulers are oblivious to the presence of far memory, resulting in suboptimal scheduling decisions that can compromise the performance guarantee of tasks. To this end, we present FaMAS, a far memory-aware datacenter scheduler that exploits the benefits of far memory to improve datacenter efficiency and avoid severe performance degradation of tasks. FaMAS tackles the far-memory scheduling problem for two objectives: reducing average job completion time and meeting deadlines. FaMAS introduces two novel far memory-aware algorithms: Job Completion Time First (JCTF) and Service Level Objective First (SLOF), tailored to the above two objectives respectively. JCTF formalizes the scheduling target with insights from the trade-off between the performance and the resource usage of individual tasks, while SLOF incorporates the growth rate of resource usage to represent tasks. The experimental results show that JCTF improves the average job completion time by up to   1.81\\times  1.81\\times    and SLOF achieves up to   3.95\\times  3.95\\times    lower deadline violations over the best traditional algorithm.}
}


@article{DBLP:journals/ton/WuZLR25,
	author = {Jinze Wu and
                  Lorenzo Zino and
                  Zhiyun Lin and
                  Alessandro Rizzo},
	title = {Distributed Finite-Time Cooperative Localization for Three-Dimensional
                  Sensor Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2764--2778},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3574678},
	doi = {10.1109/TON.2025.3574678},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WuZLR25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper addresses the distributed localization problem for a network of sensors placed in a three-dimensional space, in which sensors are able to perform range measurements, i.e., measure the relative distance between them, and exchange information on a network structure. While most existing studies primarily develop localization algorithms under the assumption that the entire sensor network is localizable, the problem of determining whether a sensor is localizable has received limited attention. However, neglecting this preliminary step can significantly hamper the accuracy of localization algorithms due to error propagation from unlocalizable sensors in iterative localization procedures. To address this research gap, we focus on two key challenges: i) deriving rigorous theoretical results and developing algorithms to verify sensor localizability, and ii) designing an efficient distributed localization algorithm that utilizes these localizability results. Specifically, we start by deriving a necessary and sufficient condition for sensor localizability using barycentric coordinates. Then, building on this theoretical result, we design a distributed localizability verification algorithm, in which we propose and employ a novel distributed finite-time algorithm for sum consensus. Finally, we develop a distributed localization algorithm based on conjugate gradient method and derive theoretical guarantees on its performance, ensuring finite-time convergence. The efficiency of our algorithm compared to the existing ones from the literature and its capability to handle scenarios with moderate levels of noise in the measurements are further demonstrated through numerical simulations.}
}


@article{DBLP:journals/ton/YangHYYC25,
	author = {Tao Yang and
                  Bingnan Hou and
                  Yifan Yang and
                  Zhenzhong Yang and
                  Zhiping Cai},
	title = {6Seeks: {A} Global IPv6 Network Periphery Scanning System},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2779--2793},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3575443},
	doi = {10.1109/TON.2025.3575443},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangHYYC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Discovering the IPv6 network periphery, i.e., the last-hop router connecting endhosts in the IPv6 Internet, is crucial for network measurement and Internet reconnaissance. However, existing solutions commonly suffer from inefficiency when applied on a global scale due to the vast IPv6 address space. To tackle this challenge, we developed 6Seeks, an innovative IPv6 scanning system designed for efficient IPv6 periphery discovery across the global IPv6 Internet without requiring seed IPv6 addresses. Specifically, we proposed to employ a heuristic method for collecting active /48 networks from the global BGP prefixes and then adopt a reinforcement learning-based dynamic probing strategy to optimize resource allocation across these networks and significantly improve efficiency. Real-world tests demonstrate that 6Seeks outperforms all existing methods in global-scale IPv6 periphery measurement experiments. In just a few hours, 6Seeks can identify over 128 million IPv6 periphery devices, while using only 37% of the probing resources required by the current state-of-the-art solution. Compared to existing public datasets, the IPv6 addresses identified by 6Seeks are more numerous and display unique characteristics, significantly enriching our IPv6 corpus.}
}


@article{DBLP:journals/ton/WangXWZLJCCL25,
	author = {Zhongjia Wang and
                  Guoqi Xie and
                  Dongsheng Wei and
                  Yunfei Zhang and
                  Yixue Lei and
                  Yuhang Jia and
                  Mingsong Chen and
                  Wanli Chang and
                  Kenli Li},
	title = {MobiPTP: Mobile Precision Time Protocol for Ubiquitous Communication
                  Scenarios},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2794--2808},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3574712},
	doi = {10.1109/TON.2025.3574712},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangXWZLJCCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G mobile communication techniques are widely applied in ubiquitous communication scenarios (e.g., smart navigation and smart transportation), which require time synchronization among mobile devices. However, the built-in time synchronization software of Android phones presents large time offsets with hundreds of milliseconds (ms), and the mainstream time synchronization techniques have specific limitations: 1) the Network Time Protocol (NTP) has too large offset to meet the real-time information interaction among mobile devices; 2) the Linux Precision Time Protocol (LinuxPTP) exists hardware dependence and cannot be implemented on Android and 5G networks; and 3) the Global Navigation Satellite System (GNSS) requires installing a hardware receiver on each mobile device. In this study, we develop a Mobile Precision Time Protocol (MobiPTP), which is hardware-independent and compatible with various network types, including Wide-Area Network (WAN), Local-Area Network (LAN), wired and wireless networks. The main challenges include signal strength instability and uplink-downlink asymmetry. We propose a dynamic time synchronization algorithm and an asymmetry compensation strategy to overcome these challenges. Regardless of high-speed mobile or crowded conditions in 5G networks, MobiPTP demonstrates an average offset of 9 ms, outperforming the NTP-based open-source software Chrony (about 30 ms). MobiPTP has been successfully deployed in multiple real-world ubiquitous communication scenarios and always demonstrates much lower offsets than Chrony.}
}


@article{DBLP:journals/ton/ZhouKE25a,
	author = {Xujin Zhou and
                  Irem Koprulu and
                  Atilla Eryilmaz},
	title = {Achieving Synchronized Fresh Communication Over Broadcast Channels},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2809--2819},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3575363},
	doi = {10.1109/TON.2025.3575363},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouKE25a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a scenario whereby the state of a common source is being updated at multiple distributed devices. We are particularly interested in the tradeoff that exists between the freshness of the updates at the distributed devices and the synchrony of the updates across them. In this paper, we explore this tradeoff in a wireless downlink setting whereby the transmitter can choose between unicast transmissions (with given success probabilities) to particular users and broadcast transmissions (with a smaller success probability) to all users. After discussing the Linear Programming (LP)-based optimal design and extreme choices of “always-unicasting” and “always-broadcasting” policies, we note that the optimal design is not scalable and the extreme policies are inefficient. This motivates us to develop two classes of policies, namely a “mixed randomized policy” and a “feature-based learning policy”, which have desirable performance and computational-complexity characteristics. Additionally we manage to provide complete analysis for the mixed randomized policy under the two-user case, which provides interesting insights and can be partially extended to general cases. We perform extensive numerical studies to compare the performance of these designs over the benchmarks to reveal their gains.}
}


@article{DBLP:journals/ton/ZhugeWHXDXYM25,
	author = {Xiangwen Zhuge and
                  Zeyu Wang and
                  Xiaowu He and
                  Shen Xu and
                  Fan Dang and
                  Jingao Xu and
                  Zheng Yang and
                  Qiang Ma},
	title = {TSNCard: Bridging the Gap in {TSN} Diagnostics via Protocol, Algorithm,
                  and Hardware},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {5},
	pages = {2820--2835},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3575508},
	doi = {10.1109/TON.2025.3575508},
	timestamp = {Sun, 09 Nov 2025 16:31:11 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhugeWHXDXYM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-Sensitive Networking (TSN) is foreseen as a foundational technology that enables Industry 4.0. It offers deterministic data transmission over Ethernet for critical applications such as industrial control and automotive systems. However, TSN is susceptible to hardware and software errors, necessitating an effective diagnostic system. Traditional network diagnostic tools are inadequate for TSN fault localization and classification due to the tightly coupled traffic and high precision requirements in TSN. In response, this paper presents TSNCard, a cross-cycle postcard-based diagnostic system tailored for Time-Aware Shaper (IEEE 802.1 Qbv) in TSN. TSNCard introduces a novel telemetry protocol that leverages the cyclical nature of TSN networks for data collection at each node. This protocol, coupled with dedicated analytic algorithms and hardware innovations within switches, forms a comprehensive system for TSN monitoring, fault localization and classification. Extensive experiments on both simulation and physical testbeds show that TSNCard can 100% detect fault location and type of the TSN misbehavior while adhering to industrial bandwidth restrictions. TSNCard not only bridges the gap in the TSN protocol stack, but also serves as a versatile toolkit for time-synchronized network analysis, paving the way for future research. The code is available at https://github.com/MobiSense/TSNCard}
}


@article{DBLP:journals/ton/GiroireHTP25,
	author = {Fr{\'{e}}d{\'{e}}ric Giroire and
                  Nicolas Huin and
                  Andrea Tomassilli and
                  St{\'{e}}phane P{\'{e}}rennes},
	title = {Data Center Scheduling With Network Tasks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2839--2853},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3578455},
	doi = {10.1109/TON.2025.3578455},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GiroireHTP25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the placement of jobs inside a data center. Traditionally, this is done by a task orchestrator without taking into account network constraints. According to recent studies, network transfers may account for up to 50% of the completion time of classical jobs. Thus, network resources must be considered when placing jobs in a data center. In this paper, we propose a new scheduling framework, introducing network tasks that need to be executed on network machines alongside traditional (CPU) tasks. The model takes into account the competition between communications for the network resources, which is not considered in the formerly proposed scheduling models with communication. Network transfers inside a data center can be easily modeled in our framework. As we show, classical algorithms do not efficiently handle a limited amount of network bandwidth. We thus propose new provably efficient algorithms with the goal of minimizing the makespan in this framework. We show their efficiency and the importance of taking into consideration network capacity through extensive simulations on workflows built from Google data center traces.}
}


@article{DBLP:journals/ton/XieLXCLW25,
	author = {Pengjin Xie and
                  Yinghui Li and
                  Zhenqiang Xu and
                  Qian Chen and
                  Yunhao Liu and
                  Jiliang Wang},
	title = {Expanding {LPWAN} Concurrency: Combating Collisions Through Orthogonal
                  Transmissions},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2854--2868},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3578187},
	doi = {10.1109/TON.2025.3578187},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XieLXCLW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low Power Wide Area Networks (LPWANs) have emerged as a promising technology for facilitating large-scale, cost-effective connections through low-power, long-range communications. Nevertheless, the deployment of existing LPWANs is impeded by severe packet collisions. In this paper, we present OrthoRa, an innovative technology that significantly enhances the concurrency of low-power, long-range LPWAN transmissions. The cornerstone of OrthoRa lies in a groundbreaking design named Orthogonal Scatter Chirp Spreading Spectrum (OSCSS), which facilitates orthogonal packet transmissions while ensuring low signal-to-noise ratio (SNR) communication within LPWANs. Utilizing OrthoRa, different nodes can transmit packets encoded with unique orthogonal scatter chirps, enabling the receiver to decode collided packets from various nodes. We provide a theoretical validation of OrthoRa, demonstrating its capacity for high concurrency in low SNR communication. To surmount practical challenges inherent in real network deployments, we address the detection of multiple packets in collisions, the identification of scatter chirps for each packet’s decoding, and the precise synchronization of packets under Carrier Frequency Offset. We implemented OrthoRa on the HackRF One platform and conducted extensive performance evaluations. The results corroborate that OrthoRa amplifies network throughput and concurrency by a factor of 50 compared to LoRa and significantly outstripping the state-of-the-art in terms of robustness against collision time offset.}
}


@article{DBLP:journals/ton/ChenLHSG25,
	author = {Fahao Chen and
                  Peng Li and
                  Zicong Hong and
                  Zhou Su and
                  Song Guo},
	title = {Communication-Efficient Sparsely-Activated Model Training via Sequence
                  Migration and Token Condensation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2869--2880},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3585359},
	doi = {10.1109/TON.2025.3585359},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLHSG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mixture-of-Experts (MoE) is an emerging technique for scaling large models with sparse activation. MoE models are typically trained in a distributed manner with an expert parallelism scheme, where experts in each MoE layer are distributed across multiple GPUs. However, the default expert parallelism suffers from the heavy network burden due to the all-to-all intermediate data exchange among GPUs before and after the expert run. Some existing works have proposed to reduce intermediate data exchanges by transferring experts to reduce the network loads, however, which would decrease parallelism level of expert execution and make computation inefficient. The weaknesses of existing works motivate us to explore whether it is possible to reduce inter-GPU traffic while maintaining a high degree of expert parallelism. This paper gives a positive response by presenting Luffy, a communication-efficient distributed MoE training system with two new techniques. First, Luffy migrates sequences among GPUs to hide heavy token pulling paths within GPUs and avoid copying experts over GPUs. Second, we propose token condensation that identifies similar tokens and then eliminates redundant transmissions. We implement Luffy based on PyTorch and evaluate its performance on a testbed of 16 V100 GPUs. Luffy system can achieve a speedup of up to   2.73\\times  2.73\\times    compared to state-of-the-art MoE training systems.}
}


@article{DBLP:journals/ton/SiewZPLRSIYJ25,
	author = {Marie Siew and
                  Haoran Zhang and
                  Jong{-}Ik Park and
                  Yuezhou Liu and
                  Yichen Ruan and
                  Lili Su and
                  Stratis Ioannidis and
                  Edmund Yeh and
                  Carlee Joe{-}Wong},
	title = {Fair Concurrent Training of Multiple Models in Federated Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2881--2896},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3579500},
	doi = {10.1109/TON.2025.3579500},
	timestamp = {Sun, 01 Feb 2026 13:44:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SiewZPLRSIYJ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) enables collaborative learning across multiple clients. In most FL work, all clients train a single learning task. However, the recent proliferation of FL applications may increasingly require multiple FL tasks to be trained simultaneously, sharing clients’ computing resources, which we call Multiple-Model Federated Learning (MMFL). Current MMFL algorithms use naïve average-based client-task allocation schemes that often lead to unfair performance when FL tasks have heterogeneous difficulty levels, as the more difficult tasks may need more client participation to train effectively. Furthermore, in the MMFL setting, we face a further challenge that some clients may prefer training specific tasks to others, and may not even be willing to train other tasks, e.g., due to high computational costs, which may exacerbate unfairness in training outcomes across tasks. We address both challenges by firstly designing FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks in each training round, based on the tasks’ current performance levels. We provide guarantees on the resulting task fairness and FedFairMMFL’s convergence rate. We then propose novel auction designs that incentivizes clients to train multiple tasks, so as to fairly distribute clients’ training efforts across the tasks, and extend our convergence guarantees to this setting. We finally evaluate our algorithm with multiple sets of learning tasks on real world datasets, showing that our algorithm improves fairness by improving the final model accuracy and convergence speed of the worst performing tasks, while maintaining the average accuracy across tasks.}
}


@article{DBLP:journals/ton/LiHZZWLLJW25,
	author = {Zhaoyi Li and
                  Jiawei Huang and
                  Tao Zhang and
                  Shengwen Zhou and
                  Qile Wang and
                  Yijun Li and
                  Jingling Liu and
                  Wanchun Jiang and
                  Jianxin Wang},
	title = {Progress-Aware Transmission Protocol for Efficient In-Network Aggregation
                  in Distributed Machine Learning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2897--2912},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3581112},
	doi = {10.1109/TON.2025.3581112},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiHZZWLLJW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale machine learning typically adopts distributed machine learning (DML) techniques to accelerate model training. Due to the large communication overhead, unfortunately, the phase of gradient aggregation has become the performance bottleneck for data-parallel DML. To reduce traffic volume, several in-network aggregation (INA) transmission protocols are proposed to offload gradient aggregation function into the programmable switches. However, since existing INA transmission protocols use synchronous congestion control mechanism to drive each round of gradient aggregation, the straggling workers lead to long iteration time and significant performance degradation. Besides, we reveal that existing INA solutions cannot provide the fairness performance among multiple jobs with varying number of workers. To solve the above problem, we propose PA-ATP, a progress-aware INA transmission protocol, which adopts the progress-aware asynchronous congestion control. PA-ATP adjusts the sending rate in accordance with the transmission progress, allowing the straggling flow to grab more bandwidth than the leading flow and control the asynchronous degree of straggling job. Moreover, to ensure the fair throughput among multiple jobs, we dynamically adjust the aggregator allocation for each job by tuning the number of hash operations. We use a P4 programmable switch and a kernel-bypass protocol stack to implement PA-ATP. The results of testbed and large-scale NS3 simulations show that PA-ATP reduces training time by up to 62% compared to the state-of-the-art INA transmission protocols.}
}


@article{DBLP:journals/ton/HeYWSXC25,
	author = {Yunhua He and
                  Tingli Yuan and
                  Bin Wu and
                  Keshav Sood and
                  Ke Xiao and
                  Xiuzhen Cheng},
	title = {Cross-Domain Identity Authentication Scheme for the IIoT Identification
                  Resolution System Based on Self-Sovereign Identity},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2913--2928},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3578035},
	doi = {10.1109/TON.2025.3578035},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeYWSXC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the Industrial Internet of Things (IIoT), the identification resolution system enhances communication and overall efficiency between isolated work islands, ensuring the trustworthiness and effectiveness of secure resource sharing across domains through cross-domain authentication. However, traditional identity authentication methods fail to empower users with control over their identity information and face challenges such as difficulty in tracking anonymous users, high computational overhead, and insufficient cross-domain trust. To address these issues, this paper proposes a cross-domain identity authentication scheme based on self-sovereign identity. The scheme leverages aggregate signature technology to enhance authentication efficiency, integrates blockchain technology and smart contracts to achieve cross-domain trust, and designs a mechanism for threshold identity tracking and revocation, as well as an attribute credential update mechanism to enable secure and efficient cross-domain authentication in the identification resolution system. The paper provides formal security definitions and proofs and evaluates the computational and storage efficiency of the scheme through theoretical analysis and experimental simulations. The results demonstrate that the proposed scheme offers significant advantages in resource-constrained scenarios within the IIoT.}
}


@article{DBLP:journals/ton/ZhouPLTLQPZX25,
	author = {Jianer Zhou and
                  Zhiyuan Pan and
                  Zhenyu Li and
                  Gareth Tyson and
                  Weichao Li and
                  Xinyi Qiu and
                  Heng Pan and
                  Xinyi Zhang and
                  Gaogang Xie},
	title = {Safety in DRL-Based Congestion Control: {A} Framework Empowered by
                  Expert Refinement},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2929--2944},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3580436},
	doi = {10.1109/TON.2025.3580436},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouPLTLQPZX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deep reinforcement learning (DRL) has been used in congestion control algorithms (CCAs) for its ability to adapt to different network environments. However, its effectiveness is often hindered by the limited availability of training data and constrained training scales. While it has been proved that combining rule-based (expert) CCAs as a guide for DRL (namely hybrid CCAs) can address this limitation, we show through experimental measurements that rule-based CCAs potentially restrict action exploration of DRL models and may cause the DRL models to overly rely on them for higher reward gains. To address this gap, this paper proposes Marten, a framework that improves the effectiveness of rule-based CCAs for DRL. Marten’s key innovations include an entropy-based dynamic exploration scheme that expands the exploration of DRL, and a reward adjustment scheme to prevent the DRL models’ over-reliance on experts in hybrid CCAs. We have implemented Marten in both simulation platform OpenAI Gym and deployment platform QUIC. Experimental results in both emulated and production networks demonstrate Marten can improve throughput by 0.31% and reduce latency by 12.69% on average compared to the state-of-the-art hybrid CCAs. Compared to BBR, Marten achieves a 2.79% increase in throughput and an 11.73% reduction in latency on average.}
}


@article{DBLP:journals/ton/ChoyLWD25,
	author = {Sharon Choy and
                  Joohan Lee and
                  Bernard Wong and
                  Khuzaima Daudjee},
	title = {Conflux: {A} Multi-Homed Adaptive Bitrate Protocol for On-Site Live
                  Video Streaming},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2945--2960},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3577974},
	doi = {10.1109/TON.2025.3577974},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChoyLWD25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On-site, live video streaming is an important application that many depend on to receive entertainment and local and international news. Streaming live video over wireless (e.g., 3G, LTE, 5G) has begun to replace television production vans that use direct microwave links to the television station since they offer greater physical flexibility and have shorter setup times. In this method, several links are often aggregated together to stream video. Efficiently using multiple links for live, adaptive video streaming is challenging because a protocol must determine the amount of data to send on each link, the video bitrate, and the appropriate level of redundancy so that video frames can be delivered within the user’s strict latency requirements. In this paper, we present Conflux: a multi-homed, adaptive video bitrate protocol for live video streaming. Conflux’s main contribution is the design of a modular live video streaming platform that supports multipath scheduling, video bitrate adaption, and adaptive Forward Error Correction. Conflux presents a probabilistic link quality model that is used in conjunction with a user-specific utility function to determine the video bitrate and redundancy levels that maximize the user’s expected utility. These models are contained in separate modules which serve as building blocks to create customized, multi-homed adaptive video bitrate protocols for users with different requirements. Our experimental results show that Conflux provides more than 22% improvement in video quality over other multi-homed systems for typical, two-link network environments and as much as 108% improvement in more challenging network environments with up to five links.}
}


@article{DBLP:journals/ton/WangMZL25,
	author = {Zhiyuan Wang and
                  Qingkai Meng and
                  Shan Zhang and
                  Hongbin Luo},
	title = {Performance Evaluation for Latency-Stability Tradeoff in Satellite-Terrestrial
                  Integrated Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2961--2976},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3578240},
	doi = {10.1109/TON.2025.3578240},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangMZL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Low-Earth-Orbit (LEO) satellite constellation is promising to extend broadband Internet services to where the terrestrial networks cannot reach, forming a satellite-terrestrial integrated network (STIN). The tradeoff between communication latency and topology stability is the core of constellation design and STIN operation. In general, low-altitude orbits reduce the propagation delay of ground-satellite links (GSLs), but result in frequent handover for GSLs. Frequent GSL handover weakens the topology stability, which is often measured by satellite pass duration and communication session duration. In this paper, we investigate the latency-stability tradeoff in STIN, and propose a stochastic model to analyze and evaluate GSL latency and pass/session duration. We analytically derive the distributions and the expectations of these metrics in closed forms, and verify the theoretic results based on empirical distributions (obtained by trajectory simulation). Compared to previous studies, our results exhibit more concise expressions and arguments. We find that the above closed-form expressions take two key arguments, i.e., the orbit period and the maximal central angle. The expected pass/session duration is directly proportional to the product of the two arguments, and the coefficients can be obtained explicitly (e.g., 0.25 for pass duration). We also demonstrate how to utilize the analytic results to draw deep insights on constellation design, aiming to reduce the GSL latency and keep the topology stable. We believe that our results in this paper could empower the operators and researchers to obtain a rapid understanding on the latency-stability tradeoff of STIN without resorting to time-consuming simulations.}
}


@article{DBLP:journals/ton/XiaWLGC25,
	author = {Junxu Xia and
                  Wenfei Wu and
                  Lailong Luo and
                  Deke Guo and
                  Geyao Cheng},
	title = {In-Network Aggregation as a Generic Service for Distributed Applications},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2977--2992},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3578180},
	doi = {10.1109/TON.2025.3578180},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaWLGC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The performance of distributed applications has long been hindered by network communication, which has emerged as a significant bottleneck. At the core of this issue, the many-to-one incast transfer stands out as one of the primary culprits. Existing works typically decompose the transmission into multiple concurrent sub-processes and utilize servers to aggregate relevant traffic, thus avoiding the incast transfer. However, limited by their theoretical bounds, these methods can only obtain limited performance improvement. In this paper, we discover that leveraging network devices for aggregating incast traffic proves highly effective in surpassing such limitations, while the advent of programmable switches further makes this envision practical. Based on this, we propose GISA as a solution for providing network acceleration across diverse distributed applications. GISA offers generic and uniform interfaces to various applications along with a switch resource sharing mechanism and policy for concurrent tasks. It also ensures correct and reliable transport while minimizing overhead through a low-overhead routing mechanism. Our FPGA-based prototype demonstrates that GISA can achieve line-rate processing when performing data aggregation with minor traffic overhead. Additionally, it supports a wide range of concurrent applications with little development effort.}
}


@article{DBLP:journals/ton/LinQWCL25,
	author = {Zheng Lin and
                  Guanqiao Qu and
                  Wei Wei and
                  Xianhao Chen and
                  Kin K. Leung},
	title = {AdaptSFL: Adaptive Split Federated Learning in Resource-Constrained
                  Edge Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {2993--3008},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3577790},
	doi = {10.1109/TON.2025.3577790},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinQWCL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing complexity of deep neural networks poses significant barriers to democratizing AI to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution that enables device-server co-training through model splitting. However, although system optimization substantially influences the performance of SFL, the problem remains largely uncharted. In this paper, we first provide a unified convergence analysis of SFL, which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on its learning performance, laying a theoretical foundation for this field. Based on this convergence bound, we introduce AdaptSFL, an adaptive SFL framework to accelerate SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls MS and client-side MA to balance communication-computing latency and training convergence. Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve target accuracy than existing benchmarks.}
}


@article{DBLP:journals/ton/JiangYZWFQCJS25,
	author = {Xuyan Jiang and
                  Xiangrui Yang and
                  Tongqing Zhou and
                  Wenfei Wu and
                  Wenwen Fu and
                  Wei Quan and
                  Yingwen Chen and
                  Yihao Jiao and
                  Zhigang Sun},
	title = {FooDog: Empower {TSN} for Efficient Policing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3009--3023},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3586982},
	doi = {10.1109/TON.2025.3586982},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/JiangYZWFQCJS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Time-Sensitive Networking (TSN) is an emerging real-time Ethernet technology that provides deterministic communication for time-sensitive (TS) traffic. At its core, TSN utilizes Per-Stream Filtering and Policing (PSFP) gates to mitigate the disruption of unavoidable frame drift. However, as first identified in this work, the naive PSFP gate design results in heavy memory usage, which hinders normal switching functions. This work proposes an efficient PSFP gate design called FooDog. FooDog employs a two-stage structure and a dual-engine policing mechanism to realize memory-efficient, logic-compact, and fast policing while maintaining minimal latency and jitter for TS traffic. Results on FPGA prototypes show that FooDog consumes only hundreds of kilobits of memory, reducing on-chip memory overheads by more than 90% compared to the unoptimized PSFP gate design. Additionally, it maintains end-to-end latency in the microsecond range and jitter below 150 nanoseconds under abnormal traffic conditions, comparable to typical TSN performance without anomalies.}
}


@article{DBLP:journals/ton/ChenLLXYSL25,
	author = {Yuxin Chen and
                  Jian Li and
                  Zhonghui Li and
                  Kaiping Xue and
                  Nenghai Yu and
                  Qibin Sun and
                  Jun Lu},
	title = {An Asynchronous Key Relay Protocol Design for Large-Scale Quantum
                  Key Distribution Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3024--3039},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3579932},
	doi = {10.1109/TON.2025.3579932},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenLLXYSL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum key distribution (QKD) networks can provide information-theoretically secure key distribution between distant end nodes through key relaying. In QKD networks, the key relay protocol is vital since it provides the coordination specifications between nodes for key relaying and thus directly determines the performance, especially as the network scale expands. However, most existing protocols adopt a synchronous contend-and-relay approach, where the contention and consumption of quantum keys occur simultaneously, neglecting the storable nature of quantum keys and presenting significant challenges in reliability and quantum key utilization. To tackle these challenges, in this paper, we propose an asynchronous key relay protocol (AKRP). AKRP considers the storable nature of quantum keys, and adopts a reserve-then-relay approach to achieve lossless and zero-queuing key relaying through precise management of quantum keys and requests. On this basis, to further improve the performance of the proposed AKRP, we design two enhanced mechanisms, i.e., collision detection and resolution mechanism and multipath routing extension. The former enhances the consensus efficiency of AKRP and provides fine-grained key utilization on each link, and the latter utilizes quantum keys on possible relay paths and thus effectively copes with quantum key exhaustion. By conducting extensive experiments on a semi-physical real QKD network platform, results demonstrate that AKRP is superior to existing schemes in terms of end-to-end key throughput, quantum key consumption, and relaying latency.}
}


@article{DBLP:journals/ton/LiuCWHZWL25,
	author = {Hongyan Liu and
                  Xiang Chen and
                  Di Wang and
                  Qun Huang and
                  Dong Zhang and
                  Chunming Wu and
                  Xuan Liu},
	title = {Toward Secure Inter-Device Coordination in Programmable Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3040--3055},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3580638},
	doi = {10.1109/TON.2025.3580638},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuCWHZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In programmable networks, some networking systems coordinate data plane switches to perform in-network functions (e.g., in-band network telemetry). However, the vulnerabilities associated with inter-device coordination remain largely unexplored and overlooked, which is highly concerning given the increasing popularity of this paradigm. In this paper, we identify three attack scenarios built upon such vulnerabilities, where attackers mislead the behaviors of networking systems. We implement 20 networking systems on Tofino-based switches and a simulator and test them against the identified attacks. Our experimental results show that our attacks severely disrupt the normal operation of these networking systems, e.g., the cache hit rate of NetCache drops by 38%. However, our analysis reveals that none of existing methods fully mitigate our attacks because they fail to verify the packets for inter-device coordination. To this end, we select characteristics from existing methods while addressing their limitations to design effective mitigation methods. Experimental results indicate that our methods perform well in mitigating our attacks and introduce acceptable overheads.}
}


@article{DBLP:journals/ton/SopranoLotoAJV25,
	author = {Nahuel Soprano{-}Loto and
                  Urtzi Ayesta and
                  Matthieu Jonckheere and
                  Ina Maria Verloop},
	title = {Decision-Epochs Matter: Unveiling Its Impact on the Stability of Scheduling
                  With Randomly Varying Connectivity},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3056--3071},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3585661},
	doi = {10.1109/TON.2025.3585661},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SopranoLotoAJV25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A classical result in queuing theory states that in a parallel-queue single-server model, the maximum stability region is unaffected by scheduling decision epochs, and in particular is the same for preemptive and non-preemptive systems. We examine a scenario where queues are randomly connected to the server and show that, unlike the classical case, the maximum stability region strongly depends on the scheduling decision epochs. We compare three settings: decisions can be made anytime (unconstrained), decisions are made only at departures (non-preemptive), and decisions occur when a   \\gamma  \\gamma   -rate exponential clock rings. We observe a significant reduction in the stability region in the non-preemptive setting compared to the unconstrained one, showing that a non-preemptive scheduler cannot take opportunistically advantage of the random varying connectivity. Also, in the   \\gamma  \\gamma   -rate clock setting, one can be arbitrarily close to the maximum stability region in the unconstrained setting if we choose   \\gamma  \\gamma    large enough. In all the settings, we show that the Longest Connected Queue (LCQ) policy achieves maximum stability. From a methodological viewpoint, we introduce a new theoretical tool called “test for fluid limits” (TFL), which offers a method to determine stability on the basis of a simple formal test.}
}


@article{DBLP:journals/ton/ZhaoLBLZWL25,
	author = {Faqi Zhao and
                  Wenhao Li and
                  Huaifeng Bao and
                  Zhaoxuan Li and
                  Guoqiao Zhou and
                  Wen Wang and
                  Feng Liu},
	title = {N{\"{u}}wa: Enhancing Network Traffic Analysis With Pre-Trained
                  Side-Channel Feature Imputation},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3072--3087},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3580557},
	doi = {10.1109/TON.2025.3580557},
	timestamp = {Sun, 18 Jan 2026 07:32:26 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoLBLZWL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic classification stands as an essential endeavor within the realms of network security and management. The recent advances in learning-based methodologies have underscored their efficacy in deducing patterns from the side-channel features of encrypted network traffic. The unpredictability of traffic bursts can result in packet loss during retransmission, thereby generating fragmented feature patterns. Unfortunately, current approaches struggle to adapt to such fragmented features, often leading to a substantial decline in performance. To surmount this challenge, this paper introduces a pre-training-based framework, denoted as Nüwa, which imputes the side-channel features of encrypted network traffic, especially focusing on the temporal attributes of missing packets within a traffic session. Firstly, we propose a word-level Sequence2Embedding (S2E) module to transform side-channel features into tokens for model pre-training, as well as a Traffic Feature Masking strategy (TFM) to simulate the original flows changes in packet loss network. Besides, we also introduce a Traffic Feature Imputation (TFI) module to restore the missing values of original traffic flows in an efficient and context-aware manner. Experiments across four diverse real-world scenarios substantiate Nüwa’s capacity to restore the performance of prevalent temporal models, while maintaining the integrity of the imputed features. Notably, Nüwa has also demonstrated an impressive resilience, even under conditions of extensive feature loss and domain adaptation. The Nüwa prototype has been made accessible to the public for further research and development (https://github.com/Timeless-zfqi/Nuwa).}
}


@article{DBLP:journals/ton/LiYG25,
	author = {Yichen Li and
                  Wenbin Yu and
                  Xinping Guan},
	title = {Trajectory Planning-Aided Cooperative Localization for Multi-AUV Networks
                  Under Harsh Communication Conditions: {A} Co-Designed Approach},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3088--3103},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3579395},
	doi = {10.1109/TON.2025.3579395},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYG25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the operation of multiple autonomous underwater vehicle (AUV) networks, AUV self-localization, inter-AUV communication, and AUV trajectory planning (collectively referred to as LCP) are fundamental technologies. Among them, accurate position estimation is often a prerequisite for effective planning, while communication is essential in both parts. Previous studies have typically addressed these three areas independently, yet their conflicting demands on resources, objectives, etc., frequently pose significant challenges to performance improvement. This paper investigates the LCP from an integrated perspective and a co-designed approach jointly addresses all of them is proposed based on graphical models and belief propagation. Specifically, guided by the various underwater communication conditions, a cooperative localization algorithm for AUVs is designed with the assistance from trajectory planning. Different from the previous, planning results are directly used for localization to resist harsh communications, rather than indirectly affecting localization accuracy by, for example, optimizing geometric relationships. As a result, the feedback enhancement from trajectory planning to AUV localization is realized and the mechanism of their mutual promotion for different communication conditions is achieved. Moreover, considering the changing trajectories, the proposed cooperative localization algorithm is further refined for underwater obstacle-avoidance scenarios. Various simulations and field experiments validate that the proposed algorithm can improve the overall performance of LCP by comparisons with state-of-the-art alternative methods.}
}


@article{DBLP:journals/ton/AaronsonC25,
	author = {Dan Aaronson and
                  Reuven Cohen},
	title = {{PB-FS:} Postcard-Based Fast Start},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3104--3115},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3580615},
	doi = {10.1109/TON.2025.3580615},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AaronsonC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose PB-FS (Postcard-Based Fast Start), a rate initialization scheme that uses direct feedback from the switches to quickly correct the rates of new datacenter flows that begin at the line rate and cause congestion. PB-FS is designed to easily integrate into any datacenter congestion control protocol. We evaluate PB-FS in two datacenter environments: a lossless network that runs RoCE, and a lossy network that uses RDMA with selective repeat. We show that PB-FS significantly reduces tail latency of short flows while maintaining throughput, in both lossless and lossy datacenters.}
}


@article{DBLP:journals/ton/MaH25,
	author = {Ruichun Ma and
                  Wenjun Hu},
	title = {Cross-Medium Networking With Transflective Flexible Metasurfaces},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3116--3131},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3584675},
	doi = {10.1109/TON.2025.3584675},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging wireless networks increasingly venture beyond over-the-air communication, such as deep-tissue networking for medical sensors, air-water communication for oceanography, and soil sensing for agriculture. They face the fundamental challenge of significant reflection and power loss at medium interfaces. We present RF-Mediator, a transflective metasurface placed near a medium interface to program the behavior of the physical boundary. Our hardware design comprises a single layer of varactor-based surface elements with specific metallic patterns and wiring. With the biasing voltage tuned element-wise, the surface dynamically mediates between the adjacent media to control and utilize both signal reflection and transmission, enhancing connectivity for wireless endpoints on both sides. A multi-stage control algorithm efficiently determines the surface configuration to handle dynamic adaptation needs for joint medium impedance matching and beamforming towards multiple endpoints simultaneously. We implement a lightweight and flexible metasurface prototype and experiment with diverse cross-medium setups. Extensive evaluation shows that RF-Mediator provides a median power gain of 8 dB for air-tissue links, reaching up to 30 dB gain for backscatter links, and up to 18 dB for multiple links concurrently.}
}


@article{DBLP:journals/ton/NaHGZZYL25,
	author = {Xin Na and
                  Yuan He and
                  Xiuzhen Guo and
                  Jia Zhang and
                  Yang Zou and
                  Zihao Yu and
                  Yunhao Liu},
	title = {Analog Backscatter for Commodity WiFi With Payload Transparency},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3132--3145},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582020},
	doi = {10.1109/TON.2025.3582020},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NaHGZZYL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Backscatter is an enabling technology for battery-free sensing in today’s Artificial Intelligence of Things (AIOT). Building a backscatter sensing system, however, is a daunting task, due to two obstacles: the unaffordable power consumption of the microprocessor and the coexistence with the ambient carrier’s traffic. In order to address the issues, we present Leggiero, the first-of-its-kind analog WiFi backscatter with payload transparency, and its enhanced version, Leggiero+. A specially designed circuit based on the varactor diode directly converts fast-varying analog sensor signals into the RF (radio frequency) signal phase, eliminating the need for a microprocessor to interface between the radio and the sensor. By precisely locating the WiFi packet’s extra long training field (LTF) section and carefully designing the reference circuit, Leggiero embeds the analog phase into the channel state information (CSI). A commodity WiFi receiver without hardware modification can simultaneously decode the WiFi and the sensor data. We implement and evaluate Leggiero and Leggiero+ under varied settings. Results show the tag’s power consumption (excluding the power of the peripheral sensor module) is   30\\mu  30\\mu   W at a 400Hz sampling rate,   4.8\\times  4.8\\times    and   4\\times  4\\times    lower than the state-of-the-art WiFi backscatter schemes. Leggiero+ demonstrates enhanced throughput, communication range, and analog signal reproduction accuracy. Our design supports a variety of sensing applications, while maintaining the WiFi carrier’s throughput performance.}
}


@article{DBLP:journals/ton/LinSC25,
	author = {Qiulin Lin and
                  Junyan Su and
                  Minghua Chen},
	title = {Optimal Algorithms for Online Age-of-Information Optimization in Energy
                  Harvesting Systems},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3146--3161},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582413},
	doi = {10.1109/TON.2025.3582413},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinSC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the scenario where an energy harvesting source sends its updates to a receiver. The source optimizes its energy allocation over a decision period to maximize a sum of time-varying functions of the age of information (AoI), representing the value of providing timely information. In a practical online setting, we need to make irrevocable energy allocation decisions at each time while the time-varying functions and the energy arrivals are only revealed sequentially. The problem is then challenging as 1) we are facing uncertain energy harvesting arrivals and time-varying functions, and 2) the energy allocation decisions and the energy harvesting process are coupled due to the capacity-limited battery. In this paper, we develop an optimal online algorithm   \\textsf {CR-RePursuit} \\textsf {CR-RePursuit}   and show it achieves   (\\ln \\theta +1) (\\ln \\theta +1)  -competitiveness, where   \\theta  \\theta    is a parameter representing the level of uncertainty of the time-varying functions. It is the optimal competitive ratio among all deterministic and randomized online algorithms. We also introduce an adaptive variant of the algorithm, +, that further exploits the revealed information to obtain significantly improved empirical performance. We conduct simulations based on real-world traces and compare our algorithms with conceivable alternatives. The results show that our algorithms achieve 15% performance improvement as compared to the state-of-the-art baseline.}
}


@article{DBLP:journals/ton/WangLDBA25,
	author = {Juncheng Wang and
                  Ben Liang and
                  Min Dong and
                  Gary Boudreau and
                  Ali Afana},
	title = {Age-of-Information Minimization With Weight Limits for Semi-Asynchronous
                  Online Distributed Optimization},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3162--3178},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3583308},
	doi = {10.1109/TON.2025.3583308},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLDBA25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider online distributed optimization where a server and multiple devices collaborate to minimize a sequence of time-varying global loss functions. To accommodate slow devices that may require multiple time slots to compute their local decisions, the server uses semi-asynchronous aggregation of the local decisions, which complicates device scheduling and performance optimization. In this work, we first analyze the convergence of semi-asynchronous aggregation in the presence of time-varying local update delays and loss-function weights. Our analysis leads to an online scheduling problem to minimize the accumulated age of information on the local decision updates, subject to individual long-term constraints on the total weights of the scheduled devices. We then design an efficient scheduling policy, termed Age-of-Information Minimization with Weight Limits (AIMWeL), through a modified Lyapunov optimization approach that uses the weighted sum of linear age-of-information values and quadratic virtual queues as a new Lyapunov function. We show that AIMWeL has bounded optimality ratio, via a novel double relaxation approach to handle the unique scheduling-dependent communication indicator with time-varying probabilities of completing local decision update caused by semi-asynchronous aggregation. When AIMWeL is applied to semi-asynchronous federated learning, our simulation results based on standard image classification datasets demonstrate that AIMWeL uses significantly less time to reach the same classification accuracy achieved by the current best alternatives for both convex logistic regression and non-convex convolutional neural networks.}
}


@article{DBLP:journals/ton/HeSSWKNHL25,
	author = {Long He and
                  Geng Sun and
                  Zemin Sun and
                  Qingqing Wu and
                  Jiawen Kang and
                  Dusit Niyato and
                  Zhu Han and
                  Victor C. M. Leung},
	title = {QoE Maximization for Multiple-UAV-Assisted Multi-Access Edge Computing
                  via an Online Joint Optimization Approach},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3179--3195},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3581531},
	doi = {10.1109/TON.2025.3581531},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HeSSWKNHL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In disaster scenarios, conventional terrestrial multi-access edge computing (MEC) paradigms, which rely on ground infrastructure, may become unavailable due to infrastructure damage. With high-probability line-of-sight (LoS) communication, flexible mobility, and low cost, uncrewed aerial vehicle (UAV)-assisted MEC is emerging as a promising paradigm to provide edge computing services for ground user devices (UDs) in disaster-stricken areas. However, the limited battery capacity, computing resources, and spectrum resources also pose serious challenges for UAV-assisted MEC, which can potentially shorten the service time of UAVs and degrade the quality of experience (QoE) of UDs without an effective control approach. To this end, in this work, we first present a hierarchical architecture of multiple-UAV-assisted MEC networks that enables the coordinated provision of edge computing services by multiple UAVs. Then, we formulate a joint task offloading, resource allocation, and UAV trajectory control optimization problem (JTRTOP) to maximize the QoE of UDs while considering the energy and resource constraints of UAVs. Since the problem is proven to be a future-dependent and NP-hard problem, we propose a novel online joint task offloading, resource allocation, and UAV trajectory control approach (OJTRTA) to solve the problem. Specifically, the JTRTOP is first transformed into a per-slot real-time optimization problem (PROP) using the Lyapunov optimization framework. Then, a two-stage optimization method based on game theory and convex optimization is proposed to solve the PROP. Simulation results show that the proposed OJTRTA outperforms various benchmark approaches and achieves at least a 10% improvement in the QoE of UDs compared to deep reinforcement learning (DRL)-based algorithms, thereby validating the superiority of the proposed approach.}
}


@article{DBLP:journals/ton/YoshinakaKTH25,
	author = {Yutaro Yoshinaka and
                  Yuki Koizumi and
                  Junji Takemasa and
                  Toru Hasegawa},
	title = {Payload Queueing for Optimizing Complex Header Processing in Programmable
                  Switches},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3196--3211},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582400},
	doi = {10.1109/TON.2025.3582400},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YoshinakaKTH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable switches offer a promising platform for fast and flexible in-network computing. However, a standard mechanism, packet recirculation, degrades throughput due to bandwidth consumption caused by the loopback of not only packet headers but also cumbersome payloads. This paper proposes P4QRS, a mechanism that reduces payload recirculation by retaining payloads within the switch. Specifically, P4QRS bifurcates packets into headers and payloads, which undergo the computation process through pipelines and the buffering process leveraging the switch’s queue behavior, respectively. The headers and payloads then rendezvous for reassembly into complete packets to be sent out. To validate its effectiveness, we evaluated P4QRS through both an analytical model and implementation on state-of-the-art hardware programmable switches. Our results demonstrate that P4QRS operates stably and significantly accelerates complex in-switch computations. Moreover, we address packet reordering, which is a fundamental concern arising because P4QRS performs payload buffering on a per-packet basis rather than per-flow. After identifying the mechanism for developing packet reordering in three stages, we design, implement, and evaluate mitigation strategies targeting each stage. These strategies effectively suppress packet reordering in both flow-aware and flow-unaware cases.}
}


@article{DBLP:journals/ton/WangWZYWHH25,
	author = {Xueyi Wang and
                  Xingwei Wang and
                  Rongfei Zeng and
                  Li Yan and
                  Dongkuo Wu and
                  Qiang He and
                  Min Huang},
	title = {Truthful Padding-Based Auction Mechanisms for Cross-Cloud Link Bandwidth
                  Allocation and Pricing},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3212--3227},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582304},
	doi = {10.1109/TON.2025.3582304},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangWZYWHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {More and more application providers (APs) start to deploy their geo-distributed services in multiple cloud environments, such as JointCloud, federated clouds and InterCloud. Thus, massive cross-cloud traffic is generated from the services of APs, who need to pay Internet service providers (ISPs) for using their bandwidth. As such, an effective cross-cloud link bandwidth allocation and pricing mechanism is needed between APs and ISPs. Existing fixed-price scheme lacks market efficiency. Thus, we propose a truthful padding-based auction mechanism (TPAM) for cross-cloud bandwidth, which introduces the padding method and well-designed pricing strategy to ensure desirable properties. This mechanism is flexible enough to allow each AP to win the whole request, or win the specified proportional request, or lose and get nothing. Specifically, we first devise a linear-program-based method to calculate the padding vector for each candidate AP. Next, we design a padding-based method to determine the winning APs and match them with ISPs who offer the cheapest bandwidth. Finally, we design a critical-value-based pricing strategy and a marginal-cost-based pricing strategy for APs and ISPs to achieve truthfulness and budget balance. Theoretical analyses prove that TPAM achieves truthfulness, budget balance, individual rationality, asymptotic efficiency and computational tractability. Trace-driven simulation results also validate the effectiveness and efficiency of TPAM.}
}


@article{DBLP:journals/ton/PromponasCT25,
	author = {Panagiotis Promponas and
                  Tingjun Chen and
                  Leandros Tassiulas},
	title = {On the Optimization and Stability of Sectorized Wireless Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3228--3243},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3583267},
	doi = {10.1109/TON.2025.3583267},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PromponasCT25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future wireless networks need to support the increasing demands for high data rates and improved coverage. One promising solution is sectorization, where an infrastructure node is equipped with multiple sectors employing directional communication. Although the concept of sectorization is not new, it is critical to fully understand the potential of sectorized networks, such as the rate gain achieved when multiple sectors can be simultaneously activated. In this paper, we focus on sectorized wireless networks, where sectorized infrastructure nodes with beam-steering capabilities form a multi-hop mesh network. We present a sectorized node model and characterize the capacity region of these sectorized networks. We define the flow extension ratio and the corresponding sectorization gain, which quantitatively measure the performance gain introduced by node sectorization as a function of the network flow. Our objective is to find the sectorization of each node that achieves the maximum flow extension ratio, and thus the sectorization gain. Towards this goal, we formulate the corresponding optimization problem and develop an efficient distributed algorithm that obtains the node sectorization under a given network flow with an approximation ratio of 2/3. Additionally, we emphasize the class of Even Homogeneous Sectorizations, which simultaneously enhances the efficiency of dynamic routing schemes with unknown arrival rates and increases network capacity. We further propose that if sectorization can be adapted dynamically over time, either a backpressure-driven or maximum weighted b-matching-based routing approach can be employed, thereby expanding the achievable capacity region while preserving stability under unknown traffic conditions. Through extensive simulations, we evaluate the sectorization gain and the performance of the proposed algorithms in various network scenarios.}
}


@article{DBLP:journals/ton/KumarHH25,
	author = {Akash Kumar and
                  Yudi Huang and
                  Ting He},
	title = {Queueing Network Topology Inference Using Passive and Active Measurements},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3244--3256},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582205},
	doi = {10.1109/TON.2025.3582205},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/KumarHH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We revisit a classic problem of inferring the routing tree for a given source in a packet-switched network from end-to-end measurements, with two critical differences from existing solutions: (i) instead of exclusively relying on active measurements obtained by probing, we strive to maximally utilize passive measurements obtained from data packets; (ii) instead of inferring a logical topology that omits degree-2 nodes, we want to recover the physical topology containing all the nodes. Our main idea is to utilize the detailed queueing dynamics inside the network to estimate a certain parameter (residual capacity) of each queue, and then use the estimated parameters as fingerprints to detect the queues shared across paths and thus infer the topology. To this end, we develop a Laplace-transform-based estimator to estimate the parameters of a tandem of queues from end-to-end delays, and efficient algorithms to infer the topology by identifying the parameters associated with the same queue. To improve the accuracy, we further develop a hybrid algorithm that uses the information from active measurements to identify (generalized) siblings and the information from passive measurements to detect shared queues on the paths from the source to each pair of identified siblings. Our inferred topology is guaranteed to converge to the ground-truth topology as the number of measurements increases, up to a permutation of the queues traversed by the same set of paths. Our evaluations in both queueing-theoretic and packet-level simulations show that the proposed solutions, particularly the hybrid algorithm, significantly improve the accuracy over the state of the art.}
}


@article{DBLP:journals/ton/MichaelidesCNB25,
	author = {Costas Michaelides and
                  Miguel Casasnovas and
                  David Nunez and
                  Boris Bellalta},
	title = {Lessons Learned From a Large-Scale Virtual Reality Experience Over
                  Wi-Fi},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3257--3269},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3582033},
	doi = {10.1109/TON.2025.3582033},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MichaelidesCNB25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large-scale virtual reality (VR) enables novel art forms in large physical environments. A key technical solution in such cases is to split the rendering process between a client and a streamer. In this article, we focused on the peculiarities of VR split rendering and particularly on the performance of Wi-Fi. First, we calculated the network delay budget and then conducted a set of experiments to evaluate the single-user and multi-user performance of VR split rendering systems over Wi-Fi in terms of latency. Our findings were summarized as recommendations on: i) the frame rate, ii) the encoding bitrate, iii) the signal strength, iv) the channel bandwidth, and v) the multiple access techniques. In particular, our multi-user tests highlighted cases where using the highest settings or latest features of Wi-Fi can compromise performance. Finally, we applied our recommendations to a large-scale multi-user case study that took place physically in the multi-purpose hall of our university campus and virtually in the Turret Singers environment of SteamVR Home. This article serves as a reference to the critical aspects of VR split rendering systems over Wi-Fi.}
}


@article{DBLP:journals/ton/YangFLSWWX25,
	author = {Yuxiang Yang and
                  Xuewei Feng and
                  Qi Li and
                  Kun Sun and
                  Ziqiang Wang and
                  Ao Wang and
                  Ke Xu},
	title = {Off-Path {TCP} Hijacking Attack to NAT-Enabled Wi-Fi Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3270--3285},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3586020},
	doi = {10.1109/TON.2025.3586020},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangFLSWWX25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we uncover a novel side-channel vulnerability arising from the shared NAT tables of Wi-Fi routers, enabling malicious insiders to hijack TCP connections between other clients and remote servers. First, by creating different NAT mappings within the shared NAT table, an off-path attacker can infer whether a victim client within the same Wi-Fi network is communicating with an external host over TCP, leveraging the widely adopted NAT port preservation strategy and insufficient reverse path validation in Wi-Fi routers. Once an active connection is detected, the attacker can manipulate the victim’s NAT mapping in the shared NAT table with spoofed TCP packets, exploiting the lack of TCP window tracking in most routers. In this way, the attacker can intercept TCP packets from the server and obtain the current sequence and acknowledgment numbers, which in turn allows the attacker to forcibly close the connection, poison the traffic in plain text, or reroute the server’s incoming packets to the attacker. We test 67 widely used routers from 30 vendors and discover that 52 of them are vulnerable. Also, we conduct an extensive measurement study on 93 real-world Wi-Fi networks and find that 75 of them (81%) are fully affected to our attack. Our case study shows that it takes about 17.5, 19.4, and 54.5 seconds on average to terminate SSH connections, download private files from FTP servers, and inject fake HTTP response packets with success rates of 87.4%, 82.6%, and 76.1%. Moreover, We evaluate the feasibility of the proposed attack in NAT-enabled IPv6 Wi-Fi networks. We responsibly disclose the vulnerability and suggest mitigation strategies to all affected vendors and have received positive feedback, including acknowledgments, CVEs, rewards, and adoption of our suggestions.}
}


@article{DBLP:journals/ton/ZhangWYWWHMWM25,
	author = {Pei Zhang and
                  Yunzhe Wang and
                  Hanyan Yin and
                  Botong Wu and
                  Qi Wang and
                  Xiaohong Huang and
                  Yan Ma and
                  Jilong Wang and
                  Congcong Miao},
	title = {Predictive Configuration on {DHCP} in WLANs},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3286--3301},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3585618},
	doi = {10.1109/TON.2025.3585618},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWYWWHMWM25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {DHCP is widely deployed in WLANs to automatically assign IP addresses to WiFi devices when users connect to the WLANs. However, frequent user mobility brings big challenges to the DHCP performance. Recently proposed IP configuration (e.g., IP lease time, size of IP address pool) decisions on DHCP are based on traditional models to study user mobility patterns which lead to poor DHCP performance since the online time of individuals varies due to their personal pReferences and the number of crowds differs spatially and temporally. In this paper, we propose PredHCP, a predictive configuration framework on DHCP to improve the DHCP performance. Specifically, PredHCP utilizes an attention-based recurrent neural network (ARNN) to learn sequential patterns of individual mobility and accurately predicts user online time to ensure the effective IP lease time configuration. Meanwhile, PredHCP introduces a spatio-temporal graph neural network (STGNN) to learn both spatial and temporal dependencies of crowd migration and accurately predict crowd size in each area to ensure effective IP pool configuration. We conduct comprehensive experiments on real network traces for a month to evaluate the performance of PredHCP. Experimental results show that PredHCP can accurately predict user mobility patterns by achieving lower prediction errors. By accurately modeling mobility patterns, PredHCP makes effective IP configuration to ensure high DHCP performance. Large-scale simulation results show that PredHCP can save up to 69% IP addresses and the IP efficiency is 41% which outperforms existing methods by 6%.}
}


@article{DBLP:journals/ton/LiLXGLQJY25,
	author = {Qing Li and
                  Jiaye Lin and
                  Guorui Xie and
                  Zhongxu Guan and
                  Zeyu Luan and
                  Zhuyun Qi and
                  Yong Jiang and
                  Zhenhui Yuan},
	title = {Distributed Multi-Task In-Network Classification on Programmable Switches
                  by Ensemble Models},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3302--3317},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3590275},
	doi = {10.1109/TON.2025.3590275},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiLXGLQJY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Offloading machine learning models for network classification on high-throughput programmable switches is a promising technology, enabling line-speed in-network classification. Existing solutions are centralized, deploying a complete but heavy model on a single switch with limited hardware resources, causing unsatisfactory accuracy, network-wide resource wastage, and non-generic single-task classification. Therefore, we propose In-Forest-M, a general distributed multi-task in-network classification framework. Firstly, we develop a Lightweight Ensemble Generic Optional Model (LEGO), which can be transformed into base models with full functionality. Each switch only needs to deploy lightweight base models rather than complete ensemble models. The significant reduction in resource consumption allows the deployment of larger models with higher accuracy and more models that support diverse tasks. We employ a fine-grained enhancement mechanism to enhance the classification performance of base models. As traffic traverses different switches, In-Forest-M aggregates the classification results of multiple enhanced base models to improve accuracy further. Secondly, we introduce a two-phase resource-aware model allocation strategy that assigns different task-specific enhanced base models to switches under resource constraints and task requirements. To respond to dynamic traffic changes, we design an optimization-driven reinforcement learning algorithm. Moreover, we propose a lightweight update mechanism for flexible model scaling. Comprehensive experiments reveal that, compared with state-of-the-art in-network classification solutions in three real network topologies, In-Forest-M achieves increased accuracy and reduced switch rules while exhibiting great generality in multi-task classification.}
}


@article{DBLP:journals/ton/FanXZCHY25,
	author = {Weibei Fan and
                  Fu Xiao and
                  Pinchang Zhang and
                  Hui Cai and
                  Lei Han and
                  Shui Yu},
	title = {Topology-Awareness Fault-Tolerant Migration for Node Cascading Failures
                  in Data Center Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3318--3333},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3583935},
	doi = {10.1109/TON.2025.3583935},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FanXZCHY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid increase in user demand for business traffic, the deployment and migration of virtual service function chains (VSFC) considering network load balancing has become a fascinating research topic. In this paper, we delve into the cascading failures in virtual network function (VNF) migration and design a topology-awareness fault-tolerant migration mechanism for VSFC. Firstly, we design a measurement model for the evolution of network nodes under cascading faults, which provides an evaluation method for node importance. Secondly, we present a Topology and Resource Aware VNF fault-tolerant migration framework (TRA) under cascading faults. Finally, we investigate a topology-awareness energy consumption optimization algorithm based on the cascading failure. The proposed algorithm can reduce network energy consumption and alleviate network link congestion while reasonably migrating virtual machines to meet system performance requirements. Through extensive simulation experiments and real testbed evaluations, TRA reduced migration time and average latency by 28.6% and 24.7% compared with the average performance of VVi, TeaVisor, and MiOvnm, respectively. In addition, TRA has improved revenue to expense ratio and load balancing index by 19.6% and 26.5%, respectively.}
}


@article{DBLP:journals/ton/ZhaoLZW25,
	author = {Shizhen Zhao and
                  Ximeng Liu and
                  Tianyu Zhu and
                  Xinbing Wang},
	title = {Pulse+: DetNet Routing Under Delay-Diff Constraint},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3334--3348},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3586040},
	doi = {10.1109/TON.2025.3586040},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoLZW25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deterministic Networking (DetNet) is a rising technology that offers deterministic delay & jitter and extremely low packet loss in large IP networks. To achieve determinism under failure scenarios, DetNet requires finding at least two paths with close end-to-end delay, i.e., a delay-diff constraint, for mission-critical flows. However, how to find two routing paths subject to the delay-diff constraint remains open. We study the DetNet routing problem in two scenarios. First, given a primary path, we propose Pulse+, which finds a secondary path whose end-to-end delay is within a range determined by the end-to-end delay of the primary path and the delay-diff requirement. Second, we propose CoSE-Pulse+, which integrates Pulse+ with a divide-and-conquer approach to find a pair of paths that meet DetNet’s delay-diff constraint. Both Pulse+ and CoSE-Pulse+ guarantee solution optimality. Notably, although Pulse+ and CoSE-Pulse+ do not have a polynomial worst-case time complexity, their empirical solver running time is better than that of other algorithms. We evaluate Pulse+ and CoSE-Pulse+ against the K-Shortest-Path and Lagrangian-dual based algorithms using synthetic test cases generated over networks with up to 10000 nodes. Both Pulse+ and CoSE-Pulse+ can solve more test cases than other algorithms under a predefined time limit. Compared to the second best algorithm, Pulse+ achieves an average-time speedup of   5\\times  5\\times    and CoSE-Pulse+ achieves an average-time speedup of   22\\times  22\\times   . Our code and test cases are available at https://gitee.com/zsz2019_shizhenzhao/drcr}
}


@article{DBLP:journals/ton/LiuSZGQ25,
	author = {Yi Liu and
                  Shouqian Shi and
                  Ruilin Zhou and
                  Yuhang Gan and
                  Chen Qian},
	title = {Parrot Hashing: Fast and Low-Memory Table Lookups for Network Applications
                  With One {CRC-8}},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3349--3362},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3587475},
	doi = {10.1109/TON.2025.3587475},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuSZGQ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Key-value lookup functions have been widely applied to network applications, including FIBs, load balancers, and content distributions. Two key performance requirements of a lookup algorithm are high throughput and small memory cost. One limitation of existing fast network lookup algorithms is that they require multiple independent and uniform hash functions, which cost high computation time and might not be available on existing hardware network devices. Recently developed learned model hashing (LMH) proposes to use a linear machine learning model to replace hash functions to avoid hash computation, but they are not optimized for memory cost. We propose a novel network lookup method called Parrot hashing, which uses a learned model to distribute keys into different buckets and applies a simple perfect hashing method to resolve the collisions of the keys in a bucket. Parrot can be implemented with only one CRC-8, which is available on all network devices. We implement Parrot in three prototypes: a software program on end hosts, a software switch, and a FIB running on a hardware programmable switch. The experimental results show that Parrot achieves the highest lookup throughput on all three prototypes, compared to existing methods. Its memory cost is also significantly lower than that of LMH.}
}


@article{DBLP:journals/ton/WeiYQZS25,
	author = {Yannan Wei and
                  Qiang Ye and
                  Kaige Qu and
                  Weihua Zhuang and
                  Xuemin Shen},
	title = {{E2E} Performance Modeling for Slice-Based Video Streaming With Layered
                  Encoding},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3363--3377},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3591401},
	doi = {10.1109/TON.2025.3591401},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WeiYQZS25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we present a performance analytical model for end-to-end (E2E) service provisioning (i.e., processing or transmission) of layer-encoded video packets over a network slice in the core network. The disparate service reliability requirements of base layer (BL) and enhancement layer (EL) packets are considered in the proposed analytical model for the E2E packet delays, deadline violation probabilities, and throughputs of BL and EL packets. Specifically, a network function virtualization (NFV) node along the routing path of the video streaming slice is split into two consecutive logical nodes, one for packet processing and the other for transmission, based on which a segment-based analysis framework is proposed for E2E service performance modeling. A two-stage queuing model is established to obtain the approximate steady-state probability distribution of queue length at the first node in the first segment, upon which the BL/EL packet delay, deadline violation probability, and throughput at the segment are derived. In addition, the inter-departure time of successive packets departing from the first segment is analyzed based on an approximate M/D/1 system, and the packet departure process at the first segment is approximated as a Poisson process under the assumption of a large packet service rate of the first node. The independence between two consecutive segments is then achieved for analysis tractability, based on which the E2E performance measures are derived. Extensive simulation results demonstrate the accuracy of our proposed performance analytical model and its effectiveness such as in transport parameter determination.}
}


@article{DBLP:journals/ton/WeiGFL25,
	author = {Wenting Wei and
                  Huaxi Gu and
                  Liying Fu and
                  Baochun Li},
	title = {Grace: Toward Routing in Dynamic Network Environments With Graph Embedding},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3378--3390},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3586010},
	doi = {10.1109/TON.2025.3586010},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WeiGFL25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent efforts have explored adaptive routing via deep reinforcement learning (DRL) techniques without handcrafted parameter engineering. Intrinsically, routing decision-making is essentially a process used to find a subgraph in a graph-structured network. However, previous works seldom took topological relationships into consideration when providing adaptive routing algorithms, causing them to suffer from suboptimal routes in dynamic network environments involving both varying traffic loads and burst traffic. In this paper, we present Grace, a novel graph embedding-based Deep Reinforcement Learning framework tailored for distributed routing algorithm optimization within the Software-Defined Networking (SDN) paradigm. Specifically, Grace leverages graph embedding to translate graph-structured entities into low-dimensional vectors, thereby enabling multiple DRL agents to learn optimal routing paths under dynamic network environments. Unfortunately, training multiple agents encounters inherent challenges in complicated and dynamic network scenarios. In response, we design an adaptive incremental training method for Grace that makes the model adapt to task complexity in a gradual manner, while speeding up its retraining efforts when environments change. To further accelerate convergence, we integrate intrinsic curiosity into Grace to tackle large environments with sparse rewards. Extensive experiments conducted on two real-world topologies demonstrate the rationality and effectiveness of Grace, and the results show throughput improvements of up to 40.1% compared to other state-of-the-art DRL routing algorithms under bursty traffic conditions.}
}


@article{DBLP:journals/ton/HuZZPJXYY25,
	author = {Yujiao Hu and
                  Yining Zhu and
                  Huayu Zhang and
                  Yan Pan and
                  Qingmin Jia and
                  Renchao Xie and
                  Gang Yang and
                  F. Richard Yu},
	title = {Deterministic Scheduling and Network Structure Optimization for Time-Critical
                  Computing Tasks in Industrial IoT},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3391--3407},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3587916},
	doi = {10.1109/TON.2025.3587916},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuZZPJXYY25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Industrial Internet of Things (IIoT) has become a critical technology to accelerate the process of digital and intelligent transformation of industries. As the cooperative relationship between smart devices in IIoT becomes more complex, obtaining deterministic responses of IIoT periodic time-critical computing tasks becomes a crucial and nontrivial problem. However, few current works in cloud/edge/fog computing focus on this problem. This paper is a pioneer in exploring deterministic scheduling and network structural optimization problems for IIoT periodic time-critical computing tasks. We first formulate the two problems and derive theorems to help quickly identify computation and network resource sharing conflicts. Based on this, we propose a deterministic scheduling algorithm, IIoTBroker, which realizes a deterministic response for each IIoT task by optimizing the fine-grained computation and network resources, and a network optimization algorithm, IIoTDeployer, which provides a cost-effective structural upgrade solution for existing IIoT networks. Our methods are illustrated to be cost-friendly, scalable, and deterministic response guaranteed with low computation cost from our simulation results.}
}


@article{DBLP:journals/ton/MaQYLWLWH25,
	author = {Lianbo Ma and
                  Ying Qian and
                  Guo Yu and
                  Zhetao Li and
                  Liang Wang and
                  Qing Li and
                  Xingwei Wang and
                  Guangjie Han},
	title = {{TBCIM:} Two-Level Blockchain-Aided Edge Resource Allocation Mechanism
                  for Federated Learning Service Market},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3408--3423},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3589017},
	doi = {10.1109/TON.2025.3589017},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MaQYLWLWH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With advances in the edge computing (EC) and federated learning (FL) technologies in jointcloud, the edge FL service market has emerged recently and it requires trading edge resources between model requesters and data owners to complete FL tasks, which needs to incentivize sufficient data owners to participate in model training tasks. However, the limitations of resource trading and incentive design for edge FL service market have not been well addressed. In this paper, we propose a two-level blockchain-aided resource trading mechanism for encouraging appropriate edge servers to compete for dynamic FL tasks from the market while incentivizing data owners to participate in the FL tasks. At the upper level, we apply the deep learning-based reverse auction to model the dynamics of the task server selection process, with the aim of maximizing the total social welfare of the edge FL service market, where the edge server, as a seller, considers not only the data contribution of edge devices but also the cost of using blockchain when bidding. At the lower level, the edge servers offer rewards in exchange for the data owners’ participation, while the parameter aggregation is completed through the blockchain in a decentralized manner, which improves the FL’s robustness. Then, we utilize the Stackelberg game to model the dynamic process that the data owners compete for the servers’ revenue. We conduct extensive simulation experiments and the experimental results show that the proposed mechanism is able to get maximized social welfare and provide effective insights and strategies for the resource trading in the edge FL market to complete the federated training.}
}


@article{DBLP:journals/ton/LiuQZCYLMZZZ25,
	author = {Kang Liu and
                  Wei Quan and
                  Yuming Zhang and
                  Nan Cheng and
                  Chengxiao Yu and
                  Mingyuan Liu and
                  Xiaoting Ma and
                  Qimiao Zeng and
                  Hongke Zhang and
                  Weihua Zhuang},
	title = {{INCC:} In-Network Congestion Control With Proactive Bottleneck Awareness},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3424--3439},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3589553},
	doi = {10.1109/TON.2025.3589553},
	timestamp = {Sun, 18 Jan 2026 07:32:27 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuQZCYLMZZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Delay-sensitive applications like telemedicine and VR/AR intensify competition for network resources and elevate congestion risks, particularly in mobile networks with highly dynamic link conditions. Traditional end-to-end congestion control methods suffer from prolonged response times, rendering them ineffective for Delay-sensitive applications. To this end, this paper proposes a novel In-Network Congestion Control (INCC) mechanism that accelerates congestion control by enabling network nodes to proactively identify bottlenecks and promptly notify end-hosts. Unlike traditional end-host-centric approaches, INCC facilitates collaborative congestion decision-making between end-hosts and in-network unit. INCC classifies congestion into two phases: “yellow” and “red” based on the local queue length bottleneck awareness and global congestion flow bottleneck statistics. For the “yellow” local congestion phrase, we design an in-network local control algorithm that performs proactive packet dropping and rate adjustment to mitigate emerging congestion. For the “red” global congestion phrase, we design an end-host and network cooperative global congestion control algorithm to make precise sending rate adaptation by proactive bottleneck awareness. We implement INCC via Linux kernel modifications and design three experiments to compare with Cubic, NewReno, and BBR. Experimental results demonstrate INCC has good performance on round-trip time and throughput, achieving 99.03% scheduling fairness in flow contention scenarios. Additionally, INCC has low execution overhead on CPU utilization and realize microsecond computational latency.}
}


@article{DBLP:journals/ton/XiaoFHWHWTZ25,
	author = {Fu Xiao and
                  Weibei Fan and
                  Xin He and
                  Junchang Wang and
                  Lei Han and
                  Xiaoliang Wang and
                  Chen Tian and
                  Xia Zhu},
	title = {Thunder: Minimum {I/O} Latency of Disaggregated Storage by Packet-Level
                  Write-Through},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3440--3454},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3587237},
	doi = {10.1109/TON.2025.3587237},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaoFHWHWTZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The state-of-the-art storage structure relies on the NVMe devices and SmartNICs to provide high IO performance and low CPU overhead. In data centers, the existing data transmission control and storage methods are not ideal, resulting in long flow completion time, especially for small IO, which directly affects the performance of disaggregated storage systems. In this paper, we present Thunder, a disaggregated storage solution designed to minimize tail latency. Firstly, Thunder achieves the minimum I/O tail latency for disaggregated storage via packet-level write-through, and has an ingenious mechanism for precise semantic conversion from message level to packet level. It refers to the process of converting message level data into packet level data and ensuring the integrity and reliability of data transmission. This process involves steps such as message segmentation, addressing, acknowledgment, and reassembly. Secondly, we present a novel optimization approach for end-to-end and information transmission processes, aiming to address a range of issues such as user usage, congestion control, and system compatibility. Finally, we conducted both testbed and large-scale simulations to verify the performance of Thunder. The results show that Thunder reduced the average latency and tail latency by 71.6% and 59.7%, respectively compared to Gimbal and Timely. Furthermore, it effectively avoids queue head blocking and congestion diffusion in PFC, increasing throughput by 2.5X and reducing tail latency by an average of 49.7%.}
}


@article{DBLP:journals/ton/SankagiriH25,
	author = {Suryanarayana Sankagiri and
                  Bruce E. Hajek},
	title = {Pricing for Routing and Flow-Control in Payment Channel Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3455--3470},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3588049},
	doi = {10.1109/TON.2025.3588049},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SankagiriH25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A payment channel network is a blockchain-based overlay mechanism that allows parties to transact more efficiently than directly using the blockchain. These networks are composed of payment channels that carry transactions between pairs of users. Due to its design, a payment channel cannot sustain a net flow of money in either direction indefinitely. Therefore, a payment channel network cannot serve transaction requests arbitrarily over a long period of time. We introduce DEBT control, a joint routing and flow-control protocol that guides a payment channel network towards an optimal operating state for any steady-state demand. In this protocol, each channel sets a price for routing transactions through it. Transacting users make flow-control and routing decisions by responding to these prices. A channel updates its price based on the net flow of money through it. We develop the protocol by formulating a network utility maximization problem and solving its dual through gradient descent. We provide convergence guarantees for the protocol and also illustrate its behavior through simulations.}
}


@article{DBLP:journals/ton/NguyenNTN25,
	author = {Duong Thuy Anh Nguyen and
                  Tarannum Nisha and
                  Ni Trieu and
                  Duong Tung Nguyen},
	title = {A Mixed-Integer Bi-Level Model for Joint Optimal Edge Resource Pricing
                  and Provisioning},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3471--3486},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3589342},
	doi = {10.1109/TON.2025.3589342},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NguyenNTN25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper studies the joint optimization of edge node activation and resource pricing in edge computing, where an edge computing platform provides heterogeneous resources to accommodate multiple services with diverse pReferences. We cast this problem as a bi-level program, with the platform acting as the leader and the services as the followers. The platform aims to maximize net profit by optimizing edge resource prices and edge node activation, with the services’ optimization problems acting as constraints. Based on the platform’s decisions, each service aims to minimize its costs and enhance user experience through optimal service placement and resource procurement decisions. The presence of integer variables in both the upper and lower-level problems renders this problem particularly challenging. Traditional techniques for transforming bi-level problems into single-level formulations are inappropriate owing to the non-convex nature of the follower problems. Drawing inspiration from the column-and-constraint generation method in robust optimization, we develop an efficient decomposition-based iterative algorithm to compute an exact optimal solution to the formulated bi-level problem. Extensive numerical results are presented to demonstrate the efficacy of the proposed model and technique.}
}


@article{DBLP:journals/ton/ChenWYHCSZ25,
	author = {Hongyang Chen and
                  Benran Wang and
                  Guangba Yu and
                  Zilong He and
                  Pengfei Chen and
                  Chen Sun and
                  Zibin Zheng},
	title = {NetScope: Fault Localization in Programmable Networking Systems With
                  Low-Cost In-Band Network Telemetry and In-Network Detection},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3487--3503},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3590034},
	doi = {10.1109/TON.2025.3590034},
	timestamp = {Sun, 01 Feb 2026 13:44:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenWYHCSZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, Software Defined Networking (SDN) has gained widespread adoption as a network infrastructure. Although the openness and programmability of SDN facilitate large complex network construction, diagnosing faults in datacenter-scale network remains challenging. Previous network diagnosis tools pose significant overhead in fine-grained telemetry and typically lack automated fine-grained fault diagnosis capabilities. Although on-demand monitoring methods have been proposed to reduce telemetry overhead, they struggle with effectively setting fixed thresholds, which requires expert experience. This paper presents NetScope, a lightweight system for real-time anomaly detection with self-adaptive thresholds and automatic root cause localization in programmable networking systems. NetScope estimates latency medians for each Flow (i.e., a pair of source and sink switches) within the switch using the proposed per-Flow quantile sketch and calculates the threshold accordingly for anomaly detection. Upon detecting anomalies, NetScope collects aggregated packet-level telemetry on demand and generates a ranked list of fine-grained fault culprits at multiple levels, including port-level, Flow-level, and switch-level. Extensive experiments demonstrate the effectiveness and efficiency of NetScope in anomaly detection and fault localization. Specifically, NetScope achieves a 32%~116% relative improvement in anomaly detection and 6%~197% improvement in root cause analysis compared with other baselines without causing any network bandwidth in anomaly detection while consuming 64.2% less telemetry bandwidth for localization.}
}


@article{DBLP:journals/ton/WangTWCZZ25,
	author = {Zhaohui Wang and
                  Ye Tian and
                  Yiwen Wu and
                  Wei Chen and
                  Xinyu Zhang and
                  Xinming Zhang},
	title = {PS-Sketch: Fast and Accurate Detection of Persistent-Spreaders in
                  High-Speed Networks},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3504--3519},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3589279},
	doi = {10.1109/TON.2025.3589279},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangTWCZZ25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Large spread and high persistence are widely observed in malicious activities such as botnets and DDoS attacks in high-speed networks, while how to identify persistent-spreaders is still a challenging issue. In this work, we present PS-Sketch, a system for estimating persistent-spreads of network flows and detecting persistent-spreaders in network data streams. PS-Sketch is based on our definitions of persistence and persistent-spread, which overcome the limitations of the conventional definitions by better capturing network flows’ behaviors, and being difficult for attackers to bypass. Within a switch’s pipeline, PS-Sketch processes packets with two adjacent sketch data structures, namely the P-sketch and the S-sketch. In the P-sketch, we employ low-pass filter (LPF) to trace an element’s persistence that incorporates occurrences in its entire history, and in the S-sketch, we extend the HyperLogLog (HLL) algorithm, and integrate an element’s persistence to the spread estimation of the flow that the element belongs to, for estimating the flow’s persistent-spread. We present theoretical analysis on the error bound of PS-Sketch. Trace-driven evaluation shows that PS-Sketch achieves high accuracy in estimating network flows’ persistent-spreads, and outperforms the existing solutions in detecting persistent-spreaders. We further prototype PS-Sketch in P4 and show that the system is deployable on commodity hardware switches.}
}


@article{DBLP:journals/ton/LiYLGWPWC25,
	author = {Fuliang Li and
                  Qianchen Yuan and
                  Yuhua Lai and
                  Zhenbei Guo and
                  Elliott Wen and
                  Tian Pan and
                  Xingwei Wang and
                  Jiannong Cao},
	title = {INT-Source: Topology-Adaptive In-Band Network-Wide Telemetry},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3520--3534},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3589194},
	doi = {10.1109/TON.2025.3589194},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYLGWPWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In-band Network Telemetry (INT) technology enables fine-grained network monitoring by encapsulating intra-switch network status into INT probes, which is essential in data center networks for ensuring Quality of Service (QoS). Existing INT-based telemetry systems leverage centralized controllers to compute non-overlapping probe paths, thereby facilitating lightweight and network-wide measurements. However, these systems fail to adapt effectively to network topology changes caused by link or device failures, primarily due to inflexible path planning under dynamic conditions. To address this problem, we propose INT-Source, a unified policy-based network-wide telemetry system for probing and forwarding. First, we design a data plane forwarding mechanism for INT probes to ensure telemetry coverage during topology changes and reduce telemetry overhead. Second, we design a probe packet structure and introduce a switch-based probe verification and discard mechanism to prevent redundant link probing. Third, we introduce two algorithms for INT-Source: a Single-Source algorithm to facilitate deployment and a Multi-Source algorithm to enable lightweight and scalable telemetry. Our evaluation shows that INT-Source reduces bandwidth overhead to 12.5% compared to existing methods across three network topologies. Even with a 10% link failure rate, INT-Source is able to monitor 93.1% of network ports, demonstrating strong robustness.}
}


@article{DBLP:journals/ton/LiuLHLZHDTJWC25,
	author = {Jingling Liu and
                  Hui Li and
                  Jiawei Huang and
                  Zhaoyi Li and
                  Ping Zhong and
                  Boyan Huang and
                  Pingping Dong and
                  Wensheng Tang and
                  Wanchun Jiang and
                  Jianxin Wang and
                  Yong Cui},
	title = {Automatic Dual Threshold Tuning for Switch Buffer Sharing in Datacenter
                  Networking},
	journal = {{IEEE} Trans. Netw.},
	volume = {33},
	number = {6},
	pages = {3535--3550},
	year = {2025},
	url = {https://doi.org/10.1109/TON.2025.3590965},
	doi = {10.1109/TON.2025.3590965},
	timestamp = {Sat, 17 Jan 2026 10:42:16 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuLHLZHDTJWC25.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {For the widely deployed on-chip shared buffer, efficient buffer management is the key to absorbing bursts and avoiding packet loss during transient congestion. However, as the buffer-per-port-per-Gbps in production data centers decreases, it becomes more challenging to provide efficient buffer management to meet the requirements of heterogeneous traffic. We observe that typical shared buffer management policies have two steps: first, they identify short flows arriving at ports and then allocate more buffer room for these ports. Unfortunately, the lack of isolation between long and short flows leads to increased queue buildup and even packet loss of short flows. To address this limitation, we propose D2T, which uses different queue length thresholds for long and short flows. Specifically, we first design a compact data structure to distinguish between long and short flows. Then when two kinds of flows coexist at the same port, the threshold of long flows will decrease to absorb the bursty short flows. What’s more, we introduce D2T  {}^{*} {}^{*}   which combines D2T with advanced DRL techniques to move toward mastering buffer management for further improving performance across various scenarios. We implement D2T at a P4-programmable switch and large-scale simulations. The results demonstrate that D2T reduces both average and tail flow completion times (FCT) of short flows by up to 29% and 62% compared with the state-of-the-art policies, respectively.}
}
