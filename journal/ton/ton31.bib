@article{DBLP:journals/ton/LiGDC23,
	author = {Huikang Li and
                  Yi Gao and
                  Wei Dong and
                  Chun Chen},
	title = {Bound-Based Network Tomography for Inferring Interesting Path Metrics},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {1--14},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3180631},
	doi = {10.1109/TNET.2022.3180631},
	timestamp = {Thu, 02 Mar 2023 10:53:05 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiGDC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the “network-as-a-service” paradigm, network operators have a strong need to know the performance of critical paths running services to the users. Network tomography is an attractive methodology for inferring internal network characteristics from end-to-end measurements between monitors. Motivated by previous results that uniquely identifying the path metrics can require a large number of monitors, we focus on calculating the performance bounds of a set of interesting paths, i.e., bound-based network tomography for interesting paths. We present an efficient solution to obtain the tightest upper and lower bounds of all interesting paths in an arbitrary network with a given set of end-to-end measurements. Based on this solution, we further develop an algorithm to place new monitors over existing ones such that the bounds of interesting paths can be maximally tightened. We formally prove the effectiveness of the proposed algorithms. We implement the algorithms and conduct extensive experiments on real ISP topologies. Compared with state-of-the-art approaches, our algorithms achieve up to\n1.2∼1.9\ntimes more reduction on the bound interval lengths of all interesting paths and use up to 50.4%~62.5% fewer monitors in various network settings.}
}


@article{DBLP:journals/ton/RuzomberkaL23,
	author = {Eric Ruzomberka and
                  David J. Love},
	title = {Interference Moral Hazard in Large Multihop Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {15--29},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3186234},
	doi = {10.1109/TNET.2022.3186234},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/RuzomberkaL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Cooperation between network nodes is critical for supporting services in ad hoc networks. Cooperation, however, is an idealized assumption that may not always be present. This assumption can fail because of moral hazard, a scenario in part caused by misaligned incentives between the requesting node and supporting node. In this paper, we characterize a moral hazard that perversely incentivizes nodes to increase their routing payments by transmitting interference into the multi-hop network. We refer to this as the interference moral hazard (IMH) problem which is inherent to strategyproof mechanisms with low overpayments. We investigate IMH as a non-cooperative game played by network nodes on a random graph. For large networks, we show that IMH can be solved in the network design space. We provide sufficient conditions on the network distribution that guarantee an equilibrium path with interference-free play. This is achieved by 1) lower-bounding the number of nodes and 2) bounding the network density slightly above the 2-connectedness threshold and below a proposed upper-bound. Simulations suggest that density plays a fundamental role in IMH.}
}


@article{DBLP:journals/ton/SabnisSNGLS23,
	author = {Anirudh Sabnis and
                  Tareq Si Salem and
                  Giovanni Neglia and
                  Michele Garetto and
                  Emilio Leonardi and
                  Ramesh K. Sitaraman},
	title = {{GRADES:} Gradient Descent for Similarity Caching},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {30--41},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3187044},
	doi = {10.1109/TNET.2022.3187044},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SabnisSNGLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A similarity cache can reply to a query for an object with similar objects stored locally. In some applications of similarity caches, queries and objects are naturally represented as points in a continuous space. This is for example the case of 360° videos where user’s head orientation—expressed in spherical coordinates—determines what part of the video needs to be retrieved, or of recommendation systems where a metric learning technique is used to embed the objects in a finite dimensional space with an opportune distance to capture content dissimilarity. Existing similarity caching policies are simple modifications of classic policies like LRU, LFU, and\nq\nLRU and ignore the continuous nature of the space where objects are embedded. In this paper, we propose GRADES, a new similarity caching policy that uses gradient descent to navigate the continuous space and find appropriate objects to store in the cache. We provide theoretical convergence guarantees and show GRADES increases the similarity of the objects served by the cache in both applications mentioned above.}
}


@article{DBLP:journals/ton/ChenXKXL23,
	author = {Xi Chen and
                  Qiao Xiang and
                  Linghe Kong and
                  Huisan Xu and
                  Xue Liu},
	title = {Learning From {FM} Communications: Toward Accurate, Efficient, All-Terrain
                  Vehicle Localization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {42--57},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3187885},
	doi = {10.1109/TNET.2022.3187885},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenXKXL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Vehicle localization service is a fundamental component of intelligent transportation systems. The widely used satellite navigation systems perform poorly in urban areas because the lines of sight to satellites are blocked by complex terrain characteristics, e.g., buildings, elevated streets and interchanges. In this paper, we design RadioLoc, a novel system achieving accurate, efficient, all-terrain vehicle localization with two key design points. First, RadioLoc harvests the frequency modulation (FM) signal, which has higher availability than satellite signal in complex terrains, as the signal source for localization. Second, RadioLoc integrates modern machine learning techniques into the processing of FM signals to efficiently learn the accurate vehicle localization in all-terrain environments. We validate the feasibility of FM-based vehicle localization and corresponding challenges and practical issues via field tests (e.g., signal distortion, signal inconsistency and limited in- vehicle radio bandwidth), and develop a series of advanced techniques in RadioLoc to address them, including adaptive batching, frequency sweeping, a novel multipath delay spread filter, a reconstructive PCA denoiser and a tailored FM feature extractor. We then develop a generic, modular localization module in RadioLoc, and design different learning-based 3D position identification algorithms for this module. We implement a prototype of RadioLoc and perform extensive field experiments to evaluate its efficiency and efficacy. Results show that (1) RadioLoc achieves a real-time localization latency of less than 100 milliseconds; (2) RadioLoc achieves a worst-case localization accuracy of 99.6% even in an underground parking lot, and (3) the horizontal error of RadioLoc is only one sixth of a dedicated GPS device even when the vehicle is moving at a high-speed (i.e., 80 km/h) in a complex highway scenario.}
}


@article{DBLP:journals/ton/WangLS23,
	author = {Haoyu Wang and
                  Zetian Liu and
                  Haiying Shen},
	title = {Machine Learning Feature Based Job Scheduling for Distributed Machine
                  Learning Clusters},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {58--73},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3190797},
	doi = {10.1109/TNET.2022.3190797},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid proliferation of Machine Learning (ML) and Deep learning (DL) applications running on modern platforms, it is crucial to satisfy application performance requirements such as meeting deadline and ensuring accuracy. To this end, researchers have proposed several job schedulers for ML clusters. However, none of the previously proposed schedulers consider ML model parallelism, though it has been proposed as an approach to increase the efficiency of running large-scale ML and DL jobs. Thus, in this paper, we propose an ML job Feature based job Scheduling system (MLFS) for ML clusters running both data parallelism and model parallelism ML jobs. MLFS first uses a heuristic scheduling method that considers an ML job’s spatial and temporal features to determine task priority for job queue ordering in order to improve job completion time (JCT) and accuracy performance. It uses the data from the heuristic scheduling method for training a deep reinforcement learning (RL) model. After the RL model is well trained, it then switches to the RL method to automatically make decisions on job scheduling. In addition, MLFS has a system load control method that selects tasks from overloaded servers to move to underloaded servers based on task priority, and also intelligently removes the tasks that generate little or no improvement on the desired accuracy performance when the system is overloaded to improve JCT and accuracy by job deadline. Furthermore, we propose Optimal ML iteration stopping method that determines the proper time to stop training ML model when this model reaches the minimum loss value. Our real experiments and large-scale simulation based on real trace show that MLFS reduces JCT by up to 53% and makespan by up to 52%, and improves accuracy by up to 64% when compared with existing ML job schedulers. We also open sourced our code.}
}


@article{DBLP:journals/ton/ZhouKEN23,
	author = {Xujin Zhou and
                  Irem Koprulu and
                  Atilla Eryilmaz and
                  Michael J. Neely},
	title = {Efficient Distributed {MAC} for Dynamic Demands: Congestion and Age
                  Based Designs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {74--87},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3191607},
	doi = {10.1109/TNET.2022.3191607},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhouKEN23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future generation wireless technologies are expected to serve an increasingly dense and dynamic population of users that generate short bundles of information to be transferred over the shared spectrum. This calls for new distributed and low-overhead Multiple-Access-Control (MAC) strategies to serve such dynamic demands with spectral efficiency characteristics. In this work, we address this need by identifying and developing two fundamentally different MAC paradigms: (i) congestion-based paradigm that estimates the congestion level in the system and adapts to it; and (ii) age-based paradigm that prioritizes demands based on their ages. Despite their apparent differences, we develop policies under each paradigm in a generic multi-channel access scenario that are provably throughput-optimal when they employ any asymptotically-efficient channel encoding/decoding mechanism. We also characterize the stability regions of the two designs, and investigate the conditions under which one design outperforms the other. We perform extensive simulations to validate the theoretical claims and investigate the non-asymptotic performances of our designs.}
}


@article{DBLP:journals/ton/MasonNZ23,
	author = {Federico Mason and
                  Gianfranco Nencioni and
                  Andrea Zanella},
	title = {Using Distributed Reinforcement Learning for Resource Orchestration
                  in a Network Slicing Scenario},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {88--102},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3187310},
	doi = {10.1109/TNET.2022.3187310},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MasonNZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Network Slicing (NS) paradigm enables the partition of physical and virtual resources among multiple logical networks, possibly managed by different tenants. In such a scenario, network resources need to be dynamically allocated according to the slice requirements. In this paper, we attack the above problem by exploiting a Deep Reinforcement Learning approach. Our framework is based on a distributed architecture, where multiple agents cooperate towards a common goal. The agent training is carried out following the Advantage Actor Critic algorithm, which permits to handle continuous action spaces. By means of extensive simulations, we show that our approach yields better performance than both a static allocation of system resources and an efficient empirical strategy. At the same time, the proposed system ensures high adaptability to different scenarios without the need for additional training.}
}


@article{DBLP:journals/ton/ZhangZWTPH23,
	author = {Jiao Zhang and
                  Xiaolong Zhong and
                  Zirui Wan and
                  Yu Tian and
                  Tian Pan and
                  Tao Huang},
	title = {{RCC:} Enabling Receiver-Driven {RDMA} Congestion Control With Congestion
                  Divide-and-Conquer in Datacenter Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {103--117},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3185105},
	doi = {10.1109/TNET.2022.3185105},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZWTPH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The development of datacenter applications leads to the need for end-to-end communication with microsecond latency. As a result, RDMA is becoming prevalent in datacenter networks to mitigate the latency caused by the slow processing speed of the traditional software network stack. However, existing RDMA congestion control mechanisms are either far from optimal in simultaneously achieving high throughput and low latency or in need of additional in-network function support. In this paper, by leveraging the observation that most congestion occurs at the last hop in datacenter networks, we propose RCC, a receiver-driven rapid congestion control mechanism for RDMA networks that combines explicit assignment and iterative window adjustment. Firstly, we propose a network congestion distinguish method to classify congestions into two types, last-hop congestion and in-network congestion. Then, an Explicit Window Assignment mechanism is proposed to solve the last-hop congestion, which enables senders to converge to a proper sending rate in one-RTT. For in-network congestion, a PID-based iterative delay-based window adjustment scheme is proposed to achieve fast convergence and near-zero queuing latency. RCC does not need additional in-network support and is friendly to hardware implementation. In our evaluation, the overall average FCT (Flow Completion Time) of RCC is\n4∼79%\nbetter than Homa, ExpressPass, DCQCN, TIMELY, and HPCC.}
}


@article{DBLP:journals/ton/ZhuZLM23,
	author = {Shaopeng Zhu and
                  Xiaolong Zheng and
                  Liang Liu and
                  Huadong Ma},
	title = {{CSMA/PJ:} {A} Protective Jamming Based {MAC} Protocol to Harmonize
                  the Long and Short Links},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {118--132},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193027},
	doi = {10.1109/TNET.2022.3193027},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhuZLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {WiFi-based Long Distance (WiLD) networks are promising to cover the rural and remote regions. But the explosive short-range WiFi deployments result in the long-short coexistence. Due to CSMA is ignorance of propagation delay, its carrier sensing is too short to detect long links, leading to the temporal hidden terminal problem that causes serious performance degradation and even starvation of long links. Existing methods for traditional hidden terminal problem are inefficient to cope with this problem because of the different causes. In this paper, we propose CSMA with Protective Jamming (CSMA/PJ), a new WiLD MAC protocol that solves the temporal hidden terminal problem with the minimized influence on uncontrollable short links. The key is generating protective jamming at the WiLD receiver that is sensible to the short links. By leveraging the asymmetric propagation delay of the WiLD transmitter and receiver, we make the jamming protective rather than destructive. We precisely control the jamming right before the arrivals of WiLD packets to set aside channel time for short links. We implement and evaluate CSMA/PJ on commercial devices. The experimental results show that CSMA/PJ can improve the throughput of the WiLD link by\n6×\nand\n5×\ncompared with the CSMA/CA and RTS/CTS methods.}
}


@article{DBLP:journals/ton/OikonomouKAS23,
	author = {Konstantinos Oikonomou and
                  George Koufoudakis and
                  Sonia A{\"{\i}}ssa and
                  Ioannis Stavrakakis},
	title = {Probabilistic Flooding Performance Analysis Exploiting Graph Spectra
                  Properties},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {133--146},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3192310},
	doi = {10.1109/TNET.2022.3192310},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/OikonomouKAS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Probabilistic flooding is an efficient information dissemination policy capable of spreading information to the network nodes by sending information messages according to a fixed forwarding probability in a per-hop manner starting from an initiator node. It is a suitable approach, especially in topologies where the number of information messages sent under traditional approaches is significantly increased. The analysis presented in this paper considers graph spectra properties such as the largest eigenvalue\nλ\n1\nof the adjacency matrix, and the eigenvector centrality. Both are analytically investigated and\n4\nλ\n1\nis derived as a lower bound of the forwarding probability that allows for global coverage, i.e., all network nodes receive the information message, under certain conditions also investigated here (e.g., the condition of the binomial approximation). It is shown that for any value of the forwarding probability equal to or larger than\n4\nλ\n1\n: (i) coverage is proportional to the initiator node’s eigenvector centrality; (ii) the probability a node receives the information message is proportional to the node’s eigenvector centrality; (iii) termination time decreases as the initiator node’s eigenvector centrality increases. If knowledge of\nλ\n1\nis not available, then the average node degree\nd\n¯\ncan be used for ensuring global coverage. If knowledge of both\nλ\n1\nand\nd\n¯\nis not available, a dissemination policy is proposed that forwards messages to\nm\n(randomly selected) neighbor nodes. It is analytically shown that any value of\nm≥4\nallows for global coverage. Simulation results demonstrate the effectiveness of the considered analytical approach and the introduced policy.}
}


@article{DBLP:journals/ton/WangDLBA23,
	author = {Juncheng Wang and
                  Min Dong and
                  Ben Liang and
                  Gary Boudreau and
                  Hatem Abou{-}Zeid},
	title = {Delay-Tolerant {OCO} With Long-Term Constraints: Algorithm and Its
                  Application to Network Resource Allocation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {147--163},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3188285},
	doi = {10.1109/TNET.2022.3188285},
	timestamp = {Mon, 20 Nov 2023 13:58:37 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangDLBA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider online convex optimization (OCO) with multi-slot feedback delay. An agent selects a sequence of online decisions to minimize the accumulation of time-varying convex loss functions, subject to short-term and long-term constraints that may be time-varying. Both the convex loss function and the long-term constraint function may experience multiple time slots of feedback delay to be received by the agent. Existing works on OCO under this general setting has focused on the static regret, which measures the gap of losses between an online decision sequence and a time-invariant static offline benchmark. In this work, besides the static regret, we also consider a more practically meaningful metric, the dynamic regret, where the benchmark is a time-varying online optimal decision sequence. We propose an efficient algorithm, termed Delay-Tolerant Constrained-OCO (DTC-OCO), which uses a novel double regularization together with a new penalty mechanism on the long-term constraint violation, to tackle the asynchrony between information feedback and decision updates. We obtain upper bounds for its static regret, dynamic regret, and constraint violation, proving that they are sublinear under mild conditions. Furthermore, we consider a variation of DTC-OCO with multi-step gradient descent, and show it provides improved dynamic regret and constraint violation bounds for strongly convex loss functions. For numerical demonstration, we apply DTC-OCO to a general network resource allocation problem. Our simulation results suggest substantial performance gain by DTC-OCO over the current best alternative.}
}


@article{DBLP:journals/ton/YunWZZZ23,
	author = {Xiaochun Yun and
                  Yipeng Wang and
                  Yongzheng Zhang and
                  Chen Zhao and
                  Zijian Zhao},
	title = {Encrypted {TLS} Traffic Classification on Cloud Platforms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {164--177},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3191312},
	doi = {10.1109/TNET.2022.3191312},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YunWZZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, encryption technology has been widely used to protect user privacy. With the explosive growth of mobile Internet, encrypted TLS traffic rises sharply and occupies a great share of current Internet traffic. In reality, the classification of encrypted TLS traffic on cloud platforms brings a new challenge to traditional encrypted traffic classification methods, because some information such as certificates in the TLS flows is no longer effective. In this paper, we apply deep learning technology to the problem of encrypted TLS traffic classification on cloud platforms, and propose NeuTic, which takes the packet sequence of each TLS flow as the input, and effectively classifies raw TLS flows generated by many “cloud” applications. Our approach is able to automatically capture the long-range dependencies between elements in the packet sequences for robust and accurate encrypted TLS traffic classification. In NeuTic, we first convert each TLS flow into three attribute sequences. Then, we train a multi-application traffic classification model using our newly designed deep learning model. Finally, we use the well-trained classification model to classify new incoming TLS flows. We conduct comprehensive experiments on real-world application traces covering multiple “cloud” applications from three different companies. In addition, we compare our experimental results of NeuTic with two deep learning-based methods for encrypted traffic classification. NeuTic outperforms the state-of-the-art approaches in classification accuracy.}
}


@article{DBLP:journals/ton/LuoFXY23,
	author = {Shouxi Luo and
                  Pingzhi Fan and
                  Huanlai Xing and
                  Hongfang Yu},
	title = {Meeting Coflow Deadlines in Data Center Networks With Policy-Based
                  Selective Completion},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {178--191},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3187821},
	doi = {10.1109/TNET.2022.3187821},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoFXY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recently, the abstraction of coflow is introduced to capture the collective data transmission patterns among modern distributed data-parallel applications. During processing, coflows generally act as barriers; accordingly, time-sensitive applications prefer their coflows to complete within deadlines, and deadline-aware coflow scheduling becomes very crucial. Regarding these data-parallel applications, we notice that many of them, including large-scale query systems , distributed iterative training , and erasure codes enabled storage , are able to tolerate loss-bounded incomplete inputs by design. This tolerance indeed brings a flexible design space for the schedule of their coflows: when getting overloaded, the network can trade coflow completeness for the timeliness, and balance the completeness of different coflows on demand. Unfortunately, existing coflow schedulers neglect this tolerance, resulting in inflexible and inefficient bandwidth allocations. In this paper, we explore this fundamental trade-off and design POCO, a POlicy-based COflow scheduler, along with a transport layer enhancement scheme, to achieve customizable selective coflow completion for emerging time-sensitive distributed applications. Internally, POCO employs a suite of novel designs along with admission controls to make flexible , work-conserving , and performance-guaranteed rate allocation to online coflow requests very efficiently. Extensive trace-based simulations indicate that POCO is highly flexible and achieves optimal coflow schedules respecting the requirements specified by applications.}
}


@article{DBLP:journals/ton/ZhangLLZBQL23,
	author = {Zhehui Zhang and
                  Yuanjie Li and
                  Qianru Li and
                  Jinghao Zhao and
                  Ghufran Baig and
                  Lili Qiu and
                  Songwu Lu},
	title = {Movement-Based Reliable Mobility Management for Beyond 5G Cellular
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {192--207},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3190788},
	doi = {10.1109/TNET.2022.3190788},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLLZBQL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Extreme mobility becomes a norm rather than an exception with emergent high-speed rails, drones, industrial IoT, and many more. However, 4G/5G mobility management is not always reliable in extreme mobility, with non-negligible failures and policy conflicts. The root cause is that, existing mobility management is primarily based on wireless signal strength. While reasonable in static and low mobility, it is vulnerable to dramatic wireless dynamics from extreme mobility in triggering, decision, and execution. We devise REM , Reliable Extreme Mobility management for beyond 5G cellular networks while maintaining backward compatibility to 4G/5G. REM shifts to movement-based mobility management in the delay-Doppler domain. Its signaling overlay relaxes feedback via cross-band estimation, simplifies policies with provable conflict freedom, and stabilizes signaling via scheduling-based OTFS modulation. Our evaluation with operational high-speed rail datasets shows that, REM reduces failures comparable to static and low mobility, with low signaling and latency cost. REM reduces the network failures by up to an order of magnitude, eliminates policy conflicts, and improves application performance by 31.8% - 88.3% compared to legacy 4G/5G.}
}


@article{DBLP:journals/ton/ZongLLLCL23,
	author = {Tongyu Zong and
                  Chen Li and
                  Yuanyuan Lei and
                  Guangyu Li and
                  Houwei Cao and
                  Yong Liu},
	title = {Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity With
                  Ensemble Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {208--219},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193680},
	doi = {10.1109/TNET.2022.3193680},
	timestamp = {Sun, 06 Aug 2023 20:51:08 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZongLLLCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge caching will play a critical role in facilitating the emerging content-rich applications. However, it faces many new challenges, in particular, the highly dynamic content popularity and the heterogeneous caching configurations. In this paper, we propose Cocktail Edge Caching, that tackles the dynamic popularity and heterogeneity through ensemble learning. Instead of trying to find a single dominating caching policy for all the caching scenarios, we employ an ensemble of constituent caching policies and adaptively select the best-performing policy to control the cache. Towards this goal, we first show through formal analysis and experiments that different variations of the LFU and LRU policies have complementary performance in different caching scenarios. We further develop a novel caching algorithm that enhances LFU/LRU with deep recurrent neural network (LSTM) based time-series analysis. Finally, we develop a deep reinforcement learning agent that adaptively combines base caching policies according to their virtual hit ratios on parallel virtual caches. Through extensive experiments driven by real content requests from two large video streaming platforms, we demonstrate that CEC not only consistently outperforms all single policies, but also improves the robustness of them. CEC can be well generalized to different caching scenarios with low computation overheads for deployment.}
}


@article{DBLP:journals/ton/ChenZTX23,
	author = {Cao Chen and
                  Fen Zhou and
                  Massimo Tornatore and
                  Shilin Xiao},
	title = {Maximizing Revenue With Adaptive Modulation and Multiple FECs in Flexible
                  Optical Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {220--233},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3194982},
	doi = {10.1109/TNET.2022.3194982},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenZTX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flexible optical networks (FONs) are being adopted to accommodate the increasingly heterogeneous traffic in today’s Internet. However, in presence of high traffic load, not all offered traffic can be satisfied at all time. As carried traffic load brings revenues to operators, traffic blocking due to limited spectrum resource leads to revenue losses. In this study, given a set of traffic requests to be provisioned, we consider the problem of maximizing operator’s revenue, subject to limited spectrum resource and physical layer impairments (PLIs), namely amplified spontaneous emission noise (ASE), self-channel interference (SCI), cross-channel interference (XCI), and node crosstalk. In FONs, adaptive modulation, multiple FEC, and the tuning of power spectrum density (PSD) can be effectively employed to mitigate the impact of PLIs. Hence, in our study, we propose a universal bandwidth-related impairment evaluation model based on channel bandwidth, which allows a performance analysis for different PSD, FEC and modulations. Leveraging this PLI model and a piecewise linear fitting function, we succeed to formulate the revenue maximization problem as a mixed integer linear program. Then, to solve the problem on larger network instances, a fast two-phase heuristic algorithm is also proposed, which is shown to be near-optimal for revenue maximization. Through simulations, we demonstrate that using adaptive modulation enables to significantly increase revenues in the scenario of high signal-to-noise ratio (SNR), where the revenue can even be doubled for high traffic load, while using multiple FECs is more profitable for scenarios with low SNR.}
}


@article{DBLP:journals/ton/ChangLCL23,
	author = {Chia{-}Ming Chang and
                  Yi{-}Jheng Lin and
                  Cheng{-}Shang Chang and
                  Duan{-}Shin Lee},
	title = {On the Stability Regions of Coded Poisson Receivers With Multiple
                  Classes of Users and Receivers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {234--247},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3188757},
	doi = {10.1109/TNET.2022.3188757},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChangLCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Motivated by the need to provide differentiated quality-of-service (QoS) in grant-free uplink transmissions in 5G networks and beyond, we extend the probabilistic analysis of coded Poisson receivers (CPR) to the setting with multiple classes of users and receivers. For such a CPR system, we prove (under certain technical conditions) that there is a region, called the stability region in this paper. Each transmitted packet can be successfully received with probability 1 when the offered load to the system is within the stability region. On the other hand, if the offered load is outside the stability region, there is a nonzero probability that a packet will fail to be received. We then extend the stability region to the\nϵ\n-stability region for CPR systems with decoding errors. We also demonstrate the capability of providing differentiated QoS in such CPR systems by comparing the stability regions under various parameter settings.}
}


@article{DBLP:journals/ton/ZhaoLXCX23,
	author = {Gongming Zhao and
                  Luyao Luo and
                  Hongli Xu and
                  Chun{-}Jen Chung and
                  Liguang Xie},
	title = {Southbound Message Delivery With Virtual Network Topology Awareness
                  in Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {248--263},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3190730},
	doi = {10.1109/TNET.2022.3190730},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoLXCX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Southbound message delivery from the control plane to the data plane is one of the essential issues in multi-tenant clouds. A natural method of southbound message delivery is that the control plane directly communicates with compute nodes in the data plane. However, due to the large number of compute nodes, this method may result in massive control overhead. The Message Queue (MQ) model can solve this challenge by aggregating and distributing messages to queues. Existing MQ-based solutions often perform message aggregation based on the physical network topology, which do not align with the fundamental requirements of southbound message delivery, leading to high message redundancy on compute nodes. To address this issue, we design and implement VITA, the first-of-its-kind work on virtual network topology-aware southbound message delivery. However, it is intractable to optimally deliver southbound messages according to the virtual attributes of messages. Thus, we design two algorithms, submodular-based approximation algorithm and simulated annealing-based algorithm, to solve different scenarios of the problem. Both experiment and simulation results show that VITA can reduce the total traffic amount of redundant messages by 45%-75% and reduce the control overhead by 33%-80% compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/MalandrinoCMO23,
	author = {Francesco Malandrino and
                  Carla{-}Fabiana Chiasserini and
                  Nuria Molner and
                  Antonio de la Oliva},
	title = {Network Support for High-Performance Distributed Machine Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {264--278},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3189077},
	doi = {10.1109/TNET.2022.3189077},
	timestamp = {Sat, 25 Feb 2023 21:35:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MalandrinoCMO23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The traditional approach to distributed machine learning is to adapt learning algorithms to the network, e.g., reducing updates to curb overhead. Networks based on intelligent edge, instead, make it possible to follow the opposite approach, i.e., to define the logical network topology around the learning task to perform, so as to meet the desired learning performance. In this paper, we propose a system model that captures such aspects in the context of supervised machine learning, accounting for both learning nodes (that perform computations) and information nodes (that provide data). We then formulate the problem of selecting (i) which learning and information nodes should cooperate to complete the learning task, and (ii) the number of epochs to run, in order to minimize the learning cost while meeting the target prediction error and execution time. After proving important properties of the above problem, we devise an algorithm, named DoubleClimb, that can find a 1+1/| \\mathcal {I}|\n-competitive solution (with \\mathcal {I}\nbeing the set of information nodes), with cubic worst-case complexity. Our performance evaluation, leveraging a real-world network topology and considering both classification and regression tasks, also shows that DoubleClimb closely matches the optimum, outperforming state-of-the-art alternatives.}
}


@article{DBLP:journals/ton/XuWWYZXWA23,
	author = {Chao Xu and
                  Jessie Hui Wang and
                  Jilong Wang and
                  Tao Yu and
                  Yipeng Zhou and
                  Yuedong Xu and
                  Di Wu and
                  Changqing An},
	title = {Offloading Elastic Transfers to Opportunistic Vehicular Networks Based
                  on Imperfect Trajectory Prediction},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {279--293},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3189047},
	doi = {10.1109/TNET.2022.3189047},
	timestamp = {Wed, 01 Mar 2023 16:53:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XuWWYZXWA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Due to the high cost of cellular networks, vehicle users would like to offload elastic traffic through vehicular networks as much as possible. This demand prompts researchers to consider how to make the vehicular network system achieve better performance for requests coming online, such as maximizing throughput. The traffic in vehicular networks is transferred through opportunistic contacts between vehicles and infrastructures. When making scheduling decisions, the scheduler must be aware of vehicles’ future trajectories. Vehicles’ future trajectories are usually predicted by trajectory prediction algorithms when users are unwilling to report their future trips. Unfortunately, no trajectory prediction algorithm can be completely accurate, and these inaccurate prediction results will degrade the throughput achieved by scheduling algorithms. In this paper, we focus on reducing the negative impact of inaccurate predictions. Specifically, we measure two data-driven trajectory prediction algorithms that have been widely used for trajectory predictions and understand the characteristics of the accuracy of predicted contacts. Based on the enlightenment from the measurement, we design a system, i.e., i-Offload, to offload elastic traffic under imperfect trajectory predictions. The experimental results show that our system has good throughput and high scheduling efficiency even under imperfect trajectory predictions. Compared with existing scheduling algorithms, our method improves the throughput by about one time.}
}


@article{DBLP:journals/ton/SobrinhoF23,
	author = {Jo{\~{a}}o Luis Sobrinho and
                  Miguel Alves Ferreira},
	title = {From Non-Optimal Routing Protocols to Routing on Multiple Optimality
                  Criteria},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {294--307},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3191808},
	doi = {10.1109/TNET.2022.3191808},
	timestamp = {Sat, 11 Mar 2023 00:13:48 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SobrinhoF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {At a suitable level of abstraction, all that standard routing protocols do is iterate extension and election operations on path attributes. An extension operation composes the attribute of a path from those of a link and another path, while an election operation produces the most preferred attribute of a set of candidate attributes, given a total order on them. These protocols are guaranteed to compute optimal paths only if the extension operation and the total order are entwined by the algebraic property of isotonicity, which states that the relative preference between two attributes is not inverted when both are extended by a third attribute. We solve the problem of computing and routing on optimal paths with generality by recognizing that every total order contains a partial order for which isotonicity holds. Then, we design a partial-order vectoring protocol where every election operation produces a subset of attributes from a set of candidate attributes, rather than a single attribute as is the case with a standard vectoring protocol; the election operation is derived from the partial order, ensuring that no attribute of the set of candidate attributes is preferred to an attribute of the elected subset. Moreover, we show how partial-order vectoring protocols can be designed to allow routing on optimal paths concurrently for diverse optimality criteria. Our evaluation over publicly available network topologies and attributes, covering both intra- and inter-AS routing, evince that the sizes of elected subsets of attributes are surprisingly small and that the partial-order vectoring protocol converges fast, sometimes faster than a standard vectoring protocol operating in the absence of isotonicity.}
}


@article{DBLP:journals/ton/QinLY23,
	author = {Xudong Qin and
                  Bin Li and
                  Lei Ying},
	title = {Efficient Distributed Threshold-Based Offloading for Large-Scale Mobile
                  Cloud Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {308--321},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193073},
	doi = {10.1109/TNET.2022.3193073},
	timestamp = {Mon, 02 Oct 2023 15:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QinLY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile cloud computing enables compute-limited mobile devices to perform real-time intensive computations such as speech recognition or object detection by leveraging powerful cloud servers. An important problem in large-scale mobile cloud computing is computational offloading, where each mobile device decides when and how much computation should be uploaded to cloud servers by considering the local processing delay and the cost of using cloud servers. In this paper, we develop a distributed threshold-based offloading algorithm where it uploads an incoming computing task to cloud servers if the number of tasks queued at the device reaches the threshold and processes it locally otherwise. The threshold is updated iteratively based on the computational load and the cost of using cloud servers. We formulate the problem as a symmetric game, and characterize the sufficient and necessary conditions for the existence and uniqueness of the Nash Equilibrium (NE) assuming exponential service times. Then, we show the convergence of our proposed distributed algorithm to the NE when the NE exists. Further, we characterize the performance gap between cost under our proposed distributed algorithm and the minimum cost in terms of Price of Anarchy (PoA) when the cost of using cloud servers is high. Finally, we perform extensive simulations to validate our theoretical findings, demonstrate the efficiency of our proposed distributed algorithm under various scenarios such as hyperexponential service times, imperfect server utilization estimation, and asynchronous threshold updates, and reveal the superior performance of threshold-based policies over their probabilistic counterpart.}
}


@article{DBLP:journals/ton/MajidiGZJZC23,
	author = {Akbar Majidi and
                  Xiaofeng Gao and
                  Shunjia Zhu and
                  Nazila Jahanbakhsh and
                  Jiaqi Zheng and
                  Guihai Chen},
	title = {MiFi: Bounded Update to Optimize Network Performance in Software-Defined
                  Data Centers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {322--335},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3192167},
	doi = {10.1109/TNET.2022.3192167},
	timestamp = {Wed, 22 Nov 2023 12:10:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MajidiGZJZC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A controller needs to solve the multi-commodity flow problem and globally update the network under tight time constraints to maintain optimal network configurations. This centralized optimization in data centers involves many variables and constraints, which has a slow convergence speed and little scalability. In this paper, we propose MiFi, which aims to Minimize Flow cost or intuitively average transmission delay (delay or latency of flows), under reconfiguration budget constraints in data centers. Thus, we formulate this optimization problem as a constrained Markov Decision Process and propose a set of algorithms to solve it in a scalable manner. We first develop a propagation algorithm to identify the flows mostly affected in terms of latency and configuration in the next update. Then, we set a limitation range (the subset of switches requiring network updates) for updating them to improve adaptability and scalability by updating a less number of flows each time to achieve fast operations. Further, based on the Drift-Plus-Penalty method in Lyapunov theory, we propose a heuristic policy without prior information of flow demand and a renewal policy with a performance guarantee to minimize the additive optimality gap. To the best of our knowledge, MiFi is the first paper that studies the range and frequency of flow reconfigurations, which has both theoretical and practical significance in the area. Emulations and numerical simulations, which are much better than the estimated theoretical bound, show that MiFi outperforms the state of the art algorithms in terms of latency by over 45% while making improvements in adaptability and scalability.}
}


@article{DBLP:journals/ton/WangSCQDXZZH23,
	author = {Ge Wang and
                  Xiaofeng Shi and
                  Haofan Cai and
                  Chen Qian and
                  Han Ding and
                  Wei Xi and
                  Kun Zhao and
                  Jizhong Zhao and
                  Jinsong Han},
	title = {A Generalized Method to Combat Multipaths for {RFID} Sensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {336--351},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3190862},
	doi = {10.1109/TNET.2022.3190862},
	timestamp = {Mon, 28 Aug 2023 21:30:47 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangSCQDXZZH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {There have been increasing interests in exploring the sensing capabilities of RFID to enable numerous IoT applications, including object localization, trajectory tracking, and human behavior sensing. However, most existing methods rely on the signal measurement either in a low multipath environment, which is unlikely to exist in many practical situations, or with special devices, which increase the operating cost. This paper investigates the possibility of measuring ‘multi-path-free’ signal information in multipath-prevalent environments simply using a commodity RFID reader. The proposed solution, Clean Physical Information Extraction (CPIX), is universal, accurate, and compatible to standard protocols and devices. CPIX improves RFID sensing quality with near zero cost – it requires no extra device. We implement CPIX and study three major RFID sensing applications: tag localization, device calibration and human behavior sensing. CPIX reduces the localization error by 30% to 50% and achieves the MOST accurate localization by commodity readers compared to existing work. It also significantly improves the quality of device calibration and human behaviour sensing.}
}


@article{DBLP:journals/ton/ZhangTHHJLL23,
	author = {Chi Zhang and
                  Haisheng Tan and
                  Haoqiang Huang and
                  Zhenhua Han and
                  Shaofeng H.{-}C. Jiang and
                  Guopeng Li and
                  Xiangyang Li},
	title = {Online Approximation Scheme for Scheduling Heterogeneous Utility Jobs
                  in Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {352--365},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193381},
	doi = {10.1109/TNET.2022.3193381},
	timestamp = {Sat, 25 Feb 2023 21:35:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangTHHJLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing systems typically handle a wide variety of applications that exhibit diverse degrees of sensitivity to job latency. Therefore, a multitude of utility functions of the job response time need to be considered by the underlying job dispatching and scheduling mechanism. Nonetheless, previous studies in edge computing mainly focused on optimizing a single utility function across all jobs, e.g., linear, sigmoid, or the hard deadline. In this paper, we design online job dispatching and scheduling strategies in which different jobs can be categorized by different non-increasing utility functions. Our goal is to maximize the total utility of all scheduled jobs. We first prove that no online deterministic algorithm could achieve a competitive ratio better than the lower bound \\Omega \\left({\\frac {1}{\\sqrt {\\epsilon }}}\\right)\nunder the (1+\\epsilon)\n-speed augmentation model. We proceed to propose an online algorithm, named as O4A , for handling jobs with heterogeneous utilities. We prove that O4A is O\\left({\\frac {1}{\\epsilon ^{2}}}\\right)\n-competitive. We also design its distributed version, i.e., DO4A . We implement O4A and DO4A on an edge computing testbed running deep learning inference jobs. With the production trace from Google Cluster, our experimental and large-scale simulation results indicate that O4A can increase the total utility by up to 50% compared with state-of-the-art methods. Besides, the performance loss of DO4A is only 2% compared with O4A with a small communication overhead involved. Moreover, both of our algorithms are robust to estimation errors in job processing time and transmission delay.}
}


@article{DBLP:journals/ton/WuZLWLZ23,
	author = {Qiang Wu and
                  Xiangping Bryce Zhai and
                  Xi Liu and
                  Chunming Wu and
                  Fangliang Lou and
                  Hongke Zhang},
	title = {Performance Tuning via Lean Measurements for Acceleration of Network
                  Functions Virtualization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {366--379},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193686},
	doi = {10.1109/TNET.2022.3193686},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WuZLWLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network Functions Virtualization (NFV) replaces the specialized hardware with the software-based forwarding to promise the flexibility, scalability and automation benefits. With an increasing range of applications, NFV must ultimately forward packets at rates that are comparable to the native and specialized hardware-based approaches. However, the transition packet forwarding from specialized hardware to software-based has turned out to be more challenging than expected. Thus, NFV acceleration is desperately needed to play a crucial role in the development of NFV. It is an interesting issue how to address the persistent performance tuning in a way that provides far greater flexibility to meet the demands of power. The existing developments are very inefficient, since that the uncontrollable and unanticipated performance regressions frequently occur. Besides, the environments for full system simulations are traditionally expensive and time consuming to evaluate the system performance. In this paper, we propose the methodology named as “NFV Acceleration via Lean Measurements (NALM)” to tune the performance for the NFV acceleration. NALM provides a holistic measurement approach through combining individual measures to quickly identify the bottlenecks, which can help developers with a better understanding of the design tradeoffs. Moreover, the environments for large scale performance simulation are replaced by a debugger. Thus, the waste is eliminated in terms of time consumption and infrastructure costs of the full system simulation. The systematic analysis of the multi-cores speedup ratio highlights the potential optimization space and rules. We further propose the improvement recommendations on efficient practices. The experiments evaluate the specific effects, and the relationship between the metrics and forwarding performance.}
}


@article{DBLP:journals/ton/HaoLZY23,
	author = {Yijun Hao and
                  Fang Li and
                  Cong Zhao and
                  Shusen Yang},
	title = {Delay-Oriented Scheduling in 5G Downlink Wireless Networks Based on
                  Reinforcement Learning With Partial Observations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {380--394},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3194953},
	doi = {10.1109/TNET.2022.3194953},
	timestamp = {Sat, 25 Feb 2023 21:35:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HaoLZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G wireless networks are expected to satisfy different delay requirements of various traffics by network resource scheduling. Existing scheduling methods perform poorly in practice due to their unrealistic assumption on the access to the full channel state information (CSI) or the explicit mathematical expression of network delay. In this paper, we consider the delay-oriented packet scheduling problem in multi-cell 5G downlink networks with multiple users and traffic types (e.g., FTP, VoIP and video streaming), and formulate it as a partially observable Markov decision process (POMDP). We design a delay-oriented downlink scheduling framework based on deep reinforcement learning (DRL) to autonomously schedule the active traffic flows without the full channel information. Furthermore, a recurrent proximal policy optimization (RPPO) algorithm is proposed to perceive the underlying state and accelerate learning under different time granularities, with the policy gradient theorem under POMDP strictly proved. By incorporating the future traffic information provided by a proposed spatial-temporal prediction algorithm, RPPO can balance the load and achieve lower delay in real-time multi-cell multi-user scenarios. Results of extensive experiments on a realistic 5G simulator demonstrate that our framework significantly outperforms existing approaches in terms of both tail delay and average delay for up to 48% and 41.7%, respectively.}
}


@article{DBLP:journals/ton/XiBMZRR23,
	author = {Shaoke Xi and
                  Kai Bu and
                  Wensen Mao and
                  Xiaoyu Zhang and
                  Kui Ren and
                  Xinxin Ren},
	title = {RuleOut Forwarding Anomalies for {SDN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {395--407},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3194970},
	doi = {10.1109/TNET.2022.3194970},
	timestamp = {Sat, 25 Feb 2023 21:35:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiBMZRR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Reliable Software-Defined Networking (SDN) should mitigate forwarding anomalies due to cross-plane rule inconsistencies. Most existing countermeasures either inject probe packets to infer forwarding correctness or collect packet traces to detect forwarding anomalies. They, however, cannot detect or filter forwarding anomalies for production packets in real time. In this paper, we propose RuleOut as the first attempt to automatically throttle SDN forwarding anomalies. It disambiguates dependent rules via augmenting their matching fields with unique tags. Leveraging source routing, we further bind each packet with the tag sequence corresponding to rules the packet should match. RuleOut thus renders each packet to match at most one rule on each switch. This completely addresses the root cause of forwarding ambiguity. To implement RuleOut, we develop a non-overlapping rule dependency graph, a series of algorithms for incremental rule update and tag generation upon it, and various optimization techniques toward scalability and efficiency. We prototype RuleOut on the Ryu controller and Open vSwitch and evaluate its performance over public rule sets such as Stanford, Internet2, and Airtel1. RuleOut can use tags of only several bits long to disambiguate thousands to millions of rules and generate tags fairly fast within a few milliseconds.}
}


@article{DBLP:journals/ton/PanBSS23,
	author = {Jiayu Pan and
                  Ahmed M. Bedewy and
                  Yin Sun and
                  Ness B. Shroff},
	title = {Optimal Sampling for Data Freshness: Unreliable Transmissions With
                  Random Two-Way Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {408--420},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3194417},
	doi = {10.1109/TNET.2022.3194417},
	timestamp = {Sat, 25 Feb 2023 21:35:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PanBSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper, we aim to design an optimal sampler for a system in which fresh samples of a signal (source) are sent through an unreliable channel to a remote estimator, and acknowledgments are sent back over a feedback channel. Both the forward and feedback channels could have random transmission times due to time varying channel conditions. Motivated by distributed sensing, the estimator can estimate the real-time value of the source signal by combining the signal samples received through the channel and the noisy signal observations collected from a local sensor. We prove that the estimation error is a non-decreasing function of the Age of Information (AoI) for the received signal samples and design an optimal sampling strategy that minimizes the long-term average estimation error subject to a sampling rate constraint. The sampling strategy is also optimal for minimizing the long-term average of general non-decreasing functions of the AoI. The optimal sampler design follows a randomized threshold strategy: If the last transmission was successful, the source waits until the expected estimation error upon delivery exceeds a threshold and then sends out a new sample. If the last transmission fails, the source immediately sends out a new sample without waiting. The threshold is the root of a fixed-point equation and can be solved with low complexity (e.g., by bisection search). The optimal sampling strategy holds for general transmission time distributions of the forward and feedback channels. Numerical simulations are provided to compare different sampling policies.}
}


@article{DBLP:journals/ton/TangZPZ23,
	author = {Shaofei Tang and
                  Sicheng Zhao and
                  Xiaoqin Pan and
                  Zuqing Zhu},
	title = {How to Use In-Band Network Telemetry Wisely: Network-Wise Orchestration
                  of Sel-INT},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {421--435},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3194086},
	doi = {10.1109/TNET.2022.3194086},
	timestamp = {Sat, 25 Feb 2023 21:35:35 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TangZPZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a promising network monitoring technique, in-band network telemetry (INT) helps to visualize networks in a fine-grained and real-time manner. Meanwhile, to address the overheads of INT, people have proposed a few selective INT (Sel-INT) approaches that only select a portion of packets in each flow to insert INT fields and distribute different types of INT fields over the selected packets. In this paper, we study how to use Sel-INT wisely in a network such that the tradeoff between monitoring accuracy/coverage and INT overheads can be balanced well. Specifically, we try to orchestrate the Sel-INT schemes of flows in both network- and flow-levels. For the network-level optimization, we model it as an INT planning problem in which the Sel-INT schemes of flows should be determined to maximize the information gain of INT as well as minimize the bandwidth overheads of INT. We formulate an integer linear programming (ILP) model to tackle the problem, prove its\nNP\n-hardness, and leverage Lagrangian relaxation to design a polynomial-time approximation algorithm for it. The flow-level optimization considers a dynamic network environment, and we propose to combine deep learning (DL) based traffic prediction with Sel-INT, such that the Sel-INT scheme of each individual flow can be updated timely and adaptively. We implement the proposal in a small but real network testbed and experimentally demonstrate self-adaptive orchestration of Sel-INT with it.}
}


@article{DBLP:journals/ton/HuangW23,
	author = {Cheng Huang and
                  Xudong Wang},
	title = {Distributed Scheduling With Centralized Coordination for Scalable
                  Wireless Mesh Networking},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {436--451},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3195680},
	doi = {10.1109/TNET.2022.3195680},
	timestamp = {Mon, 26 Feb 2024 15:48:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A coordinated carrier sense multiple access (C-CSMA) scheme is developed to schedule packet transmissions in multihop wireless networks. It integrates distributed scheduling and centralized coordination at different time scales. At a small time scale, a distributed scheduling algorithm, which is incorporated into CSMA, runs on each node to determine key parameters for CSMA. These parameters are optimized by ensuring multiple network attributes, i.e., average link date rate, node transmission probability, and the conditional probability of a link being selected for transmission, approaching their corresponding optimal values at a large time scale, e.g., a few seconds. Such optimal values are obtained from a centralized algorithm running on a portal node. The algorithm collects topology, link, and traffic information from the network at a large time scale and determines the attributes above to fulfill the optimal tradeoff between throughput and fairness. Since the distributed algorithm is shepherded by the centralized algorithm, C-CSMA has high scalability: first, it achieves long-term optimal performance with low information collection overhead; second, it schedules packet transmissions in a distributed way and responds quickly to changes in networks. It is proved that network attributes resulting from the distributed scheduling algorithm converge to optimal values provided by the centralized algorithm. Simulation experiments demonstrate that the convergence speed is fast. Moreover, extensive simulation results show that C-CSMA achieves much higher throughput, better fairness, and lower delay than existing scheduling schemes under different carrier sensing thresholds and traffic conditions.}
}


@article{DBLP:journals/ton/FuLSX23,
	author = {Chuanpu Fu and
                  Qi Li and
                  Meng Shen and
                  Ke Xu},
	title = {Frequency Domain Feature Based Robust Malicious Traffic Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {1},
	pages = {452--467},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3195871},
	doi = {10.1109/TNET.2022.3195871},
	timestamp = {Fri, 12 May 2023 15:00:49 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuLSX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) based malicious traffic detection is an emerging security paradigm, particularly for zero-day attack detection, which is complementary to existing rule based detection. However, the existing ML based detection achieves low detection accuracy and low throughput incurred by inefficient traffic features extraction. Thus, they cannot detect attacks in realtime, especially in high throughput networks. Particularly, these detection systems similar to the existing rule based detection can be easily evaded by sophisticated attacks. To this end, we propose Whisper, a realtime ML based malicious traffic detection system that achieves both high accuracy and high throughput by utilizing frequency domain features. It utilizes sequential information represented by the frequency domain features to achieve bounded information loss, which ensures high detection accuracy, and meanwhile constrains the scale of features to achieve high detection throughput. In particular, attackers cannot easily interfere with the frequency domain features and thus Whisper is robust against various evasion attacks. Our experiments with 74 types of attacks demonstrate that, compared with the state-of-the-art systems, Whisper can accurately detect various sophisticated and stealthy attacks, achieving at most 18.36% improvement of AUC, while achieving two orders of magnitude throughput. Even under various evasion attacks, Whisper is still able to maintain around 90% detection accuracy.}
}


@article{DBLP:journals/ton/ChangCZL23,
	author = {Hao{-}Hsuan Chang and
                  Hao Chen and
                  Jianzhong Zhang and
                  Lingjia Liu},
	title = {Decentralized Deep Reinforcement Learning Meets Mobility Load Balancing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {473--484},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3176528},
	doi = {10.1109/TNET.2022.3176528},
	timestamp = {Tue, 02 Apr 2024 11:00:12 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChangCZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobility load balancing (MLB) aims to solve the problem of uneven resource utilization in cellular networks. Since network dynamics are usually complicated and non-stationary, conventional model-based MLB methods fail to cover all scenarios of cellular networks. On the other hand, deep reinforcement learning (DRL) can provide a flexible framework to learn to distribute cell load evenly without explicit modeling of the underlying network dynamics. In this paper, we introduce a novel decentralized DRL-based MLB method where each cell has a DRL agent to learn its handover parameters and antenna tilt angle. As the number of cells increases, the decentralized framework is more computationally efficient than its centralized counterpart by dividing the action space. Furthermore, our designed decentralized DRL architecture only requires readily known information defined in existing cellular standards, and it can achieve a more balanced cell load distribution than the centralized DRL one by using individual reward functions. To provide realistic performance evaluation, a network simulator is introduced strictly following the Third Generation Partnership Project (3GPP) specifications. Furthermore, field data is used to construct the underlying cellular environment. Extensive evaluations have been conducted to demonstrate the fact that the introduced decentralized DRL-based MLB method can achieve a more balanced cell load distribution and a better performance of edge users than the state-of-the-art MLB methods.}
}


@article{DBLP:journals/ton/HeWZSXSS23,
	author = {Yunhua He and
                  Yueting Wu and
                  Cui Zhang and
                  Jialong Shen and
                  Ke Xiao and
                  Keshav Sood and
                  Limin Sun},
	title = {A Sparse Protocol Parsing Method for IIoT Based on BPSO-vote-HMM Hybrid
                  Model},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {485--496},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3184751},
	doi = {10.1109/TNET.2022.3184751},
	timestamp = {Wed, 12 Jun 2024 21:04:43 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HeWZSXSS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the development of the Industrial Internet of Things, industrial control systems have become more open and intelligent. However, large numbers of unknown protocols exist in IIoT, threatening the security of IIoT devices and systems. Protocol reverse engineering extracts the grammar and semantics of the protocol by monitoring and analyzing the traffic trace or the execution process of instructions, without the need for protocol description. As the executable programs are mainly integrated into the IIoT devices and the communication traffic is relatively sparse, the traditional protocol analyzing method is not suitable for the IIoT environment. This paper proposes an improved sparse protocol parsing method of IIoT protocol based on the BPSO-vote-HMM hybrid model. The binary particle swarm optimization algorithm is introduced to expand the captured IIoT protocol message sequence, solving the problems of sparse samples in IIoT and the low efficiency of the GA-based data expansion model. Besides, we improve on the parameter training part to improve the efficiency and get better model parameters by dividing the training set into several sub-sets, conducting the parameter update parallel, and inputting the results into a voter to generate the final parameter of HMM, which is used in protocol field prediction. Finally, by combining the BPSO-based data expansion model and the protocol field parsing model based on vote-HMM, a hybrid analytical model is constructed to improve the analytical accuracy in a gradual evolutionary manner. Through a series of comparative experiments, the improved protocol field parsing model has better performance on IIoT protocol.}
}


@article{DBLP:journals/ton/ShiCLGCY23,
	author = {Tuo Shi and
                  Zhipeng Cai and
                  Jianzhong Li and
                  Hong Gao and
                  Jiancheng Chen and
                  Ming Yang},
	title = {Services Management and Distributed Multihop Requests Routing in Mobile
                  Edge Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {497--510},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3196267},
	doi = {10.1109/TNET.2022.3196267},
	timestamp = {Sat, 29 Apr 2023 19:27:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiCLGCY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-access Edge Computing (MEC) is an emerging computing architecture to release the resource burden of the centralized cloud and reduce the mobile application latency. Services management and MEC requests routing is a major problem in MEC systems. Existing works mainly focus on the one-hop centralized request routing strategies. However, the centralized one-hop routing method is not suitable enough since the MEC network is a distributed system, and the number of MEC requests increases dramatically. In this paper, we have proposed an online problem. In such problem, we jointly consider the mobile edge service management and the distributed multi-hop requests routing in an MEC network in which the MEC requests randomly generate. We prove that such problem is NP-Hard even in the off-line scenario. Furthermore, we propose an approximation algorithm to manage the MEC services and two distributed online algorithms to route MEC requests. The approximation ratio and competitive ratio of these algorithms have been analyzed. Experiments are carried out to evaluate the performance of the algorithms and simulation results imply that these algorithms are effective and efficient.}
}


@article{DBLP:journals/ton/LeiLZLLJW23,
	author = {Kai Lei and
                  Guanjie Lin and
                  Meimei Zhang and
                  Keke Li and
                  Qi Li and
                  Xiaojun Jing and
                  Peng Wang},
	title = {Measuring the Consistency Between Data and Control Plane in {SDN}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {511--525},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3193698},
	doi = {10.1109/TNET.2022.3193698},
	timestamp = {Sat, 29 Apr 2023 19:27:00 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LeiLZLLJW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software Defined Networking (SDN) simplifies network control and management by decoupling the control plane from the data plane. However, the actual packet behaviors, conforming to the rules in the data plane flow tables, may violate the original policies in the controller due to the inconsistency between the data plane and control plane. To address this problem, we propose 2MVeri, a framework for measuring the consistency between the Data and Control plane, defined as the consistency between the control plane policies and data plane rules. 2MVeri uses a modules, a Bloom filter and a two-dimensional vector as a tag which is inserted in the packet header and is updated in each switch that the packet traverses. By exploiting path information compressed in the tag, 2MVeri can verify the consistency between the data and control plane. Moreover, when verification fails, 2MVeri is able to localize the faulty switch. Experimental results show that in the k = 4 fat tree topology, the verification accuracy of 2MVeri is as high as 100%. In addition, when the actual path is inconsistent with the expected path, 2MVeri can locate the wrong switch with an accuracy of 99.8%.}
}


@article{DBLP:journals/ton/DuXXZSWL23,
	author = {Xinle Du and
                  Ke Xu and
                  Lei Xu and
                  Kai Zheng and
                  Meng Shen and
                  Bo Wu and
                  Tong Li},
	title = {{R-AQM:} Reverse {ACK} Active Queue Management in Multitenant Data
                  Centers},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {526--541},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3197973},
	doi = {10.1109/TNET.2022.3197973},
	timestamp = {Fri, 12 May 2023 15:00:49 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DuXXZSWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {TCP incast has become a practical problem for high-bandwidth, low-latency transmissions, resulting in throughput degradation of up to 90% and delays of hundreds of milliseconds, severely impacting application performance. However, in virtualized multi-tenant data centers, host-based advancements in the TCP stack are hard to deploy from the operators’ perspective. Operators only provide infrastructure in the form of virtual machines, in which only tenants can directly modify the end-host TCP stack. In this paper, we present R-AQM, a switch-powered reverse ACK active queue management (R-AQM) mechanism for enhancing ACK-clocking effects through assisting legacy TCP. Specifically, R-AQM proactively intercepts ACKs and paces the ACK-clocked in-flight data packets, preventing TCP from suffering incast collapse. We implement and evaluate R-AQM in NS-3 simulation and NetFPGA-based hardware switch. Both simulation and testbed results show that R-AQM greatly improves TCP performance under heavy incast workloads by significantly lowering packet loss rate, reducing retransmission timeouts, and supporting 16 times (i.e., 60 to 1000) more senders. Meanwhile, the forward queuing delays are also reduced by 4.6 times.}
}


@article{DBLP:journals/ton/ZhaoQWZWX23,
	author = {Yi Zhao and
                  Meina Qiao and
                  Haiyang Wang and
                  Rui Zhang and
                  Dan Wang and
                  Ke Xu},
	title = {Friendship Inference in Mobile Social Networks: Exploiting Multi-Source
                  Information With Two-Stage Deep Learning Framework},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {542--557},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3198105},
	doi = {10.1109/TNET.2022.3198105},
	timestamp = {Sat, 30 Sep 2023 10:29:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoQWZWX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the tremendous growth of mobile social networks (MSNs), people are highly relying on it to connect with friends and further expand their social circles. However, the conventional friendship inference techniques have issues handling such a large yet sparse multi-source data. The related friend recommendation systems are therefore suffering from reduced accuracy and limited scalability. To address this issue, we propose a Two-stage Deep learning framework for Friendship Inference, namely TDFI. This approach enables MSNs to exploit multi-source information simultaneously, rather than hierarchically. Therefore, there is no need to manually set which information is more important and the order in which the various information is applied. In details, we apply an Extended Adjacency Matrix (EAM) to represent the multi-source information. We then adopt an improved Deep Auto-Encoder Network (iDAEN) to extract the fused feature vector for each user. Our framework also provides an improved Deep Siamese Network (iDSN) to measure user similarity. To provide a substantial description and evaluation of the proposed methodology, we evaluate the effectiveness and robustness on three large-scale real-world datasets. Trace-driven evaluation results demonstrate that TDFI can effectively handle the sparse multi-source data while providing better accuracy for friendship inference. Through the comparison with numerous state-of-the-art methods, we find that TDFI can achieve superior performance via real-world multi-source information. Meanwhile, it demonstrates that the proposed pipeline can not only integrate structural information and attribute information, but also be compatible with different attribute information, which further enhances the overall applicability of friend-recommendation systems under information-rich MSNs.}
}


@article{DBLP:journals/ton/WangMOCP23,
	author = {Haibo Wang and
                  Chaoyi Ma and
                  Olufemi O. Odegbile and
                  Shigang Chen and
                  Jih{-}Kwon Peir},
	title = {Randomized Error Removal for Online Spread Estimation in High-Speed
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {558--573},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3197968},
	doi = {10.1109/TNET.2022.3197968},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangMOCP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Flow spread measurement provides fundamental statistics that can help network operators better understand flow characteristics and traffic patterns with applications in traffic engineering, cybersecurity and quality of service. Past decades have witnessed tremendous performance improvement for single-flow spread estimation. However, when dealing with numerous flows in a packet stream, it remains a significant challenge to measure per-flow spread accurately while reducing memory footprint. The goal of this paper is to introduce new multi-flow spread estimation designs that incur much smaller processing overhead and query overhead than the state of the art, yet achieves significant accuracy improvement in spread estimation. We formally analyze the performance of these new designs. We implement them in both hardware and software, and use real-world data traces to evaluate their performance in comparison with the state of the art. The experimental results show that our best sketch significantly improves over the best existing work in terms of estimation accuracy, packet processing throughput, and online query throughput.}
}


@article{DBLP:journals/ton/DingYJLHZ23,
	author = {Yi Ding and
                  Yu Yang and
                  Wenchao Jiang and
                  Yunhuai Liu and
                  Tian He and
                  Desheng Zhang},
	title = {Nationwide Deployment and Operation of a Virtual Arrival Detection
                  System in the Wild},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {574--589},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3196806},
	doi = {10.1109/TNET.2022.3196806},
	timestamp = {Thu, 19 Oct 2023 07:36:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DingYJLHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We report a 30-month nationwide deployment and operation study of an indoor arrival detection system based on Bluetooth Low Energy called VALID in 364 Chinese cities. VALID is pilot-studied, deployed, and operated in the wild to infer real-time indoor arrival status of couriers, and improve their status reporting behavior based on the detection. During its full nationwide operation (2018/12-2021/01), VALID consists of virtual devices at 3 million shops and restaurants, where 530,859 of them are in multi-story malls and markets to infer and influence 1 million couriers’ behavior, and assist the scheduling of 3.9 billion orders for 186 million customers. Although indoor arrival detection is straightforward in controlled environments, the scale of our platform makes the cost prohibitively high. In this work, we explore to use merchants’ smartphones under their consent as a virtual infrastructure to design, build, deploy, and operate VALID from in-lab conception to nationwide operation in three phases for 30 months. We consider metrics including system evolution, reliability, utility, participation, energy, privacy, monetary benefits, along with couriers’ behavior changes. We share three lessons and their implications for similar wireless sensing or communication systems with large geospatial operations.}
}


@article{DBLP:journals/ton/ChenCXLLWTWFC23,
	author = {Lili Chen and
                  Kai Chen and
                  Jie Xiong and
                  Ke Li and
                  Sunghoon Ivan Lee and
                  Fuwei Wang and
                  Zhanyong Tang and
                  Zheng Wang and
                  Dingyi Fang and
                  Xiaojiang Chen},
	title = {Toward Wide-Area Contactless Wireless Sensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {590--605},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3196744},
	doi = {10.1109/TNET.2022.3196744},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenCXLLWTWFC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Contactless wireless sensing without attaching a device to the target has achieved promising progress in recent years. However, one severe limitation is the small sensing range. This paper presents Widesee to realize wide-area sensing with only one transceiver pair. Widesee utilizes the LoRa signal to achieve a larger range of sensing and further incorporates drone’s mobility to broaden the sensing area. Widesee presents solutions across software and hardware to overcome two aspects of challenges for wide-range contactless sensing: (i) the interference brought by device mobility and LoRa’s high sensitivity; and (ii) the ambiguous target information such as location when employing just a single pair of transceivers for sensing. We have developed a working prototype of Widesee for human target detection and localization that are especially useful in emergency scenarios such as rescue search, and evaluated Widesee with both controlled experiments and the field study in a high-rise building. Extensive experiments demonstrate the great potential of Widesee for wide-area contactless sensing with a single LoRa transceiver pair hosted on a drone.}
}


@article{DBLP:journals/ton/MichelCMCMB23,
	author = {Fran{\c{c}}ois Michel and
                  Alejandro Cohen and
                  Derya Malak and
                  Quentin De Coninck and
                  Muriel M{\'{e}}dard and
                  Olivier Bonaventure},
	title = {FlEC: Enhancing {QUIC} With Application-Tailored Reliability Mechanisms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {606--619},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3195611},
	doi = {10.1109/TNET.2022.3195611},
	timestamp = {Wed, 17 May 2023 21:57:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MichelCMCMB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet losses are common events in today’s networks. They usually result in longer delivery times for application data since retransmissions are the de facto technique to recover from such losses. Retransmissions is a good strategy for many applications but it may lead to poor performance with latency-sensitive applications compared to network coding. Although different types of network coding techniques have been proposed to reduce the impact of losses by transmitting redundant information, they are not widely used. Some niche applications include their own variant of Forward Erasure Correction (FEC) techniques, but there is no generic protocol that enables many applications to easily use them. We close this gap by designing, implementing and evaluating a new Flexible Erasure Correction (FlEC) framework inside the newly standardized QUIC protocol. With FlEC, an application can easily select the reliability mechanism that meets its requirements, from pure retransmissions to various forms of FEC. We consider three different use cases: (i)\nbulk data transfer, (ii)\nfile transfers with restricted buffers and (iii)\ndelay-constrained messages. We demonstrate that modern transport protocols such as QUIC may benefit from application knowledge by leveraging this knowledge in FlEC to provide better loss recovery and stream scheduling. Our evaluation over a wide range of scenarios shows that the FlEC framework outperforms the standard QUIC reliability mechanisms from a latency viewpoint.}
}


@article{DBLP:journals/ton/TianZL23,
	author = {Xiang Tian and
                  Baoxian Zhang and
                  Cheng Li},
	title = {Distributed Stable Multisource Global Broadcast for SINR-Based Wireless
                  Multihop Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {620--633},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3198331},
	doi = {10.1109/TNET.2022.3198331},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TianZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-source global broadcast is a fundamental problem in multi-hop wireless networks. The Static Multi-source Global Broadcast problem (SMGB), which considers static packet injection at all source nodes, has been extensively studied in recent years. However, packets are more likely to be continuously injected over time in realistic multi-hop wireless networks. In this paper, we focus on studying the Dynamic Multi-source Global Broadcast problem (DMGB), in which packets are continuously injected to\nk\n(\nk≥2\n) source nodes in the network according to a widely-used dynamic packet injection model and the objective is to disseminate each injected packet across the whole network quickly. We solve this DMGB problem under the Signal-to-Interference-plus-Noise-Ratio (SINR) interference model. Specifically, we first present a distributed randomized algorithm for solving the SMGB problem. We then iterate this SMGB algorithm repeatedly to construct a distributed DMGB algorithm. We prove the proposed DMGB algorithm is stable, i.e., the expected number of packets in each node’s message queue is bounded at any time and further the expected global broadcast latency for each injected packet is bounded. Simulation results validate the effectiveness of the proposed DMGB algorithm.}
}


@article{DBLP:journals/ton/BaoPW23,
	author = {Yixin Bao and
                  Yanghua Peng and
                  Chuan Wu},
	title = {Deep Learning-Based Job Placement in Distributed Machine Learning
                  Clusters With Heterogeneous Workloads},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {634--647},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3202529},
	doi = {10.1109/TNET.2022.3202529},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BaoPW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Nowadays, most leading IT companies host a variety of distributed machine learning (ML) workloads in ML clusters to support AI-driven services, such as speech recognition, machine translation, and image processing. While multiple jobs are executed concurrently in a shared cluster to improve resource utilization, interference among co-located ML jobs can lead to significant performance downgrade. Existing cluster schedulers, such as YARN and Mesos, are interference-agnostic in their job placement, leading to suboptimal resource efficiency and usage. Some literature has studied interference-aware job placement policy, but relies on detailed workload profiling and interference modeling, which is not a general solution. In this work, we present Harmony, a deep learning-driven ML cluster scheduler that places heterogeneous training jobs (either with parameter server architecture or all-reduce architecture) in a manner that minimizes interference and maximizes performance (i.e., training completion time minimization). The design of Harmony is based on a carefully designed deep reinforcement learning (DRL) framework enhanced with reward modeling. The DRL integrates a dynamic sequence-to-sequence model with the state-of-the-art techniques to stabilize training and improve convergence, including actor-critic algorithm, job-aware action space exploration, multi-head attention, and experience replay. In view of a common lack of reward samples corresponding to different placement decisions, we build an auxiliary sequence-to-sequence reward prediction model, which is trained with historical samples and used for producing reward for unseen placement. Experiments using real ML workloads in a Kubernetes cluster of 6 GPU servers show that Harmony outperforms representative schedulers by 16%–42% in terms of average job completion time.}
}


@article{DBLP:journals/ton/WangZLXYYW23,
	author = {En Wang and
                  Mijia Zhang and
                  Wenbin Liu and
                  Haoyi Xiong and
                  Bo Yang and
                  Yongjian Yang and
                  Jie Wu},
	title = {Outlier-Concerned Data Completion Exploiting Intra- and Inter-Data
                  Correlations in Sparse CrowdSensing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {648--663},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3201545},
	doi = {10.1109/TNET.2022.3201545},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangZLXYYW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile CrowdSensing (MCS) is a popular data collection paradigm which usually faces the problem of sparse sensed data because of the limited sensing cost. In order to address the situation of sparse data, sparse MCS recruits users to sense important areas and infers completed data by data completion, which is crucial in sparse MCS for urban sensing applications (e.g. enhancing data expression, improving urban analysis, guiding city planning, etc.) To achieve accurate completion results, previous methods usually utilize the universal similarity and conventional tendency while incorporating only a single dataset to infer the full map. However, in real-world scenarios, there may exist many kinds of data (inter-data), that could help to complement each other. Moreover, for each kind of data (intra-data), there usually exist a few but important outliers caused by the special events (e.g., parking peak, traffic congestion, or festival parade), which may behave in a different way as the statistical patterns. These outliers cannot be ignored, while it is difficult to detect and recover them in data completion because of the following challenges: 1) the infrequency and unpredictability of outliers’ occurrence, 2) the large deviations against the means compared to normal values, and 3) the complex spatiotemporal relations among inter-data. To this end, focusing on spatiotemporal data with both intra- and inter-data correlations, we propose a matrix completion method that takes outliers’ effects into consideration and exploits both intra- and inter-data correlations for enhancing performance. Specifically, we first conduct the Deep Matrix Factorization (DMF) with multiple auxiliary Neural Networks, which named Stacked Deep Matrix Factorization (SDMF). Note that the loss function of SDMF is no longer the previous MSE loss function, but replaced with an Outlier Value Loss (OVL) function to effectively detect and recover the outliers. Moreover, a spatiotemporal outlier value memory network is added for further enhancing the outlier inference. Finally, we take extensive qualitative and quantitative experiments on two popular datasets each with two types of sensing data, and the experimental results indicate the advantages of our method that outperforms the state-of-the-art methods.}
}


@article{DBLP:journals/ton/RuanSLL23,
	author = {Na Ruan and
                  Hanyi Sun and
                  Zenan Lou and
                  Jie Li},
	title = {A General Quantitative Analysis Framework for Attacks in Blockchain},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {664--679},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3201493},
	doi = {10.1109/TNET.2022.3201493},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RuanSLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Decentralized cryptocurrency systems have become primary targets for attackers due to substantial profit gain and economic rewards. A number of attack models have been proposed during last few years. However, the evaluation and comparison of those attack models remain problematic due to the lack of systematic framework to analyze them. In this work, we propose a general quantitative analysis framework for attack models in the network and consensus layer of blockchain. We identify the problem statement and evolution process. And we show how to apply our general framework in previous attacks such as selfish mining and bribery attack. We also explained that the framework is suitable for other attacks in blockchain. For further exploration, we simulate the success rate and benefits of different attacks through experiments. We provide several defensive strategies, and study how these strategies against previous attack models.}
}


@article{DBLP:journals/ton/OzanDJ23,
	author = {Waseem Ozan and
                  Izzat Darwazeh and
                  Kyle Jamieson},
	title = {Partial {OFDM} Symbol Recovery to Improve Interfering Wireless Networks
                  Operation in Collision Environments},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {680--694},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3202857},
	doi = {10.1109/TNET.2022.3202857},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/OzanDJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The uplink data rate region for interfering transmissions in wireless networks has been characterised and proven, yet its underlying model assumes a complete temporal overlap. Practical unplanned networks, however, adopt packetized transmissions and eschew tight inter-network coordination, resulting in packet collisions that often partially overlap, but rarely ever completely overlap. In this work, we report a new design called Partial Symbol Recovery (PSR), that specifically targets the parts of data symbols that experience no interference during a packet collision. PSR bootstraps a successive interference cancellation (SIC) like decoder from these strong signals, thus improving performance over techniques oblivious to such partial packet overlaps. We have implemented PSR on the WARP software-defined radio platform and in trace-based simulation. Our performance evaluation presents experimental results from this implementation operating in a 12-node software network testbed, spread over two rooms in a non-line-of-sight indoor office environment. Experimental results confirm that our proposal PSR decoder is capable of decoding up to 60% of collided frames depending on the type of data and modulation used. This consistently leads to throughput enhancement over conventional Wi-Fi under different scenarios and for the various data types tested, namely downlink bulk TCP, downlink video-on-demand, and uplink UDP.}
}


@article{DBLP:journals/ton/TutuncuogluJD23,
	author = {Feridun T{\"{u}}t{\"{u}}nc{\"{u}}oglu and
                  Sladana Josilo and
                  Gy{\"{o}}rgy D{\'{a}}n},
	title = {Online Learning for Rate-Adaptive Task Offloading Under Latency Constraints
                  in Serverless Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {695--709},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3197669},
	doi = {10.1109/TNET.2022.3197669},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TutuncuogluJD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the interplay between latency constrained applications and function-level resource management in a serverless edge computing environment. We develop a game theoretic model of the interaction between rate adaptive applications and a load balancing operator under a function-oriented pay-as-you-go pricing model. We show that under perfect information, the strategic interaction between the applications can be formulated as a generalized Nash equilibrium problem, and use variational inequality theory to prove that the game admits an equilibrium. For the case of imperfect information, we propose an online learning algorithm for applications to maximize their utility through rate adaptation and resource reservation. We show that the proposed algorithm can converge to equilibria and achieves zero regret asymptotically, and our simulation results show that the algorithm achieves good system performance at equilibrium, ensures fast convergence, and enables applications to meet their latency constraints.}
}


@article{DBLP:journals/ton/QiW23,
	author = {Jianpeng Qi and
                  Rui Wang},
	title = {{R2:} {A} Distributed Remote Function Execution Mechanism With Built-In
                  Metadata},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {710--723},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3198467},
	doi = {10.1109/TNET.2022.3198467},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QiW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Named data networking (NDN) constructs a network by names, providing a flexible and decentralized way to manage resources within the edge computing continuum. This paper aims to solve the question, “Given a function with its parameters and metadata, how to select the executor in a distributed manner and obtain the result in NDN?” To answer it, we design R2 that involves the following stages. First, we design a name structure including data, function names, and other function parameters. Second, we develop a 2-phase mechanism, where in the first phase, the function request from a client-first reaches the data source and retrieves the metadata. Then the best node is selected while the metadata responds to the client. In the second phase, the chosen node directly retrieves the data, executes the function, and provides the result to the client. Furthermore, we propose a stop condition to intelligently reduce the processing time of the first phase and provide a simple proof and range analysis. Simulations confirm that R2 outperforms the current solutions in terms of resource allocation, especially when the data volume and the function complexity are high. In the experiments, when the data size is 100 KiB and the function complexity is\nO(\nn\n2\n)\n, the speedup ratio is 4.61. To further evaluate R2, we also implement a general intermediate data processing logic named “Bolt” implemented on an app-level in ndnSIM. We believe that R2 shall help the researchers and developers to verify their ideas smoothly.}
}


@article{DBLP:journals/ton/TangXHL23,
	author = {Lu Tang and
                  Yao Xiao and
                  Qun Huang and
                  Patrick P. C. Lee},
	title = {A High-Performance Invertible Sketch for Network-Wide Superspreader
                  Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {724--737},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3198738},
	doi = {10.1109/TNET.2022.3198738},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TangXHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Superspreaders (i.e., hosts with numerous distinct connections) remain severe threats to production networks. How to accurately detect superspreaders in real-time at scale remains a non-trivial yet challenging issue. We present SpreadSketch, an invertible sketch data structure for network-wide superspreader detection with the theoretical guarantees on memory space, performance, and accuracy. SpreadSketch tracks candidate superspreaders and embeds estimated fan-outs in binary hash strings inside small and static memory space, such that multiple SpreadSketch instances can be readily merged to provide a network-wide measurement view for recovering superspreaders and their estimated fan-outs. We present formal theoretical analysis on SpreadSketch in terms of space and time complexities as well as error bounds. We further extend SpreadSketch with a fast and small data structure that filters out the packets of high-frequency connections from sketch processing, so as to improve the update performance of SpreadSketch while maintaining the accuracy guarantees. Trace-driven evaluation shows that SpreadSketch achieves higher accuracy and performance over state-of-the-art sketches and remains accurate in detecting real-world worms and DDoS attacks. Furthermore, we prototype SpreadSketch in P4 and show its feasible deployment in commodity hardware switches.}
}


@article{DBLP:journals/ton/HuangZLLLYW23,
	author = {Jiawei Huang and
                  Wenlu Zhang and
                  Yijun Li and
                  Lin Li and
                  Zhaoyi Li and
                  Jin Ye and
                  Jianxin Wang},
	title = {ChainSketch: An Efficient and Accurate Sketch for Heavy Flow Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {738--753},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3199506},
	doi = {10.1109/TNET.2022.3199506},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangZLLLYW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Identifying heavy flows is essential for network management. However, it is challenging to detect heavy flow quickly and accurately under the highly dynamic traffic and rapid growth of network capacity. Existing heavy flow detection schemes can make a trade-off in efficiency, accuracy and speed. However, these schemes still require memory large enough to obtain acceptable performance. To address this issue, we propose ChainSketch, which has the advantages of good memory efficiency, high accuracy and fast detection. Specifically, ChainSketch uses the selective replacement strategy to mitigate the over-estimation issue. Meanwhile, ChainSketch utilizes the hash chain and compact structure to improve memory efficiency. We implement the ChainSketch on OVS platform, P4-based testbed and large-scale simulations to process heavy hitter and heavy changer detection. The results of trace-driven tests show that, ChainSketch greatly improves the F1-score by up to\n3.43×\ncompared with the state-of-the-art solutions especially for small memory.}
}


@article{DBLP:journals/ton/AhmadATJAZAUQ23,
	author = {Mukhtiar Ahmad and
                  Syed Muhammad Nawazish Ali and
                  Muhammad Taimoor Tariq and
                  Syed Usman Jafri and
                  Adnan Abbas and
                  Syeda Mashal Abbas Zaidi and
                  Muhammad Basit Iqbal Awan and
                  Zartash Afzal Uzmi and
                  Zafar Ayyub Qazi},
	title = {Neutrino: {A} Fast and Consistent Edge-Based Cellular Control Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {754--769},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3202496},
	doi = {10.1109/TNET.2022.3202496},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AhmadATJAZAUQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {5G and next-generation cellular networks aim to support tactile internet to enable immersive and real-time applications by providing ultra-low latency and extremely high reliability. This imposes new requirements on the design of cellular core networks. A key component of the cellular core is the control plane. Time to complete cellular control plane operations (e.g., mobility handoff, service establishment) directly impacts the delay experienced by end-user applications. In this paper, we design Neutrino, a cellular control plane that provides users an abstraction of reliable access to cellular services while ensuring lower latency. Our testbed evaluations based on real cellular control traffic traces show that Neutrino provides an improvement in control procedure completion times by up to\n3.1×\nwithout failures, and up to\n5.6×\nunder control plane failures, over existing 5G. We also show how these improvements translate into improving end-user application performance: for AR/VR applications and self-driving cars, Neutrino improves performance by up to\n2.5×\nand\n2.8×\n, respectively.}
}


@article{DBLP:journals/ton/ArrigoniBMT23,
	author = {Viviana Arrigoni and
                  Novella Bartolini and
                  Annalisa Massini and
                  Federico Trombetti},
	title = {A Bayesian Approach to Network Monitoring for Progressive Failure
                  Localization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {770--783},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3200249},
	doi = {10.1109/TNET.2022.3200249},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ArrigoniBMT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Boolean Network Tomography (BNT) aims at identifying failures of internal network components by means of end-to-end monitoring paths. However, when the number of failures is not known a priori, failure identification may require a huge number of monitoring paths. We address this problem by designing a Bayesian approach that progressively selects the next path to probe on the basis of its expected information utility, conditioned on prior observations. As the complexity of the computation of posterior probabilities of node failures is exponential in the number of failed paths, we propose a polynomial-time greedy strategy which approximates these values. To consider aging of information in dynamic failure scenarios where node states can change during a monitoring period, we propose a monitoring technique based on a sliding observation window of adaptive length. By means of numerical experiments conducted on real network topologies we demonstrate the practical applicability of our approach, and the superiority of our algorithms with respect to state of the art solutions based on classic BNT as well as sequential group testing.}
}


@article{DBLP:journals/ton/TripathiTM23,
	author = {Vishrant Tripathi and
                  Rajat Talak and
                  Eytan H. Modiano},
	title = {Information Freshness in Multihop Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {784--799},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3201751},
	doi = {10.1109/TNET.2022.3201751},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TripathiTM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of minimizing age of information in multihop wireless networks and propose three classes of policies to solve the problem - stationary randomized, age difference, and age debt. For the unicast setting with fixed routes between each source-destination pair, we first develop a procedure to find age optimal Stationary Randomized policies. These policies are easy to implement and allow us to derive closed-form expression for average AoI. Next, for the same unicast setting, we develop a class of heuristic policies, called Age Difference, based on the idea that if neighboring nodes try to reduce their age differential then all nodes will have fresher updates. This approach is useful in practice since it relies only on the local age differential between nodes to make scheduling decisions. Finally, we propose the class of policies called Age Debt, which can handle 1) non-linear AoI cost functions; 2) unicast, multicast and broadcast flows; and 3) no fixed routes specified per flow beforehand. Here, we convert AoI optimization problems into equivalent network stability problems and use Lyapunov drift to find scheduling and routing schemes that stabilize the network. We also provide numerical results comparing our proposed classes of policies with the best known scheduling and routing schemes available in the literature for a wide variety of network settings.}
}


@article{DBLP:journals/ton/LuHLHCLXT23,
	author = {Yu{-}Han Lu and
                  Sandy Hsin{-}Yu Hsiao and
                  Chi{-}Yu Li and
                  Yi{-}Chen Hsieh and
                  Po{-}Yi Chou and
                  Yao{-}Yu Li and
                  Tian Xie and
                  Guan{-}Hua Tu},
	title = {Insecurity of Operational {IMS} Call Systems: Vulnerabilities, Attacks,
                  and Countermeasures},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {800--815},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3205183},
	doi = {10.1109/TNET.2022.3205183},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LuHLHCLXT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {IMS (IP Multimedia Subsystem) is an essential 4G/5G component to offer multimedia services. It is used worldwide to support two call services: VoLTE (Voice over LTE) and VoWiFi (Voice over WiFi). In this study, it is shown that the signaling and voice sessions of VoWiFi can both be hijacked by a malicious adversary. By hijacking the signaling session, s(he) gains the ability to make ghost calls to launch stealthy DoS (Denial of Service) or caller-ID spoofing attacks against specific cellular users. Such attacks can be carried out without any malware or network information, and require only the victim’s phone number to be known. It is shown that phones vulnerable to the call DoS attacks can be detected at run time by exploiting a vulnerability of cellular network infrastructures referred to as call information leakage, which is exposed based on a machine learning method. Especially, the call DoS attacks can prevent victims from receiving incoming calls for up to 99.0% time without user awareness. Moreover, by hijacking the voice session, an adversary can launch stealthy free data transfer attacks based on phone numbers alone rather than IP addresses. The identified vulnerabilities/attacks are validated in the operational 4G networks of four top-tier carriers across Asia and North America with seven phone brands. The study concludes by presenting a suite of solutions to address them.}
}


@article{DBLP:journals/ton/ZhangLY23,
	author = {Jielun Zhang and
                  Fuhao Li and
                  Feng Ye},
	title = {Sustaining the High Performance of AI-Based Network Traffic Classification
                  Models},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {816--827},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3203227},
	doi = {10.1109/TNET.2022.3203227},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic classification plays an essential role in network measurement and management. Emerging Artificial Intelligence (AI) algorithms have become a viable solution to encrypted network traffic classification. Nonetheless, the classification performance of existing AI-based traffic classifiers is restricted to a limited number of network applications depending on the coverage of the knowledge database. Such AI-based traffic classifiers cannot maintain high performance to provide accurate traffic classification when dealing with updated or new network applications. To tackle the issues, we present an autonomous model update mechanism to sustain the high performance of AI-based traffic classifiers. Specifically, an instability check algorithm is derived to evaluate if the current classifier requires an update. A filtering algorithm is proposed to extract unknown traffic and build a new knowledge database based on a new metric, i.e., familiarity, defined based on the prediction confidence and instability. Extensive experiment results demonstrate that our proposed updating mechanism can provide prompt model updates and establish a proper new knowledge base to maintain high accuracy in various experimental scenarios. Moreover, the comparison is conducted and the results show the proposed familiarity-based filtering algorithm can filter about 7 and 3 times more true positive packets in the two considered scenarios, respectively.}
}


@article{DBLP:journals/ton/DongQWZSLH23,
	author = {Tianjian Dong and
                  Qi Qi and
                  Jingyu Wang and
                  Zirui Zhuang and
                  Haifeng Sun and
                  Jianxin Liao and
                  Zhu Han},
	title = {Standing on the Shoulders of Giants: Cross-Slice Federated Meta Learning
                  for Resource Orchestration to Cold-Start Slice},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {828--845},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3200853},
	doi = {10.1109/TNET.2022.3200853},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DongQWZSLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a key technology in 6G communication systems to support numerous vertical applications for all scenes while providing resources on demand. Due to more time-varying and dynamic traffic flows, it is difficult for traditional methods to manage complex and highly dynamic 6G networks. Therefore, intelligent method such as Deep Reinforcement Learning (DRL) is employed into network management since DRL is a model-free and experience-driven approach. However, it is difficult to leverage one DRL model to provide customized intra-slice orchestration for various applications because of their diverse flow characteristics and service requirements. Moreover, training a DRL model is notoriously time-consuming so that it is not realistic for network operator to individually orchestrate customized network slice for each application with the DRL algorithm. Additionally, the data privacy of each application should be considered into the training process. In this paper, we propose a Federated Meta Reinforcement Learning (FedMRL) approach to tackle the cold-start problem in network slice orchestration, while reserving the data privacy. The Meta Reinforcement Learning (MRL) is leveraged to train a meta policy for rapidly learn a local policy for a specific slice orchestration task by finding a common initialization that allows for a quick adaptation towards each optimal solution. With the help of federated learning setting, the training process of meta policy is not required to collect raw data of applications to the centralized server. Experimental results show that FedMRL outperforms three baselines in terms of overall costs, end-to-end latency and convergence speed.}
}


@article{DBLP:journals/ton/WangZLLWLX23,
	author = {Shicheng Wang and
                  Menghao Zhang and
                  Guanyu Li and
                  Chang Liu and
                  Zhiliang Wang and
                  Ying Liu and
                  Mingwei Xu},
	title = {Bolt: Scalable and Cost-Efficient Multistring Pattern Matching With
                  Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {846--861},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3202523},
	doi = {10.1109/TNET.2022.3202523},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangZLLWLX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-string pattern matching is a crucial building block for many network security applications and thus of great importance. Since every byte of a packet has to be inspected by a large set of patterns, it often becomes a bottleneck of these applications and dominates the performance of an entire system. Many existing studies have been devoted to alleviating this performance bottleneck either by algorithm optimization or hardware acceleration. However, neither one provides the desired scalability and costs that keep pace with the drastic increase in network bandwidth and traffic today. To address these issues, in this paper, we present BOLT, a scalable and cost-efficient multi-string pattern matching system leveraging the capability of emerging programmable switches. BOLT combines the following techniques: (1) an efficient state encoding scheme to fit a large number of strings into the limited memory on a programmable switch; (2) a variable\nk\n-stride transition mechanism to increase the throughput significantly with the same level of memory cost; and (3) a compact pattern2rule mapping method to accommodate multiple co-existing strings in one rule. We implement a prototype of BOLT and make its source code publicly available. Extensive evaluations demonstrate that BOLT can provide multi-hundred Gbps throughput and scales well with various pattern sets and workloads.}
}


@article{DBLP:journals/ton/XueLPYCJY23,
	author = {Guangtao Xue and
                  Yijie Li and
                  Hao Pan and
                  Lanqing Yang and
                  Yi{-}Chao Chen and
                  Xiaoyu Ji and
                  Jiadi Yu},
	title = {ScreenID: Enhancing QRCode Security by Utilizing Screen Dimming Feature},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {862--876},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3203044},
	doi = {10.1109/TNET.2022.3203044},
	timestamp = {Fri, 12 Jan 2024 21:08:47 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XueLPYCJY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quick response (QR) codes have been widely used in mobile applications, especially mobile payments, such as Alipay, WeChat, PayPal, etc due to their convenience and the pervasive built-in cameras on smartphones. Recently, however, attacks against QR codes have been reported and attackers can capture a QR code of the victim and replay it to achieve a fraudulent transaction or intercept private information, just before the original QR code is scanned. In this study, we enhance the security of a QR code by identifying its authenticity. We propose ScreenID, which embeds a QR code with information of the screen which displays it, thereby the QR code can reveal whether it is reproduced by an adversary or not. In ScreenID, PWM frequency of screens is exploited as the unique screen fingerprint. To improve the estimation accuracy of PWM frequency, ScreenID incorporates a model for the interaction between the camera and screen in the temporal and spatial domains. Extensive experiments demonstrate that ScreenID can differentiate screens of different models, types, and manufacturers and thus improve the security of QR codes.}
}


@article{DBLP:journals/ton/LiWLHZZZXZ23,
	author = {Jiawei Li and
                  Chuyu Wang and
                  Ang Li and
                  Dianqi Han and
                  Yan Zhang and
                  Jinhang Zuo and
                  Rui Zhang and
                  Lei Xie and
                  Yanchao Zhang},
	title = {Rhythmic {RFID} Authentication},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {877--890},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3204204},
	doi = {10.1109/TNET.2022.3204204},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiWLHZZZXZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Passive RFID technology is widely used in user authentication and access control. We propose RF-Rhythm, a secure and usable two-factor RFID authentication system with strong resilience to lost/stolen/cloned RFID cards. In RF-Rhythm, each legitimate user performs a sequence of taps on his/her RFID card according to a self-chosen secret melody. Such rhythmic taps can induce phase changes in the backscattered signals, which the RFID reader can detect to recover the user’s tapping rhythm. In addition to verifying the RFID card’s identification information as usual, the backend server compares the extracted tapping rhythm with what it acquires in the user enrollment phase. The user passes authentication checks if and only if both verifications succeed. We also propose a novel phase-hopping protocol in which the RFID reader emits Continuous Wave (CW) with random phases for extracting the user’s secret tapping rhythm. Our protocol can prevent a capable adversary from extracting and then replaying a legitimate tapping rhythm from sniffed RFID signals. Comprehensive user experiments confirm the high security and usability of RF-Rhythm with false-positive and false-negative rates close to zero.}
}


@article{DBLP:journals/ton/GaoZY23,
	author = {Yang Gao and
                  Hongli Zhang and
                  Xiangzhan Yu},
	title = {Higher-Order Community Detection: On Information Degeneration and
                  Its Elimination},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {891--903},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3201668},
	doi = {10.1109/TNET.2022.3201668},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GaoZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Community detection aims to identify the cohesive vertex sets in a network. It is widely used in many domains, e.g., World Wide Web, online social networks, and communication networks. Many clustering models are proposed in the literature. However, most of them are designed directly on the original structure of a network, they usually achieve low accuracy in practice, since real-world networks are presenting fuzzy community structures. Recently, higher-order network units are introduced to community detection, these models typically define a higher-order hypergraph, where communities are extracted. Although the higher-order models are effective in terms of accuracy, many essential edges are completely eliminated or trivialized in the hypergraph. To address the problem, we propose a novel connectivity pattern with a mixture of standard edges and higher-order connections, whereby we define biased personalized PageRank diffusion for local community detection and develop a local approach to compute the PageRank vectors. Moreover, we present a higher-order seeding strategy to derive the starting seeds. Extensive experiments demonstrate that the proposed framework largely outperforms the approaches in the state of the art in terms of accuracy.}
}


@article{DBLP:journals/ton/KongSCCLZLCW23,
	author = {Dezhang Kong and
                  Yi Shen and
                  Xiang Chen and
                  Qiumei Cheng and
                  Hongyan Liu and
                  Dong Zhang and
                  Xuan Liu and
                  Shuangxi Chen and
                  Chunming Wu},
	title = {Combination Attacks and Defenses on {SDN} Topology Discovery},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {904--919},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3203561},
	doi = {10.1109/TNET.2022.3203561},
	timestamp = {Wed, 25 Oct 2023 22:40:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KongSCCLZLCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The topology discovery service in Software-Defined Networking (SDN) provides the controller with a global view of the substrate network topology, allowing for central management of the entire network. Unfortunately, emerging topology attacks can poison the network topology and result in unforeseeable disasters. Although researchers have made great efforts to mitigate this problem, security hazards still exist. In this paper, we propose Invisible Assailant Attack (IAA), the first combination topology attack capable of injecting and maintaining fake links even when 12 existing defense strategies are deployed simultaneously. IAA consists of 14 attack phases that apply multiple attack strategies. Attackers skillfully disguise the attack traffic in each phase so that it looks like normal network traffic, and perform these phases in a well-planned sequence, thereby bypassing existing defenses step by step. To mitigate this attack, we propose a Route Path Verification (RPV) mechanism that orchestrates multiple defense strategies to identify fake links. According to the experiments, RPV can successfully detect IAA with low overhead: its detection completes within 1 ms while its per-flow storage consumption is only a few KB.}
}


@article{DBLP:journals/ton/HuangLTMMW23,
	author = {Haojun Huang and
                  Zhaoxi Li and
                  Jialin Tian and
                  Geyong Min and
                  Wang Miao and
                  Dapeng Oliver Wu},
	title = {Accurate Prediction of Required Virtual Resources via Deep Reinforcement
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {920--933},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3204790},
	doi = {10.1109/TNET.2022.3204790},
	timestamp = {Mon, 01 May 2023 13:02:25 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HuangLTMMW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Resource provisioning for the ever-increasing applications to host the necessary network functions necessitates the efficient and accurate prediction of required resources. However, the current efforts fail to leverage the inherent features hidden in network traffic, such as temporal stability, service correlation and periodicity, to predict the required resources in an intelligent manner, incurring coarse-grain prediction accuracies. To tackle this problem, in this paper, we propose an Accurate Prediction of Required virtual Resources (APRR) approach via Deep Reinforcement Learning (DRL). We first confirm the resource requests have more similar features and identify the high-dimensional required resources in computing, storage and bandwidth can be effectively consolidated into a single standardized value. Built upon these observations, we then model the required resources as a time-variant network matrix, which includes a number of elements, obtained from the network measurements, and some missing elements needed to be inferred. To obtain accurately predicted results, DRL-based matrix factorization with a set of available rules has been introduced into APRR and alternately executed in agent to minimize the prediction errors. Moreover, the error-prioritized designed for model training with quicker convergence. Simulation experiments on real-world datasets illustrate that APRR can accurately predict the required virtual resources compared with the related approaches.}
}


@article{DBLP:journals/ton/NamLPYS23,
	author = {Jaehyun Nam and
                  Seungsoo Lee and
                  Phillip A. Porras and
                  Vinod Yegneswaran and
                  Seungwon Shin},
	title = {Secure Inter-Container Communications Using XDP/eBPF},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {2},
	pages = {934--947},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3206781},
	doi = {10.1109/TNET.2022.3206781},
	timestamp = {Sat, 29 Apr 2023 19:27:01 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NamLPYS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While the use of containerization technologies for virtual application deployment has grown at an astonishing rate, the question of the robustness of container networking has not been well scrutinized from a security perspective, even though inter-container networking is indispensable for microservices. Thus, this paper first analyzes container networks from a security perspective, discussing the implications based on their architectural limitations. Then, it presents Bastion+, a secure inter-container communication bridge. Bastion+ introduces (\ni\n) a network security enforcement stack that provides fine-grained control per container application and securely isolates inter- container traffic in a point-to-point manner. Bastion+ also supports (\nii\n) selective security function chaining, enabling various security functions to be chained between containers for further security inspections (e.g., deep packet inspection) according to the container’s network context. Bastion+ incorporates (\niii\n) a security policy assistant that helps an administrator discover inter-container networking dependencies correctly. Our evaluation demonstrates how Bastion+ can effectively mitigate several adversarial attacks in container networks while improving the overall performance up to 25.4% within single-host containers and 17.7% for cross-host container communications.}
}


@article{DBLP:journals/ton/YangLDWRWWZ23,
	author = {Wei Yang and
                  Chi Lin and
                  Haipeng Dai and
                  Pengfei Wang and
                  Jiankang Ren and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Robust Wireless Rechargeable Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {949--964},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3199389},
	doi = {10.1109/TNET.2022.3199389},
	timestamp = {Fri, 24 Nov 2023 12:41:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangLDWRWWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless rechargeable sensor networks have become a hot research issue as it can overcome the limited energy bottleneck of wireless sensor networks owing to the recent breakthrough of wireless power transfer technology. Though network lifetime is prolonged and sensor nodes can sustain immortally, the issue of network robustness is overlooked, yielding most theoretical work unsuitable for practical applications when confronting with unpredictable packet loss. In this paper, we address the network robustness issue by maximizing the charging utility in a risk-averse view. First, we build a risk-averse model based on the concept of CVaR (Conditional Value at Risk), which trades-off charging utility and risk aversion for quantifying robustness. Then, we propose a spatial discretization scheme to construct a charging route for mobile charger, which can reduce computational overhead. Afterwards, a path optimization scheme is designed to further improve the charging utility. We convert the original problem into the submodular function maximization problem and propose a method with a performance guarantee while maximizing the system robustness. Finally, testbed experiments and simulations are conducted, and the results demonstrate that our schemes outperform comparison algorithms by at least 22.4% in effective energy in the presence of risks to guarantee system robustness.}
}


@article{DBLP:journals/ton/WangYL23,
	author = {Xiong Wang and
                  Jiancheng Ye and
                  John C. S. Lui},
	title = {Decentralized Scheduling and Dynamic Pricing for Edge Computing: {A}
                  Mean Field Game Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {965--978},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3204698},
	doi = {10.1109/TNET.2022.3204698},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangYL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Edge computing provides a platform facilitating edge servers to contribute to computation offloading while economizing their resources. Traditional offloading solutions are mostly centralized, which are unscalable for large-scale edge computing networks due to complex interactions among many edge servers. Meanwhile, dynamic pricing for an operator is equally, if not more, important to accommodate users’ time-varying demands for computing services. In this paper, we develop a decentralized online optimization framework to jointly minimize the server’s cost of workload scheduling while maximizing the operator’s utility of service pricing. Specifically, we employ the mean field game to model the collective scheduling behavior of all edge servers, thereby enabling optimal decision making only based on the server’s local information. Considering the service price in practice is not adjusted as frequently as the scheduling process, we establish a two-timescale optimization framework, where workload scheduling at a small timescale is tightly embedded into service pricing at a large timescale. Using mean field approximation, we derive the closed-form expression for the minimum scheduling cost, and the approximation error is\nO(\n1\nM\n√\n)\nwhich declines as the number of edge servers\nM\nincreases. By characterizing the influence of workload scheduling on dynamic pricing, we transform the complex service utility maximization into a succinct but equivalent problem, and thus we can make use of Lyapunov optimization to determine the optimal price over time. Extensive evaluations validate the effectiveness and optimality of our scheduling and pricing schemes.}
}


@article{DBLP:journals/ton/FaltelliBQPB23,
	author = {Marco Faltelli and
                  Giacomo Belocchi and
                  Francesco Quaglia and
                  Salvatore Pontarelli and
                  Giuseppe Bianchi},
	title = {Metronome: Adaptive and Precise Intermittent Packet Retrieval in {DPDK}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {979--993},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3208799},
	doi = {10.1109/TNET.2022.3208799},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FaltelliBQPB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing performance requirements of modern applications place a significant burden on software-based packet processing. Most of today’s software input/output accelerations achieve high performance at the expense of reserving CPU resources dedicated to continuously poll the Network Interface Card. This is specifically the case with DPDK (Data Plane Development Kit), probably the most widely used framework for software-based packet processing today. The approach presented in this paper, descriptively called Metronome, has the dual goals of providing CPU utilization proportional to the load, and allowing flexible sharing of CPU resources between I/O tasks and applications. Metronome replaces DPDK’s continuous polling with an intermittent sleep&wake mode, and revolves around a new multi-threaded operation, which improves service continuity. Since the proposed operation trades CPU usage with buffering delay, we propose an analytical model devised to dynamically adapt the sleep&wake parameters to the actual traffic load, meanwhile providing a target average latency. Our experimental results show a significant reduction of the CPU cycles, improvements in power usage, and robustness to CPU sharing even when challenged with CPU-intensive applications.}
}


@article{DBLP:journals/ton/ChenWZRNL23,
	author = {Haoxian Chen and
                  Chenyuan Wu and
                  Andrew Zhao and
                  Mukund Raghothaman and
                  Mayur Naik and
                  Boon Thau Loo},
	title = {Synthesizing Formal Network Specifications From Input-Output Examples},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {994--1009},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3208551},
	doi = {10.1109/TNET.2022.3208551},
	timestamp = {Tue, 07 May 2024 20:25:36 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenWZRNL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose NetSpec, a tool that synthesizes network specifications in a declarative logic programming language from input-output examples. NetSpec aims to accelerate the adoption of formal verification in networking practice, by reducing the effort and expertise required to specify network models or properties. NetSpec aims to be i) highly expressive, capable of synthesizing network specifications with complex semantics; ii) scalable, by virtue of using a novel best-first search algorithm to efficiently explore an unbounded solution space, and iii) robust, avoiding the need for exhaustive input-output examples by actively generating new examples. Our experiments demonstrate that NetSpec can synthesize a wide range of specifications used in network verification, analysis, and implementations. Furthermore, NetSpec improves upon existing approaches in terms of expressiveness, robustness to examples, and the quality of synthesized programs.}
}


@article{DBLP:journals/ton/DuHSCGW23,
	author = {Yang Du and
                  He Huang and
                  Yu{-}E Sun and
                  Shigang Chen and
                  Guoju Gao and
                  Xiaocan Wu},
	title = {Self-Adaptive Sampling Based Per-Flow Traffic Measurement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1010--1025},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3212066},
	doi = {10.1109/TNET.2022.3212066},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DuHSCGW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Per-flow traffic measurement in the high-speed network plays an important role in many practical applications. Due to the limited on-chip memory and the mismatch between off-chip memory speed and line rate, sampling-based methods select and forward a part of flow traffic to off-chip memory, which complements sketch-based solutions in estimation accuracy and online query support. However, most current work uses the same sampling probability for all flows, leading to the waste in storage and communication resources. In practice, different flows often require different sampling rates to meet the same accuracy constraint. This paper presents self-adaptive sampling, a framework to sample each flow with a probability adapted to flow size/spread. Then we propose three algorithms, SAS-LC, SAS-LOG, and SAS-HYB. SAS-LC and SAS-LOG are geared towards per-flow spread estimation and per-flow size estimation by using different compression functions. SAS-HYB combines the advantages of SAS-LC and SAS-LOG, showing higher efficiency when both small flows and large flows are interested. We implement our estimators in hardware using NetFPGA. Experimental results based on real Internet traces show that, compared to the state-of-the-art in per-flow spread estimation, SAS-LC can save around 10% on-chip space and reduce up to 40% communication cost for large flows. In per-flow size estimation, SAS-LOG can save 40% on-chip space and reduce up to 96% communication costs for large flows. Moreover, SAS-HYB’s on-chip memory usage will not be larger than SAS-LC or SAS-LOG and can save up to 19% on-chip space than SAS-LOG when both small flows and large flows are interested.}
}


@article{DBLP:journals/ton/LinGZLXHXXR23,
	author = {Feng Lin and
                  Ming Gao and
                  Lingfeng Zhang and
                  Yimin Li and
                  Weiye Xu and
                  Jinsong Han and
                  Xian Xu and
                  Wenyao Xu and
                  Kui Ren},
	title = {Mobile Communication Among {COTS} IoT Devices via a Resonant Gyroscope
                  With Ultrasound},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1026--1041},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3205151},
	doi = {10.1109/TNET.2022.3205151},
	timestamp = {Fri, 22 Mar 2024 09:02:03 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinGZLXHXXR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Incompatible protocols and electromagnetic interference obstruct the realization of an everything-connected Internet of Things (IoT) communication network. Our system, Deaf-Aid, utilizes a stealthy speaker-to-gyroscope channel to build robust communication. Compared with existing solutions adopting physical covert channels, Deaf-Aid is free from the limitations of manual receiver distinction, additional hardware, conditional placement, or physical contact. It exploits ultrasounds to force gyroscopes embedded in receivers to resonate, so as to convey information. We investigate the relationship among axes in a gyroscope to deal with frequency offset and support multi-channel communication. Meanwhile, receivers are identified automatically via device fingerprints consisting of diversity of gyroscopes’ resonant frequency ranges. Furthermore, we enable Deaf-Aid the capability of mobile communication, which is an essential demand for IoT devices. We address the challenge of recovering accurate signals from motion interference. Extensive evaluations, including that on the commercial off-the-shelf devices, demonstrate that Deaf-Aid yields 47 bps with BER below 1%. To our best knowledge, Deaf-Aid is the first work to enable stealthy mobile IoT communication based on inertial sensors.}
}


@article{DBLP:journals/ton/YuYL23,
	author = {Kan Yu and
                  Jiguo Yu and
                  Chuanwen Luo},
	title = {The Impact of Mobility on Physical Layer Security of 5G IoT Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1042--1055},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3208212},
	doi = {10.1109/TNET.2022.3208212},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuYL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet of Things (IoT) is rapidly spreading and reaching a multitude of different domains, since the fifth generation (5G) wireless technologies are the key enablers of many IoT applications. It is hence apparent that the broadcast nature of IoT devices makes data security unprecedentedly critical. Compared with traditional cryptography algorithms, which cannot cater for the features of IoT devices characterized by the severe limits in terms of energy, computation and storage capabilities, physical layer security (PLS) has been regarded as a promising solution to facilitate secure communications by exploiting the intrinsic randomness of the wireless medium. However, most of previous works assumed that all devices are static, and the impact of mobility on PLS deserves further investigation. In this paper, applying two types of random mobile models, i.e., the models of Random WayPoint (RWP) and Random Direction (RD), we study the impact of mobility on PLS in a scenario with three types of wireless devices (i.e., a destination, multiple interferers and an eavesdropper). Specifically, we establish an analytical framework for secrecy transmission capacity (STC), a fundamental metric in the study of PLS, under RWP and RD models. To the best of our knowledge, this is the first paper to derive STC and present the condition to achieve a positive STC with the consideration of mobility. We conclude that the RWP mobile destination can achieve a higher STC than that achievable in RD mobile and static scenarios, while RWP mobile eavesdropper is a challenging scenario to obtain a positive STC. Therefore, we propose an effective secrecy improvement strategy for the latter. Simulation validates the theoretical analyses.}
}


@article{DBLP:journals/ton/TehZCB23,
	author = {Min Yee Teh and
                  Shizhen Zhao and
                  Peirui Cao and
                  Keren Bergman},
	title = {Enabling Quasi-Static Reconfigurable Networks With Robust Topology
                  Engineering},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1056--1070},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3210534},
	doi = {10.1109/TNET.2022.3210534},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TehZCB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many optical circuit switched data center networks (DCN) have been proposed in the last decade to attain higher capacity and topology reconfigurability, though commercial adoption of these architectures have been minimal. One major challenge these architectures face is the difficulty of handling uncertain traffic demands using commercial optical circuit switches (OCS) with high switching latency. Prior works have generally focused on developing fast-switching OCS prototypes to quickly react to traffic variations through frequent reconfigurations. This approach, however, adds tremendous complexity overhead to the control plane, and raises the barrier for commercial adoption of optical circuit switched data center networks. We propose COUDER, a robust topology and routing optimization framework for reconfigurable optical circuit switched data centers. COUDER co-optimizes topology and routing based on a convex set of traffic matrices, and offers strict throughput guarantees for any future traffic matrices bounded by the convex set. For the bursty traffic demands that are unbounded by the convex set, we employ a desensitization technique to reduce performance hit. This enables COUDER to generate topology and routing solutions capable of handling unexpected traffic changes without relying on frequent topology reconfigurations. Our extensive evaluations based on Facebook’s production DCN traces show that, even with daily reconfigurations which could be realized by current commercial MEMS-based OCSs from Calient Technologies, COUDER achieves about 20% lower max link utilization, and about 32% lower average hop count compared to cost-equivalent static topologies. Our work shows that adoption of reconfigurable topologies in commercial DCNs is feasible even without fast OCSs.}
}


@article{DBLP:journals/ton/WangHLL23,
	author = {Yipeng Wang and
                  Huijie He and
                  Yingxu Lai and
                  Alex X. Liu},
	title = {A Two-Phase Approach to Fast and Accurate Classification of Encrypted
                  Traffic},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1071--1086},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3209979},
	doi = {10.1109/TNET.2022.3209979},
	timestamp = {Mon, 28 Aug 2023 21:30:55 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangHLL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Encryption technology has been widely used in today’s network communications. The early classification of encrypted flows is of great value to the control, allocation and management of resources in TCP/IP networks. In this paper, we propose TaTic, an early classification method for encrypted traffic, which aims to reduce the time spent observing the encrypted flows to be classified, and at the same time ensure the flow classification accuracy. TaTic is based on our key observation that the majority of encrypted flows can be classified accurately using only the first few packets, and we call such flows “easy flows”, whereas the rest of encrypted flows requires more packets for fine-grained analysis to achieve accurate traffic classification, and we call such flows “hard flows”. Given an encrypted flow, in the first phase, we use only the first few packets to quickly determine whether it is an easy flow or a hard flow; if it is an easy flow, we directly classify it in this phase; otherwise, we use more packets to perform traffic classification in the second phase. Therefore, we can greatly reduce the time spent in observing the flows without sacrificing the classification accuracy. Our experimental results show that TaTic can greatly reduce the unnecessary time spent in observing the flow to be classified, and at the same time ensure high classification accuracy. We compare our experimental results of TaTic with four existing methods. TaTic is superior to the existing methods in terms of both classification accuracy and average waiting time.}
}


@article{DBLP:journals/ton/ElsayedR23,
	author = {Karim Elsayed and
                  Amr Rizk},
	title = {Time-to-Live Caching With Network Delays: Exact Analysis and Computable
                  Approximations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1087--1100},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3207914},
	doi = {10.1109/TNET.2022.3207914},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ElsayedR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider Time-to-Live (TTL) caches that tag every object in cache with a specific (and possibly renewable) expiration time. State-of-the-art models for TTL caches assume zero object fetch delay, i.e., the time required to fetch a requested object that is not in cache from a different cache or the origin server. Particularly, in cache hierarchies this delay has a significant impact on performance metrics such as the object hit probability. Recent work suggests that the impact of the object fetch delay on the cache performance will continue to increase due to the scaling mismatch between shrinking inter-request times (due to higher data center link rates) in contrast to processing and memory access times. In this paper, we analyze tree-based cache hierarchies with random object fetch delays and provide an exact analysis of the corresponding object hit probability. Our analysis allows understanding the impact of random delays and TTLs on cache metrics for a wide class of request stream models characterized through Markov arrival processes. This is expressed through a metric that we denote delay impairment of the hit probability. In addition, we analyze and extend state-of-the-art approximations of the hit probability to take the delay into account. We provide numerical and trace-based simulation-based evaluation results showing that larger TTLs do not efficiently compensate the detrimental effect of object fetch delays. Our evaluations also show that unlike our exact model the state-of-the-art approximations do not capture the impact of the object fetch delay well especially for cache hierarchies. Surprisingly, we show for single caches that the impact of the delay on the hit probability can be non-monotonic and that the range of delays for which a positive effect exists arises as a root of a polynomial in the ratio of the expected TTL to the expected inter-request time.}
}


@article{DBLP:journals/ton/LuYZLH23,
	author = {Xiaofeng Lu and
                  Fan Yang and
                  Luwen Zou and
                  Pietro Li{\`{o}} and
                  Pan Hui},
	title = {An {LTE} Authentication and Key Agreement Protocol Based on the {ECC}
                  Self-Certified Public Key},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1101--1116},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3207360},
	doi = {10.1109/TNET.2022.3207360},
	timestamp = {Thu, 27 Jul 2023 08:18:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LuYZLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {After analyzing the long-term evolution (LTE) authentication and key agreement process (EPS-AKA), its existing security vulnerabilities are pointed out. Based on elliptic curve cryptography (ECC) self-certified public keys, this paper proposes an ECC self-certified authentication key agreement scheme (ESC-AKA). This scheme includes the addition of a trusted center (TC), which generates the public keys for the home subscriber server (HSS), the mobility management entity (MME), and the user equipment (UE). Three communication protocols are designed, including MME/HSS registration, UE registration, and UE access. A strand space model is used to carry out the formal analysis, and performance and security analyses are carried out. The results show that this scheme can compensate for the security vulnerabilities of the original EPS-AKA scheme. It implements the encrypted transmission of the international mobile subscriber identity (IMSI), and realizes the mutual authentication between the HSS and MME, the MME and UE, and the HSS and UE. Because the self-certified public key cryptosystem is adopted in this scheme, communication encryption is ensured, and the risk of the TC simultaneously mastering the public and private keys is avoided. This scheme is proven to be effective in protecting the communication security of the LTE network.}
}


@article{DBLP:journals/ton/LiuCZHLR23,
	author = {Jianwei Liu and
                  Kaiyan Cui and
                  Xiang Zou and
                  Jinsong Han and
                  Feng Lin and
                  Kui Ren},
	title = {Reliable Multi-Factor User Authentication With One Single Finger Swipe},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1117--1131},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3208002},
	doi = {10.1109/TNET.2022.3208002},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuCZHLR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multi-factor user authentication becomes increasingly popular due to its superior security comparing with single-factor user authentication. However, existing multi-factor user authentication methods usually require multiple interactions between users and different authentication components when inputting the multiple factors, leading to extra overhead and bad user experience. In this paper, we propose a secure and user-friendly multi-factor user authentication system named BioDraw. It utilizes four categories of biometrics (impedance, geometry, behavior, and composition) of human hand plus the pattern-based password to identify and authenticate users. User only needs to draw a pattern on a radio frequency identification tag array, while four biometrics can be collected simultaneously. Specifically, we first design a gradient-based pattern recognition algorithm to precisely extract user’s secret pattern. Then, a convolutional neural network- and long short-term memory-based classifier is utilized for user recognition. Furthermore, to guarantee the systemic security, an anti-replay method called Binary ALOHA is proposed to detect replayed signals. We conduct extensive experiments with 30 volunteers. The experiment results show that BioDraw can achieve high authentication accuracy (with a 2%– false reject rate) and is effective in defending against various attacks.}
}


@article{DBLP:journals/ton/YuanWZG23,
	author = {Longzhi Yuan and
                  Qiwei Wang and
                  Jia Zhao and
                  Wei Gong},
	title = {Multiprotocol Backscatter With Commodity Radios for Personal IoT Sensors},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1132--1144},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3213913},
	doi = {10.1109/TNET.2022.3213913},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YuanWZG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We present multiscatter, a novel battery-free backscatter design that can simultaneously work with multiple excitation signals for personal IoT sensors. Specifically, we show for the first time that the backscatter tag can identify various excitation signals in an ultra-low-power way, including WiFi, Bluetooth, and ZigBee. Further, we employ a new modulation approach, overlay modulation, that can leverage those excitation signals to convey tag data on top of productive data, which makes decoding both data possible with only a single personal radio. Moreover, we introduce a low-power listening scheme to improve energy efficiency. Since 2.4 GHz signals and personal radios are everywhere, multiscatter is readily deployable in our everyday IoT applications. We prototype multiscatter using an FPGA and various commodity radios. Extensive experiments show that for mixed 802.11b&n, Bluetooth and ZigBee signals, the average identification accuracy of four protocols is more than 93%. The maximal aggregate throughput of both productive and tag data is 278.4 kbps with a single Bluetooth radio. When the transmitter-to-tag distance is increased from 0.2 to 1.8 m, the maximal communication for BLE drops from 71 m to 29 m. And it can leverage excitation diversity to provide uninterrupted communication and greater throughput gains, whereas the single-protocol tag being idle when carrier signals are unavailable. With indoor office light as harvesting sources, the low-power listening scheme can support backscatter rate at 12 pkts/s.}
}


@article{DBLP:journals/ton/LiuHLWH23,
	author = {Jingling Liu and
                  Jiawei Huang and
                  Weihe Li and
                  Jianxin Wang and
                  Tian He},
	title = {Asymmetry-Aware Load Balancing With Adaptive Switching Granularity
                  in Data Center},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1145--1158},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3208201},
	doi = {10.1109/TNET.2022.3208201},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuHLWH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Datacenter networks provide large bisection bandwidth by load balancing traffic over rich parallel paths in multi-rooted tree topologies. Nevertheless, production datacenters operate under various path diversities caused by traffic dynamics, hardware failures and heterogeneous switching equipment. Therefore, the load balancing schemes in data center should be resilient to network asymmetry. Prior fine-grained schemes such as RPS and Presto are prone to experience packet reordering problem under asymmetric topology since they split flows into small units which are spread across all parallel paths. The coarse-grained solutions such as ECMP and LetFlow effectively avoid packet reordering, but easily leading to under-utilization of multiple paths. To solve these problems, we propose a load balancing mechanism called AG, which adaptively adjusts switching granularity according to the asymmetric degree of multiple paths. AG increases switching granularity to alleviate packet reordering under large degrees of topology asymmetry, while reducing switching granularity to obtain high link utilization under small degrees of topology asymmetry. Moreover, we design a switch-based scheme which measures the difference of one-way delay of multiple paths to obtain accurate state of topology asymmetry with low overhead. AG is a practical switch-based solution without modification at end hosts. The experimental results of NS2 simulations and real implementation show that AG reduces the average and 99^{th}\nflow completion time by up to 54% and 65% compared with the state-of-the-art load balancing schemes, respectively.}
}


@article{DBLP:journals/ton/HouXZ23,
	author = {Ningning Hou and
                  Xianjin Xia and
                  Yuanqing Zheng},
	title = {CloakLoRa: {A} Covert Channel Over LoRa {PHY}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1159--1172},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3209255},
	doi = {10.1109/TNET.2022.3209255},
	timestamp = {Fri, 07 Jul 2023 23:32:42 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HouXZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper describes our design and implementation of a covert channel over LoRa physical layer (PHY). LoRa adopts a unique modulation scheme (chirp spread spectrum (CSS)) to enable long range communication at low-power consumption. CSS uses the initial frequencies of LoRa chirps to differentiate LoRa symbols, while simply ignoring other RF parameters (e.g., amplitude and phase). Our study reveals that the LoRa physical layer leaves sufficient room to build a covert channel by embedding covert information with a modulation scheme orthogonal to CSS. To demonstrate the feasibility of building a covert channel, we implement CloakLoRa . CloakLoRa embeds covert information into a regular LoRa packet by modulating the amplitudes of LoRa chirps while keeping the frequency intact. As amplitude modulation is orthogonal to CSS, a regular LoRa node receives the LoRa packet as if no secret information is embedded into the packet. Such an embedding method is transparent to all security mechanisms at upper layers in current LoRaWAN. As such, an attacker can create an amplitude modulated covert channel over LoRa without being detected by current LoRaWAN security mechanism. We conduct comprehensive evaluations with COTS LoRa nodes and receive-only software defined radios and experiment results show that CloakLoRa can send covert information over 250 m.}
}


@article{DBLP:journals/ton/SalemNC23,
	author = {Tareq Si Salem and
                  Giovanni Neglia and
                  Damiano Carra},
	title = {Ascent Similarity Caching With Approximate Indexes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1173--1186},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3217012},
	doi = {10.1109/TNET.2022.3217012},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SalemNC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Similarity search is a key operation in multimedia retrieval systems and recommender systems, and it will play an important role also for future machine learning and augmented reality applications. When these systems need to serve large objects with tight delay constraints, edge servers close to the end-user can operate as similarity caches to speed up the retrieval. In this paper we present AÇAI, a new similarity caching policy which improves on the state of the art by using (i) an (approximate) index for the whole catalog to decide which objects to serve locally and which to retrieve from the remote server, and (ii) a mirror ascent algorithm to update the set of local objects with strong guarantees even when the request process does not exhibit any statistical regularity.}
}


@article{DBLP:journals/ton/DoshiME23,
	author = {Vishwaraj Doshi and
                  Shailaja Mallick and
                  Do Young Eun},
	title = {Convergence of Bi-Virus Epidemic Models With Non-Linear Rates on Networks
                  - {A} Monotone Dynamical Systems Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1187--1201},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3213015},
	doi = {10.1109/TNET.2022.3213015},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DoshiME23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study convergence properties of competing epidemic models of the Susceptible-Infected-Susceptible (\nSIS\n) type. The SIS epidemic model has seen widespread popularity in modelling the spreading dynamics of contagions such as viruses, infectious diseases, or even rumors/opinions over contact networks (graphs). We analyze the case of two such viruses spreading on overlaid graphs, with non-linear rates of infection spread and recovery. We call this the non-linear bi-virus model and, building upon recent results, obtain precise conditions for global convergence of the solutions to a trichotomy of possible outcomes: a virus-free state, a single-virus state, and to a coexistence state. Our techniques are based on the theory of monotone dynamical systems (MDS), in contrast to Lyapunov based techniques that have only seen partial success in determining convergence properties in the setting of competing epidemics. We demonstrate how the existing works have been unsuccessful in characterizing a large subset of the model parameter space for bi-virus epidemics, including all scenarios leading to coexistence of the epidemics. To the best of our knowledge, our results are the first in providing complete convergence analysis for the bi-virus system with non-linear infection and recovery rates on general graphs.}
}


@article{DBLP:journals/ton/HurSKY23,
	author = {JunNyung Hur and
                  Hyeon Gy Shon and
                  Young Jae Kim and
                  Myungkeun Yoon},
	title = {Packet Chunking for File Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1202--1215},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3215549},
	doi = {10.1109/TNET.2022.3215549},
	timestamp = {Tue, 15 Aug 2023 15:43:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HurSKY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network-based intrusion detection and data leakage prevention systems inspect packets to detect if critical files such as malware or confidential documents are transferred. However, this kind of detection requires heavy computing resources in reassembling packets and only well-known protocols can be interpreted. Besides, finding similar files from a storage requires pairwise comparisons. In this paper, we present a new network-based file identification scheme that inspects packets independently without reassembly and finds similar files through inverted indexing instead of pairwise comparison. We use a content-based chunking algorithm to consistently divide both files and packets into multiple byte sequences, called chunks. If a packet is a part of a file, they would have common chunks. The challenging problem is that packet chunking and inverted-index search should be fast and scalable enough for packet processing. The file identification should be accurate although many chunks are noises. In this paper, we use a small Bloom filter and a two-level threshold strategy to solve the problems. To the best of our knowledge, this is the first scheme that identifies a specific critical file from a packet over unknown protocols. Experimental results show that the proposed scheme can successfully identify a critical file from a packet without packet reassembly.}
}


@article{DBLP:journals/ton/XuXWLJXZWC23,
	author = {Wenzheng Xu and
                  Hongbin Xie and
                  Chenxi Wang and
                  Weifa Liang and
                  Xiaohua Jia and
                  Zichuan Xu and
                  Pan Zhou and
                  Weigang Wu and
                  Xiang Chen},
	title = {An Approximation Algorithm for the h-Hop Independently Submodular
                  Maximization Problem and Its Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1216--1229},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3210825},
	doi = {10.1109/TNET.2022.3210825},
	timestamp = {Thu, 20 Jun 2024 15:06:44 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XuXWLJXZWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This study is motivated by the maximum connected coverage problem (MCCP), which is to deploy a connected UAV network with given K\nUAVs in the top of a disaster area such that the number of users served by the UAVs is maximized. The deployed UAV network must be connected, since the received data by a UAV from its served users need to be sent to the Internet through relays of other UAVs. Motivated by this application, in this paper we study a more generalized problem – the h\n-hop independently submodular maximization problem, where the MCCP problem is one of its special cases with h=4\n. We propose a \\frac {1-1/e}{2h+3}\n-approximation algorithm for the h\n-hop independently submodular maximization problem, where e\nis the base of the natural logarithm. Then, one direct result is a \\frac {1-1/e}{11}\n-approximate solution to the MCCP problem with h=4\n, which significantly improves its currently best \\frac {1-1/e}{32}\n-approximate solution. We finally evaluate the performance of the proposed algorithm for the MCCP problem in the application of deploying UAV networks, and experimental results show that the number of users served by deployed UAVs delivered by the proposed algorithm is up to 12.5% larger than those by existing algorithms.}
}


@article{DBLP:journals/ton/RashelbachRS23,
	author = {Alon Rashelbach and
                  Ori Rottenstreich and
                  Mark Silberstein},
	title = {Scaling by Learning: Accelerating Open vSwitch Data Path With Neural
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1230--1243},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3215143},
	doi = {10.1109/TNET.2022.3215143},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/RashelbachRS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Open vSwitch (OVS) is a widely used open-source virtual switch implementation. In this work, we seek to scale up OVS to support hundreds of thousands of OpenFlow rules by accelerating the core component of its data-path - the packet classification mechanism. To do so we use NuevoMatch, a recent algorithm that uses neural network inference to match packets, and promises significant scalability and performance benefits. We overcome the primary algorithmic challenge of the slow training rate in the vanilla NuevoMatch, speeding it up by over three orders of magnitude. This improvement enables two design options to integrate NuevoMatch with OVS: (1) as an extra caching layer in front of OVS’s megaflow cache, and (2) using it to completely replace OVS’s data-path while performing classification directly on OpenFlow rules, and obviating control-path upcalls. Comprehensive evaluation on real-world packet traces and ClassBench rules demonstrates geometric mean speedups of\n1.9×\nand\n12.3×\nfor the first and second designs, respectively, for 500K rules, with the latter also supporting up to 60K OpenFlow rule updates/second, by far exceeding the original OVS.}
}


@article{DBLP:journals/ton/MeskarL23,
	author = {Erfan Meskar and
                  Ben Liang},
	title = {Fair Multi-Resource Allocation in Heterogeneous Servers With an External
                  Resource Type},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1244--1262},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3213426},
	doi = {10.1109/TNET.2022.3213426},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MeskarL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper considers the problem of fair allocation of multiple types of resources in heterogeneous servers, along with a resource type external to those servers. Our work is motivated by the need for fair multi-resource allocation in mobile edge computing (MEC), where the users must upload their tasks over a single dedicated wireless communication link that exists outside the computing servers. We propose a fair multi-resource allocation mechanism for this environment, termed Task Share Fairness with External Resource (TSF-ER), which finds the Kalai-Smorodinsky bargaining solution satisfying important fairness properties. We show that TSF-ER is envy-free, Pareto optimal, and strategy-proof, and it satisfies the property of sharing incentive. Large-scale simulation driven by Google and Alibaba cluster trace further shows that TSF-ER significantly outperforms the existing utilitarian, Nash social welfare maximizer, and egalitarian solutions, leading to fairer resource allocation while maintaining a high level of resource utilization.}
}


@article{DBLP:journals/ton/KalorP23,
	author = {Anders E. Kal{\o}r and
                  Petar Popovski},
	title = {Timely Monitoring of Dynamic Sources With Observations From Multiple
                  Wireless Sensors},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1263--1276},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3212794},
	doi = {10.1109/TNET.2022.3212794},
	timestamp = {Fri, 07 Jul 2023 23:32:42 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KalorP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Age of Information (AoI) has recently received much attention due to its relevance for IoT sensing and monitoring. In this paper, we consider the problem of minimizing the AoI in a system in which a set of sources are observed by multiple sensors in a many-to-many relationship, and the probability that a sensor observes a source depends on the source’s state. This model represents many practical scenarios, such as when multiple cameras or microphones are deployed to monitor objects moving in certain areas. We formulate the scheduling problem as a Markov Decision Process, and show how the age-optimal scheduling policy can be obtained. We further consider partially observable variants of the problem, and devise approximate policies for large state spaces. The evaluations show that the approximate policies work well in the considered scenarios, while the fact that sensors can observe multiple sources is beneficial, especially when there is high uncertainty of the source states.}
}


@article{DBLP:journals/ton/SerbetciLSCE23,
	author = {Berksan Serbetci and
                  Eleftherios Lampiris and
                  Thrasyvoulos Spyropoulos and
                  Giuseppe Caire and
                  Petros Elia},
	title = {Multi-Transmitter Coded Caching Networks With Transmitter-Side Knowledge
                  of File Popularity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1277--1292},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3219161},
	doi = {10.1109/TNET.2022.3219161},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SerbetciLSCE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work presents a new way of exploiting non-uniform file popularity in coded caching networks. Focusing on a fully-connected fully-interfering wireless setting with multiple cache-enabled transmitters and receivers, we show how non-uniform file popularity can be used very efficiently to accelerate the impact of transmitter-side data redundancy on receiver-side coded caching. This approach is motivated by the recent discovery that, under any realistic file-size constraint, having content appear in multiple transmitters can in fact dramatically boost the speed-up factor attributed to coded caching. We formulate an optimization problem that exploits file popularity to optimize the placement of files at the transmitters. Consequently, we propose a search algorithm that solves the problem at hand while reducing the variable search space significantly. We also prove an analytical performance upper bound, which is in fact met by our algorithm in the regime of many receivers. Our work reflects the benefits of allocating higher cache redundancy to more popular files, but also reflects a law of diminishing returns where for example very popular files may in fact benefit from minimum redundancy. In the end, this work reveals that in the context of coded caching, employing multiple transmitters can be a catalyst in fully exploiting file popularity, as it avoids various asymmetry complications that appear when file popularity is used to alter the receiver-side cache placement.}
}


@article{DBLP:journals/ton/SalaniRCT23,
	author = {Matteo Salani and
                  Cristina Rottondi and
                  Leopoldo Cer{\'{e}} and
                  Massimo Tornatore},
	title = {Dual-Stage Planning for Elastic Optical Networks Integrating Machine-Learning-Assisted
                  QoT Estimation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1293--1307},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3213970},
	doi = {10.1109/TNET.2022.3213970},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SalaniRCT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Following the emergence of Elastic Optical Networks (EONs), Machine Learning (ML) has been intensively investigated as a promising methodology to address complex network management tasks, including, e.g., Quality of Transmission (QoT) estimation, fault management, and automatic adjustment of transmission parameters. Though several ML-based solutions for specific tasks have been proposed, how to integrate the outcome of such ML approaches inside Routing and Spectrum Assignment (RSA) models (which address the fundamental planning problem in EONs) is still an open research problem. In this study, we propose a dual-stage iterative RSA optimization framework that incorporates the QoT estimations provided by a ML regressor, used to define lightpaths’ reach constraints, into a Mixed Integer Linear Programming (MILP) formulation. The first stage minimizes the overall spectrum occupation, whereas the second stage maximizes the minimum inter-channel spacing between neighbor channels, without increasing the overall spectrum occupation obtained in the previous stage. During the second stage, additional interference constraints are generated, and these constraints are then added to the MILP at the next iteration round to exclude those lightpaths combinations that would exhibit unacceptable QoT. Our illustrative numerical results on realistic EON instances show that the proposed ML-assisted framework achieves spectrum occupation savings up to 52.4% (around 33% on average) in comparison to a traditional MILP-based RSA framework that uses conservative reach constraints based on margined analytical models.}
}


@article{DBLP:journals/ton/GaoCD23,
	author = {Shuqin Gao and
                  Costas Courcoubetis and
                  Lingjie Duan},
	title = {Distributed Double Auction Mechanisms for Large-Scale Device-to-Device
                  Resource Trading},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1308--1323},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3218552},
	doi = {10.1109/TNET.2022.3218552},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GaoCD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While some mobile users in wireless networks may experience temporal scarcity of wireless network resources such as data plan, computation capacity and energy storage, some others may leave them underutilized. If the appropriate market existed, users connected locally with D2D links could exchange such resources with low communication cost and realize significant efficiency gains by reducing waste and achieving resource pooling. This paper proposes such a D2D trading market that scales for large numbers of users. Contrary to traditional resource allocation solutions that are mostly centralized, our double auction mechanism exploits local D2D connectivity and uses distributed computation to achieve near-optimal allocative efficiency. The final prices for each matched pair of buyer and seller are adjusted in a way to induce incentive compatibility and depend on their own declarations in terms of quantity and valuation. We prove that the overall mechanism has significant social welfare gains compared to other widely-used distributed pricing mechanisms. It is also individually rational, ex-ante budget balanced using a subscription fee, and robust to perturbations of the model parameters. To render the system fully manipulation-proof, we further propose a distributed auditing scheme that prevents users from altering the decentralized computation to increase their profits. Finally, we model the repeated execution of the mechanism and determine the best trading frequency by taking into account the arrivals and departures of new participants.}
}


@article{DBLP:journals/ton/BressanaZS23,
	author = {Pietro Bressana and
                  Noa Zilberman and
                  Robert Soul{\'{e}}},
	title = {{PTA:} Finding Hard-to-Find Data Plane Bugs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1324--1337},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3214062},
	doi = {10.1109/TNET.2022.3214062},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BressanaZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bugs in network hardware can cause tremendous problems. However, programmable network devices have the potential to provide greater visibility into the internal behavior of devices, allowing us to more quickly find and identify problems. In this paper, we provide a taxonomy of data plane bugs, and use the taxonomy to derive a Portable Test Architecture (PTA) which offers essential abstractions for testing on a variety of network hardware devices. PTA is implemented with a novel data plane design that (i) separates target-specific from target-independent components, allowing for portability, and (ii) allows users to write a test program once at compile time, but dynamically alter the behavior via runtime configuration. We report 12 diverse bugs on different hardware targets, and their associated software, exposed using PTA.}
}


@article{DBLP:journals/ton/FelembanMP23,
	author = {Noor Felemban and
                  Fidan Mehmeti and
                  Thomas F. La Porta},
	title = {VidQ: Video Query Using Optimized Audio-Visual Processing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1338--1352},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3215601},
	doi = {10.1109/TNET.2022.3215601},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FelembanMP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As mobile devices become more prevalent in everyday life and the amount of recorded and stored videos increases, efficient techniques for searching video content become more important. When a user sends a query searching for a specific action in a large amount of data, the goal is to respond to the query accurately and fast. In this paper, we address the problem of responding to queries which search for specific actions in mobile devices in a timely manner by utilizing both visual and audio processing approaches. We build a system, called VidQ, which consists of several stages, and that uses various Convolutional Neural Networks (CNNs) and Speech APIs to respond to such queries. As the state-of-the-art computer vision and speech algorithms are computationally intensive, we use servers with GPUs to assist mobile users in the process. After a query is issued, we identify the different stages of processing that will take place. Then, we identify the order of these stages. Finally, solving an optimization problem that captures the system behavior, we distribute the process among the available network resources to minimize the processing time. Results show that VidQ reduces the completion time by at least 50% compared to other approaches.}
}


@article{DBLP:journals/ton/PingWLC23,
	author = {Haodi Ping and
                  Yongcai Wang and
                  Deying Li and
                  Wenping Chen},
	title = {Understanding Node Localizability in Barycentric Linear Localization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1353--1368},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3216204},
	doi = {10.1109/TNET.2022.3216204},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PingWLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The barycentric linear localization (BLL) methods provide a lightweight, distributed way to calculate locations for resource-limited IoT devices. A crucial requirement for BLL is that the nodes participating in the iterative location propagation are localizable. Otherwise, the unlocalizable nodes will continuously pose error information in the location propagation process, making even the theoretically localizable nodes converge to the wrong locations. However, the research on node localizability in BLL is much lacked, greatly limiting the application scope of BLL. In specific, BLL node localizability is detected on a generated graph $\\mathcal {G^{A}}$ . For any node, its neighbors appear in $\\mathcal {G^{A}}$ only when the neighbors can form triangle(s), so that $\\mathcal {G^{A}}$ is much sparser than the original $\\mathcal G$ . Thus, the node localizability condition in BLL is harder to be satisfied than that in traditional localization methods. Moreover, the distributed algorithm to detect BLL localizable nodes is still open. This paper thoroughly investigates the node localizability conditions and distributed localizable node detection algorithms in BLL. At first, an efficient and fully distributed Negative Edge Inference (NEI) algorithm is proposed for each node to infer implicit edges in its neighborhood. NEI strengthens the distance graph by revealing more distance constraints so that enables more neighboring triangles. Then a new sufficient condition, i.e., the recursive three disjoint path condition (Recursive-3DP) on the strengthened distance graph is proposed to identify BLL localizable nodes much more accurately. Secondly, a distributed Path Extension and Pruning (PEP) algorithm is proposed for distributed localizable node detection. PEP is proved to detect all the theoretically Recursive-3DP nodes in the strengthened distance graph. A Fast-PEP algorithm is further proposed, which misses very limited Recursive-3DP nodes while bringing significant improvement in efficiency. PEP and Fast-PEP guarantee to identify BLL localizable nodes in $2H$ rounds, where $H$ is the maximum hop number of the node disjoint paths. Finally, by using NEI and PEP (Fast-PEP), a localizability-aware BLL (LABEL) method is proposed, which correctly identifies localizable nodes and guarantees their correct location convergence. Extensive analysis and experiments show the advantages in localizability and location accuracy of the proposed schemes over the state-of-the-art methods.}
}


@article{DBLP:journals/ton/LinYHLX23,
	author = {Peng Lin and
                  Kejiang Ye and
                  Yishen Hu and
                  Yanying Lin and
                  Cheng{-}Zhong Xu},
	title = {A Novel Multimodal Deep Learning Framework for Encrypted Traffic Classification},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1369--1384},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3215507},
	doi = {10.1109/TNET.2022.3215507},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LinYHLX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic classification is essential for cybersecurity maintenance and network management, and has been widely used in QoS (Quality of Service) guarantees, intrusion detection, and other tasks. Recently, with the emergence of SSL/TLS encryption protocols in the modern Internet environment, the traditional payload-based classification methods are no longer effective. Some researchers have used machine learning methods to model the flow features of encrypted traffics (e.g. message type, length sequence, statistical features, etc.), and achieved good results in some cases. However, these high-level hand-designed features cannot be used for more fine-grained operations and may lead to the loss of important information, thus affecting the classification accuracy. To overcome this limitation, in this paper, we designed a novel multimodal deep learning framework for encrypted traffic classification called PEAN. PEAN uses the raw bytes and length sequence as the input, and uses the self-attention mechanism to learn the deep relationship among network packets in a biflow. Furthermore, unsupervised pre-training was introduced to enhance PEAN’s ability to characterize network packets. Experiments on a real trace set captured in a large data center demonstrate the effectiveness of PEAN, which achieves better results than the state-of-the-art methods.}
}


@article{DBLP:journals/ton/LiZBSW23,
	author = {Wenhao Li and
                  Xiaoyu Zhang and
                  Huaifeng Bao and
                  Haichao Shi and
                  Qiang Wang},
	title = {ProGraph: Robust Network Traffic Identification With Graph Propagation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1385--1399},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3216603},
	doi = {10.1109/TNET.2022.3216603},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiZBSW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network traffic identification is critical for effective network management. Existing methods mostly focus on invariant network environments with stable attribute distributions. Unfortunately, however, they can hardly be adaptive to the variation of practical networks and suffer from significant performance degradation. This problem largely stems from the over-dependence of existing methods on the vulnerable side-channel features. To address this issue, in this paper we propose a graph-based approach, namely ProGraph, to ensure robust network traffic classification among various network environments. The core idea of ProGraph is to construct a correlation graph with session clusters aggregated from different networks, based on which graph propagation can be effectively implemented to predict labels of testing nodes in an iterative manner. ProGraph enhances the correlation between clusters of the same class to provide reliable paths for label dissemination from the labeled clusters to the testing ones. It is encouraging to see that the proposed ProGraph achieves an accuracy of 92.25% in networks with constant attributes, while remaining stable with the accuracy of 90.89% when deployed in different networks, which significantly outperforms the state-of-the-art approaches. Meanwhile, ProGraph can accurately identify the novel classes which do not exist in the training dataset, with an AUC of 95.11. Last but not least, a carefully constructed dataset, namely CrossNet2021, containing network traffic of 20 classes of applications from two distinct networking scenarios, is made publicly available to support further research.}
}


@article{DBLP:journals/ton/ChenLHZZWLY23,
	author = {Xiang Chen and
                  Hongyan Liu and
                  Qun Huang and
                  Dong Zhang and
                  Haifeng Zhou and
                  Chunming Wu and
                  Xuan Liu and
                  Qiang Yang},
	title = {Toward Low-Latency and Accurate State Synchronization for Programmable
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1400--1415},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3218446},
	doi = {10.1109/TNET.2022.3218446},
	timestamp = {Fri, 08 Sep 2023 07:53:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenLHZZWLY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Programmable switches empower stateful packet processing, in which incoming packets continuously update states in the data plane, while applications in the control plane read and write states. However, since the data plane and control plane are separated, a consistent view of states in both planes is required for stateful packet processing. Existing approaches suffer from either high latency or low accuracy. In this paper, we propose ApproSync, a framework that offers approximate state synchronization with low latency and high accuracy. To achieve low latency, ApproSync directly transfers states between switch ASICs and the control plane by bypassing switch operating systems. To achieve high accuracy, ApproSync utilizes the resources in the switch ASIC to realize rate control in state synchronization, such that it avoids potential state loss. It also bounds the divergence between the states in the data plane and that in the control plane under limited link capacity. We prototype ApproSync on Barefoot Tofino switches. The experimental results indicate that compared to existing approaches, ApproSync achieves order-of-magnitude latency reduction while maintaining high accuracy of state synchronization. Also, our experiments demonstrate that ApproSync provides significant latency benefits to existing network management applications and well preserves high application-level accuracy.}
}


@article{DBLP:journals/ton/CaoXLSY23,
	author = {Jiahao Cao and
                  Mingwei Xu and
                  Qi Li and
                  Kun Sun and
                  Yuan Yang},
	title = {The {LOFT} Attack: Overflowing {SDN} Flow Tables at a Low Rate},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {3},
	pages = {1416--1431},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3225211},
	doi = {10.1109/TNET.2022.3225211},
	timestamp = {Fri, 07 Jul 2023 23:32:41 +0200},
	biburl = {https://dblp.org/rec/journals/ton/CaoXLSY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The emerging Software-Defined Networking (SDN) is being adopted by data centers and cloud service providers to enable flexible control. Meanwhile, the current SDN design brings new vulnerabilities. In this paper, we explore a stealthy attack that uses a minimum rate of attack packets to disrupt SDN data plane. To achieve this, we propose the LOFT attack that computes the lower bound of attack rate to overflow flow tables based on the inferred network configurations. Particularly, each attack packet always triggers or maintains consumption of one flow rule. LOFT can ensure the attack effect under various network configurations while reducing the possibility of being captured. We demonstrate its feasibility and effectiveness in a real SDN testbed consisting of commercial hardware switches. The experimental results show that LOFT incurs significant network performance degradation and potential network DoS at an attack rate of only tens of Kbps. To defeat the attack, we develop a data-to-control plane collaborative defense system named LOFTGuard, which is lightweight and transparent to SDN applications. Evaluations show that LOFTGuard effectively protects SDN against the attack and introduces a small overhead.}
}


@article{DBLP:journals/ton/YeZGC23,
	author = {Minghao Ye and
                  Junjie Zhang and
                  Zehua Guo and
                  H. Jonathan Chao},
	title = {FlexDATE: Flexible and Disturbance-Aware Traffic Engineering With
                  Reinforcement Learning in Software-Defined Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1433--1448},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3217083},
	doi = {10.1109/TNET.2022.3217083},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YeZGC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Traffic Engineering (TE) is an important network operation that routes/reroutes flows based on network topology and traffic demands to optimize network performance. Recently, new emerging applications pose challenges to TE with dynamic network conditions, where frequent routing updates are required to maintain good network performance with Software-Defined Networking (SDN). However, flow rerouting operations could lead to considerable Quality of Service (QoS) degradation and service disruption, which is often neglected by existing TE solutions. In this paper, we apply a new QoS metric named network disturbance to measure the negative impact of flow rerouting operations performed by TE. To achieve near-optimal load balancing performance and mitigate network disturbance together in dynamic network scenarios, we propose a flexible and disturbance-aware TE solution called FlexDATE that combines Reinforcement Learning (RL) and Linear Programming (LP). Specifically, FlexDATE leverages RL to intelligently identify flexible numbers of critical flows for each traffic matrix and reroutes these critical flows based on LP optimization to improve network performance with low disturbance. Empowered by a customized actor-critic architecture coupled with Graph Neural Networks (GNNs), FlexDATE can generalize well to unseen traffic scenarios and remain resilient to single link failures. Extensive simulations are conducted on five real-world network topologies to evaluate FlexDATE with real and synthetic traffic traces. The results show that FlexDATE can achieve the performance target (i.e., 90% of optimal performance) in 99% of network scenarios and effectively mitigate the average and maximum network disturbance by up to 9.1% and 38.6%, respectively, compared to state-of-the-art TE solutions.}
}


@article{DBLP:journals/ton/ZhangWRHHZ23,
	author = {Yongmin Zhang and
                  Wei Wang and
                  Ju Ren and
                  Jinge Huang and
                  Shibo He and
                  Yaoxue Zhang},
	title = {Efficient Revenue-Based {MEC} Server Deployment and Management in
                  Mobile Edge-Cloud Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1449--1462},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3217280},
	doi = {10.1109/TNET.2022.3217280},
	timestamp = {Mon, 25 Mar 2024 12:48:07 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWRHHZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the explosive growth of mobile applications, the development of mobile edge computing (MEC) has been greatly promoted since it can ably improve the quality of service for mobile applications by providing low latency and high-quality computation services. Most existing works focus on improving the efficiency of MEC with an assumption that the MEC servers have already been deployed. However, without appropriate deployment of MEC servers, the profitability of the MEC system can be significantly restrained, which hinders the rapid promotion of the MEC. To address this issue, we formulate an MEC server deployment problem for the MEC operator as a revenue maximization problem. Firstly, we model and analyze the various factors that affect the revenue. Secondly, we formulate a revenue maximization problem, which is NP-hard, but it is proved to be convex with respect to the total available computation units. Based on this feature, we propose a three-layer optimization algorithm, named EDM, in which the location, the deployed computation units, and the wholesaled computation resources are determined gradually, to maximize the total revenue. Experimental results demonstrate that the proposed EDM algorithm has significant advantages on revenue improvement compared to competitive benchmarks.}
}


@article{DBLP:journals/ton/LinGSTRX23,
	author = {Xu Lin and
                  Deke Guo and
                  Yulong Shen and
                  Guoming Tang and
                  Bangbang Ren and
                  Ming Xu},
	title = {SFT-Box: An Online Approach for Minimizing the Embedding Cost of Multiple
                  Hybrid SFCs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1463--1477},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3221868},
	doi = {10.1109/TNET.2022.3221868},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LinGSTRX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In Network Function Virtualization (NFV), a series of Virtual Network Functions (VNFs) organized in a specific order (called Service Function Chain, SFC) could offer an end-to-end network service for a network flow. Recently, with the new results of the exploration of VNF parallelism, hybrid SFC (SFC contains parallel VNFs) is proposed to reduce the SFC execution delay. However, it remains challenging and open to optimally embed multiple hybrid SFCs into the network. In this paper, we target at the optimal embedding problem of multiple hybrid SFCs with the purpose of minimizing the cost in an online scenario. Specifically, we propose SFT-Box, an online approach that can respond to hybrid SFC embedding requests in real-time. SFT-Box is designed to i) transform SFCs from the traditional sequential form to a standardized hierarchical Service Function Tree (SFT) form, ii) calculate and store the low-cost sub-solutions of embedding common SFTs, and iii) provide prompt solution response based on stored sub-solutions. To the best of our knowledge, this is the first work to address the online optimal embedding problem of multiple hybrid SFCs. With extensive evaluations, we demonstrate that, compared with the benchmark methods, SFT-Box can achieve up to 30% cost-saving and at least\n22×\nlatency reduction in enabling real-time response.}
}


@article{DBLP:journals/ton/FuLZWYGDX23,
	author = {Songtao Fu and
                  Qi Li and
                  Min Zhu and
                  Xiaoliang Wang and
                  Su Yao and
                  Yangfei Guo and
                  Xinle Du and
                  Ke Xu},
	title = {{MASK:} Practical Source and Path Verification Based on Multi-AS-Key},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1478--1493},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3222610},
	doi = {10.1109/TNET.2022.3222610},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuLZWYGDX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The source and path verification in Path-Aware Networking considers the two critical issues: (1) end hosts could verify that the network follows their forwarding decisions, and (2) both on-path routers and destination host could authenticate the source of packets and filter the malicious traffic. Unfortunately, the state-of-the-art mechanisms require heavy communication overhead in the network and computation overhead in the router; moreover, it is difficult to meet the dynamic requirements of the end host. We propose a user-driven mechanism, source and path verification based on Multi-AS-Key (MASK). MASK decreases the communication overhead by a short additional packet header and reduces the computation overhead by separating the control and data plane in terms of the cryptographic operation. Furthermore, it utilizes the stateful user to instruct the stateless routers to process the packet with a user-driven policy, thus satisfying the user’s requirements such as detecting the packet drop and replay attack. With the plausible design, the communication overhead for realistic path lengths is 1/2 to 1/10 compared with the state-of-the-art mechanisms. We implement MASK in the BMv2 environment and commodity Barefoot Tofino programmable switch, testify that MASK introduces significantly less overhead than the state-of-the-art mechanisms, and demonstrate that MASK could achieve the verification in the programmable switch at line rate.}
}


@article{DBLP:journals/ton/MaLSJF23,
	author = {Junchao Ma and
                  Lingjia Liu and
                  Bodong Shang and
                  Shashank Jere and
                  Pingzhi Fan},
	title = {Performance Analysis and Optimization for Layer-Based Scalable Video
                  Caching in 6G Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1494--1506},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3222931},
	doi = {10.1109/TNET.2022.3222931},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MaLSJF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Scalable video caching is a promising technique to alleviate backbone traffic in sixth generation (6G) networks, and to serve users with video quality that adapts to varying channel conditions. In this paper, we develop a layer-based scalable video caching technique with non-orthogonal transmission by taking advantage of the layer feature in the scalable video. In addition, the impact of different serving base station selection algorithms is investigated. Our results indicate that both the caching placement design and transmission scheme design dominate the caching performance. To evaluate the interplay of these two policies, a tractable metric of Caching Aided Data Rate (CADR) is characterized and maximized by jointly optimizing the aforementioned two policies. Together with extensive Monte Carlo simulations, numerical results are also evaluated in this paper, demonstrating that the proposed Layer-based video Caching scheme with Non-Orthogonal Transmission (LCNOT) can achieve higher CADR performance than other baseline schemes.}
}


@article{DBLP:journals/ton/SuSHLLF23,
	author = {Jian Su and
                  Zhengguo Sheng and
                  Chenxi Huang and
                  Gang Li and
                  Alex X. Liu and
                  Zhangjie Fu},
	title = {Identifying {RFID} Tags in Collisions},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1507--1520},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3219016},
	doi = {10.1109/TNET.2022.3219016},
	timestamp = {Fri, 02 Feb 2024 16:58:38 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SuSHLLF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {How to obtain the information from massive tags is a key focus of RFID applications. The occurrence of collisions leads to problems such as reduced identification efficiency in RFID networks. To tackle such challenges, most tag collision arbitration protocols focus on scheduling tag identification with collision avoidance. However, how to effectively identify tags in collisions to improve identification efficiency has not been well explored. In this paper, we propose a group query allocation method to divide the string space into mutually disjoint subsets which contains several strings. Each string can be viewed as a full ID or partial ID of a tag. When multiple string from a subset are sent simultaneously, the reader can identify all of them in a time slot. Based on the group query allocation method, a segment detection based characteristic group query tree (SD-CGQT) protocol is presented for fast tag identification by significantly reducing the collision slots and transmitted bits. Numerous experimental results verify the superiority of the proposed SD-CGQT, compared to prior arts in system efficiency, total identification time, communication complexity and energy consumption.}
}


@article{DBLP:journals/ton/GorenVM23,
	author = {Guy Goren and
                  Shay Vargaftik and
                  Yoram Moses},
	title = {Distributed Dispatching in the Parallel Server Model},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1521--1534},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3220931},
	doi = {10.1109/TNET.2022.3220931},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GorenVM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid increase in the size and volume of cloud services and data centers, architectures with multiple job dispatchers are quickly becoming the norm. Load balancing is a key element of such systems. Nevertheless, current solutions to load balancing in such systems admit a paradoxical behavior in which more accurate information regarding server queue lengths degrades performance due to herding and detrimental incast effects. Indeed, both in theory and in practice, there is a common doubt regarding the value of information in the context of multi-dispatcher load balancing. As a result, both researchers and system designers resort to more straightforward solutions, such as the power-of-two-choices to avoid worst-case scenarios, potentially sacrificing overall resource utilization and system performance. A principal focus of our investigation concerns the value of information about queue lengths in the multi-dispatcher setting. We argue that, at its core, load balancing with multiple dispatchers is a distributed computing task. In that light, we propose a new job dispatching approach, called Tidal Water Filling, which addresses the distributed nature of the system. Specifically, by incorporating the existence of other dispatchers into the decision-making process, our protocols outperform previous solutions in many scenarios. In particular, when the dispatchers have complete and accurate information regarding the server queues, our policies significantly outperform all existing solutions.}
}


@article{DBLP:journals/ton/ShiH23,
	author = {Qi Shi and
                  Dong Hao},
	title = {Social Sourcing: Incorporating Social Networks Into Crowdsourcing
                  Contest Design},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1535--1549},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3223367},
	doi = {10.1109/TNET.2022.3223367},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a crowdsourcing contest, a principal holding a task posts it to a crowd. People in the crowd then compete with each other to win the rewards. Although in real life, a crowd is usually networked and people influence each other via social ties, existing crowdsourcing contest theories do not aim to answer how interpersonal relationships influence people’s incentives and behaviors and thereby affect the crowdsourcing performance. In this work, we novelly take people’s social ties as a key factor in the modeling and designing of agents’ incentives in crowdsourcing contests. We establish two contest mechanisms by which the principal can impel the agents to invite their neighbors to contribute to the task. The first mechanism has a symmetric Bayesian Nash equilibrium, and it is very simple for agents to play and easy for the principal to predict the contest performance. The second mechanism has an asymmetric Bayesian Nash equilibrium, and agents’ behaviors in equilibrium show a vast diversity which is strongly related to their social relations. The Bayesian Nash equilibrium analysis of these new mechanisms reveals that, besides agents’ intrinsic abilities, the social relations among them also play a central role in decision-making. Moreover, we design an effective algorithm to automatically compute the Bayesian Nash equilibrium of the invitation crowdsourcing contest and further adapt it to a large graph dataset. Both theoretical and empirical results show that the new invitation crowdsourcing contests can substantially enlarge the number of participants, whereby the principal can obtain significantly better solutions without a large advertisement expenditure.}
}


@article{DBLP:journals/ton/WangFZZQLB23,
	author = {Na Wang and
                  Junsong Fu and
                  Shancheng Zhang and
                  Zheng Zhang and
                  Jiawen Qiao and
                  Jianwei Liu and
                  Bharat K. Bhargava},
	title = {Secure and Distributed IoT Data Storage in Clouds Based on Secret
                  Sharing and Collaborative Blockchain},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1550--1565},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3218933},
	doi = {10.1109/TNET.2022.3218933},
	timestamp = {Thu, 16 May 2024 21:47:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangFZZQLB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of 5G/6G, most Internet of Things (IoT) devices will embrace wireless connection in the near future. A public concern is how to securely organize, store and retrieve data generated from IoT devices. Many cloud-based IoT data storage schemes have been proposed recently. However, for an untrusted or vulnerable cloud server, the stored IoT data can be easily accessed, modified and even destroyed given that the IoT data are stored in total centralization. Moreover, the servers in a cloud are generally homogeneous and thus vulnerable to attacks. For improvements, we design a novel framework for secure and efficient IoT data storage based on secret sharing and a collaborative blockchain. First, an ultra-lightweight secret sharing algorithm is designed to map original messages generated by IoT devices to a set of shorter message shares. Second, all the shares of IoT messages are separately delivered to different clouds for storage. To guarantee the security of shares, the delivery is notarized on a proposed blockchain. Specifically, both hash values of the shares and their information of location are embedded in blocks which are then chained to form a blockchain. Third, we create a balanced index structure about the shares for each cloud storage node based on the information in the blockchain, and we also propose a depth-first data search algorithm to improve IoT data retrieval efficiency. Theoretical analysis and simulation results illustrate that our scheme can store and retrieve the IoT data securely and efficiently.}
}


@article{DBLP:journals/ton/ZhouQLLTDWPW23,
	author = {Jianer Zhou and
                  Xinyi Qiu and
                  Zhenyu Li and
                  Qing Li and
                  Gareth Tyson and
                  Jingpu Duan and
                  Yi Wang and
                  Heng Pan and
                  Qinghua Wu},
	title = {A Machine Learning-Based Framework for Dynamic Selection of Congestion
                  Control Algorithms},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1566--1581},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3220225},
	doi = {10.1109/TNET.2022.3220225},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhouQLLTDWPW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Most congestion control algorithms (CCAs) are designed for specific network environments. As such, there is no known algorithm that achieves uniformly good performance in all scenarios for all flows. Rather than devising a one-size-fits-all algorithm (which is a likely impossible task), we propose a system to dynamically switch between the most suitable CCAs for specific flows in specific environments. This raises a number of challenges, which we address through the design and implementation of Antelope, a system that can dynamically reconfigure the stack to use the most suitable CCA for individual flows. We build a machine learning model to learn which algorithm works best for individual conditions and implement kernel-level support for dynamically switching between CCAs. The framework also takes application requirements of performance into consideration to fine-tune the selection based on application-layer needs. Moreover, to reduce the overhead introduced by machine learning on individual front-end servers, we (optionally) implement the CCA selection process in the cloud, which allows the share of models and the selection among front-end servers. We have implemented Antelope in Linux, and evaluated it in both emulated and production networks. The results demonstrate the effectiveness of Antelope via dynamic adjusting the CCAs for individual flows. Specifically, Antelope achieves an average 16% improvement in throughput compared with BBR, and an average 19% improvement in throughput and 10% reduction in delay compared with CUBIC.}
}


@article{DBLP:journals/ton/PokhrelCW23,
	author = {Shiva Raj Pokhrel and
                  Jinho Choi and
                  Anwar Walid},
	title = {Fair and Efficient Distributed Edge Learning With Hybrid Multipath
                  {TCP}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1582--1594},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3219924},
	doi = {10.1109/TNET.2022.3219924},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/PokhrelCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The bottleneck of distributed edge learning (DEL) over wireless has shifted from computing to communication, primarily the aggregation-averaging (Agg-Avg) process of DEL. The existing transmission control protocol (TCP)-based data networking schemes for DEL are application-agnostic and fail to deliver adjustments according to application layer requirements. As a result, they introduce massive excess time and undesired issues such as unfairness and stragglers. Other prior mitigation solutions have significant limitations as they balance data flow rate from workers across paths but often incur imbalanced backlogs when the paths exhibit variance, causing stragglers. To facilitate a more productive DEL, we develop a hybrid multipath TCP (MPTCP) by combining model-based and deep reinforcement learning (DRL) based MPTCP for DEL that strives to realize quicker iteration of DEL and better fairness (by ameliorating stragglers). Hybrid MPTCP essentially integrates two radical TCP developments: i) successful existing model-based MPTCP control strategies and ii) advanced emerging DRL-based techniques, and introduce a novel hybrid MPTCP data transport for easing the communication of Agg-Avg process. Extensive emulation results demonstrate that the proposed hybrid MPTCP can overcome excess time consumption and ameliorate the application layer unfairness of DEL effectively without injecting additional inconstancy and stragglers.}
}


@article{DBLP:journals/ton/HanYKKSH23,
	author = {Juhyeng Han and
                  Insu Yun and
                  Seongmin Kim and
                  Taesoo Kim and
                  Sooel Son and
                  Dongsu Han},
	title = {Scalable and Secure Virtualization of {HSM} With ScaleTrust},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1595--1610},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3220427},
	doi = {10.1109/TNET.2022.3220427},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HanYKKSH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Hardware security modules (HSMs) have been utilized as a trustworthy foundation for cloud services. Unfortunately, existing systems using HSMs fail to meet multi-tenant scalability arising from the emerging trends such as microservices, which utilize frequent cryptographic operations. As an alternative, cloud vendors provide HSMs as a service. However, such cloud-managed HSM usage models raise security concerns due to their untrusted and shared operating environment. We propose ScaleTrust, a scalable and secure system for key management. ScaleTrust allows us to scale the number of virtual HSM partitions, each of which is isolated with respect to each other and is robust against cloud insider attacks, while preserving physical isolation of the root of trust. To enable this, ScaleTrust uses Intel SGX and multiple HSM features, such as restricting key usage by controlling key attributes of in-HSM keys and establishing a secure channel using only HSM commands. Finally, we apply ScaleTrust to four real-world systems: Keyless SSL for TLS private key offloading, JSON Web Token authentication for microservices, key provisioning, and encryption in database systems. Our evaluation shows that ScaleTrust achieves multi-tenancy in a scalable way by providing multiple virtual HSMs with legacy HSM devices that are designed to support a single tenant. ScaleTrust provides security against insider threats while incurring 11.9% and 39.0% of end-to-end throughput and latency overhead for Keyless SSL compared to stand-alone HSMs.}
}


@article{DBLP:journals/ton/ChengSSG23,
	author = {Xia Cheng and
                  Junyang Shi and
                  Mo Sha and
                  Linke Guo},
	title = {Revealing Smart Selective Jamming Attacks in WirelessHART Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1611--1625},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3224358},
	doi = {10.1109/TNET.2022.3224358},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChengSSG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As a leading industrial wireless standard, WirelessHART has been widely implemented to build wireless sensor-actuator networks (WSANs) in industrial facilities, such as oil refineries, chemical plants, and factories. For instance, 54,835 WSANs that implement the WirelessHART standard have been deployed globally by Emerson process management, a WirelessHART network supplier, to support process automation. While the existing research to improve industrial WSANs focuses mainly on enhancing network performance, the security aspects have not been given enough attention. We have identified a new threat to WirelessHART networks, namely smart selective jamming attacks, where the attacker first cracks the channel usage, routes, and parameter configuration of the victim network and then jams the transmissions of interest on their specific communication channels in their specific time slots, which makes the attacks energy efficient and hardly detectable. In this paper, we present this severe, stealthy threat by demonstrating the step-by-step attack process on a 50-node network that runs a publicly accessible WirelessHART implementation. Experimental results show that the smart selective jamming attacks significantly reduce the network reliability without triggering network updates.}
}


@article{DBLP:journals/ton/HellemansKH23,
	author = {Tim Hellemans and
                  Grzegorz Kielanski and
                  Benny Van Houdt},
	title = {Performance of Load Balancers With Bounded Maximum Queue Length in
                  Case of Non-Exponential Job Sizes},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1626--1641},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3221283},
	doi = {10.1109/TNET.2022.3221283},
	timestamp = {Sun, 22 Oct 2023 11:16:15 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HellemansKH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In large-scale distributed systems, balancing the load in an efficient way is crucial in order to achieve low latency. Recently, some load balancing policies have been suggested which are able to achieve a bounded maximum queue length in the large-scale limit. However, these policies have thus far only been studied in case of exponential job sizes. As job sizes are more variable in real systems, we investigate how the performance of these policies (and in particular the value of these bounds) is impacted by the job size distribution. We present a unified analysis which can be used to compute the bound on the queue length in case of phase-type distributed job sizes for four load balancing policies. We find that in most cases, the bound on the maximum queue length can be expressed in closed form. In addition, we obtain job size (in)dependent bounds on the expected response time. Our methodology relies on the use of the cavity process. That is, we conjecture that the cavity process captures the behaviour of the real system as the system size grows large. For each policy, we illustrate the accuracy of the cavity process by means of simulation.}
}


@article{DBLP:journals/ton/LindnerPM23,
	author = {Steffen Lindner and
                  Gabriel Paradzik and
                  Michael Menth},
	title = {Alternative Best Effort {(ABE)} for Service Differentiation: Trading
                  Loss Versus Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1642--1656},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3221553},
	doi = {10.1109/TNET.2022.3221553},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LindnerPM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The idea of an Alternative Best Effort (ABE) per-hop behaviour (PHB) emerged about 20 years ago. It provides a low-delay traffic class in the Internet at the expense of more packet loss than Best Effort (BE). Therefore, ABE is better suited than BE for loss-tolerant but delay-sensitive applications. Furthermore, ABE traffic should not degrade the service for BE traffic in terms of packet loss and delay. Therefore, Internet service providers may leave the choice of using BE or ABE to their customers as they achieve service differentiation without compromising other traffic. In this work, we revisit ABE and pursue the fundamental question whether an ABE service is technically feasible, how its service would look like and interact with existing transport protocols? We present a novel scheduler called Deadlines, Saved Credits, and Decay (DSCD) for combined scheduling of BE and ABE traffic. It allows to control ABE’s delay advantage over BE and copes with varying bandwidth. We provide an implementation of DSCD in the Linux network stack and demonstrate its efficiency. A side product of the implementation is an efficient approximation of the exponential function in the kernel and a bandwidth estimation method that even works at moderate link utilization. We study DSCD in a semi-virtualized testbed with real networking stacks to understand implications for transport protocols in a BE/ABE Internet. The study analyzes ABE’s impact on loss and delay under various conditions and gives recommendations for configuration.}
}


@article{DBLP:journals/ton/LinHYWWWZ23,
	author = {Chi Lin and
                  Shibo Hao and
                  Wei Yang and
                  Pengfei Wang and
                  Lei Wang and
                  Guowei Wu and
                  Qiang Zhang},
	title = {Maximizing Energy Efficiency of Period-Area Coverage With a {UAV}
                  for Wireless Rechargeable Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1657--1673},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3220927},
	doi = {10.1109/TNET.2022.3220927},
	timestamp = {Fri, 24 Nov 2023 12:41:34 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinHYWWWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Wireless Rechargeable Sensor Networks (WRSNs) with perpetual network lifetime have been used in many Internet of Things (IoT) applications, like oceanic monitoring and precision agriculture. Rechargeable sensors, together with an Unmanned Aerial Vehicle (UAV), are collaboratively employed for fulfilling periodic coverage missions. However, traditional coverage solutions are normally based on static deployment of sensors and not suitable for such novel coverage requirements. In this paper, we propose the concept of Period-Area Coverage (PAC) problem, which requires the data of the overall area must be collected/monitored periodically. To solve the PAC problem, we employ a UAV that simultaneously acts as a mobile charger and sensor. It is responsible for charging nearly exhausted sensors and sensing vacant regions to realize complete event monitoring. To maximize the energy efficiency of the UAV, we propose a heuristic hexagon-based scheduling algorithm (HSA) which can also balance energy consumption. Furthermore, we develop an emergent node charging scheduling method to prevent node exhaustion, and introduce a grid-based boustrophedon scheduling algorithm (GBSA) to reduce the complexity. Finally, we present a charging re-allocation mechanism to further enhance energy efficiency. Extensive simulations demonstrate that the proposed schemes can solve the PAC problem and enhance energy efficiency by at least 18.2% compared to prior arts. Test-bed experiments conducted both in agriculture and oceanic monitoring applications validate the applicability of the proposed scheme in practical scenarios.}
}


@article{DBLP:journals/ton/NarasimhaSY23,
	author = {Dheeraj Narasimha and
                  Srinivas Shakkottai and
                  Lei Ying},
	title = {Age-Dependent Distributed {MAC} for Ultra-Dense Wireless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1674--1687},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3228173},
	doi = {10.1109/TNET.2022.3228173},
	timestamp = {Mon, 02 Oct 2023 15:15:02 +0200},
	biburl = {https://dblp.org/rec/journals/ton/NarasimhaSY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider an ultra-dense wireless network with N\nchannels and M = N\ndevices. Messages with fresh information are generated at each device according to a random process and need to be transmitted to an access point. The value of a message decreases as it ages, so each device searches for an idle channel to transmit the message as soon as it can. However, each channel probing is associated with a fixed cost (energy), so a device needs to adapt its probing rate based on the “age” of the message. At each device, the design of the optimal probing strategy can be formulated as an infinite horizon Markov Decision Process (MDP) where the devices compete with each other to find idle channels. While it is natural to view the system as a Bayesian game, it is often intractable to analyze such a system. Thus, we use the Mean Field Game (MFG) approach to analyze the system in a large-system regime, where the number of devices is very large, to understand the structure of the problem and to find efficient probing strategies. We present an analysis based on the MFG perspective. We begin by characterizing the space of valid policies and use this to show the existence of a Mean Field Nash Equilibrium (MFNE) in a constrained set for any general increasing cost functions with diminishing rewards. Further we provide an algorithm for computing the equilibrium for any given device, and the corresponding age-dependent channel probing policy.}
}


@article{DBLP:journals/ton/TsanikidisG23,
	author = {Christos Tsanikidis and
                  Javad Ghaderi},
	title = {Randomized Scheduling of Real-Time Traffic in Wireless Networks Over
                  Fading Channels},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1688--1701},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3223315},
	doi = {10.1109/TNET.2022.3223315},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TsanikidisG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the rich literature on scheduling algorithms for wireless networks, algorithms that can provide deadline guarantees on packet delivery for general traffic and interference models are very limited. In this paper, we study the problem of scheduling real-time traffic under a conflict-graph interference model with unreliable links due to channel fading. Packets that are not successfully delivered within their deadlines are of no value. We consider traffic (packet arrival and deadline) and fading (link reliability) processes that evolve as an unknown finite-state Markov chain. The performance metric is efficiency ratio which is the fraction of packets of each link which are delivered within their deadlines compared to that under the optimal (unknown) policy. We first show a conversion result that shows classical non-real-time scheduling algorithms can be ported to the real-time setting and yield a constant efficiency ratio. In particular, Max-Weight Scheduling (MWS) yields an efficiency ratio of 1/2. We then propose randomized algorithms that achieve efficiency ratios strictly higher than 1/2, by carefully randomizing over the maximal schedules. Further, we propose low-complexity and myopic distributed randomized algorithms, and characterize their efficiency ratio. Simulation results are presented that verify that the randomized algorithms outperform classical ones such as MWS and GMS for scheduling real-time traffic over fading channels.}
}


@article{DBLP:journals/ton/MalandrinoCG23,
	author = {Francesco Malandrino and
                  Carla{-}Fabiana Chiasserini and
                  Giuseppe Di Giacomo},
	title = {Efficient Distributed DNNs in the Mobile-Edge-Cloud Continuum},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1702--1716},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3222640},
	doi = {10.1109/TNET.2022.3222640},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/MalandrinoCG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the mobile-edge-cloud continuum, a plethora of heterogeneous data sources and computation-capable nodes are available. Such nodes can cooperate to perform a distributed learning task, aided by a learning controller (often located at the network edge). The controller is required to make decisions concerning (i) data selection, i.e., which data sources to use; (ii) model selection, i.e., which machine learning model to adopt, and (iii) matching between the layers of the model and the available physical nodes. All these decisions influence each other, to a significant extent and often in counter-intuitive ways. In this paper, we formulate a problem addressing all of the above aspects and present a solution concept called RightTrain, aiming at making the aforementioned decisions in a joint manner, minimizing energy consumption subject to learning quality and latency constraints. RightTrain leverages an expanded-graph representation of the system and a delay-aware Steiner tree to obtain a provably near-optimal solution while keeping the time complexity low. Specifically, it runs in polynomial time and its decisions exhibit a competitive ratio of\n2(1+ϵ)\n, outperforming state-of-the-art solutions by over 50%. Our approach is also validated through a real-world implementation.}
}


@article{DBLP:journals/ton/ChenLZHZWY23,
	author = {Xiang Chen and
                  Hongyan Liu and
                  Dong Zhang and
                  Qun Huang and
                  Haifeng Zhou and
                  Chunming Wu and
                  Qiang Yang},
	title = {Eliminating Control Plane Overload via Measurement Task Placement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1717--1731},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3223420},
	doi = {10.1109/TNET.2022.3223420},
	timestamp = {Fri, 08 Sep 2023 07:53:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ChenLZHZWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent efforts in network measurement place measurement tasks on programmable switches to measure high-speed traffic. These tasks extract flow data, i.e., events, from packets and send events to the control plane. However, the tasks may generate massive events in a short time. In this context, the links transferring events to the control plane and the control plane servers that handle events may be overloaded, i.e., control plane overload. None of existing solutions can eliminate control plane overload. In this paper, we propose MTP, a framework that eliminates control plane overload via careful measurement task placement. Our key idea is to allocate enough resources for each task during task placement to avoid control plane overload at runtime. For each task, MTP estimates its maximum possible rate of sending events to the control plane. Then its optimization framework addresses the resource restrictions of both switches and the control plane. The experiments on Tofino switches indicate that MTP outperforms existing solutions with higher accuracy in several use cases.}
}


@article{DBLP:journals/ton/GaoSWLZLZZDZL23,
	author = {Kaihui Gao and
                  Chen Sun and
                  Shuai Wang and
                  Dan Li and
                  Yu Zhou and
                  Hongqiang Harry Liu and
                  Lingjun Zhu and
                  Ming Zhang and
                  Xiang Deng and
                  Cheng Zhou and
                  Lu Lu},
	title = {Buffer-Based High-Coverage and Low-Overhead Request Event Monitoring
                  in the Cloud},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1732--1747},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3224610},
	doi = {10.1109/TNET.2022.3224610},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GaoSWLZLZZDZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Request latency directly affects the performance of modern cloud applications. Due to various causes in hosts and networks, requests can suffer from request latency anomalies (RLAs), which may violate the Service-Level Agreement. However, existing performance monitoring tools have incomplete coverage and inconsistent semantics for monitoring requests and cannot accurately diagnose RLAs. This paper presents BufScope , a high-coverage and low-overhead request event monitoring system, which monitors buffers to capture most RLA-related abnormal events with consistent request-level semantics in the end-to-end datapath of request. First, BufScope models the datapath of request as a buffer chain and defines events based on three properties of buffers, so as to end-to-end monitor the root causes of RLA. Then, to achieve consistent semantics for captured events, BufScope designs a request-level semantics injection mechanism to make events captured in networks have the victim requests’ ID. Finally, BufScope offloads the semantics operations and event collection in software to SmartNICs for low CPU overhead . We have implemented BufScope on commodity SmartNICs and programmable switches. Evaluation results show that BufScope can diagnose 98% RLAs with < 0.08% network bandwidth overhead and 0.6% application throughput decline.}
}


@article{DBLP:journals/ton/GaoWQLMLZZSGZFKCLLYS23,
	author = {Kaihui Gao and
                  Shuai Wang and
                  Kun Qian and
                  Dan Li and
                  Rui Miao and
                  Bo Li and
                  Yu Zhou and
                  Ennan Zhai and
                  Chen Sun and
                  Jiaqi Gao and
                  Dai Zhang and
                  Binzhang Fu and
                  Frank Kelly and
                  Dennis Cai and
                  Hongqiang Harry Liu and
                  Yan Li and
                  Hongwei Yang and
                  Tao Sun},
	title = {Dependable Virtualized Fabric on Programmable Data Plane},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1748--1764},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3224617},
	doi = {10.1109/TNET.2022.3224617},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GaoWQLMLZZSGZFKCLLYS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In modern multi-tenant data centers, each tenant desires reassuring dependability from the virtualized network fabric – bandwidth guarantee with work conservation, bounded tail latency and resilient reachability. However, the slow convergence of prior works under network dynamics and uncertainties can hardly provide the dependability for tenants. Further, state-of-the-art load balance schemes are guarantee-agnostic and bring great risks on breaking bandwidth guarantee, which is overlooked in prior works. In this paper, we propose vFab, a dependable virtualized fabric framework which can (1) quickly detect network failure in data plane, (2) explicitly select proper paths for all flows, and (3) converge to ideal bandwidth allocation at sub-millisecond. The core idea of vFab is to leverage the programmable data plane to build a fusion of an active edge (e.g., NIC) and an informative core (e.g., switch), where the core sends link status and tenant information to the edge via telemetry to help the latter make a timely and accurate decision on path selection and traffic admission. We fully implement vFab with commodity SmartNICs and programmable switches. Extensive evaluations show that vFab can keep bandwidth guarantee with high bandwidth utilization, low and bounded latency, and resilient reachability under various network scenarios with limited overhead. Application-level experiments show that vFab can improve QPS by\n2.4×\nand cut tail latency by\n10×\ncompared to the alternatives.}
}


@article{DBLP:journals/ton/YangWWZMMZZ23,
	author = {Zheng Yang and
                  Xu Wang and
                  Jiahang Wu and
                  Yi Zhao and
                  Qiang Ma and
                  Xin Miao and
                  Li Zhang and
                  Zimu Zhou},
	title = {EdgeDuet: Tiling Small Object Detection for Edge Assisted Autonomous
                  Mobile Vision},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1765--1778},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3223412},
	doi = {10.1109/TNET.2022.3223412},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YangWWZMMZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Accurate, real-time object detection on resource-constrained devices enables autonomous mobile vision applications such as traffic surveillance, situational awareness, and safety inspection, where it is crucial to detect both small and large objects in crowded scenes. Prior studies either perform object detection locally on-board or offload the task to the edge/cloud. Local object detection yields low accuracy on small objects since it operates on low-resolution videos to fit in mobile memory. Offloaded object detection incurs high latency due to uploading high-resolution videos to the edge/cloud. Rather than either pure local processing or offloading, we propose to detect large objects locally while offloading small object detection to the edge. The key challenge is to reduce the latency of small object detection. Accordingly, we develop EdgeDuet, the first edge-device collaborative framework for enhancing small object detection with tile-level parallelism. It optimizes the offloaded detection pipeline in tiles rather than the entire frame for high accuracy and low latency. Evaluations on drone vision datasets under LTE, WiFi 2.4GHz, WiFi 5GHz show that EdgeDuet outperforms local object detection in small object detection accuracy by 233.0%. It also improves the detection accuracy by 44.7% and latency by 34.2% over the state-of-the-art offloading schemes.}
}


@article{DBLP:journals/ton/TsaiW23,
	author = {Cho{-}Hsin Tsai and
                  Chih{-}Chun Wang},
	title = {Distribution-Oblivious Online Algorithms for Age-of-Information Penalty
                  Minimization},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1779--1794},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3230009},
	doi = {10.1109/TNET.2022.3230009},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/TsaiW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The ever-increasing needs of supporting real-time applications have spurred new studies on minimizing Age-of-Information (AoI), a novel metric characterizing the data freshness of the system. This work studies the single-queue information update system and strengthens the seminal results of Sun et al. on the following fronts: (i) When designing the optimal offline schemes with full knowledge of the delay distributions, a new fixed-point-based method is proposed with quadratic convergence rate , an order-of-magnitude improvement over the state-of-the-art; (ii) When the distributional knowledge is unavailable (which is the norm in practice), two new low-complexity online algorithms are proposed, which provably attain the optimal average AoI penalty; and (iii) the online schemes also admit a modular architecture, which allows the designer to upgrade certain components to handle additional practical challenges. Two such upgrades are proposed for the situations: (iii.1) The AoI penalty function is also unknown and must be estimated on the fly, and (iii.2) the unknown delay distribution is Markovian instead of i.i.d. The performance of our schemes is either provably optimal or within 3% of the omniscient optimal offline solutions in all simulation scenarios.}
}


@article{DBLP:journals/ton/JinWZ23,
	author = {Meng Jin and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Key Agreement on IoT Devices With Echo Profiling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1795--1808},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3230642},
	doi = {10.1109/TNET.2022.3230642},
	timestamp = {Thu, 07 Sep 2023 10:27:47 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JinWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Secure Device-to-Device (D2D) communication is important for the Internet-of-Things (IoT) devices. Key agreement between devices is the important first step in building a secure D2D channel. Due to the lack of third-party certification, key agreement for IoT devices has to rely on the untrusted channel between them, which makes the pairing process vulnerable to attacks such as eavesdropping, jamming, and predictable channel attack. We solve this problem with a novel key agreement method named EchoKey, where two nearby devices can generate a symmetric key independently, leveraging the ambient sound signal that can be locally collected by devices. The intuition of this idea is that the propagation delays of the ambient sounds and the echoes will carry fine-grained information about the spatial context of the receiving device, which can be transformed to a special key for that device. So, nearby devices which have similar spatial context will generate similar echo profiles. We implement a prototype of EchoKey and evaluate its performance in resisting different attacks. The results tell that, with EchoKey, the key agreement process is even undetectable by an attacker which is only 50cm away from the pairing devices, and is thus resistant to all attacks mentioned above.}
}


@article{DBLP:journals/ton/LiuLM23,
	author = {Bai Liu and
                  Qingkai Liang and
                  Eytan H. Modiano},
	title = {Tracking MaxWeight: Optimal Control for Partially Observable and Controllable
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1809--1821},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3225752},
	doi = {10.1109/TNET.2022.3225752},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiuLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Modern networks are complex and may include components that cannot be fully controlled or observed. Such network models can be characterized by overlay-underlay structures, where the network controller can only observe and operate on overlay nodes, and the underlay nodes are neither observable nor controllable. Classic network control algorithms may fail to work properly if they are only applied to the overlay nodes. To tackle this issue, we propose the Tracking MaxWeight (TMW*) algorithm that does not require direct observations of underlay nodes and only operates on overlay nodes. TMW* maintains virtual queues that track the dynamics of the underlay nodes and makes control decisions based on those virtual queues. We show that TMW* is throughput optimal as long as the network is stabilizable. We further extend our analysis to the setting that the estimates of the underlay state is erroneous and show that as long as the errors scale sub-linearly in time, TMW* preserves throughput optimality.}
}


@article{DBLP:journals/ton/ShuklaHVHSHSF23,
	author = {Apoorv Shukla and
                  Kevin Nico Hudemann and
                  Zsolt V{\'{a}}gi and
                  Lily H{\"{u}}gerich and
                  Georgios Smaragdakis and
                  Artur Hecker and
                  Stefan Schmid and
                  Anja Feldmann},
	title = {Runtime Verification for Programmable Switches},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1822--1837},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3234931},
	doi = {10.1109/TNET.2023.3234931},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShuklaHVHSHSF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We introduce a runtime verification framework for programmable switches that complements static analysis. To evaluate our approach, we design and develop P6 , a runtime verification system that automatically detects, localizes, and patches software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by P6 . P6 is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a P6 prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms, namely, behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs, e.g., switch.p4 without triggering any regressions.}
}


@article{DBLP:journals/ton/HaPPKY23,
	author = {Young{-}Mok Ha and
                  Eunji Pak and
                  Jongkil Park and
                  Taeho Kim and
                  Ji Won Yoon},
	title = {Clock Offset Estimation for Systems With Asymmetric Packet Delays},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1838--1853},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3229407},
	doi = {10.1109/TNET.2022.3229407},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HaPPKY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper proposes a new clock offset estimation that mitigates unwanted link asymmetry for precise clock synchronization. The main contribution is to address the primary and traditional design issue of the IEEE 1588 standard precision time protocol (PTP), which estimates clock offset under the assumption that the delays of exchanged packets are symmetric. To mitigate the issue, we focus on the fact that PTP measures asymmetry variation through the derivatives of its timestamps with respect to the time step. By exploiting the measurement of the variation, the proposed approach defines the asymmetry in the form of a linear differential equation (LDE) and leverages the LDE to define and exclude asymmetry-induced errors. Additionally, we clearly derive the state transition of the asymmetry. Subsequently, we derive a novel state-space model from our approach. The model describes PTP clock offset estimation perfectly, allowing optimal clock offset estimation. We verify the theoretical validity of the proposed method with real data. Our approach improves PTP accuracy by more than thousand times and achieves an accuracy at the level of tens to hundreds of nanoseconds on an asymmetric communication link. Our approach realizes an accuracy comparable to that of PTPv2, without the cost of specialized hardware.}
}


@article{DBLP:journals/ton/ShiXQLYXW23,
	author = {Qilong Shi and
                  Yuchen Xu and
                  Jiuhua Qi and
                  Wenjun Li and
                  Tong Yang and
                  Yang Xu and
                  Yi Wang},
	title = {Cuckoo Counter: Adaptive Structure of Counters for Accurate Frequency
                  and Top-k Estimation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1854--1869},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3232098},
	doi = {10.1109/TNET.2022.3232098},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ShiXQLYXW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Frequency estimation and top-k flows identification are fundamental problems in network traffic measurement. Sketch, as a basic probabilistic data structure, has been extensively investigated and used in different management applications. However, few of them is suitable for both estimating frequency and finding top-k flows due to the unbalanced distribution of real-world network streams. By introducing a pre-filtering stage to isolate elephant and mice flows, the recently proposed Augmented Sketch (ASketch) significantly improves accuracy for both tasks. However, it suffers from serious performance degradation because of frequent flow exchanges. In this paper, we propose Cuckoo Counter (CC), an adaptive structure that consists of several buckets organized in a specific way. The size of the entry in each bucket is carefully designed to match the actual distribution of streams. During processing, CC hashes a flow to buckets and uses the idea of cuckoo hashing to relocate the flow if an overflow or collision happens, which contributes to fully utilizing memory. Therefore, the replacement strategy helps CC precisely record elephant flows and cover more mice flows, and also guarantees the throughput. Extensive experimental results show that CC has the highest (Freq.) accuracy, excellent (Heavy hitter / change) accuracy, highest (Top-k) precision, and competitive throughput compared to the state-of-the-art. Specifically, CC improves the throughput and accuracy by around 1 and 2 orders of magnitude respectively compared to the well-known ASketch.}
}


@article{DBLP:journals/ton/HouCWYZ23,
	author = {Bingnan Hou and
                  Zhiping Cai and
                  Kui Wu and
                  Tao Yang and
                  Tongqing Zhou},
	title = {6Scan: {A} High-Efficiency Dynamic Internet-Wide IPv6 Scanner With
                  Regional Encoding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1870--1885},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3233953},
	doi = {10.1109/TNET.2023.3233953},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/HouCWYZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Efficient Internet-wide scanning plays a vital role in network measurement and cybersecurity analysis. While Internet-wide IPv4 scanning is a solved problem, Internet-wide scanning for IPv6 is still a mission yet to be accomplished due to its vast address space. To tackle this challenge, IPv6 scanning generally needs to use pre-defined seed addresses to guide further IPv6 scanning directions. Under this general principle, various solutions have been developed, but all suffer from two primary pitfalls, low hit rate and low probing speed, caused by the inherent sparse distribution of active IPv6 addresses and the high computational complexity of the search algorithms, respectively. We develop 6Scan, a novel asynchronous IPv6 scanner that effectively addresses the above two problems. To increase the hit rate, 6Scan infers the promising search directions by encoding the regional identifiers of the target addresses within the probing packets and recording the regional activities from the asynchronously arrived replies. It then dynamically adjusts the search directions according to the scanning result of the previous steps. To speed up the search algorithm, 6Scan leverages the regional identifier encoding to quickly adjust search direction without excessive computation. Real-world experiments over the IPv6 Internet in a billion-scale probing budget show that compared with the state-of-the-art solutions, on average 6Scan can discover 6% more active addresses with nearly the same scanning time.}
}


@article{DBLP:journals/ton/LiLYWQL23,
	author = {Zhenhua Li and
                  Xingyao Li and
                  Xinlei Yang and
                  Xianlong Wang and
                  Feng Qian and
                  Yunhao Liu},
	title = {Fast Uplink Bandwidth Testing for Internet Users},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1886--1901},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3234265},
	doi = {10.1109/TNET.2023.3234265},
	timestamp = {Thu, 31 Aug 2023 19:51:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLYWQL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Access bandwidth measurement is crucial to emerging Internet applications for network-aware content delivery. However, today’s bandwidth testing services (BTSes) are slow and costly—the tests take a long time to run, consume a great deal of data usage, and usually require large-scale test server deployments. The inefficiency and high cost of BTSes root in their methodologies that use excessive temporal/spatial redundancies for combating noises in Internet measurement. In particular, compared to downlink BTSes, uplink BTSes are subject to more severe performance problems and technical challenges. This paper presents FastUpBTS to make uplink BTS fast and cheap while maintaining high accuracy. The key idea is to strategically accommodate and exploit the noise rather than repetitively and exhaustively suppress the impact of noise. This is achieved by a novel statistical sampling framework termed fuzzy rejection sampling. We build FastUpBTS as an end-to-end BTS that implements fuzzy rejection sampling based on memorization-reinforced throughput denoising, data-driven server selection, and informed multi-homing support. Our evaluation shows that with only 30 test servers, FastUpBTS achieves the same level of accuracy compared to the state-of-the-art BTS ( SpeedTest.net ) that deploys ~16,000 servers. Most importantly, FastUpBTS makes bandwidth tests\n5.4×\nfaster and\n6.8×\nmore data-efficient.}
}


@article{DBLP:journals/ton/LiSKWLLZ23,
	author = {Jiahui Li and
                  Geng Sun and
                  Hui Kang and
                  Aimin Wang and
                  Shuang Liang and
                  Yanheng Liu and
                  Ying Zhang},
	title = {Multi-Objective Optimization Approaches for Physical Layer Secure
                  Communications Based on Collaborative Beamforming in {UAV} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {4},
	pages = {1902--1917},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3234324},
	doi = {10.1109/TNET.2023.3234324},
	timestamp = {Wed, 26 Jun 2024 19:56:33 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiSKWLLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicle (UAV) communications and networks are promising technologies in the forthcoming 5G/6G wireless communications. However, they have challenges for realizing secure communications. In this paper, we consider to construct a virtual antenna array consists UAV elements and use collaborative beamforming (CB) to achieve the UAV secure communications with different base stations (BSs), subject to the known and unknown eavesdroppers on the ground. To achieve a better secure performance, the UAV elements can fly to optimal positions with optimal excitation current weights for performing CB transmissions. However, this leads to extra motion energy consumption. We formulate a physical layer secure communication multi-objective optimization problem (MOP) of UAV networks to simultaneously improve the total secrecy rates, total maximum sidelobe level (SLL) and total motion energy consumption of UAVs by jointly optimizing the positions and excitation current weights of UAVs, and the order of communicating with different BSs. Due to the complexity and NP-hardness of the formulated MOP, we propose an improved multi-objective dragonfly algorithm with chaotic solution initialization and hybrid solution update operators (IMODACH) and a parallel-IMODACH (P-IMODACH) to solve the problem. Simulation results verify that the proposed approaches can effectively solve the formulated MOP and it has better performance than some other benchmark algorithms and approaches. Moreover, some unexpected circumstances are considered and discussed.}
}


@article{DBLP:journals/ton/XiaoCQTCL23,
	author = {Qingjun Xiao and
                  Xuyuan Cai and
                  Yifei Qin and
                  Zhiying Tang and
                  Shigang Chen and
                  Yu Liu},
	title = {Universal and Accurate Sketch for Estimating Heavy Hitters and Moments
                  in Data Streams},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1919--1934},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3216025},
	doi = {10.1109/TNET.2022.3216025},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiaoCQTCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In computer networks, traffic measurement is a module in a network probe to measure flow-level statistics from an IP packet stream, which are the basis for network performance monitoring and malicious activity detection. This module extracts the flow IDs from incoming IP packets, classifies packets into flows, and counts the number of packets (or bytes) for each flow. It is a great challenge to measure the per-flow statistics for a high-speed network device, using only the size-limited SRAM on its line cards. Therefore, many algorithms using sublinear memory have been proposed, such as CountMin and CountSketch. However, most of previous algorithms are designed for specific measurement tasks. To obtain multiple types of statistics, people have to deploy multiple sketches, which demands more resources of a network device. It is useful to design a universal sketch that can track not only the top- k largest individual flows (called heavy hitters) but also the overall traffic distribution statistics (called moments). Prior work named UnivMon successfully tackled this ambitious quest. However, it incurs large and variable per-packet processing overhead, which may result in a significant throughput bottleneck in high-rate packet stream, given that each packet requires 33 hashes and 32 memory accesses on average and many times of that in the worst case. To address this performance issue, we fundamentally redesign the solution architecture from hierarchical sampling to new progressive sampling and from CountSketch to new GenericCM, which ensure that per-packet overhead is a small constant (5 hashes and 8 memory accesses in the worst case), making it more suitable for online operations, especially for hardware pipeline implementation. This new design also makes effort to reduce memory footprint or equivalently improve measurement accuracy under the same memory. Our experiments show that our solution reduces measurement error by roughly 98.1% for second-order moment and by 91.5% for entropy, when given the same 0.2MB memory as UnivMon.}
}


@article{DBLP:journals/ton/AndersonIL23,
	author = {Daron Anderson and
                  George Iosifidis and
                  Douglas J. Leith},
	title = {Lazy Lagrangians for Optimistic Learning With Budget Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1935--1949},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3222404},
	doi = {10.1109/TNET.2022.3222404},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AndersonIL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the general problem of online convex optimization with time-varying budget constraints in the presence of predictions for the next cost and constraint functions, that arises in a plethora of network resource management problems. A novel saddle-point algorithm is designed by combining a Follow-The-Regularized-Leader iteration with prediction-adaptive dynamic steps. The algorithm achieves \\mathcal O(T^{(3-\\beta)/4})\nregret and \\mathcal O(T^{(1+\\beta)/2})\nconstraint violation bounds that are tunable via parameter \\beta \\!\\in \\![1/2,1\n) and have constant factors that shrink with the predictions quality, achieving eventually \\mathcal O(1)\nregret for perfect predictions. Our work extends the seminal FTRL framework for this new OCO setting and outperforms the respective state-of-the-art greedy-based solutions which naturally cannot benefit from predictions, without imposing conditions on the (unknown) quality of predictions, the cost functions or the geometry of constraints, beyond convexity.}
}


@article{DBLP:journals/ton/YanYT23,
	author = {Shangyao Yan and
                  Zhimeng Yin and
                  Guang Tan},
	title = {CurveLight: An Accurate and Practical Light Positioning System},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1950--1964},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3224817},
	doi = {10.1109/TNET.2022.3224817},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YanYT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents CurveLight, an accurate and practical light positioning system. In CurveLight, the signal transmitter includes an infrared LED, covered by a hemispherical and rotatable shade, and the receiver detects the light signals with a photosensitive diode. When the shade is rotating, the transmitter generates a unique sequence of light signals for each point in the covered space. The main novelty of the system design is a set of curves that define different regions, either transparent or translucent, on the shade. The regions allow the light signals to create patterns from which the receiver can calculate its angles with respect to the transmitter. We design the curves in such a way that the angular information is most robust to errors caused by signal noise and motor jitters. Moreover, the shade is divided into multiple sectors, each providing independent positioning function, so as to maximize the position update rate. Experiments in various environments show that the system achieves 2-3 cm accuracy on average, with a 36 Hz update rate with a single transmitter. We present a product quality implementation of the system, and report the deployment experience in real-world environments, including autonomous driving and robotics navigation. CurveLight consistently offers centimeter-level accuracy and low latency, serving as a key component of the hybrid navigation solution for real systems in challenging scenarios.}
}


@article{DBLP:journals/ton/GuoDWX23,
	author = {Zehua Guo and
                  Songshi Dou and
                  Wenfei Wu and
                  Yuanqing Xia},
	title = {Toward Flexible and Predictable Path Programmability Recovery Under
                  Multiple Controller Failures in Software-Defined WANs},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1965--1980},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3227423},
	doi = {10.1109/TNET.2022.3227423},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/GuoDWX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Software-Defined Networking (SDN) promises good network performance in Wide Area Networks (WANs) with the logically centralized control using physically distributed controllers. In Software-Defined WANs (SD-WANs), maintaining path programmability, which enables flexible path change on flows, is crucial for maintaining network performance under traffic variation. However, when controllers fail, existing solutions are essentially coarse-grained switch-controller mapping solutions and only recover the path programmability of a limited number of offline flows, which traverse offline switches controlled by failed controllers. In this paper, we propose FlexibleProgrammabilityMedic (FlexPM) to provide predictable path programmability recovery under multiple controller failures in SD-WANs. The key idea of FlexPM is to approximately realize flow-controller mappings using hybrid SDN/legacy routing supported by high-end commercial SDN switches. Using the hybrid routing, we can recover programmability by selecting a routing mode for each offline flow at each offline switch in a fine-grained way to fit the given control resource from active controllers and release a few control resource of active controllers by reasonably configuring some normal flows under legacy routing mode. Thus, FlexPM can promise ample control resource to improve the recovery efficiency and further effectively map offline switches to active controllers. Simulation results show that FlexPM outperforms existing switch-level solutions by maintaining balanced programmability and increasing the total programmability of recovered offline flows up to 660% under AT&T topology and 590% under Belnet topology.}
}


@article{DBLP:journals/ton/ZhangLZLHJ23,
	author = {Tinghao Zhang and
                  Kwok{-}Yan Lam and
                  Jun Zhao and
                  Feng Li and
                  Huimei Han and
                  Norziana Jamil},
	title = {Enhancing Federated Learning With Spectrum Allocation Optimization
                  and Device Selection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1981--1996},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3231986},
	doi = {10.1109/TNET.2022.3231986},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangLZLHJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Machine learning (ML) is a widely accepted means for supporting customized services for mobile devices and applications. Federated Learning (FL), which is a promising approach to implement machine learning while addressing data privacy concerns, typically involves a large number of wireless mobile devices to collect model training data. Under such circumstances, FL is expected to meet stringent training latency requirements in the face of limited resources such as demand for wireless bandwidth, power consumption, and computation constraints of participating devices. Due to practical considerations, FL selects a portion of devices to participate in the model training process at each iteration. Therefore, the tasks of efficient resource management and device selection will have a significant impact on the practical uses of FL. In this paper, we propose a spectrum allocation optimization mechanism for enhancing FL over a wireless mobile network. Specifically, the proposed spectrum allocation optimization mechanism minimizes the time delay of FL while considering the energy consumption of individual participating devices; thus ensuring that all the participating devices have sufficient resources to train their local models. In this connection, to ensure fast convergence of FL, a robust device selection is also proposed to help FL reach convergence swiftly, especially when the local datasets of the devices are not independent and identically distributed (non-iid). Experimental results show that (1) the proposed spectrum allocation optimization method optimizes time delay while satisfying the individual energy constraints; (2) the proposed device selection method enables FL to achieve the fastest convergence on non-iid datasets.}
}


@article{DBLP:journals/ton/FuM23,
	author = {Xinzhe Fu and
                  Eytan H. Modiano},
	title = {Optimal Routing to Parallel Servers With Unknown Utilities - Multi-Armed
                  Bandit With Queues},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {1997--2012},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3227136},
	doi = {10.1109/TNET.2022.3227136},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FuM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the optimal routing problem in a discrete-time system with a job dispatcher connected to $M$ parallel servers. At every time slot, the job dispatcher sends the incoming jobs to a server for execution, with each server having a queue that stores the jobs. The arrival process of incoming jobs, and the service processes of the servers are stochastic with unknown and possibly heterogeneous rates. Each server $s_{m}$ is associated with an underlying utility $v_{m}$ that is initially unknown. Whenever server $s_{m}$ completes a job, a utility of $v_{m}$ is obtained and a noisy observation of $v_{m}$ is received. The goal is to design a policy that makes routing decisions to maximize the total utility obtained by the end of a finite time horizon $T$ . The performance of policies is measured in terms of regret, which is the additive difference between the expected total utility obtained by the policy and the supremum of the expected total utility over all the policies. The optimal routing problem can be interpreted as a problem of multi-armed bandit with queues where each server is viewed as an arm and the completion of a job is viewed as a pull of an arm. The key distinction between the optimal routing problem and traditional multi-armed bandit problems is in the queueing dynamics at the server, which arises due to the stochastic nature of the arrival and service processes. Our results combine techniques from control of stochastic queueing systems and stochastic multi-armed bandits to provide insights to the design and analysis of policies for the optimal routing problem. We first present analytical bounds that link the regret to the utilization and queue length of servers. Next, we start by assuming that the ordering of the underlying utilities is known and introduce the Priority- $K$ routing policy which makes priority-based routing decisions that send the incoming jobs to the server of the highest underlying utility with queue length no larger than a threshold $K$ . We prove that Priority- $K$ achieves $O(\\log T)$ -regret with an appropriately chosen $K$ . Next, removing the assumption of known utility ordering, we propose the Upper-Confidence Priority- $K$ policy, which essentially combines the Priority- $K$ policy with the ordering based on the upper-confidence bounds of the underlying utilities, and establish that the Upper-Confidence Priority- $K$ policy achieves an instance-dependent $O(\\log ^{3} T)$ -regret. Finally, we extend our results to the a generalized version of the optimal routing problem with multiple job dispatchers in a bipartite network. Our theoretical results are also validated by simulations.}
}


@article{DBLP:journals/ton/JiangW23,
	author = {Suhan Jiang and
                  Jie Wu},
	title = {Approaching an Optimal Bitcoin Mining Overlay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2013--2026},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3235307},
	doi = {10.1109/TNET.2023.3235307},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JiangW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Bitcoin builds upon an unstructured peer-to-peer overlay network to disseminate transactions and blocks. Broadcast in such a network is slow and brings inconsistencies, i. e., peers have different views of the system state. Due to the delayed block propagation and the competition of mining, forking, i. e., the blockchain temporarily diverges into two or more branches, occurs, which wastes computation power and causes security issues. This paper proposes an autonomous and distributed topology optimization mechanism to reduce block propagation delay and hence reduce the occurrence of blockchain forks. In the proposed mechanism, a node can autonomously update his neighbor set using the information provided by his current neighbors, since each neighbor will recommend a peer from his own neighbor set, i. e., a neighbor’s neighbor, to this node. Each recommendation is based on a peer’s propagation ability, which is characterized as a criteria function obtained through a combination of empirical analysis and machine learning. We further propose some metrics to evaluate a Bitcoin network topology. Experiment results reflect the effectiveness of the proposed mechanism and indicate the correlation between block propagation time and fork rate. Thus, we analyze the relation between block propagation time and fork rate by applying an epidemic model to capture the block propagation process. We prove that a Bitcoin network topology with a relatively small network delay variance among all nodes produces a lower fork rate than another topology if its average block propagation time to 84% of the entire network is shorter.}
}


@article{DBLP:journals/ton/ZhongWZC23,
	author = {Jincheng Zhong and
                  Ziling Wei and
                  Shuang Zhao and
                  Shuhui Chen},
	title = {TupleTree: {A} High-Performance Packet Classification Algorithm Supporting
                  Fast Rule-Set Updates},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2027--2041},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3227206},
	doi = {10.1109/TNET.2022.3227206},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhongWZC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Packet classification plays a crucial role in various network functions such as access control and routing. In recent years, the rapid development of SDN and NFV poses new challenges for packet classification to support fast rule-set updates as introducing strong dynamics for the structure of networks. To this end, this paper proposes a novel scheme, TupleTree, to perform high-speed packet classification while providing fast rule-set update ability. TupleTree is a hybrid scheme combining decision tree and tuple space. In TupleTree, it organizes rules in a decision tree-like structure, but distributes rules in each node into child nodes through hashing rather than cutting or splitting. With the decision tree structure, for each classification, one leaf node containing a few rules can be rapidly indexed. Hence, a high classification performance can be achieved. Meanwhile, with hashing instead of cutting or splitting, it is easy to support fast rule-set updates due to having avoided the rule replication problem. Compared to state-of-the-art schemes that support fast rule-set updates, experimental results show that our proposed scheme achieves a classification performance improvement of 85% to 237% while retaining close update performance for large rule-sets.}
}


@article{DBLP:journals/ton/XiaDZGWWC23,
	author = {Rui Xia and
                  Haipeng Dai and
                  Jiaqi Zheng and
                  Rong Gu and
                  Xiaoyu Wang and
                  Weijun Wang and
                  Guihai Chen},
	title = {{SAFE:} Service Availability via Failure Elimination Through {VNF}
                  Scaling},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2042--2057},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3233488},
	doi = {10.1109/TNET.2022.3233488},
	timestamp = {Wed, 22 Nov 2023 12:10:46 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaDZGWWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtualized network functions (VNFs) enable software applications to replace traditional middleboxes, which are more flexible and scalable in the network service provision. This paper focuses on ensuring Service Availability via Failure Elimination (SAFE) using VNF scaling, that is, given the resource requirements of VNF instances, finding an optimal and robust instance consolidation strategy, which can recover from one instance failure quickly. To address the above problem, we present a framework based on rounding and dynamic programming. First, we discretize the range of resource requirements into several sub-ranges, and thus the number of instance types becomes a constant. Second, we further reduce the number of instance types by gathering several small instances into a bigger one. Third, we propose an algorithm built on dynamic programming to solve the instance consolidation problem with a limited number of instance types. Finally, we set up a testbed to profile the functional relationship between the resource and the throughput for different types of VNFs, and conduct simulations to validate our theoretical results according to profiling results. The simulation results show that our algorithm outperforms the standby deployment model by 27.33% on average in terms of the number of servers required. Furthermore, SAFE has marginal overheads, around 7.22%, compared to the instance consolidation strategy without VNF backup consideration.}
}


@article{DBLP:journals/ton/QiuLF23,
	author = {Tianyou Qiu and
                  Yiping Li and
                  Xisheng Feng},
	title = {Optimal Broadcast Scheduling Algorithm for a Multi-AUV Acoustic Communication
                  Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2058--2069},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3232956},
	doi = {10.1109/TNET.2022.3232956},
	timestamp = {Fri, 27 Oct 2023 20:39:53 +0200},
	biburl = {https://dblp.org/rec/journals/ton/QiuLF23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In systems of multiple autonomous underwater vehicles (AUVs), to achieve cooperative operation and cluster intelligence, information is often disseminated via broadcasting. However, due to the long propagation delay and slow transmission rate of underwater acoustic communication, traditional broadcast scheduling algorithms require a long broadcast period to avoid signal collision. To improve the channel utilization rate as much as possible and improve the update rate for broadcast information, we propose an optimal broadcast scheduling algorithm. This algorithm uses the location information of AUVs to adjust the broadcast sequence and broadcast schedule, to achieve the shortest possible collision-free broadcast period in the broadcast network for the current node distribution. Simulation experiments show that this algorithm can achieve a broadcast period much shorter than that of traditional TDMA and higher channel utilization without signal collision. In addition, the simulations prove the feasibility of applying the algorithm in an actual MAC protocol.}
}


@article{DBLP:journals/ton/ChenW23,
	author = {Qian Chen and
                  Jiliang Wang},
	title = {AlignTrack: Push the {SNR} Limit of LoRa Collision Decoding},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2070--2085},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3235041},
	doi = {10.1109/TNET.2023.3235041},
	timestamp = {Wed, 01 Nov 2023 08:59:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {LoRa has been shown as a promising Low-Power Wide Area Network (LPWAN) technology to connect millions of devices for the Internet of Things by providing long-distance low-power communication when the SNR is very low. Real LoRa networks, however, suffer from severe packet collisions. Existing collision resolution approaches introduce a high SNR loss, i.e., require a much higher SNR than LoRa. To push the limit of LoRa collision decoding, we present AlignTrack, the first LoRa collision decoding approach that can work in the SNR limit of the original LoRa. Our key finding is that a LoRa chirp aligned with a decoding window should lead to the highest peak in the frequency domain and thus has the least SNR loss. By aligning a moving window with different packets, we separate packets by identifying the aligned chirp in each window. We theoretically prove this leads to the minimal SNR loss. In practical implementation, we address two key challenges: (1) accurately detecting the start of each packet, and (2) separating collided packets in each window in the presence of CFO and inter-packet interference. We implement AlignTrack on HackRF One and compare its performance with the state-of-the-arts. The evaluation results show that AlignTrack improves network throughput by\n1.68×\ncompared with NScale and\n3×\ncompared with CoLoRa.}
}


@article{DBLP:journals/ton/SahayNZYJDB23,
	author = {Rajeev Sahay and
                  Serena Nicoll and
                  Minjun Zhang and
                  Tsung{-}Yen Yang and
                  Carlee Joe{-}Wong and
                  Kerrie A. Douglas and
                  Christopher G. Brinton},
	title = {Predicting Learning Interactions in Social Learning Networks: {A}
                  Deep Learning Enabled Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2086--2100},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3237978},
	doi = {10.1109/TNET.2023.3237978},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/SahayNZYJDB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider the problem of predicting link formation in Social Learning Networks (SLN), a type of social network that forms when people learn from one another through structured interactions. While link prediction has been studied for general types of social networks, the evolution of SLNs over their lifetimes coupled with their dependence on which topics are being discussed presents new challenges for this type of network. To address these challenges, we develop a series of autonomous link prediction methodologies that utilize spatial and time-evolving network architectures to pass network state between space and time periods, and that models over three types of SLN features updated in each period: neighborhood-based (e.g., resource allocation), path-based (e.g., shortest path), and post-based (e.g., topic similarity). Through evaluation on six real-world datasets from Massive Open Online Course (MOOC) discussion forums and from Purdue University, we find that our method obtains substantial improvements over Bayesian models, linear classifiers, and graph neural networks, with AUCs typically above 0.91 and reaching 0.99 depending on the dataset. Our feature importance analysis shows that while neighborhood and path-based features contribute the most to the results, post-based features add additional information that may not always be relevant for link prediction. The code and four of the datasets used in this work are available at https://github.com/Jess-jpg-txt/sln-learning .}
}


@article{DBLP:journals/ton/XieOWXLLCW23,
	author = {Kun Xie and
                  Yudian Ouyang and
                  Xin Wang and
                  Gaogang Xie and
                  Kenli Li and
                  Wei Liang and
                  Jiannong Cao and
                  Jigang Wen},
	title = {Deep Adversarial Tensor Completion for Accurate Network Traffic Measurement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2101--2116},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3233908},
	doi = {10.1109/TNET.2022.3233908},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XieOWXLLCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network trouble shooting, failure location, and anomaly detection rely heavily on network traffic measurement data. Due to the lack of measurement infrastructure, the high measurement cost, and the unavoidable transmission loss, network monitoring systems suffer from the problem that the network traffic data are incomplete. This article models the traffic data as a tensor to exploit its strong ability of feature extraction to recover the missing data. Different from traditional tensor completion which relies on tensor factorization, we design a novel Deep Adversarial Tensor Completion (DATC) scheme based on Deep Learning (DL) techniques. DATC is the first scheme that exploits the data reconstruction ability of autoencoder and the power of adversarial training from Generative Adversarial Networks to infer the missing data. Despite that DL techniques achieve great success in the image field, designing an algorithm based on DL techniques to recover the traffic data with missing entries faces additional challenges due to the skewed distribution and the sparsity of traffic data. To conquer these challenges, we propose the use of two techniques, adversarial training and missing data aware convolution. These techniques help DATC to learn the complex features of the traffic data and infer the missing data following the data distribution of traffic data. Our extensive experimental results using two public real-world network traffic datasets and running both offline and online demonstrate that DATC can achieve significantly better recovery accuracy while capturing the data distribution of the traffic data even when the sampling ratio is very low.}
}


@article{DBLP:journals/ton/KongLYCXL23,
	author = {Hao Kong and
                  Li Lu and
                  Jiadi Yu and
                  Yingying Chen and
                  Xiangyu Xu and
                  Feng Lyu},
	title = {Toward Multi-User Authentication Using WiFi Signals},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2117--2132},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3237686},
	doi = {10.1109/TNET.2023.3237686},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/KongLYCXL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {User authentication nowadays has become an important support for not only security guarantees but also emerging novel applications. Although WiFi signal-based user authentication has achieved initial success, it works in single-user scenarios while multi-user authentication remains a challenging task. In this paper, we present MultiAuth, a multi-user authentication system that can authenticate multiple users with a single pair of commodity WiFi devices. The basic idea is to profile multipath components of WiFi signals, and leverage the multipath components to characterize each user individually for multi-user authentication. MultiAuth first profiles multipath components of WiFi signals through a proposed MUltipath Time-of-Arrival estimation algorithm (MUTA). Then, after matching corresponding multipath components to each user in complex multi-user scenarios, MultiAuth constructs individual CSI based on the multipath components to characterize each user individually. An AoA-based approach is exploited to further separate individual CSI constructed by the users with same ToA. To identify users through their activities, MultiAuth extracts user behavior profiles based on the individual CSI, and leverages a dual-task neural network for robust user authentication. Extensive experiments involving 3 simultaneously present users demonstrate that MultiAuth is effective in multi-user authentication with 86.2% average accuracy and 9.5% average false accept rate.}
}


@article{DBLP:journals/ton/DasalaJK23,
	author = {Keerthi Priya Dasala and
                  Josep Miquel Jornet and
                  Edward W. Knightly},
	title = {Scaling Multi-User mmWave WLANs: The Case for Concurrent Uplink Transmissions
                  on a Single {RF} Chain},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2133--2146},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3239438},
	doi = {10.1109/TNET.2023.3239438},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DasalaJK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Today’s mmWave WLANs can realize simultaneous multi-user multi-stream transmission solely on the downlink. In this paper, we present Uplink Multi-user Beamforming on single RF chain AP (UMBRA), a novel framework for supporting multi-stream multi-user uplink transmissions via a single RF chain. We design multi-user overlayed constellations and multi-user receiver mechanisms to enable concurrent time-triggered uplink multi-user transmissions received on a single RF chain AP. We devise exemplary beam selection policies to jointly adapt beams at users and the AP for targeting aggregate rate maximization without increasing training requirements compared to single-user systems. We implement the key components of UMBRA using a programmable WLAN testbed using software-defined radios and commercial 60-GHz transceivers and collect over-the-air measurements using phased-array antennas and horn antennas with varying beamwidth. We find that in comparison to single-user transmissions, UMBRA achieves more than\n1.45×\nimprovement in aggregate rate regardless of the choice of the user group, geometric separation, receiver beamwidth, and also under LOS blockage.}
}


@article{DBLP:journals/ton/XiongWYL23,
	author = {Guojun Xiong and
                  Shufan Wang and
                  Gang Yan and
                  Jian Li},
	title = {Reinforcement Learning for Dynamic Dimensioning of Cloud Caches: {A}
                  Restless Bandit Approach},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2147--2161},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3235480},
	doi = {10.1109/TNET.2023.3235480},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XiongWYL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study the dynamic cache dimensioning problem, where the objective is to decide how much storage to place in the cache to minimize the total costs with respect to the storage and content delivery latency. We formulate this problem as a Markov decision process, which turns out to be a restless multi-armed bandit problem and is provably hard to solve. For given dimensioning decisions, it is possible to develop solutions based on the celebrated Whittle index policy. However, Whittle index policy has not been studied for dynamic cache dimensioning, mainly because cache dimensioning needs to be repeatedly solved and jointly optimized with content caching. To overcome this difficulty, we propose a low-complexity fluid Whittle index policy, which jointly determines dimensioning and content caching. We show that this policy is asymptotically optimal. We further develop a lightweight reinforcement learning augmented algorithm dubbed fW-UCB when the content request and delivery rates are unavailable. fW-UCB is shown to achieve a sub-linear regret as it fully exploits the structure of the near-optimal fluid Whittle index policy and hence can be easily implemented. Extensive simulations using real traces support our theoretical results.}
}


@article{DBLP:journals/ton/ZhangRDMYXYZY23,
	author = {Jia Zhang and
                  Shaorui Ren and
                  Enhuan Dong and
                  Zili Meng and
                  Yuan Yang and
                  Mingwei Xu and
                  Sijie Yang and
                  Miao Zhang and
                  Yang Yue},
	title = {Reducing Mobile Web Latency Through Adaptively Selecting Transport
                  Protocol},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2162--2177},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3235907},
	doi = {10.1109/TNET.2023.3235907},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangRDMYXYZY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To improve the performance of mobile web services, a new transport protocol, QUIC, has been recently proposed as a substitute for TCP. However, with pros and cons of QUIC, it is challenging to decide whether and when to use QUIC in large-scale real-world mobile web services. Complex temporal correlation of network conditions, high user heterogeneity in a nationwide deployment, implementation diversity of QUIC variants limited, and resources on mobile devices all affect the selection of transport protocols. In this paper, we present WiseTrans, an adaptive transport protocol selection mechanism, to switch transport protocols for mobile web services online and improve the completion time of web requests. WiseTrans introduces machine learning techniques to deal with temporal heterogeneity, makes decisions with historical information to handle spatial heterogeneity, adopts an online learning method to keep pace with implementation variation, and switches transport protocols at the request level to reach high performance with acceptable overhead. We implement WiseTrans on two platforms (Android and iOS) in a popular mobile web service application of Baidu. Comprehensive experiments demonstrate that WiseTrans can reduce request completion time by up to 25.8% on average compared to the usage of a single protocol.}
}


@article{DBLP:journals/ton/AbolhassaniTE23,
	author = {Bahman Abolhassani and
                  John Tadrous and
                  Atilla Eryilmaz},
	title = {Optimal Load-Splitting and Distributed-Caching for Dynamic Content
                  Over the Wireless Edge},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2178--2190},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3244039},
	doi = {10.1109/TNET.2023.3244039},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AbolhassaniTE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we consider the problem of ‘fresh’ caching at distributed (front-end) local caches of content that is subject to ‘dynamic’ updates at the (back-end) database. We first provide new models and analyses of the average operational cost of a network of distributed edge-caches that utilizes wireless multicast to refresh aging content. We attack the problems of what to cache in each edge-cache and how to split the incoming demand amongst them (also called “load-splitting” in the rest of the paper) in order to minimize the operational cost. While the general form of the problem comes with an NP-hard Knapsack structure, we were able to completely solve the problem by judiciously choosing the number of edge-caches to be deployed over the network This reduces the complex problem to a solvable special case. Interestingly, our findings reveal that the optimal caching policy necessitates unequal load-splitting over the edge-caches even when all conditions are symmetric. Moreover, we find that edge-caches with higher load will generally cache fewer but relatively more popular content. We further investigate the tradeoffs between cost reduction and cache savings when employing equal and optimal load-splitting solutions for demand with Zipf(\nz\n) popularity distribution. Our analysis reveals that equal load-splitting to edge-caches achieves close-to-optimal for less predictable demand (\nz<2\n) while also saving in the cache size. On the other hand, for more predictable demand (\nz>2\n), optimal load-splitting results in substantial cost gains while decreasing the cache occupancy.}
}


@article{DBLP:journals/ton/BaiocchiT23,
	author = {Andrea Baiocchi and
                  Ion Turcanu},
	title = {On Flow Control and Optimized Back-Off in Non-Saturated {CSMA}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2191--2206},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3239410},
	doi = {10.1109/TNET.2023.3239410},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/BaiocchiT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Medium Access Control (MAC) main functions encompass contention for channel access, packet scheduling, error control, and data integrity. Channel contention is a collective function involving all stations in the network, while data integrity pertains to data flows of each individual station. We propose a design where contention related functions are separated from other data management functions. The hinge connecting contention and other data management functions is a flow control algorithm, aiming at guaranteeing stability of contention queues and load on the MAC channel. With reference to Carrier-Sense Multiple Access (CSMA), we define an analytical model of contention queues under non saturated traffic. An asymptotic analysis of the model for large number of stations yields a closed form of the optimal flow control rate. The insight gained from the model is used to design an adaptive flow control algorithm that guarantees throughput optimality for all values of the number of stations.}
}


@article{DBLP:journals/ton/ZengCGBB23,
	author = {Yijing Zeng and
                  Roberto Calvo{-}Palomino and
                  Domenico Giustiniano and
                  G{\'{e}}r{\^{o}}me Bovet and
                  Suman Banerjee},
	title = {Adaptive Uplink Data Compression in Spectrum Crowdsensing Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2207--2221},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3239378},
	doi = {10.1109/TNET.2023.3239378},
	timestamp = {Sat, 28 Oct 2023 13:59:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZengCGBB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Understanding spectrum activity is challenging when attempted at scale. The wireless community has recently risen to this challenge in designing spectrum monitoring systems that utilize many low-cost spectrum sensors to gather large volumes of sampled data across space, time, and frequencies. These crowdsensing systems are limited by the uplink bandwidth available to backhaul the raw in-phase and quadrature (IQ) samples and power spectrum density (PSD) data needed to run various applications. This paper presents FlexSpec, a framework based on the Walsh-Hadamard transform to compress spectrum data collected from distributed and low-cost sensors for real-time applications. This transformation allows sensors to significantly save uplink bandwidth thanks to its inherent properties both when it is applied to IQ and PSD data. Additionally, by leveraging a feedback loop between the sensor and the edge device it connects to, FlexSpec carefully adapts the compression ratio over time to changes in the spectrum and different applications, jointly considering data size, application performance, and spectrum variations. We experimentally evaluate FlexSpec in several applications. Our results show that FlexSpec is particularly suitable for IoT transmissions and signals close to the noise floor. Compared with prior work, FlexSpec provides up to\n7×\nmore reduction of uplink data size for signal detection based on PSD data, and reduces up to\n6×\nto\n8×\nthe number of undecodable messages for IQ sample decoding.}
}


@article{DBLP:journals/ton/ZouLMZW23,
	author = {Renpeng Zou and
                  Xixiang Lyu and
                  Jing Ma and
                  Bowen Zhang and
                  Danfang Wu},
	title = {{BCMIX:} {A} Blockchain-Based Dynamic Self-Reconfigurable Mixnet},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2222--2235},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3244962},
	doi = {10.1109/TNET.2023.3244962},
	timestamp = {Sat, 28 Oct 2023 13:59:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZouLMZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The increasing awareness of privacy preservation has led to a strong focus on mix networks (mixnets) protecting anonymity. As an efficient mixnet, cMix greatly reduces the latency, but brings privacy leakage risks due to the use of presetting mix nodes controlled by service providers. Besides, cMix is susceptible to blocking attacks that paralyze the anonymous service. In this paper, we combine blockchain with cMix, and come up with a BlockChain-based dynamic self-reconfigurable MIXnet (BCMIX) approach for anonymous communication. In BCMIX, we design PoW voting, IP sharding and VRF voting algorithms to select mix nodes from blockchain miners. With these voting algorithms, BCMIX can mitigate blocking attacks and blockchain Sybil attacks. Then we present an additive homomorphic mixnet protocol altered from cMix. Furthermore, we design a transaction-based key-exchange protocol for users and mix nodes to negotiate keys with each other, which resists Man-in-the-Middle attacks that exist in cMix. We also demonstrate the security and anonymity of BCMIX under formal security models. To evaluate BCMIX, we leverage the distribution of mining pools in the real-world to test the system’s performance and ability to resist the mentioned attacks. The results show that with the proper mining difficulty, the probability of an attacker launching collusion attacks and Sybil attacks is negligible. Compared with the existing anonymous systems, BCMIX provides better resilience to known attacks while enabling low-latency anonymous communication without requiring significant bandwidth or storage resources.}
}


@article{DBLP:journals/ton/LiLSYI23,
	author = {Yuanyuan Li and
                  Yuezhou Liu and
                  Lili Su and
                  Edmund Yeh and
                  Stratis Ioannidis},
	title = {Experimental Design Networks: {A} Paradigm for Serving Heterogeneous
                  Learners Under Networking Constraints},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2236--2250},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3243534},
	doi = {10.1109/TNET.2023.3243534},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/LiLSYI23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Significant advances in edge computing capabilities enable learning to occur at geographically diverse locations. In general, the training data needed in those learning tasks are not only heterogeneous but also not fully generated locally. In this paper, we propose an experimental design network paradigm, wherein learner nodes train possibly different Bayesian linear regression models via consuming data streams generated by data source nodes over a network. We formulate this problem as a social welfare optimization problem in which the global objective is defined as the sum of experimental design objectives of individual learners, and the decision variables are the data transmission strategies subject to network constraints. We first show that, assuming Poisson data streams in steady state, the global objective is a continuous DR-submodular function. We then propose a Frank-Wolfe type algorithm that outputs a solution within a\n1−1/e\nfactor from the optimal. Our algorithm contains a novel gradient estimation component which is carefully designed based on Poisson tail bounds and sampling. Finally, we complement our theoretical findings through extensive experiments. Our numerical evaluation shows that the proposed algorithm outperforms several baseline algorithms both in maximizing the global objective and in the quality of the trained models.}
}


@article{DBLP:journals/ton/YasodharanK23,
	author = {Sarath Yasodharan and
                  Anurag Kumar},
	title = {Revenue Optimal Bandwidth Allocation in a Class of Multihop Networks:
                  Algorithms and Asymptotic Optimality},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2251--2266},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3249480},
	doi = {10.1109/TNET.2023.3249480},
	timestamp = {Sat, 20 Apr 2024 20:42:38 +0200},
	biburl = {https://dblp.org/rec/journals/ton/YasodharanK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study Bandwidth Reservation (BR) policies for the Bandwidth on Demand (BoD) problem in a class of multihop networks. We motivate an Erlang fixed-point BR heuristic for the general BoD problem by first establishing the optimality of BR on a class of multihop networks. The motivating problem is a wireline network comprising\nk\nlinks in tandem, each link of which is shared by two types of bandwidth demands, one type requiring one unit of bandwidth from every link, and the other type (being dedicated to the link) requiring one unit of bandwidth as well. First, for this\nk\n-hop tandem network, when each link has unit bandwidth, we demonstrate that a policy of BR form is optimal. We then study the BoD problem for a more general\nk\n-hop tandem network, in the “Kelly” limiting regime, where the arrival rates as well as the link bandwidths become large. For certain parameter regimes of the\nk\n-hop tandem network, we show that an admission control policy of BR form is asymptotically optimal. Motivated by these results, we propose an Erlang fixed-point based, link-by-link, heuristic algorithm for computing a BR policy for the BoD problem in a general network. We, finally, evaluate this proposal numerically.}
}


@article{DBLP:journals/ton/FangZXWY23,
	author = {Jin Fang and
                  Gongming Zhao and
                  Hongli Xu and
                  Changbo Wu and
                  Zhuolong Yu},
	title = {{GRID:} Gradient Routing With In-Network Aggregation for Distributed
                  Training},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2267--2280},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3244794},
	doi = {10.1109/TNET.2023.3244794},
	timestamp = {Sat, 28 Oct 2023 13:59:34 +0200},
	biburl = {https://dblp.org/rec/journals/ton/FangZXWY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As the scale of distributed training increases, it brings huge communication overhead in clusters. Some works try to reduce the communication cost through gradient compression or communication scheduling. However, these methods either downgrade the training accuracy or do not reduce the total transmission amount. One promising approach, called in-network aggregation, is proposed to mitigate the bandwidth bottleneck in clusters by aggregating gradients in programmable hardware (e.g., Intel Tofino switches). However, existing solutions mainly implement in-network aggregation through fixed (or default) routing paths, resulting in load imbalancing and long communication time. To deal with this issue, we propose GRID, the first-of-its-kind work on Gradient Routing with In-network Aggregation for Distributed Training. In the control plane, we present an efficient gradient routing algorithm based on randomized rounding and formally analyze the approximation performance. In the data plane, we realize in-network aggregation by carefully designing the logic of workers and programmable switches. We implement GRID and evaluate its performance on a small-scale testbed consisting of 3 Intel Tofino switches and 9 commodity servers. With a combination of testbed experiments and large-scale simulations, we show that GRID can reduce the communication time by 38.4%–60.1% and speed up distributed training by 17.4%–52.7% compared with state-of-the-art solutions.}
}


@article{DBLP:journals/ton/AkhtarGBS23,
	author = {Md Shahbaz Akhtar and
                  Krishnakumar G and
                  Vishnu B and
                  Abhishek Sinha},
	title = {Fast and Secure Routing Algorithms for Quantum Key Distribution Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2281--2296},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3246114},
	doi = {10.1109/TNET.2023.3246114},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AkhtarGBS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We investigate the problem of fast and secure packet routing in multi-hop Quantum Key Distribution (QKD) networks. We consider a practical trusted-node setup where a QKD protocol randomly generates symmetric private key pairs over each QKD-enabled link in a network. Packets are first encrypted with the available quantum keys and then transmitted on a point-to-point basis. A fundamental problem in this setting is the design of a secure and capacity-achieving routing policy that takes into account the time-varying availability of the encryption keys and diverse physical-layer link capacities. To address this problem, we propose a new secure throughput-optimal policy called Tandem Queue Decomposition (TQD). The TQD policy is designed by incorporating the QKD process into the Universal Max Weight (UMW) routing policy. We show that the TQD policy achieves the entire secure capacity region for a broad class of traffic, including unicast, broadcast, multicast, and anycast. The TQD policy operates by reducing the problem to the generalized network flow problem without the key availability constraints over a transformed network. The throughput-optimality of the TQD policy is established using the Lyapunov stability theory by carefully analyzing the interdependent queueing process and the key-storage dynamics. Finally, we demonstrate the practical efficiency of the TQD policy over the existing routing algorithms by numerically comparing their performance on a realistic simulator built on top of the state-of-the-art OMNeT++ network simulator platform.}
}


@article{DBLP:journals/ton/XingXZHLW23,
	author = {Yitao Xing and
                  Kaiping Xue and
                  Yuan Zhang and
                  Jiangping Han and
                  Jian Li and
                  David S. L. Wei},
	title = {An Online Learning Assisted Packet Scheduler for {MPTCP} in Mobile
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2297--2312},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3246168},
	doi = {10.1109/TNET.2023.3246168},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/XingXZHLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Multipath TCP is designed to utilize multiple network paths to achieve improved throughput and robustness against network failure. These features are supposed to make MPTCP preferable to single-path TCP in mobile networks. However, it fails to achieve the expected performance in practice. A key challenge of using MPTCP in mobile networks is how to effectively spread packets over heterogeneous and unstable network paths to mobile devices with limited buffers. If packets are not sent in an effective way, MPTCP may only provide equal or even lower throughput than single-path TCP. Several packet scheduling algorithms have been designed to tackle this challenge. Unfortunately, they still cannot achieve the expected performance in dynamic scenarios such as mobile networks. In this paper, we propose an Online-Learning Assisted Packet Scheduler (OLAPS) to solve the packet scheduling problem by modeling it as a multi-armed bandit problem. Over time, OLAPS can adaptively learn from current network conditions to make the best scheduling policy to provide the highest possible throughput in a dynamic environment. Moreover, when the inbuilt reward monitor detects the mismatch between network conditions and the learned policy, OLAPS aborts the outdated policy and switches to a new one swiftly. We implement OLAPS as a Linux kernel module and evaluate it over a wide range of ns-3 -simulated network conditions. The results show that OLAPS retains MPTCP’s ability to provide higher throughput and also significantly improves the throughput performance of MPTCP when other in-kernel schedulers suffer a dramatic throughput decline.}
}


@article{DBLP:journals/ton/JinJJQZCL23,
	author = {Yibo Jin and
                  Lei Jiao and
                  Mingtao Ji and
                  Zhuzhong Qian and
                  Sheng Zhang and
                  Ning Chen and
                  Sanglu Lu},
	title = {Scheduling In-Band Network Telemetry With Convergence-Preserving Federated
                  Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2313--2328},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3253302},
	doi = {10.1109/TNET.2023.3253302},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JinJJQZCL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Conducting federated learning across distributed sites with In-Band Network Telemetry (INT) based data collection faces critical challenges, including control decisions of different frequencies, convergence of the models being trained, and resource provisioning coupled over time. To study this problem, we formulate a non-linear mixed-integer program to optimize the long-term INT overhead, resource cost, and federated learning cost. We then design polynomial-time online algorithms to solve this problem with only observable inputs on the fly, featuring laziness-aware resource adaption, online-learning-based INT flow selection and model aggregation control, as well as expectation-preserving randomized dependent rounding. We rigorously prove the parameterized-constant competitive ratio of our approach against the offline optimum, and the time-averaged constraint violation that vanishes in the long run. With extensive trace-driven evaluations, we confirm the superiority of our approach over other alternative approaches for reducing total cost and the efficacy of our trained models for solving real machine learning problems, reducing the real-time cost by 34% on average.}
}


@article{DBLP:journals/ton/FuLGLZ23,
	author = {Pengtao Fu and
                  Lailong Luo and
                  Deke Guo and
                  Shangsen Li and
                  Yun Zhou},
	title = {A Shifting Filter Framework for Dynamic Set Queries},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2329--2344},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3247628},
	doi = {10.1109/TNET.2023.3247628},
	timestamp = {Wed, 01 Nov 2023 08:59:12 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FuLGLZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Set query is a fundamental problem in computer systems. Plenty of applications rely on the query results of membership, association, and multiplicity. A traditional method that addresses such a fundamental problem is derived from Bloom filter. However, such methods may fail to support element deletion, require additional filters or apriori knowledge, making them unamenable to a high-performance implementation for dynamic set representation and query. In this paper, we envision a novel sketch framework that is multi-functional, non-parametric, space efficient, and deletable. As far as we know, none of the existing designs can guarantee such features simultaneously. To this end, we present a general shifting framework to represent auxiliary information (such as multiplicity, association) with the offset. Thereafter, we specify such design philosophy for a hash table horizontally at the slot level, as well as vertically at the bucket level. Theoretical and experimental results jointly demonstrate that our design works exceptionally well with three types of set queries under small memory.}
}


@article{DBLP:journals/ton/JiaWWLLZL23,
	author = {Riheng Jia and
                  Jinhao Wu and
                  Xiong Wang and
                  Jianfeng Lu and
                  Feilong Lin and
                  Zhonglong Zheng and
                  Minglu Li},
	title = {Energy Cost Minimization in Wireless Rechargeable Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2345--2360},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3248088},
	doi = {10.1109/TNET.2023.3248088},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/JiaWWLLZL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile chargers (MCs) are usually dispatched to deliver energy to sensors in wireless rechargeable sensor networks (WRSNs) due to its flexibility and easy maintenance. This paper concerns the fundamental issue of charging path DEsign with the Minimized energy cOst (DEMO), i.e., given a set of rechargeable sensors, we appropriately design the MC’s charging path to minimize the energy cost which is due to the wireless charging and the MC’s movement, such that the different charging demand of each sensor is satisfied. Solving DEMO is NP-hard and involves handling the tradeoff between the charging efficiency and the moving cost. To address DEMO, we first investigate how to identify a single charging position where the MC could stay to charge a set of sensors distributed within a small area with the maximized charging efficiency. Then, based on the result obtained in the case of optimizing a single charging position, we develop a computational geometry-based algorithm to deploy multiple charging positions within the whole network, by considering the fixed and finite charging range of the MC. We prove that the designed algorithm has the approximation ratio of\nO(lnN)\n, where\nN\nis the number of sensors. Then we construct the charging path by calculating the shortest Hamiltonian cycle passing through all the deployed charging positions within the network. In addition, we investigate the impact of the network topology as well as the distribution of charging demands among sensors on the MC’s energy cost during a charging tour. Extensive evaluations validate the superiority of our path design in terms of the MC’s energy cost minimization, compared with existing main algorithms.}
}


@article{DBLP:journals/ton/ZhangMLR23,
	author = {Yiran Zhang and
                  Qingkai Meng and
                  Yifan Liu and
                  Fengyuan Ren},
	title = {Revisiting Congestion Detection in Lossless Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2361--2375},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3250484},
	doi = {10.1109/TNET.2023.3250484},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/ZhangMLR23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Congestion detection is the cornerstone of end-to-end congestion control. Through in-depth observations and understandings, we reveal that existing congestion detection mechanisms in mainstream lossless networks (i.e., Converged Enhanced Ethernet and InfiniBand) are improper, due to failing to cognize the interaction between hop-by-hop flow controls and congestion detection behaviors in switches. We define the ternary states of switch ports and present Ternary Congestion Detection (TCD) for mainstream lossless networks. TCD utilizes the ON-OFF sending pattern and the feature of queue length evolutions to detect the transitions among ternary states. We also enable TCD under the practical multiple queues scenario by TCD-MQ. Testbed and extensive simulations demonstrate that TCD can detect congestion ports accurately and identify flows contributing to congestion as well as flows only affected by hop-by-hop flow controls. Meanwhile, we shed light on how to incorporate TCD with rate control. Case studies show that existing congestion control algorithms can achieve\n3.3×\nand\n2.0×\nbetter median and 99th-percentile FCT slowdown by combining with TCD.}
}


@article{DBLP:journals/ton/WangQHWC23,
	author = {Shengling Wang and
                  Xidi Qu and
                  Qin Hu and
                  Xia Wang and
                  Xiuzhen Cheng},
	title = {An Uncertainty- and Collusion-Proof Voting Consensus Mechanism in
                  Blockchain},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2376--2388},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3249206},
	doi = {10.1109/TNET.2023.3249206},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/WangQHWC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Though voting-based consensus algorithms in blockchain outperform proof-based ones in energy- and transaction-efficiency, they are prone to incur wrong elections and bribery elections. The former originates from the uncertainties of candidates’ capability and availability, and the latter comes from the egoism of voters and candidates. Hence, in this paper, we propose an uncertainty- and collusion-proof voting consensus mechanism, including the selection pressure-based voting algorithm and the trustworthiness evaluation algorithm. The first algorithm can decrease the side effects of candidates’ uncertainties, lowering wrong elections while trading off the balance between efficiency and fairness in voting miners. The second algorithm adopts an incentive-compatible scoring rule to evaluate the trustworthiness of voting, motivating voters to report true beliefs on candidates by making egoism consistent with altruism so as to avoid bribery elections. A salient feature of our work is theoretically analyzing the proposed voting consensus mechanism by the large deviation theory. Our analysis provides not only the voting failure rate of a candidate but also its decay speed. The voting failure rate measures the incompetence of any candidate from a personal perspective by voting, based on which the concepts of the effective selection valve and the effective expectation of merit are introduced to help the system designer determine the optimal voting standard and guide a candidate to behave in an optimal way for lowering the voting failure rate.}
}


@article{DBLP:journals/ton/AkramU23,
	author = {Vahid Khalilpour Akram and
                  Onur Ugurlu},
	title = {Detecting the Most Vital Articulation Points in Wireless Multi-Hop
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {5},
	pages = {2389--2402},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3308142},
	doi = {10.1109/TNET.2023.3308142},
	timestamp = {Fri, 27 Oct 2023 20:39:54 +0200},
	biburl = {https://dblp.org/rec/journals/ton/AkramU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {An articulation point is a node whose removal partitions the network into disconnected segments. The articulation points may affect the reliability and efficiency of wireless multi-hop networks from different aspects. Although all articulation points destroy the connectivity of the network, their negative impact on the network is not equal. Removing some articulation points may disconnect a large subset of nodes or generate a large number of partitions, while removing some other articulation points may only disconnect a few nodes. In this paper, we present two novel problems for identifying the most vital articulation points that significantly impact the network. The first problem is finding the p\nmost important articulation points that minimize the largest connected component in the remaining network. The second problem is finding the p\nmost important articulation points whose removal maximizes the number of partitions in the network. We prove that both problems are NP-Hard and propose a distributed algorithm to identify the vital articulation points in both problems. The proposed algorithm establishes a distributed depth-first search tree to identify the articulation points, assigns a score to each articulation point, and selects the prominent articulation points based on their scores. We compare the proposed algorithm with a brute force-based exact algorithm. The simulation result shows that after removing the detected prominent articulation points by the proposed algorithm, the maximum difference between the largest partition size and the number of partitions with the optimal solutions are less than 27.6% and 28.2%, respectively, while the sent bytes of the proposed algorithm can be 89.9% lower.}
}


@article{DBLP:journals/ton/ZhangYSWWGLLZRG23,
	author = {Han Zhang and
                  Xia Yin and
                  Xingang Shi and
                  Jilong Wang and
                  Zhiliang Wang and
                  Yingya Guo and
                  Tian Lan and
                  Yahui Li and
                  Yongqing Zhu and
                  Ke Ruan and
                  Haijun Geng},
	title = {Achieving High Availability in Inter-DC {WAN} Traffic Engineering},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2406--2421},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3216592},
	doi = {10.1109/TNET.2022.3216592},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangYSWWGLLZRG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Inter-DataCenter Wide Area Network (Inter-DC WAN) that connects geographically distributed data centers is becoming one of the most critical network infrastructures. Due to limited bandwidth and inevitable link failures, it is highly challenging to guarantee network availability for services, especially those with stringent bandwidth demands, over inter-DC WAN. We present \\mathsf {TEDAT}\n, a novel Traffic Engineering (TE) framework for Diverse Availability Targets (DAT), where a Service Level Agreement (SLA) is defined to ensure that each bandwidth demand must be satisfied with a stipulated probability, when subjected to the network capacity and possible failures of the inter-DC WAN. \\mathsf {TEDAT}\nhas two core components, i.e., traffic scheduling and failure recovery, which are crystalized through different mathematical models and theoretically analyzed. They are also extensively compared against state-of-the-art TE schemes, using a testbed as well as real trace driven simulations across different topologies, traffic matrices and failure scenarios. Our evaluations show that, compared with the optimal admission strategy, \\mathsf {TEDAT}\ncan speed up the online admission control by 30\\times\nat the expense of less than 4% false rejections. On the other hand, compared with the latest TE schemes like FFC and TEAVAR, \\mathsf {TEDAT}\ncan meet the bandwidth availability SLAs for 23%~60% more demands under normal loads, and when network failure causes SLA violations, it can retain 10%~20% more profit under a pricing and refunding model.}
}


@article{DBLP:journals/ton/WangSXLLW23,
	author = {Yuntao Wang and
                  Zhou Su and
                  Qichao Xu and
                  Ruidong Li and
                  Tom H. Luan and
                  Pinghui Wang},
	title = {A Secure and Intelligent Data Sharing Scheme for UAV-Assisted Disaster
                  Rescue},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2422--2438},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3226458},
	doi = {10.1109/TNET.2022.3226458},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangSXLLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Unmanned aerial vehicles (UAVs) have the potential to establish flexible and reliable emergency networks in disaster sites when terrestrial communication infrastructures go down. Nevertheless, potential security threats may occur on UAVs during data transmissions due to the untrusted environment and open-access UAV networks. Moreover, UAVs typically have limited battery and computation capacity, making them unaffordable for heavy security provisioning operations when performing complicated rescue tasks. In this paper, we develop RescueChain, a secure and efficient information sharing scheme for UAV-assisted disaster rescue. Specifically, we first implement a lightweight blockchain-based framework to safeguard data sharing under disasters and immutably trace misbehaving entities. A reputation-based consensus protocol is devised to adapt the weakly connected environment with improved consensus efficiency and promoted UAVs’ honest behaviors. Furthermore, we introduce a novel vehicular fog computing (VFC)-based off-chain mechanism by leveraging ground vehicles as moving fog nodes to offload UAVs’ heavy data processing and storage tasks. To offload computational tasks from the UAVs to ground vehicles with idle computing resources, an optimal allocation strategy is developed by choosing payoffs that achieve equilibrium in a Stackelberg game formulation of the allocation problem. For lack of sufficient knowledge on network model parameters and users’ private cost parameters in practical environment, we also design a two-tier deep reinforcement learning-based algorithm to seek the optimal payment and resource strategies of UAVs and vehicles with improved learning efficiency. Simulation results show that RescueChain can effectively accelerate consensus process, improve offloading efficiency, reduce energy consumption, and enhance user payoffs.}
}


@article{DBLP:journals/ton/XieL23,
	author = {Hong Xie and
                  John C. S. Lui},
	title = {Cooperation Preference Aware Shapley Value: Modeling, Algorithms and
                  Applications},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2439--2453},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3228933},
	doi = {10.1109/TNET.2022.3228933},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XieL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The Shapley value is a cornerstone in cooperative game theory and has been widely applied in networking, data science, etc. The classical Shapley value assumes that each player has an equal preference to cooperate with each other. Since the cooperation preference is an important factor of a variety of networking applications, we first generalize the classical Shapley value to allow general degree of the cooperation preference. In particular, we develop mathematical models to solicit two types of cooperation preferences, i.e., (1) group-wise preferences and (2) pair-wise preferences, and extend the classical Shapley value to capture this feature. Our second contribution is tackling the intrinsic computational challenge because even for the classical Shapley value, it is computationally expensive to evaluate. We design computationally efficient randomized algorithms with theoretical guarantees to fully cover the computational space of our generalized Shapley value. We also extend our models and algorithms to divide payoffs for multiple coalitions with dynamic preferences. We demonstrate the versatility of our framework by applying it to divide the revenue among ISPs in deploying new Internet architectures, as well as to divide the reward among workers in crowdsourcing systems.}
}


@article{DBLP:journals/ton/AnZPLXLM23,
	author = {Congkai An and
                  Anfu Zhou and
                  Jialiang Pei and
                  Xi Liu and
                  Dongzhu Xu and
                  Liang Liu and
                  Huadong Ma},
	title = {Octopus: Exploiting the Edge Intelligence for Accessible 5G Mobile
                  Performance Enhancement},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2454--2469},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3224369},
	doi = {10.1109/TNET.2022.3224369},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AnZPLXLM23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {While 5G has rolled out since 2019 and exhibited versatile advantages, its performance under high/extreme mobility scenes (e.g., driving, high-speed railway or HSR) remains mysterious. In this work, we carry out a large-scale field-trial campaign, taking >13,000 Km round-trips on HSR moving at 250–350 Km/h, with operational 5G cellular coverage along the railway. Our empirical study reveals that coupling interaction among high mobility, 5G handover characteristics, and applications’ sluggish reaction to handover, results in catastrophic damage to user experience: low TCP bandwidth utilization of 26.6% and glitchy 4K VoD streaming. To solve the problem, we propose an edge-assisted mobility management framework called Octopus. Different from previous works, Octopus aims at a standard-compatible and easy-to-deploy solution, thus we take a new design paradigm of exploiting the edge intelligence on multi-access edge computing (MEC). We realize Octopus as a universal MEC service ready for benefiting any third-party mobile applications. We prototype, deploy, and evaluate Octopus in operational 5G, which demonstrates the significant performance gain across the full-range mobile scenarios, e.g., HSR, driving, and walking.}
}


@article{DBLP:journals/ton/SanchezMBLMNWIC23,
	author = {Sara Garcia Sanchez and
                  Guillem Reus Muns and
                  Carlos Bocanegra and
                  Yanyu Li and
                  Ufuk Muncuk and
                  M. Yousof Naderi and
                  Yanzhi Wang and
                  Stratis Ioannidis and
                  Kaushik Roy Chowdhury},
	title = {AirNN: Over-the-Air Computation for Neural Networks via Reconfigurable
                  Intelligent Surfaces},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2470--2482},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3225883},
	doi = {10.1109/TNET.2022.3225883},
	timestamp = {Wed, 24 Jan 2024 17:49:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/SanchezMBLMNWIC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Over-the-air analog computation allows offloading computation to the wireless environment through carefully constructed transmitted signals. In this paper, we design and implement the first-of-its-kind convolution that uses over-the-air computation and demonstrate it for inference tasks in a convolutional neural network (CNN). We engineer the ambient wireless propagation environment through reconfigurable intelligent surfaces (RIS) to design such an architecture, which we call ’AirNN’. AirNN leverages the physics of wave reflection to represent a digital convolution, an essential part of a CNN architecture, in the analog domain. In contrast to classical communication, where the receiver must react to the channel-induced transformation, generally represented as finite impulse response (FIR) filter, AirNN proactively creates the signal reflections to emulate specific FIR filters through RIS. AirNN involves two steps: first, the weights of the neurons in the CNN are drawn from a finite set of channel impulse responses (CIR) that correspond to realizable FIR filters. Second, each CIR is engineered through RIS, and reflected signals combine at the receiver to determine the output of the convolution. This paper presents a proof-of-concept of AirNN by experimentally demonstrating convolutions with over-the-air computation. We then validate the entire resulting CNN model accuracy via simulations for an example task of modulation classification.}
}


@article{DBLP:journals/ton/DanilchenkoNS23,
	author = {Kiril Danilchenko and
                  Zeev Nutov and
                  Michael Segal},
	title = {Covering Users With QoS by a Connected Swarm of Drones: Graph Theoretical
                  Approach and Experiments},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2483--2498},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3231184},
	doi = {10.1109/TNET.2022.3231184},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DanilchenkoNS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this work, we study the connected version of the covering problem motivated by the coverage of ad-hoc drones’ swarm. We focus on the situation where the number of drones is given, and this number is not necessarily enough to cover all users. That is, we deal with a budget optimization problem, where the budget is the number of given drones. We assume that each ground user has different QoS requirements. Additionally, each ground user has a weight that corresponds to the importance (rank) of the user. Moreover, we consider the case when there is no third-party entity that provides connectivity to the drones. In this paper, we propose a 3D deployment scheme with the given number of drones such that the sum of the weights (ranks) of the ground users covered by drones is maximized (when the covering radii satisfy QoS of these users), and the drones form a connected graph. We present a number of approximate solutions with provable guaranteed performance evaluation that have been validated also through the simulation platform.}
}


@article{DBLP:journals/ton/YunKBJJBPK23,
	author = {Won Joon Yun and
                  Yunseok Kwak and
                  Hankyul Baek and
                  Soyi Jung and
                  Mingyue Ji and
                  Mehdi Bennis and
                  Jihong Park and
                  Joongheon Kim},
	title = {SlimFL: Federated Learning With Superposition Coding Over Slimmable
                  Neural Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2499--2514},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2022.3231864},
	doi = {10.1109/TNET.2022.3231864},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YunKBJJBPK23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Federated learning (FL) is a key enabler for efficient communication and computing, leveraging devices’ distributed computing capabilities. However, applying FL in practice is challenging due to the local devices’ heterogeneous energy, wireless channel conditions, and non-independently and identically distributed (non-IID) data distributions. To cope with these issues, this paper proposes a novel learning framework by integrating FL and width-adjustable slimmable neural networks (SNN). Integrating FL with SNNs is challenging due to time-varying channel conditions and data distributions. In addition, existing multi-width SNN training algorithms are sensitive to the data distributions across devices, which makes SNN ill-suited for FL. Motivated by this, we propose a communication and energy-efficient SNN-based FL (named SlimFL) that jointly utilizes superposition coding (SC) for global model aggregation and superposition training (ST) for updating local models. By applying SC, SlimFL exchanges the superposition of multiple-width configurations decoded as many times as possible for a given communication throughput. Leveraging ST, SlimFL aligns the forward propagation of different width configurations while avoiding inter-width interference during backpropagation. We formally prove the convergence of SlimFL. The result reveals that SlimFL is not only communication-efficient but also deals with non-IID data distributions and poor channel conditions, which is also corroborated by data-intensive simulations.}
}


@article{DBLP:journals/ton/LiaoCH23,
	author = {Guocheng Liao and
                  Xu Chen and
                  Jianwei Huang},
	title = {Privacy Protection Under Incomplete Social and Data Correlation Information},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2515--2528},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3254549},
	doi = {10.1109/TNET.2023.3254549},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiaoCH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Data reporters have privacy concerns when they are requested to contribute personal data to a data collector. Such privacy concerns are strengthened by data correlation and social relationship, as the data correlation could inevitably cause privacy issues to their socially-connected individuals who even do not report the data. However, both factors are hard to quantify precisely in practice due to their private nature. Such an incomplete information situation poses great challenges for the data reporters to determine their coupled privacy-preserving strategies and for the data collector to choose a proper privacy-preserving mechanism. This motivates us to propose a novel Bayesian game-theoretic framework to analyze the data reporters’ behaviors. We show that the game has a symmetric Bayesian Nash Equilibrium (BNE) with a threshold structure, which builds a connection between the data reporter’s action and privacy concern under incomplete information. The complicated relationship between the BNE and the data collector’s strategy makes it difficult to solve the data collector’s optimization problem. However, by exploiting the unimodal feature of the problem, we present a low-complexity algorithm to compute the optimal privacy-preserving mechanism. Through analytical and numerical studies, we find that the lack of complete information could cause the data reporters to adopt more conservative strategies but make the data collector adopt a less conservative mechanism, resulting in an overall privacy protection degradation. The simulations further demonstrate that the degradation could be alleviated by stronger data correlation and social relationship, and a higher probability of serious privacy concerns.}
}


@article{DBLP:journals/ton/LvZ23,
	author = {Qian Lv and
                  Zuqing Zhu},
	title = {On the Multilayer Planning of Filterless Optical Networks With {OTN}
                  Encryption},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2529--2544},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3256409},
	doi = {10.1109/TNET.2023.3256409},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LvZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With enhanced cost-effectiveness, filterless optical networks (FONs) have been considered as a promising candidate for future optical infrastructure. However, as the transmission in FON relies on the “select-and-broadcast” scenario, it is more vulnerable to eavesdropping. Therefore, encrypting the communications in FONs will be indispensable, and this can be realized by introducing the optical transport network (OTN) encryption technologies that leverage high-speed encryption cards (ECs) to protect the integrity of OTN payload frames. In this paper, we study the problem of security-aware multilayer planning of FONs with OTN encryption. We first formulate a mixed integer linear programming (MILP) model (i.e., w-MILP) to solve the problem exactly. Then, to reduce the time complexity of problem-solving, we transform w-MILP into two correlated MILP models for establishing fiber trees for an FON (t-MILP) and planning flows in the fiber trees (s-MILP), respectively. The optimization in t-MILP is further transformed into a weighted set partitioning problem, which can be solved time-efficiently. As for s-MILP, we propose a polynomial-time approximation algorithm based on linear programming (LP) relaxation and randomized rounding. Extensive simulations verify the performance of our proposals.}
}


@article{DBLP:journals/ton/WangLYCRZZ23,
	author = {Wei Wang and
                  Xin Liu and
                  Yao Yao and
                  Zicheng Chi and
                  Stuart Ray and
                  Ting Zhu and
                  Yanchao Zhang},
	title = {Simultaneous Data Dissemination Among WiFi and ZigBee Devices},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2545--2558},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3243070},
	doi = {10.1109/TNET.2023.3243070},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangLYCRZZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Recent advances in Cross-Technology Communication (CTC) have opened a new door for cooperation among heterogeneous IoT devices to support ubiquitous applications, such as smart homes and smart offices. However, existing work mainly focuses on physical layer performance improvements. In this paper, we explore how to leverage the latest CTC techniques for network layer performance improvements. Specifically, we introduce Waves, which leverages WiFi to ZigBee CTC and WiFi access point’s adaptive transmit power control techniques for reliable and fast data dissemination in low-duty-cycle ZigBee networks. We extensively evaluate our design under various settings. Evaluation results show that Waves can provide reliable data dissemination and is 33.5 times faster than the state-of-the-art protocol in terms of dissemination time.}
}


@article{DBLP:journals/ton/LaiLZWW23,
	author = {Zeqi Lai and
                  Hewu Li and
                  Qi Zhang and
                  Qian Wu and
                  Jianping Wu},
	title = {StarFront: Cooperatively Constructing Pervasive and Low-Latency CDNs
                  Upon Emerging {LEO} Satellites and Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2559--2574},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3260166},
	doi = {10.1109/TNET.2023.3260166},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LaiLZWW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Internet content providers (ICPs) typically exploit content distribution networks (CDNs) to provide wide-area data access with high availability and low latency. However, our analysis on a large-scale trace collected from seven major CDN operators has revealed that: from a global perspective, there are still a large portion of users suffering from high user-perceived latency due to the insufficient deployment of terrestrial cloud infrastructures, especially in remote or rural areas where even the closest available cache server is too far away. This paper presents STAR FRONT, a cost-effective content distribution framework to optimize global CDNs and enable low content access latency anywhere. STAR FRONT collaboratively builds CDNs upon emerging low earth orbit (LEO) constellations and existing cloud platforms to satisfy the low latency requirements while minimizing the operational cost. Specifically, STAR FRONT exploits a key insight that emerging mega-constellations will consist of thousands of LEO satellites which can be equipped with high-speed data links and storage, and thus can potentially work as “cache in space” to enable pervasive and low-latency data access. STAR FRONT judiciously places replicas on either LEO satellite caches or terrestrial cloud caches, and dynamically assigns user requests to proper cache servers based on different constellation parameters, cloud/user distributions and pricing policies. We have implemented a STAR FRONT prototype in our testbed, and extensive trace-driven evaluations covering multiple geo-distributed vantage points have demonstrated that STAR FRONT can effectively reduce the global content access latency with acceptable operational cost under representative CDN traffic.}
}


@article{DBLP:journals/ton/DouQYG23,
	author = {Songshi Dou and
                  Li Qi and
                  Chao Yao and
                  Zehua Guo},
	title = {Exploring the Impact of Critical Programmability on Controller Placement
                  for Software-Defined Wide Area Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2575--2588},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3252639},
	doi = {10.1109/TNET.2023.3252639},
	timestamp = {Tue, 07 May 2024 20:25:35 +0200},
	biburl = {https://dblp.org/rec/journals/ton/DouQYG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Control latency is a critical concern for deploying Software-Defined Networking (SDN) into Wide Area Networks (WANs). A Software-Defined WAN (SD-WAN) can be divided into multiple domains controlled by multiple controllers with a logically centralized view. The control latency is related to the placement of controllers and mappings between switches and controllers. Existing solutions usually consider the propagation delay between switches and controllers as the evaluation metric and fail to consider many important factors of dynamic network states. In this paper, we propose ProgrammabilityExplorer (PE) to optimize the control latency in SD-WAN. Inspired by the selection of critical flows, which have a critical impact on network performance, PE considers the programmability of critical flows at switches and uses this metric to decide the placement of controllers and mappings between switches and controllers. Simulation results show that PE can reduce the control latency by up to 62.3%, 27.5%, 58.3%, and 61.7% under GÉANT, Abilene, Sprintlink, and Tiscali topologies respectively, compared with baseline algorithms.}
}


@article{DBLP:journals/ton/NosykKLSJD23,
	author = {Yevheniya Nosyk and
                  Maciej Korczynski and
                  Qasim Lone and
                  Marcin Skwarek and
                  Baptiste Jonglez and
                  Andrzej Duda},
	title = {The Closed Resolver Project: Measuring the Deployment of Inbound Source
                  Address Validation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2589--2603},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3257413},
	doi = {10.1109/TNET.2023.3257413},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/NosykKLSJD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Ingress filtering, commonly referred to as Source Address Validation (SAV), is a practice aimed at discarding packets with spoofed source IP addresses at the network periphery. Outbound SAV, i.e., dropping traffic with spoofed source IP addresses as it leaves its source network, has received widespread attention in operational and research communities. It is one of the most effective ways to prevent Reflection-based Distributed Denial-of-Service (DDoS) attacks. Contrariwise, inbound SAV, i.e., dropping incoming spoofed traffic at the destination network edge, has received less attention, even though it provides protection for the deploying network. In this paper, we present the results of the Closed Resolver Project, our initiative aimed at finding networks without inbound SAV and raising awareness of the issue. We perform the first Internet-wide active measurement study to enumerate networks that enforce (or not) inbound SAV. We reach open and closed Domain Name System (DNS) resolvers in tested networks and determine whether they resolve requests with spoofed source IP addresses. Our method provides unprecedented insight into inbound SAV deployment by network operators, revealing 49% IPv4 and 26% IPv6 Autonomous Systems (AS) that suffer from a consistent or partial absence of inbound filtering. By identifying dual-stack DNS resolvers and ASes, we further show that inbound filtering is generally deployed consistently across IPv4 and IPv6. Finally, the lack of inbound SAV exposes 2.5M IPv4 and 100K IPv6 purportedly closed DNS resolvers to many types of external attacks, including NXNSAttack, zone poisoning, or zero-day vulnerabilities in DNS software.}
}


@article{DBLP:journals/ton/FuASCB23,
	author = {Yongquan Fu and
                  Lun An and
                  Siqi Shen and
                  Kai Chen and
                  Pere Barlet{-}Ros},
	title = {A One-Pass Clustering Based Sketch Method for Network Monitoring},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2604--2613},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3251981},
	doi = {10.1109/TNET.2023.3251981},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FuASCB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network monitoring solutions need to cope with increasing network traffic volumes, as a result, sketch-based monitoring methods have been extensively studied to trade accuracy for memory scalability and storage reduction. However, sketches are sensitive to skewness in network flow distributions due to hash collisions, and need complicated performance optimization to adapt to line-rate packet streams. We provide Jellyfish, an efficient sketch method that performs one-pass clustering over the network stream. One-pass clustering is realized by adapting the monitoring granularity from the whole network flow to fragments called subflows, which not only reduces the ingestion rate but also provides an efficient intermediate representation for the input to the sketch. Jellyfish provides the network-flow level query interface by reconstructing the network-flow level counters by merging subflow records from the same network flow. We provide probabilistic analysis of the expected accuracy of both existing sketch methods and Jellyfish. Real-world trace-driven experiments show that Jellyfish reduces the average estimation errors by up to six orders of magnitude for per-flow queries, by six orders of magnitude for entropy queries, and up to ten times for heavy-hitter queries.}
}


@article{DBLP:journals/ton/AbdelmoniemB23,
	author = {Ahmed M. Abdelmoniem and
                  Brahim Bensaou},
	title = {Enhancing {TCP} via Hysteresis Switching: Theoretical Analysis and
                  Empirical Evaluation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2614--2623},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3262564},
	doi = {10.1109/TNET.2023.3262564},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AbdelmoniemB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In this paper we study the relationship between the TCP packet loss cycle and the performance of time-sensitive traffic in data centers. Using real traffic measurements and analysis, we find that such loss cycles are not long enough to enable most partition-aggregate time-sensitive TCP applications to recover their packet losses via the TCP 3-dup ACKs mechanism. As a result, the Timeout (RTO) mechanism is frequently triggered, leading to the expansion of the flow completion times (FCT) of such applications by orders of magnitude. Hence, we seek an alternative method that does not change the virtual machines and that can effectively expand the loss cycle duration to enable short flows to finish their transfer without incurring the cost of the RTO. To this end, we propose a novel TCP-AQM mechanism that alternates between a slow constant bitrate (CBR) mode and a fast TCP rate via hysteresis switching to expand the loss cycle. We prove the stability of the proposed TCP-AQM via a control theoretic model, then evaluate its performance gains via small and large scale NS2 simulation and by real FPGA implementation of a prototype on the NetFPGA platform. The results show considerable improvements in FCT distribution and reduction of missed deadlines in simulation and real experiments.}
}


@article{DBLP:journals/ton/LinWCZXWCLQSJ23,
	author = {Fusheng Lin and
                  Hongyu Wang and
                  Guo Chen and
                  Guihua Zhou and
                  Tingting Xu and
                  Dehui Wei and
                  Li Chen and
                  Yuanwei Lu and
                  Andrew Qu and
                  Hua Shao and
                  Hongbo Jiang},
	title = {Fast, Scalable and Robust Centralized Routing for Data Center Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2624--2639},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3259541},
	doi = {10.1109/TNET.2023.3259541},
	timestamp = {Wed, 24 Jan 2024 17:49:58 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LinWCZXWCLQSJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a fast and robust centralized data center network (DCN) routing solution, called Primus. For fast routing calculation, Primus uses centralized controllers to collect/disseminate the network’s link-states (LS), and offload the actual routing calculation onto each switch. Observing that the routing changes can be classified into a few fixed patterns in DCNs which have regular topologies, we simplify each switch’s routing calculation into a table-lookup manner, i.e., comparing LS changes with pre-installed base topology and updating routing paths according to predefined rules. As such, the routing calculation time at each switch only needs 10s of us even in a large network topology containing 10K+ switches. For efficient controller fault-tolerance, Primus purposely uses reporter switch to ensure the LS updates successfully delivered to all affected switches. As such, Primus can use multiple stateless controllers and little redundant traffic to tolerate failures, which incurs little overhead under normal case, and keeps 10s of ms fast routing reaction time even under complex data-/control-plane failures. We design, implement and evaluate Primus with extensive experiments on Linux-machine controllers and white-box switches. Primus provides ~1200x and ~100x shorter convergence time than current distributed protocol BGP and the state-of-the-art centralized routing solution, respectively. Furthermore, Primus maintains good routing controllability/manageability thanks to its centralized architecture, which enables us to build several advanced routing features in our testbed, including routing failure visualization and weighted-cost-multi-path routing.}
}


@article{DBLP:journals/ton/LiuYZWCW23,
	author = {Xuezheng Liu and
                  Zirui Yan and
                  Yipeng Zhou and
                  Di Wu and
                  Xu Chen and
                  Jessie Hui Wang},
	title = {Optimizing Parameter Mixing Under Constrained Communications in Parallel
                  Federated Learning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2640--2652},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3257236},
	doi = {10.1109/TNET.2023.3257236},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuYZWCW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In vanilla Federated Learning (FL) systems, a centralized parameter server (PS) is responsible for collecting, aggregating and distributing model parameters with decentralized clients. However, the communication link of a single PS can be easily overloaded by concurrent communications with a massive number of clients. To overcome this drawback, multiple PSes can be deployed to form a parallel FL (PFL) system, in which each PS only communicates with a subset of clients and its neighbor PSes. On one hand, each PS conducts iterations with clients in its subset. On the other hand, PSes communicate with each other periodically to mix their parameters so that they can finally reach a consensus. In this paper, we propose a novel parallel federated learning algorithm called Fed-PMA, which optimizes such parallel FL under constrained communications by conducting parallel parameter mixing and averaging with theoretic guarantees. We formally analyze the convergence rate of Fed-PMA with convex loss, and further derive the optimal number of times each PS should mix with its neighbor PSes so as to maximize the final model accuracy within a fixed span of training time. Theoretical study manifests that PSes should mix their parameters more frequently if the connection between PSes is sparse or the time cost of mixing is low. Inspired by our analysis, we propose the Fed-APMA algorithm that can adaptively determine the near-optimal number of mixing times with non-convex loss under dynamic communication conditions. Extensive experiments with realistic datasets are carried out to demonstrate that both Fed-PMA and its adaptive version Fed-APMA significantly outperform the state-of-the-art baselines.}
}


@article{DBLP:journals/ton/MiaoZZWZYLJ23,
	author = {Ruijie Miao and
                  Yinda Zhang and
                  Zihao Zheng and
                  Ruixin Wang and
                  Ruwen Zhang and
                  Tong Yang and
                  Zaoxing Liu and
                  Junchen Jiang},
	title = {CocoSketch: High-Performance Sketch-Based Measurement Over Arbitrary
                  Partial Key Query},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2653--2668},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3257226},
	doi = {10.1109/TNET.2023.3257226},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MiaoZZWZYLJ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketch-based measurement has emerged as a promising solutions due to its high accuracy and resource efficiency. Prior sketches focus on measuring single flow keys and cannot support measurement on multiple keys. This work takes a significant step towards supporting arbitrary partial key queries, which aims to provide information for any key in the predefined range of possible flow keys. The designed system, CocoSketch, casts arbitrary partial key queries to the subset sum estimation problem and makes the theoretical tools for subset sum estimation practical. CocoSketch utilizes two techniques: (1) stochastic variance minimization to significantly reduce per-packet update delay, and (2) removing circular dependencies in the per-packet update logic to make the implementation hardware-friendly. This paper extends the conference version by discussing how CocoSketch adapts to new measurement requirements, including: (1) collecting the exact information of specified flow keys, and (2) distributed measurement. CocoSketch is implemented on five popular platforms (CPU, Open vSwitch, Redis, P4, and FPGA). Experiment results show that compared to baselines that use traditional single-key sketches, CocoSketch improves average packet processing throughput by\n27.2×\nand accuracy by\n10.4×\nwhen measuring six flow keys.}
}


@article{DBLP:journals/ton/ChitavisutthivongSNZYG23,
	author = {Kanatip Chitavisutthivong and
                  Sucha Supittayapornpong and
                  Pooria Namyar and
                  Mingyang Zhang and
                  Minlan Yu and
                  Ramesh Govindan},
	title = {Optimal Oblivious Routing With Concave Objectives for Structured Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2669--2681},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3264632},
	doi = {10.1109/TNET.2023.3264632},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChitavisutthivongSNZYG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Oblivious routing distributes traffic from sources to destinations following predefined routes with rules independent of traffic demands. While finding optimal oblivious routing with a concave objective is intractable for general topologies, we show that it is tractable for structured topologies often used in datacenter networks. To achieve this, we apply graph automorphism and prove the existence of the optimal automorphism-invariant solution. This result reduces the search space to targeting the optimal automorphism-invariant solution. We design an iterative algorithm to obtain such a solution by alternating between convex optimization and a linear program. The convex optimization finds an automorphism-invariant solution based on representative variables and constraints, making the problem tractable. The linear program generates adversarial demands to ensure the final result satisfies all possible demands. Since the construction of the representative variables and constraints are combinatorial problems, we design polynomial-time algorithms for the construction. We evaluate the iterative algorithm in terms of throughput performance, scalability, and generality over three potential applications. The algorithm i) improves the throughput up to 87.5% for partially deployed FatTree and achieves up to\n2.55×\nthroughput gain for DRing over heuristic algorithms, ii) scales for three considered topologies with a thousand switches, iii) applies to a general structured topology with non-uniform link capacity and server distribution.}
}


@article{DBLP:journals/ton/GangulyHKBALC23,
	author = {Bhargav Ganguly and
                  Seyyedali Hosseinalipour and
                  Kwang Taik Kim and
                  Christopher G. Brinton and
                  Vaneet Aggarwal and
                  David J. Love and
                  Mung Chiang},
	title = {Multi-Edge Server-Assisted Dynamic Federated Learning With an Optimized
                  Floating Aggregation Point},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2682--2697},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3262482},
	doi = {10.1109/TNET.2023.3262482},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GangulyHKBALC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We propose cooperative edge-assisted dynamic federated learning ( CE-FL ). CE-FL introduces a distributed machine learning (ML) architecture, where data collection is carried out at the end devices, while the model training is conducted cooperatively at the end devices and the edge servers, enabled via data offloading from the end devices to the edge servers through base stations. CE-FL also introduces floating aggregation point, where the local models generated at the devices and the servers are aggregated at an edge server, which varies from one model training round to another to cope with the network evolution in terms of data distribution and users’ mobility. CE-FL considers the heterogeneity of network elements in terms of communication/computation models and the proximity to one another. CE-FL further presumes a dynamic environment with online variation of data at the network devices which causes a drift at the ML model performance. We model the processes taken during CE-FL , and conduct analytical convergence analysis of its ML model training. We then formulate network-aware CE-FL which aims to adaptively optimize all the network elements via tuning their contribution to the learning process, which turns out to be a non-convex mixed integer problem. Motivated by the large scale of the system, we propose a distributed optimization solver to break down the computation of the solution across the network elements. We finally demonstrate the effectiveness of our framework with the data collected from a real-world testbed.}
}


@article{DBLP:journals/ton/ZhengBV23,
	author = {Jiaxiao Zheng and
                  Albert Banchs and
                  Gustavo de Veciana},
	title = {Constrained Network Slicing Games: Achieving Service Guarantees and
                  Network Efficiency},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2698--2713},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3262810},
	doi = {10.1109/TNET.2023.3262810},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhengBV23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a key capability for next generation mobile networks. It enables infrastructure providers to cost effectively customize logical networks over a shared infrastructure. A critical component of network slicing is resource allocation, which needs to ensure that slices receive the resources needed to support their services while optimizing network efficiency. In this paper, we propose a novel approach to slice-based resource allocation named Guaranteed seRvice Efficient nETwork slicing (GREET). The underlying concept is to set up a constrained resource allocation game, where ( i\n) slices unilaterally optimize their allocations to best meet their (dynamic) customer loads, while ( ii\n) constraints are imposed to guarantee that, if they wish so, slices receive a pre-agreed share of the network resources. The resulting game is a variation of the well-known Fisher market, where slices are provided a budget to contend for network resources (as in a traditional Fisher market), but (unlike a Fisher market) prices are constrained for some resources to ensure that the pre-agreed guarantees are met for each slice. In this way, GREET combines the advantages of a share-based approach (high efficiency by flexible sharing) and reservation-based ones (which provide guarantees by assigning a fixed amount of resources). We characterize the Nash equilibrium, best response dynamics, and propose a practical slice strategy with provable convergence properties. Extensive simulations exhibit substantial improvements over network slicing state-of-the-art benchmarks.}
}


@article{DBLP:journals/ton/ShaoCH23,
	author = {Qi Shao and
                  Man Hon Cheung and
                  Jianwei Huang},
	title = {Crowdfunding With Cognitive Limitations},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2714--2729},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3274114},
	doi = {10.1109/TNET.2023.3274114},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShaoCH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To achieve the desirable funding target in a crowdfunding campaign, the project creator needs to accurately anticipate the pledging behaviors of contributors with practical cognitive limitations. In this paper, we present a study on how the contributors’ cognitive bounded rationality affects the creator’s campaign decisions. Specifically, we consider a two-stage crowdfunding model, where a creator first announces the project decisions (i.e., price and the minimum number of required contributors), and then the contributors choose their pledging behaviors (whether and how to contribute) as they arrive stochastically. We consider the cognitive hierarchy model, where contributors are classified into different levels according to their capability of anticipating other contributors’ behaviors. Surprisingly, we show that the cognitive limitation may improve the chance of project success, especially when the crowdfunding target is high. We prove that with a high average contributor cognitive level, a low funding target, and a low difficulty of motivating the lowest cognitive level contributors, the creator can achieve a close-to-optimal revenue by simply assuming that contributors are fully rational. Otherwise, ignoring the contributors’ cognitive limitations can lead to a significant revenue loss.}
}


@article{DBLP:journals/ton/WangHYC23,
	author = {Chen Wang and
                  Qin Hu and
                  Dongxiao Yu and
                  Xiuzhen Cheng},
	title = {Online Learning for Failure-Aware Edge Backup of Service Function
                  Chains With the Minimum Latency},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2730--2744},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3265127},
	doi = {10.1109/TNET.2023.3265127},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangHYC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Virtual network functions (VNFs) have been widely deployed in mobile edge computing (MEC) to flexibly and efficiently serve end users running resource-intensive applications, which can be further serialized to form service function chains (SFCs), providing customized networking services. To ensure the availability of SFCs, it turns out to be effective to place redundant SFC backups at the edge for quickly recovering from any failures. The existing research largely overlooks the influences of SFC popularity, backup completeness, and failure rate on the optimal deployment of SFC backups on edge servers. In this paper, we comprehensively consider from the perspectives of both the end users and edge system to backup SFCs for providing popular services with the lowest latency. To overcome the challenges resulted from unknown SFC popularity and failure rate, as well as the known system parameter constraints, we take advantage of the online bandit learning technique to cope with the uncertainty issue. Combining the Prim -inspired method with the greedy strategy, we propose a Real-Time Selection and Deployment (RTSD) algorithm. Extensive simulation experiments are conducted to demonstrate the superiority of our proposed algorithms.}
}


@article{DBLP:journals/ton/ShenLDGCWL23,
	author = {Dian Shen and
                  Junzhou Luo and
                  Fang Dong and
                  Xiaolin Guo and
                  Ciyuan Chen and
                  Kai Wang and
                  John C. S. Lui},
	title = {Enabling Distributed and Optimal {RDMA} Resource Sharing in Large-Scale
                  Data Center Networks: Modeling, Analysis, and Implementation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2745--2760},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3263562},
	doi = {10.1109/TNET.2023.3263562},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShenLDGCWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Remote Direct Memory Access (RDMA) suffers from unfairness issues and performance degradation when multiple applications share RDMA network resources. Hence, an efficient resource scheduling mechanism is urged to optimally allocates RDMA resources among applications. However, traditional Network Utility Maximization (NUM) based solutions are inadequate for RDMA due to three challenges: 1) The standard NUM-oriented algorithm cannot deal with coupling variables introduced by multiple dependent RDMA operations; 2) The stringent constraint of RDMA on-board resources complicates the standard NUM by bringing extra optimization dimensions; 3) Naively applying traditional algorithms for NUM suffers from scalability issues in solving a large-scale RDMA resource scheduling problem. In this paper, we present how to optimally share the RDMA resources in large-scale data center networks with a distributed manner. First, we propose Distributed RDMA NUM (DRUM) to model the RDMA resource scheduling problem as a new variation of the NUM problem. Second, we present distributed algorithms to efficiently solve the large-scale, interdependent RDMA resource sharing problem for different RDMA use cases. Through theoretical analysis, the convergence and parallelism of proposed algorithms are guaranteed. Finally, we implement the algorithms as a kernel-level indirection module in the real-world RDMA environment, so as to provide end-to-end resource sharing and performance guarantee. Through extensive evaluations by large-scale simulations and testbed experiments, we show that our method significantly improves applications’ performance under resource contention, achieving\n1.7−3.1×\nhigher throughput, and in a dynamic context, the largest performance improvement reaches 98.1% and 64.1% in terms of latency and throughput, respectively.}
}


@article{DBLP:journals/ton/HoangNP23,
	author = {Linh T. Hoang and
                  Chuyen T. Nguyen and
                  Anh T. Pham},
	title = {Deep Reinforcement Learning-Based Online Resource Management for UAV-Assisted
                  Edge Computing With Dual Connectivity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2761--2776},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3263538},
	doi = {10.1109/TNET.2023.3263538},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HoangNP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile Edge Computing (MEC) is a key technology towards delay-sensitive and computation-intensive applications in future cellular networks. In this paper, we consider a multi-user, multi-server system where the cellular base station is assisted by a UAV, both of which provide additional MEC services to the terrestrial users. Via dual connectivity (DC), each user can simultaneously offload tasks to the macro base station and the UAV-mounted MEC server for parallel computing, while also processing some tasks locally. We aim to propose an online resource management framework that minimizes the average power consumption of the whole system, considering long-term constraints on queue stability and computational delay of the queueing system. Due to the coexistence of two servers, the problem is highly complex and formulated as a multi-stage mixed integer non-linear programming (MINLP) problem. To solve the MINLP with reduced computational complexity, we first adopt Lyapunov optimization to transform the original multi-stage problem into deterministic problems that are manageable in each time slot. Afterward, the transformed problem is solved using an integrated learning-optimization approach, where model-free Deep Reinforcement Learning (DRL) is combined with model-based optimization. Via extensive simulation and theoretical analyses, we show that the proposed framework is guaranteed to converge and can produce nearly the same performance as the optimal solution obtained via an exhaustive search.}
}


@article{DBLP:journals/ton/ZhaoQ23,
	author = {Yangming Zhao and
                  Chunming Qiao},
	title = {Distributed Transport Protocols for Quantum Data Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2777--2792},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3262547},
	doi = {10.1109/TNET.2023.3262547},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhaoQ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Quantum computing holds great promise and this work proposes to use new quantum data networks (QDNs) to connect multiple small quantum computers to form a cluster. Such a QDN differs from existing quantum key distribution (QKD) networks in that the former must deliver data quantum bits (i.e., qubits) reliably between different quantum computers. Two families of QDNs are studied, one using teleportation, named Tele-QDN, and the other using tell-and-go (TAG), named TAG-QDN. In order to provide reliable delivery of data qubits, while addressing QDN-specific constraints imposed by quantum physics laws such as the no-cloning theorem, and limited availability of quantum memory, two corresponding transport layer protocols suitable for distributed implementation are designed and evaluated. Such distributed quantum transport protocols (DTPs), named Tele-DTP and TAG-DTP, are the first-of-its-kind and are complementary to existing works on the protocol stack for QDNs which are at the network layer and below. Both analysis and extensive simulations show that the proposed DTPs can achieve high throughput and fairness. This study also offers new insights into potential tradeoffs involved in using different types of QDNs.}
}


@article{DBLP:journals/ton/LiJZSLLJW23,
	author = {Ruixuan Li and
                  Xiaofeng Jia and
                  Zhenyong Zhang and
                  Jun Shao and
                  Rongxing Lu and
                  Jingqiang Lin and
                  Xiaoqi Jia and
                  Guiyi Wei},
	title = {A Longitudinal and Comprehensive Measurement of {DNS} Strict Privacy},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2793--2808},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3262651},
	doi = {10.1109/TNET.2023.3262651},
	timestamp = {Mon, 22 Jan 2024 17:00:28 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiJZSLLJW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The DNS privacy protection mechanisms, DNS over TLS (DoT) and DNS over HTTPS (DoH), only work correctly if both the server and client support the Strict Privacy profile and no vulnerability exists in the implemented TLS/HTTPS. A natural question then arises: what is the landscape of DNS Strict Privacy? To this end, we provide the first longitudinal and comprehensive measurement of DoT/DoH deployments in recursive resolvers, authoritative servers, and browsers. With the collected data, we find the number of DoT/DoH servers increased substantially during our ten-month-long scan. However, around 60% of DoT and 44% of DoH recursive resolver certificates are invalid. Worryingly, our measurements confirm the centralization problem of DoT/DoH. Furthermore, we classify DNS Strict Privacy servers into four levels according to daily scanning results on TLS/HTTPS-related security features. Unfortunately, around 25% of DoH Strict Privacy recursive resolvers fail to meet the minimum level requirements. To help the Internet community better perceive the landscape of DNS Strict Privacy, we implement a DoT/DoH server search engine and recommender system. Additionally, we investigate five popular browsers across four operating systems and find some inconsistent behavior with their DNS privacy implementations. For example, Firefox in Windows, Linux, and Android allows DoH communication with the server without the SAN certificate. At last, we advocate that all participants head together for a bright DNS Strict Privacy landscape by discussing current hindrances and controversies in DNS privacy.}
}


@article{DBLP:journals/ton/YangCZZS23,
	author = {Dong Yang and
                  Zongrong Cheng and
                  Weiting Zhang and
                  Hongke Zhang and
                  Xuemin Shen},
	title = {Burst-Aware Time-Triggered Flow Scheduling With Enhanced Multi-CQF
                  in Time-Sensitive Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2809--2824},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3264583},
	doi = {10.1109/TNET.2023.3264583},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangCZZS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Deterministic transmission guarantee in time-sensitive networks (TSN) relies on queue models (such as CQF, TAS, ATS) and resource scheduling algorithms. Thanks to its ease of use, the CQF queue model has been widely adopted. However, the existing resource scheduling algorithms of CQF model only focus on periodic time-triggered (TT) flows without consideration of bursting flows. Considering that the bursting flows often carry high-priority data in real systems, in this paper we investigate the mixed-flow (i.e., TT and bursting flows) scheduling problem in CQF-based TSN aiming to maximize the number of schedulable flows and system load balance while satisfying the deterministic demands of delay, jitter, and reliability for both TT and bursting flows. Unfortunately, it is challenging to schedule the mixed flows with the original CQF model because of the huge difference between TT and bursting flows. To resolve this problem, we firstly design an enhanced Multi-CQF model to satisfy the basic demands of bursting flows sent at any time without affecting the deterministic transmission of TT flows. Given the complexity of mixed-flow scheduling and the proposed queue model, it is difficult for traditional algorithms to fully utilize network resources. Thus, we further propose a uline time-correlated uline DRL uline resource uline scheduling (TimeDRS) algorithm to optimize the resource allocation. TimeDRS can be extended to other time-related resource scheduling scenarios, such as TDMA-based scheduling. Experimental results demonstrate that our proposed approaches can greatly reduce frame loss and end-to-end latency for bursting flows, and well balance runtime and schedulability compared with state-of-the-art benchmarks.}
}


@article{DBLP:journals/ton/LuoFLGZW23,
	author = {Lailong Luo and
                  Pengtao Fu and
                  Shangsen Li and
                  Deke Guo and
                  Qianzhen Zhang and
                  Huaimin Wang},
	title = {Ark Filter: {A} General and Space-Efficient Sketch for Network Flow
                  Analysis},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2825--2839},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3263839},
	doi = {10.1109/TNET.2023.3263839},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LuoFLGZW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Sketches are widely deployed to represent network flows to support complex flow analysis. Typical sketches usually employ hash functions to map elements into a hash table or bit array. Such sketches still suffer from potential weaknesses upon throughput, flexibility, and functionality. To this end, we propose Ark filter, a novel sketch that stores the element information with either of two candidate buckets indexed by the quotient or remainder between the fingerprint and filter length. In this way, no further hash calculations are required for future queries or reallocations. We further extend the Ark filter to enable capacity elasticity and more functionalities (such as frequency estimation and top-\nk\nquery). Comprehensive experiments demonstrate that, compared with Cuckoo filter, Ark filter has\n2.08×\n,\n1.34×\n, and\n1.68×\nthroughput of deletion, insertion, and hybrid query, respectively; compared with Quotient filter, Ark filter has\n4.55×\n,\n1.74×\n, and\n22.12×\nthroughput of deletion, insertion, and hybrid query, respectively; compared with Bloom filter, Ark filter has\n2.55×\nand\n2.11×\nthroughput of insertion and hybrid query, respectively.}
}


@article{DBLP:journals/ton/HuangGZGC23,
	author = {Xiuqi Huang and
                  Yuanning Gao and
                  Xinyi Zhou and
                  Xiaofeng Gao and
                  Guihai Chen},
	title = {An Adaptive Metadata Management Scheme Based on Deep Reinforcement
                  Learning for Large-Scale Distributed File Systems},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2840--2853},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3266400},
	doi = {10.1109/TNET.2023.3266400},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangGZGC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {A major challenge confronting today’s distributed metadata management schemes is how to meet the dynamic requirements of various applications through effectively mapping and migrating metadata nodes to different metadata servers (MDS’s). Most of the existing works dynamically reallocate nodes to different servers adopting history-based coarse-grained methods, failing to make a timely and efficient update on the distribution of nodes. In this paper, we present the first deep reinforcement learning-leveraged distributed metadata management scheme, AdaM, to address the aforementioned dilemma. AdaM is an adaptive fine-grained metadata management scheme that trains an actor-critic network to migrate “hot” metadata nodes to different MDS’s based on its observations of the current “states” (i.e., access pattern, the structure of namespace tree and current distribution of nodes on MDS’s). Adaptive to varying access patterns, AdaM can automatically migrate hot metadata nodes among servers to keep load balancing while maintaining metadata locality. Besides, we propose a self-adaptive metadata cache policy, which dynamically combines the two strategies of managing caches on the server side and the client side to gain better query performance. Last but not least, we design a distributed metadata processing 2PC Protocol called MST-based 2PC to ensure data consistency. Experiments on a real-world dataset demonstrate the superiority of our proposed method over other schemes.}
}


@article{DBLP:journals/ton/ZhangWLSLH23,
	author = {Zhibo Zhang and
                  Huiqiang Wang and
                  Hongwu Lv and
                  Jiayu Sun and
                  Guodong Li and
                  Xin Han},
	title = {{CHAT:} Accurate Network Latency Measurement for 5G {E2E} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2854--2869},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3264009},
	doi = {10.1109/TNET.2023.3264009},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangWLSLH23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {As numerous latency-sensitive applications have emerged with the popularization of 5G networking, accurate and rapid end-to-end latency measurement has come to play an essential role in network fault diagnosis and optimization. Although the Bloom hash-based timestamp aggregation method has been reported to be scalable and efficient, two shortcomings that reduce its accuracy have yet to be fully considered: frequent hash collisions and its fixed measurement interval. To address these challenges, we construct an end-to-end network latency measurement framework named Cuckoo Hash Adjustive Table exchange (CHAT). By employing an improved cuckoo filter, we decrease the number of hash collisions to assess the latency more accurately. Moreover, CHAT adjusts the receiver-side measurement interval dynamically based on a gain indicator, maximizing the total number of valid packets used for latency estimation. Additionally, the proposed measurement framework minimizes the number of packets transferred over links to avoid interfering with the end-to-end latency measurement in an actual network. Finally, extensive experiments on simulations and a practical real-world environment show the effectiveness and applicability of CHAT.}
}


@article{DBLP:journals/ton/CaoZZLXTLWZ23,
	author = {Peirui Cao and
                  Shizhen Zhao and
                  Dai Zhang and
                  Zhuotao Liu and
                  Mingwei Xu and
                  Min Yee Teh and
                  Yunzhuo Liu and
                  Xinbing Wang and
                  Chenghu Zhou},
	title = {Threshold-Based Routing-Topology Co-Design for Optical Data Center},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2870--2885},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3265276},
	doi = {10.1109/TNET.2023.3265276},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CaoZZLXTLWZ23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Despite the bandwidth scaling limit of electrical switching and the high cost of building Clos data center networks (DCNs), the adoption of optical DCNs is still limited. There are two reasons. First, existing optical DCN designs usually face high deployment complexity. Second, these designs are not full-optical and the performance benefit over the non-blocking Clos DCN is not clear. After exploring the design tradeoffs of the existing optical DCN designs, we propose TROD (Threshold Routing based Optical Datacenter), a low-complexity optical DCN with superior performance than other optical DCNs. There are two novel designs in TROD that contribute to its success. First, TROD performs robust topology optimization based on the recurring traffic patterns and thus does not need to react to every traffic change, which lowers deployment and management complexity. Second, TROD introduces tVLB (threshold-based Valiant Load Balance), which can avoid network congestion as much as possible even under unexpected traffic bursts. We conduct simulation based on both Facebook’s real DCN traces and our synthesized highly bursty DCN traces. TROD reduces flow completion time (FCT) by about 1.15-\n2.16×\ncompared to Google’s Jupiter DCN, at least\n2×\ncompared to other optical DCN designs, and about 2.4-\n3.2×\ncompared to expander graph DCN. Compared with the non-blocking Clos, TROD reduces the hop count of the majority packets by one, and could even outperform the non-blocking Clos with proper bandwidth over-provision at the optical layer. Note that TROD can be built with commercially available hardware and does not require host modifications.}
}


@article{DBLP:journals/ton/MalikSE23,
	author = {Adeel Malik and
                  Berksan Serbetci and
                  Petros Elia},
	title = {Coded Caching in Networks With Heterogeneous User Activity},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2886--2901},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3270567},
	doi = {10.1109/TNET.2023.3270567},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MalikSE23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This work elevates coded caching networks from their purely information-theoretic framework to a stochastic setting, by exploring the effect of random user activity and by exploiting correlations in the activity patterns of different users. In particular, the work studies the\nK\n-user cache-aided broadcast channel with a limited number of cache states (i.e., the content stored at the cache of a certain user), and explores the effect of cache state association strategies in the presence of arbitrary user activity levels; a combination that strikes at the very core of the coded caching problem and its crippling subpacketization bottleneck. We first present a statistical analysis of the average worst-case delay performance of such subpacketization-constrained (state-constrained) coded caching networks, and provide computationally efficient performance bounds as well as scaling laws for any arbitrary probability distribution of the user-activity levels. The achieved performance is a result of a novel user-to-cache state association algorithm that leverages the knowledge of probabilistic user-activity levels. We then follow a data-driven approach that exploits the prior history on user-activity levels and correlations, in order to predict interference patterns, and thus better design the caching algorithm. This optimized strategy is based on the principle that users that overlap more, interfere more, and thus have higher priority to secure complementary cache states. This strategy is proven here to be within a small constant factor from the optimal. Finally, the above analysis is validated numerically using synthetic data following the Pareto principle. To the best of our understanding, this is the first work that seeks to exploit user-activity levels and correlations, in order to map future interference and design optimized coded caching algorithms that better handle this interference.}
}


@article{DBLP:journals/ton/MohammadpourSB23,
	author = {Ehsan Mohammadpour and
                  Eleni Stai and
                  Jean{-}Yves Le Boudec},
	title = {Improved Network-Calculus Nodal Delay-Bounds in Time-Sensitive Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2902--2917},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3275910},
	doi = {10.1109/TNET.2023.3275910},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MohammadpourSB23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In time-sensitive networks, bounds on worst-case delays are typically obtained by using network calculus and assuming that flows are constrained by bit-level arrival curves. However, in IEEE TSN or IETF DetNet, source flows are constrained on the number of packets rather than bits. A common approach to obtain a delay bound is to derive a bit-level arrival curve from a packet-level arrival curve. However, such a method is not tight: we show that better bounds can be obtained by directly exploiting the arrival curves expressed at the packet level. Our analysis method also obtains better bounds when flows are constrained with g-regulation, such as the recently proposed Length-Rate Quotient rule. It can also be used to generalize some recently proposed network-calculus delay-bounds for a service curve element with known transmission rate.}
}


@article{DBLP:journals/ton/MoltafetLCY23,
	author = {Mohammad Moltafet and
                  Markus Leinonen and
                  Marian Codreanu and
                  Roy D. Yates},
	title = {Status Update Control and Analysis Under Two-Way Delay},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2918--2933},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3270388},
	doi = {10.1109/TNET.2023.3270388},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/MoltafetLCY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We study status updating under two-way delay in a system consisting of a sampler, a sink, and a controller residing at the sink. The controller drives the sampling process by sending request packets to the sampler. Upon receiving a request, the sampler generates a sample and transmits the status update packet to the sink. Transmissions of both request and status update packets encounter random delays. We develop optimal control policies to minimize the average age of information (AoI) using the tools of Markov decision processes in two scenarios. We begin with the system having at most one active request, i.e., a generated request for which the sink has not yet received a status update packet. Then, as the main distinctive feature of this paper, we initiate pipelined-type status updating by studying a system having at most two active requests. Furthermore, we conduct AoI analysis by deriving the average AoI expressions for the Zero-Wait-1, Zero-Wait-2, and Wait-1 policies. According to the Zero-Wait-1 policy, whenever a status update packet is delivered to the sink, a new request packet is inserted into the system. The Zero-Wait-2 policy operates similarly, except that the system can hold two active requests. According to the Wait-1 policy, whenever a status update packet is delivered to the sink, a new request is sent after a waiting time which is a function of the current AoI. Numerical results illustrate the performance of each status updating policy under varying system parameter values.}
}


@article{DBLP:journals/ton/XiangZZDGD23,
	author = {Zhengzhe Xiang and
                  Yuhang Zheng and
                  Zengwei Zheng and
                  Shuiguang Deng and
                  Minyi Guo and
                  Schahram Dustdar},
	title = {Cost-Effective Traffic Scheduling and Resource Allocation for Edge
                  Service Provisioning},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2934--2949},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3265002},
	doi = {10.1109/TNET.2023.3265002},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiangZZDGD23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The multi-access edge computing (MEC) paradigm has emerged as a critical solution to address the exponential growth in mobile web services and devices. By implementing an edge-based service provisioning system (EPS) with servers located at the network’s edge, both transmission and computation efficiency can be significantly enhanced. Nevertheless, it is also essential to carefully consider the resource allocation for services, the traffic management of requests, and the path arrangement for data delivery to ensure the cost-effective operation of the EPS. Therefore, we investigate and quantify the relationship between the performance and cost of the EPS in this paper, and model the cost-effective service provisioning problem as a multi-phase convex optimization problem. An online algorithm whose name is RDC based on the Lyapunov framework is proposed to decompose this problem into several sub-problems.Additionally, a heuristic approach that partitions edge servers into several clusters, called RDC-NeP and based on RDC , has also been proposed to reduce computational complexity. A series of experiments were conducted to evaluate the proposed approach. The results demonstrate that RDC can effectively balance expense and performance, while RDC-NeP significantly simplifies the processing of RDC when the problem scale increases.}
}


@article{DBLP:journals/ton/ShenHWWL23,
	author = {Shihao Shen and
                  Yiwen Han and
                  Xiaofei Wang and
                  Shiqiang Wang and
                  Victor C. M. Leung},
	title = {Collaborative Learning-Based Scheduling for Kubernetes-Oriented Edge-Cloud
                  Network},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2950--2964},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3267168},
	doi = {10.1109/TNET.2023.3267168},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ShenHWWL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Kubernetes (k8s) has the potential to coordinate distributed edge resources and centralized cloud resources, but currently lacks a specialized scheduling framework for edge-cloud networks. Besides, the hierarchical distribution of heterogeneous resources makes the modeling and scheduling of k8s-oriented edge-cloud network particularly challenging. In this paper, we introduce KaiS, a learning-based scheduling framework for such edge-cloud network to improve the long-term throughput rate of request processing. First, we design a coordinated multiagent actor-critic algorithm to cater to decentralized request dispatch and dynamic dispatch spaces within the edge cluster. Second, for diverse system scales and structures, we use graph neural networks to embed system state information, and combine the embedding results with multiple policy networks to reduce the orchestration dimensionality by stepwise scheduling. Finally, we adopt a two-time-scale scheduling mechanism to harmonize request dispatch and service orchestration, and present the implementation design of deploying the above algorithms compatible with native k8s components. Experiments using real workload traces show that KaiS can successfully learn appropriate scheduling policies, irrespective of request arrival patterns and system scales. Moreover, KaiS can enhance the average system throughput rate by 15.9% while reducing scheduling cost by 38.4% compared to baselines.}
}


@article{DBLP:journals/ton/BorkotokyP23,
	author = {Siddhartha S. Borkotoky and
                  Michael B. Pursley},
	title = {Analytical Techniques for Performance Evaluation of Fountain-Coded
                  File Distribution in Packet Radio Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2965--2977},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3270263},
	doi = {10.1109/TNET.2023.3270263},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/BorkotokyP23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Convenient analytical techniques are developed for use in preliminary design of systems and protocols for the delivery of fountain-coded files from a source node to a destination node with the assistance of a relay node. Each node has a half-duplex packet radio, and the time-varying disturbance on each link is represented by a lumpable finite-state Markov chain. The analytical techniques include the use of first passage times and recurrence times in the derivation of an exact expression for the completion time of file transfer over a radio link and the application of results on Markov-correlated Bernoulli trials to obtain exact expressions for completion times of relay-aided file transfer in wireless networks. In the development of analytical techniques, we employ capacity-achieving channel codes and ideal fountain codes; however, we present simulation results for packet radios that employ practical channel codes and for which the fountain code is a standard systematic raptor code.}
}


@article{DBLP:journals/ton/AyalaRomeroGCI23,
	author = {Jose A. Ayala{-}Romero and
                  Andres Garcia{-}Saavedra and
                  Xavier P{\'{e}}rez Costa and
                  George Iosifidis},
	title = {EdgeBOL: {A} Bayesian Learning Approach for the Joint Orchestration
                  of vRANs and Mobile Edge {AI}},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2978--2993},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3268981},
	doi = {10.1109/TNET.2023.3268981},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/AyalaRomeroGCI23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Future mobile networks need to support intelligent services which collect and process data streams at the network edge, so as to offer real-time and accurate inferences to users. However, the widespread deployment of these services is hindered by the unprecedented energy cost they induce to the network, and by the difficulties in optimizing their end-to-end operation. To address these challenges, we propose a Bayesian learning framework for jointly configuring the service and the Radio Access Network (RAN), aiming to minimize the total energy consumption while respecting accuracy and latency service requirements. Using a fully-fledged prototype with a software-defined base station (vBS) and a GPU-enabled edge server, we profile a typical video analytics service and identify new performance trade-offs and optimization opportunities. Accordingly, we tailor the proposed learning framework to account for the (possibly varying) network conditions, user needs, and service metrics, and apply it to a range of experiments with real traces. Our findings suggest that this approach effectively adapts to different hardware platforms and service requirements, and outperforms state-of-the-art benchmarks based on neural networks.}
}


@article{DBLP:journals/ton/XiaoCCC23,
	author = {Qingjun Xiao and
                  Yuexiao Cai and
                  Yunpeng Cao and
                  Shigang Chen},
	title = {Accurate and O(1)-Time Query of Per-Flow Cardinality in High-Speed
                  Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {2994--3009},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3268980},
	doi = {10.1109/TNET.2023.3268980},
	timestamp = {Fri, 08 Mar 2024 13:21:43 +0100},
	biburl = {https://dblp.org/rec/journals/ton/XiaoCCC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {On a high-speed link, there may be tens of millions of IP packets per second and millions of active flows. Maintaining the state of each flow is a fundamental task underlying many network functions, such as load balancing and network anomaly detection. There are two important kinds of per-flow states: per-flow size (e.g., the number of packets received by an arbitrary destination IP) and per-flow cardinality (e.g., the number of distinct source IP addresses that contacted each destination IP). In this paper, we focus on the latter kind of states, and define a new problem: online query of per-flow cardinality, in which we query any given flow’s cardinality entirely on the data plane with low time complexity. For this problem, we propose three solutions named On-vHLL, Ton-vHLL and Aton-vHLL, whose time cost are\nO(1)\neven for the query operation. Our proposed techniques are three folds. First, we redesign the traditional vHLL with new supplementary data structures called incremental update units (IUUs). When a certain flow’s cardinality is queried, these IUUs can avoid scanning the whole data structure and reduce the time complexity to\nO(1)\n. Second, we apply a HLL register compression technique called TailCut to the On-vHLL sketch, which can save memory cost by 50%. Third, we add a prefilter based on min-heap, alongside the Ton-vHLL sketch. The prefilter is to give each currently sampled top-\nk\nsuperspreader a dedicated HyperLogLog estimator for better accuracy. It can also absorb the superspreaders’ packets bypassing the sketch. We evaluate our new sketches by simulation with CAIDA traces. The results show that our On-vHLL, Ton-vHLL and Aton-vHLL sketches need about 5 memory accesses per packet. The time cost of query operation decreases by hundreds of times than the traditional vHLL that can only be queried offline. Meanwhile, the estimation error of flow spread by our Aton-vHLL is comparable to vHLL.}
}


@article{DBLP:journals/ton/DengLZFC23,
	author = {Lei Deng and
                  Xiao{-}Yang Liu and
                  Haifeng Zheng and
                  Xinxin Feng and
                  Zhizhang Chen},
	title = {Graph-Tensor Neural Networks for Network Traffic Data Imputation},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3010--3024},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3268982},
	doi = {10.1109/TNET.2023.3268982},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/DengLZFC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {It is important to estimate the global network traffic data from partial traffic measurements for many network management tasks, including status monitoring and fault detection. However, existing estimation approaches cannot well handle the topological correlations hidden in network traffic and suffer from limited imputation performance. This paper proposes a deep learning approach for network traffic imputation, which well exploits the topological structure of network traffic. We first model the network traffic as a novel graph-tensor and derive a theoretical recovery guarantee. Then we develop an iterative graph-tensor completion algorithm and propose a graph neural network for network traffic imputation by unfolding the iterative algorithm. The proposed graph neural network well captures the topological correlations of network traffic and achieves accurate imputation. Extensive experiments on real-world datasets show that the proposed graph neural network achieves about one-half lower relative square error while at least ten times faster imputation speed than the existing methods.}
}


@article{DBLP:journals/ton/PrasadS23,
	author = {Reshma Prasad and
                  Albert Sunny},
	title = {Scheduling Slice Requests in 5G Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3025--3036},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3274480},
	doi = {10.1109/TNET.2023.3274480},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/PrasadS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network slicing is a 5G paradigm that enables the creation of on-demand logical networks over shared physical infrastructure. In this paper, we present a framework that allows users to make advance slice reservations with the End-to-End Orchestrator (EEO). Our reservation mechanism enables the EEO to make admission decisions instantly upon request arrival, providing guarantees as to when the request can be enabled. We then proceed to address a relevant revenue maximization problem through an optimal solution, which has factorial time complexity. We also propose a low-complexity algorithm that can efficiently allocate resources for the online version of the problem. We conduct evaluations that demonstrate how the reservation mechanism can potentially improve EEO’s revenue. Additionally, we conduct a study on scenarios where the arrival rates of slice requests exhibit a positive correlation with reservation discounts provided by EEO.}
}


@article{DBLP:journals/ton/ChenY23,
	author = {Paizhuo Chen and
                  Zhice Yang},
	title = {Understanding {PTP} Performance in Today's Wi-Fi Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3037--3050},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3267748},
	doi = {10.1109/TNET.2023.3267748},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ChenY23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Emerging mobile applications involving distributed control and sensing call for accurate time synchronization over wireless links. This paper systematically studies the performance of Precision Time Protocol (PTP) in today’s Wi-Fi networks. We investigate both software and hardware PTP implementations. Our study uncovers the root causes of software PTP synchronization errors. We show that with fine-tuned system configurations and an online calibration procedure, software PTP can achieve reasonable accuracy with off-the-shelf Wi-Fi devices. Hardware PTP requires a PTP hardware timestamper clock not contained in Wi-Fi NICs. We propose a method to make use of the hardware TSF counter to emulate the PTP clock. Rigorous tests traversing various conditions show that both software and hardware PTP implementations can achieve 1- \\mu s level accuracy on current ARM and x86 mobile platforms with practical Wi-Fi settings.}
}


@article{DBLP:journals/ton/VassBBHT23,
	author = {Bal{\'{a}}zs Vass and
                  Erika R. B{\'{e}}rczi{-}Kov{\'{a}}cs and
                  {\'{A}}bel Barab{\'{a}}s and
                  Zsombor L. Hajd{\'{u}} and
                  J{\'{a}}nos Tapolcai},
	title = {A Whirling Dervish: Polynomial-Time Algorithm for the Regional SRLG-Disjoint
                  Paths Problem},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3051--3062},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3276815},
	doi = {10.1109/TNET.2023.3276815},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/VassBBHT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {The current best practice in survivable routing is to compute link or node disjoint paths in the network topology graph. It can protect single-point failures; however, several failure events may cause the interruption of multiple network elements. The set of network elements subject to potential failure events is called Shared Risk Link Group (SRLG), identified during network planning. Unfortunately, for any given list of SRLGs, finding two paths that can survive a single SRLG failure is NP-Complete. In this paper, we provide a polynomial-time SRLG-disjoint routing algorithm for planar network topologies and a large set of SRLGs. Namely, we focus on regional failures, where the failed network elements must not be far from each other. We use a flexible definition of regional failure, where the only restrictions are that i) the topology is a planar graph, ii) each SRLG forms a set of connected edges in the dual of the planar graph, and iii) for each node\nv\n, the links incident to\nv\nare part of an SRLG. The proposed algorithm is based on a max-min theorem. Through extensive simulations, we show that the algorithm scales well with the network size, and one of the paths returned by the algorithm is only 4% longer than the shortest path on average.}
}


@article{DBLP:journals/ton/ZhangZXXYZQHX23,
	author = {Qianyu Zhang and
                  Gongming Zhao and
                  Liguang Xie and
                  Hongli Xu and
                  Zhuolong Yu and
                  Yangming Zhao and
                  Chunming Qiao and
                  Liusheng Huang and
                  Ying Xiong},
	title = {Scalable and Robust East-West Forwarding Framework for Hyperscale
                  Clouds},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3063--3079},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3269772},
	doi = {10.1109/TNET.2023.3269772},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/ZhangZXXYZQHX23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the broad deployment of distributed applications on clouds, east-west traffic is now dominating the majority of cloud networks. The existing communication solutions are tightly coupled with either the control plane ( e.g. , preprogrammed model) or the location of compute nodes ( e.g. , conventional gateway model). As a result, it is difficult to flexibly respond to the rapidly expanding networks and frequent abnormal events ( e.g. , burst traffic and device failures). Accordingly, they may not provide high-performance east-west forwarding while ensuring scalability and robustness. To address this issue, we design Zeta, a scalable and robust east-west forwarding framework with gateway clusters for hyperscale clouds. Zeta abstracts the traffic forwarding capability as a Gateway Cluster Layer, decoupled from the logic of control plane and the location of compute nodes. Specifically, Zeta adopts gateway clusters to support large-scale networks and cope with burst traffic. Moreover, a transparent Multi IPs Migration is proposed for fast recovery from unpredictable failures. We implement Zeta based on eXpress Data Path (XDP) and evaluate its scalability and robustness through comprehensive experiments with up to 100k container instances. Our evaluation shows that Zeta reduces the 99% RTT by\n5.1×\nin burst video traffic, and reduces the gateway pure recovery delay by\n10.8×\ncompared with the state-of-the-art solutions.}
}


@article{DBLP:journals/ton/GalmesPSRXSCBC23,
	author = {Miquel Ferriol Galm{\'{e}}s and
                  Jordi Paillisse and
                  Jos{\'{e}} Su{\'{a}}rez{-}Varela and
                  Krzysztof Rusek and
                  Shihan Xiao and
                  Xiang Shi and
                  Xiangle Cheng and
                  Pere Barlet{-}Ros and
                  Albert Cabellos{-}Aparicio},
	title = {RouteNet-Fermi: Network Modeling With Graph Neural Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3080--3095},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3269983},
	doi = {10.1109/TNET.2023.3269983},
	timestamp = {Mon, 05 Feb 2024 20:24:14 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GalmesPSRXSCBC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles — e.g., with complex non-Markovian models — and arbitrary routing and queue scheduling configurations. Our experimental results show that RouteNet-Fermi achieves similar accuracy as computationally-expensive packet-level simulators and scales accurately to larger networks. Our model produces delay estimates with a mean relative error of 6.24% when applied to a test dataset of 1,000 samples, including network topologies one order of magnitude larger than those seen during training. Finally, we have also evaluated RouteNet-Fermi with measurements from a physical testbed and packet traces from a real-life network.}
}


@article{DBLP:journals/ton/CohenCGS23,
	author = {Itamar Cohen and
                  Carla{-}Fabiana Chiasserini and
                  Paolo Giaccone and
                  Gabriel Scalosub},
	title = {Dynamic Service Provisioning in the Edge-Cloud Continuum With Bounded
                  Resources},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3096--3111},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3271674},
	doi = {10.1109/TNET.2023.3271674},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/CohenCGS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a hierarchical edge-cloud architecture in which services are provided to mobile users as chains of virtual network functions. Each service has specific computation requirements and target delay performance, which require placing the corresponding chain properly and allocating a suitable amount of computing resources. Furthermore, chain migration may be necessary to meet the services’ target delay. We model and formalize the problem of finding a feasible chain placement and resource allocation, while minimizing the migration, bandwidth, and computation costs. We tackle this problem by partitioning it into a (i) CPU allocation problem, and a (ii) placement problem. For the CPU allocation problem, we find an optimal solution. For the placement problem, we show that even finding a feasible solution is NP-hard, and envision an algorithm that is guaranteed to find a feasible solution while leveraging a bounded amount of resource augmentation. Our algorithms are incorporated into a solution framework that aims to minimize both the cost and the required resource augmentation. The results, obtained through trace-driven, large-scale simulations, show that our framework can provide a close-to-optimal solution while running several orders of magnitude faster than an ILP solver.}
}


@article{DBLP:journals/ton/LiZCS23,
	author = {Xionglve Li and
                  Tongqing Zhou and
                  Zhiping Cai and
                  Jinshu Su},
	title = {Realizing Fine-Grained Inference of {AS} Path With a Generative Measurable
                  Process},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3112--3127},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3270565},
	doi = {10.1109/TNET.2023.3270565},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiZCS23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In the global Internet, the paths between two autonomous systems (ASes), which are used for the exchange of traffic, are essential for understanding the behavior of the Internet routing system and they can help improve the performance of many applications of the Internet. Popular approaches to obtain the AS path between an AS pair (AP) are measurement based (e.g., Traceroute), but considering the size of the modern Internet and the limitations of measurement resources, only paths between a very small portion of APs can be measured. In recent years, a large body of path inference approaches has been proposed to bridge the gap in measurement resources. However, as we show with experiments, they perform poorly in accuracy and coverage. We propose a generative measurable path inference (GMPI) framework for AS-level path measurement, which performs well in accuracy and coverage. GMPI addresses two limitations of previous approaches: 1) Information incompleteness due to unrevealed real-world AS-level routing policies and insufficient measuring resources. 2) Knowledge isolation caused by distributed AS knowledge with different sources and inconsistent forms. To overcome these challenges, the data-driven GMPI framework invents heuristic path generation to address incompleteness and a dual-attention network to integrate the isolated knowledge. GMPI does not perform any measurement or impose any burden on the network. Our performance evaluation shows that our framework GMPI outperforms state-of-the-art approaches in terms of accuracy and coverage. In particular, compared to the state-of-the-art stitching-based baseline, GMPI provides a 42.45% improvement in coverage and a 39.97% improvement in accuracy. The experimental results demonstrate that GMPI can accurately infer paths for nearly arbitrary APs.}
}


@article{DBLP:journals/ton/HuMHZZMBPG23,
	author = {Jiangqi Hu and
                  Sabarish Krishna Moorthy and
                  Ankush Harindranath and
                  Josh Zhaoxi Zhang and
                  Zhiyuan Zhao and
                  Nicholas Mastronarde and
                  Elizabeth Serena Bentley and
                  Scott Pudlewski and
                  Zhangyu Guan},
	title = {A Mobility-Resilient Spectrum Sharing Framework for Operating Wireless
                  UAVs in the 6 GHz Band},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3128--3142},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3274354},
	doi = {10.1109/TNET.2023.3274354},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuMHZZMBPG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {To mitigate the long-term spectrum crunch problem, the FCC recently opened up the 6 GHz frequency band for unlicensed use. However, the existing spectrum sharing strategies cannot support the operation of access points in moving vehicles such as cars and UAVs. This is primarily because of the directionality-based spectrum sharing among the incumbent systems in this band and the high mobility of the moving vehicles, which together make it challenging to control the cross-system interference. In this paper, we propose SwarmShare, a mobility-resilient spectrum sharing framework for swarm UAV networking in the 6 GHz band. We first present a mathematical formulation of the SwarmShare problem, where the objective is to maximize the spectral efficiency of the UAV network by jointly controlling the flight and transmission power of the UAVs and their association with the ground users, under the interference constraints of the incumbent system. We find that there are no closed-form mathematical models that can be used to characterize the statistical behaviors of the aggregate interference from the UAVs to the incumbent system. Then we propose a data-driven three-phase spectrum sharing approach, including Initial Power Enforcement, Offline-dataset Guided Online Power Adaptation, and Reinforcement Learning-based UAV Optimization. We validate the effectiveness of SwarmShare through an extensive simulation campaign. Results indicate that, based on SwarmShare, the aggregate interference from the UAVs to the incumbent system can be effectively kept below the target level without requiring the real-time cross-system channel state information. The mobility resilience of SwarmShare is also validated in coexisting networks with no precise UAV location information.}
}


@article{DBLP:journals/ton/TangNMA23,
	author = {Bin Tang and
                  Hung Ngo and
                  Yan Ma and
                  Basil Alhakami},
	title = {DAO\({}^{\mbox{2}}\): Overcoming Overall Storage Overflow in Intermittently
                  Connected Sensor Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3143--3158},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3273553},
	doi = {10.1109/TNET.2023.3273553},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TangNMA23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Many emerging sensor network applications operate in challenging environments wherein the base station is unavailable. Data generated from such intermittently connected sensor networks (ICSNs) must be stored inside the network for some unpredictable time before uploading opportunities become available. Consequently, sensory data could overflow the limited storage capacity available in the entire network, making discarding valuable data inevitable. To overcome such overall storage overflow in ICSNs, we propose and study a new algorithmic framework called d ata a ggregation for o verall storage o verflow (\nDAO\n2\n). Utilizing spatial data correlation that commonly exists among sensory data,\nDAO\n2\nemploys data aggregation techniques to reduce the overflow data size while minimizing the total energy consumption in data aggregation. At the core of our framework are two new graph theoretical problems that have not been studied. We refer to them as t raveling s alesmen p lacement p roblem (\nTSP\n2\n) and quota traveling salesmen placement problem (Q-\nTSP\n2\n). Different from the well-known multiple traveling salesman problem (mTSP) and its variants, which mainly focus on the routing of multiple salesmen initially located at fixed locations,\nTSP\n2\nand Q-\nTSP\n2\nmust decide the placement as well as the routing of the traveling salesmen. We prove that both problems are NP-hard and design approximation, heuristic, and distributed algorithms. Our algorithms outperform the state-of-the-art data aggregation work with base stations by up to 71.8% in energy consumption.}
}


@article{DBLP:journals/ton/YangLWLSSLW23,
	author = {Yongjian Yang and
                  Kaihao Lou and
                  En Wang and
                  Wenbin Liu and
                  Jianwen Shang and
                  Xueting Song and
                  Dawei Li and
                  Jie Wu},
	title = {Multi-Agent Reinforcement Learning Based File Caching Strategy in
                  Mobile Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3159--3174},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3278032},
	doi = {10.1109/TNET.2023.3278032},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/YangLWLSSLW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Mobile edge computing (MEC) reduces data service latency by pushing data to the network edge. However, due to the dynamic and diverse requests of mobile users, the problem of mobile edge caching is more complex than cloud caching. Therefore, the existing model-based caching strategies cannot be directly used in the mobile edge caching environment. Besides, when taking the cooperative storage relationship between neighbor edge servers into consideration, the caching problem becomes more difficult. To this end, we formulate an mobile edge caching problem to minimize the total latency in mobile edge computing. Firstly, a heuristic caching strategy is proposed to solve the mobile edge caching problem in the single-time-slot scenario. Then, with the consideration of users’ mobility and the correlation of files, we propose a caching strategy for the multiple-time-slot scenario based on multi-agent deep reinforcement learning. To address the cold start problem in deep reinforcement learning, we adopt the proposed heuristic caching strategy used in the single-time-slot scenario to further optimize the training results. Extensive experiments on generated data and real-world datasets are conducted to verify that the proposed edge caching strategies can achieve the minimum latency compared with the state-of-the-art strategies.}
}


@article{DBLP:journals/ton/GopalamHW23,
	author = {Swaroop Gopalam and
                  Stephen V. Hanly and
                  Philip Whiting},
	title = {Distributed Resource Allocation and Flow Control Algorithms for mmWave
                  {IAB} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3175--3190},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3273349},
	doi = {10.1109/TNET.2023.3273349},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/GopalamHW23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper presents a new distributed slot reservation frame-work for joint resource allocation and flow control in mmWave IAB networks. We derive the Dynamic Slot Reservation (DSR) algorithm from a novel approach to solve a minimum clearing time linear program in a completely distributed manner. The algorithm to solve this problem, the Static Slot Reservation (SSR) algorithm, is also a contribution of the paper. We compare the delay performance of the DSR algorithm with a well known optimal, centralized algorithm, the joint-MWM algorithm, for a realistic IAB network scenario of multi-hop flows. We show that flows that traverse several links have significantly lower delays under DSR than under the joint-MWM algorithm. This paper also provides an instantaneous rate control policy for IAB networks which changes flow rates based on the number of flows at each node in the network. The flow rates under this policy are the same as the steady-state flow rates achieved by the DSR algorithm. We prove that the proposed flow control policy provides stability for all flow arrival rate vectors that are achievable by any flow control policy. This paper provides distributed admission control policies to provide rate and/or latency guarantees to flows under dynamic scenarios with stochastic flow arrivals and changing access link rates.}
}


@article{DBLP:journals/ton/FanHWGWLYTU23,
	author = {Zhuochen Fan and
                  Zhoujing Hu and
                  Yuhan Wu and
                  Jiarui Guo and
                  Sha Wang and
                  Wenrui Liu and
                  Tong Yang and
                  Yaofeng Tu and
                  Steve Uhlig},
	title = {PISketch: Finding Persistent and Infrequent Flows},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3191--3206},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3272287},
	doi = {10.1109/TNET.2023.3272287},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/FanHWGWLYTU23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding persistent and low-active activity periods is very helpful in practice, for example to detect intrusion activities. Most of the literature focuses on finding persistent flows or frequent flows. No previous work is able to find persistent and infrequent flows. In this paper, we propose a novel sketch data structure, PISketch, to find persistent and infrequent flows in real time. The key idea of PISketch is to define a weight and its Reward and Penalty System for each flow to combine and balance the information of both persistency and infrequency, and to keep high-weighted flows in a limited space through a strategy. We implement PISketch on P4, FPGA, and CPU platforms, and compare the performance of PISketch with two strawman solutions (On-Off + CM sketch, and PIE + CM sketch), in terms of finding persistent and infrequent flows. Our experimental results demonstrate the advantage of PISketch, by comparing it to two strawman solutions: 1) The F1 Score of PISketch is around 22.1% and 57.6% higher than two strawman solutions, respectively; 2) The Average Relative Error (ARE) of PISketch is around 820.9 (up to 1188.8) and 126.2 (up to 265.6) times lower than two strawman solutions, respectively; 3) The insertion throughput of PISketch is around 1.23 and 16.5 times higher than two strawman solutions, respectively. Moreover, we implement two concrete cases of PISketch through end-to-end experiments. All of our codes are available at GitHub.}
}


@article{DBLP:journals/ton/TangHL23,
	author = {Lu Tang and
                  Qun Huang and
                  Patrick P. C. Lee},
	title = {MVPipe: Enabling Lightweight Updates and Fast Convergence in Hierarchical
                  Heavy Hitter Detection},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3207--3221},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3273307},
	doi = {10.1109/TNET.2023.3273307},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/TangHL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {Finding hierarchical heavy hitters (HHHs) (i.e., hierarchical aggregates with exceptionally huge amounts of traffic) is critical to network management, yet it is often challenged by the requirements of fast packet processing, real-time and accurate detection, as well as resource efficiency. Existing HHH detection schemes either incur expensive packet updates for multiple aggregation levels in the IP address hierarchy, or need to process sufficient packets to converge to the required detection accuracy. We present MVPipe, an invertible sketch that achieves both lightweight updates and fast convergence in HHH detection. MVPipe builds on the skewness property of IP traffic to process packets via a pipeline of majority voting executions, such that most packets can be updated for only one or few aggregation levels in the IP address hierarchy. We show how MVPipe can be feasibly deployed in P4-based programmable switches subject to limited switch resources. We also theoretically analyze the accuracy and coverage properties of MVPipe. Evaluation with real-world Internet traces shows that MVPipe achieves high accuracy, high throughput, and fast convergence compared to six state-of-the-art HHH detection schemes. It also incurs low resource overhead in the Tofino switch deployment.}
}


@article{DBLP:journals/ton/LiYWYYLC23,
	author = {Feng Li and
                  Xuyang Yuan and
                  Lina Wang and
                  Huan Yang and
                  Dongxiao Yu and
                  Weifeng Lyu and
                  Xiuzhen Cheng},
	title = {Collaborative Learning in General Graphs With Limited Memorization:
                  Complexity, Learnability, and Reliability},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3222--3237},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3273501},
	doi = {10.1109/TNET.2023.3273501},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiYWYYLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {We consider a\nK\n-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorizing capabilities and communication bandwidth. The goal is to let each of the agents eventually learn the best arm. Although recent studies show the power of collaboration among the agents in improving the efficacy of learning, it is assumed in these studies that the communication graph should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since the agents memorize and communicate very few experiences. Additionally, an agent may be corrupted to share falsified experiences to its peers, while the resource limit in terms of memorization and communication may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their latest experiences with each other through light-weight random walks in a general communication graph, and then make decisions on which arms to pull according to the recommendations received from their peers. The agents finally update their adoptions (i.e., preferences to the arms) based on the reward obtained by pulling the arms. Our theoretical analysis shows that, when there are a sufficient number of agents participating in the collaborative learning process, all the agents eventually learn the best arm with high probability, even with limited memorizing capabilities and light-weight communications. We also reveal in our theoretical analysis the upper bound on the number of corrupted agents our algorithm can tolerate. The efficacy of our proposed three-staged collaborative learning algorithm is finally verified by extensive experiments on both synthetic and real datasets.}
}


@article{DBLP:journals/ton/LiuLYLC23,
	author = {Kai Liu and
                  Chunhui Liu and
                  Guozhi Yan and
                  Victor C. S. Lee and
                  Jiannong Cao},
	title = {Accelerating {DNN} Inference With Reliability Guarantee in Vehicular
                  Edge Computing},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3238--3253},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3279512},
	doi = {10.1109/TNET.2023.3279512},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/LiuLYLC23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {This paper explores on accelerating Deep Neural Network (DNN) inference with reliability guarantee in Vehicular Edge Computing (VEC) by considering the synergistic impacts of vehicle mobility and Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. First, we show the necessity of striking a balance between DNN inference acceleration and reliability in VEC, and give insights into the design rationale by analyzing the features of overlapped DNN partitioning and mobility-aware task offloading. Second, we formulate the Cooperative Partitioning and Offloading (CPO) problem by presenting a cooperative DNN partitioning and offloading scenario, followed by deriving an offloading reliability model and a DNN inference delay model. The CPO is proved as NP-hard. Third, we propose two approximation algorithms, i.e., Submodular Approximation Allocation Algorithm (SA3) and Feed Me the Rest algorithm (FMtR). In particular, SA3 determines the edge allocation in a centralized way, which achieves 1/3-optimal approximation on maximizing the inference reliability. On this basis, FMtR partitions the DNN models and offloads the tasks to the allocated edge nodes in a distributed way, which achieves 1/2-optimal approximation on maximizing the inference reliability. Finally, we build the simulation model and give a comprehensive performance evaluation, which demonstrates the superiority of the proposed solutions.}
}


@article{DBLP:journals/ton/WangYMGL23,
	author = {Xiaolong Wang and
                  Haipeng Yao and
                  Tianle Mai and
                  Song Guo and
                  Yun{-}Jie Liu},
	title = {Reinforcement Learning-Based Particle Swarm Optimization for End-to-End
                  Traffic Scheduling in {TSN-5G} Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3254--3268},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3276363},
	doi = {10.1109/TNET.2023.3276363},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/WangYMGL23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {With the rapid development of the Industrial Internet of Things (IIoT), massive IIoT devices connect to industrial networks via wired and wireless. Furthermore, industrial networks pose new requirements on communications, such as strict latency boundaries, ultra-reliable transmission, and so on. To this end, time-sensitive networking (TSN) embedded fifth-generation (5G) wireless communication technology (i.e., TSN-5G networks), is considered the most promising solution to address these challenges. TSN can provide deterministic end-to-end latency and reliability for real-time applications in wired networks. 5G supports ultra-reliable and low-latency communications (uRLLC), providing increased flexibility and inherent mobility support in the wireless network. Thus, the integration of TSN and 5G provides numerous benefits, including increased flexibility, lower commissioning costs, and seamless interoperability of various devices, regardless of whether they use a wired or wireless interface. Nonetheless, the potential barriers between the TSN and 5G systems, such as clock synchronization and end-to-end traffic scheduling, are inevitable. Time synchronization has been studied in many works, so this paper focuses on the end-to-end traffic scheduling problem in TSN-5G networks. We propose a novel integrated TSN and 5G industrial network architecture, where the 5G system acts as a logical TSN-capable bridge. Based on this network architecture, we design a Double Q-learning based hierarchical particle swarm optimization algorithm (DQHPSO) to search for the optimal scheduling solution. The DQHPSO algorithm adopts a level-based population structure and introduces Double Q-learning to adjust the number of levels in the population, which evades the local optimum to further improve the search efficiency. Extensive simulations demonstrate that the DQHPSO algorithm can increase the scheduling success ratio of time-triggered flows compared to other algorithms.}
}


@article{DBLP:journals/ton/RohrerT23,
	author = {Elias Rohrer and
                  Florian Tschorsch},
	title = {Kadcast-NG: {A} Structured Broadcast Protocol for Blockchain Networks},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3269--3283},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3279897},
	doi = {10.1109/TNET.2023.3279897},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/RohrerT23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In order to propagate transactions and blocks, today’s blockchain systems rely on unstructured peer-to-peer overlay networks. In such networks, broadcast is known to be an inefficient operation in terms of message complexity and overhead. In addition to the impact on the system performance, inefficient or delayed block propagation may have severe consequences regarding security and fairness of the consensus layer. In contrast, the Kadcast protocol is a structured peer-to-peer protocol for block and transaction propagation in blockchain networks. Kadcast utilizes the well-known overlay topology of Kademlia to realize an efficient broadcast operation with tunable overhead. We study the security and privacy of the Kadcast protocol based on probabilistic models and analyze its resilience to packet losses and node failures. Moreover, we evaluate Kadcast’s block delivery performance, broadcast reliability, efficiency, and security based on advanced network simulations. Lastly, we introduce a QUIC-based prototype implementation of the Kadcast protocol and show its merits through deployment in a global-scale cloud-based testbed.}
}


@article{DBLP:journals/ton/HuangPLXYZG23,
	author = {Huawei Huang and
                  Xiaowen Peng and
                  Yue Lin and
                  Miaoyong Xu and
                  Guang Ye and
                  Zibin Zheng and
                  Song Guo},
	title = {Scheduling Most Valuable Committees for the Sharded Blockchain},
	journal = {{IEEE/ACM} Trans. Netw.},
	volume = {31},
	number = {6},
	pages = {3284--3299},
	year = {2023},
	url = {https://doi.org/10.1109/TNET.2023.3278456},
	doi = {10.1109/TNET.2023.3278456},
	timestamp = {Sat, 13 Jan 2024 17:36:31 +0100},
	biburl = {https://dblp.org/rec/journals/ton/HuangPLXYZG23.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	abstract = {In a sharded blockchain, transactions are processed by a number of parallel committees. Thus, the transaction throughput can be largely boosted. A problem is that some groups of blockchain nodes consume large latency to form committees at the beginning of each epoch. Moreover, the heterogeneous processing capabilities of different committees also result in imbalanced consensus latency. Such imbalanced two-phase latency brings a large cumulative age to the transactions pending in transaction pool. Consequently, the blockchain throughput can be significantly degraded. We believe that a good committee-scheduling strategy can reduce the cumulative age of transactions, and thus benefit the throughput. However, we have not yet found a committee-scheduling mechanism that works for accelerating block formation in the context of blockchain sharding. To this end, this paper studies a fine-balanced tradeoff between the transactions’ throughput and their cumulative age in a large-scale sharded blockchain. We formulate this tradeoff as a utility-maximization problem, which is proved NP-hard. To solve this problem, we propose an online distributed Stochastic-Exploration (SE) algorithm, which guarantees a near-optimal system utility. We then rigorously analyze three theoretical properties of the proposed algorithm, including the theoretical convergence time, the probability of committees’ failure due to Sybil attacks, as well as the performance perturbation brought by committees’ offline events. Finally, we evaluate the proposed algorithm using the dataset of real-world blockchain transactions. The simulation results demonstrate that the proposed SE algorithm outperforms other baselines in terms of system utility, the valuable degree of yielded solutions, latency, and throughput performance.}
}
